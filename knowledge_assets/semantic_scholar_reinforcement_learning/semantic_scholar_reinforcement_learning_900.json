[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a8ef08940341381390d9a5672546354d0ce51328",
            "@type": "ScholarlyArticle",
            "paperId": "a8ef08940341381390d9a5672546354d0ce51328",
            "corpusId": 49310753,
            "url": "https://www.semanticscholar.org/paper/a8ef08940341381390d9a5672546354d0ce51328",
            "title": "Maximum a Posteriori Policy Optimisation",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963884015",
                "ArXiv": "1806.06920",
                "DBLP": "conf/iclr/AbdolmalekiSTMH18",
                "CorpusId": 49310753
            },
            "abstract": "We introduce a new algorithm for reinforcement learning called Maximum aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relative entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings while achieving similar or better final performance.",
            "referenceCount": 51,
            "citationCount": 377,
            "influentialCitationCount": 67,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.06920"
            },
            "citationStyles": {
                "bibtex": "@Article{Abdolmaleki2018MaximumAP,\n author = {A. Abdolmaleki and J. T. Springenberg and Yuval Tassa and R. Munos and N. Heess and Martin A. Riedmiller},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Maximum a Posteriori Policy Optimisation},\n volume = {abs/1806.06920},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e0456b744af84f07cb5e750217463214f96c921e",
            "@type": "ScholarlyArticle",
            "paperId": "e0456b744af84f07cb5e750217463214f96c921e",
            "corpusId": 2984847,
            "url": "https://www.semanticscholar.org/paper/e0456b744af84f07cb5e750217463214f96c921e",
            "title": "Relative Entropy Policy Search",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "conf/aaai/PetersMA10",
                "MAG": "1499669280",
                "DOI": "10.1609/aaai.v24i1.7727",
                "CorpusId": 2984847
            },
            "abstract": "\n \n Policy search is a successful approach to reinforcement learning. However, policy improvements often result in the loss of information. Hence, it has been marred by premature convergence and implausible solutions. As first suggested in the context of covariant policy gradients, many of these problems may be addressed by constraining the information loss. In this paper, we continue this path of reasoning and suggest the Relative Entropy Policy Search (REPS) method. The resulting method differs significantly from previous policy gradient approaches and yields an exact update step. It can be shown to work well on typical reinforcement learning benchmark problems.\n \n",
            "referenceCount": 19,
            "citationCount": 656,
            "influentialCitationCount": 87,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/7727/7588",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-07-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Peters2010RelativeEP,\n author = {Jan Peters and Katharina Muelling and Y. Altun},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {1607-1612},\n title = {Relative Entropy Policy Search},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d355e339298fc2ab920688c1709d4ba6476a2bc6",
            "@type": "ScholarlyArticle",
            "paperId": "d355e339298fc2ab920688c1709d4ba6476a2bc6",
            "corpusId": 3517962,
            "url": "https://www.semanticscholar.org/paper/d355e339298fc2ab920688c1709d4ba6476a2bc6",
            "title": "Distributed Distributional Deterministic Policy Gradients",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2952257357",
                "ArXiv": "1804.08617",
                "DBLP": "journals/corr/abs-1804-08617",
                "CorpusId": 3517962
            },
            "abstract": "This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of $N$-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.",
            "referenceCount": 28,
            "citationCount": 385,
            "influentialCitationCount": 56,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1804.08617"
            },
            "citationStyles": {
                "bibtex": "@Article{Barth-Maron2018DistributedDD,\n author = {Gabriel Barth-Maron and Matthew W. Hoffman and D. Budden and Will Dabney and Dan Horgan and TB Dhruva and Alistair Muldal and N. Heess and T. Lillicrap},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Distributed Distributional Deterministic Policy Gradients},\n volume = {abs/1804.08617},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:27dfecb6bb0308c7484e13dcaefd5eeebba677d3",
            "@type": "ScholarlyArticle",
            "paperId": "27dfecb6bb0308c7484e13dcaefd5eeebba677d3",
            "corpusId": 3536221,
            "url": "https://www.semanticscholar.org/paper/27dfecb6bb0308c7484e13dcaefd5eeebba677d3",
            "title": "Model-Ensemble Trust-Region Policy Optimization",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2952152670",
                "DBLP": "journals/corr/abs-1802-10592",
                "ArXiv": "1802.10592",
                "CorpusId": 3536221
            },
            "abstract": "Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity, which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks.",
            "referenceCount": 47,
            "citationCount": 381,
            "influentialCitationCount": 45,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.10592"
            },
            "citationStyles": {
                "bibtex": "@Article{Kurutach2018ModelEnsembleTP,\n author = {Thanard Kurutach and I. Clavera and Yan Duan and Aviv Tamar and P. Abbeel},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Model-Ensemble Trust-Region Policy Optimization},\n volume = {abs/1802.10592},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ee9aae0190c3ff177e7d1c9cdc977ce7d971bcc1",
            "@type": "ScholarlyArticle",
            "paperId": "ee9aae0190c3ff177e7d1c9cdc977ce7d971bcc1",
            "corpusId": 15898758,
            "url": "https://www.semanticscholar.org/paper/ee9aae0190c3ff177e7d1c9cdc977ce7d971bcc1",
            "title": "Input Convex Neural Networks",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2525954470",
                "DBLP": "conf/icml/AmosXK17",
                "ArXiv": "1609.07152",
                "CorpusId": 15898758
            },
            "abstract": "This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with a minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.",
            "referenceCount": 51,
            "citationCount": 383,
            "influentialCitationCount": 90,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-09-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Amos2016InputCN,\n author = {Brandon Amos and Lei Xu and J. Z. Kolter},\n booktitle = {International Conference on Machine Learning},\n pages = {146-155},\n title = {Input Convex Neural Networks},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:93a636f0b6d450217fda5aaa26c74bb6b232f498",
            "@type": "ScholarlyArticle",
            "paperId": "93a636f0b6d450217fda5aaa26c74bb6b232f498",
            "corpusId": 3502463,
            "url": "https://www.semanticscholar.org/paper/93a636f0b6d450217fda5aaa26c74bb6b232f498",
            "title": "Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1802-09127",
                "ArXiv": "1802.09127",
                "MAG": "2949264726",
                "CorpusId": 3502463
            },
            "abstract": "Recent advances in deep reinforcement learning have made significant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for flexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting.",
            "referenceCount": 58,
            "citationCount": 301,
            "influentialCitationCount": 49,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.09127"
            },
            "citationStyles": {
                "bibtex": "@Article{Riquelme2018DeepBB,\n author = {C. Riquelme and G. Tucker and Jasper Snoek},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling},\n volume = {abs/1802.09127},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:96893c9d1e00213ffb8570391f7f4f7f3c2c4e56",
            "@type": "ScholarlyArticle",
            "paperId": "96893c9d1e00213ffb8570391f7f4f7f3c2c4e56",
            "corpusId": 205431649,
            "url": "https://www.semanticscholar.org/paper/96893c9d1e00213ffb8570391f7f4f7f3c2c4e56",
            "title": "Choice, uncertainty and value in prefrontal and cingulate cortex",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "1983230739",
                "DOI": "10.1038/nn2066",
                "CorpusId": 205431649,
                "PubMed": "18368045"
            },
            "abstract": null,
            "referenceCount": 104,
            "citationCount": 839,
            "influentialCitationCount": 29,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2008-04-01",
            "journal": {
                "name": "Nature Neuroscience",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Rushworth2008ChoiceUA,\n author = {M. Rushworth and T. Behrens},\n booktitle = {Nature Neuroscience},\n journal = {Nature Neuroscience},\n pages = {389-397},\n title = {Choice, uncertainty and value in prefrontal and cingulate cortex},\n volume = {11},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:16cd7a24d117f61da645fba1b4cc31839dff2805",
            "@type": "ScholarlyArticle",
            "paperId": "16cd7a24d117f61da645fba1b4cc31839dff2805",
            "corpusId": 4311535,
            "url": "https://www.semanticscholar.org/paper/16cd7a24d117f61da645fba1b4cc31839dff2805",
            "title": "Computational roles for dopamine in behavioural control",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2165368112",
                "DOI": "10.1038/nature03015",
                "CorpusId": 4311535,
                "PubMed": "15483596"
            },
            "abstract": null,
            "referenceCount": 73,
            "citationCount": 957,
            "influentialCitationCount": 39,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2004-10-14",
            "journal": {
                "name": "Nature",
                "volume": "431"
            },
            "citationStyles": {
                "bibtex": "@Article{Montague2004ComputationalRF,\n author = {P. Montague and S. Hyman and J. Cohen and J. Cohen},\n booktitle = {Nature},\n journal = {Nature},\n pages = {760-767},\n title = {Computational roles for dopamine in behavioural control},\n volume = {431},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7f4bc8fca3f65f452caee4ce1edc0e806cb37a5f",
            "@type": "ScholarlyArticle",
            "paperId": "7f4bc8fca3f65f452caee4ce1edc0e806cb37a5f",
            "corpusId": 13032768,
            "url": "https://www.semanticscholar.org/paper/7f4bc8fca3f65f452caee4ce1edc0e806cb37a5f",
            "title": "INFLUENCE OF MODEL'S REINFORCEMENT CONTINGENCIES ON THE ACQUISITION OF IMITATIVE RESPONSES.",
            "venue": "Journal of Personality and Social Psychology",
            "publicationVenue": {
                "id": "urn:research:dc42ed9c-43ca-4994-b7c6-6321b95442a1",
                "name": "Journal of Personality and Social Psychology",
                "alternate_names": [
                    "J Personal Soc Psychol"
                ],
                "issn": "0022-3514",
                "url": "http://www.apa.org/journals/psp.html"
            },
            "year": 1965,
            "externalIds": {
                "MAG": "2114860556",
                "DOI": "10.1037/H0022070",
                "CorpusId": 13032768,
                "PubMed": "14300234"
            },
            "abstract": "In order to test the hypothesis that reinforcements administered to a model influence the performance but not the acquisition of matching responses, groups of children observed an aggressive film-mediated model either rewarded, punished, or left without consequences. A postexposure test revealed that response consequences to the model had produced differential amounts of imitative behavior. Children in the model-punished condition performed significantly fewer matching responses than children in both the model-rewarded and the no-consequences groups. Children in all 3 treatment conditions were then offered attractive reinforcers contingent on their reproducing the model's aggressive responses. The introduction of positive incentives completely wiped out the previously observed performance differences, revealing an equivalent amount of learning among children in the model-rewarded, model-punished, and the no-consequences conditions.",
            "referenceCount": 18,
            "citationCount": 989,
            "influentialCitationCount": 47,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1965-06-01",
            "journal": {
                "name": "Journal of personality and social psychology",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Bandura1965INFLUENCEOM,\n author = {A. Bandura},\n booktitle = {Journal of Personality and Social Psychology},\n journal = {Journal of personality and social psychology},\n pages = {\n          589-95\n        },\n title = {INFLUENCE OF MODEL'S REINFORCEMENT CONTINGENCIES ON THE ACQUISITION OF IMITATIVE RESPONSES.},\n volume = {1},\n year = {1965}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fe1e8ce89501bacfba2cc04dbc781538739c7dcf",
            "@type": "ScholarlyArticle",
            "paperId": "fe1e8ce89501bacfba2cc04dbc781538739c7dcf",
            "corpusId": 16893374,
            "url": "https://www.semanticscholar.org/paper/fe1e8ce89501bacfba2cc04dbc781538739c7dcf",
            "title": "A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "journals/jmlr/RossPCK11",
                "MAG": "2168839459",
                "DOI": "10.5555/1953048.2021055",
                "CorpusId": 16893374
            },
            "abstract": "Bayesian learning methods have recently been shown to provide an elegant solution to the exploration-exploitation trade-off in reinforcement learning. However most investigations of Bayesian reinforcement learning to date focus on the standard Markov Decision Processes (MDPs). The primary focus of this paper is to extend these ideas to the case of partially observable domains, by introducing the Bayes-Adaptive Partially Observable Markov Decision Processes. This new framework can be used to simultaneously (1) learn a model of the POMDP domain through interaction with the environment, (2) track the state of the system under partial observability, and (3) plan (near-)optimal sequences of actions. An important contribution of this paper is to provide theoretical results showing how the model can be finitely approximated while preserving good learning performance. We present approximate algorithms for belief tracking and planning in this model, as well as empirical results that illustrate how the model estimate and agent's return improve as a function of experience.",
            "referenceCount": 62,
            "citationCount": 142,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-02-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Ross2011ABA,\n author = {St\u00e9phane Ross and Joelle Pineau and B. Chaib-draa and Pierre Kreitmann},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1729-1770},\n title = {A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes},\n volume = {12},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:188030804d0e5816762cca70e12cea9f18e0a182",
            "@type": "ScholarlyArticle",
            "paperId": "188030804d0e5816762cca70e12cea9f18e0a182",
            "corpusId": 141411992,
            "url": "https://www.semanticscholar.org/paper/188030804d0e5816762cca70e12cea9f18e0a182",
            "title": "A Model of How the Basal Ganglia Generate and Use Neural Signals That Predict Reinforcement",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1994,
            "externalIds": {
                "MAG": "125252782",
                "DOI": "10.7551/mitpress/4708.003.0020",
                "CorpusId": 141411992
            },
            "abstract": "This chapter contains sections titled: Introduction, Dopamine Neurons, Organization of Strtosomal Modules, Mechanism of Responsiveness to Predictors of Reinforcement, Correspondence with the Theory of Adaptive Critics, Learning to Predict Primary Reinforcement, Learning Earlier Predictors of Reinforcement, Relation to the Actor-Critic Architecture, More Realistic Assumptions, Summary, Acknowledgments, References",
            "referenceCount": 0,
            "citationCount": 682,
            "influentialCitationCount": 55,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Houk1994AMO,\n author = {J. Houk and Joel L. Davis and D. Beiser},\n pages = {249-270},\n title = {A Model of How the Basal Ganglia Generate and Use Neural Signals That Predict Reinforcement},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1f35bc07e58aed3bdd86e2ce3f644362480dba12",
            "@type": "ScholarlyArticle",
            "paperId": "1f35bc07e58aed3bdd86e2ce3f644362480dba12",
            "corpusId": 8868406,
            "url": "https://www.semanticscholar.org/paper/1f35bc07e58aed3bdd86e2ce3f644362480dba12",
            "title": "Optimal decision making and the anterior cingulate cortex",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2009639618",
                "DOI": "10.1038/nn1724",
                "CorpusId": 8868406,
                "PubMed": "16783368"
            },
            "abstract": null,
            "referenceCount": 52,
            "citationCount": 882,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Study"
            ],
            "publicationDate": "2006-06-18",
            "journal": {
                "name": "Nature Neuroscience",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Kennerley2006OptimalDM,\n author = {S. Kennerley and M. Walton and T. Behrens and M. Buckley and M. Rushworth},\n booktitle = {Nature Neuroscience},\n journal = {Nature Neuroscience},\n pages = {940-947},\n title = {Optimal decision making and the anterior cingulate cortex},\n volume = {9},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5866d8dc03f32487e4318612ba11642f633e358d",
            "@type": "ScholarlyArticle",
            "paperId": "5866d8dc03f32487e4318612ba11642f633e358d",
            "corpusId": 14588175,
            "url": "https://www.semanticscholar.org/paper/5866d8dc03f32487e4318612ba11642f633e358d",
            "title": "Neural networks and brain function",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1546197084",
                "DOI": "10.1093/acprof:oso/9780198524328.001.0001",
                "CorpusId": 14588175
            },
            "abstract": "Pattern association memory autoassociation memory competitive networks, including self-organizing maps error-correcting networks - perceptrons, backpropagation of error in multilayer networks, and reinforcement learning algorithms hippocampus and memory pattern association in the brain - amygdala and orbitofrontal cortex cortical networks for invariant pattern recognition motor systems - cerebellum and basal ganglia cerebral neocortex. Appendix 1: introduction to linear algebra for neural networks. Appendix 2: Information theory. Appendix 3: Pattern associators. Appendix 4: Autoassociators. Appendix 5: Recurrent dynamics.",
            "referenceCount": 609,
            "citationCount": 872,
            "influentialCitationCount": 58,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Rolls1998NeuralNA,\n author = {E. Rolls and A. Treves},\n title = {Neural networks and brain function},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:682d194235ba3b573889836ba118502e8b525728",
            "@type": "ScholarlyArticle",
            "paperId": "682d194235ba3b573889836ba118502e8b525728",
            "corpusId": 3535369,
            "url": "https://www.semanticscholar.org/paper/682d194235ba3b573889836ba118502e8b525728",
            "title": "Backpropagation through the Void: Optimizing control variates for black-box gradient estimation",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2963851840",
                "DBLP": "journals/corr/abs-1711-00123",
                "ArXiv": "1711.00123",
                "CorpusId": 3535369
            },
            "abstract": "Gradient-based optimization is the foundation of deep learning and reinforcement learning. Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables. Our method uses gradients of a neural network trained jointly with model parameters or policies, and is applicable in both discrete and continuous settings. We demonstrate this framework for training discrete latent-variable models. We also give an unbiased, action-conditional extension of the advantage actor-critic reinforcement learning algorithm.",
            "referenceCount": 41,
            "citationCount": 272,
            "influentialCitationCount": 52,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-10-31",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1711.00123"
            },
            "citationStyles": {
                "bibtex": "@Article{Grathwohl2017BackpropagationTT,\n author = {Will Grathwohl and Dami Choi and Yuhuai Wu and Geoffrey Roeder and D. Duvenaud},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},\n volume = {abs/1711.00123},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:37088dec26231bc5a4937054ebc862bb83a3db4d",
            "@type": "ScholarlyArticle",
            "paperId": "37088dec26231bc5a4937054ebc862bb83a3db4d",
            "corpusId": 15938338,
            "url": "https://www.semanticscholar.org/paper/37088dec26231bc5a4937054ebc862bb83a3db4d",
            "title": "Neural Episodic Control",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/PritzelUSBVHWB17",
                "MAG": "2594466397",
                "ArXiv": "1703.01988",
                "CorpusId": 15938338
            },
            "abstract": "Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.",
            "referenceCount": 47,
            "citationCount": 284,
            "influentialCitationCount": 45,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1703.01988"
            },
            "citationStyles": {
                "bibtex": "@Article{Pritzel2017NeuralEC,\n author = {Alexander Pritzel and Benigno Uria and Sriram Srinivasan and A. Badia and Oriol Vinyals and Demis Hassabis and Daan Wierstra and Charles Blundell},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Neural Episodic Control},\n volume = {abs/1703.01988},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:621c03cd67b0b7bac665f3c7887481b4b42f269c",
            "@type": "ScholarlyArticle",
            "paperId": "621c03cd67b0b7bac665f3c7887481b4b42f269c",
            "corpusId": 4449439,
            "url": "https://www.semanticscholar.org/paper/621c03cd67b0b7bac665f3c7887481b4b42f269c",
            "title": "Asynchronous Stochastic Approximation and Q-Learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1994,
            "externalIds": {
                "MAG": "2147750403",
                "DOI": "10.1023/A:1022689125041",
                "CorpusId": 4449439
            },
            "abstract": null,
            "referenceCount": 18,
            "citationCount": 749,
            "influentialCitationCount": 72,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1022689125041.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1994-09-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "16"
            },
            "citationStyles": {
                "bibtex": "@Article{Tsitsiklis1994AsynchronousSA,\n author = {J. Tsitsiklis},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {185-202},\n title = {Asynchronous Stochastic Approximation and Q-Learning},\n volume = {16},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:154e8048e5171b79dcff76c8642afd02a5466768",
            "@type": "ScholarlyArticle",
            "paperId": "154e8048e5171b79dcff76c8642afd02a5466768",
            "corpusId": 41302553,
            "url": "https://www.semanticscholar.org/paper/154e8048e5171b79dcff76c8642afd02a5466768",
            "title": "Learning in embedded systems",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1993,
            "externalIds": {
                "MAG": "1931792391",
                "DBLP": "phd/us/Kaelbling90",
                "DOI": "10.7551/mitpress/4168.001.0001",
                "CorpusId": 41302553
            },
            "abstract": "This dissertation addresses the problem of designing algorithms for learning in embedded systems. This problem differs from the traditional supervised learning problem. An agent, finding itself in a particular input situation must generate an action. It then receives a reinforcement value from the environment, indicating how valuable the current state of the environment is for the agent. The agent cannot, however, deduce the reinforcement value that would have resulted from executing any of its other actions. A number of algorithms for learning action strategies from reinforcement values are presented and compared empirically with existing reinforcement-learning algorithms. \nThe interval-estimation algorithm uses the statistical notion of confidence intervals to guide its generation of actions in the world, trading off acting to gain information against acting to gain reinforcement. It performs well in simple domains but does not exhibit any generalization and is computationally complex. \nThe cascade algorithm is a structural credit-assignment method that allows an action strategy with many output bits to be learned by a collection of reinforcement-learning modules that learn Boolean functions. This method represents an improvement in computational complexity and often in learning rate. \nTwo algorithms for learning Boolean functions in k-DNF are described. Both are based on Valiant's algorithm for learning such functions from input-output instances. The first uses Sutton's techniques for linear association and reinforcement comparison, while the second uses techniques from the interval estimation algorithm. They both perform well and have tractable complexity. \nA generate-and-test reinforcement-learning algorithm is presented. It allows symbolic representations of Boolean functions to be constructed incrementally and tested in the environment. It is highly parametrized and can be tuned to learn a broad range of function classes. Low-complexity functions can be learned very efficiently even in the presence of large numbers of irrelevant input bits. This algorithm is extended to construct simple sequential networks using a set-reset operator, which allows the agent to learn action strategies with state. \nThese algorithms, in addition to being studied in simulation, were implemented and tested on a physical mobile robot.",
            "referenceCount": 0,
            "citationCount": 798,
            "influentialCitationCount": 46,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1993-05-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Kaelbling1993LearningIE,\n author = {L. Kaelbling},\n pages = {I-XI, 1-176},\n title = {Learning in embedded systems},\n year = {1993}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4e9d797427cd56be90932d6092fc3b6282dfb96f",
            "@type": "ScholarlyArticle",
            "paperId": "4e9d797427cd56be90932d6092fc3b6282dfb96f",
            "corpusId": 6042780,
            "url": "https://www.semanticscholar.org/paper/4e9d797427cd56be90932d6092fc3b6282dfb96f",
            "title": "On the Convergence of Stochastic Iterative Dynamic Programming Algorithms",
            "venue": "Neural Computation",
            "publicationVenue": {
                "id": "urn:research:69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                "name": "Neural Computation",
                "alternate_names": [
                    "Neural Comput"
                ],
                "issn": "0899-7667",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667"
            },
            "year": 1994,
            "externalIds": {
                "DBLP": "journals/neco/JaakkolaJS94",
                "DOI": "10.1162/neco.1994.6.6.1185",
                "CorpusId": 6042780
            },
            "abstract": "Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments. These algorithms, including the TD() algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming (DP). In this paper we provide a rigorous proof of convergence of these DP-based learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem. The theorem establishes a general class of convergent algorithms to which both TD() and Q-learning belong.",
            "referenceCount": 12,
            "citationCount": 968,
            "influentialCitationCount": 46,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Neural Computation",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Jaakkola1994OnTC,\n author = {T. Jaakkola and Michael I. Jordan and Satinder Singh},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {1185-1201},\n title = {On the Convergence of Stochastic Iterative Dynamic Programming Algorithms},\n volume = {6},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:35c96b1cf334a61ec9c81b7ee0b71f06bcc87d60",
            "@type": "ScholarlyArticle",
            "paperId": "35c96b1cf334a61ec9c81b7ee0b71f06bcc87d60",
            "corpusId": 1394000,
            "url": "https://www.semanticscholar.org/paper/35c96b1cf334a61ec9c81b7ee0b71f06bcc87d60",
            "title": "Ventromedial frontal cortex mediates affective shifting in humans: evidence from a reversal learning paradigm.",
            "venue": "Brain : a journal of neurology",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2158147140",
                "DOI": "10.1093/BRAIN/AWG180",
                "CorpusId": 1394000,
                "PubMed": "12821528"
            },
            "abstract": "How do the frontal lobes support behavioural flexibility? One key element is the ability to adjust responses when the reinforcement value of stimuli change. In monkeys, this ability--a form of affective shifting known as reversal learning--depends on orbitofrontal cortex. The present study examines the anatomical bases of reversal learning in humans. Subjects with lesions of the ventromedial prefrontal cortex were compared with a group with dorsolateral frontal lobe damage, as well as with normal controls on a simple reversal learning task. Neither form of frontal damage affected initial stimulus-reinforcement learning; ventromedial frontal damage selectively impaired reversal learning.",
            "referenceCount": 31,
            "citationCount": 614,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.psych.upenn.edu/%7Emfarah/Profrontal-Ventromedial.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2003-08-01",
            "journal": {
                "name": "Brain : a journal of neurology",
                "volume": "126 Pt 8"
            },
            "citationStyles": {
                "bibtex": "@Article{Fellows2003VentromedialFC,\n author = {L. Fellows and M. Farah},\n booktitle = {Brain : a journal of neurology},\n journal = {Brain : a journal of neurology},\n pages = {\n          1830-7\n        },\n title = {Ventromedial frontal cortex mediates affective shifting in humans: evidence from a reversal learning paradigm.},\n volume = {126 Pt 8},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e78f3223bf67c45c7fe45301ff8400b438c99e1d",
            "@type": "ScholarlyArticle",
            "paperId": "e78f3223bf67c45c7fe45301ff8400b438c99e1d",
            "corpusId": 12798902,
            "url": "https://www.semanticscholar.org/paper/e78f3223bf67c45c7fe45301ff8400b438c99e1d",
            "title": "Learning Control in Robotics",
            "venue": "IEEE robotics & automation magazine",
            "publicationVenue": {
                "id": "urn:research:bb803f8e-3f8e-4fd1-8192-391b7d4de1f1",
                "name": "IEEE robotics & automation magazine",
                "alternate_names": [
                    "IEEE robot  autom mag",
                    "IEEE Robotics & Automation Magazine",
                    "IEEE Robot  Autom Mag"
                ],
                "issn": "1070-9932",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=100"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "journals/ram/SchaalA10",
                "MAG": "1976207411",
                "DOI": "10.1109/MRA.2010.936957",
                "CorpusId": 12798902
            },
            "abstract": "Recent trends in robot learning are to use trajectory-based optimal control techniques and reinforcement learning to scale complex robotic systems. On the one hand, increased computational power and multiprocessing, and on the other hand, probabilistic reinforcement learning methods and function approximation, have contributed to a steadily increasing interest in robot learning. Imitation learning has helped significantly to start learning with reasonable initial behavior. However, many applications are still restricted to rather lowdimensional domains and toy applications. Future work will have to demonstrate the continual and autonomous learning abilities, which were alluded to in the introduction.",
            "referenceCount": 85,
            "citationCount": 177,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-06-07",
            "journal": {
                "name": "IEEE Robotics & Automation Magazine",
                "volume": "17"
            },
            "citationStyles": {
                "bibtex": "@Article{Schaal2010LearningCI,\n author = {S. Schaal and C. Atkeson},\n booktitle = {IEEE robotics & automation magazine},\n journal = {IEEE Robotics & Automation Magazine},\n pages = {20-29},\n title = {Learning Control in Robotics},\n volume = {17},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a441728f9fd6af1946368240162a72c2028c8cb1",
            "@type": "ScholarlyArticle",
            "paperId": "a441728f9fd6af1946368240162a72c2028c8cb1",
            "corpusId": 15264404,
            "url": "https://www.semanticscholar.org/paper/a441728f9fd6af1946368240162a72c2028c8cb1",
            "title": "Deep Exploration via Randomized Value Functions",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2974778612",
                "ArXiv": "1703.07608",
                "DBLP": "journals/jmlr/OsbandRRW19",
                "CorpusId": 15264404
            },
            "abstract": "We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation.",
            "referenceCount": 83,
            "citationCount": 256,
            "influentialCitationCount": 30,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-03-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1703.07608"
            },
            "citationStyles": {
                "bibtex": "@Article{Osband2017DeepEV,\n author = {Ian Osband and Daniel Russo and Zheng Wen and Benjamin Van Roy},\n booktitle = {Journal of machine learning research},\n journal = {ArXiv},\n title = {Deep Exploration via Randomized Value Functions},\n volume = {abs/1703.07608},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c84be78816fd8ed9fedaf4beabbb3386c60d41dd",
            "@type": "ScholarlyArticle",
            "paperId": "c84be78816fd8ed9fedaf4beabbb3386c60d41dd",
            "corpusId": 61209441,
            "url": "https://www.semanticscholar.org/paper/c84be78816fd8ed9fedaf4beabbb3386c60d41dd",
            "title": "Strategic Learning and Its Limits",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2160057639",
                "DOI": "10.1093/acprof:oso/9780199269181.001.0001",
                "CorpusId": 61209441
            },
            "abstract": "In this concise book based on his Arne Ryde Lectures in 2002, Young suggests a conceptual framework for studying strategic learning and highlights theoretical developments in the area. He discusses the interactive learning problem; reinforcement and regret; equilibrium; conditional no-regret learning; prediction, postdiction, and calibration; fictitious play and its variants; Bayesian learning; and hypothesis testing. Young's framework emphasizes the amount of information required to implement different types of learning rules, criteria for evaluating their performance, and alternative notions of equilibrium to which they converge. He also stresses the limits of what can be achieved: for a given type of game and a given amount of information, there may exist no learning procedure that satisfies certain reasonable criteria of performance and convergence. In short, Young has provided a valuable primer that delineates what we know, what we would like to know, and the limits of what we can know, when we try to learn about a system that is composed of other learners.",
            "referenceCount": 0,
            "citationCount": 580,
            "influentialCitationCount": 44,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Young2004StrategicLA,\n author = {H. Young},\n title = {Strategic Learning and Its Limits},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:abe0bd94e134c7ac0b1c78922ed17cf3bec08d5e",
            "@type": "ScholarlyArticle",
            "paperId": "abe0bd94e134c7ac0b1c78922ed17cf3bec08d5e",
            "corpusId": 1985491,
            "url": "https://www.semanticscholar.org/paper/abe0bd94e134c7ac0b1c78922ed17cf3bec08d5e",
            "title": "A stochastic model of human-machine interaction for learning dialog strategies",
            "venue": "IEEE Transactions on Speech and Audio Processing",
            "publicationVenue": {
                "id": "urn:research:cd5799dd-1165-414f-87b0-ea5e184781c0",
                "name": "IEEE Transactions on Speech and Audio Processing",
                "alternate_names": [
                    "IEEE Trans Speech Audio Process"
                ],
                "issn": "1063-6676",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=89"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2132997613",
                "DBLP": "journals/taslp/LevinPE00",
                "DOI": "10.1109/89.817450",
                "CorpusId": 1985491
            },
            "abstract": "We propose a quantitative model for dialog systems that can be used for learning the dialog strategy. We claim that the problem of dialog design can be formalized as an optimization problem with an objective function reflecting different dialog dimensions relevant for a given application. We also show that any dialog system can be formally described as a sequential decision process in terms of its state space, action set, and strategy. With additional assumptions about the state transition probabilities and cost assignment, a dialog system can be mapped to a stochastic model known as Markov decision process (MDP). A variety of data driven algorithms for finding the optimal strategy (i.e., the one that optimizes the criterion) is available within the MDP framework, based on reinforcement learning. For an effective use of the available training data we propose a combination of supervised and reinforcement learning: the supervised learning is used to estimate a model of the user, i.e., the MDP parameters that quantify the user's behavior. Then a reinforcement learning algorithm is used to estimate the optimal strategy while the system interacts with the simulated user. This approach is tested for learning the strategy in an air travel information system (ATIS) task. The experimental results we present in this paper show that it is indeed possible to find a simple criterion, a state space representation, and a simulated user parameterization in order to automatically learn a relatively complex dialog behavior, similar to one that was heuristically designed by several research groups.",
            "referenceCount": 37,
            "citationCount": 604,
            "influentialCitationCount": 42,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "IEEE Trans. Speech Audio Process.",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Levin2000ASM,\n author = {E. Levin and R. Pieraccini and W. Eckert},\n booktitle = {IEEE Transactions on Speech and Audio Processing},\n journal = {IEEE Trans. Speech Audio Process.},\n pages = {11-23},\n title = {A stochastic model of human-machine interaction for learning dialog strategies},\n volume = {8},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5129a9cbb6de3c6579f6a7d974394d392ac29829",
            "@type": "ScholarlyArticle",
            "paperId": "5129a9cbb6de3c6579f6a7d974394d392ac29829",
            "corpusId": 10899248,
            "url": "https://www.semanticscholar.org/paper/5129a9cbb6de3c6579f6a7d974394d392ac29829",
            "title": "Control of Memory, Active Perception, and Action in Minecraft",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/icml/OhCSL16",
                "ArXiv": "1605.09128",
                "MAG": "2418628973",
                "CorpusId": 10899248
            },
            "abstract": "In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.",
            "referenceCount": 43,
            "citationCount": 279,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-05-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1605.09128"
            },
            "citationStyles": {
                "bibtex": "@Article{Oh2016ControlOM,\n author = {Junhyuk Oh and Valliappa Chockalingam and Satinder Singh and Honglak Lee},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Control of Memory, Active Perception, and Action in Minecraft},\n volume = {abs/1605.09128},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ec5540f6da71eb79a18bbcacb48b8ea847cad120",
            "@type": "ScholarlyArticle",
            "paperId": "ec5540f6da71eb79a18bbcacb48b8ea847cad120",
            "corpusId": 29136678,
            "url": "https://www.semanticscholar.org/paper/ec5540f6da71eb79a18bbcacb48b8ea847cad120",
            "title": "Learning to Learn",
            "venue": "Springer US",
            "publicationVenue": {
                "id": "urn:research:9e813668-7b2e-4506-9e7e-4a285f318e46",
                "name": "Springer US",
                "alternate_names": [
                    "Springer U"
                ],
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1621791442",
                "ArXiv": "2002.12364",
                "DBLP": "books/sp/98/Baxter98",
                "DOI": "10.1007/978-1-4615-5529-2",
                "CorpusId": 29136678
            },
            "abstract": null,
            "referenceCount": 233,
            "citationCount": 714,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2002.12364",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1998-05-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Baxter1998LearningTL,\n author = {Jonathan Baxter},\n booktitle = {Springer US},\n pages = {71-94},\n title = {Learning to Learn},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1389772b8a0f9c7fc43057f9da41a7d0ebf0308b",
            "@type": "ScholarlyArticle",
            "paperId": "1389772b8a0f9c7fc43057f9da41a7d0ebf0308b",
            "corpusId": 11789392,
            "url": "https://www.semanticscholar.org/paper/1389772b8a0f9c7fc43057f9da41a7d0ebf0308b",
            "title": "Generalization and Exploration via Randomized Value Functions",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2962767126",
                "ArXiv": "1402.0635",
                "DBLP": "conf/icml/OsbandRW16",
                "CorpusId": 11789392
            },
            "abstract": "We propose randomized least-squares value iteration (RLSVI) -- a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions. We explain why versions of least-squares value iteration that use Boltzmann or epsilon-greedy exploration can be highly inefficient, and we present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish an upper bound on the expected regret of RLSVI that demonstrates near-optimality in a tabula rasa learning context. More broadly, our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning: synthesizing efficient exploration and effective generalization.",
            "referenceCount": 47,
            "citationCount": 274,
            "influentialCitationCount": 44,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-02-03",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1402.0635"
            },
            "citationStyles": {
                "bibtex": "@Article{Osband2014GeneralizationAE,\n author = {Ian Osband and Benjamin Van Roy and Zheng Wen},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Generalization and Exploration via Randomized Value Functions},\n volume = {abs/1402.0635},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dd518836fee1aa71ca9971586e8e9ced59c4708d",
            "@type": "ScholarlyArticle",
            "paperId": "dd518836fee1aa71ca9971586e8e9ced59c4708d",
            "corpusId": 52150370,
            "url": "https://www.semanticscholar.org/paper/dd518836fee1aa71ca9971586e8e9ced59c4708d",
            "title": "Dopamine neurons projecting to the posterior striatum reinforce avoidance of threatening stimuli",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2888695486",
                "PubMedCentral": "6160326",
                "DOI": "10.1038/s41593-018-0222-1",
                "CorpusId": 52150370,
                "PubMed": "30177795"
            },
            "abstract": null,
            "referenceCount": 68,
            "citationCount": 213,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc6160326?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Biology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-08-07",
            "journal": {
                "name": "Nature neuroscience",
                "volume": "21"
            },
            "citationStyles": {
                "bibtex": "@Article{Menegas2018DopamineNP,\n author = {William Menegas and Korleki Akiti and Ryunosuke Amo and N. Uchida and Mitsuko Watabe-Uchida},\n booktitle = {Nature Neuroscience},\n journal = {Nature neuroscience},\n pages = {1421 - 1430},\n title = {Dopamine neurons projecting to the posterior striatum reinforce avoidance of threatening stimuli},\n volume = {21},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2b11305f69641ecb8bd4a5e59cfebe41ad9ed989",
            "@type": "ScholarlyArticle",
            "paperId": "2b11305f69641ecb8bd4a5e59cfebe41ad9ed989",
            "corpusId": 14742574,
            "url": "https://www.semanticscholar.org/paper/2b11305f69641ecb8bd4a5e59cfebe41ad9ed989",
            "title": "TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play",
            "venue": "Neural Computation",
            "publicationVenue": {
                "id": "urn:research:69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                "name": "Neural Computation",
                "alternate_names": [
                    "Neural Comput"
                ],
                "issn": "0899-7667",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667"
            },
            "year": 1994,
            "externalIds": {
                "DBLP": "journals/neco/Tesauro94",
                "MAG": "2041367235",
                "DOI": "10.1162/neco.1994.6.2.215",
                "CorpusId": 14742574
            },
            "abstract": "TD-Gammon is a neural network that is able to teach itself to play backgammon solely by playing against itself and learning from the results, based on the TD() reinforcement learning algorithm (Sutton 1988). Despite starting from random initial weights (and hence random initial strategy), TD-Gammon achieves a surprisingly strong level of play. With zero knowledge built in at the start of learning (i.e., given only a raw description of the board state), the network learns to play at a strong intermediate level. Furthermore, when a set of hand-crafted features is added to the network's input representation, the result is a truly staggering level of performance: the latest version of TD-Gammon is now estimated to play at a strong master level that is extremely close to the world's best human players.",
            "referenceCount": 6,
            "citationCount": 933,
            "influentialCitationCount": 43,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1994-03-01",
            "journal": {
                "name": "Neural Computation",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Tesauro1994TDGammonAS,\n author = {G. Tesauro},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {215-219},\n title = {TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play},\n volume = {6},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b0a20de53b48a64de006ffb7b064484c785a3e63",
            "@type": "ScholarlyArticle",
            "paperId": "b0a20de53b48a64de006ffb7b064484c785a3e63",
            "corpusId": 203609286,
            "url": "https://www.semanticscholar.org/paper/b0a20de53b48a64de006ffb7b064484c785a3e63",
            "title": "Believing in dopamine",
            "venue": "Nature Reviews Neuroscience",
            "publicationVenue": {
                "id": "urn:research:74b0478a-292c-4fe3-bb2f-71f438f00cc7",
                "name": "Nature Reviews Neuroscience",
                "alternate_names": [
                    "Nat Rev Neurosci"
                ],
                "issn": "1471-003X",
                "url": "http://www.nature.com/nrn/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2976141814",
                "DOI": "10.1038/s41583-019-0220-7",
                "CorpusId": 203609286,
                "PubMed": "31570826"
            },
            "abstract": null,
            "referenceCount": 123,
            "citationCount": 134,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2019-09-30",
            "journal": {
                "name": "Nature Reviews Neuroscience",
                "volume": "20"
            },
            "citationStyles": {
                "bibtex": "@Article{Gershman2019BelievingID,\n author = {S. Gershman and N. Uchida},\n booktitle = {Nature Reviews Neuroscience},\n journal = {Nature Reviews Neuroscience},\n pages = {703 - 714},\n title = {Believing in dopamine},\n volume = {20},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a9da09d1e63686706d64782e654d69f13fd292ad",
            "@type": "ScholarlyArticle",
            "paperId": "a9da09d1e63686706d64782e654d69f13fd292ad",
            "corpusId": 291920,
            "url": "https://www.semanticscholar.org/paper/a9da09d1e63686706d64782e654d69f13fd292ad",
            "title": "Learning from Demonstration",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1996,
            "externalIds": {
                "MAG": "2148112459",
                "DBLP": "conf/nips/Schaal96",
                "CorpusId": 291920
            },
            "abstract": "By now it is widely accepted that learning a task from scratch, i.e., without any prior knowledge, is a daunting undertaking. Humans, however, rarely attempt to learn from scratch. They extract initial biases as well as strategies how to approach a learning problem from instructions and/or demonstrations of other humans. For teaming control, this paper investigates how learning from demonstration can be applied in the context of reinforcement learning. We consider priming the Q-function, the value function, the policy, and the model of the task dynamics as possible areas where demonstrations can speed up learning. In general nonlinear learning problems, only model-based reinforcement learning shows significant speed-up after a demonstration, while in the special case of linear quadratic regulator (LQR) problems, all methods profit from the demonstration. In an implementation of pole balancing on a complex anthropomorphic robot arm, we demonstrate that, when facing the complexities of real signal processing, model-based reinforcement learning offers the most robustness for LQR problems. Using the suggested methods, the robot learns pole balancing in just a single trial after a 30 second long demonstration of the human instructor.",
            "referenceCount": 25,
            "citationCount": 624,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1996-12-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Schaal1996LearningFD,\n author = {S. Schaal},\n booktitle = {Neural Information Processing Systems},\n pages = {1040-1046},\n title = {Learning from Demonstration},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b5d4e896760013bff4dc181f20238276a9238f71",
            "@type": "ScholarlyArticle",
            "paperId": "b5d4e896760013bff4dc181f20238276a9238f71",
            "corpusId": 6268491,
            "url": "https://www.semanticscholar.org/paper/b5d4e896760013bff4dc181f20238276a9238f71",
            "title": "Distinct error-correcting and incidental learning of location relative to landmarks and boundaries",
            "venue": "Proceedings of the National Academy of Sciences of the United States of America",
            "publicationVenue": {
                "id": "urn:research:bb95bf2e-8383-4748-bf9d-d6906d091085",
                "name": "Proceedings of the National Academy of Sciences of the United States of America",
                "alternate_names": [
                    "PNAS",
                    "PNAS online",
                    "Proceedings of the National Academy of Sciences of the United States of America.",
                    "Proc National Acad Sci",
                    "Proceedings of the National Academy of Sciences",
                    "Proc National Acad Sci u s Am"
                ],
                "issn": "0027-8424",
                "url": "https://www.jstor.org/journal/procnatiacadscie"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2145405131",
                "DOI": "10.1073/pnas.0711433105",
                "CorpusId": 6268491,
                "PubMed": "18413609"
            },
            "abstract": "Associative reinforcement provides a powerful explanation of learned behavior. However, an unproven but long-held conjecture holds that spatial learning can occur incidentally rather than by reinforcement. Using a carefully controlled virtual-reality object-location memory task, we formally demonstrate that locations are concurrently learned relative to both local landmarks and local boundaries but that landmark-learning obeys associative reinforcement (showing \u201covershadowing\u201d and \u201cblocking\u201d or \u201clearned irrelevance\u201d), whereas boundary-learning is incidental, showing neither overshadowing nor blocking nor learned irrelevance. Crucially, both types of learning occur at similar rates and do not reflect differences in levels of performance, cue salience, or instructions. These distinct types of learning likely reflect the distinct neural systems implicated in processing of landmarks and boundaries: the striatum and hippocampus, respectively [Doeller CF, King JA, Burgess N (2008) Proc Natl Acad Sci USA 105:5915\u20135920]. In turn, our results suggest the use of fundamentally different learning rules by these two systems, potentially explaining their differential roles in procedural and declarative memory more generally. Our results suggest a privileged role for surface geometry in determining spatial context and support the idea of a \u201cgeometric module,\u201d albeit for location rather than orientation. Finally, the demonstration that reinforcement learning applies selectively to formally equivalent aspects of task-performance supports broader consideration of two-system models in analyses of learning and decision making.",
            "referenceCount": 58,
            "citationCount": 271,
            "influentialCitationCount": 25,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc2311326?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-04-15",
            "journal": {
                "name": "Proceedings of the National Academy of Sciences",
                "volume": "105"
            },
            "citationStyles": {
                "bibtex": "@Article{Doeller2008DistinctEA,\n author = {Christian F. Doeller and N. Burgess},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {5909 - 5914},\n title = {Distinct error-correcting and incidental learning of location relative to landmarks and boundaries},\n volume = {105},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:564d1c036ce2fb2d5f0aa066c8183c0d5e368734",
            "@type": "ScholarlyArticle",
            "paperId": "564d1c036ce2fb2d5f0aa066c8183c0d5e368734",
            "corpusId": 6162664,
            "url": "https://www.semanticscholar.org/paper/564d1c036ce2fb2d5f0aa066c8183c0d5e368734",
            "title": "Dopaminergic Drugs Modulate Learning Rates and Perseveration in Parkinson's Patients in a Dynamic Foraging Task",
            "venue": "Journal of Neuroscience",
            "publicationVenue": {
                "id": "urn:research:52edfc09-8d65-4876-8560-84530d7259b9",
                "name": "Journal of Neuroscience",
                "alternate_names": [
                    "The Journal of Neuroscience",
                    "J Neurosci"
                ],
                "issn": "0270-6474",
                "url": "https://www.jneurosci.org/"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2015770890",
                "DOI": "10.1523/JNEUROSCI.3524-09.2009",
                "CorpusId": 6162664,
                "PubMed": "19955362"
            },
            "abstract": "Making appropriate choices often requires the ability to learn the value of available options from experience. Parkinson's disease is characterized by a loss of dopamine neurons in the substantia nigra, neurons hypothesized to play a role in reinforcement learning. Although previous studies have shown that Parkinson's patients are impaired in tasks involving learning from feedback, they have not directly tested the widely held hypothesis that dopamine neuron activity specifically encodes the reward prediction error signal used in reinforcement learning models. To test a key prediction of this hypothesis, we fit choice behavior from a dynamic foraging task with reinforcement learning models and show that treatment with dopaminergic drugs alters choice behavior in a manner consistent with the theory. More specifically, we found that dopaminergic drugs selectively modulate learning from positive outcomes. We observed no effect of dopaminergic drugs on learning from negative outcomes. We also found a novel dopamine-dependent effect on decision making that is not accounted for by reinforcement learning models: perseveration in choice, independent of reward history, increases with Parkinson's disease and decreases with dopamine therapy.",
            "referenceCount": 64,
            "citationCount": 235,
            "influentialCitationCount": 25,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.jneurosci.org/content/jneuro/29/48/15104.full.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "ClinicalTrial"
            ],
            "publicationDate": "2009-12-02",
            "journal": {
                "name": "The Journal of Neuroscience",
                "volume": "29"
            },
            "citationStyles": {
                "bibtex": "@Article{Rutledge2009DopaminergicDM,\n author = {R. Rutledge and Stephanie C. Lazzaro and B. Lau and C. Myers and M. Gluck and P. Glimcher},\n booktitle = {Journal of Neuroscience},\n journal = {The Journal of Neuroscience},\n pages = {15104 - 15114},\n title = {Dopaminergic Drugs Modulate Learning Rates and Perseveration in Parkinson's Patients in a Dynamic Foraging Task},\n volume = {29},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:24f1faaf935e77de89cd551b8fd2320a0f32aa97",
            "@type": "ScholarlyArticle",
            "paperId": "24f1faaf935e77de89cd551b8fd2320a0f32aa97",
            "corpusId": 144336589,
            "url": "https://www.semanticscholar.org/paper/24f1faaf935e77de89cd551b8fd2320a0f32aa97",
            "title": "Conditioning And Associative Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1983,
            "externalIds": {
                "MAG": "2048732459",
                "DOI": "10.2307/1422540",
                "CorpusId": 144336589
            },
            "abstract": "The study of conditioning in animals Classical and instrumental conditioning Theoretical analysis of classical conditioning Theoretical analysis of instrumental conditioning Appetitive and aversive reinforcement Avoidance learning Contiguity and contingency: excitatory and inhibitory conditioning Laws of association Discrimination learning References Indexes.",
            "referenceCount": 0,
            "citationCount": 785,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Hill1983ConditioningAA,\n author = {W. F. Hill and N. Mackintosh},\n title = {Conditioning And Associative Learning},\n year = {1983}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2e7d1e21409a90e66106722506aeb434ee7a18f3",
            "@type": "ScholarlyArticle",
            "paperId": "2e7d1e21409a90e66106722506aeb434ee7a18f3",
            "corpusId": 19318639,
            "url": "https://www.semanticscholar.org/paper/2e7d1e21409a90e66106722506aeb434ee7a18f3",
            "title": "A unified view of entropy-regularized Markov decision processes",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1705.07798",
                "DBLP": "journals/corr/NeuJG17",
                "MAG": "2619268125",
                "CorpusId": 19318639
            },
            "abstract": "We propose a general framework for entropy-regularized average-reward reinforcement learning in Markov decision processes (MDPs). Our approach is based on extending the linear-programming formulation of policy optimization in MDPs to accommodate convex regularization functions. Our key result is showing that using the conditional entropy of the joint state-action distributions as regularization yields a dual optimization problem closely resembling the Bellman optimality equations. This result enables us to formalize a number of state-of-the-art entropy-regularized reinforcement learning algorithms as approximate variants of Mirror Descent or Dual Averaging, and thus to argue about the convergence properties of these methods. In particular, we show that the exact version of the TRPO algorithm of Schulman et al. (2015) actually converges to the optimal policy, while the entropy-regularized policy gradient methods of Mnih et al. (2016) may fail to converge to a fixed point. Finally, we illustrate empirically the effects of using various regularization techniques on learning performance in a simple reinforcement learning setup.",
            "referenceCount": 57,
            "citationCount": 218,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-05-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1705.07798"
            },
            "citationStyles": {
                "bibtex": "@Article{Neu2017AUV,\n author = {Gergely Neu and Anders Jonsson and V. G\u00f3mez},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A unified view of entropy-regularized Markov decision processes},\n volume = {abs/1705.07798},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7200969d70cf6f3fd343f48e97b8ebf7d563a584",
            "@type": "ScholarlyArticle",
            "paperId": "7200969d70cf6f3fd343f48e97b8ebf7d563a584",
            "corpusId": 7292590,
            "url": "https://www.semanticscholar.org/paper/7200969d70cf6f3fd343f48e97b8ebf7d563a584",
            "title": "Graying the black box: Understanding DQNs",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1602.02658",
                "MAG": "2270696664",
                "DBLP": "conf/icml/ZahavyBM16",
                "CorpusId": 7292590
            },
            "abstract": "In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. Moreover we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize deep neural networks in reinforcement learning.",
            "referenceCount": 57,
            "citationCount": 237,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-02-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1602.02658"
            },
            "citationStyles": {
                "bibtex": "@Article{Zahavy2016GrayingTB,\n author = {Tom Zahavy and Nir Ben-Zrihem and Shie Mannor},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Graying the black box: Understanding DQNs},\n volume = {abs/1602.02658},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:78020db7e3d968f6e6cc26d18e31e5b668ca7fee",
            "@type": "ScholarlyArticle",
            "paperId": "78020db7e3d968f6e6cc26d18e31e5b668ca7fee",
            "corpusId": 1153355,
            "url": "https://www.semanticscholar.org/paper/78020db7e3d968f6e6cc26d18e31e5b668ca7fee",
            "title": "Eligibility Traces for Off-Policy Policy Evaluation",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2000,
            "externalIds": {
                "DBLP": "conf/icml/PrecupSS00",
                "MAG": "1514587017",
                "CorpusId": 1153355
            },
            "abstract": "Eligibility traces have been shown to speed reinforcement learning, to make it more robust to hidden states, and to provide a link between Monte Carlo and temporal-difference methods. Here we generalize eligibility traces to off-policy learning, in which one learns about a policy different from the policy that generates the data. Off-policy methods can greatly multiply learning, as many policies can be learned about from the same data stream, and have been identified as particularly useful for learning about subgoals and temporally extended macro-actions. In this paper we consider the off-policy version of the policy evaluation problem, for which only one eligibility trace algorithm is known, a Monte Carlo method. We analyze and compare this and four new eligibility trace algorithms, emphasizing their relationships to the classical statistical technique known as importance sampling. Our main results are 1) to establish the consistency and bias properties of the new methods and 2) to empirically rank the new methods, showing improvement over one-step and Monte Carlo methods. Our results are restricted to model-free, table-lookup methods and to offline updating (at the end of each episode) although several of the algorithms could be applied more generally.",
            "referenceCount": 13,
            "citationCount": 709,
            "influentialCitationCount": 92,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2000-06-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Precup2000EligibilityTF,\n author = {Doina Precup and R. Sutton and Satinder Singh},\n booktitle = {International Conference on Machine Learning},\n pages = {759-766},\n title = {Eligibility Traces for Off-Policy Policy Evaluation},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0067343a36c0292f36e627eb353f751c8a39f99a",
            "@type": "ScholarlyArticle",
            "paperId": "0067343a36c0292f36e627eb353f751c8a39f99a",
            "corpusId": 10513082,
            "url": "https://www.semanticscholar.org/paper/0067343a36c0292f36e627eb353f751c8a39f99a",
            "title": "Linear Off-Policy Actor-Critic",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2012,
            "externalIds": {
                "ArXiv": "1205.4839",
                "MAG": "2394928340",
                "DBLP": "conf/icml/DegrisWS12",
                "CorpusId": 10513082
            },
            "abstract": "This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in off-policy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, however, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems.",
            "referenceCount": 37,
            "citationCount": 340,
            "influentialCitationCount": 44,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2012-05-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1205.4839"
            },
            "citationStyles": {
                "bibtex": "@Article{Degris2012LinearOA,\n author = {T. Degris and Martha White and R. Sutton},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Linear Off-Policy Actor-Critic},\n volume = {abs/1205.4839},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:337020f17344dadc2793ccfbcca4102ffa88505b",
            "@type": "ScholarlyArticle",
            "paperId": "337020f17344dadc2793ccfbcca4102ffa88505b",
            "corpusId": 1071032,
            "url": "https://www.semanticscholar.org/paper/337020f17344dadc2793ccfbcca4102ffa88505b",
            "title": "Orbitofrontal Cortex and Representation of Incentive Value in Associative Learning",
            "venue": "Journal of Neuroscience",
            "publicationVenue": {
                "id": "urn:research:52edfc09-8d65-4876-8560-84530d7259b9",
                "name": "Journal of Neuroscience",
                "alternate_names": [
                    "The Journal of Neuroscience",
                    "J Neurosci"
                ],
                "issn": "0270-6474",
                "url": "https://www.jneurosci.org/"
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1762920573",
                "DOI": "10.1523/JNEUROSCI.19-15-06610.1999",
                "CorpusId": 1071032,
                "PubMed": "10414988"
            },
            "abstract": "Clinical evidence indicates that damage to ventromedial prefrontal cortex disrupts goal-directed actions that are guided by motivational and emotional factors. As a consequence, patients with such damage characteristically engage in maladaptive behaviors. Other research has shown that neurons in the corresponding orbital region of prefrontal cortex in laboratory animals encode information regarding the incentive properties of goals or expected events. The present study investigates the effect of neurotoxic orbitofrontal cortex (OFC) lesions in the rat on responses that are normally influenced by associations between a conditioned stimulus (CS) and the incentive value of reinforcement. Rats were first trained to associate a visual CS with delivery of food pellets to a food cup. As a consequence of learning, rats approached the food cup during the CS in anticipation of reinforcement. In a second training phase, injection of LiCl followed consumption of the food unconditioned stimulus (US) in the home cage, a procedure used to alter the incentive value of the US. Subsequently, rats were returned to the conditioning chamber, and their responding to the CS in the absence of the food US was tested. Lesions of OFC did not affect either the initial acquisition of a conditioned response to the light CS in the first training phase or taste aversion learning in the second training phase. In the test for devaluation, however, OFC rats exhibited no change in conditioned responding to the visual CS. This outcome contrasts with the behavior of control rats; after devaluation of the US a significant decrease occurred in approach to the food cup during presentation of the CS. The results reveal an inability of a cue to access representational information about the incentive value of associated reinforcement after OFC damage.",
            "referenceCount": 27,
            "citationCount": 551,
            "influentialCitationCount": 30,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.jneurosci.org/content/jneuro/19/15/6610.full.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1999-08-01",
            "journal": {
                "name": "The Journal of Neuroscience",
                "volume": "19"
            },
            "citationStyles": {
                "bibtex": "@Article{Gallagher1999OrbitofrontalCA,\n author = {M. Gallagher and R. McMahan and G. Schoenbaum},\n booktitle = {Journal of Neuroscience},\n journal = {The Journal of Neuroscience},\n pages = {6610 - 6614},\n title = {Orbitofrontal Cortex and Representation of Incentive Value in Associative Learning},\n volume = {19},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ba378579fb44007db9f02699889721dcd2b5b3a0",
            "@type": "ScholarlyArticle",
            "paperId": "ba378579fb44007db9f02699889721dcd2b5b3a0",
            "corpusId": 2398966,
            "url": "https://www.semanticscholar.org/paper/ba378579fb44007db9f02699889721dcd2b5b3a0",
            "title": "Model-Free Episodic Control",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/BlundellUPLRLRW16",
                "MAG": "2436711315",
                "ArXiv": "1606.04460",
                "CorpusId": 2398966
            },
            "abstract": "State of the art deep reinforcement learning algorithms take many millions of interactions to attain human-level performance. Humans, on the other hand, can very quickly exploit highly rewarding nuances of an environment upon first discovery. In the brain, such rapid learning is thought to depend on the hippocampus and its capacity for episodic memory. Here we investigate whether a simple model of hippocampal episodic control can learn to solve difficult sequential decision-making tasks. We demonstrate that it not only attains a highly rewarding strategy significantly faster than state-of-the-art deep reinforcement learning algorithms, but also achieves a higher overall reward on some of the more challenging domains.",
            "referenceCount": 46,
            "citationCount": 203,
            "influentialCitationCount": 32,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science",
                "Biology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-06-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1606.04460"
            },
            "citationStyles": {
                "bibtex": "@Article{Blundell2016ModelFreeEC,\n author = {Charles Blundell and Benigno Uria and Alexander Pritzel and Yazhe Li and Avraham Ruderman and Joel Z. Leibo and Jack W. Rae and Daan Wierstra and Demis Hassabis},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Model-Free Episodic Control},\n volume = {abs/1606.04460},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cfeccccd73bd65738e0b2d403cf371eac9b7c940",
            "@type": "ScholarlyArticle",
            "paperId": "cfeccccd73bd65738e0b2d403cf371eac9b7c940",
            "corpusId": 38169079,
            "url": "https://www.semanticscholar.org/paper/cfeccccd73bd65738e0b2d403cf371eac9b7c940",
            "title": "SSL: a theory of how people learn to select strategies.",
            "venue": "Journal of experimental psychology. General",
            "publicationVenue": {
                "id": "urn:research:ba388b36-981e-4c1b-8048-464cdaa9c9fc",
                "name": "Journal of experimental psychology. General",
                "alternate_names": [
                    "J Exp Psychol Gen",
                    "J exp psychol Gen",
                    "Journal of Experimental Psychology: General"
                ],
                "issn": "0096-3445",
                "url": "http://www.apa.org/journals/xge.html"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2132415562",
                "DOI": "10.1037/0096-3445.135.2.207",
                "CorpusId": 38169079,
                "PubMed": "16719651"
            },
            "abstract": "The assumption that people possess a repertoire of strategies to solve the inference problems they face has been raised repeatedly. However, a computational model specifying how people select strategies from their repertoire is still lacking. The proposed strategy selection learning (SSL) theory predicts a strategy selection process on the basis of reinforcement learning. The theory assumes that individuals develop subjective expectations for the strategies they have and select strategies proportional to their expectations, which are then updated on the basis of subsequent experience. The learning assumption was supported in 4 experimental studies. Participants substantially improved their inferences through feedback. In all 4 studies, the best-performing strategy from the participants' repertoires most accurately predicted the inferences after sufficient learning opportunities. When testing SSL against 3 models representing extensions of SSL and against an exemplar model assuming a memory-based inference process, the authors found that SSL predicted the inferences most accurately.",
            "referenceCount": 120,
            "citationCount": 573,
            "influentialCitationCount": 38,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-05-01",
            "journal": {
                "name": "Journal of experimental psychology. General",
                "volume": "135 2"
            },
            "citationStyles": {
                "bibtex": "@Article{Rieskamp2006SSLAT,\n author = {J. Rieskamp and Philipp E Otto},\n booktitle = {Journal of experimental psychology. General},\n journal = {Journal of experimental psychology. General},\n pages = {\n          207-36\n        },\n title = {SSL: a theory of how people learn to select strategies.},\n volume = {135 2},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:97404fc464af31fd1d55d738c1f67bbcc2832bc7",
            "@type": "ScholarlyArticle",
            "paperId": "97404fc464af31fd1d55d738c1f67bbcc2832bc7",
            "corpusId": 10346631,
            "url": "https://www.semanticscholar.org/paper/97404fc464af31fd1d55d738c1f67bbcc2832bc7",
            "title": "Drosophila mushroom body mutants are deficient in olfactory learning.",
            "venue": "Journal of neurogenetics",
            "publicationVenue": {
                "id": "urn:research:3304566f-229b-495c-9929-f135e9a6ccd4",
                "name": "Journal of neurogenetics",
                "alternate_names": [
                    "Journal of Neurogenetics",
                    "J neurogenetics",
                    "J Neurogenetics"
                ],
                "issn": "0167-7063",
                "url": "http://www.gbhap-us.com/journals/359/359-top.htm"
            },
            "year": 1985,
            "externalIds": {
                "MAG": "2142667969",
                "DOI": "10.3109/01677068509100140",
                "CorpusId": 10346631,
                "PubMed": "4020527"
            },
            "abstract": "Two Drosophila mutants are described in which the connections between the input to and the output from the mushroom bodies is largely interrupted. In all forms of the flies (larva, imago, male, female) showing the structural defect, olfactory conditioning is impaired. Learning is completely abolished when electroshock is used as reinforcement and partially suppressed in reward learning with sucrose. No influence of the mushroom body defect on the perception of the conditioning stimuli or on spontaneous olfactory behavior is observed. The defect seems not to impair learning of color discrimination tasks or operant learning involving visual cues.",
            "referenceCount": 26,
            "citationCount": 703,
            "influentialCitationCount": 38,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Biology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Study"
            ],
            "publicationDate": "1985-02-01",
            "journal": {
                "name": "Journal of neurogenetics",
                "volume": "2 1"
            },
            "citationStyles": {
                "bibtex": "@Article{Heisenberg1985DrosophilaMB,\n author = {Martin Heisenberg and Alexander Borst and Sibylle Wagner and Duncan Byers},\n booktitle = {Journal of neurogenetics},\n journal = {Journal of neurogenetics},\n pages = {\n          1-30\n        },\n title = {Drosophila mushroom body mutants are deficient in olfactory learning.},\n volume = {2 1},\n year = {1985}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cf8ed2793bc6aec88da5306fe2de560dc0be9b15",
            "@type": "ScholarlyArticle",
            "paperId": "cf8ed2793bc6aec88da5306fe2de560dc0be9b15",
            "corpusId": 12529428,
            "url": "https://www.semanticscholar.org/paper/cf8ed2793bc6aec88da5306fe2de560dc0be9b15",
            "title": "Delving into adversarial attacks on deep policies",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/iclr/KosS17",
                "MAG": "2952471392",
                "ArXiv": "1705.06452",
                "CorpusId": 12529428
            },
            "abstract": "Adversarial examples have been shown to exist for a variety of deep learning architectures. Deep reinforcement learning has shown promising results on training agent policies directly on raw inputs such as image pixels. In this paper we present a novel study into adversarial attacks on deep reinforcement learning polices. We compare the effectiveness of the attacks using adversarial examples vs. random noise. We present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack, based on the value function. We further explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples.",
            "referenceCount": 4,
            "citationCount": 188,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-02-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1705.06452"
            },
            "citationStyles": {
                "bibtex": "@Article{Kos2017DelvingIA,\n author = {Jernej Kos and D. Song},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Delving into adversarial attacks on deep policies},\n volume = {abs/1705.06452},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:03d3fbad90f047d238b35a742e44b6cd5946d591",
            "@type": "ScholarlyArticle",
            "paperId": "03d3fbad90f047d238b35a742e44b6cd5946d591",
            "corpusId": 8015489,
            "url": "https://www.semanticscholar.org/paper/03d3fbad90f047d238b35a742e44b6cd5946d591",
            "title": "Knows what it knows: a\u00a0framework for\u00a0self-aware learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/icml/LiLW08",
                "MAG": "2488247662",
                "DOI": "10.1007/s10994-010-5225-4",
                "CorpusId": 8015489
            },
            "abstract": null,
            "referenceCount": 63,
            "citationCount": 237,
            "influentialCitationCount": 18,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10994-010-5225-4.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-07-05",
            "journal": {
                "name": "Machine Learning",
                "volume": "82"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2008KnowsWI,\n author = {Lihong Li and M. Littman and Thomas J. Walsh and Alexander L. Strehl},\n booktitle = {International Conference on Machine Learning},\n journal = {Machine Learning},\n pages = {399-443},\n title = {Knows what it knows: a\u00a0framework for\u00a0self-aware learning},\n volume = {82},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fce6b98e3e5007fd9d645b972a3ae09bf2c79f7f",
            "@type": "ScholarlyArticle",
            "paperId": "fce6b98e3e5007fd9d645b972a3ae09bf2c79f7f",
            "corpusId": 5613334,
            "url": "https://www.semanticscholar.org/paper/fce6b98e3e5007fd9d645b972a3ae09bf2c79f7f",
            "title": "TAMER: Training an Agent Manually via Evaluative Reinforcement",
            "venue": "2008 7th IEEE International Conference on Development and Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2110064869",
                "DOI": "10.1109/DEVLRN.2008.4640845",
                "CorpusId": 5613334
            },
            "abstract": "Though computers have surpassed humans at many tasks, especially computationally intensive ones, there are many tasks for which human expertise remains necessary and/or useful. For such tasks, it is desirable for a human to be able to transmit knowledge to a learning agent as quickly and effortlessly as possible, and, ideally, without any knowledge of the details of the agentpsilas learning process. This paper proposes a general framework called Training an Agent Manually via Evaluative Reinforcement (TAMER) that allows a human to train a learning agent to perform a common class of complex tasks simply by giving scalar reward signals in response to the agentpsilas observed actions. Specifically, in sequential decision making tasks, an agent models the humanpsilas reward function and chooses actions that it predicts will receive the most reward. Our novel algorithm is fully implemented and tested on the game Tetris. Leveraging the human trainerspsila feedback, the agent learns to clear an average of more than 50 lines by its third game, an order of magnitude faster than the best autonomous learning agents.",
            "referenceCount": 20,
            "citationCount": 136,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.utexas.edu/~pstone/Papers/bib2html-links/ICDL08-knox.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference"
            ],
            "publicationDate": "2008-10-10",
            "journal": {
                "name": "2008 7th IEEE International Conference on Development and Learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Conference{Knox2008TAMERTA,\n author = {W. B. Knox and P. Stone},\n booktitle = {2008 7th IEEE International Conference on Development and Learning},\n journal = {2008 7th IEEE International Conference on Development and Learning},\n pages = {292-297},\n title = {TAMER: Training an Agent Manually via Evaluative Reinforcement},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9b8d5df729aad2e9fb9bcfaf931622afa4daa247",
            "@type": "ScholarlyArticle",
            "paperId": "9b8d5df729aad2e9fb9bcfaf931622afa4daa247",
            "corpusId": 323396,
            "url": "https://www.semanticscholar.org/paper/9b8d5df729aad2e9fb9bcfaf931622afa4daa247",
            "title": "Prefrontal cortex and decision making in a mixed-strategy game",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2102068574",
                "DOI": "10.1038/nn1209",
                "CorpusId": 323396,
                "PubMed": "15004564"
            },
            "abstract": null,
            "referenceCount": 48,
            "citationCount": 632,
            "influentialCitationCount": 31,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Study"
            ],
            "publicationDate": "2004-04-01",
            "journal": {
                "name": "Nature Neuroscience",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Barraclough2004PrefrontalCA,\n author = {D. J. Barraclough and M. L. Conroy and Daeyeol Lee},\n booktitle = {Nature Neuroscience},\n journal = {Nature Neuroscience},\n pages = {404-410},\n title = {Prefrontal cortex and decision making in a mixed-strategy game},\n volume = {7},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b0db76855dcca9ae1bc82830e8171140ff118535",
            "@type": "ScholarlyArticle",
            "paperId": "b0db76855dcca9ae1bc82830e8171140ff118535",
            "corpusId": 142708797,
            "url": "https://www.semanticscholar.org/paper/b0db76855dcca9ae1bc82830e8171140ff118535",
            "title": "The principles of learning and behavior",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1982,
            "externalIds": {
                "MAG": "1600984297",
                "DOI": "10.1037/034377",
                "CorpusId": 142708797
            },
            "abstract": "1. Introduction. 2. Elicited Behavior, Habituation, and Sensitization. 3. Classical Conditioning: Foundations. 4. Classical Conditioning: Mechanisms. 5. Instrumental Conditioning: Foundations. 6. Schedules of Reinforcement and Choice Behavior. 7. Instrumental Conditioning: Motivational Mechanisms. 8. Stimulus Control of Behavior. 9. Extinction of Conditioned Behavior. 10. Averse Control: Avoidance and Punishment. 11. Comparative Cognition I: Memory Mechanisms. 12. Comparative Cognition II: Special Topics. References.",
            "referenceCount": 0,
            "citationCount": 712,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Domjan1982ThePO,\n author = {M. Domjan},\n title = {The principles of learning and behavior},\n year = {1982}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cfb94d52bcfcd4c9e9fbd282c456e4987935df53",
            "@type": "ScholarlyArticle",
            "paperId": "cfb94d52bcfcd4c9e9fbd282c456e4987935df53",
            "corpusId": 145627541,
            "url": "https://www.semanticscholar.org/paper/cfb94d52bcfcd4c9e9fbd282c456e4987935df53",
            "title": "Neuroleptics and operant behavior: The anhedonia hypothesis",
            "venue": "Behavioral and Brain Sciences",
            "publicationVenue": {
                "id": "urn:research:f51399af-b5cb-4819-9d81-57ec1d17ebf0",
                "name": "Behavioral and Brain Sciences",
                "alternate_names": [
                    "Behav Brain Sci"
                ],
                "issn": "0140-525X",
                "url": "http://www.bbsonline.org/"
            },
            "year": 1982,
            "externalIds": {
                "MAG": "2143839686",
                "DOI": "10.1017/S0140525X00010372",
                "CorpusId": 145627541
            },
            "abstract": "Abstract Neuroleptic drugs disrupt the learning and performance of operant habits motivated by a variety of positive reinforcers, including food, water, brain stimulation, intravenous opiates, stimulants, and barbiturates. This disruption has been demonstrated in several kinds of experiments with doses that do not significantly limit normal response capacity. With continuous reinforcement neuroleptics gradually cause responding to cease, as in extinction or satiation. This pattern is not due to satiation, however, because it also occurs with nonsatiating reinforcement (such as saccharin or brain stimulation). Repeated tests with neuroleptics result in earlier and earlier response cessation reminiscent of the kind of decreased resistance to extinction caused by repeated tests without the expected reward. Indeed, withholding reward can have the same effect on responding under later neuroleptic treatment as prior experience with neuroleptics themselves; this suggests that there is a transfer of learning (really unlearning) from nonreward to neuroleptic conditions. These tests under continuous reinforcement schedules suggest that neuroleptics blunt the ability of reinforcers to sustain responding at doses which largely spare the ability of the animal to initiate responding. Animals trained under partial reinforcement, however, do not respond as well during neuroleptic testing as animals trained under continuous reinforcement. Thus, neuroleptics can also impair responding (though not response capacity) that is normally sustained by environmental stimuli (and associated expectancies) in the absence of the primary reinforcer. Neuroleptics also blunt the euphoric impact of amphetamine in humans. These data suggest that the most subtle and interesting effect of neuroleptics is a selective attenuation of motivational arousal which is (a) critical for goal-directed behavior, (b) normally induced by reinforcers and associated environmental stimuli, and (c) normally accompanied by the subjective experience of pleasure. Because these drugs are used to treat schizophrenia and because they cause parkinsonian-like side effects, this action has implications for a better understanding of human pathology as well as normal motivational processes.",
            "referenceCount": 214,
            "citationCount": 1052,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1982-03-01",
            "journal": {
                "name": "Behavioral and Brain Sciences",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Wise1982NeurolepticsAO,\n author = {R. Wise},\n booktitle = {Behavioral and Brain Sciences},\n journal = {Behavioral and Brain Sciences},\n pages = {39 - 53},\n title = {Neuroleptics and operant behavior: The anhedonia hypothesis},\n volume = {5},\n year = {1982}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:77826df024f97583eb05700a28e11056a4aab848",
            "@type": "ScholarlyArticle",
            "paperId": "77826df024f97583eb05700a28e11056a4aab848",
            "corpusId": 9769916,
            "url": "https://www.semanticscholar.org/paper/77826df024f97583eb05700a28e11056a4aab848",
            "title": "Latent Intention Dialogue Models",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2950808653",
                "DBLP": "journals/corr/WenMBY17",
                "ArXiv": "1705.10229",
                "CorpusId": 9769916
            },
            "abstract": "Developing a dialogue agent that is capable of making autonomous decisions and communicating by natural language is one of the long-term goals of machine learning research. Traditional approaches either rely on hand-crafting a small state-action set for applying reinforcement learning that is not scalable or constructing deterministic models for learning dialogue sentences that fail to capture natural conversational variability. In this paper, we propose a Latent Intention Dialogue Model (LIDM) that employs a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference. In a goal-oriented dialogue scenario, these latent intentions can be interpreted as actions guiding the generation of machine responses, which can be further refined autonomously by reinforcement learning. The experimental evaluation of LIDM shows that the model out-performs published benchmarks for both corpus-based and human evaluation, demonstrating the effectiveness of discrete latent variable models for learning goal-oriented dialogues.",
            "referenceCount": 47,
            "citationCount": 143,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-05-29",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1705.10229"
            },
            "citationStyles": {
                "bibtex": "@Article{Wen2017LatentID,\n author = {Tsung-Hsien Wen and Yishu Miao and Phil Blunsom and S. Young},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Latent Intention Dialogue Models},\n volume = {abs/1705.10229},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8515c4d23864a9a67bf6a9226d3d0d60bfb3a4c3",
            "@type": "ScholarlyArticle",
            "paperId": "8515c4d23864a9a67bf6a9226d3d0d60bfb3a4c3",
            "corpusId": 29700362,
            "url": "https://www.semanticscholar.org/paper/8515c4d23864a9a67bf6a9226d3d0d60bfb3a4c3",
            "title": "Toward empirical behavior laws. I. positive reinforcement.",
            "venue": "Psychology Review",
            "publicationVenue": {
                "id": "urn:research:dfb7f114-609c-4c34-9ee7-8cb1fc3e4a6b",
                "name": "Psychology Review",
                "alternate_names": [
                    "Psychol rev",
                    "Psychological review",
                    "Psychol Rev",
                    "Psychological Review"
                ],
                "issn": "1354-1129",
                "url": "http://www.apa.org/journals/rev/"
            },
            "year": 1959,
            "externalIds": {
                "MAG": "2091695392",
                "DOI": "10.1037/H0040891",
                "CorpusId": 29700362,
                "PubMed": "13668003"
            },
            "abstract": "This account of reinforcement is based upon a generalization, not a theory. Few cases underly the generalization, but that which is generalized to are measurable properties of behavior. If accurate, the present generalization will provide: first, an explanation of reinforcement; second, a criterion for evaluating the logical need for motivation constructs; and third, a possible basis for an empirical quantitative account of learning. This first paper, however, deals mainly with positive reinforcement. Learning, motivation, and the aversive case require independent treatment, though some reference to these topics will occur here.",
            "referenceCount": 18,
            "citationCount": 564,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1959-07-01",
            "journal": {
                "name": "Psychological review",
                "volume": "66 4"
            },
            "citationStyles": {
                "bibtex": "@Article{Premack1959TowardEB,\n author = {D. Premack},\n booktitle = {Psychology Review},\n journal = {Psychological review},\n pages = {\n          219-33\n        },\n title = {Toward empirical behavior laws. I. positive reinforcement.},\n volume = {66 4},\n year = {1959}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5c59dd407735b46eb3956125b10c1bc52e343736",
            "@type": "ScholarlyArticle",
            "paperId": "5c59dd407735b46eb3956125b10c1bc52e343736",
            "corpusId": 146273745,
            "url": "https://www.semanticscholar.org/paper/5c59dd407735b46eb3956125b10c1bc52e343736",
            "title": "Teacher Praise: A Functional Analysis",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1981,
            "externalIds": {
                "MAG": "2156830112",
                "DOI": "10.3102/00346543051001005",
                "CorpusId": 146273745
            },
            "abstract": "Classroom-process data indicate that teachers\u2019 verbal praise cannot be equated with reinforcement. Typically, such praise is used infrequently, without contingency, specificity, or credibility. Often it is not even intended as reinforcement, and even when it is, it frequently has some other function. The meanings and functions of behaviors typically included under the category of teacher praise are determined by the degree of congruence between verbal and nonverbal components and by the context in which the interaction occurs. Much teacher praise is determined more by teachers\u2019 perceptions of student needs than by the quality of student conduct or performance. Considerations of classroom feasibility and probable student response to teachers\u2019 attempts at social reinforcement suggest that teacher praise should remain infrequent, but that it could be made much more effective. Attribution theory is an important supplement to social learning/reinforcement theory for suggesting guidelines for praising effectively.",
            "referenceCount": 93,
            "citationCount": 837,
            "influentialCitationCount": 94,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1981-03-01",
            "journal": {
                "name": "Review of Educational Research",
                "volume": "51"
            },
            "citationStyles": {
                "bibtex": "@Article{Brophy1981TeacherPA,\n author = {J. Brophy},\n journal = {Review of Educational Research},\n pages = {32 - 5},\n title = {Teacher Praise: A Functional Analysis},\n volume = {51},\n year = {1981}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b3412ded0375f8fe7336e82dc534eed994cac088",
            "@type": "ScholarlyArticle",
            "paperId": "b3412ded0375f8fe7336e82dc534eed994cac088",
            "corpusId": 2029607,
            "url": "https://www.semanticscholar.org/paper/b3412ded0375f8fe7336e82dc534eed994cac088",
            "title": "Transfer Learning via Inter-Task Mappings for Temporal Difference Learning",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2133040789",
                "DBLP": "journals/jmlr/TaylorSL07",
                "DOI": "10.5555/1314498.1314569",
                "CorpusId": 2029607
            },
            "abstract": "Temporal difference (TD) learning (Sutton and Barto, 1998) has become a popular reinforcement learning technique in recent years. TD methods, relying on function approximators to generalize learning to novel situations, have had some experimental successes and have been shown to exhibit some desirable properties in theory, but the most basic algorithms have often been found slow in practice. This empirical result has motivated the development of many methods that speed up reinforcement learning by modifying a task for the learner or helping the learner better generalize to novel situations. This article focuses on generalizing across tasks, thereby speeding up learning, via a novel form of transfer using handcoded task relationships. We compare learning on a complex task with three function approximators, a cerebellar model arithmetic computer (CMAC), an artificial neural network (ANN), and a radial basis function (RBF), and empirically demonstrate that directly transferring the action-value function can lead to a dramatic speedup in learning with all three. Using transfer via inter-task mapping (TVITM), agents are able to learn one task and then markedly reduce the time it takes to learn a more complex task. Our algorithms are fully implemented and tested in the RoboCup soccer Keepaway domain. \n \nThis article contains and extends material published in two conference papers (Taylor and Stone, 2005; Taylor et al., 2005).",
            "referenceCount": 41,
            "citationCount": 240,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Taylor2007TransferLV,\n author = {Matthew E. Taylor and P. Stone and Yaxin Liu},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {2125-2167},\n title = {Transfer Learning via Inter-Task Mappings for Temporal Difference Learning},\n volume = {8},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:59163c328ff67408ae58f38149f8818535817535",
            "@type": "ScholarlyArticle",
            "paperId": "59163c328ff67408ae58f38149f8818535817535",
            "corpusId": 15940744,
            "url": "https://www.semanticscholar.org/paper/59163c328ff67408ae58f38149f8818535817535",
            "title": "Market forces meet behavioral biases: cost misallocation and irrational pricing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2117277881",
                "DOI": "10.1111/J.0741-6261.2008.00011.X",
                "CorpusId": 15940744
            },
            "abstract": "Psychological and experimental evidence, as well as a wealth of anecdotal examples, suggests that firms may confound fixed, sunk, and variable costs, leading to distorted pricing decisions. This article investigates the extent to which market forces and learning eventually eliminate these distortions. We envision firms that experiment with cost methodologies that are consistent with real-world accounting practices, including ones that confuse the relevance of variable, fixed, and sunk costs to pricing decisions. Firms follow naive adaptive learning to adjust prices and reinforcement learning to modify their costing methodologies. Costing and pricing practices that increase profits are reinforced. In some market structures, but not in others, this process of reinforcement causes pricing practices of all firms to systematically depart from standard equilibrium predictions. Copyright (c)2008, RAND.",
            "referenceCount": 36,
            "citationCount": 461,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Economics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Economics",
                    "source": "external"
                },
                {
                    "category": "Business",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2008-03-01",
            "journal": {
                "name": "The RAND Journal of Economics",
                "volume": "39"
            },
            "citationStyles": {
                "bibtex": "@Article{Al-Najjar2008MarketFM,\n author = {Nabil I. Al-Najjar and Sandeep Baliga and David Besanko},\n journal = {The RAND Journal of Economics},\n pages = {214-237},\n title = {Market forces meet behavioral biases: cost misallocation and irrational pricing},\n volume = {39},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:675be3c1f8a57015a91be5cd191a8d262a9061fb",
            "@type": "ScholarlyArticle",
            "paperId": "675be3c1f8a57015a91be5cd191a8d262a9061fb",
            "corpusId": 61096220,
            "url": "https://www.semanticscholar.org/paper/675be3c1f8a57015a91be5cd191a8d262a9061fb",
            "title": "Interactions between learning and evolution",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1991,
            "externalIds": {
                "MAG": "1526317612",
                "DOI": "10.7551/mitpress/1432.003.0027",
                "CorpusId": 61096220
            },
            "abstract": "Artificial Life (AL) test bed Results Decision Making Project Applicability Motivation This combines two completely different time scales Learning is a individual behaviour Evolution operates at the level of populations over extended periods of time \" An entire life of learning is but one tick of the clock for evolution. \" What is the motivation to create a system that combines genetic programming with reinforcement learning? Motivation Extremely difficult to study the interaction between evolution and learning in the \" real world \" fossils provide information about evolution but provide very little data about day-today life can study evolution of some life forms (viruses, bacteria, flies) that have a short enough life span to study a significant number of generations, but unfortunately these have little ability to learn Motivation",
            "referenceCount": 2,
            "citationCount": 510,
            "influentialCitationCount": 26,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Ackley1991InteractionsBL,\n author = {D. Ackley and M. Littman},\n title = {Interactions between learning and evolution},\n year = {1991}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8302c2a57c98c08d634aa2212a87cd9009ecc2fe",
            "@type": "ScholarlyArticle",
            "paperId": "8302c2a57c98c08d634aa2212a87cd9009ecc2fe",
            "corpusId": 7089976,
            "url": "https://www.semanticscholar.org/paper/8302c2a57c98c08d634aa2212a87cd9009ecc2fe",
            "title": "Learning to control an inverted pendulum using neural networks",
            "venue": "IEEE Control Systems",
            "publicationVenue": {
                "id": "urn:research:c7be0e82-620f-44c5-b6de-46310d59c7b8",
                "name": "IEEE Control Systems",
                "alternate_names": [
                    "IEEE Control Syst",
                    "IEEE Control Systems Magazine",
                    "IEEE Control Syst Mag"
                ],
                "issn": "1066-033X",
                "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5488303"
            },
            "year": 1989,
            "externalIds": {
                "MAG": "2102295697",
                "DOI": "10.1109/37.24809",
                "CorpusId": 7089976
            },
            "abstract": "An inverted pendulum is simulated as a control task with the goal of learning to balance the pendulum with no a priori knowledge of the dynamics. In contrast to other applications of neural networks to the inverted pendulum task, performance feedback is assumed to be unavailable on each step, appearing only as a failure signal when the pendulum falls or reaches the bounds of a horizontal track. To solve this task, the controller must deal with issues of delayed performance evaluation, learning under uncertainty, and the learning of nonlinear functions. Reinforcement and temporal-difference learning methods are presented that deal with these issues to avoid unstable conditions and balance the pendulum.<<ETX>>",
            "referenceCount": 33,
            "citationCount": 577,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1989-04-01",
            "journal": {
                "name": "IEEE Control Systems Magazine",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Anderson1989LearningTC,\n author = {Charles Anderson},\n booktitle = {IEEE Control Systems},\n journal = {IEEE Control Systems Magazine},\n pages = {31-37},\n title = {Learning to control an inverted pendulum using neural networks},\n volume = {9},\n year = {1989}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1852fc02b4cdbd6d0d97a978b61ca6e12458c5ce",
            "@type": "ScholarlyArticle",
            "paperId": "1852fc02b4cdbd6d0d97a978b61ca6e12458c5ce",
            "corpusId": 8014503,
            "url": "https://www.semanticscholar.org/paper/1852fc02b4cdbd6d0d97a978b61ca6e12458c5ce",
            "title": "Rational and Convergent Learning in Stochastic Games",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "361876",
                "DBLP": "conf/ijcai/BowlingV01",
                "CorpusId": 8014503
            },
            "abstract": "This paper investigates the problem of policy learning in multiagent environments using the stochastic game framework, which we briefly overview. We introduce two properties as desirable for a learning agent when in the presence of other learning agents, namely rationality and convergence. We examine existing reinforcement learning algorithms according to these two properties and notice that they fail to simultaneously meet both criteria. We then contribute a new learning algorithm, WoLF policy hillclimbing, that is based on a simple principle: \u201clearn quickly while losing, slowly while winning.\u201d The algorithm is proven to be rational and we present empirical results for a number of stochastic games showing the algorithm converges.",
            "referenceCount": 146,
            "citationCount": 346,
            "influentialCitationCount": 48,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2001-08-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bowling2001RationalAC,\n author = {Michael Bowling and M. Veloso},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {1021-1026},\n title = {Rational and Convergent Learning in Stochastic Games},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:15bd4f2f5276937c03c57dda3692699ff4500cea",
            "@type": "ScholarlyArticle",
            "paperId": "15bd4f2f5276937c03c57dda3692699ff4500cea",
            "corpusId": 14900223,
            "url": "https://www.semanticscholar.org/paper/15bd4f2f5276937c03c57dda3692699ff4500cea",
            "title": "The Curse of Planning",
            "venue": "Psychology Science",
            "publicationVenue": {
                "id": "urn:research:a990ee76-444c-4bda-b714-df4dad0efa5d",
                "name": "Psychology Science",
                "alternate_names": [
                    "Psychol Sci",
                    "Psychological Science"
                ],
                "issn": "1614-9947",
                "url": "http://www.psychologie-aktuell.com/index.php?id=204"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2111535985",
                "DOI": "10.1177/0956797612463080",
                "CorpusId": 14900223,
                "PubMed": "23558545"
            },
            "abstract": "A number of accounts of human and animal behavior posit the operation of parallel and competing valuation systems in the control of choice behavior. In these accounts, a flexible but computationally expensive model-based reinforcement-learning system has been contrasted with a less flexible but more efficient model-free reinforcement-learning system. The factors governing which system controls behavior\u2014and under what circumstances\u2014are still unclear. Following the hypothesis that model-based reinforcement learning requires cognitive resources, we demonstrated that having human decision makers perform a demanding secondary task engenders increased reliance on a model-free reinforcement-learning strategy. Further, we showed that, across trials, people negotiate the trade-off between the two systems dynamically as a function of concurrent executive-function demands, and people\u2019s choice latencies reflect the computational expenses of the strategy they employ. These results demonstrate that competition between multiple learning systems can be controlled on a trial-by-trial basis by modulating the availability of cognitive resources.",
            "referenceCount": 33,
            "citationCount": 216,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc3843765?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-04-04",
            "journal": {
                "name": "Psychological Science",
                "volume": "24"
            },
            "citationStyles": {
                "bibtex": "@Article{Otto2013TheCO,\n author = {A. R. Otto and Samuel J. Gershman and A. Markman and Nathaniel D. Daw},\n booktitle = {Psychology Science},\n journal = {Psychological Science},\n pages = {751 - 761},\n title = {The Curse of Planning},\n volume = {24},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a27a1fb4b97c0f256ffc1226f6d683bc5fdcfae6",
            "@type": "ScholarlyArticle",
            "paperId": "a27a1fb4b97c0f256ffc1226f6d683bc5fdcfae6",
            "corpusId": 16997122,
            "url": "https://www.semanticscholar.org/paper/a27a1fb4b97c0f256ffc1226f6d683bc5fdcfae6",
            "title": "The Role of the Ventromedial Prefrontal Cortex in Abstract State-Based Inference during Decision Making in Humans",
            "venue": "Journal of Neuroscience",
            "publicationVenue": {
                "id": "urn:research:52edfc09-8d65-4876-8560-84530d7259b9",
                "name": "Journal of Neuroscience",
                "alternate_names": [
                    "The Journal of Neuroscience",
                    "J Neurosci"
                ],
                "issn": "0270-6474",
                "url": "https://www.jneurosci.org/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2041639631",
                "DOI": "10.1523/JNEUROSCI.1010-06.2006",
                "CorpusId": 16997122,
                "PubMed": "16899731"
            },
            "abstract": "Many real-life decision-making problems incorporate higher-order structure, involving interdependencies between different stimuli, actions, and subsequent rewards. It is not known whether brain regions implicated in decision making, such as the ventromedial prefrontal cortex (vmPFC), use a stored model of the task structure to guide choice (model-based decision making) or merely learn action or state values without assuming higher-order structure as in standard reinforcement learning. To discriminate between these possibilities, we scanned human subjects with functional magnetic resonance imaging while they performed a simple decision-making task with higher-order structure, probabilistic reversal learning. We found that neural activity in a key decision-making region, the vmPFC, was more consistent with a computational model that exploits higher-order structure than with simple reinforcement learning. These results suggest that brain regions, such as the vmPFC, use an abstract model of task structure to guide behavioral choice, computations that may underlie the human capacity for complex social interactions and abstract strategizing.",
            "referenceCount": 41,
            "citationCount": 506,
            "influentialCitationCount": 30,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.jneurosci.org/content/jneuro/26/32/8360.full.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-08-09",
            "journal": {
                "name": "The Journal of Neuroscience",
                "volume": "26"
            },
            "citationStyles": {
                "bibtex": "@Article{Hampton2006TheRO,\n author = {Alan N. Hampton and P. Bossaerts and J. O\u2019Doherty},\n booktitle = {Journal of Neuroscience},\n journal = {The Journal of Neuroscience},\n pages = {8360 - 8367},\n title = {The Role of the Ventromedial Prefrontal Cortex in Abstract State-Based Inference during Decision Making in Humans},\n volume = {26},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b51593d0e4919b06b86868c15bd6c846d30930c9",
            "@type": "ScholarlyArticle",
            "paperId": "b51593d0e4919b06b86868c15bd6c846d30930c9",
            "corpusId": 146254668,
            "url": "https://www.semanticscholar.org/paper/b51593d0e4919b06b86868c15bd6c846d30930c9",
            "title": "The \"supersitition\" experiment: A reexamination of its implications for the principles of adaptive behavior.",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1971,
            "externalIds": {
                "MAG": "2162733293",
                "DOI": "10.1037/H0030305",
                "CorpusId": 146254668
            },
            "abstract": "Replication and extension of Skinner's \"superstition\" experiment showed the development of two kinds of behavior at asymptote: interim activities (related to adjunctive behavior) occurred just after food delivery; the terminal response (a discriminated operant) occurred toward the end of the interval and continued until food delivery. These data suggest a view of operant conditioning (the terminal response) in terms of two sets of principles: principles of behavioral variation that describe the origins of behavior \"appropriate\" to a situation, in advance of reinforcement; and principles of reinforcement that describe the selective elimination of behavior so produced. This approach was supported by (a) an account of the parallels between the Law of Effect and evolution by means of natural selection, (fc) its ability to shed light on persistent problems in learning (e.g., continuity vs. noncontinuity, variability associated with extinction, the relationship between classical and instrumental conditioning, the controversy between behaviorist and cognitive approaches to learning), and (c) its ability to deal with a number of recent anomalies in the learning literature (\"instinctive drift,\" auto-shaping, and auto-maintenance). The interim activities were interpreted in terms of interactions among motivational systems, and this view was supported by a review of the literature on adjunctive behavior and by comparison with similar phenomena in ethology (displacement, redirection, and \"vacuum\" activities). The proposed theoretical scheme represents a shift away from hypothetical \"laws of learning\" toward an interpretation of behavioral change in terms of interaction and competition among tendencies to action according to principles evolved in phylogeny.",
            "referenceCount": 102,
            "citationCount": 1066,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Psychological Review",
                "volume": "78"
            },
            "citationStyles": {
                "bibtex": "@Article{Staddon1971TheE,\n author = {J. Staddon and Virginia L. Simmelhag},\n journal = {Psychological Review},\n pages = {3-43},\n title = {The \"supersitition\" experiment: A reexamination of its implications for the principles of adaptive behavior.},\n volume = {78},\n year = {1971}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f3e10675b2ef79d8431b8011f909ee0d05e92d92",
            "@type": "ScholarlyArticle",
            "paperId": "f3e10675b2ef79d8431b8011f909ee0d05e92d92",
            "corpusId": 2568346,
            "url": "https://www.semanticscholar.org/paper/f3e10675b2ef79d8431b8011f909ee0d05e92d92",
            "title": "Incremental multi-step Q-learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1994,
            "externalIds": {
                "MAG": "2610686804",
                "DBLP": "conf/icml/PengW94",
                "DOI": "10.1007/BF00114731",
                "CorpusId": 2568346
            },
            "abstract": null,
            "referenceCount": 19,
            "citationCount": 409,
            "influentialCitationCount": 40,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/BF00114731.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1994-07-10",
            "journal": {
                "name": "Machine Learning",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Peng1994IncrementalMQ,\n author = {Jing Peng and Ronald J. Williams},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {283-290},\n title = {Incremental multi-step Q-learning},\n volume = {22},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:55c7cb8ca85c751f7a418ae06143d9f3473ce526",
            "@type": "ScholarlyArticle",
            "paperId": "55c7cb8ca85c751f7a418ae06143d9f3473ce526",
            "corpusId": 6988226,
            "url": "https://www.semanticscholar.org/paper/55c7cb8ca85c751f7a418ae06143d9f3473ce526",
            "title": "Least-Squares Temporal Difference Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1597303641",
                "DBLP": "conf/icml/Boyan99",
                "CorpusId": 6988226
            },
            "abstract": "TD( ) is a popular family of algorithms for approximate policy evaluation in large MDPs. TD( ) works by incrementally updating the value function after each observed transition. It has two major drawbacks: it makes ine cient use of data, and it requires the user to manually tune a stepsize schedule for good performance. For the case of linear value function approximations and = 0, the Least-Squares TD (LSTD) algorithm of Bradtke and Barto (Bradtke and Barto, 1996) eliminates all stepsize parameters and improves data e ciency. This paper extends Bradtke and Barto's work in three signi cant ways. First, it presents a simpler derivation of the LSTD algorithm. Second, it generalizes from = 0 to arbitrary values of ; at the extreme of = 1, the resulting algorithm is shown to be a practical formulation of supervised linear regression. Third, it presents a novel, intuitive interpretation of LSTD as a model-based reinforcement learning technique.",
            "referenceCount": 12,
            "citationCount": 320,
            "influentialCitationCount": 38,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1999-06-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Boyan1999LeastSquaresTD,\n author = {J. Boyan},\n booktitle = {International Conference on Machine Learning},\n pages = {49-56},\n title = {Least-Squares Temporal Difference Learning},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d71c82fcb1fa2d8bb79bbd848b20f6ec53b076a9",
            "@type": "ScholarlyArticle",
            "paperId": "d71c82fcb1fa2d8bb79bbd848b20f6ec53b076a9",
            "corpusId": 8469001,
            "url": "https://www.semanticscholar.org/paper/d71c82fcb1fa2d8bb79bbd848b20f6ec53b076a9",
            "title": "Creating Advice-Taking Reinforcement Learners",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2174230203",
                "DBLP": "journals/ml/MaclinS96",
                "DOI": "10.1023/A:1018020625251",
                "CorpusId": 8469001
            },
            "abstract": null,
            "referenceCount": 64,
            "citationCount": 158,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023%2FA%3A1018020625251.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1998-05-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Maclin1998CreatingAR,\n author = {R. Maclin and J. Shavlik},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {251-281},\n title = {Creating Advice-Taking Reinforcement Learners},\n volume = {22},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:58fb60c5592224901a26dd84220a2f3332c1fcf5",
            "@type": "ScholarlyArticle",
            "paperId": "58fb60c5592224901a26dd84220a2f3332c1fcf5",
            "corpusId": 3300406,
            "url": "https://www.semanticscholar.org/paper/58fb60c5592224901a26dd84220a2f3332c1fcf5",
            "title": "Eigenoption Discovery through the Deep Successor Representation",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2765308067",
                "DBLP": "journals/corr/abs-1710-11089",
                "ArXiv": "1710.11089",
                "CorpusId": 3300406
            },
            "abstract": "Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning. However, autonomously learning effective sets of options is still a major challenge in the field. In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process. Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment. We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available. We propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels. It exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation. We use traditional tabular domains to provide intuition about our approach and Atari 2600 games to demonstrate its potential.",
            "referenceCount": 36,
            "citationCount": 115,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-10-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1710.11089"
            },
            "citationStyles": {
                "bibtex": "@Article{Machado2017EigenoptionDT,\n author = {Marlos C. Machado and C. Rosenbaum and Xiaoxiao Guo and Miao Liu and G. Tesauro and Murray Campbell},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Eigenoption Discovery through the Deep Successor Representation},\n volume = {abs/1710.11089},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6df43f70f383007a946448122b75918e3a9d6682",
            "@type": "ScholarlyArticle",
            "paperId": "6df43f70f383007a946448122b75918e3a9d6682",
            "corpusId": 5538688,
            "url": "https://www.semanticscholar.org/paper/6df43f70f383007a946448122b75918e3a9d6682",
            "title": "Learning to Achieve Goals",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 1993,
            "externalIds": {
                "MAG": "1594201624",
                "DBLP": "conf/ijcai/Kaelbling93",
                "CorpusId": 5538688
            },
            "abstract": "Temporal di(cid:11)erence methods solve the temporal credit assignment problem for reinforcement learning. An important subproblem of general reinforcement learning is learning to achieve dynamic goals. Although existing temporal di(cid:11)erence methods, such as Q learning, can be applied to this problem, they do not take advantage of its special structure. This paper presents the DG-learning algorithm, which learns e(cid:14)ciently to achieve dynamically changing goals and exhibits good knowledge transfer between goals. In addition, this paper shows how traditional relaxation techniques can be applied to the problem. Finally, experimental results are given that demonstrate the su-periority of DG learning over Q learning in a moderately large, synthetic, non-deterministic domain.",
            "referenceCount": 12,
            "citationCount": 367,
            "influentialCitationCount": 39,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kaelbling1993LearningTA,\n author = {L. Kaelbling},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {1094-1099},\n title = {Learning to Achieve Goals},\n year = {1993}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:616465ae17bd026f110aedb5dfbb32e564358b3e",
            "@type": "ScholarlyArticle",
            "paperId": "616465ae17bd026f110aedb5dfbb32e564358b3e",
            "corpusId": 142947174,
            "url": "https://www.semanticscholar.org/paper/616465ae17bd026f110aedb5dfbb32e564358b3e",
            "title": "Learning and Complex Behavior",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1993,
            "externalIds": {
                "MAG": "1690644297",
                "CorpusId": 142947174
            },
            "abstract": "Written from the perspective of selectionist theory, this text presents a theoretically integrated approach to the study of animal learning and human cognition that co-ordinates behavioural research and research in neuroscience. It covers traditional topics such as acquisition and extinction of behaviour, stimulus control and schedules of reinforcement, and also deals with topics of student interest such as perception, memory, problem solving and verbal behaviour. All of these topics are discussed in terms of principles established by experimental analysis at the behavioural and neural levels, and scientific interpretation based on those principles.",
            "referenceCount": 0,
            "citationCount": 450,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1993-10-08",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Donahoe1993LearningAC,\n author = {J. Donahoe and D. C. Palmer},\n title = {Learning and Complex Behavior},\n year = {1993}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dd3145dc29eca12796eff725fd26ae34532df8df",
            "@type": "ScholarlyArticle",
            "paperId": "dd3145dc29eca12796eff725fd26ae34532df8df",
            "corpusId": 11866897,
            "url": "https://www.semanticscholar.org/paper/dd3145dc29eca12796eff725fd26ae34532df8df",
            "title": "Sparse odor representation and olfactory learning",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2051144047",
                "DOI": "10.1038/nn.2192",
                "CorpusId": 11866897,
                "PubMed": "18794840"
            },
            "abstract": null,
            "referenceCount": 44,
            "citationCount": 150,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc3124899?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-09-14",
            "journal": {
                "name": "Nature Neuroscience",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Ito2008SparseOR,\n author = {I. Ito and R. C. Ong and B. Raman and M. Stopfer},\n booktitle = {Nature Neuroscience},\n journal = {Nature Neuroscience},\n pages = {1177-1184},\n title = {Sparse odor representation and olfactory learning},\n volume = {11},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:add2e65674285d50a795c64aba915fe0c5ed3359",
            "@type": "ScholarlyArticle",
            "paperId": "add2e65674285d50a795c64aba915fe0c5ed3359",
            "corpusId": 13052705,
            "url": "https://www.semanticscholar.org/paper/add2e65674285d50a795c64aba915fe0c5ed3359",
            "title": "Learning to Coordinate without Sharing Information",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 1994,
            "externalIds": {
                "DBLP": "conf/aaai/SenSH94",
                "MAG": "2142839172",
                "CorpusId": 13052705
            },
            "abstract": "Researchers in the field of Distributed Artificial Intelligence (DAI) have been developing efficient mechanisms to coordinate the activities of multiple autonomous agents. The need for coordination arises because agents have to share resources and expertise required to achieve their goals. Previous work in the area includes using sophisticated information exchange protocols, investigating heuristics for negotiation, and developing formal models of possibilities of conflict and cooperation among agent interests. In order to handle the changing requirements of continuous and dynamic environments, we propose learning as a means to provide additional possibilities for effective coordination. We use reinforcement learning techniques on a block pushing problem to show that agents can learn complimentary policies to follow a desired path without any knowledge about each other. We theoretically analyze and experimentally verify the effects of learning rate on system convergence, and demonstrate benefits of using learned coordination knowledge on similar problems. Reinforcement learning based coordination can be achieved in both cooperative and non-cooperative domains, and in domains with noisy communication channels and other stochastic characteristics that present a formidable challenge to using other coordination schemes.",
            "referenceCount": 14,
            "citationCount": 383,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1994-10-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sen1994LearningTC,\n author = {S. Sen and Mahendra Sekaran and John Hale},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {426-431},\n title = {Learning to Coordinate without Sharing Information},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:85f98aeb7cf4dce4f5ab8dd06f94a0080b0a96c2",
            "@type": "ScholarlyArticle",
            "paperId": "85f98aeb7cf4dce4f5ab8dd06f94a0080b0a96c2",
            "corpusId": 2622974,
            "url": "https://www.semanticscholar.org/paper/85f98aeb7cf4dce4f5ab8dd06f94a0080b0a96c2",
            "title": "Model of birdsong learning based on gradient estimation by dynamic perturbation of neural conductances.",
            "venue": "Journal of Neurophysiology",
            "publicationVenue": {
                "id": "urn:research:d2b57141-ecff-4b3d-b490-7928e8aafc17",
                "name": "Journal of Neurophysiology",
                "alternate_names": [
                    "J Neurophysiol"
                ],
                "issn": "0022-3077",
                "url": "https://www.physiology.org/journal/jn"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2106437793",
                "DOI": "10.1152/JN.01311.2006",
                "CorpusId": 2622974,
                "PubMed": "17652414"
            },
            "abstract": "We propose a model of songbird learning that focuses on avian brain areas HVC and RA, involved in song production, and area LMAN, important for generating song variability. Plasticity at HVC --> RA synapses is driven by hypothetical \"rules\" depending on three signals: activation of HVC --> RA synapses, activation of LMAN --> RA synapses, and reinforcement from an internal critic that compares the bird's own song with a memorized template of an adult tutor's song. Fluctuating glutamatergic input to RA from LMAN generates behavioral variability for trial-and-error learning. The plasticity rules perform gradient-based reinforcement learning in a spiking neural network model of song production. Although the reinforcement signal is delayed, temporally imprecise, and binarized, the model learns in a reasonable amount of time in numerical simulations. Varying the number of neurons in HVC and RA has little effect on learning time. The model makes specific predictions for the induction of bidirectional long-term plasticity at HVC --> RA synapses.",
            "referenceCount": 78,
            "citationCount": 166,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-10-01",
            "journal": {
                "name": "Journal of neurophysiology",
                "volume": "98 4"
            },
            "citationStyles": {
                "bibtex": "@Article{Fiete2007ModelOB,\n author = {I. Fiete and M. Fee and Sebastian Seung and Karin Sch\u00f6fegger},\n booktitle = {Journal of Neurophysiology},\n journal = {Journal of neurophysiology},\n pages = {\n          2038-57\n        },\n title = {Model of birdsong learning based on gradient estimation by dynamic perturbation of neural conductances.},\n volume = {98 4},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3262878cd903e63fcf09458f356fe4a4e31912c8",
            "@type": "ScholarlyArticle",
            "paperId": "3262878cd903e63fcf09458f356fe4a4e31912c8",
            "corpusId": 24198002,
            "url": "https://www.semanticscholar.org/paper/3262878cd903e63fcf09458f356fe4a4e31912c8",
            "title": "Amygdaloid complex lesions differentially affect retention of tasks using appetitive and aversive reinforcement.",
            "venue": "Behavioral Neuroscience",
            "publicationVenue": {
                "id": "urn:research:cf409ff2-7895-45cd-afcc-61bfa0f60ff8",
                "name": "Behavioral Neuroscience",
                "alternate_names": [
                    "Behav Neurosci"
                ],
                "issn": "0735-7044",
                "url": "http://www.apa.org/journals/bne/"
            },
            "year": 1990,
            "externalIds": {
                "MAG": "2076867211",
                "DOI": "10.1037//0735-7044.104.4.532",
                "CorpusId": 24198002,
                "PubMed": "2206424"
            },
            "abstract": "The hypothesis that the amygdaloid complex (AC) is involved in the formation of stimulus-reward associations was examined. A series of experiments (1A-1C) directly compared the effects of lesions (produced by injection of the excitotoxin N-methyl-D-aspartic acid) on 1-trial appetitive and 1-trial aversive learning in rats. Experiments 1A and 1B, which used different degrees of reinforcement, revealed no effect of lesions on the appetitive task, whereas acquisition of the aversive task was significantly impaired. This impairment depended on the nature of the aversive reinforcement used: Impairment was seen when a highly aversive stimulus (footshock) was used but not when a less aversive stimulus (0.2% quinine solution) was used. Control experiments indicated that the effect of lesions was not due to reduced sensitivity to the foot-shock. In Experiment 2, a novel odor conditioning task examined further the effect of AC lesions on the acquisition of appetitive and aversive stimulus-reinforcement associations. As in Experiment 1, the AC lesions impaired learning of the aversive association but did not significantly influence the appetitive association. It is argued that, although the AC may be involved in some types of appetitively rewarded learning, the findings of a differential effect of AC lesions on aversively rewarded learning suggest a role in learning beyond the formation of stimulus-reinforcement associations.",
            "referenceCount": 7,
            "citationCount": 243,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1990-08-01",
            "journal": {
                "name": "Behavioral neuroscience",
                "volume": "104 4"
            },
            "citationStyles": {
                "bibtex": "@Article{Cahill1990AmygdaloidCL,\n author = {L. Cahill and J. D. McGaugh},\n booktitle = {Behavioral Neuroscience},\n journal = {Behavioral neuroscience},\n pages = {\n          532-43\n        },\n title = {Amygdaloid complex lesions differentially affect retention of tasks using appetitive and aversive reinforcement.},\n volume = {104 4},\n year = {1990}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0bc161be3f413c79fd12c2538a999c3db2efb26f",
            "@type": "ScholarlyArticle",
            "paperId": "0bc161be3f413c79fd12c2538a999c3db2efb26f",
            "corpusId": 8235461,
            "url": "https://www.semanticscholar.org/paper/0bc161be3f413c79fd12c2538a999c3db2efb26f",
            "title": "Neocognitron: A neural network model for a mechanism of visual pattern recognition",
            "venue": "IEEE Transactions on Systems, Man and Cybernetics",
            "publicationVenue": {
                "id": "urn:research:336446b6-e859-4f7b-9121-d2d40357fe0a",
                "name": "IEEE Transactions on Systems, Man and Cybernetics",
                "alternate_names": [
                    "IEEE Trans Syst Man Cybern",
                    "IEEE Transactions on Systems, Man, and Cybernetics"
                ],
                "issn": "0018-9472",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=21"
            },
            "year": 1983,
            "externalIds": {
                "MAG": "2256679588",
                "DBLP": "journals/tsmc/FukushimaMI83",
                "DOI": "10.1109/TSMC.1983.6313076",
                "CorpusId": 8235461
            },
            "abstract": "A recognition with a large-scale network is simulated on a PDP-11/34 minicomputer and is shown to have a great capability for visual pattern recognition. The model consists of nine layers of cells. The authors demonstrate that the model can be trained to recognize handwritten Arabic numerals even with considerable deformations in shape. A learning-with-a-teacher process is used for the reinforcement of the modifiable synapses in the new large-scale model, instead of the learning-without-a-teacher process applied to a previous model. The authors focus on the mechanism for pattern recognition rather than that for self-organization.",
            "referenceCount": 0,
            "citationCount": 812,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1983-09-01",
            "journal": {
                "name": "IEEE Transactions on Systems, Man, and Cybernetics",
                "volume": "SMC-13"
            },
            "citationStyles": {
                "bibtex": "@Article{Fukushima1983NeocognitronAN,\n author = {K. Fukushima and S. Miyake and Takayuki Ito},\n booktitle = {IEEE Transactions on Systems, Man and Cybernetics},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics},\n pages = {826-834},\n title = {Neocognitron: A neural network model for a mechanism of visual pattern recognition},\n volume = {SMC-13},\n year = {1983}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:15ba832dbb1882b2c516e06cb0f8f002d16bc4cf",
            "@type": "ScholarlyArticle",
            "paperId": "15ba832dbb1882b2c516e06cb0f8f002d16bc4cf",
            "corpusId": 1425,
            "url": "https://www.semanticscholar.org/paper/15ba832dbb1882b2c516e06cb0f8f002d16bc4cf",
            "title": "Efficient Structure Learning in Factored-State MDPs",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "1537180453",
                "DBLP": "conf/aaai/StrehlDL07",
                "CorpusId": 1425
            },
            "abstract": "We consider the problem of reinforcement learning in factored-state MDPs in the setting in which learning is conducted in one long trial with no resets allowed. We show how to extend existing efficient algorithms that learn the conditional probability tables of dynamic Bayesian networks (DBNs) given their structure to the case in which DBN structure is not known in advance. Our method learns the DBN structures as part of the reinforcement-learning process and provably provides an efficient learning algorithm when combined with factored Rmax.",
            "referenceCount": 12,
            "citationCount": 135,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2007-07-22",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Strehl2007EfficientSL,\n author = {Alexander L. Strehl and Carlos Diuk and M. Littman},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {645-650},\n title = {Efficient Structure Learning in Factored-State MDPs},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:41edbf60dab9e75718913e52ddc92ff224d3a7ca",
            "@type": "ScholarlyArticle",
            "paperId": "41edbf60dab9e75718913e52ddc92ff224d3a7ca",
            "corpusId": 15718136,
            "url": "https://www.semanticscholar.org/paper/41edbf60dab9e75718913e52ddc92ff224d3a7ca",
            "title": "Selective enhancement of associative learning by microstimulation of the anterior caudate",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2167694167",
                "DOI": "10.1038/nn1662",
                "CorpusId": 15718136,
                "PubMed": "16501567"
            },
            "abstract": null,
            "referenceCount": 29,
            "citationCount": 229,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-04-01",
            "journal": {
                "name": "Nature Neuroscience",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Williams2006SelectiveEO,\n author = {Ziv M. Williams and E. Eskandar},\n booktitle = {Nature Neuroscience},\n journal = {Nature Neuroscience},\n pages = {562-568},\n title = {Selective enhancement of associative learning by microstimulation of the anterior caudate},\n volume = {9},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea98f21a7cace5d9f5387d2c5a10e5304d8a9a6d",
            "@type": "ScholarlyArticle",
            "paperId": "ea98f21a7cace5d9f5387d2c5a10e5304d8a9a6d",
            "corpusId": 16503204,
            "url": "https://www.semanticscholar.org/paper/ea98f21a7cace5d9f5387d2c5a10e5304d8a9a6d",
            "title": "Addiction as a Computational Process Gone Awry",
            "venue": "Science",
            "publicationVenue": {
                "id": "urn:research:f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
                "name": "Science",
                "alternate_names": null,
                "issn": "0193-4511",
                "url": "https://www.jstor.org/journal/science"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2119631764",
                "DOI": "10.1126/SCIENCE.1102384",
                "CorpusId": 16503204,
                "PubMed": "15591205"
            },
            "abstract": "Addictive drugs have been hypothesized to access the same neurophysiological mechanisms as natural learning systems. These natural learning systems can be modeled through temporal-difference reinforcement learning (TDRL), which requires a reward-error signal that has been hypothesized to be carried by dopamine. TDRL learns to predict reward by driving that reward-error signal to zero. By adding a noncompensable drug-induced dopamine increase to a TDRL model, a computational model of addiction is constructed that over-selects actions leading to drug receipt. The model provides an explanation for important aspects of the addiction literature and provides a theoretic view-point with which to address other aspects.",
            "referenceCount": 65,
            "citationCount": 488,
            "influentialCitationCount": 31,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2004-12-10",
            "journal": {
                "name": "Science",
                "volume": "306"
            },
            "citationStyles": {
                "bibtex": "@Article{Mehlmann2004AddictionAA,\n author = {L. Mehlmann and Yoshinaga Saeki and Shigeru Tanaka and Thomas J. Brennan and A. Evsikov and F. L. Pendola and Barbara B. Knowles and J. Eppig and Laurinda A. Jaffe},\n booktitle = {Science},\n journal = {Science},\n pages = {1944 - 1947},\n title = {Addiction as a Computational Process Gone Awry},\n volume = {306},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:966e41903b4aff42601a188bd7b26d71ef120d11",
            "@type": "ScholarlyArticle",
            "paperId": "966e41903b4aff42601a188bd7b26d71ef120d11",
            "corpusId": 1907951,
            "url": "https://www.semanticscholar.org/paper/966e41903b4aff42601a188bd7b26d71ef120d11",
            "title": "Accelerated Neural Evolution through Cooperatively Coevolved Synapses",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "journals/jmlr/GomezSM08",
                "MAG": "2148067905",
                "DOI": "10.5555/1390681.1390712",
                "CorpusId": 1907951
            },
            "abstract": "Many complex control problems require sophisticated solutions that are not amenable to traditional controller design. Not only is it difficult to model real world systems, but often it is unclear what kind of behavior is required to solve the task. Reinforcement learning (RL) approaches have made progress by using direct interaction with the task environment, but have so far not scaled well to large state spaces and environments that are not fully observable. In recent years, neuroevolution, the artificial evolution of neural networks, has had remarkable success in tasks that exhibit these two properties. In this paper, we compare a neuroevolution method called Cooperative Synapse Neuroevolution (CoSyNE), that uses cooperative coevolution at the level of individual synaptic weights, to a broad range of reinforcement learning algorithms on very difficult versions of the pole balancing problem that involve large (continuous) state spaces and hidden state. CoSyNE is shown to be significantly more efficient and powerful than the other methods on these tasks.",
            "referenceCount": 69,
            "citationCount": 336,
            "influentialCitationCount": 39,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-06-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Gomez2008AcceleratedNE,\n author = {F. Gomez and J. Schmidhuber and R. Miikkulainen},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {937-965},\n title = {Accelerated Neural Evolution through Cooperatively Coevolved Synapses},\n volume = {9},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:82affc06a3e500a8e5cf79c656a769d22582c02a",
            "@type": "ScholarlyArticle",
            "paperId": "82affc06a3e500a8e5cf79c656a769d22582c02a",
            "corpusId": 145576255,
            "url": "https://www.semanticscholar.org/paper/82affc06a3e500a8e5cf79c656a769d22582c02a",
            "title": "Behavior Analysis and Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2100450123",
                "DOI": "10.4324/9781410608987",
                "CorpusId": 145576255
            },
            "abstract": "A Science of Behavior: Perspective, History, and Assumptions. The Experimental Analysis of Behavior. Reflexive Behavior and Respondent Conditioning. Reinforcement and Extinction of Operant Behavior. Schedules of Reinforcement. Aversive Control of Behavior. Operant-Respondent Interrelationships and the Biological Context of Conditioning. Stimulus Control. Choice and Preference. Conditioned Reinforcement. Correspondent Relations: Imitation and Rule-Governed Behavior. Verbal Behavior. Applied Behavior Analysis. Three Levels of Selection: Biology, Behavior, and Culture.",
            "referenceCount": 0,
            "citationCount": 352,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1998-07-09",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Pierce1998BehaviorAA,\n author = {W. Pierce and C. Cheney and E. Rasmussen and C. Clay},\n title = {Behavior Analysis and Learning},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:61469df2cff2ae67a012fd53e6148419130ebd96",
            "@type": "ScholarlyArticle",
            "paperId": "61469df2cff2ae67a012fd53e6148419130ebd96",
            "corpusId": 142436039,
            "url": "https://www.semanticscholar.org/paper/61469df2cff2ae67a012fd53e6148419130ebd96",
            "title": "Species-specific defense reactions and avoidance learning",
            "venue": "Pavlovian Journal of Biological Science",
            "publicationVenue": {
                "id": "urn:research:ea0dbf61-95b6-44fb-b515-0cf1d8266635",
                "name": "Pavlovian Journal of Biological Science",
                "alternate_names": [
                    "Cond reflex",
                    "Pavlov J Biological Sci",
                    "Conditional reflex"
                ],
                "issn": "0093-2213",
                "url": "https://link.springer.com/journal/volumesAndIssues/12124"
            },
            "year": 1982,
            "externalIds": {
                "MAG": "1504363854",
                "DOI": "10.1007/BF03001275",
                "CorpusId": 142436039
            },
            "abstract": null,
            "referenceCount": 63,
            "citationCount": 400,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "1982-10-01",
            "journal": {
                "name": "The Pavlovian journal of biological science : official journal of the Pavlovian",
                "volume": "17"
            },
            "citationStyles": {
                "bibtex": "@Article{Crawford1982SpeciesspecificDR,\n author = {M. Crawford and F. Masterson},\n booktitle = {Pavlovian Journal of Biological Science},\n journal = {The Pavlovian journal of biological science : official journal of the Pavlovian},\n pages = {204-214},\n title = {Species-specific defense reactions and avoidance learning},\n volume = {17},\n year = {1982}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0120eefaf05bfad5293e87f56d2e787c05f78cf7",
            "@type": "ScholarlyArticle",
            "paperId": "0120eefaf05bfad5293e87f56d2e787c05f78cf7",
            "corpusId": 5915714,
            "url": "https://www.semanticscholar.org/paper/0120eefaf05bfad5293e87f56d2e787c05f78cf7",
            "title": "Pattern-recognizing stochastic learning automata",
            "venue": "IEEE Transactions on Systems, Man and Cybernetics",
            "publicationVenue": {
                "id": "urn:research:336446b6-e859-4f7b-9121-d2d40357fe0a",
                "name": "IEEE Transactions on Systems, Man and Cybernetics",
                "alternate_names": [
                    "IEEE Trans Syst Man Cybern",
                    "IEEE Transactions on Systems, Man, and Cybernetics"
                ],
                "issn": "0018-9472",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=21"
            },
            "year": 1985,
            "externalIds": {
                "DBLP": "journals/tsmc/BartoA85",
                "MAG": "2021801581",
                "DOI": "10.1109/TSMC.1985.6313371",
                "CorpusId": 5915714
            },
            "abstract": "A class of learning tasks is described that combines aspects of learning automation tasks and supervised learning pattern-classification tasks. These tasks are called associative reinforcement learning tasks. An algorithm is presented, called the associative reward-penalty, or AR-P algorithm for which a form of optimal performance is proved. This algorithm simultaneously generalizes a class of stochastic learning automata and a class of supervised learning pattern-classification methods related to the Robbins-Monro stochastic approximation procedure. The relevance of this hybrid algorithm is discussed with respect to the collective behaviour of learning automata and the behaviour of networks of pattern-classifying adaptive elements. Simulation results are presented that illustrate the associative reinforcement learning task and the performance of the AR-P algorithm as compared with that of several existing algorithms.",
            "referenceCount": 32,
            "citationCount": 328,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1985-05-01",
            "journal": {
                "name": "IEEE Transactions on Systems, Man, and Cybernetics",
                "volume": "SMC-15"
            },
            "citationStyles": {
                "bibtex": "@Article{Barto1985PatternrecognizingSL,\n author = {A. Barto and Padmanabhan Anandan},\n booktitle = {IEEE Transactions on Systems, Man and Cybernetics},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics},\n pages = {360-375},\n title = {Pattern-recognizing stochastic learning automata},\n volume = {SMC-15},\n year = {1985}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4e1eebc0ad44480df84cb37545c1a39c52c7c798",
            "@type": "ScholarlyArticle",
            "paperId": "4e1eebc0ad44480df84cb37545c1a39c52c7c798",
            "corpusId": 141456489,
            "url": "https://www.semanticscholar.org/paper/4e1eebc0ad44480df84cb37545c1a39c52c7c798",
            "title": "A Longitudinal Test of Social Learning Theory: Adolescent Smoking",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1996,
            "externalIds": {
                "MAG": "155599834",
                "DOI": "10.1177/002204269602600203",
                "CorpusId": 141456489
            },
            "abstract": "A general social learning theory of deviance is applied to adolescent smoking as a form of sustance use and tested with data from a 5-year longitudinal study of a panel (N=454) of respondents in grades 7 through 12 in an Iowa community. The major components of the process specified in the theory are differential association, differential reinforcement, definitions (attitudes), and modeling. The process is one in which the operation of these variables produces abstinence or smoking, but with some reciprocal effects of smoking behavior on the social learning variables. Previous research on various kinds of deviance and substance use has been supportive of the theory. The findings in this study from LISREL models of the overall social learning process and each of the component of association, reinforcement, and definitions are also supportive.",
            "referenceCount": 73,
            "citationCount": 301,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Sociology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1996-04-01",
            "journal": {
                "name": "Journal of Drug Issues",
                "volume": "26"
            },
            "citationStyles": {
                "bibtex": "@Article{Akers1996ALT,\n author = {R. Akers and Gang Lee},\n journal = {Journal of Drug Issues},\n pages = {317 - 343},\n title = {A Longitudinal Test of Social Learning Theory: Adolescent Smoking},\n volume = {26},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:144eef982af70b0631877be42121f56222e52e41",
            "@type": "ScholarlyArticle",
            "paperId": "144eef982af70b0631877be42121f56222e52e41",
            "corpusId": 145591412,
            "url": "https://www.semanticscholar.org/paper/144eef982af70b0631877be42121f56222e52e41",
            "title": "Learning and Behavior",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1966,
            "externalIds": {
                "MAG": "2127479638",
                "DOI": "10.1177/001139216601400302",
                "CorpusId": 145591412
            },
            "abstract": "Chapter 1 History, Background, and Basic Concepts Chapter 2 Innate Behavior Patterns and Habituation Chapter 3 Basic Principles of Classical Conditioning Chapter 4 Theories and Research on Classical Conditioning Chapter 5 Basic Principles of Operant Conditioning Chapter 6 Reinforcement Schedules: Experimental Analyses and Applications Chapter 7 Avoidance and Punishment Chapter 8 Theories and Research on Operant Conditioning Chapter 9 Stimulus Control and Concept Learning Chapter 10 Comparative Cognition Chapter 11 Learning by Observation Chapter 12 Learning Motor Skills Chapter 13 Choice Glossary References Acknowledgments Author Index Subject Index 2. FULL TABLE OF CONTENTS Preface Chapter 1: History, Background, and Basic Concepts The Search for General Principles of Learning The Associationists Aristotle The British Associationists: Simple and Complex Ideas Ebbinghaus,s Experiments on Memory The Effects of Repetition The Effects of Time The Role of Contiguity The Influence of the Associationists and Ebbinghaus Behavioral and Cognitive Approaches to Learning The Use of Animal Subjects The Emphasis on External Events The Physiological Approach: Brain and Behavior The Basic Characteristics of Neurons Physiological Research on Simple Sensations Physiological Research on Feature Detectors Physiological Research on Learning Summary Review Questions Chapter 2: Innate Behavior Patterns and Habituation Characteristics of Goal-Directed Systems Reflexes Tropisms and Orientation Kineses Taxes Sequences of Behavior Fixed Action Patterns Reaction Chains Innate Human Abilities and Predispositions Habituation General Principles of Habituation Physiological Mechanisms of Habituation Habituation in Emotional Responses: The Opponent-Process Theory Summary Review Questions Chapter 3: Basic Principles of Classical Conditioning Pavlov,s Discovery and Its Impact The Standard Paradigm of Classical Conditioning The Variety of Conditioned Responses Pavlov,s Stimulus Substitution Theory S-S or S-R Connections? Basic Conditioning Phenomena Acquisition Extinction Spontaneous Recovery, Disinhibition, and Rapid Reacquisition Conditioned Inhibition Generalization and Discrimination The Importance of Timing in Classical Conditioning CS-US Correlations Higher Order Conditioning Classical Conditioning Outside the Laboratory Classical Conditioning and Emotional Responses Classical Conditioning and the Immune System Applications in Behavior Therapy Summary Review Questions Chapter 4: Theories and Research on Classical Conditioning Theories of Associative Learning The Blocking Effect The Rescorla-Wagner Model Other Theories Summary Types of Associations Associations in First-Order Conditioning Associations in Second-Order Conditioning Associations with Contextual Stimuli CS-CS Associations Occasion Setting Summary Biological Constraints on Classical Conditioning The Contiguity Principle and Taste-Aversion Learning Biological Preparedness in Taste-Aversion Learning Biological Preparedness in Human Learning Biological Constraints and the General-Principle Approach The Form of the Conditioned Response Drug Tolerance and Drug Cravings as Conditioned Responses Conditioned Opponent Theories Physiological Research on Classical Conditioning Summary Review Questions Chapter 5: Basic Principles of Operant Conditioning The Law of Effect Thorndike,s Experiments Guthrie and Horton: Evidence for a Mechanical Strengthening Process Superstitious Behaviors The Procedure of Shaping, or Successive Approximations Shaping Lever Pressing in a Rat Shaping Behaviors in the Classroom Shaping as a Tool in Behavior Modification Making Shaping More Precise: Percentile Schedules Versatility of the Shaping Process The Research of B. F. Skinner The Free Operant The Three-Term Contingency Basic Principles of Operant Conditioning Resurgence Conditioned Reinforcement Response Chains Biological Constraints on Operant Conditioning Instinctive Drift Autoshaping Reconciling Reinforcement Theory and Biological Constraints Summary Review Questions Chapter 6: Reinforcement Schedules: Experimental Analyses and Applications Plotting Moment-to-Moment Behavior: The Cumulative Recorder The Four Simple Reinforcement Schedules Fixed Ratio Variable Ratio Fixed Interval Variable Interval Extinction and the Four Simple Schedules Other Reinforcement Schedules Factors Affecting Performance on Reinforcement Schedules Behavioral Momentum Contingency-Shaped versus Rule-Governed Behaviors Reinforcement History Summary The Experimental Analysis of Reinforcement Schedules Cause of the FR Postreinforcement Pause Comparisons of VR and VI Response Rates Applications of Operant Conditioning Teaching Language to Children with Autism Token Reinforcement Organizational Behavior Management Behavior Therapy for Marital Problems Conclusions Summary Review Questions Chapter 7: Avoidance and Punishment Avoidance A Representative Experiment Two-Factor Theory Evidence Supporting Two-Factor Theory Problems with Two-Factor Theory One-Factor Theory Cognitive Theory Biological Constraints in Avoidance Learning Conclusions about the Theories of Avoidance Flooding as Behavior Therapy Learned Helplessness Punishment Is Punishment the Opposite of Reinforcement? Factors Influencing the Effectiveness of Punishment Disadvantages of Using Punishment Negative Punishment Behavior Decelerators in Behavior Therapy Positive Punishment Negative Punishment: Response Cost and Time-Out Other Techniques for Behavior Deceleration Summary Review Questions Chapter 8: Theories and Research on Operant Conditioning The Role of the Response The Role of the Reinforcer Is Reinforcement Necessary for Operant Conditioning? Expectations about the Reinforcer Can Reinforcement Control Visceral Responses? Biofeedback How Can We Predict What Will Be a Reinforcer? Need Reduction Drive Reduction Trans-situationality Premack,s Principle Response Deprivation Theory The Functional Analysis of Behaviors and Reinforcers Behavioral Economics Optimization: Theory and Research Elasticity and Inelasticity of Demand Behavioral Economics and Drug Abuse Other Applications Summary Review Questions Chapter 9: Stimulus Control and Concept Learning Generalization Gradients Measuring Generalization Gradients What Causes Generalization Gradients? Is Stimulus Control Absolute or Relational? Transposition and Peak Shift Spence,s Theory of Excitatory and Inhibitory Gradients The Intermediate-Size Problem Other Data, and Some Conclusions Behavioral Contrast \"Errorless\" Discrimination Learning Transfer of Learning and Learning Sets Concept Learning The Structure of Natural Categories Animal Studies on Natural Concept Learning Developing Stimulus Equivalence Stimulus Control in Behavior Modification Stimulus Equivalence Training Study Habits and Health Habits Insomnia Summary Review Questions Chapter 10: Comparative Cognition Memory and Rehearsal Short-Term Memory, or Working Memory Rehearsal Long-Term Memory, Retrieval, and Forgetting Time, Number, and Serial Patterns Experiments on an \"Internal Clock\" Counting Serial Pattern Learning Chunking Language and Reasoning Teaching Language to Animals Reasoning by Animals Summary Review Questions Chapter 11: Learning by Observation Theories of Imitation Imitation as an Instinct Imitation as an Operant Response Imitation as a Generalized Operant Response Bandura,s Theory of Imitation Which Theory of Imitation Is Best? Mirror Neurons and Imitation Interactions Between Observational Learning and Operant Conditioning Achievement Motivation Aggression Effects of the Mass Media Television Violence and Aggressive Behavior Video Games and Popular Music What Can Be Learned Through Observation? Phobias Drug Use and Addictions Cognitive Development Moral Standards and Behavior Modeling in Behavior Therapy Facilitation of Low-Probability Behaviors Acquisition of New Behaviors Elimination of Fears and Unwanted Behaviors Video Self-Modeling Conclusions: The Sophisticated Skill of Learning by Observation Summary Review Questions Chapter 12: Learning Motor Skills The Variety of Motor Skills Variables Affecting Motor Learning and Performance Reinforcement and Knowledge of Results Knowledge of Performance Distribution of Practice Observational Learning of Motor Skills Transfer from Previous Training Ironic Errors in Movement Theories of Motor-Skill Learning Adams,s Two-Stage Theory Schmidt,s Schema Theory What is the Best Way to Practice? Learning Movement Sequences The Response Chain Approach Motor Programs Dynamic Pattern Theory Summary Review Questions Chapter 13: Choice The Matching Law Herrnstein,s Experiment Other Experiments on Matching Deviations from Matching Varying the Quality and Amount of Reinforcement An Application to Single Schedules Theories of Choice Behavior Matching Theory and Melioration Theory Optimization Theory Momentary Maximization Theory Other Theories of Choice Self-Control Choices Delay Discounting The Ainslie-Rachlin Theory Animal Studies on Self-Control Factors Affecting Self-Control in Children Techniques for Improving Self-Control Other Choice Situations Risk Taking The Tragedy of the Commons",
            "referenceCount": 0,
            "citationCount": 450,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1966-12-01",
            "journal": {
                "name": "Current Sociology",
                "volume": "14"
            },
            "citationStyles": {
                "bibtex": "@Article{Mazur1966LearningAB,\n author = {J. E. Mazur and A. Odum},\n journal = {Current Sociology},\n pages = {51 - 7},\n title = {Learning and Behavior},\n volume = {14},\n year = {1966}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:087d5ea60b0b15fb9d3396ad321a7f941f88e720",
            "@type": "ScholarlyArticle",
            "paperId": "087d5ea60b0b15fb9d3396ad321a7f941f88e720",
            "corpusId": 710455,
            "url": "https://www.semanticscholar.org/paper/087d5ea60b0b15fb9d3396ad321a7f941f88e720",
            "title": "Transfer of learning by composing solutions of elemental sequential tasks",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "journals/ml/Singh92",
                "DOI": "10.1007/BF00992700",
                "CorpusId": 710455
            },
            "abstract": null,
            "referenceCount": 25,
            "citationCount": 192,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2FBF00992700.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Singh2004TransferOL,\n author = {Satinder Singh},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {323-339},\n title = {Transfer of learning by composing solutions of elemental sequential tasks},\n volume = {8},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d8e77021f21e576ed785d1ae2cbaf869f9f1fea9",
            "@type": "ScholarlyArticle",
            "paperId": "d8e77021f21e576ed785d1ae2cbaf869f9f1fea9",
            "corpusId": 142800323,
            "url": "https://www.semanticscholar.org/paper/d8e77021f21e576ed785d1ae2cbaf869f9f1fea9",
            "title": "Learning and Behavior",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1979,
            "externalIds": {
                "MAG": "1603793087",
                "DOI": "10.4324/9781315665146",
                "CorpusId": 142800323
            },
            "abstract": "1. Introduction: Learning to Change. 2. The Study of Learning and Behavior. 3. Pavlovian Conditioning. 4. Pavlovian Applications. 5. Operant Learning: Reinforcement. 6. Reinforcement: Beyond Habit. 7. Schedules of Reinforcement. 8. Operant Learning: Punishment. 9. Operant Applications. 10. Observational Learning. 11. Generalization, Discrimination, and Stimulus Control. 12. Forgetting. 13. The Limits of Learning.",
            "referenceCount": 0,
            "citationCount": 371,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Chance1979LearningAB,\n author = {P. Chance},\n title = {Learning and Behavior},\n year = {1979}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cd94282256db82fb814480f08f8612045f0725f1",
            "@type": "ScholarlyArticle",
            "paperId": "cd94282256db82fb814480f08f8612045f0725f1",
            "corpusId": 2879680,
            "url": "https://www.semanticscholar.org/paper/cd94282256db82fb814480f08f8612045f0725f1",
            "title": "CHILD: A First Step Towards Continual Learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1997,
            "externalIds": {
                "DBLP": "books/sp/98/Ring98",
                "MAG": "1584431645",
                "DOI": "10.1023/A:1007331723572",
                "CorpusId": 2879680
            },
            "abstract": null,
            "referenceCount": 45,
            "citationCount": 205,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023%2FA%3A1007331723572.pdf",
                "status": "CLOSED"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1997-07-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "28"
            },
            "citationStyles": {
                "bibtex": "@Article{Ring1997CHILDAF,\n author = {Mark B. Ring},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {77-104},\n title = {CHILD: A First Step Towards Continual Learning},\n volume = {28},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:28db4d4bcb2a37774c23d65df6c07966ecf78f2f",
            "@type": "ScholarlyArticle",
            "paperId": "28db4d4bcb2a37774c23d65df6c07966ecf78f2f",
            "corpusId": 59878791,
            "url": "https://www.semanticscholar.org/paper/28db4d4bcb2a37774c23d65df6c07966ecf78f2f",
            "title": "Learning Automata: Theory and Applications",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1994,
            "externalIds": {
                "MAG": "140232297",
                "CorpusId": 59878791
            },
            "abstract": "Preface. Contents. Notations. Introduction. Basic notations and definitions. Introduction. Contolled finite system. Control strategies. Dynamic characteristics of controlled finite systems and their structures. Adaptive strategies and learning automata. Classification of problems of adaptive control of finite systems. Reinforcement schemes for average loss function minimization. Introduction. Adaptive control and static systems and linear programming problem. Reinforcement schemes. Properties of reinforcement schemes Behaviour of learning automata for different Reinforcement Schemes. Introduction Reinforcement scheme of Narendra-Shapiro. Reinforcement scheme of Luce and Varshavskii-Verontsova. Bush-Mosteller reinforcement scheme. Projectional stochastic approximation algorithm. Conclusion. Multilevel systems for Automata. Introduction Hierarchical system .The connection between two level adaptive control and bilinear programming problem. Two-level hierarchical system of learning automata using a projectional stochastic approximation algorithm. Two level hierarchical system with transmission of current information to the lower level. Multilevel hierarchical learning system. Conclusion. Multimodal function optimization using learning automata. Introduction. Optimization using single learning automata. Optimization using a two level hierarchical system of learning automata. Conclusion. Application of Learning Automata. Introduction. Practical aspects. Multilevel learning control of a drying furnace. Hierarchical learning control of absorption column. Learning control of an evaporator. Adaptive choice of cyclic code in communications systems. Optimization of multimodal functions ( without constraints). Optimization in presence of constraints. Application of learning automaton to neural network synthesis. Conclusion. Nomenclature. References. Appendix. Index",
            "referenceCount": 0,
            "citationCount": 257,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Najim1994LearningAT,\n author = {K. Najim and A. Poznyak},\n title = {Learning Automata: Theory and Applications},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6104d0e68c7f67da58b2f84a663df45d82d86b18",
            "@type": "ScholarlyArticle",
            "paperId": "6104d0e68c7f67da58b2f84a663df45d82d86b18",
            "corpusId": 9134331,
            "url": "https://www.semanticscholar.org/paper/6104d0e68c7f67da58b2f84a663df45d82d86b18",
            "title": "Learning to perceive and act by trial and error",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1991,
            "externalIds": {
                "MAG": "2052117683",
                "DOI": "10.1007/BF00058926",
                "CorpusId": 9134331
            },
            "abstract": null,
            "referenceCount": 60,
            "citationCount": 228,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023%2FA%3A1022619109594.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1991-07-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Whitehead1991LearningTP,\n author = {S. Whitehead and D. Ballard},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {45-83},\n title = {Learning to perceive and act by trial and error},\n volume = {7},\n year = {1991}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:561569780e42114e78ddd576ca8fa6a6a6c2c3dd",
            "@type": "ScholarlyArticle",
            "paperId": "561569780e42114e78ddd576ca8fa6a6a6c2c3dd",
            "corpusId": 30697309,
            "url": "https://www.semanticscholar.org/paper/561569780e42114e78ddd576ca8fa6a6a6c2c3dd",
            "title": "Behavioral momentum and the Law of Effect",
            "venue": "Behavioral and Brain Sciences",
            "publicationVenue": {
                "id": "urn:research:f51399af-b5cb-4819-9d81-57ec1d17ebf0",
                "name": "Behavioral and Brain Sciences",
                "alternate_names": [
                    "Behav Brain Sci"
                ],
                "issn": "0140-525X",
                "url": "http://www.bbsonline.org/"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2104374990",
                "DOI": "10.1017/S0140525X00002405",
                "CorpusId": 30697309,
                "PubMed": "11303339"
            },
            "abstract": "In the metaphor of behavioral momentum, the rate of a free operant in the presence of a discriminative stimulus is analogous to the velocity of a moving body, and resistance to change measures an aspect of behavior that is analogous to its inertial mass. An extension of the metaphor suggests that preference measures an analog to the gravitational mass of that body. The independent functions relating resistance to change and preference to the conditions of reinforcement may be construed as convergent measures of a single construct, analogous to physical mass, that represents the effects of a history of exposure to the signaled conditions of reinforcement and that unifies the traditionally separate notions of the strength of learning and the value of incentives. Research guided by the momentum metaphor encompasses the effects of reinforcement on response rate, resistance to change, and preference and has implications for clinical interventions, drug addiction, and self-control. In addition, its principles can be seen as a modern, quantitative version of Thorndike's (1911) Law of Effect, providing a new perspective on some of the challenges to his postulation of strengthening by reinforcement.",
            "referenceCount": 226,
            "citationCount": 449,
            "influentialCitationCount": 50,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2000-02-01",
            "journal": {
                "name": "Behavioral and Brain Sciences",
                "volume": "23"
            },
            "citationStyles": {
                "bibtex": "@Article{Nevin2000BehavioralMA,\n author = {J. Nevin and R. Grace},\n booktitle = {Behavioral and Brain Sciences},\n journal = {Behavioral and Brain Sciences},\n pages = {73 - 90},\n title = {Behavioral momentum and the Law of Effect},\n volume = {23},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:984ff06fd7733bb60e20823071e09c2df2a5f9df",
            "@type": "ScholarlyArticle",
            "paperId": "984ff06fd7733bb60e20823071e09c2df2a5f9df",
            "corpusId": 7046272,
            "url": "https://www.semanticscholar.org/paper/984ff06fd7733bb60e20823071e09c2df2a5f9df",
            "title": "Market power and efficiency in a computational electricity market with discriminatory double-auction pricing",
            "venue": "IEEE Transactions on Evolutionary Computation",
            "publicationVenue": {
                "id": "urn:research:79644985-a91b-42a7-ac72-bb961c283f5e",
                "name": "IEEE Transactions on Evolutionary Computation",
                "alternate_names": [
                    "IEEE Trans Evol Comput"
                ],
                "issn": "1089-778X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=4235"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "1515480810",
                "DBLP": "journals/tec/NicolaisenPT01",
                "DOI": "10.1109/4235.956714",
                "CorpusId": 7046272
            },
            "abstract": "This study reports experimental market power and efficiency outcomes for a computational wholesale electricity market operating in the short run under systematically varied concentration and capacity conditions. The pricing of electricity is determined by means of a clearinghouse double auction with discriminatory midpoint pricing. Buyers and sellers use a modified Roth-Erev individual reinforcement learning algorithm (1995) to determine their price and quantity offers in each auction round. It is shown that high market efficiency is generally attained and that market microstructure is strongly predictive for the relative market power of buyers and sellers, independently of the values set for the reinforcement learning parameters. Results are briefly compared against results from an earlier study in which buyers and sellers instead engage in social mimicry learning via genetic algorithms.",
            "referenceCount": 32,
            "citationCount": 432,
            "influentialCitationCount": 25,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ageconsearch.umn.edu/record/18195/files/er52.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Economics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Economics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-10-01",
            "journal": {
                "name": "IEEE Trans. Evol. Comput.",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Nicolaisen2001MarketPA,\n author = {J. Nicolaisen and V. Petrov and L. Tesfatsion},\n booktitle = {IEEE Transactions on Evolutionary Computation},\n journal = {IEEE Trans. Evol. Comput.},\n pages = {504-523},\n title = {Market power and efficiency in a computational electricity market with discriminatory double-auction pricing},\n volume = {5},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d7f1329926f23356c3ad0de5e989d25e2db3c617",
            "@type": "ScholarlyArticle",
            "paperId": "d7f1329926f23356c3ad0de5e989d25e2db3c617",
            "corpusId": 142661777,
            "url": "https://www.semanticscholar.org/paper/d7f1329926f23356c3ad0de5e989d25e2db3c617",
            "title": "Learning: Behavior and cognition",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1990,
            "externalIds": {
                "MAG": "1567789223",
                "DOI": "10.1037/034374",
                "CorpusId": 142661777
            },
            "abstract": "An introduction to associative learning. Classical conditioning: Foundations of conditioning Principles and applications Theories of conditioning. Instrumental conditioning: Reinforcement Response suppression Applications Theories of reinforcement: the law of effect revisited. Theoretical processes in associative learning: Learning in an evolutionary context What is learned? Associative versus cognitive theories of learning How is it learned? An information processing model Is associative learning simple or complex?",
            "referenceCount": 0,
            "citationCount": 136,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Lieberman1990LearningBA,\n author = {D. A. Lieberman},\n title = {Learning: Behavior and cognition},\n year = {1990}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2980dfe5c99658dc3e508d9d6e1d7f26e6fc8934",
            "@type": "ScholarlyArticle",
            "paperId": "2980dfe5c99658dc3e508d9d6e1d7f26e6fc8934",
            "corpusId": 18060048,
            "url": "https://www.semanticscholar.org/paper/2980dfe5c99658dc3e508d9d6e1d7f26e6fc8934",
            "title": "A possibility for implementing curiosity and boredom in model-building neural controllers",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1991,
            "externalIds": {
                "MAG": "172298727",
                "DOI": "10.7551/mitpress/3115.003.0030",
                "CorpusId": 18060048
            },
            "abstract": "This paper introduces a framework for `curious neural controllers' which employ an adaptive world model for goal directed on-line learning. First an on-line reinforcement learning algorithm for autonomous `animats' is described. The algorithm is based on two fully recurrent `self-supervised' continually running networks which learn in parallel. One of the networks learns to represent a complete model of the environmental dynamics and is called the `model network'. It provides complete `credit assignment paths' into the past for the second network which controls the animats physical actions in a possibly reactive environment. The an-imats goal is to maximize cumulative reinforcement and minimize cumulative `pain'. The algorithm has properties which allow to implement something like the desire to improve the model network's knowledge about the world. This is related to curiosity. It is described how the particular algorithm (as well as similar model-building algorithms) may be augmented by dynamic curiosity and boredom in a natural manner. This may be done by introducing (delayed) reinforcement for actions that increase the model network's knowledge about the world. This in turn requires the model network to model its own ignorance, thus showing a rudimentary form of self-introspective behavior.",
            "referenceCount": 12,
            "citationCount": 518,
            "influentialCitationCount": 28,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://mediatum.ub.tum.de/doc/814958/document.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1991-02-14",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Schmidhuber1991APF,\n author = {J. Schmidhuber},\n pages = {222-227},\n title = {A possibility for implementing curiosity and boredom in model-building neural controllers},\n year = {1991}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cbbcf78c6fc189f1f8d1b76dbf314b1132114cfc",
            "@type": "ScholarlyArticle",
            "paperId": "cbbcf78c6fc189f1f8d1b76dbf314b1132114cfc",
            "corpusId": 15648605,
            "url": "https://www.semanticscholar.org/paper/cbbcf78c6fc189f1f8d1b76dbf314b1132114cfc",
            "title": "Potential-Based Shaping and Q-Value Initialization are Equivalent",
            "venue": "Journal of Artificial Intelligence Research",
            "publicationVenue": {
                "id": "urn:research:aef12dca-60a0-4ca3-819b-cad26d309d4e",
                "name": "Journal of Artificial Intelligence Research",
                "alternate_names": [
                    "JAIR",
                    "J Artif Intell Res",
                    "The Journal of Artificial Intelligence Research"
                ],
                "issn": "1076-9757",
                "url": "http://www.jair.org/"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "journals/corr/abs-1106-5267",
                "ArXiv": "1106.5267",
                "DOI": "10.1613/jair.1190",
                "CorpusId": 15648605
            },
            "abstract": "Shaping has proven to be a powerful but precarious means of improving reinforcement learning performance. Ng, Harada, and Russell (1999) proposed the potential-based shaping algorithm for adding shaping rewards in a way that guarantees the learner will learn optimal behavior. In this note, we prove certain similarities between this shaping algorithm and the initialization step required for several reinforcement learning algorithms. More specifically, we prove that a reinforcement learner with initial Q-values based on the shaping algorithm's potential function make the same updates throughout learning as a learner receiving potential-based shaping rewards. We further prove that under a broad category of policies, the behavior of these two learners are indistinguishable. The comparison provides intuition on the theoretical properties of the shaping algorithm as well as a suggestion for a simpler method for capturing the algorithm's benefit. In addition, the equivalence raises previously unaddressed issues concerning the efficiency of learning with potential-based shaping.",
            "referenceCount": 3,
            "citationCount": 152,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jair.org/index.php/jair/article/download/10338/24713",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-06-26",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1106.5267"
            },
            "citationStyles": {
                "bibtex": "@Article{Wiewiora2011PotentialBasedSA,\n author = {Eric Wiewiora},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {ArXiv},\n title = {Potential-Based Shaping and Q-Value Initialization are Equivalent},\n volume = {abs/1106.5267},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3d64cb65ba029dba56a8577f6399dafc8492100b",
            "@type": "ScholarlyArticle",
            "paperId": "3d64cb65ba029dba56a8577f6399dafc8492100b",
            "corpusId": 21567385,
            "url": "https://www.semanticscholar.org/paper/3d64cb65ba029dba56a8577f6399dafc8492100b",
            "title": "Limbic-Striatal Memory Systems and Drug Addiction",
            "venue": "Neurobiology of Learning and Memory",
            "publicationVenue": {
                "id": "urn:research:029ae4a0-5391-4160-8380-a1555528e9f4",
                "name": "Neurobiology of Learning and Memory",
                "alternate_names": [
                    "Neurobiol Learn Mem"
                ],
                "issn": "1074-7427",
                "url": "https://www.journals.elsevier.com/neurobiology-of-learning-and-memory"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2059208609",
                "DOI": "10.1006/nlme.2002.4103",
                "CorpusId": 21567385,
                "PubMed": "12559840"
            },
            "abstract": "Drug addiction can be understood as a pathological subversion of normal brain learning and memory processes strengthened by the motivational impact of drug-associated stimuli, leading to the establishment of compulsive drug-seeking habits. Such habits evolve through a cascade of complex associative processes with Pavlovian and instrumental components that may depend on the integration and coordination of output from several somewhat independent neural systems of learning and memory, each contributing to behavioral performance. Data are reviewed that help to define the influences of conditioned Pavlovian stimuli on goal-directed behavior via sign-tracking, motivational arousal, and conditioned reinforcement. Such influences are mediated via defined corticolimbic-striatal systems converging on the ventral striatum and driving habit-based learning that may depend on the dorsal striatum. These systems include separate and overlapping influences from the amygdala, hippocampus, and cingulate and medial prefrontal cortex on drug-seeking as well as drug-taking behavior, including the propensity to relapse.",
            "referenceCount": 32,
            "citationCount": 412,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2002-11-01",
            "journal": {
                "name": "Neurobiology of Learning and Memory",
                "volume": "78"
            },
            "citationStyles": {
                "bibtex": "@Article{Robbins2002LimbicStriatalMS,\n author = {T. Robbins and B. Everitt},\n booktitle = {Neurobiology of Learning and Memory},\n journal = {Neurobiology of Learning and Memory},\n pages = {625-636},\n title = {Limbic-Striatal Memory Systems and Drug Addiction},\n volume = {78},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4d473b5625176bcdc1a7709a8e1eeb93586968d6",
            "@type": "ScholarlyArticle",
            "paperId": "4d473b5625176bcdc1a7709a8e1eeb93586968d6",
            "corpusId": 38855442,
            "url": "https://www.semanticscholar.org/paper/4d473b5625176bcdc1a7709a8e1eeb93586968d6",
            "title": "Dynamics of drug dependence. Implications of a conditioning theory for research and treatment.",
            "venue": "Archives of General Psychiatry",
            "publicationVenue": {
                "id": "urn:research:f8b3ce82-148b-49f4-a74d-f737fe75e826",
                "name": "Archives of General Psychiatry",
                "alternate_names": [
                    "Arch Gen Psychiatry"
                ],
                "issn": "0003-990X",
                "url": "http://archpsyc.ama-assn.org/"
            },
            "year": 1973,
            "externalIds": {
                "MAG": "1997255693",
                "DOI": "10.1001/ARCHPSYC.1973.01750350005001",
                "CorpusId": 38855442,
                "PubMed": "4700675"
            },
            "abstract": "Though usually initiated through social reinforcement, self-administration of psychoactive drugs (SAPD) is soon reinforced pharmacologically through suppression (by each successive dose) of a homeostatic need generated by successive central counteradaptive changes (CCCs) that develop unconditionally in response to the initial (receptor site) actions of such drugs and their \"reflex\" consequences (signs of drug effects). Temporal contiguity between pharmacological reinforcement and recurring exteroceptive and interoceptive stimuli (CCs) results in increasing probability of occurrence of both CCCs and SAPD in the presence of the CSs (appetitive conditioning), even long after detoxification (DTX). To prevent relapse, both conditioned CCCs (conditioned abstinence, CA) and conditioned SAPD should be actively extinguished after DTX by elicitation of CA and programmed SAPD under conditions preventing pharmacological reinforcement. Associated psychopathologies and previous state-dependent learning may pose problems in rehabilitation.",
            "referenceCount": 32,
            "citationCount": 611,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1973-05-01",
            "journal": {
                "name": "Archives of general psychiatry",
                "volume": "28 5"
            },
            "citationStyles": {
                "bibtex": "@Article{Wikler1973DynamicsOD,\n author = {A. Wikler},\n booktitle = {Archives of General Psychiatry},\n journal = {Archives of general psychiatry},\n pages = {\n          611-6\n        },\n title = {Dynamics of drug dependence. Implications of a conditioning theory for research and treatment.},\n volume = {28 5},\n year = {1973}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c0720cba764469053b3fea2c3f6adb8127106490",
            "@type": "ScholarlyArticle",
            "paperId": "c0720cba764469053b3fea2c3f6adb8127106490",
            "corpusId": 14505901,
            "url": "https://www.semanticscholar.org/paper/c0720cba764469053b3fea2c3f6adb8127106490",
            "title": "Adaptive linear quadratic control using policy iteration",
            "venue": "American Control Conference",
            "publicationVenue": {
                "id": "urn:research:fe4d09f8-d278-4bfb-b73a-1a6a0e22f6a3",
                "name": "American Control Conference",
                "alternate_names": [
                    "Int Conf Adv Comput Control",
                    "ACC",
                    "Advances in Computing and Communications",
                    "Adv Comput Commun",
                    "Am Control Conf",
                    "Advances in Computer and Communication",
                    "International Conference on Advanced Computer Control"
                ],
                "issn": "2767-2875",
                "url": "http://a2c2.org/conferences/american-control-conferences"
            },
            "year": 1994,
            "externalIds": {
                "MAG": "1616818660",
                "DOI": "10.1109/ACC.1994.735224",
                "CorpusId": 14505901
            },
            "abstract": "In this paper we present the stability and convergence results for dynamic programming-based reinforcement learning applied to linear quadratic regulation (LQR). The specific algorithm we analyze is based on Q-learning and it is proven to converge to an optimal controller provided that the underlying system is controllable and a particular signal vector is persistently excited. This is the first convergence result for DP-based reinforcement learning algorithms for a continuous problem.",
            "referenceCount": 12,
            "citationCount": 368,
            "influentialCitationCount": 31,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference"
            ],
            "publicationDate": "1994-06-01",
            "journal": {
                "name": "Proceedings of 1994 American Control Conference - ACC '94",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Conference{Bradtke1994AdaptiveLQ,\n author = {Steven J. Bradtke and B. Ydstie and A. Barto},\n booktitle = {American Control Conference},\n journal = {Proceedings of 1994 American Control Conference - ACC '94},\n pages = {3475-3479 vol.3},\n title = {Adaptive linear quadratic control using policy iteration},\n volume = {3},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:01f8f26ec31b47aed045ef4d2d907987ad0d690f",
            "@type": "ScholarlyArticle",
            "paperId": "01f8f26ec31b47aed045ef4d2d907987ad0d690f",
            "corpusId": 1676584,
            "url": "https://www.semanticscholar.org/paper/01f8f26ec31b47aed045ef4d2d907987ad0d690f",
            "title": "Dealing with non-stationary environments using context detection",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "conf/icml/SilvaBBE06",
                "MAG": "2115524942",
                "DOI": "10.1145/1143844.1143872",
                "CorpusId": 1676584
            },
            "abstract": "In this paper we introduce RL-CD, a method for solving reinforcement learning problems in non-stationary environments. The method is based on a mechanism for creating, updating and selecting one among several partial models of the environment. The partial models are incrementally built according to the system's capability of making predictions regarding a given sequence of observations. We propose, formalize and show the efficiency of this method both in a simple non-stationary environment and in a noisy scenario. We show that RL-CD performs better than two standard reinforcement learning algorithms and that it has advantages over methods specifically designed to cope with non-stationarity. Finally, we present known limitations of the method and future works.",
            "referenceCount": 9,
            "citationCount": 170,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2006-06-25",
            "journal": {
                "name": "Proceedings of the 23rd international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Silva2006DealingWN,\n author = {Bruno C. da Silva and Eduardo W. Basso and A. Bazzan and P. Engel},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the 23rd international conference on Machine learning},\n title = {Dealing with non-stationary environments using context detection},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:10c048c51263aeb850f5419f0677fa5d756d4f87",
            "@type": "ScholarlyArticle",
            "paperId": "10c048c51263aeb850f5419f0677fa5d756d4f87",
            "corpusId": 14460997,
            "url": "https://www.semanticscholar.org/paper/10c048c51263aeb850f5419f0677fa5d756d4f87",
            "title": "Two Competing Models of How People Learn in Games",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2171731198",
                "DOI": "10.1111/J.1468-0262.2002.00436.X",
                "CorpusId": 14460997
            },
            "abstract": "Reinforcement learning and stochastic fictitious play are apparent rivals as models of humans learning. They embody quite different assumptions about the processing of information and optimisation. This paper compares their properties and finds that they are far more similar than were thought. In particular, the expected motion of stochastic fictitious play and reinforcement learning with experimentation can both be written as a perturbed form of the evolutionary replicator dynamics. Therefore they will in many cases have the same asymptotic behaviour. In particular, they have identical local stability properties at mixed equilibria. The main identifiable difference between two models is speed: stochastic fictitious play gives rise to faster learning.",
            "referenceCount": 41,
            "citationCount": 220,
            "influentialCitationCount": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.pure.ed.ac.uk/ws/files/19360757/two_competing_models.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2002-11-01",
            "journal": {
                "name": "Econometrica",
                "volume": "70"
            },
            "citationStyles": {
                "bibtex": "@Article{Hopkins2002TwoCM,\n author = {E. Hopkins},\n journal = {Econometrica},\n pages = {2141-2166},\n title = {Two Competing Models of How People Learn in Games},\n volume = {70},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:841c5f535174bedab6b8b4c608977d946f65a67b",
            "@type": "ScholarlyArticle",
            "paperId": "841c5f535174bedab6b8b4c608977d946f65a67b",
            "corpusId": 2166565,
            "url": "https://www.semanticscholar.org/paper/841c5f535174bedab6b8b4c608977d946f65a67b",
            "title": "Bayesian sparse sampling for on-line reward optimization",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2071814471",
                "DBLP": "conf/icml/WangLBS05",
                "DOI": "10.1145/1102351.1102472",
                "CorpusId": 2166565
            },
            "abstract": "We present an efficient \"sparse sampling\" technique for approximating Bayes optimal decision making in reinforcement learning, addressing the well known exploration versus exploitation tradeoff. Our approach combines sparse sampling with Bayesian exploration to achieve improved decision making while controlling computational cost. The idea is to grow a sparse lookahead tree, intelligently, by exploiting information in a Bayesian posterior---rather than enumerate action branches (standard sparse sampling) or compensate myopically (value of perfect information). The outcome is a flexible, practical technique for improving action selection in simple reinforcement learning scenarios.",
            "referenceCount": 31,
            "citationCount": 125,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2005-08-07",
            "journal": {
                "name": "Proceedings of the 22nd international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Wang2005BayesianSS,\n author = {Tao Wang and D. Lizotte and Michael Bowling and Dale Schuurmans},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the 22nd international conference on Machine learning},\n title = {Bayesian sparse sampling for on-line reward optimization},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:659b252218e76d52ad280363524ce329fef7e091",
            "@type": "ScholarlyArticle",
            "paperId": "659b252218e76d52ad280363524ce329fef7e091",
            "corpusId": 18719298,
            "url": "https://www.semanticscholar.org/paper/659b252218e76d52ad280363524ce329fef7e091",
            "title": "Editor's Note and Announcement of a New Journal Section",
            "venue": "Journal of Cognitive Neuroscience",
            "publicationVenue": {
                "id": "urn:research:b7f1bc59-118a-410a-afc3-aa5cbfb833ff",
                "name": "Journal of Cognitive Neuroscience",
                "alternate_names": [
                    "J Cogn Neurosci"
                ],
                "issn": "0898-929X",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=0898929x"
            },
            "year": 1999,
            "externalIds": {
                "MAG": "2020622867",
                "DOI": "10.1162/089892999563157",
                "CorpusId": 18719298
            },
            "abstract": "Reinforcement learning, as understood by Sutton and Barto, is a fusion of the trial-and-error \u201claw-of-effect\u201d tradition in psychology, optimal control theory in engineering, the secondary reinforcement tradition in learning, and the use of decaying stimulus traces in, for example, Hull\u2019s (1952) concept of a goal gradient and, more recently, Wagner\u2019s (1981) model of conditioning. This fusion has given researchers in artiacial intelligence a number of ideas for computer algorithms that learn a policy that maximizes the agent\u2019s long term return (amount of reward) from the performance of a task. Although many of the ideas behind reinforcement learning originated in psychological theorizing, in recent years these ideas have been most extensively developed within the artiacial learning community, particularly by the authors of this important summary, their students, and colleagues. The book is intended for use in a one-semester course for students interested in machine learning and artiacial intelligence. It would probably not be suitable for a course intended for psychology and neuroscience students, because it does not present models of experimentally established behavioral or neuroscientiac phenomena, and the problems given to illustrate how reinforcement learning algorithms may be applied are not necessarily problems that animals (even human animals) are notably good at solving (e.g., efacient scheduling problems). However, reinforcement learning, incentive, and utility remain central concepts in contemporary work on the neurobiological basis of learned, goaldirected behavior (Schultz, Dayan, & Montague, 1997; Shizgal, 1997), and this book is the place to look for the latest ideas on how these concepts may be developed into effective models for the direction of action. The preface says that the only mathematical background assumed is familiarity with elementary concepts of probability, such as expectations of random variables. Most students will feel that rather more than that is in fact assumed. Nonetheless, the material is presented in an intuitively understandable form, emphasizing the basic ideas and giving helpful illustrations of their application, rather than elaborating proofs. It can be read with proat by motivated neuroscience and psychology students interested in a more rigorous development of these psychologically important ideas. The biographical and historical sections at the end of each chapter are useful for the perspective they give. The many suggested exercises are challenging, open ended, and thought provoking.",
            "referenceCount": 91,
            "citationCount": 176,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Journal of Cognitive Neuroscience",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Gazzaniga1999EditorsNA,\n author = {M. S. Gazzaniga},\n booktitle = {Journal of Cognitive Neuroscience},\n journal = {Journal of Cognitive Neuroscience},\n pages = {iii-iii},\n title = {Editor's Note and Announcement of a New Journal Section},\n volume = {11},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:54766230de13caafee3509ff495effbee22456f8",
            "@type": "ScholarlyArticle",
            "paperId": "54766230de13caafee3509ff495effbee22456f8",
            "corpusId": 3255627,
            "url": "https://www.semanticscholar.org/paper/54766230de13caafee3509ff495effbee22456f8",
            "title": "Bellman goes relational",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "conf/icml/KerstingOR04",
                "MAG": "1521016937",
                "DOI": "10.1145/1015330.1015401",
                "CorpusId": 3255627
            },
            "abstract": "Motivated by the interest in relational reinforcement learning, we introduce a novel relational Bellman update operator called REBEL. It employs a constraint logic programming language to compactly represent Markov decision processes over relational domains. Using REBEL, a novel value iteration algorithm is developed in which abstraction (over states and actions) plays a major role. This framework provides new insights into relational reinforcement learning. Convergence results as well as experiments are presented.",
            "referenceCount": 17,
            "citationCount": 119,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://lirias.kuleuven.be/bitstream/123456789/134702/1/04-icml.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2004-07-04",
            "journal": {
                "name": "Proceedings of the twenty-first international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kersting2004BellmanGR,\n author = {K. Kersting and M. V. Otterlo and L. D. Raedt},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the twenty-first international conference on Machine learning},\n title = {Bellman goes relational},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e79f19aa33d8f2db2e356cf4eb2e8e389156ea1f",
            "@type": "ScholarlyArticle",
            "paperId": "e79f19aa33d8f2db2e356cf4eb2e8e389156ea1f",
            "corpusId": 143654028,
            "url": "https://www.semanticscholar.org/paper/e79f19aa33d8f2db2e356cf4eb2e8e389156ea1f",
            "title": "The free food (contrafreeloading) phenomenon: A review and analysis",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1977,
            "externalIds": {
                "MAG": "1974745975",
                "DOI": "10.3758/BF03209232",
                "CorpusId": 143654028
            },
            "abstract": null,
            "referenceCount": 154,
            "citationCount": 197,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.3758/BF03209232.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "1977-09-01",
            "journal": {
                "name": "Animal Learning & Behavior",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Osborne1977TheFF,\n author = {S. Osborne},\n journal = {Animal Learning & Behavior},\n pages = {221-235},\n title = {The free food (contrafreeloading) phenomenon: A review and analysis},\n volume = {5},\n year = {1977}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
            "@type": "ScholarlyArticle",
            "paperId": "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
            "corpusId": 160705,
            "url": "https://www.semanticscholar.org/paper/f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
            "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2964059111",
                "DBLP": "conf/icml/GalG16",
                "ArXiv": "1506.02142",
                "CorpusId": 160705
            },
            "abstract": "Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.",
            "referenceCount": 56,
            "citationCount": 6984,
            "influentialCitationCount": 1195,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-06-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Gal2015DropoutAA,\n author = {Y. Gal and Zoubin Ghahramani},\n booktitle = {International Conference on Machine Learning},\n pages = {1050-1059},\n title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c",
            "@type": "ScholarlyArticle",
            "paperId": "2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c",
            "corpusId": 69997935,
            "url": "https://www.semanticscholar.org/paper/2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c",
            "title": "Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2884745705",
                "CorpusId": 69997935
            },
            "abstract": "Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks-scikit-learn and TensorFlow-author Aurelien Geron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started. Explore the machine learning landscape, particularly neural nets Use scikit-learn to track an example machine-learning project end-to-end Explore several training models, including support vector machines, decision trees, random forests, and ensemble methods Use the TensorFlow library to build and train neural nets Dive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learning Learn techniques for training and scaling deep neural nets Apply practical code examples without acquiring excessive machine learning theory or algorithm details",
            "referenceCount": 0,
            "citationCount": 2319,
            "influentialCitationCount": 326,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2017-04-18",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{G\u00e9ron2017HandsOnML,\n author = {Aur\u00e9lien G\u00e9ron},\n title = {Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8dd53f10ca5fa14faeed2bd2951d247f1ac60f40",
            "@type": "ScholarlyArticle",
            "paperId": "8dd53f10ca5fa14faeed2bd2951d247f1ac60f40",
            "corpusId": 115606413,
            "url": "https://www.semanticscholar.org/paper/8dd53f10ca5fa14faeed2bd2951d247f1ac60f40",
            "title": "A State-of-the-Art Survey on Deep Learning Theory and Architectures",
            "venue": "Electronics",
            "publicationVenue": {
                "id": "urn:research:ccd8e532-73c6-414f-bc91-271bbb2933e2",
                "name": "Electronics",
                "alternate_names": null,
                "issn": "1450-5843",
                "url": "http://www.electronics.etfbl.net/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2919358988",
                "DOI": "10.3390/ELECTRONICS8030292",
                "CorpusId": 115606413
            },
            "abstract": "In recent years, deep learning has garnered tremendous success in a variety of application domains. This new field of machine learning has been growing rapidly and has been applied to most traditional application domains, as well as some new areas that present more opportunities. Different methods have been proposed based on different categories of learning, including supervised, semi-supervised, and un-supervised learning. Experimental results show state-of-the-art performance using deep learning when compared to traditional machine learning approaches in the fields of image processing, computer vision, speech recognition, machine translation, art, medical imaging, medical information processing, robotics and control, bioinformatics, natural language processing, cybersecurity, and many others. This survey presents a brief survey on the advances that have occurred in the area of Deep Learning (DL), starting with the Deep Neural Network (DNN). The survey goes on to cover Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). Additionally, we have discussed recent developments, such as advanced variant DL techniques based on these DL approaches. This work considers most of the papers published after 2012 from when the history of deep learning began. Furthermore, DL approaches that have been explored and evaluated in different application domains are also included in this survey. We also included recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys that have been published on DL using neural networks and a survey on Reinforcement Learning (RL). However, those papers have not discussed individual advanced techniques for training large-scale deep learning models and the recently developed method of generative models.",
            "referenceCount": 272,
            "citationCount": 944,
            "influentialCitationCount": 31,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2079-9292/8/3/292/pdf?version=1552274432",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2019-03-05",
            "journal": {
                "name": "Electronics",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Alom2019ASS,\n author = {Md. Zahangir Alom and T. Taha and C. Yakopcic and Stefan Westberg and P. Sidike and M. S. Nasrin and Mahmudul Hasan and B. V. Van Essen and A. Awwal and V. Asari},\n booktitle = {Electronics},\n journal = {Electronics},\n title = {A State-of-the-Art Survey on Deep Learning Theory and Architectures},\n year = {2019}\n}\n"
            }
        }
    }
]