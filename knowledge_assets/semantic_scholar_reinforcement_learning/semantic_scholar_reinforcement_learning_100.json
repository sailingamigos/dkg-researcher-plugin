[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ed195eea1a2e40b2f0bf1a47150694f9e610eb5a",
            "@type": "ScholarlyArticle",
            "paperId": "ed195eea1a2e40b2f0bf1a47150694f9e610eb5a",
            "corpusId": 85449748,
            "url": "https://www.semanticscholar.org/paper/ed195eea1a2e40b2f0bf1a47150694f9e610eb5a",
            "title": "Reinforcement Learning and Optimal Control by",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "CorpusId": 85449748
            },
            "abstract": "This is Chapter 4 of the draft textbook \u201cReinforcement Learning and Optimal Control.\u201d The chapter represents \u201cwork in progress,\u201d and it will be periodically updated. It more than likely contains errors (hopefully not serious ones). Furthermore, its references to the literature are incomplete. Your comments and suggestions to the author at dimitrib@mit.edu are welcome. The date of last revision is given below. (A \u201crevision\u201d is any version of the chapter that involves the addition or the deletion of at least one paragraph or mathematically significant equation.)",
            "referenceCount": 183,
            "citationCount": 469,
            "influentialCitationCount": 62,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Bertsekas2018ReinforcementLA,\n author = {D. Bertsekas},\n title = {Reinforcement Learning and Optimal Control by},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:633870d249f03d39849224adc3381712fbb23ed8",
            "@type": "ScholarlyArticle",
            "paperId": "633870d249f03d39849224adc3381712fbb23ed8",
            "corpusId": 3318551,
            "url": "https://www.semanticscholar.org/paper/633870d249f03d39849224adc3381712fbb23ed8",
            "title": "Mean Field Multi-Agent Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1802-05438",
                "ArXiv": "1802.05438",
                "MAG": "2785315072",
                "CorpusId": 3318551
            },
            "abstract": "Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of user interactions. In this paper, we present Mean Field Reinforcement Learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent's optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies. We develop practical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution. Experiments on resource allocation, Ising model estimation, and battle game tasks verify the learning effectiveness of our mean field approaches in handling many-agent interactions in population.",
            "referenceCount": 55,
            "citationCount": 442,
            "influentialCitationCount": 61,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-02-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.05438"
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2018MeanFM,\n author = {Yaodong Yang and Rui Luo and Minne Li and M. Zhou and Weinan Zhang and Jun Wang},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Mean Field Multi-Agent Reinforcement Learning},\n volume = {abs/1802.05438},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:16a931f72e51b8cc4acc2a586ee53fb9b0d8959d",
            "@type": "ScholarlyArticle",
            "paperId": "16a931f72e51b8cc4acc2a586ee53fb9b0d8959d",
            "corpusId": 4570620,
            "url": "https://www.semanticscholar.org/paper/16a931f72e51b8cc4acc2a586ee53fb9b0d8959d",
            "title": "Learning Synergies Between Pushing and Grasping with Self-Supervised Deep Reinforcement Learning",
            "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
            "publicationVenue": {
                "id": "urn:research:37275deb-3fcf-4d16-ae77-95db9899b1f3",
                "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                "alternate_names": [
                    "IROS",
                    "Intelligent Robots and Systems",
                    "Intell Robot Syst",
                    "IEEE/RJS Int Conf Intell Robot Syst"
                ],
                "issn": null,
                "url": "http://www.iros.org/"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1803.09956",
                "MAG": "2950925451",
                "DBLP": "journals/corr/abs-1803-09956",
                "DOI": "10.1109/IROS.2018.8593986",
                "CorpusId": 4570620
            },
            "abstract": "Skilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. In this work, we demonstrate that it is possible to discover and learn these synergies from scratch through model-free deep reinforcement learning. Our method involves training two fully convolutional networks that map from visual observations to actions: one infers the utility of pushes for a dense pixel-wise sampling of end-effector orientations and locations, while the other does the same for grasping. Both networks are trained jointly in a Q-learning framework and are entirely self-supervised by trial and error, where rewards are provided from successful grasps. In this way, our policy learns pushing motions that enable future grasps, while learning grasps that can leverage past pushes. During picking experiments in both simulation and real-world scenarios, we find that our system quickly learns complex behaviors even amid challenging cases of tightly packed clutter, and achieves better grasping success rates and picking efficiencies than baseline alternatives after a few hours of training. We further demonstrate that our method is capable of generalizing to novel objects. Qualitative results (videos), code, pre-trained models, and simulation environments are available at http://vpg.cs.princeton.edu",
            "referenceCount": 42,
            "citationCount": 437,
            "influentialCitationCount": 59,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dspace.mit.edu/bitstream/1721.1/130010/2/1803.09956.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-03-27",
            "journal": {
                "name": "2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zeng2018LearningSB,\n author = {Andy Zeng and Shuran Song and Stefan Welker and Johnny Lee and Alberto Rodriguez and T. Funkhouser},\n booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},\n journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\n pages = {4238-4245},\n title = {Learning Synergies Between Pushing and Grasping with Self-Supervised Deep Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:50df19aff9e4a68fedfc7dad3fca48a060fc9085",
            "@type": "ScholarlyArticle",
            "paperId": "50df19aff9e4a68fedfc7dad3fca48a060fc9085",
            "corpusId": 3579986,
            "url": "https://www.semanticscholar.org/paper/50df19aff9e4a68fedfc7dad3fca48a060fc9085",
            "title": "Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/icml/ZhangYL0B18",
                "MAG": "2963747324",
                "ArXiv": "1802.08757",
                "CorpusId": 3579986
            },
            "abstract": "We consider the problem of \\emph{fully decentralized} multi-agent reinforcement learning (MARL), where the agents are located at the nodes of a time-varying communication network. Specifically, we assume that the reward functions of the agents might correspond to different tasks, and are only known to the corresponding agent. Moreover, each agent makes individual decisions based on both the information observed locally and the messages received from its neighbors over the network. Within this setting, the collective goal of the agents is to maximize the globally averaged return over the network through exchanging information with their neighbors. To this end, we propose two decentralized actor-critic algorithms with function approximation, which are applicable to large-scale MARL problems where both the number of states and the number of agents are massively large. Under the decentralized structure, the actor step is performed individually by each agent with no need to infer the policies of others. For the critic step, we propose a consensus update via communication over the network. Our algorithms are fully incremental and can be implemented in an online fashion. Convergence analyses of the algorithms are provided when the value functions are approximated within the class of linear functions. Extensive simulation results with both linear and nonlinear function approximations are presented to validate the proposed algorithms. Our work appears to be the first study of fully decentralized MARL algorithms for networked agents with function approximation, with provable convergence guarantees.",
            "referenceCount": 99,
            "citationCount": 449,
            "influentialCitationCount": 53,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-02-23",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2018FullyDM,\n author = {K. Zhang and Zhuoran Yang and Han Liu and Tong Zhang and T. Ba\u015far},\n booktitle = {International Conference on Machine Learning},\n pages = {5867-5876},\n title = {Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3aadab924520c58be81781aafd51e6807e9c4576",
            "@type": "ScholarlyArticle",
            "paperId": "3aadab924520c58be81781aafd51e6807e9c4576",
            "corpusId": 49670252,
            "url": "https://www.semanticscholar.org/paper/3aadab924520c58be81781aafd51e6807e9c4576",
            "title": "Visual Reinforcement Learning with Imagined Goals",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1807.04742",
                "DBLP": "journals/corr/abs-1807-04742",
                "MAG": "2964342357",
                "CorpusId": 49670252
            },
            "abstract": "For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised \"practice\" phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.",
            "referenceCount": 50,
            "citationCount": 463,
            "influentialCitationCount": 54,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-07-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nair2018VisualRL,\n author = {Ashvin Nair and Vitchyr H. Pong and Murtaza Dalal and Shikhar Bahl and Steven Lin and S. Levine},\n booktitle = {Neural Information Processing Systems},\n pages = {9209-9220},\n title = {Visual Reinforcement Learning with Imagined Goals},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:653d7a9b40160e9ab2feb036736fd853c003d6bc",
            "@type": "ScholarlyArticle",
            "paperId": "653d7a9b40160e9ab2feb036736fd853c003d6bc",
            "corpusId": 218563317,
            "url": "https://www.semanticscholar.org/paper/653d7a9b40160e9ab2feb036736fd853c003d6bc",
            "title": "Adaptive Fault-Tolerant Tracking Control for Discrete-Time Multiagent Systems via Reinforcement Learning Algorithm",
            "venue": "IEEE Transactions on Cybernetics",
            "publicationVenue": {
                "id": "urn:research:404813e7-95da-4137-be14-2ba73d2df4fd",
                "name": "IEEE Transactions on Cybernetics",
                "alternate_names": [
                    "IEEE Trans Cybern"
                ],
                "issn": "2168-2267",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6221036"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/tcyb/LiWC21a",
                "MAG": "3021399679",
                "DOI": "10.1109/TCYB.2020.2982168",
                "CorpusId": 218563317,
                "PubMed": "32386171"
            },
            "abstract": "This article investigates the adaptive fault-tolerant tracking control problem for a class of discrete-time multiagent systems via a reinforcement learning algorithm. The action neural networks (NNs) are used to approximate unknown and desired control input signals, and the critic NNs are employed to estimate the cost function in the design procedure. Furthermore, the direct adaptive optimal controllers are designed by combining the backstepping technique with the reinforcement learning algorithm. Comparing the existing reinforcement learning algorithm, the computational burden can be effectively reduced by using the method of less learning parameters. The adaptive auxiliary signals are established to compensate for the influence of the dead zones and actuator faults on the control performance. Based on the Lyapunov stability theory, it is proved that all signals of the closed-loop system are semiglobally uniformly ultimately bounded. Finally, some simulation results are presented to illustrate the effectiveness of the proposed approach.",
            "referenceCount": 43,
            "citationCount": 222,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-05-04",
            "journal": {
                "name": "IEEE Transactions on Cybernetics",
                "volume": "51"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2020AdaptiveFT,\n author = {Hongyi Li and Ying Wu and Mou Chen},\n booktitle = {IEEE Transactions on Cybernetics},\n journal = {IEEE Transactions on Cybernetics},\n pages = {1163-1174},\n title = {Adaptive Fault-Tolerant Tracking Control for Discrete-Time Multiagent Systems via Reinforcement Learning Algorithm},\n volume = {51},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fa54b47df8641dff1579b5e8e0f18f057de68e73",
            "@type": "ScholarlyArticle",
            "paperId": "fa54b47df8641dff1579b5e8e0f18f057de68e73",
            "corpusId": 4900548,
            "url": "https://www.semanticscholar.org/paper/fa54b47df8641dff1579b5e8e0f18f057de68e73",
            "title": "DRN: A Deep Reinforcement Learning Framework for News Recommendation",
            "venue": "The Web Conference",
            "publicationVenue": {
                "id": "urn:research:e07422f9-c065-40c3-a37b-75e98dce79fe",
                "name": "The Web Conference",
                "alternate_names": [
                    "Web Conf",
                    "WWW"
                ],
                "issn": null,
                "url": "http://www.iw3c2.org/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/www/ZhengZZXY0L18",
                "MAG": "2787933113",
                "DOI": "10.1145/3178876.3185994",
                "CorpusId": 4900548
            },
            "abstract": "In this paper, we propose a novel Deep Reinforcement Learning framework for news recommendation. Online personalized news recommendation is a highly challenging problem due to the dynamic nature of news features and user preferences. Although some online recommendation models have been proposed to address the dynamic nature of news recommendation, these methods have three major issues. First, they only try to model current reward (e.g., Click Through Rate). Second, very few studies consider to use user feedback other than click / no click labels (e.g., how frequent user returns) to help improve recommendation. Third, these methods tend to keep recommending similar news to users, which may cause users to get bored. Therefore, to address the aforementioned challenges, we propose a Deep Q-Learning based recommendation framework, which can model future reward explicitly. We further consider user return pattern as a supplement to click / no click label in order to capture more user feedback information. In addition, an effective exploration strategy is incorporated to find new attractive news for users. Extensive experiments are conducted on the offline dataset and online production environment of a commercial news recommendation application and have shown the superior performance of our methods.",
            "referenceCount": 53,
            "citationCount": 546,
            "influentialCitationCount": 46,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://dl.acm.org/ft_gateway.cfm?id=3185994&type=pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-04-10",
            "journal": {
                "name": "Proceedings of the 2018 World Wide Web Conference",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Zheng2018DRNAD,\n author = {Guanjie Zheng and Fuzheng Zhang and Zihan Zheng and Yang Xiang and Nicholas Jing Yuan and Xing Xie and Z. Li},\n booktitle = {The Web Conference},\n journal = {Proceedings of the 2018 World Wide Web Conference},\n title = {DRN: A Deep Reinforcement Learning Framework for News Recommendation},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:324effa5a7c737257b675f85bd93d777fc486878",
            "@type": "ScholarlyArticle",
            "paperId": "324effa5a7c737257b675f85bd93d777fc486878",
            "corpusId": 220424803,
            "url": "https://www.semanticscholar.org/paper/324effa5a7c737257b675f85bd93d777fc486878",
            "title": "SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3041764008",
                "ArXiv": "2007.04938",
                "DBLP": "journals/corr/abs-2007-04938",
                "CorpusId": 220424803
            },
            "abstract": "Model-free deep reinforcement learning (RL) has been successful in a range of challenging domains. However, there are some remaining issues, such as stabilizing the optimization of nonlinear function approximators, preventing error propagation due to the Bellman backup in Q-learning, and efficient exploration. To mitigate these issues, we present SUNRISE, a simple unified ensemble method, which is compatible with various off-policy RL algorithms. SUNRISE integrates three key ingredients: (a) bootstrap with random initialization which improves the stability of the learning process by training a diverse ensemble of agents, (b) weighted Bellman backups, which prevent error propagation in Q-learning by reweighing sample transitions based on uncertainty estimates from the ensembles, and (c) an inference method that selects actions using highest upper-confidence bounds for efficient exploration. Our experiments show that SUNRISE significantly improves the performance of existing off-policy RL algorithms, such as Soft Actor-Critic and Rainbow DQN, for both continuous and discrete control tasks on both low-dimensional and high-dimensional environments. Our training code is available at this https URL.",
            "referenceCount": 61,
            "citationCount": 149,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-07-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2007.04938"
            },
            "citationStyles": {
                "bibtex": "@Article{Lee2020SUNRISEAS,\n author = {Kimin Lee and M. Laskin and A. Srinivas and P. Abbeel},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning},\n volume = {abs/2007.04938},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7fb9ebfd30789db24c27790048db668334ed7060",
            "@type": "ScholarlyArticle",
            "paperId": "7fb9ebfd30789db24c27790048db668334ed7060",
            "corpusId": 227260253,
            "url": "https://www.semanticscholar.org/paper/7fb9ebfd30789db24c27790048db668334ed7060",
            "title": "Autonomous navigation of stratospheric balloons using reinforcement learning",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/nature/BellemareCCGMMP20",
                "MAG": "3107153805",
                "DOI": "10.1038/s41586-020-2939-8",
                "CorpusId": 227260253,
                "PubMed": "33268863"
            },
            "abstract": null,
            "referenceCount": 54,
            "citationCount": 209,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-12-01",
            "journal": {
                "name": "Nature",
                "volume": "588"
            },
            "citationStyles": {
                "bibtex": "@Article{Bellemare2020AutonomousNO,\n author = {Marc G. Bellemare and S. Candido and P. S. Castro and Jun Gong and Marlos C. Machado and Subhodeep Moitra and Sameera S. Ponda and Ziyun Wang},\n booktitle = {Nature},\n journal = {Nature},\n pages = {77 - 82},\n title = {Autonomous navigation of stratospheric balloons using reinforcement learning},\n volume = {588},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0bd07bcdede3fecef852556168471b0098223e9f",
            "@type": "ScholarlyArticle",
            "paperId": "0bd07bcdede3fecef852556168471b0098223e9f",
            "corpusId": 211069002,
            "url": "https://www.semanticscholar.org/paper/0bd07bcdede3fecef852556168471b0098223e9f",
            "title": "Provable Self-Play Algorithms for Competitive Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/icml/BaiJ20",
                "MAG": "3005199613",
                "ArXiv": "2002.04017",
                "CorpusId": 211069002
            },
            "abstract": "Self-play, where the algorithm learns by playing against itself without requiring any direct supervision, has become the new weapon in modern Reinforcement Learning (RL) for achieving superhuman performance in practice. However, the majority of exisiting theory in reinforcement learning only applies to the setting where the agent plays against a fixed environment; it remains largely open whether self-play algorithms can be provably effective, especially when it is necessary to manage the exploration/exploitation tradeoff. We study self-play in competitive reinforcement learning under the setting of Markov games, a generalization of Markov decision processes to the two-player case. We introduce a self-play algorithm---Value Iteration with Upper/Lower Confidence Bound (VI-ULCB)---and show that it achieves regret $\\tilde{\\mathcal{O}}(\\sqrt{T})$ after playing $T$ steps of the game, where the regret is measured by the agent's performance against a \\emph{fully adversarial} opponent who can exploit the agent's strategy at \\emph{any} step. We also introduce an explore-then-exploit style algorithm, which achieves a slightly worse regret of $\\tilde{\\mathcal{O}}(T^{2/3})$, but is guaranteed to run in polynomial time even in the worst case. To the best of our knowledge, our work presents the first line of provably sample-efficient self-play algorithms for competitive reinforcement learning.",
            "referenceCount": 55,
            "citationCount": 124,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-02-10",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bai2020ProvableSA,\n author = {Yu Bai and Chi Jin},\n booktitle = {International Conference on Machine Learning},\n pages = {551-560},\n title = {Provable Self-Play Algorithms for Competitive Reinforcement Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:82959dbab0aebb739967368f679970697cceb9a3",
            "@type": "ScholarlyArticle",
            "paperId": "82959dbab0aebb739967368f679970697cceb9a3",
            "corpusId": 58007060,
            "url": "https://www.semanticscholar.org/paper/82959dbab0aebb739967368f679970697cceb9a3",
            "title": "Multi-Agent Deep Reinforcement Learning for Dynamic Power Allocation in Wireless Networks",
            "venue": "IEEE Journal on Selected Areas in Communications",
            "publicationVenue": {
                "id": "urn:research:68f20e73-515e-4c73-9cd5-5684926b45f7",
                "name": "IEEE Journal on Selected Areas in Communications",
                "alternate_names": [
                    "IEEE J Sel Area Commun"
                ],
                "issn": "0733-8716",
                "url": "http://www.comsoc.org/jsac/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "3102812201",
                "DBLP": "journals/jsac/NasirG19",
                "ArXiv": "1808.00490",
                "DOI": "10.1109/JSAC.2019.2933973",
                "CorpusId": 58007060
            },
            "abstract": "This work demonstrates the potential of deep reinforcement learning techniques for transmit power control in wireless networks. Existing techniques typically find near-optimal power allocations by solving a challenging optimization problem. Most of these algorithms are not scalable to large networks in real-world scenarios because of their computational complexity and instantaneous cross-cell channel state information (CSI) requirement. In this paper, a distributively executed dynamic power allocation scheme is developed based on model-free deep reinforcement learning. Each transmitter collects CSI and quality of service (QoS) information from several neighbors and adapts its own transmit power accordingly. The objective is to maximize a weighted sum-rate utility function, which can be particularized to achieve maximum sum-rate or proportionally fair scheduling. Both random variations and delays in the CSI are inherently addressed using deep ${Q}$ -learning. For a typical network architecture, the proposed algorithm is shown to achieve near-optimal power allocation in real time based on delayed CSI measurements available to the agents. The proposed scheme is especially suitable for practical scenarios where the system model is inaccurate and CSI delay is non-negligible.",
            "referenceCount": 53,
            "citationCount": 341,
            "influentialCitationCount": 44,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-08-01",
            "journal": {
                "name": "IEEE Journal on Selected Areas in Communications",
                "volume": "37"
            },
            "citationStyles": {
                "bibtex": "@Article{Nasir2018MultiAgentDR,\n author = {Yasar Sinan Nasir and Dongning Guo},\n booktitle = {IEEE Journal on Selected Areas in Communications},\n journal = {IEEE Journal on Selected Areas in Communications},\n pages = {2239-2250},\n title = {Multi-Agent Deep Reinforcement Learning for Dynamic Power Allocation in Wireless Networks},\n volume = {37},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c9c47277b67e32e4cfd3c8fb1861029d0259eb62",
            "@type": "ScholarlyArticle",
            "paperId": "c9c47277b67e32e4cfd3c8fb1861029d0259eb62",
            "corpusId": 49316519,
            "url": "https://www.semanticscholar.org/paper/c9c47277b67e32e4cfd3c8fb1861029d0259eb62",
            "title": "IntelliLight: A Reinforcement Learning Approach for Intelligent Traffic Light Control",
            "venue": "Knowledge Discovery and Data Mining",
            "publicationVenue": {
                "id": "urn:research:a0edb93b-1e95-4128-a295-6b1659149cef",
                "name": "Knowledge Discovery and Data Mining",
                "alternate_names": [
                    "KDD",
                    "Knowl Discov Data Min"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigkdd/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2809148419",
                "DBLP": "conf/kdd/WeiZYL18",
                "DOI": "10.1145/3219819.3220096",
                "CorpusId": 49316519
            },
            "abstract": "The intelligent traffic light control is critical for an efficient transportation system. While existing traffic lights are mostly operated by hand-crafted rules, an intelligent traffic light control system should be dynamically adjusted to real-time traffic. There is an emerging trend of using deep reinforcement learning technique for traffic light control and recent studies have shown promising results. However, existing studies have not yet tested the methods on the real-world traffic data and they only focus on studying the rewards without interpreting the policies. In this paper, we propose a more effective deep reinforcement learning model for traffic light control. We test our method on a large-scale real traffic dataset obtained from surveillance cameras. We also show some interesting case studies of policies learned from the real data.",
            "referenceCount": 26,
            "citationCount": 432,
            "influentialCitationCount": 40,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3219819.3220096",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-07-19",
            "journal": {
                "name": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Wei2018IntelliLightAR,\n author = {Hua Wei and Guanjie Zheng and Huaxiu Yao and Z. Li},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},\n title = {IntelliLight: A Reinforcement Learning Approach for Intelligent Traffic Light Control},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c1f4ef741242d629d1f56e442a09a7ba29595a0e",
            "@type": "ScholarlyArticle",
            "paperId": "c1f4ef741242d629d1f56e442a09a7ba29595a0e",
            "corpusId": 966543,
            "url": "https://www.semanticscholar.org/paper/c1f4ef741242d629d1f56e442a09a7ba29595a0e",
            "title": "A Distributional Perspective on Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2951328804",
                "ArXiv": "1707.06887",
                "DBLP": "conf/icml/BellemareDM17",
                "CorpusId": 966543
            },
            "abstract": "In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.",
            "referenceCount": 49,
            "citationCount": 1156,
            "influentialCitationCount": 251,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-07-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bellemare2017ADP,\n author = {Marc G. Bellemare and Will Dabney and R. Munos},\n booktitle = {International Conference on Machine Learning},\n pages = {449-458},\n title = {A Distributional Perspective on Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f802802b3af5a22b79ac65d033ba3cbee33da91b",
            "@type": "ScholarlyArticle",
            "paperId": "f802802b3af5a22b79ac65d033ba3cbee33da91b",
            "corpusId": 47021339,
            "url": "https://www.semanticscholar.org/paper/f802802b3af5a22b79ac65d033ba3cbee33da91b",
            "title": "Randomized Prior Functions for Deep Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/nips/OsbandAC18",
                "MAG": "2962723954",
                "ArXiv": "1806.03335",
                "CorpusId": 47021339
            },
            "abstract": "Dealing with uncertainty is essential for efficient reinforcement learning. There is a growing literature on uncertainty estimation for deep learning from fixed datasets, but many of the most popular approaches are poorly-suited to sequential decision problems. Other methods, such as bootstrap sampling, have no mechanism for uncertainty that does not come from the observed data. We highlight why this can be a crucial shortcoming and propose a simple remedy through addition of a randomized untrainable `prior' network to each ensemble member. We prove that this approach is efficient with linear representations, provide simple illustrations of its efficacy with nonlinear representations and show that this approach scales to large-scale problems far better than previous attempts.",
            "referenceCount": 76,
            "citationCount": 309,
            "influentialCitationCount": 50,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.03335"
            },
            "citationStyles": {
                "bibtex": "@Article{Osband2018RandomizedPF,\n author = {Ian Osband and J. Aslanides and Albin Cassirer},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Randomized Prior Functions for Deep Reinforcement Learning},\n volume = {abs/1806.03335},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e0121c1d2dc5c8cc77f4b1570e28f2443ece2a8f",
            "@type": "ScholarlyArticle",
            "paperId": "e0121c1d2dc5c8cc77f4b1570e28f2443ece2a8f",
            "corpusId": 60440666,
            "url": "https://www.semanticscholar.org/paper/e0121c1d2dc5c8cc77f4b1570e28f2443ece2a8f",
            "title": "Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/icml/JaquesLHGOSLF19",
                "MAG": "2952370515",
                "CorpusId": 60440666
            },
            "abstract": "We propose a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents' actions. Causal influence is assessed using counterfactual reasoning. At each timestep, an agent simulates alternate actions that it could have taken, and computes their effect on the behavior of other agents. Actions that lead to bigger changes in other agents' behavior are considered influential and are rewarded. We show that this is equivalent to rewarding agents for having high mutual information between their actions. Empirical results demonstrate that influence leads to enhanced coordination and communication in challenging social dilemma environments, dramatically increasing the learning curves of the deep RL agents, and leading to more meaningful learned communication protocols. The influence rewards for all agents can be computed in a decentralized way by enabling agents to learn a model of other agents using deep neural networks. In contrast, key previous works on emergent communication in the MARL setting were unable to learn diverse policies in a decentralized manner and had to resort to centralized training. Consequently, the influence reward opens up a window of new opportunities for research in this area.",
            "referenceCount": 48,
            "citationCount": 332,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-10-19",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Jaques2018SocialIA,\n author = {Natasha Jaques and Angeliki Lazaridou and Edward Hughes and \u00c7aglar G\u00fcl\u00e7ehre and Pedro A. Ortega and D. Strouse and Joel Z. Leibo and Nando de Freitas},\n booktitle = {International Conference on Machine Learning},\n pages = {3040-3049},\n title = {Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2fc328f3702d6f8730235b1b3ddf7cc5fc096c0d",
            "@type": "ScholarlyArticle",
            "paperId": "2fc328f3702d6f8730235b1b3ddf7cc5fc096c0d",
            "corpusId": 119111734,
            "url": "https://www.semanticscholar.org/paper/2fc328f3702d6f8730235b1b3ddf7cc5fc096c0d",
            "title": "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1904-06387",
                "ArXiv": "1904.06387",
                "MAG": "2962937519",
                "CorpusId": 119111734
            },
            "abstract": "A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.",
            "referenceCount": 60,
            "citationCount": 246,
            "influentialCitationCount": 31,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-04-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Brown2019ExtrapolatingBS,\n author = {Daniel S. Brown and Wonjoon Goo and P. Nagarajan and S. Niekum},\n booktitle = {International Conference on Machine Learning},\n pages = {783-792},\n title = {Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:59a916cdc943f0282908e6f3fa0360f4c5fb78d0",
            "@type": "ScholarlyArticle",
            "paperId": "59a916cdc943f0282908e6f3fa0360f4c5fb78d0",
            "corpusId": 204578308,
            "url": "https://www.semanticscholar.org/paper/59a916cdc943f0282908e6f3fa0360f4c5fb78d0",
            "title": "Stabilizing Transformers for Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/icml/ParisottoSRPGJJ20",
                "ArXiv": "1910.06764",
                "MAG": "2980433389",
                "CorpusId": 204578308
            },
            "abstract": "Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP), achieving state-of-the-art results in domains such as language modeling and machine translation. Harnessing the transformer's ability to process long time horizons of information could provide a similar performance boost in partially observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL, trained using the same losses, has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical. GTrXL offers an easy-to-train, simple-to-implement but substantially more expressive architectural alternative to the standard multi-layer LSTM ubiquitously used for RL agents in partially observable environments.",
            "referenceCount": 62,
            "citationCount": 251,
            "influentialCitationCount": 28,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-10-13",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Parisotto2019StabilizingTF,\n author = {Emilio Parisotto and H. F. Song and Jack W. Rae and Razvan Pascanu and \u00c7aglar G\u00fcl\u00e7ehre and Siddhant M. Jayakumar and Max Jaderberg and Raphael Lopez Kaufman and Aidan Clark and Seb Noury and M. Botvinick and N. Heess and R. Hadsell},\n booktitle = {International Conference on Machine Learning},\n pages = {7487-7498},\n title = {Stabilizing Transformers for Reinforcement Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6cd5dfccd9f52538b19a415e00031d0ee4e5b181",
            "@type": "ScholarlyArticle",
            "paperId": "6cd5dfccd9f52538b19a415e00031d0ee4e5b181",
            "corpusId": 1740355,
            "url": "https://www.semanticscholar.org/paper/6cd5dfccd9f52538b19a415e00031d0ee4e5b181",
            "title": "Designing Neural Network Architectures using Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2951886768",
                "ArXiv": "1611.02167",
                "DBLP": "journals/corr/BakerGNR16",
                "CorpusId": 1740355
            },
            "abstract": "At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using $Q$-learning with an $\\epsilon$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.",
            "referenceCount": 44,
            "citationCount": 1328,
            "influentialCitationCount": 134,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.02167"
            },
            "citationStyles": {
                "bibtex": "@Article{Baker2016DesigningNN,\n author = {Bowen Baker and O. Gupta and Nikhil Naik and R. Raskar},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Designing Neural Network Architectures using Reinforcement Learning},\n volume = {abs/1611.02167},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5646b7e555fc7768db1e3e9a792b59a6553b1d7e",
            "@type": "ScholarlyArticle",
            "paperId": "5646b7e555fc7768db1e3e9a792b59a6553b1d7e",
            "corpusId": 212633747,
            "url": "https://www.semanticscholar.org/paper/5646b7e555fc7768db1e3e9a792b59a6553b1d7e",
            "title": "Reinforcement Learning for Combinatorial Optimization: A Survey",
            "venue": "Computers & Operations Research",
            "publicationVenue": {
                "id": "urn:research:81508ffd-4573-400c-ba72-a7fbc89a2d02",
                "name": "Computers & Operations Research",
                "alternate_names": [
                    "Comput  Oper Res"
                ],
                "issn": "0305-0548",
                "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/300/description#description"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3170112077",
                "DBLP": "journals/cor/MazyavkinaSIB21",
                "ArXiv": "2003.03600",
                "DOI": "10.1016/j.cor.2021.105400",
                "CorpusId": 212633747
            },
            "abstract": null,
            "referenceCount": 157,
            "citationCount": 315,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2003.03600",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-03-07",
            "journal": {
                "name": "Comput. Oper. Res.",
                "volume": "134"
            },
            "citationStyles": {
                "bibtex": "@Article{Mazyavkina2020ReinforcementLF,\n author = {Nina Mazyavkina and S. Sviridov and Sergei Ivanov and Evgeny Burnaev},\n booktitle = {Computers & Operations Research},\n journal = {Comput. Oper. Res.},\n pages = {105400},\n title = {Reinforcement Learning for Combinatorial Optimization: A Survey},\n volume = {134},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cbd569036fc72ae7ff747350b91816440282596b",
            "@type": "ScholarlyArticle",
            "paperId": "cbd569036fc72ae7ff747350b91816440282596b",
            "corpusId": 25156106,
            "url": "https://www.semanticscholar.org/paper/cbd569036fc72ae7ff747350b91816440282596b",
            "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2751448157",
                "DBLP": "journals/corr/abs-1709-00103",
                "ArXiv": "1709.00103",
                "CorpusId": 25156106
            },
            "abstract": "Relational databases store a significant amount of the worlds data. However, accessing this data currently requires users to understand a query language such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model uses rewards from in the loop query execution over the database to learn a policy to generate the query, which contains unordered parts that are less suitable for optimization via cross entropy loss. Moreover, Seq2SQL leverages the structure of SQL to prune the space of generated queries and significantly simplify the generation problem. In addition to the model, we release WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables fromWikipedia that is an order of magnitude larger than comparable datasets. By applying policy based reinforcement learning with a query execution environment to WikiSQL, Seq2SQL outperforms a state-of-the-art semantic parser, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.",
            "referenceCount": 44,
            "citationCount": 827,
            "influentialCitationCount": 230,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-08-31",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1709.00103"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhong2017Seq2SQLGS,\n author = {Victor Zhong and Caiming Xiong and R. Socher},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning},\n volume = {abs/1709.00103},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:896e5529de1da1e4494033404721b70339bb9557",
            "@type": "ScholarlyArticle",
            "paperId": "896e5529de1da1e4494033404721b70339bb9557",
            "corpusId": 140269588,
            "url": "https://www.semanticscholar.org/paper/896e5529de1da1e4494033404721b70339bb9557",
            "title": "Challenges of Real-World Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1904.12901",
                "DBLP": "journals/corr/abs-1904-12901",
                "MAG": "2942608247",
                "CorpusId": 140269588
            },
            "abstract": "Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. We present a set of nine unique challenges that must be addressed to productionize RL to real world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real world problems. We also present an example domain that has been modified to present these challenges as a testbed for practical RL research.",
            "referenceCount": 75,
            "citationCount": 446,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-04-29",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1904.12901"
            },
            "citationStyles": {
                "bibtex": "@Article{Dulac-Arnold2019ChallengesOR,\n author = {Gabriel Dulac-Arnold and D. Mankowitz and Todd Hester},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Challenges of Real-World Reinforcement Learning},\n volume = {abs/1904.12901},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1298dae5751fb06184f6b067d1503bde8037bdb7",
            "@type": "ScholarlyArticle",
            "paperId": "1298dae5751fb06184f6b067d1503bde8037bdb7",
            "corpusId": 3147007,
            "url": "https://www.semanticscholar.org/paper/1298dae5751fb06184f6b067d1503bde8037bdb7",
            "title": "Deep Reinforcement Learning for Dialogue Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2016,
            "externalIds": {
                "ACL": "D16-1127",
                "ArXiv": "1606.01541",
                "DBLP": "conf/emnlp/LiMRJGG16",
                "MAG": "2951559297",
                "DOI": "10.18653/v1/D16-1127",
                "CorpusId": 3147007
            },
            "abstract": "Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.",
            "referenceCount": 56,
            "citationCount": 1235,
            "influentialCitationCount": 166,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D16-1127.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1606.01541"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2016DeepRL,\n author = {Jiwei Li and Will Monroe and Alan Ritter and Dan Jurafsky and Michel Galley and Jianfeng Gao},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Deep Reinforcement Learning for Dialogue Generation},\n volume = {abs/1606.01541},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9bfefdf18016e146f2afb38f1807dffabb84a7c6",
            "@type": "ScholarlyArticle",
            "paperId": "9bfefdf18016e146f2afb38f1807dffabb84a7c6",
            "corpusId": 52245597,
            "url": "https://www.semanticscholar.org/paper/9bfefdf18016e146f2afb38f1807dffabb84a7c6",
            "title": "Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/cvpr/TangTLL018",
                "MAG": "2798644314",
                "DOI": "10.1109/CVPR.2018.00558",
                "CorpusId": 52245597
            },
            "abstract": "In this paper, we propose a deep progressive reinforcement learning (DPRL) method for action recognition in skeleton-based videos, which aims to distil the most informative frames and discard ambiguous frames in sequences for recognizing actions. Since the choices of selecting representative frames are multitudinous for each video, we model the frame selection as a progressive process through deep reinforcement learning, during which we progressively adjust the chosen frames by taking two important factors into account: (1) the quality of the selected frames and (2) the relationship between the selected frames to the whole video. Moreover, considering the topology of human body inherently lies in a graph-based structure, where the vertices and edges represent the hinged joints and rigid bones respectively, we employ the graph-based convolutional neural network to capture the dependency between the joints for action recognition. Our approach achieves very competitive performance on three widely used benchmarks.",
            "referenceCount": 50,
            "citationCount": 327,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-06-01",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tang2018DeepPR,\n author = {Yansong Tang and Yi Tian and Jiwen Lu and Peiyang Li and Jie Zhou},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {5323-5332},\n title = {Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:222baa4e9e7ce691fdfddbc826a70e027daed70d",
            "@type": "ScholarlyArticle",
            "paperId": "222baa4e9e7ce691fdfddbc826a70e027daed70d",
            "corpusId": 201645209,
            "url": "https://www.semanticscholar.org/paper/222baa4e9e7ce691fdfddbc826a70e027daed70d",
            "title": "Reinforcement Learning in Healthcare: A Survey",
            "venue": "ACM Computing Surveys",
            "publicationVenue": {
                "id": "urn:research:7b2adce0-d53f-49d6-8784-b0645604fe62",
                "name": "ACM Computing Surveys",
                "alternate_names": [
                    "ACM Comput Surv"
                ],
                "issn": "0360-0300",
                "url": "http://www.acm.org/pubs/surveys/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1908-08796",
                "ArXiv": "1908.08796",
                "MAG": "2969281871",
                "DOI": "10.1145/3477600",
                "CorpusId": 201645209
            },
            "abstract": "As a subfield of machine learning, reinforcement learning (RL) aims at optimizing decision making by using interaction samples of an agent with its environment and the potentially delayed feedbacks. In contrast to traditional supervised learning that typically relies on one-shot, exhaustive, and supervised reward signals, RL tackles sequential decision-making problems with sampled, evaluative, and delayed feedbacks simultaneously. Such a distinctive feature makes RL techniques a suitable candidate for developing powerful solutions in various healthcare domains, where diagnosing decisions or treatment regimes are usually characterized by a prolonged period with delayed feedbacks. By first briefly examining theoretical foundations and key methods in RL research, this survey provides an extensive overview of RL applications in a variety of healthcare domains, ranging from dynamic treatment regimes in chronic diseases and critical care, automated medical diagnosis, and many other control or scheduling problems that have infiltrated every aspect of the healthcare system. In addition, we discuss the challenges and open issues in the current research and highlight some potential solutions and directions for future research.",
            "referenceCount": 372,
            "citationCount": 309,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1908.08796",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-08-22",
            "journal": {
                "name": "ACM Computing Surveys (CSUR)",
                "volume": "55"
            },
            "citationStyles": {
                "bibtex": "@Article{Yu2019ReinforcementLI,\n author = {Chao Yu and Jiming Liu and S. Nemati},\n booktitle = {ACM Computing Surveys},\n journal = {ACM Computing Surveys (CSUR)},\n pages = {1 - 36},\n title = {Reinforcement Learning in Healthcare: A Survey},\n volume = {55},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8c54e8575e7c17a4097838305915e6e7b00fd4af",
            "@type": "ScholarlyArticle",
            "paperId": "8c54e8575e7c17a4097838305915e6e7b00fd4af",
            "corpusId": 204955905,
            "url": "https://www.semanticscholar.org/paper/8c54e8575e7c17a4097838305915e6e7b00fd4af",
            "title": "Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1910-11956",
                "MAG": "3028830971",
                "ArXiv": "1910.11956",
                "CorpusId": 204955905
            },
            "abstract": "We present relay policy learning, a method for imitation and reinforcement learning that can solve multi-stage, long-horizon robotic tasks. This general and universally-applicable, two-phase approach consists of an imitation learning stage that produces goal-conditioned hierarchical policies, and a reinforcement learning phase that finetunes these policies for task performance. Our method, while not necessarily perfect at imitation learning, is very amenable to further improvement via environment interaction, allowing it to scale to challenging long-horizon tasks. We simplify the long-horizon policy learning problem by using a novel data-relabeling algorithm for learning goal-conditioned hierarchical policies, where the low-level only acts for a fixed number of steps, regardless of the goal achieved. While we rely on demonstration data to bootstrap policy learning, we do not assume access to demonstrations of every specific tasks that is being solved, and instead leverage unstructured and unsegmented demonstrations of semantically meaningful behaviors that are not only less burdensome to provide, but also can greatly facilitate further improvement using reinforcement learning. We demonstrate the effectiveness of our method on a number of multi-stage, long-horizon manipulation tasks in a challenging kitchen simulation environment. Videos are available at this https URL",
            "referenceCount": 37,
            "citationCount": 242,
            "influentialCitationCount": 28,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-10-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1910.11956"
            },
            "citationStyles": {
                "bibtex": "@Article{Gupta2019RelayPL,\n author = {Abhishek Gupta and Vikash Kumar and Corey Lynch and S. Levine and Karol Hausman},\n booktitle = {Conference on Robot Learning},\n journal = {ArXiv},\n title = {Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning},\n volume = {abs/1910.11956},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8ef87e938b53c7f3ffdf47dfc317aa9b82848535",
            "@type": "ScholarlyArticle",
            "paperId": "8ef87e938b53c7f3ffdf47dfc317aa9b82848535",
            "corpusId": 148567317,
            "url": "https://www.semanticscholar.org/paper/8ef87e938b53c7f3ffdf47dfc317aa9b82848535",
            "title": "Reinforcement Learning: Theory and Algorithms",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "CorpusId": 148567317
            },
            "abstract": null,
            "referenceCount": 139,
            "citationCount": 172,
            "influentialCitationCount": 47,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Agarwal2019ReinforcementLT,\n author = {Alekh Agarwal and Nan Jiang and S. Kakade},\n title = {Reinforcement Learning: Theory and Algorithms},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:65fb1b37c41902793ac65db3532a6e51631a9aff",
            "@type": "ScholarlyArticle",
            "paperId": "65fb1b37c41902793ac65db3532a6e51631a9aff",
            "corpusId": 29160737,
            "url": "https://www.semanticscholar.org/paper/65fb1b37c41902793ac65db3532a6e51631a9aff",
            "title": "A Lyapunov-based Approach to Safe Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1805-07708",
                "MAG": "2950851680",
                "ArXiv": "1805.07708",
                "CorpusId": 29160737
            },
            "abstract": "In many real-world reinforcement learning (RL) problems, besides optimizing the main objective function, an agent must concurrently avoid violating a number of constraints. In particular, besides optimizing performance it is crucial to guarantee the safety of an agent during training as well as deployment (e.g. a robot should avoid taking actions - exploratory or not - which irrevocably harm its hardware). To incorporate safety in RL, we derive algorithms under the framework of constrained Markov decision problems (CMDPs), an extension of the standard Markov decision problems (MDPs) augmented with constraints on expected cumulative costs. Our approach hinges on a novel \\emph{Lyapunov} method. We define and present a method for constructing Lyapunov functions, which provide an effective way to guarantee the global safety of a behavior policy during training via a set of local, linear constraints. Leveraging these theoretical underpinnings, we show how to use the Lyapunov approach to systematically transform dynamic programming (DP) and RL algorithms into their safe counterparts. To illustrate their effectiveness, we evaluate these algorithms in several CMDP planning and decision-making tasks on a safety benchmark domain. Our results show that our proposed method significantly outperforms existing baselines in balancing constraint satisfaction and performance.",
            "referenceCount": 44,
            "citationCount": 384,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-05-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chow2018ALA,\n author = {Yinlam Chow and Ofir Nachum and Edgar A. Du\u00e9\u00f1ez-Guzm\u00e1n and M. Ghavamzadeh},\n booktitle = {Neural Information Processing Systems},\n pages = {8103-8112},\n title = {A Lyapunov-based Approach to Safe Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3f43f08611cbcfba62bb9e0c5339c2a8f0cc3e4b",
            "@type": "ScholarlyArticle",
            "paperId": "3f43f08611cbcfba62bb9e0c5339c2a8f0cc3e4b",
            "corpusId": 202540003,
            "url": "https://www.semanticscholar.org/paper/3f43f08611cbcfba62bb9e0c5339c2a8f0cc3e4b",
            "title": "A survey and critique of multiagent deep reinforcement learning",
            "venue": "Autonomous Agents and Multi-Agent Systems",
            "publicationVenue": {
                "id": "urn:research:86b22730-8744-40a9-ae4d-d21830dfb282",
                "name": "Autonomous Agents and Multi-Agent Systems",
                "alternate_names": [
                    "Auton Agent Multi-agent Syst"
                ],
                "issn": "1387-2532",
                "url": "https://www.springer.com/computer/ai/journal/10458"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1810-05587",
                "MAG": "2981038142",
                "ArXiv": "1810.05587",
                "DOI": "10.1007/s10458-019-09421-1",
                "CorpusId": 202540003
            },
            "abstract": null,
            "referenceCount": 393,
            "citationCount": 420,
            "influentialCitationCount": 26,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1810.05587",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-10-12",
            "journal": {
                "name": "Autonomous Agents and Multi-Agent Systems",
                "volume": "33"
            },
            "citationStyles": {
                "bibtex": "@Article{Hernandez-Leal2018ASA,\n author = {Pablo Hernandez-Leal and Bilal Kartal and Matthew E. Taylor},\n booktitle = {Autonomous Agents and Multi-Agent Systems},\n journal = {Autonomous Agents and Multi-Agent Systems},\n pages = {750 - 797},\n title = {A survey and critique of multiagent deep reinforcement learning},\n volume = {33},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d7878c2044fb699e0ce0cad83e411824b1499dc8",
            "@type": "ScholarlyArticle",
            "paperId": "d7878c2044fb699e0ce0cad83e411824b1499dc8",
            "corpusId": 3649804,
            "url": "https://www.semanticscholar.org/paper/d7878c2044fb699e0ce0cad83e411824b1499dc8",
            "title": "Neural Combinatorial Optimization with Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1611.09940",
                "DBLP": "journals/corr/BelloPLNB16",
                "MAG": "2952332632",
                "CorpusId": 3649804
            },
            "abstract": "This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent network using a policy gradient method. We compare learning the network parameters on a set of training graphs against learning them on individual test graphs. Despite the computational expense, without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied to the KnapSack, another NP-hard problem, the same method obtains optimal solutions for instances with up to 200 items.",
            "referenceCount": 46,
            "citationCount": 1093,
            "influentialCitationCount": 200,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-29",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.09940"
            },
            "citationStyles": {
                "bibtex": "@Article{Bello2016NeuralCO,\n author = {Irwan Bello and Hieu Pham and Quoc V. Le and Mohammad Norouzi and Samy Bengio},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Neural Combinatorial Optimization with Reinforcement Learning},\n volume = {abs/1611.09940},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9aa3ae820772b5f25df9d498bd60c72e70d1b5e6",
            "@type": "ScholarlyArticle",
            "paperId": "9aa3ae820772b5f25df9d498bd60c72e70d1b5e6",
            "corpusId": 57574477,
            "url": "https://www.semanticscholar.org/paper/9aa3ae820772b5f25df9d498bd60c72e70d1b5e6",
            "title": "Guidelines for reinforcement learning in healthcare",
            "venue": "Nature Network Boston",
            "publicationVenue": {
                "id": "urn:research:9e995b6d-f30b-4ab4-a13b-3dc2cc992f47",
                "name": "Nature Network Boston",
                "alternate_names": [
                    "Nat Netw Boston",
                    "Nat Med",
                    "Nature Medicine"
                ],
                "issn": "1744-7933",
                "url": "https://www.nature.com/nature/articles?code=archive_news"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2904730732",
                "DOI": "10.1038/s41591-018-0310-5",
                "CorpusId": 57574477,
                "PubMed": "30617332"
            },
            "abstract": null,
            "referenceCount": 6,
            "citationCount": 286,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://spiral.imperial.ac.uk/bitstream/10044/1/65646/2/119057_2_merged_1542275441.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-01-01",
            "journal": {
                "name": "Nature Medicine",
                "volume": "25"
            },
            "citationStyles": {
                "bibtex": "@Article{Gottesman2019GuidelinesFR,\n author = {Omer Gottesman and Fredrik D. Johansson and M. Komorowski and Aldo A. Faisal and D. Sontag and F. Doshi-Velez and L. Celi},\n booktitle = {Nature Network Boston},\n journal = {Nature Medicine},\n pages = {16 - 18},\n title = {Guidelines for reinforcement learning in healthcare},\n volume = {25},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4ee70fb32981f84f9dddc57bd59a69e677c91759",
            "@type": "ScholarlyArticle",
            "paperId": "4ee70fb32981f84f9dddc57bd59a69e677c91759",
            "corpusId": 195791801,
            "url": "https://www.semanticscholar.org/paper/4ee70fb32981f84f9dddc57bd59a69e677c91759",
            "title": "Benchmarking Model-Based Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2953708620",
                "ArXiv": "1907.02057",
                "DBLP": "journals/corr/abs-1907-02057",
                "CorpusId": 195791801
            },
            "abstract": "Model-based reinforcement learning (MBRL) is widely seen as having the potential to be significantly more sample efficient than model-free RL. However, research in model-based RL has not been very standardized. It is fairly common for authors to experiment with self-designed environments, and there are several separate lines of research, which are sometimes closed-sourced or not reproducible. Accordingly, it is an open question how these various existing MBRL algorithms perform relative to each other. To facilitate research in MBRL, in this paper we gather a wide collection of MBRL algorithms and propose over 18 benchmarking environments specially designed for MBRL. We benchmark these algorithms with unified problem settings, including noisy environments. Beyond cataloguing performance, we explore and unify the underlying algorithmic differences across MBRL algorithms. We characterize three key research challenges for future MBRL research: the dynamics bottleneck, the planning horizon dilemma, and the early-termination dilemma. Finally, to maximally facilitate future research on MBRL, we open-source our benchmark in this http URL.",
            "referenceCount": 59,
            "citationCount": 274,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-07-03",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1907.02057"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2019BenchmarkingMR,\n author = {Tingwu Wang and Xuchan Bao and I. Clavera and Jerrick Hoang and Yeming Wen and Eric D. Langlois and Matthew Shunshi Zhang and Guodong Zhang and P. Abbeel and Jimmy Ba},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Benchmarking Model-Based Reinforcement Learning},\n volume = {abs/1907.02057},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b3dae9529f3caeeec9cc6872e94aa690418acb22",
            "@type": "ScholarlyArticle",
            "paperId": "b3dae9529f3caeeec9cc6872e94aa690418acb22",
            "corpusId": 19100390,
            "url": "https://www.semanticscholar.org/paper/b3dae9529f3caeeec9cc6872e94aa690418acb22",
            "title": "Reinforcement Learning for Relation Classification From Noisy Data",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1808-08013",
                "ArXiv": "1808.08013",
                "MAG": "2776652360",
                "DOI": "10.1609/aaai.v32i1.12063",
                "CorpusId": 19100390
            },
            "abstract": "\n \n Existing relation classification methods that rely on distant supervision assume that a bag of sentences mentioning an entity pair are all describing a relation for the entity pair. Such methods, performing classification at the bag level, cannot identify the mapping between a relation and a sentence, and largely suffers from the noisy labeling problem. In this paper, we propose a novel model for relation classification at the sentence level from noisy data. The model has two modules: an instance selector and a relation classifier. The instance selector chooses high-quality sentences with reinforcement learning and feeds the selected sentences into the relation classifier, and the relation classifier makes sentence-level prediction and provides rewards to the instance selector. The two modules are trained jointly to optimize the instance selection and relation classification processes.Experiment results show that our model can deal with the noise of data effectively and obtains better performance for relation classification at the sentence level.\n \n",
            "referenceCount": 32,
            "citationCount": 311,
            "influentialCitationCount": 36,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/12063/11922",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-04-26",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Feng2018ReinforcementLF,\n author = {Jun Feng and Minlie Huang and Li Zhao and Yang Yang and Xiaoyan Zhu},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {5779-5786},\n title = {Reinforcement Learning for Relation Classification From Noisy Data},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cfb68baa23048e3e0f8845c099fa013797bd623f",
            "@type": "ScholarlyArticle",
            "paperId": "cfb68baa23048e3e0f8845c099fa013797bd623f",
            "corpusId": 166228224,
            "url": "https://www.semanticscholar.org/paper/cfb68baa23048e3e0f8845c099fa013797bd623f",
            "title": "Explainable Reinforcement Learning Through a Causal Lens",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2945022240",
                "DBLP": "journals/corr/abs-1905-10958",
                "ArXiv": "1905.10958",
                "DOI": "10.1609/AAAI.V34I03.5631",
                "CorpusId": 166228224
            },
            "abstract": "Prominent theories in cognitive science propose that humans understand and represent the knowledge of the world through causal relationships. In making sense of the world, we build causal models in our mind to encode cause-effect relations of events and use these to explain why new events happen by referring to counterfactuals \u2014 things that did not happen. In this paper, we use causal models to derive causal explanations of the behaviour of model-free reinforcement learning agents. We present an approach that learns a structural causal model during reinforcement learning and encodes causal relationships between variables of interest. This model is then used to generate explanations of behaviour based on counterfactual analysis of the causal model. We computationally evaluate the model in 6 domains and measure performance and task prediction accuracy. We report on a study with 120 participants who observe agents playing a real-time strategy game (Starcraft II) and then receive explanations of the agents' behaviour. We investigate: 1) participants' understanding gained by explanations through task prediction; 2) explanation satisfaction and 3) trust. Our results show that causal model explanations perform better on these measures compared to two other baseline explanation models.",
            "referenceCount": 41,
            "citationCount": 261,
            "influentialCitationCount": 22,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/5631/5487",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-05-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Madumal2019ExplainableRL,\n author = {Prashan Madumal and Tim Miller and L. Sonenberg and F. Vetere},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {2493-2500},\n title = {Explainable Reinforcement Learning Through a Causal Lens},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e0889fcee1acd985af76a3907d5d0029bf260be9",
            "@type": "ScholarlyArticle",
            "paperId": "e0889fcee1acd985af76a3907d5d0029bf260be9",
            "corpusId": 186206882,
            "url": "https://www.semanticscholar.org/paper/e0889fcee1acd985af76a3907d5d0029bf260be9",
            "title": "Search on the Replay Buffer: Bridging Planning and Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/nips/EysenbachSL19",
                "MAG": "2950885698",
                "ArXiv": "1906.05253",
                "CorpusId": 186206882
            },
            "abstract": "The history of learning for control has been an exciting back and forth between two broad classes of algorithms: planning and reinforcement learning. Planning algorithms effectively reason over long horizons, but assume access to a local policy and distance metric over collision-free paths. Reinforcement learning excels at learning policies and the relative values of states, but fails to plan over long horizons. Despite the successes of each method in various domains, tasks that require reasoning over long horizons with limited feedback and high-dimensional observations remain exceedingly challenging for both planning and reinforcement learning algorithms. Frustratingly, these sorts of tasks are potentially the most useful, as they are simple to design (a human only need to provide an example goal state) and avoid reward shaping, which can bias the agent towards finding a sub-optimal solution. We introduce a general control algorithm that combines the strengths of planning and reinforcement learning to effectively solve these tasks. Our aim is to decompose the task of reaching a distant goal state into a sequence of easier tasks, each of which corresponds to reaching a subgoal. Planning algorithms can automatically find these waypoints, but only if provided with suitable abstractions of the environment -- namely, a graph consisting of nodes and edges. Our main insight is that this graph can be constructed via reinforcement learning, where a goal-conditioned value function provides edge weights, and nodes are taken to be previously seen observations in a replay buffer. Using graph search over our replay buffer, we can automatically generate this sequence of subgoals, even in image-based environments. Our algorithm, search on the replay buffer (SoRB), enables agents to solve sparse reward tasks over one hundred steps, and generalizes substantially better than standard RL algorithms.",
            "referenceCount": 66,
            "citationCount": 205,
            "influentialCitationCount": 31,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-06-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Eysenbach2019SearchOT,\n author = {Benjamin Eysenbach and R. Salakhutdinov and S. Levine},\n booktitle = {Neural Information Processing Systems},\n pages = {15220-15231},\n title = {Search on the Replay Buffer: Bridging Planning and Reinforcement Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b4e69b0172d69c80f83366c296b6222805360445",
            "@type": "ScholarlyArticle",
            "paperId": "b4e69b0172d69c80f83366c296b6222805360445",
            "corpusId": 202888699,
            "url": "https://www.semanticscholar.org/paper/b4e69b0172d69c80f83366c296b6222805360445",
            "title": "SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2975999915",
                "DBLP": "conf/iclr/ReddyDL20",
                "CorpusId": 202888699
            },
            "abstract": "Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial imitation learning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that involve adversarial training. We propose a simple alternative that still uses RL, but does not require learning a reward function. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. We accomplish this by giving the agent a constant reward of r=+1 for matching the demonstrated action in a demonstrated state, and a constant reward of r=0 for all other behavior. Our method, which we call soft Q imitation learning (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, we show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation. Empirically, we show that SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo.",
            "referenceCount": 37,
            "citationCount": 184,
            "influentialCitationCount": 30,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-05-27",
            "journal": {
                "name": "arXiv: Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Reddy2019SQILIL,\n author = {S. Reddy and A. Dragan and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Learning},\n title = {SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:929bf1a2ff229d34f7907886989c621444c2b8fd",
            "@type": "ScholarlyArticle",
            "paperId": "929bf1a2ff229d34f7907886989c621444c2b8fd",
            "corpusId": 216056491,
            "url": "https://www.semanticscholar.org/paper/929bf1a2ff229d34f7907886989c621444c2b8fd",
            "title": "Chip Placement with Deep Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2004.10746",
                "DBLP": "journals/corr/abs-2004-10746",
                "MAG": "3018195242",
                "CorpusId": 216056491
            },
            "abstract": "In this work, we present a learning-based approach to chip placement, one of the most complex and time-consuming stages of the chip design process. Unlike prior methods, our approach has the ability to learn from past experience and improve over time. In particular, as we train over a greater number of chip blocks, our method becomes better at rapidly generating optimized placements for previously unseen chip blocks. To achieve these results, we pose placement as a Reinforcement Learning (RL) problem and train an agent to place the nodes of a chip netlist onto a chip canvas. To enable our RL policy to generalize to unseen blocks, we ground representation learning in the supervised task of predicting placement quality. By designing a neural architecture that can accurately predict reward across a wide variety of netlists and their placements, we are able to generate rich feature embeddings of the input netlists. We then use this architecture as the encoder of our policy and value networks to enable transfer learning. Our objective is to minimize PPA (power, performance, and area), and we show that, in under 6 hours, our method can generate placements that are superhuman or comparable on modern accelerator netlists, whereas existing baselines require human experts in the loop and take several weeks.",
            "referenceCount": 48,
            "citationCount": 167,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-04-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2004.10746"
            },
            "citationStyles": {
                "bibtex": "@Article{Mirhoseini2020ChipPW,\n author = {Azalia Mirhoseini and Anna Goldie and M. Yazgan and J. Jiang and Ebrahim M. Songhori and Shen Wang and Young-Joon Lee and Eric Johnson and Omkar Pathak and Sungmin Bae and Azade Nazi and Jiwoo Pak and Andy Tong and Kavya Srinivasa and W. Hang and Emre Tuncer and Ananda Babu and Quoc V. Le and J. Laudon and Richard Ho and Roger Carpenter and J. Dean},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Chip Placement with Deep Reinforcement Learning},\n volume = {abs/2004.10746},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e1b77eb03d753da13d1c6ce7cb45dd98e64c7fb6",
            "@type": "ScholarlyArticle",
            "paperId": "e1b77eb03d753da13d1c6ce7cb45dd98e64c7fb6",
            "corpusId": 218613667,
            "url": "https://www.semanticscholar.org/paper/e1b77eb03d753da13d1c6ce7cb45dd98e64c7fb6",
            "title": "Explainable Reinforcement Learning: A Survey",
            "venue": "International Cross-Domain Conference on Machine Learning and Knowledge Extraction",
            "publicationVenue": {
                "id": "urn:research:774cadb7-0e9c-4d06-be58-c9bc77655652",
                "name": "International Cross-Domain Conference on Machine Learning and Knowledge Extraction",
                "alternate_names": [
                    "CD-MAKE",
                    "Int Cross-domain Conf Mach Learn Knowl Extr"
                ],
                "issn": null,
                "url": "https://cd-make.net/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2005.06247",
                "DBLP": "journals/corr/abs-2005-06247",
                "MAG": "3025747022",
                "DOI": "10.1007/978-3-030-57321-8_5",
                "CorpusId": 218613667
            },
            "abstract": null,
            "referenceCount": 75,
            "citationCount": 166,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2005.06247",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-05-13",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2005.06247"
            },
            "citationStyles": {
                "bibtex": "@Article{Puiutta2020ExplainableRL,\n author = {Erika Puiutta and Eric M. S. P. Veith},\n booktitle = {International Cross-Domain Conference on Machine Learning and Knowledge Extraction},\n journal = {ArXiv},\n title = {Explainable Reinforcement Learning: A Survey},\n volume = {abs/2005.06247},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:86257a79958f2bb351970272b1a1673c55227ed6",
            "@type": "ScholarlyArticle",
            "paperId": "86257a79958f2bb351970272b1a1673c55227ed6",
            "corpusId": 239970385,
            "url": "https://www.semanticscholar.org/paper/86257a79958f2bb351970272b1a1673c55227ed6",
            "title": "Deep Reinforcement Learning for Power System Applications: An Overview",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "CorpusId": 239970385
            },
            "abstract": "Due to increasing complexity, uncertainty and data dimensions in power systems, conventional methods often meet bottlenecks when attempting to solve decision and control problems. Therefore, data-driven methods toward solving such problems are being extensively studied. Deep reinforcement learning (DRL) is one of these data-driven methods and is regarded as real artificial intelligence (AI). DRL is a combination of deep learning (DL) and reinforcement learning (RL). This field of research has been applied to solve a wide range of complex sequential decision-making problems, including those in power systems. This paper firstly reviews the basic ideas, models, algorithms and techniques of DRL. Applications in power systems such as energy management, demand response, electricity market, operational control, and others are then considered. In addition, recent advances in DRL including the combination of RL with other classical methods, and the prospect and challenges of applications in power systems are also discussed.",
            "referenceCount": 115,
            "citationCount": 175,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Zhang2020DeepRL,\n author = {Zidong Zhang and Dongxia Zhang and R. Qiu},\n title = {Deep Reinforcement Learning for Power System Applications: An Overview},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7af7f2f539cd3479faae4c66bbef49b0f66202fa",
            "@type": "ScholarlyArticle",
            "paperId": "7af7f2f539cd3479faae4c66bbef49b0f66202fa",
            "corpusId": 2305273,
            "url": "https://www.semanticscholar.org/paper/7af7f2f539cd3479faae4c66bbef49b0f66202fa",
            "title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning",
            "venue": "IEEE International Conference on Robotics and Automation",
            "publicationVenue": {
                "id": "urn:research:3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                "name": "IEEE International Conference on Robotics and Automation",
                "alternate_names": [
                    "International Conference on Robotics and Automation",
                    "Int Conf Robot Autom",
                    "ICRA",
                    "IEEE Int Conf Robot Autom"
                ],
                "issn": "2152-4092",
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/ZhuMKLGFF16",
                "MAG": "2522340145",
                "ArXiv": "1609.05143",
                "DOI": "10.1109/ICRA.2017.7989381",
                "CorpusId": 2305273
            },
            "abstract": "Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new goals, and (2) data inefficiency, i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows better generalization. To address the second issue, we propose the AI2-THOR framework, which provides an environment with high-quality 3D scenes and a physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently. We show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment.",
            "referenceCount": 55,
            "citationCount": 1292,
            "influentialCitationCount": 104,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1609.05143",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-09-16",
            "journal": {
                "name": "2017 IEEE International Conference on Robotics and Automation (ICRA)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhu2016TargetdrivenVN,\n author = {Yuke Zhu and Roozbeh Mottaghi and Eric Kolve and Joseph J. Lim and A. Gupta and Li Fei-Fei and Ali Farhadi},\n booktitle = {IEEE International Conference on Robotics and Automation},\n journal = {2017 IEEE International Conference on Robotics and Automation (ICRA)},\n pages = {3357-3364},\n title = {Target-driven visual navigation in indoor scenes using deep reinforcement learning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b68b8b980db62308864b2a7d33718182c5f8335b",
            "@type": "ScholarlyArticle",
            "paperId": "b68b8b980db62308864b2a7d33718182c5f8335b",
            "corpusId": 225039804,
            "url": "https://www.semanticscholar.org/paper/b68b8b980db62308864b2a7d33718182c5f8335b",
            "title": "Accelerating Reinforcement Learning with Learned Skill Priors",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2010-11944",
                "ArXiv": "2010.11944",
                "MAG": "3094484961",
                "CorpusId": 225039804
            },
            "abstract": "Intelligent agents rely heavily on prior experience when learning a new task, yet most modern reinforcement learning (RL) approaches learn every task from scratch. One approach for leveraging prior knowledge is to transfer skills learned on prior tasks to the new task. However, as the amount of prior experience increases, the number of transferable skills grows too, making it challenging to explore the full set of available skills during downstream learning. Yet, intuitively, not all skills should be explored with equal probability; for example information about the current state can hint which skills are promising to explore. In this work, we propose to implement this intuition by learning a prior over skills. We propose a deep latent variable model that jointly learns an embedding space of skills and the skill prior from offline agent experience. We then extend common maximum-entropy RL approaches to use skill priors to guide downstream learning. We validate our approach, SPiRL (Skill-Prior RL), on complex navigation and robotic manipulation tasks and show that learned skill priors are essential for effective skill transfer from rich datasets. Videos and code are available at this https URL.",
            "referenceCount": 50,
            "citationCount": 147,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-10-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2010.11944"
            },
            "citationStyles": {
                "bibtex": "@Article{Pertsch2020AcceleratingRL,\n author = {Karl Pertsch and Youngwoon Lee and Joseph J. Lim},\n booktitle = {Conference on Robot Learning},\n journal = {ArXiv},\n title = {Accelerating Reinforcement Learning with Learned Skill Priors},\n volume = {abs/2010.11944},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:629d0ce250581471f07083bbab95f23623b00201",
            "@type": "ScholarlyArticle",
            "paperId": "629d0ce250581471f07083bbab95f23623b00201",
            "corpusId": 220404280,
            "url": "https://www.semanticscholar.org/paper/629d0ce250581471f07083bbab95f23623b00201",
            "title": "Responsive Safety in Reinforcement Learning by PID Lagrangian Methods",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/icml/StookeAA20",
                "MAG": "3041244724",
                "ArXiv": "2007.03964",
                "CorpusId": 220404280
            },
            "abstract": "Lagrangian methods are widely used algorithms for constrained optimization problems, but their learning dynamics exhibit oscillations and overshoot which, when applied to safe reinforcement learning, leads to constraint-violating behavior during agent training. We address this shortcoming by proposing a novel Lagrange multiplier update method that utilizes derivatives of the constraint function. We take a controls perspective, wherein the traditional Lagrange multiplier update behaves as \\emph{integral} control; our terms introduce \\emph{proportional} and \\emph{derivative} control, achieving favorable learning dynamics through damping and predictive measures. We apply our PID Lagrangian methods in deep RL, setting a new state of the art in Safety Gym, a safe RL benchmark. Lastly, we introduce a new method to ease controller tuning by providing invariance to the relative numerical scales of reward and cost. Our extensive experiments demonstrate improved performance and hyperparameter robustness, while our algorithms remain nearly as simple to derive and implement as the traditional Lagrangian approach.",
            "referenceCount": 39,
            "citationCount": 154,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-07-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Stooke2020ResponsiveSI,\n author = {Adam Stooke and Joshua Achiam and P. Abbeel},\n booktitle = {International Conference on Machine Learning},\n pages = {9133-9143},\n title = {Responsive Safety in Reinforcement Learning by PID Lagrangian Methods},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e8350467d7d19e1d5f991be23f7b4826a9303a8",
            "@type": "ScholarlyArticle",
            "paperId": "5e8350467d7d19e1d5f991be23f7b4826a9303a8",
            "corpusId": 212747698,
            "url": "https://www.semanticscholar.org/paper/5e8350467d7d19e1d5f991be23f7b4826a9303a8",
            "title": "ROMA: Multi-Agent Reinforcement Learning with Emergent Roles",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2003-08039",
                "MAG": "3034971464",
                "ArXiv": "2003.08039",
                "CorpusId": 212747698
            },
            "abstract": "The role concept provides a useful tool to design and understand complex multi-agent systems, which allows agents with a similar role to share similar behaviors. However, existing role-based methods use prior domain knowledge and predefine role structures and behaviors. In contrast, multi-agent reinforcement learning (MARL) provides flexibility and adaptability, but less efficiency in complex tasks. In this paper, we synergize these two paradigms and propose a role-oriented MARL framework (ROMA). In this framework, roles are emergent, and agents with similar roles tend to share their learning and to be specialized on certain sub-tasks. To this end, we construct a stochastic role embedding space by introducing two novel regularizers and conditioning individual policies on roles. Experiments show that our method can learn specialized, dynamic, and identifiable roles, which help our method push forward the state of the art on the StarCraft II micromanagement benchmark. Demonstrative videos are available at this https URL.",
            "referenceCount": 68,
            "citationCount": 128,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-03-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2003.08039"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2020ROMAMR,\n author = {Tonghan Wang and Heng Dong and V. Lesser and Chongjie Zhang},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {ROMA: Multi-Agent Reinforcement Learning with Emergent Roles},\n volume = {abs/2003.08039},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:10dd4db543a4689aff533427e0f7e83ba68c808f",
            "@type": "ScholarlyArticle",
            "paperId": "10dd4db543a4689aff533427e0f7e83ba68c808f",
            "corpusId": 226965494,
            "url": "https://www.semanticscholar.org/paper/10dd4db543a4689aff533427e0f7e83ba68c808f",
            "title": "SoftGym: Benchmarking Deep Reinforcement Learning for Deformable Object Manipulation",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/corl/LinWOH20",
                "ArXiv": "2011.07215",
                "MAG": "3105162718",
                "CorpusId": 226965494
            },
            "abstract": "Manipulating deformable objects has long been a challenge in robotics due to its high dimensional state representation and complex dynamics. Recent success in deep reinforcement learning provides a promising direction for learning to manipulate deformable objects with data driven methods. However, existing reinforcement learning benchmarks only cover tasks with direct state observability and simple low-dimensional dynamics or with relatively simple image-based environments, such as those with rigid objects. In this paper, we present SoftGym, a set of open-source simulated benchmarks for manipulating deformable objects, with a standard OpenAI Gym API and a Python interface for creating new environments. Our benchmark will enable reproducible research in this important area. Further, we evaluate a variety of algorithms on these tasks and highlight challenges for reinforcement learning algorithms, including dealing with a state representation that has a high intrinsic dimensionality and is partially observable. The experiments and analysis indicate the strengths and limitations of existing methods in the context of deformable object manipulation that can help point the way forward for future methods development. Code and videos of the learned policies can be found on our project website.",
            "referenceCount": 55,
            "citationCount": 128,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-11-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2011.07215"
            },
            "citationStyles": {
                "bibtex": "@Article{Lin2020SoftGymBD,\n author = {Xingyu Lin and Yufei Wang and Jake Olkin and David Held},\n booktitle = {Conference on Robot Learning},\n journal = {ArXiv},\n title = {SoftGym: Benchmarking Deep Reinforcement Learning for Deformable Object Manipulation},\n volume = {abs/2011.07215},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:16e83f3f0f78ceb203746eeb88f1f5aae9ba3092",
            "@type": "ScholarlyArticle",
            "paperId": "16e83f3f0f78ceb203746eeb88f1f5aae9ba3092",
            "corpusId": 225127589,
            "url": "https://www.semanticscholar.org/paper/16e83f3f0f78ceb203746eeb88f1f5aae9ba3092",
            "title": "Deep reinforcement learning: a survey",
            "venue": "Frontiers of Information Technology & Electronic Engineering",
            "publicationVenue": {
                "id": "urn:research:44aabbb9-28ab-46d0-845f-3dd44a78fb59",
                "name": "Frontiers of Information Technology & Electronic Engineering",
                "alternate_names": [
                    "Front Inf Technol  Electron Eng"
                ],
                "issn": "2095-9184",
                "url": "http://erf.sbb.spk-berlin.de/han/caj/eng.oversea.cnki.net/kns55/oldnavi/n_item.aspx?BaseID=JZUS&Flg=local&NaviID=48&NaviLink="
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3093426589",
                "DBLP": "journals/jzusc/WangLZFHLZ20",
                "DOI": "10.1631/FITEE.1900533",
                "CorpusId": 225127589
            },
            "abstract": "Deep reinforcement learning (RL) has become one of the most popular topics in artificial intelligence research. It has been widely used in various fields, such as end-to-end control, robotic control, recommendation systems, and natural language dialogue systems. In this survey, we systematically categorize the deep RL algorithms and applications, and provide a detailed review over existing deep RL algorithms by dividing them into modelbased methods, model-free methods, and advanced RL methods. We thoroughly analyze the advances including exploration, inverse RL, and transfer RL. Finally, we outline the current representative applications, and analyze four open problems for future research.",
            "referenceCount": 117,
            "citationCount": 132,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-10-15",
            "journal": {
                "name": "Frontiers of Information Technology & Electronic Engineering",
                "volume": "21"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2020DeepRL,\n author = {Haonan Wang and Ning Liu and Yiyun Zhang and Dawei Feng and Feng Huang and Dongsheng Li and Yiming Zhang},\n booktitle = {Frontiers of Information Technology & Electronic Engineering},\n journal = {Frontiers of Information Technology & Electronic Engineering},\n pages = {1726 - 1744},\n title = {Deep reinforcement learning: a survey},\n volume = {21},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
            "@type": "ScholarlyArticle",
            "paperId": "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
            "corpusId": 1932843,
            "url": "https://www.semanticscholar.org/paper/65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
            "title": "Reinforcement learning in robotics: A survey",
            "venue": "Int. J. Robotics Res.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "journals/ijrr/KoberBP13",
                "MAG": "1977655452",
                "DOI": "10.1177/0278364913495721",
                "CorpusId": 1932843
            },
            "abstract": "Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.",
            "referenceCount": 241,
            "citationCount": 2741,
            "influentialCitationCount": 90,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://figshare.com/articles/journal_contribution/Reinforcement_Learning_in_Robotics_A_Survey/6560648/1/files/12042932.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2013-08-23",
            "journal": {
                "name": "The International Journal of Robotics Research",
                "volume": "32"
            },
            "citationStyles": {
                "bibtex": "@Article{Kober2013ReinforcementLI,\n author = {J. Kober and J. Bagnell and Jan Peters},\n booktitle = {Int. J. Robotics Res.},\n journal = {The International Journal of Robotics Research},\n pages = {1238 - 1274},\n title = {Reinforcement learning in robotics: A survey},\n volume = {32},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8ede7ddf99986d69562455bc8d69222fc3e27350",
            "@type": "ScholarlyArticle",
            "paperId": "8ede7ddf99986d69562455bc8d69222fc3e27350",
            "corpusId": 59345798,
            "url": "https://www.semanticscholar.org/paper/8ede7ddf99986d69562455bc8d69222fc3e27350",
            "title": "Recurrent Experience Replay in Distributed Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2908064123",
                "DBLP": "conf/iclr/KapturowskiOQMD19",
                "CorpusId": 59345798
            },
            "abstract": "Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. Using a single network architecture and \ufb01xed set of hyper-parameters, the resulting agent, Recurrent Replay Distributed DQN, quadruples the previous state of the art on Atari-57, and matches the state of the art on DMLab-30. It is the \ufb01rst agent to exceed human-level performance in 52 of the 57 Atari games.",
            "referenceCount": 25,
            "citationCount": 394,
            "influentialCitationCount": 62,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-27",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Kapturowski2018RecurrentER,\n author = {Steven Kapturowski and Georg Ostrovski and John Quan and R. Munos and Will Dabney},\n booktitle = {International Conference on Learning Representations},\n title = {Recurrent Experience Replay in Distributed Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:29c2131880f05097c86814630b0a8d52611b0dc8",
            "@type": "ScholarlyArticle",
            "paperId": "29c2131880f05097c86814630b0a8d52611b0dc8",
            "corpusId": 219981344,
            "url": "https://www.semanticscholar.org/paper/29c2131880f05097c86814630b0a8d52611b0dc8",
            "title": "Provably Efficient Reinforcement Learning for Discounted MDPs with Feature Mapping",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2006-13165",
                "ArXiv": "2006.13165",
                "MAG": "3036849408",
                "CorpusId": 219981344
            },
            "abstract": "Modern tasks in reinforcement learning are always with large state and action spaces. To deal with them efficiently, one often uses predefined feature mapping to represents states and actions in a low dimensional space. In this paper, we study reinforcement learning with feature mapping for discounted Markov Decision Processes (MDPs). We propose a novel algorithm which makes use of the feature mapping and obtains a $\\tilde O(d\\sqrt{T}/(1-\\gamma)^2)$ regret, where $d$ is the dimension of the feature space, $T$ is the time horizon and $\\gamma$ is the discount factor of the MDP. To the best of our knowledge, this is the first polynomial regret bound without accessing to a generative model or making strong assumptions such as ergodicity of the MDP. By constructing a special class of MDPs, we also show that for any algorithms, the regret is lower bounded by $\\Omega(d\\sqrt{T}/(1-\\gamma)^{1.5})$. Our upper and lower bound results together suggest that the proposed reinforcement learning algorithm is near-optimal up to a $(1-\\gamma)^{-0.5}$ factor.",
            "referenceCount": 41,
            "citationCount": 117,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-06-23",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhou2020ProvablyER,\n author = {Dongruo Zhou and Jiafan He and Quanquan Gu},\n booktitle = {International Conference on Machine Learning},\n pages = {12793-12802},\n title = {Provably Efficient Reinforcement Learning for Discounted MDPs with Feature Mapping},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2d85f63a193cd88741c8398ceb98c55e1e89387d",
            "@type": "ScholarlyArticle",
            "paperId": "2d85f63a193cd88741c8398ceb98c55e1e89387d",
            "corpusId": 212877887,
            "url": "https://www.semanticscholar.org/paper/2d85f63a193cd88741c8398ceb98c55e1e89387d",
            "title": "The Ingredients of Real-World Robotic Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2004.12570",
                "MAG": "3020712699",
                "DBLP": "journals/corr/abs-2004-12570",
                "CorpusId": 212877887
            },
            "abstract": "The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.",
            "referenceCount": 54,
            "citationCount": 120,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-04-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2004.12570"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhu2020TheIO,\n author = {Henry Zhu and Justin Yu and Abhishek Gupta and Dhruv Shah and Kristian Hartikainen and Avi Singh and Vikash Kumar and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {The Ingredients of Real-World Robotic Reinforcement Learning},\n volume = {abs/2004.12570},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea64dbd29ab90d8944bcc6a05680aeaacc86fc0a",
            "@type": "ScholarlyArticle",
            "paperId": "ea64dbd29ab90d8944bcc6a05680aeaacc86fc0a",
            "corpusId": 49561741,
            "url": "https://www.semanticscholar.org/paper/ea64dbd29ab90d8944bcc6a05680aeaacc86fc0a",
            "title": "Human-level performance in 3D multiplayer games with population-based reinforcement learning",
            "venue": "Science",
            "publicationVenue": {
                "id": "urn:research:f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
                "name": "Science",
                "alternate_names": null,
                "issn": "0193-4511",
                "url": "https://www.jstor.org/journal/science"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2947862328",
                "ArXiv": "1807.01281",
                "DBLP": "journals/corr/abs-1807-01281",
                "DOI": "10.1126/science.aau6249",
                "CorpusId": 49561741,
                "PubMed": "31147514"
            },
            "abstract": "Artificial teamwork Artificially intelligent agents are getting better and better at two-player games, but most real-world endeavors require teamwork. Jaderberg et al. designed a computer program that excels at playing the video game Quake III Arena in Capture the Flag mode, where two multiplayer teams compete in capturing the flags of the opposing team. The agents were trained by playing thousands of games, gradually learning successful strategies not unlike those favored by their human counterparts. Computer agents competed successfully against humans even when their reaction times were slowed to match those of humans. Science, this issue p. 859 Teams of artificial agents compete successfully against humans in the video game Quake III Arena in Capture the Flag mode. Reinforcement learning (RL) has shown great success in increasingly complex single-agent environments and two-player turn-based games. However, the real world contains multiple agents, each learning and acting independently to cooperate and compete with other agents. We used a tournament-style evaluation to demonstrate that an agent can achieve human-level performance in a three-dimensional multiplayer first-person video game, Quake III Arena in Capture the Flag mode, using only pixels and game points scored as input. We used a two-tier optimization process in which a population of independent RL agents are trained concurrently from thousands of parallel matches on randomly generated environments. Each agent learns its own internal reward signal and rich representation of the world. These results indicate the great potential of multiagent reinforcement learning for artificial intelligence research.",
            "referenceCount": 99,
            "citationCount": 575,
            "influentialCitationCount": 23,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://science.sciencemag.org/content/sci/364/6443/859.full.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-07-03",
            "journal": {
                "name": "Science",
                "volume": "364"
            },
            "citationStyles": {
                "bibtex": "@Article{Jaderberg2018HumanlevelPI,\n author = {Max Jaderberg and Wojciech M. Czarnecki and Iain Dunning and Luke Marris and Guy Lever and Antonio Garc\u00eda Casta\u00f1eda and Charlie Beattie and Neil C. Rabinowitz and Ari S. Morcos and Avraham Ruderman and Nicolas Sonnerat and Tim Green and Louise Deason and Joel Z. Leibo and David Silver and D. Hassabis and K. Kavukcuoglu and T. Graepel},\n booktitle = {Science},\n journal = {Science},\n pages = {859 - 865},\n title = {Human-level performance in 3D multiplayer games with population-based reinforcement learning},\n volume = {364},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:28e66d188efbd0bbb64242b611d96769be910c15",
            "@type": "ScholarlyArticle",
            "paperId": "28e66d188efbd0bbb64242b611d96769be910c15",
            "corpusId": 57189444,
            "url": "https://www.semanticscholar.org/paper/28e66d188efbd0bbb64242b611d96769be910c15",
            "title": "Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications",
            "venue": "IEEE Transactions on Cybernetics",
            "publicationVenue": {
                "id": "urn:research:404813e7-95da-4137-be14-2ba73d2df4fd",
                "name": "IEEE Transactions on Cybernetics",
                "alternate_names": [
                    "IEEE Trans Cybern"
                ],
                "issn": "2168-2267",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6221036"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2908261578",
                "ArXiv": "1812.11794",
                "DBLP": "journals/tcyb/NguyenNN20",
                "DOI": "10.1109/TCYB.2020.2977374",
                "CorpusId": 57189444,
                "PubMed": "32203045"
            },
            "abstract": "Reinforcement learning (RL) algorithms have been around for decades and employed to solve various sequential decision-making problems. These algorithms, however, have faced great challenges when dealing with high-dimensional environments. The recent development of deep learning has enabled RL methods to drive optimal policies for sophisticated and capable agents, which can perform efficiently in these challenging environments. This article addresses an important aspect of deep RL related to situations that require multiple agents to communicate and cooperate to solve complex tasks. A survey of different approaches to problems related to multiagent deep RL (MADRL) is presented, including nonstationarity, partial observability, continuous state and action spaces, multiagent training schemes, and multiagent transfer learning. The merits and demerits of the reviewed methods will be analyzed and discussed with their corresponding applications explored. It is envisaged that this review provides insights about various MADRL methods and can lead to the future development of more robust and highly useful multiagent learning methods for solving real-world problems.",
            "referenceCount": 137,
            "citationCount": 487,
            "influentialCitationCount": 22,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1812.11794",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-12-31",
            "journal": {
                "name": "IEEE Transactions on Cybernetics",
                "volume": "50"
            },
            "citationStyles": {
                "bibtex": "@Article{Nguyen2018DeepRL,\n author = {T. Nguyen and Ngoc Duy Nguyen and S. Nahavandi},\n booktitle = {IEEE Transactions on Cybernetics},\n journal = {IEEE Transactions on Cybernetics},\n pages = {3826-3839},\n title = {Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications},\n volume = {50},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:aaf51f96ca1fe18852f586764bc3aa6e852d0cb6",
            "@type": "ScholarlyArticle",
            "paperId": "aaf51f96ca1fe18852f586764bc3aa6e852d0cb6",
            "corpusId": 49407715,
            "url": "https://www.semanticscholar.org/paper/aaf51f96ca1fe18852f586764bc3aa6e852d0cb6",
            "title": "A Tour of Reinforcement Learning: The View from Continuous Control",
            "venue": "Annu. Rev. Control. Robotics Auton. Syst.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2822752092",
                "DBLP": "journals/corr/abs-1806-09460",
                "ArXiv": "1806.09460",
                "DOI": "10.1146/ANNUREV-CONTROL-053018-023825",
                "CorpusId": 49407715
            },
            "abstract": "This article surveys reinforcement learning from the perspective of optimization and control, with a focus on continuous control applications. It reviews the general formulation, terminology, and typical experimental implementations of reinforcement learning as well as competing solution paradigms. In order to compare the relative merits of various techniques, it presents a case study of the linear quadratic regulator (LQR) with unknown dynamics, perhaps the simplest and best-studied problem in optimal control. It also describes how merging techniques from learning theory and control can provide nonasymptotic characterizations of LQR performance and shows that these characterizations tend to match experimental behavior. In turn, when revisiting more complex applications, many of the observed phenomena in LQR persist. In particular, theory and experiment demonstrate the role and importance of models and the cost of generality in reinforcement learning algorithms. The article concludes with a discussion of some of the challenges in designing learning systems that safely and reliably interact with complex and uncertain environments and how tools from reinforcement learning and control might be combined to approach these challenges.",
            "referenceCount": 97,
            "citationCount": 488,
            "influentialCitationCount": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1806.09460",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-06-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.09460"
            },
            "citationStyles": {
                "bibtex": "@Article{Recht2018ATO,\n author = {B. Recht},\n booktitle = {Annu. Rev. Control. Robotics Auton. Syst.},\n journal = {ArXiv},\n title = {A Tour of Reinforcement Learning: The View from Continuous Control},\n volume = {abs/1806.09460},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2ba00083a72558d45e4baecff0425ef6272f7a40",
            "@type": "ScholarlyArticle",
            "paperId": "2ba00083a72558d45e4baecff0425ef6272f7a40",
            "corpusId": 53040150,
            "url": "https://www.semanticscholar.org/paper/2ba00083a72558d45e4baecff0425ef6272f7a40",
            "title": "Optimization of Molecules via Deep Reinforcement Learning",
            "venue": "Scientific Reports",
            "publicationVenue": {
                "id": "urn:research:f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                "name": "Scientific Reports",
                "alternate_names": [
                    "Sci Rep"
                ],
                "issn": "2045-2322",
                "url": "http://www.nature.com/srep/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2962764565",
                "DBLP": "journals/corr/abs-1810-08678",
                "ArXiv": "1810.08678",
                "PubMedCentral": "6656766",
                "DOI": "10.1038/s41598-019-47148-x",
                "CorpusId": 53040150,
                "PubMed": "31341196"
            },
            "abstract": null,
            "referenceCount": 44,
            "citationCount": 394,
            "influentialCitationCount": 25,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41598-019-47148-x.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-10-19",
            "journal": {
                "name": "Scientific Reports",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhou2018OptimizationOM,\n author = {Zhenpeng Zhou and S. Kearnes and Li Li and R. Zare and Patrick F. Riley},\n booktitle = {Scientific Reports},\n journal = {Scientific Reports},\n title = {Optimization of Molecules via Deep Reinforcement Learning},\n volume = {9},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2b429774b5d867ef60f8b464cfa0b4886ffd77f0",
            "@type": "ScholarlyArticle",
            "paperId": "2b429774b5d867ef60f8b464cfa0b4886ffd77f0",
            "corpusId": 29166962,
            "url": "https://www.semanticscholar.org/paper/2b429774b5d867ef60f8b464cfa0b4886ffd77f0",
            "title": "Deep Reinforcement Learning Based Resource Allocation for V2V Communications",
            "venue": "IEEE Transactions on Vehicular Technology",
            "publicationVenue": {
                "id": "urn:research:983b0731-eddf-4f05-9c9b-81059a9f9c51",
                "name": "IEEE Transactions on Vehicular Technology",
                "alternate_names": [
                    "IEEE Trans Veh Technol"
                ],
                "issn": "0018-9545",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=25"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2950675322",
                "DBLP": "journals/tvt/YeLJ19",
                "ArXiv": "1805.07222",
                "DOI": "10.1109/tvt.2019.2897134",
                "CorpusId": 29166962
            },
            "abstract": "In this paper, we develop a novel decentralized resource allocation mechanism for vehicle-to-vehicle (V2V) communications based on deep reinforcement learning, which can be applied to both unicast and broadcast scenarios. According to the decentralized resource allocation mechanism, an autonomous \u201cagent,\u201d a V2V link or a vehicle, makes its decisions to find the optimal sub-band and power level for transmission without requiring or having to wait for global information. Since the proposed method is decentralized, it incurs only limited transmission overhead. From the simulation results, each agent can effectively learn to satisfy the stringent latency constraints on V2V links while minimizing the interference to vehicle-to-infrastructure communications.",
            "referenceCount": 28,
            "citationCount": 428,
            "influentialCitationCount": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/25/8692737/08633948.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-05-16",
            "journal": {
                "name": "IEEE Transactions on Vehicular Technology",
                "volume": "68"
            },
            "citationStyles": {
                "bibtex": "@Article{Ye2018DeepRL,\n author = {Hao Ye and Geoffrey Y. Li and B. Juang},\n booktitle = {IEEE Transactions on Vehicular Technology},\n journal = {IEEE Transactions on Vehicular Technology},\n pages = {3163-3173},\n title = {Deep Reinforcement Learning Based Resource Allocation for V2V Communications},\n volume = {68},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:944bd3b472c8a30163bbfc1b5cbab8545693c3e0",
            "@type": "ScholarlyArticle",
            "paperId": "944bd3b472c8a30163bbfc1b5cbab8545693c3e0",
            "corpusId": 56475856,
            "url": "https://www.semanticscholar.org/paper/944bd3b472c8a30163bbfc1b5cbab8545693c3e0",
            "title": "Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2950586357",
                "ArXiv": "1803.11347",
                "DBLP": "conf/iclr/NagabandiCLFALF19",
                "CorpusId": 56475856
            },
            "abstract": "Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach uses meta-learning to train a dynamics model prior such that, when combined with recent data, this prior can be rapidly adapted to the local context. Our experiments demonstrate online adaptation for continuous control tasks on both simulated and real-world agents. We first show simulated agents adapting their behavior online to novel terrains, crippled body parts, and highly-dynamic environments. We also illustrate the importance of incorporating online adaptation into autonomous agents that operate in the real world by applying our method to a real dynamic legged millirobot. We demonstrate the agent's learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads.",
            "referenceCount": 68,
            "citationCount": 411,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-03-30",
            "journal": {
                "name": "arXiv: Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Nagabandi2018LearningTA,\n author = {Anusha Nagabandi and I. Clavera and Simin Liu and R. Fearing and P. Abbeel and S. Levine and Chelsea Finn},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Learning},\n title = {Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:399dbf5ba3d00f37b8476bef04ad0dee90f85102",
            "@type": "ScholarlyArticle",
            "paperId": "399dbf5ba3d00f37b8476bef04ad0dee90f85102",
            "corpusId": 21709652,
            "url": "https://www.semanticscholar.org/paper/399dbf5ba3d00f37b8476bef04ad0dee90f85102",
            "title": "Optimal and Autonomous Control Using Reinforcement Learning: A Survey",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/tmm/KiumarsiVML18",
                "MAG": "2772589676",
                "DOI": "10.1109/TNNLS.2017.2773458",
                "CorpusId": 21709652,
                "PubMed": "29771662"
            },
            "abstract": "This paper reviews the current state of the art on reinforcement learning (RL)-based feedback control solutions to optimal regulation and tracking of single and multiagent systems. Existing RL solutions to both optimal <inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {H}_{2}$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {H}_\\infty $ </tex-math></inline-formula> control problems, as well as graphical games, will be reviewed. RL methods learn the solution to optimal control and game problems online and using measured data along the system trajectories. We discuss Q-learning and the integral RL algorithm as core algorithms for discrete-time (DT) and continuous-time (CT) systems, respectively. Moreover, we discuss a new direction of off-policy RL for both CT and DT systems. Finally, we review several applications.",
            "referenceCount": 0,
            "citationCount": 502,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-06-01",
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "29"
            },
            "citationStyles": {
                "bibtex": "@Article{Kiumarsi2018OptimalAA,\n author = {Bahare Kiumarsi and K. Vamvoudakis and H. Modares and F. Lewis},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {2042-2062},\n title = {Optimal and Autonomous Control Using Reinforcement Learning: A Survey},\n volume = {29},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2ed619fbc7902155d54f6f21da16ad6c120eac63",
            "@type": "ScholarlyArticle",
            "paperId": "2ed619fbc7902155d54f6f21da16ad6c120eac63",
            "corpusId": 57189150,
            "url": "https://www.semanticscholar.org/paper/2ed619fbc7902155d54f6f21da16ad6c120eac63",
            "title": "Learning to Walk via Deep Reinforcement Learning",
            "venue": "Robotics: Science and Systems",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963339188",
                "DBLP": "journals/corr/abs-1812-11103",
                "ArXiv": "1812.11103",
                "DOI": "10.15607/RSS.2019.XV.011",
                "CorpusId": 57189150
            },
            "abstract": "Deep reinforcement learning (deep RL) holds the promise of automating the acquisition of complex controllers that can map sensory inputs directly to low-level actions. In the domain of robotic locomotion, deep RL could enable learning locomotion skills with minimal engineering and without an explicit model of the robot dynamics. Unfortunately, applying deep RL to real-world robotic tasks is exceptionally difficult, primarily due to poor sample complexity and sensitivity to hyperparameters. While hyperparameters can be easily tuned in simulated domains, tuning may be prohibitively expensive on physical systems, such as legged robots, that can be damaged through extensive trial-and-error learning. In this paper, we propose a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We apply this method to learning walking gaits on a real-world Minitaur robot. Our method can acquire a stable gait from scratch directly in the real world in about two hours, without relying on any model or simulation, and the resulting policy is robust to moderate variations in the environment. We further show that our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. Videos of training and the learned policy can be found on the project website.",
            "referenceCount": 62,
            "citationCount": 354,
            "influentialCitationCount": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.15607/rss.2019.xv.011",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-12-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1812.11103"
            },
            "citationStyles": {
                "bibtex": "@Article{Haarnoja2018LearningTW,\n author = {Tuomas Haarnoja and Aurick Zhou and Sehoon Ha and Jie Tan and G. Tucker and S. Levine},\n booktitle = {Robotics: Science and Systems},\n journal = {ArXiv},\n title = {Learning to Walk via Deep Reinforcement Learning},\n volume = {abs/1812.11103},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bb905c6b8d9abe84adad1c6784e6037dc9516e95",
            "@type": "ScholarlyArticle",
            "paperId": "bb905c6b8d9abe84adad1c6784e6037dc9516e95",
            "corpusId": 59316728,
            "url": "https://www.semanticscholar.org/paper/bb905c6b8d9abe84adad1c6784e6037dc9516e95",
            "title": "Multi-Agent Reinforcement Learning",
            "venue": "Deep Reinforcement Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3040652553",
                "DOI": "10.1007/978-981-15-4095-0_11",
                "CorpusId": 59316728
            },
            "abstract": null,
            "referenceCount": 84,
            "citationCount": 103,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Deep Reinforcement Learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wen2020MultiAgentRL,\n author = {Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\n booktitle = {Deep Reinforcement Learning},\n journal = {Deep Reinforcement Learning},\n title = {Multi-Agent Reinforcement Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cff6566e92e71c8fcc5cfa5d16eef34e95b1a1f3",
            "@type": "ScholarlyArticle",
            "paperId": "cff6566e92e71c8fcc5cfa5d16eef34e95b1a1f3",
            "corpusId": 226964355,
            "url": "https://www.semanticscholar.org/paper/cff6566e92e71c8fcc5cfa5d16eef34e95b1a1f3",
            "title": "PLAS: Latent Action Space for Offline Reinforcement Learning",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3101483449",
                "ArXiv": "2011.07213",
                "DBLP": "journals/corr/abs-2011-07213",
                "CorpusId": 226964355
            },
            "abstract": "The goal of offline reinforcement learning is to learn a policy from a fixed dataset, without further interactions with the environment. This setting will be an increasingly more important paradigm for real-world applications of reinforcement learning such as robotics, in which data collection is slow and potentially dangerous. Existing off-policy algorithms have limited performance on static datasets due to extrapolation errors from out-of-distribution actions. This leads to the challenge of constraining the policy to select actions within the support of the dataset during training. We propose to simply learn the Policy in the Latent Action Space (PLAS) such that this requirement is naturally satisfied. We evaluate our method on continuous control benchmarks in simulation and a deformable object manipulation task with a physical robot. We demonstrate that our method provides competitive performance consistently across various continuous control tasks and different types of datasets, outperforming existing offline reinforcement learning methods with explicit constraints. Videos and code are available at this https URL.",
            "referenceCount": 26,
            "citationCount": 95,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-11-14",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhou2020PLASLA,\n author = {Wenxuan Zhou and Sujay Bajracharya and David Held},\n booktitle = {Conference on Robot Learning},\n pages = {1719-1735},\n title = {PLAS: Latent Action Space for Offline Reinforcement Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f5275f5eb6569ddb5ba9a959ede09875d56e3bac",
            "@type": "ScholarlyArticle",
            "paperId": "f5275f5eb6569ddb5ba9a959ede09875d56e3bac",
            "corpusId": 227054483,
            "url": "https://www.semanticscholar.org/paper/f5275f5eb6569ddb5ba9a959ede09875d56e3bac",
            "title": "Parrot: Data-Driven Behavioral Priors for Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/iclr/SinghLZYRL21",
                "ArXiv": "2011.10024",
                "MAG": "3103763075",
                "CorpusId": 227054483
            },
            "abstract": "Reinforcement learning provides a general framework for flexible decision making and control, but requires extensive data collection for each new task that an agent needs to learn. In other machine learning fields, such as natural language processing or computer vision, pre-training on large, previously collected datasets to bootstrap learning for new tasks has emerged as a powerful paradigm to reduce data requirements when learning a new task. In this paper, we ask the following question: how can we enable similarly useful pre-training for RL agents? We propose a method for pre-training behavioral priors that can capture complex input-output relationships observed in successful trials from a wide range of previously seen tasks, and we show how this learned prior can be used for rapidly learning new tasks without impeding the RL agent's ability to try out novel behaviors. We demonstrate the effectiveness of our approach in challenging robotic manipulation domains involving image observations and sparse reward functions, where our method outperforms prior works by a substantial margin.",
            "referenceCount": 82,
            "citationCount": 103,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-11-19",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2011.10024"
            },
            "citationStyles": {
                "bibtex": "@Article{Singh2020ParrotDB,\n author = {Avi Singh and Huihan Liu and G. Zhou and Albert Yu and Nicholas Rhinehart and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Parrot: Data-Driven Behavioral Priors for Reinforcement Learning},\n volume = {abs/2011.10024},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bcdb21ca1703fc6f62df420626e36d138480a6a1",
            "@type": "ScholarlyArticle",
            "paperId": "bcdb21ca1703fc6f62df420626e36d138480a6a1",
            "corpusId": 59316583,
            "url": "https://www.semanticscholar.org/paper/bcdb21ca1703fc6f62df420626e36d138480a6a1",
            "title": "Action Robust Reinforcement Learning and Applications in Continuous Control",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2952981100",
                "ArXiv": "1901.09184",
                "DBLP": "conf/icml/TesslerEM19",
                "CorpusId": 59316583
            },
            "abstract": "A policy is said to be robust if it maximizes the reward while considering a bad, or even adversarial, model. In this work we formalize two new criteria of robustness to action uncertainty. Specifically, we consider two scenarios in which the agent attempts to perform an action $a$, and (i) with probability $\\alpha$, an alternative adversarial action $\\bar a$ is taken, or (ii) an adversary adds a perturbation to the selected action in the case of continuous action space. We show that our criteria are related to common forms of uncertainty in robotics domains, such as the occurrence of abrupt forces, and suggest algorithms in the tabular case. Building on the suggested algorithms, we generalize our approach to deep reinforcement learning (DRL) and provide extensive experiments in the various MuJoCo domains. Our experiments show that not only does our approach produce robust policies, but it also improves the performance in the absence of perturbations. This generalization indicates that action-robustness can be thought of as implicit regularization in RL problems.",
            "referenceCount": 46,
            "citationCount": 156,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-01-26",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tessler2019ActionRR,\n author = {Chen Tessler and Yonathan Efroni and Shie Mannor},\n booktitle = {International Conference on Machine Learning},\n pages = {6215-6224},\n title = {Action Robust Reinforcement Learning and Applications in Continuous Control},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9a8e6feb271bf1cce8b1393cf41e70692a7f6625",
            "@type": "ScholarlyArticle",
            "paperId": "9a8e6feb271bf1cce8b1393cf41e70692a7f6625",
            "corpusId": 46893003,
            "url": "https://www.semanticscholar.org/paper/9a8e6feb271bf1cce8b1393cf41e70692a7f6625",
            "title": "Verifiable Reinforcement Learning via Policy Extraction",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2964231903",
                "DBLP": "journals/corr/abs-1805-08328",
                "ArXiv": "1805.08328",
                "CorpusId": 46893003
            },
            "abstract": "While deep reinforcement learning has successfully solved many challenging control tasks, its real-world applicability has been limited by the inability to ensure the safety of learned policies. We propose an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies (since they are nonparametric), yet can be efficiently verified using existing techniques (since they are highly structured). The challenge is that decision tree policies are difficult to train. We propose VIPER, an algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy (called the oracle) and its Q-function, and show that it substantially outperforms two baselines. We use VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that provably never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves performance equal to that of the original DNN policy.",
            "referenceCount": 39,
            "citationCount": 234,
            "influentialCitationCount": 45,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-05-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bastani2018VerifiableRL,\n author = {O. Bastani and Yewen Pu and Armando Solar-Lezama},\n booktitle = {Neural Information Processing Systems},\n pages = {2499-2509},\n title = {Verifiable Reinforcement Learning via Policy Extraction},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d766b67868b3a9a97ad4407cd496760698d2a427",
            "@type": "ScholarlyArticle",
            "paperId": "d766b67868b3a9a97ad4407cd496760698d2a427",
            "corpusId": 211731573,
            "url": "https://www.semanticscholar.org/paper/d766b67868b3a9a97ad4407cd496760698d2a427",
            "title": "MetaLight: Value-Based Meta-Reinforcement Learning for Traffic Signal Control",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "2998332605",
                "DBLP": "conf/aaai/ZangYZXXL20",
                "DOI": "10.1609/AAAI.V34I01.5467",
                "CorpusId": 211731573
            },
            "abstract": "Using reinforcement learning for traffic signal control has attracted increasing interests recently. Various value-based reinforcement learning methods have been proposed to deal with this classical transportation problem and achieved better performances compared with traditional transportation methods. However, current reinforcement learning models rely on tremendous training data and computational resources, which may have bad consequences (e.g., traffic jams or accidents) in the real world. In traffic signal control, some algorithms have been proposed to empower quick learning from scratch, but little attention is paid to learning by transferring and reusing learned experience. In this paper, we propose a novel framework, named as MetaLight, to speed up the learning process in new scenarios by leveraging the knowledge learned from existing scenarios. MetaLight is a value-based meta-reinforcement learning workflow based on the representative gradient-based meta-learning algorithm (MAML), which includes periodically alternate individual-level adaptation and global-level adaptation. Moreover, MetaLight improves the-state-of-the-art reinforcement learning model FRAP in traffic signal control by optimizing its model structure and updating paradigm. The experiments on four real-world datasets show that our proposed MetaLight not only adapts more quickly and stably in new traffic scenarios, but also achieves better performance.",
            "referenceCount": 27,
            "citationCount": 93,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/5467/5323",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-04-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zang2020MetaLightVM,\n author = {Xinshi Zang and Huaxiu Yao and Guanjie Zheng and Nan Xu and Kai Xu and Zhenhui},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {1153-1160},\n title = {MetaLight: Value-Based Meta-Reinforcement Learning for Traffic Signal Control},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea63cca868cc792d4c0d20120f244f866d9aa752",
            "@type": "ScholarlyArticle",
            "paperId": "ea63cca868cc792d4c0d20120f244f866d9aa752",
            "corpusId": 225277420,
            "url": "https://www.semanticscholar.org/paper/ea63cca868cc792d4c0d20120f244f866d9aa752",
            "title": "A Survey of Multi-Task Deep Reinforcement Learning",
            "venue": "Electronics",
            "publicationVenue": {
                "id": "urn:research:ccd8e532-73c6-414f-bc91-271bbb2933e2",
                "name": "Electronics",
                "alternate_names": null,
                "issn": "1450-5843",
                "url": "http://www.electronics.etfbl.net/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3081310128",
                "DOI": "10.3390/electronics9091363",
                "CorpusId": 225277420
            },
            "abstract": "Driven by the recent technological advancements within the field of artificial intelligence research, deep learning has emerged as a promising representation learning technique across all of the machine learning classes, especially within the reinforcement learning arena. This new direction has given rise to the evolution of a new technological domain named deep reinforcement learning, which combines the representational learning power of deep learning with existing reinforcement learning methods. Undoubtedly, the inception of deep reinforcement learning has played a vital role in optimizing the performance of reinforcement learning-based intelligent agents with model-free based approaches. Although these methods could improve the performance of agents to a greater extent, they were mainly limited to systems that adopted reinforcement learning algorithms focused on learning a single task. At the same moment, the aforementioned approach was found to be relatively data-inefficient, particularly when reinforcement learning agents needed to interact with more complex and rich data environments. This is primarily due to the limited applicability of deep reinforcement learning algorithms to many scenarios across related tasks from the same environment. The objective of this paper is to survey the research challenges associated with multi-tasking within the deep reinforcement arena and present the state-of-the-art approaches by comparing and contrasting recent solutions, namely DISTRAL (DIStill & TRAnsfer Learning), IMPALA(Importance Weighted Actor-Learner Architecture) and PopArt that aim to address core challenges such as scalability, distraction dilemma, partial observability, catastrophic forgetting and negative knowledge transfer.",
            "referenceCount": 53,
            "citationCount": 92,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2079-9292/9/9/1363/pdf?version=1598088777",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2020-08-22",
            "journal": {
                "name": "Electronics",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Varghese2020ASO,\n author = {Nelson Vithayathil Varghese and Q. Mahmoud},\n booktitle = {Electronics},\n journal = {Electronics},\n title = {A Survey of Multi-Task Deep Reinforcement Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0772905d40b9afa3dc087a88184f09f3b3e1464f",
            "@type": "ScholarlyArticle",
            "paperId": "0772905d40b9afa3dc087a88184f09f3b3e1464f",
            "corpusId": 53391180,
            "url": "https://www.semanticscholar.org/paper/0772905d40b9afa3dc087a88184f09f3b3e1464f",
            "title": "Learning to Communicate with Deep Multi-Agent Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/FoersterAFW16a",
                "ArXiv": "1605.06676",
                "MAG": "2951097037",
                "CorpusId": 53391180
            },
            "abstract": "We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.",
            "referenceCount": 28,
            "citationCount": 1306,
            "influentialCitationCount": 145,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-05-21",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1605.06676"
            },
            "citationStyles": {
                "bibtex": "@Article{Foerster2016LearningTC,\n author = {Jakob N. Foerster and Yannis Assael and Nando de Freitas and Shimon Whiteson},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},\n volume = {abs/1605.06676},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a065d0cbac991d05692d138b7f3f34d087b4bb27",
            "@type": "ScholarlyArticle",
            "paperId": "a065d0cbac991d05692d138b7f3f34d087b4bb27",
            "corpusId": 49183164,
            "url": "https://www.semanticscholar.org/paper/a065d0cbac991d05692d138b7f3f34d087b4bb27",
            "title": "Deep reinforcement learning based computation offloading and resource allocation for MEC",
            "venue": "IEEE Wireless Communications and Networking Conference",
            "publicationVenue": {
                "id": "urn:research:27235614-bd3e-4d6b-be38-5ede18f4e209",
                "name": "IEEE Wireless Communications and Networking Conference",
                "alternate_names": [
                    "IEEE Wirel Commun Netw Conf",
                    "WCNC",
                    "Wireless Communications and Networking Conference",
                    "Wirel Commun Netw Conf"
                ],
                "issn": null,
                "url": "http://www.ieee-wcnc.org/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2808381205",
                "DBLP": "conf/wcnc/Li0LL18",
                "DOI": "10.1109/WCNC.2018.8377343",
                "CorpusId": 49183164
            },
            "abstract": "Mobile edge computing (MEC) has the potential to enable computation-intensive applications in 5G networks. MEC can extend the computational capacity at the edge of wireless networks by migrating the computation-intensive tasks to the MEC server. In this paper, we consider a multi-user MEC system, where multiple user equipments (UEs) can perform computation offloading via wireless channels to an MEC server. We formulate the sum cost of delay and energy consumptions for all UEs as our optimization objective. In order to minimize the sum cost of the considered MEC system, we jointly optimize the offloading decision and computational resource allocation. However, it is challenging to obtain an optimal policy in such a dynamic system. Besides immediate reward, Reinforcement Learning (RL) also takes a long-term goal into consideration, which is very important to a time-variant dynamic systems, such as our considered multi-user wireless MEC system. To this end, we propose RL-based optimization framework to tackle the resource allocation in wireless MEC. Specifically, the Q-learning based and Deep Reinforcement Learning (DRL) based schemes are proposed, respectively. Simulation results show that the proposed scheme achieves significant reduction on the sum cost compared to other baselines.",
            "referenceCount": 18,
            "citationCount": 326,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-04-15",
            "journal": {
                "name": "2018 IEEE Wireless Communications and Networking Conference (WCNC)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Li2018DeepRL,\n author = {Ji Li and Hui Gao and Tiejun Lv and Yueming Lu},\n booktitle = {IEEE Wireless Communications and Networking Conference},\n journal = {2018 IEEE Wireless Communications and Networking Conference (WCNC)},\n pages = {1-6},\n title = {Deep reinforcement learning based computation offloading and resource allocation for MEC},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5a6b2b9bc3b51ff187826fc2dc21a967e04125ed",
            "@type": "ScholarlyArticle",
            "paperId": "5a6b2b9bc3b51ff187826fc2dc21a967e04125ed",
            "corpusId": 108339287,
            "url": "https://www.semanticscholar.org/paper/5a6b2b9bc3b51ff187826fc2dc21a967e04125ed",
            "title": "Model-based reinforcement learning: A survey",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2923762280",
                "CorpusId": 108339287
            },
            "abstract": "Reinforcement learning is an important branch of machine learning and artificial intelligence. Compared with traditional reinforcement learning, model-based reinforcement learning obtains the action of the next state by the model that has been learned",
            "referenceCount": 83,
            "citationCount": 327,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Yi2018ModelbasedRL,\n author = {Fengji Yi and Wenlong Fu and Huan Liang},\n title = {Model-based reinforcement learning: A survey},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:041d99f442dc22cf51118dc992095be9aa0972e0",
            "@type": "ScholarlyArticle",
            "paperId": "041d99f442dc22cf51118dc992095be9aa0972e0",
            "corpusId": 5011374,
            "url": "https://www.semanticscholar.org/paper/041d99f442dc22cf51118dc992095be9aa0972e0",
            "title": "A Study on Overfitting in Deep Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1804-06893",
                "ArXiv": "1804.06893",
                "MAG": "2797527950",
                "CorpusId": 5011374
            },
            "abstract": "Recent years have witnessed significant progresses in deep Reinforcement Learning (RL). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging RL problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep RL techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard RL agents and find that they could overfit in various ways. Moreover, overfitting could happen \"robustly\": commonly used techniques in RL that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in RL. We conclude with a general discussion on overfitting in RL and a study of the generalization behaviors from the perspective of inductive bias.",
            "referenceCount": 59,
            "citationCount": 321,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-04-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1804.06893"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2018ASO,\n author = {Chiyuan Zhang and Oriol Vinyals and R. Munos and Samy Bengio},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Study on Overfitting in Deep Reinforcement Learning},\n volume = {abs/1804.06893},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e74aded7d0839af48706c51a7b55af2ea20f0603",
            "@type": "ScholarlyArticle",
            "paperId": "e74aded7d0839af48706c51a7b55af2ea20f0603",
            "corpusId": 209479151,
            "url": "https://www.semanticscholar.org/paper/e74aded7d0839af48706c51a7b55af2ea20f0603",
            "title": "Sharing Knowledge in Multi-Task Deep Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/iclr/DEramoTBR020",
                "MAG": "2995481444",
                "CorpusId": 209479151
            },
            "abstract": "We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning. We leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction compared to learning a single task. Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms. We prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks, extending the well-known finite-time bounds of Approximate Value-Iteration to the multi-task setting. In addition, we complement our analysis by proposing multi-task extensions of three Reinforcement Learning algorithms that we empirically evaluate on widely used Reinforcement Learning benchmarks showing significant improvements over the single-task counterparts in terms of sample efficiency and performance.",
            "referenceCount": 36,
            "citationCount": 89,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-04-30",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{D'Eramo2020SharingKI,\n author = {Carlo D'Eramo and Davide Tateo and Andrea Bonarini and Marcello Restelli and J. Peters},\n booktitle = {International Conference on Learning Representations},\n title = {Sharing Knowledge in Multi-Task Deep Reinforcement Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d",
            "@type": "ScholarlyArticle",
            "paperId": "0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d",
            "corpusId": 76564,
            "url": "https://www.semanticscholar.org/paper/0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d",
            "title": "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning",
            "venue": "Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:96018464-22dc-4b5c-a172-c2f4a30ce131",
                "name": "Artificial Intelligence",
                "alternate_names": [
                    "Artif Intell"
                ],
                "issn": "0004-3702",
                "url": "http://www.elsevier.com/locate/artint"
            },
            "year": 1999,
            "externalIds": {
                "DBLP": "journals/ai/SuttonPS99",
                "MAG": "2109910161",
                "DOI": "10.1016/S0004-3702(99)00052-1",
                "CorpusId": 76564
            },
            "abstract": null,
            "referenceCount": 83,
            "citationCount": 3329,
            "influentialCitationCount": 416,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1999-08-01",
            "journal": {
                "name": "Artif. Intell.",
                "volume": "112"
            },
            "citationStyles": {
                "bibtex": "@Article{Sutton1999BetweenMA,\n author = {R. Sutton and Doina Precup and Satinder Singh},\n booktitle = {Artificial Intelligence},\n journal = {Artif. Intell.},\n pages = {181-211},\n title = {Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning},\n volume = {112},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3002d4f76f36bd78bf9e2118fbc830648cc5ea08",
            "@type": "ScholarlyArticle",
            "paperId": "3002d4f76f36bd78bf9e2118fbc830648cc5ea08",
            "corpusId": 229459548,
            "url": "https://www.semanticscholar.org/paper/3002d4f76f36bd78bf9e2118fbc830648cc5ea08",
            "title": "A survey on multi-agent deep reinforcement learning: from the perspective of challenges and applications",
            "venue": "Artificial Intelligence Review",
            "publicationVenue": {
                "id": "urn:research:ea8553fe-2467-4367-afee-c4deb3754820",
                "name": "Artificial Intelligence Review",
                "alternate_names": [
                    "Artif Intell Rev"
                ],
                "issn": "0269-2821",
                "url": "https://link.springer.com/journal/10462"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/air/DuD21",
                "MAG": "3106649810",
                "DOI": "10.1007/s10462-020-09938-y",
                "CorpusId": 229459548
            },
            "abstract": null,
            "referenceCount": 124,
            "citationCount": 88,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-11-24",
            "journal": {
                "name": "Artificial Intelligence Review",
                "volume": "54"
            },
            "citationStyles": {
                "bibtex": "@Article{Du2020ASO,\n author = {Wei Du and Shifei Ding},\n booktitle = {Artificial Intelligence Review},\n journal = {Artificial Intelligence Review},\n pages = {3215 - 3238},\n title = {A survey on multi-agent deep reinforcement learning: from the perspective of challenges and applications},\n volume = {54},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e2c4e7b3302549b3718601c44d9af6c7554efef",
            "@type": "ScholarlyArticle",
            "paperId": "5e2c4e7b3302549b3718601c44d9af6c7554efef",
            "corpusId": 21529792,
            "url": "https://www.semanticscholar.org/paper/5e2c4e7b3302549b3718601c44d9af6c7554efef",
            "title": "Learning Robust Rewards with Adversarial Inverse Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2766610320",
                "DBLP": "journals/corr/abs-1710-11248",
                "ArXiv": "1710.11248",
                "CorpusId": 21529792
            },
            "abstract": "Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose adverserial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.",
            "referenceCount": 26,
            "citationCount": 657,
            "influentialCitationCount": 158,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-10-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1710.11248"
            },
            "citationStyles": {
                "bibtex": "@Article{Fu2017LearningRR,\n author = {Justin Fu and Katie Luo and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning Robust Rewards with Adversarial Inverse Reinforcement Learning},\n volume = {abs/1710.11248},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:67d10cea808937089d6492acff14ad9ef156e3c5",
            "@type": "ScholarlyArticle",
            "paperId": "67d10cea808937089d6492acff14ad9ef156e3c5",
            "corpusId": 7559418,
            "url": "https://www.semanticscholar.org/paper/67d10cea808937089d6492acff14ad9ef156e3c5",
            "title": "Minimax Regret Bounds for Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2964054583",
                "DBLP": "journals/corr/AzarOM17",
                "ArXiv": "1703.05449",
                "CorpusId": 7559418
            },
            "abstract": "We consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of $\\tilde{O}( \\sqrt{HSAT} + H^2S^2A+H\\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states, $A$ the number of actions and $T$ the number of time-steps. This result improves over the best previous known bound $\\tilde{O}(HS \\sqrt{AT})$ achieved by the UCRL2 algorithm of Jaksch et al., 2010. The key significance of our new results is that when $T\\geq H^3S^3A$ and $SA\\geq H$, it leads to a regret of $\\tilde{O}(\\sqrt{HSAT})$ that matches the established lower bound of $\\Omega(\\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contains two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in $S$), and we define Bernstein-based \"exploration bonuses\" that use the empirical variance of the estimated values at the next states (to improve scaling in $H$).",
            "referenceCount": 39,
            "citationCount": 633,
            "influentialCitationCount": 168,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Azar2017MinimaxRB,\n author = {M. G. Azar and Ian Osband and R. Munos},\n booktitle = {International Conference on Machine Learning},\n pages = {263-272},\n title = {Minimax Regret Bounds for Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:857176d022369e963d3ff1be2cb9e1ca2f674520",
            "@type": "ScholarlyArticle",
            "paperId": "857176d022369e963d3ff1be2cb9e1ca2f674520",
            "corpusId": 20667722,
            "url": "https://www.semanticscholar.org/paper/857176d022369e963d3ff1be2cb9e1ca2f674520",
            "title": "DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1707.06690",
                "ACL": "D17-1060",
                "DBLP": "journals/corr/XiongHW17",
                "MAG": "2962886429",
                "DOI": "10.18653/v1/D17-1060",
                "CorpusId": 20667722
            },
            "abstract": "We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector-space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.",
            "referenceCount": 28,
            "citationCount": 552,
            "influentialCitationCount": 128,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D17-1060.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-07-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Xiong2017DeepPathAR,\n author = {Wenhan Xiong and Thi-Lan-Giao Hoang and William Yang Wang},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {564-573},\n title = {DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fe3e91e40a950c6b6601b8f0a641884774d949ae",
            "@type": "ScholarlyArticle",
            "paperId": "fe3e91e40a950c6b6601b8f0a641884774d949ae",
            "corpusId": 139930,
            "url": "https://www.semanticscholar.org/paper/fe3e91e40a950c6b6601b8f0a641884774d949ae",
            "title": "Distributional Reinforcement Learning with Quantile Regression",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1710.10044",
                "MAG": "2765302304",
                "DBLP": "journals/corr/abs-1710-10044",
                "DOI": "10.1609/aaai.v32i1.11791",
                "CorpusId": 139930
            },
            "abstract": "\n \n In reinforcement learning (RL), an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.\n \n",
            "referenceCount": 30,
            "citationCount": 544,
            "influentialCitationCount": 145,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11791/11650",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-10-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dabney2017DistributionalRL,\n author = {Will Dabney and Mark Rowland and Marc G. Bellemare and R. Munos},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {2892-2901},\n title = {Distributional Reinforcement Learning with Quantile Regression},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1796f6f928079f935748dad59f6ed6fdbba960c8",
            "@type": "ScholarlyArticle",
            "paperId": "1796f6f928079f935748dad59f6ed6fdbba960c8",
            "corpusId": 204193309,
            "url": "https://www.semanticscholar.org/paper/1796f6f928079f935748dad59f6ed6fdbba960c8",
            "title": "Graph Convolutional Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/iclr/JiangDHL20",
                "MAG": "2975343435",
                "ArXiv": "1810.09202",
                "CorpusId": 204193309
            },
            "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.",
            "referenceCount": 37,
            "citationCount": 227,
            "influentialCitationCount": 39,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-10-22",
            "journal": {
                "name": "arXiv: Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Jiang2018GraphCR,\n author = {Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Learning},\n title = {Graph Convolutional Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f65020fc3b1692d7989e099d6b6e698be5a50a93",
            "@type": "ScholarlyArticle",
            "paperId": "f65020fc3b1692d7989e099d6b6e698be5a50a93",
            "corpusId": 207155342,
            "url": "https://www.semanticscholar.org/paper/f65020fc3b1692d7989e099d6b6e698be5a50a93",
            "title": "Apprenticeship learning via inverse reinforcement learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "conf/icml/PieterN04",
                "MAG": "1999874108",
                "DOI": "10.1145/1015330.1015430",
                "CorpusId": 207155342
            },
            "abstract": "We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using \"inverse reinforcement learning\" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.",
            "referenceCount": 21,
            "citationCount": 3043,
            "influentialCitationCount": 398,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-07-04",
            "journal": {
                "name": "Proceedings of the twenty-first international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Abbeel2004ApprenticeshipLV,\n author = {P. Abbeel and A. Ng},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the twenty-first international conference on Machine learning},\n title = {Apprenticeship learning via inverse reinforcement learning},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9f1e9e56d80146766bc2316efbc54d8b770a23df",
            "@type": "ScholarlyArticle",
            "paperId": "9f1e9e56d80146766bc2316efbc54d8b770a23df",
            "corpusId": 17540505,
            "url": "https://www.semanticscholar.org/paper/9f1e9e56d80146766bc2316efbc54d8b770a23df",
            "title": "Deep Reinforcement Learning: An Overview",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2580175322",
                "DBLP": "journals/corr/Li17b",
                "ArXiv": "1701.07274",
                "CorpusId": 17540505
            },
            "abstract": "We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions. \nPlease see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.",
            "referenceCount": 578,
            "citationCount": 1131,
            "influentialCitationCount": 89,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-01-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1701.07274"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2017DeepRL,\n author = {Yuxi Li},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Deep Reinforcement Learning: An Overview},\n volume = {abs/1701.07274},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ddea0e5c27ff91cda7ba50e1231aa7f7d076e58b",
            "@type": "ScholarlyArticle",
            "paperId": "ddea0e5c27ff91cda7ba50e1231aa7f7d076e58b",
            "corpusId": 122539846,
            "url": "https://www.semanticscholar.org/paper/ddea0e5c27ff91cda7ba50e1231aa7f7d076e58b",
            "title": "Reinforcement Learning, Fast and Slow",
            "venue": "Trends in Cognitive Sciences",
            "publicationVenue": {
                "id": "urn:research:20cb626a-518d-4016-826a-0157cf2f8fd6",
                "name": "Trends in Cognitive Sciences",
                "alternate_names": [
                    "Trends Cogn Sci"
                ],
                "issn": "1364-6613",
                "url": "https://www.cell.com/trends/cognitive-sciences/home"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2938321354",
                "DOI": "10.1016/j.tics.2019.02.006",
                "CorpusId": 122539846,
                "PubMed": "31003893"
            },
            "abstract": null,
            "referenceCount": 106,
            "citationCount": 428,
            "influentialCitationCount": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cell.com/article/S1364661319300610/pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2019-05-01",
            "journal": {
                "name": "Trends in Cognitive Sciences",
                "volume": "23"
            },
            "citationStyles": {
                "bibtex": "@Article{Botvinick2019ReinforcementLF,\n author = {M. Botvinick and S. Ritter and Jane X. Wang and Z. Kurth-Nelson and C. Blundell and D. Hassabis},\n booktitle = {Trends in Cognitive Sciences},\n journal = {Trends in Cognitive Sciences},\n pages = {408-422},\n title = {Reinforcement Learning, Fast and Slow},\n volume = {23},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:30de3c20bbe9562e1fbce162d84de593a073bc15",
            "@type": "ScholarlyArticle",
            "paperId": "30de3c20bbe9562e1fbce162d84de593a073bc15",
            "corpusId": 44137923,
            "url": "https://www.semanticscholar.org/paper/30de3c20bbe9562e1fbce162d84de593a073bc15",
            "title": "Prefrontal cortex as a meta-reinforcement learning system",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2795725605",
                "DOI": "10.1038/s41593-018-0147-8",
                "CorpusId": 44137923,
                "PubMed": "29760527"
            },
            "abstract": null,
            "referenceCount": 81,
            "citationCount": 431,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Biology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-04-13",
            "journal": {
                "name": "Nature Neuroscience",
                "volume": "21"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2018PrefrontalCA,\n author = {Jane X. Wang and Z. Kurth-Nelson and D. Kumaran and Dhruva Tirumala and Hubert Soyer and Joel Z. Leibo and D. Hassabis and M. Botvinick},\n booktitle = {Nature Neuroscience},\n journal = {Nature Neuroscience},\n pages = {860 - 868},\n title = {Prefrontal cortex as a meta-reinforcement learning system},\n volume = {21},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2a49a71c9d40051a03c4445fe49025bc75d9eeb6",
            "@type": "ScholarlyArticle",
            "paperId": "2a49a71c9d40051a03c4445fe49025bc75d9eeb6",
            "corpusId": 43966764,
            "url": "https://www.semanticscholar.org/paper/2a49a71c9d40051a03c4445fe49025bc75d9eeb6",
            "title": "Meta-Gradient Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2803767077",
                "DBLP": "conf/nips/XuHS18",
                "ArXiv": "1805.09801",
                "CorpusId": 43966764
            },
            "abstract": "The goal of reinforcement learning algorithms is to estimate and/or optimise the value function. However, unlike supervised learning, no teacher or oracle is available to provide the true value function. Instead, the majority of reinforcement learning algorithms estimate and/or optimise a proxy for the value function. This proxy is typically based on a sampled and bootstrapped approximation to the true value function, known as a return. The particular choice of return is one of the chief components determining the nature of the algorithm: the rate at which future rewards are discounted; when and how values should be bootstrapped; or even the nature of the rewards themselves. It is well-known that these decisions are crucial to the overall success of RL algorithms. We discuss a gradient-based meta-learning algorithm that is able to adapt the nature of the return, online, whilst interacting and learning from the environment. When applied to 57 games on the Atari 2600 environment over 200 million frames, our algorithm achieved a new state-of-the-art performance.",
            "referenceCount": 63,
            "citationCount": 274,
            "influentialCitationCount": 28,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-05-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Xu2018MetaGradientRL,\n author = {Zhongwen Xu and H. V. Hasselt and David Silver},\n booktitle = {Neural Information Processing Systems},\n pages = {2402-2413},\n title = {Meta-Gradient Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e37b999f0c96d7136db07b0185b837d5decd599a",
            "@type": "ScholarlyArticle",
            "paperId": "e37b999f0c96d7136db07b0185b837d5decd599a",
            "corpusId": 18389147,
            "url": "https://www.semanticscholar.org/paper/e37b999f0c96d7136db07b0185b837d5decd599a",
            "title": "Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates",
            "venue": "IEEE International Conference on Robotics and Automation",
            "publicationVenue": {
                "id": "urn:research:3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                "name": "IEEE International Conference on Robotics and Automation",
                "alternate_names": [
                    "International Conference on Robotics and Automation",
                    "Int Conf Robot Autom",
                    "ICRA",
                    "IEEE Int Conf Robot Autom"
                ],
                "issn": "2152-4092",
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1610.00633",
                "MAG": "2575705757",
                "DBLP": "conf/icra/GuHLL17",
                "DOI": "10.1109/ICRA.2017.7989385",
                "CorpusId": 18389147
            },
            "abstract": "Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.",
            "referenceCount": 43,
            "citationCount": 1281,
            "influentialCitationCount": 23,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1610.00633",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-10-03",
            "journal": {
                "name": "2017 IEEE International Conference on Robotics and Automation (ICRA)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Gu2016DeepRL,\n author = {S. Gu and E. Holly and T. Lillicrap and S. Levine},\n booktitle = {IEEE International Conference on Robotics and Automation},\n journal = {2017 IEEE International Conference on Robotics and Automation (ICRA)},\n pages = {3389-3396},\n title = {Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea5988da726cb50d2376584b04d668008aad0a3f",
            "@type": "ScholarlyArticle",
            "paperId": "ea5988da726cb50d2376584b04d668008aad0a3f",
            "corpusId": 221203027,
            "url": "https://www.semanticscholar.org/paper/ea5988da726cb50d2376584b04d668008aad0a3f",
            "title": "Explainability in Deep Reinforcement Learning",
            "venue": "Knowledge-Based Systems",
            "publicationVenue": {
                "id": "urn:research:12fff95b-d469-49a0-84a5-4fd4696c3f28",
                "name": "Knowledge-Based Systems",
                "alternate_names": [
                    "Knowl Based Syst",
                    "Knowledge Based Systems",
                    "Knowledge-based Syst"
                ],
                "issn": "0950-7051",
                "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2008-06693",
                "ArXiv": "2008.06693",
                "MAG": "3048977193",
                "DOI": "10.1016/j.knosys.2020.106685",
                "CorpusId": 221203027
            },
            "abstract": null,
            "referenceCount": 123,
            "citationCount": 164,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://manuscript.elsevier.com/S0950705120308145/pdf/S0950705120308145.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-08-15",
            "journal": {
                "name": "Knowl. Based Syst.",
                "volume": "214"
            },
            "citationStyles": {
                "bibtex": "@Article{Heuillet2020ExplainabilityID,\n author = {Alexandre Heuillet and Fabien Couthouis and Natalia D\u00edaz Rodr\u00edguez},\n booktitle = {Knowledge-Based Systems},\n journal = {Knowl. Based Syst.},\n pages = {106685},\n title = {Explainability in Deep Reinforcement Learning},\n volume = {214},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751",
            "@type": "ScholarlyArticle",
            "paperId": "f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751",
            "corpusId": 53111609,
            "url": "https://www.semanticscholar.org/paper/f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751",
            "title": "Deep Reinforcement Learning",
            "venue": "Reinforcement Learning for Cyber-Physical Systems",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2888561335",
                "ArXiv": "1810.06339",
                "DBLP": "journals/corr/abs-1810-06339",
                "DOI": "10.1007/978-3-319-94463-0_9",
                "CorpusId": 53111609
            },
            "abstract": null,
            "referenceCount": 936,
            "citationCount": 304,
            "influentialCitationCount": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1810.06339",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Art",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-10-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1810.06339"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2018DeepRL,\n author = {Yuxi Li},\n booktitle = {Reinforcement Learning for Cyber-Physical Systems},\n journal = {ArXiv},\n title = {Deep Reinforcement Learning},\n volume = {abs/1810.06339},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2cdf553c4d3e68b9189217cc1f2d6b90e3f6eb7e",
            "@type": "ScholarlyArticle",
            "paperId": "2cdf553c4d3e68b9189217cc1f2d6b90e3f6eb7e",
            "corpusId": 3655441,
            "url": "https://www.semanticscholar.org/paper/2cdf553c4d3e68b9189217cc1f2d6b90e3f6eb7e",
            "title": "Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2789824229",
                "DBLP": "journals/corr/abs-1803-00101",
                "ArXiv": "1803.00101",
                "CorpusId": 3655441
            },
            "abstract": "Recent model-free reinforcement learning algorithms have proposed incorporating learned dynamics models as a source of additional data with the intention of reducing sample complexity. Such methods hold the promise of incorporating imagined data coupled with a notion of model uncertainty to accelerate the learning of continuous control tasks. Unfortunately, they rely on heuristics that limit usage of the dynamics model. We present model-based value expansion, which controls for uncertainty in the model by only allowing imagination to fixed depth. By enabling wider use of learned dynamics models within a model-free reinforcement learning algorithm, we improve value estimation, which, in turn, reduces the sample complexity of learning.",
            "referenceCount": 15,
            "citationCount": 259,
            "influentialCitationCount": 34,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-28",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1803.00101"
            },
            "citationStyles": {
                "bibtex": "@Article{Feinberg2018ModelBasedVE,\n author = {Vladimir Feinberg and Alvin Wan and I. Stoica and Michael I. Jordan and Joseph E. Gonzalez and S. Levine},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning},\n volume = {abs/1803.00101},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7dc156eb9d84ae8fd521ecac5ccc5b5426a42b50",
            "@type": "ScholarlyArticle",
            "paperId": "7dc156eb9d84ae8fd521ecac5ccc5b5426a42b50",
            "corpusId": 182952502,
            "url": "https://www.semanticscholar.org/paper/7dc156eb9d84ae8fd521ecac5ccc5b5426a42b50",
            "title": "A Survey of Reinforcement Learning Informed by Natural Language",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/ijcai/LuketinaNFFAGWR19",
                "MAG": "2948380112",
                "ArXiv": "1906.03926",
                "DOI": "10.24963/ijcai.2019/880",
                "CorpusId": 182952502
            },
            "abstract": "To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.",
            "referenceCount": 102,
            "citationCount": 238,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ijcai.org/proceedings/2019/0880.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2019-06-10",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1906.03926"
            },
            "citationStyles": {
                "bibtex": "@Article{Luketina2019ASO,\n author = {Jelena Luketina and Nantas Nardelli and Gregory Farquhar and Jakob N. Foerster and Jacob Andreas and Edward Grefenstette and Shimon Whiteson and Tim Rockt\u00e4schel},\n booktitle = {International Joint Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {A Survey of Reinforcement Learning Informed by Natural Language},\n volume = {abs/1906.03926},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:36089b937a5f29e5ca8df93508e5cb8b9f42b6a5",
            "@type": "ScholarlyArticle",
            "paperId": "36089b937a5f29e5ca8df93508e5cb8b9f42b6a5",
            "corpusId": 69637608,
            "url": "https://www.semanticscholar.org/paper/36089b937a5f29e5ca8df93508e5cb8b9f42b6a5",
            "title": "Review on the research and practice of deep learning and reinforcement learning in smart grids",
            "venue": "CSEE Journal of Power and Energy Systems",
            "publicationVenue": {
                "id": "urn:research:c3aa4aa5-d42b-4b6c-a442-77301e266cd9",
                "name": "CSEE Journal of Power and Energy Systems",
                "alternate_names": [
                    "CSEE J Power Energy Syst"
                ],
                "issn": "2096-0042",
                "url": "http://jpes.csee.org.cn/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2892841407",
                "DOI": "10.17775/CSEEJPES.2018.00520",
                "CorpusId": 69637608
            },
            "abstract": "Smart grids are the developmental trend of power systems and they have attracted much attention all over the world. Due to their complexities, and the uncertainty of the smart grid and high volume of information being collected, artificial intelligence techniques represent some of the enabling technologies for its future development and success. Owing to the decreasing cost of computing power, the profusion of data, and better algorithms, AI has entered into its new developmental stage and AI 2.0 is developing rapidly. Deep learning (DL), reinforcement learning (RL) and their combination-deep reinforcement learning (DRL) are representative methods and relatively mature methods in the family of AI 2.0. This article introduces the concept and status quo of the above three methods, summarizes their potential for application in smart grids, and provides an overview of the research work on their application in smart grids.",
            "referenceCount": 0,
            "citationCount": 305,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2018-09-24",
            "journal": {
                "name": "CSEE Journal of Power and Energy Systems",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2018ReviewOT,\n author = {Dongxia Zhang and Xiaoqing Han and Chunyu Deng},\n booktitle = {CSEE Journal of Power and Energy Systems},\n journal = {CSEE Journal of Power and Energy Systems},\n title = {Review on the research and practice of deep learning and reinforcement learning in smart grids},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:954b01151ff13aef416d27adc60cd9a076753b1a",
            "@type": "ScholarlyArticle",
            "paperId": "954b01151ff13aef416d27adc60cd9a076753b1a",
            "corpusId": 14763001,
            "url": "https://www.semanticscholar.org/paper/954b01151ff13aef416d27adc60cd9a076753b1a",
            "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1611.02779",
                "MAG": "2578206533",
                "DBLP": "journals/corr/DuanSCBSA16",
                "CorpusId": 14763001
            },
            "abstract": "Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a \"fast\" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL$^2$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose (\"slow\") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the \"fast\" RL algorithm on the current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL$^2$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL$^2$ on a vision-based navigation task and show that it scales up to high-dimensional problems.",
            "referenceCount": 65,
            "citationCount": 837,
            "influentialCitationCount": 118,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.02779"
            },
            "citationStyles": {
                "bibtex": "@Article{Duan2016RL2FR,\n author = {Yan Duan and J. Schulman and Xi Chen and P. Bartlett and Ilya Sutskever and P. Abbeel},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning},\n volume = {abs/1611.02779},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ad0a1b0991a9150b765c2a45eb2b368702b35cd1",
            "@type": "ScholarlyArticle",
            "paperId": "ad0a1b0991a9150b765c2a45eb2b368702b35cd1",
            "corpusId": 46955236,
            "url": "https://www.semanticscholar.org/paper/ad0a1b0991a9150b765c2a45eb2b368702b35cd1",
            "title": "Deep Variational Reinforcement Learning for POMDPs",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2803743229",
                "DBLP": "journals/corr/abs-1806-02426",
                "ArXiv": "1806.02426",
                "CorpusId": 46955236
            },
            "abstract": "Many real-world sequential decision making problems are partially observable by nature, and the environment model is typically unknown. Consequently, there is great need for reinforcement learning methods that can tackle such problems given only a stream of incomplete and noisy observations. In this paper, we propose deep variational reinforcement learning (DVRL), which introduces an inductive bias that allows an agent to learn a generative model of the environment and perform inference in that model to effectively aggregate the available information. We develop an n-step approximation to the evidence lower bound (ELBO), allowing the model to be trained jointly with the policy. This ensures that the latent state representation is suitable for the control task. In experiments on Mountain Hike and flickering Atari we show that our method outperforms previous approaches relying on recurrent neural networks to encode the past.",
            "referenceCount": 52,
            "citationCount": 220,
            "influentialCitationCount": 34,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-06-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Igl2018DeepVR,\n author = {Maximilian Igl and L. Zintgraf and T. Le and Frank Wood and Shimon Whiteson},\n booktitle = {International Conference on Machine Learning},\n pages = {2122-2131},\n title = {Deep Variational Reinforcement Learning for POMDPs},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dea6aeb514b1969ab879c793d46a0d2eceaa2cbf",
            "@type": "ScholarlyArticle",
            "paperId": "dea6aeb514b1969ab879c793d46a0d2eceaa2cbf",
            "corpusId": 21671720,
            "url": "https://www.semanticscholar.org/paper/dea6aeb514b1969ab879c793d46a0d2eceaa2cbf",
            "title": "Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1805-04276",
                "MAG": "2785423445",
                "ArXiv": "1805.04276",
                "CorpusId": 21671720
            },
            "abstract": "Program synthesis is the task of automatically generating a program consistent with a specification. Recent years have seen proposal of a number of neural approaches for program synthesis, many of which adopt a sequence generation paradigm similar to neural machine translation, in which sequence-to-sequence models are trained to maximize the likelihood of known reference programs. While achieving impressive results, this strategy has two key limitations. First, it ignores Program Aliasing: the fact that many different programs may satisfy a given specification (especially with incomplete specifications such as a few input-output examples). By maximizing the likelihood of only a single reference program, it penalizes many semantically correct programs, which can adversely affect the synthesizer performance. Second, this strategy overlooks the fact that programs have a strict syntax that can be efficiently checked. To address the first limitation, we perform reinforcement learning on top of a supervised model with an objective that explicitly maximizes the likelihood of generating semantically correct programs. For addressing the second limitation, we introduce a training procedure that directly maximizes the probability of generating syntactically correct programs that fulfill the specification. We show that our contributions lead to improved accuracy of the models, especially in cases where the training data is limited.",
            "referenceCount": 40,
            "citationCount": 184,
            "influentialCitationCount": 38,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1805.04276"
            },
            "citationStyles": {
                "bibtex": "@Article{Bunel2018LeveragingGA,\n author = {Rudy Bunel and Matthew J. Hausknecht and Jacob Devlin and Rishabh Singh and Pushmeet Kohli},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis},\n volume = {abs/1805.04276},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:00ec8123dd2ba03afab7c1fa02f774062f769181",
            "@type": "ScholarlyArticle",
            "paperId": "00ec8123dd2ba03afab7c1fa02f774062f769181",
            "corpusId": 51868784,
            "url": "https://www.semanticscholar.org/paper/00ec8123dd2ba03afab7c1fa02f774062f769181",
            "title": "Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2804948070",
                "DBLP": "conf/icml/IcarteKVM18",
                "CorpusId": 51868784
            },
            "abstract": "In this paper we propose Reward Machines \u2013 a type of finite state machine that supports the specification of reward functions while exposing reward function structure to the learner and supporting decomposition. We then present Q-Learning for Reward Machines (QRM), an algorithm which appropriately decomposes the reward machine and uses off-policy q-learning to simultaneously learn subpolicies for the different components. QRM is guaranteed to converge to an optimal policy in the tabular case, in contrast to Hierarchical Reinforcement Learning methods which might converge to suboptimal policies. We demonstrate this behavior experimentally in two discrete domains. We also show how function approximation methods like neural networks can be incorporated into QRM, and that doing so can find better policies more quickly than hierarchical methods in a domain with a continuous state space.",
            "referenceCount": 25,
            "citationCount": 192,
            "influentialCitationCount": 35,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-07-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Icarte2018UsingRM,\n author = {Rodrigo Toro Icarte and Toryn Q. Klassen and R. Valenzano and Sheila A. McIlraith},\n booktitle = {International Conference on Machine Learning},\n pages = {2112-2121},\n title = {Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5956c34032126185d8ad19695e4a1a191c08b5a1",
            "@type": "ScholarlyArticle",
            "paperId": "5956c34032126185d8ad19695e4a1a191c08b5a1",
            "corpusId": 24131880,
            "url": "https://www.semanticscholar.org/paper/5956c34032126185d8ad19695e4a1a191c08b5a1",
            "title": "Deep reinforcement learning for page-wise recommendations",
            "venue": "ACM Conference on Recommender Systems",
            "publicationVenue": {
                "id": "urn:research:61275a16-1e0d-479f-ac4e-f295310761f0",
                "name": "ACM Conference on Recommender Systems",
                "alternate_names": [
                    "Conf Recomm Syst",
                    "RecSys",
                    "ACM Conf Recomm Syst",
                    "Conference on Recommender Systems"
                ],
                "issn": null,
                "url": "http://recsys.acm.org/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "3102899483",
                "DBLP": "journals/corr/abs-1805-02343",
                "ArXiv": "1805.02343",
                "DOI": "10.1145/3240323.3240374",
                "CorpusId": 24131880
            },
            "abstract": "Recommender systems can mitigate the information overload problem by suggesting users' personalized items. In real-world recommendations such as e-commerce, a typical interaction between the system and its users is - users are recommended a page of items and provide feedback; and then the system recommends a new page of items. To effectively capture such interaction for recommendations, we need to solve two key problems - (1) how to update recommending strategy according to user's real-time feedback, and 2) how to generate a page of items with proper display, which pose tremendous challenges to traditional recommender systems. In this paper, we study the problem of page-wise recommendations aiming to address aforementioned two challenges simultaneously. In particular, we propose a principled approach to jointly generate a set of complementary items and the corresponding strategy to display them in a 2-D page; and propose a novel page-wise recommendation framework based on deep reinforcement learning, DeepPage, which can optimize a page of items with proper display based on real-time feedback from users. The experimental results based on a real-world e-commerce dataset demonstrate the effectiveness of the proposed framework.",
            "referenceCount": 55,
            "citationCount": 277,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3240323.3240374",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book"
            ],
            "publicationDate": "2018-05-07",
            "journal": {
                "name": "Proceedings of the 12th ACM Conference on Recommender Systems",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhao2018DeepRL,\n author = {Xiangyu Zhao and Long Xia and L. Zhang and Zhuoye Ding and Dawei Yin and Jiliang Tang},\n booktitle = {ACM Conference on Recommender Systems},\n journal = {Proceedings of the 12th ACM Conference on Recommender Systems},\n title = {Deep reinforcement learning for page-wise recommendations},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f7c15e9ac6653330b7dd18a89301a3b333927db3",
            "@type": "ScholarlyArticle",
            "paperId": "f7c15e9ac6653330b7dd18a89301a3b333927db3",
            "corpusId": 199543559,
            "url": "https://www.semanticscholar.org/paper/f7c15e9ac6653330b7dd18a89301a3b333927db3",
            "title": "A review of cooperative multi-agent deep reinforcement learning",
            "venue": "Applied intelligence (Boston)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1908.03963",
                "DBLP": "journals/apin/OroojlooyH23",
                "MAG": "2968526727",
                "DOI": "10.1007/s10489-022-04105-y",
                "CorpusId": 199543559
            },
            "abstract": null,
            "referenceCount": 271,
            "citationCount": 195,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1908.03963",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-08-11",
            "journal": {
                "name": "Applied Intelligence",
                "volume": "53"
            },
            "citationStyles": {
                "bibtex": "@Article{Oroojlooyjadid2019ARO,\n author = {Afshin Oroojlooyjadid and Davood Hajinezhad},\n booktitle = {Applied intelligence (Boston)},\n journal = {Applied Intelligence},\n pages = {13677 - 13722},\n title = {A review of cooperative multi-agent deep reinforcement learning},\n volume = {53},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3660f76126fe1343c91f065f452845981041206c",
            "@type": "ScholarlyArticle",
            "paperId": "3660f76126fe1343c91f065f452845981041206c",
            "corpusId": 77394159,
            "url": "https://www.semanticscholar.org/paper/3660f76126fe1343c91f065f452845981041206c",
            "title": "A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems",
            "venue": "Journal of Artificial Intelligence Research",
            "publicationVenue": {
                "id": "urn:research:aef12dca-60a0-4ca3-819b-cad26d309d4e",
                "name": "Journal of Artificial Intelligence Research",
                "alternate_names": [
                    "JAIR",
                    "J Artif Intell Res",
                    "The Journal of Artificial Intelligence Research"
                ],
                "issn": "1076-9757",
                "url": "http://www.jair.org/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/jair/SilvaC19",
                "MAG": "2921955147",
                "DOI": "10.1613/jair.1.11396",
                "CorpusId": 77394159
            },
            "abstract": "\n \n \nMultiagent Reinforcement Learning (RL) solves complex tasks that require coordination with other agents through autonomous exploration of the environment. However, learning a complex task from scratch is impractical due to the huge sample complexity of RL algorithms. For this reason, reusing knowledge that can come from previous experience or other agents is indispensable to scale up multiagent RL algorithms. This survey provides a unifying view of the literature on knowledge reuse in multiagent RL. We define a taxonomy of solutions for the general knowledge reuse problem, providing a comprehensive discussion of recent progress on knowledge reuse in Multiagent Systems (MAS) and of techniques for knowledge reuse across agents (that may be actuating in a shared environment or not). We aim at encouraging the community to work towards reusing all the knowledge sources available in a MAS. For that, we provide an in-depth discussion of current lines of research and open questions. \n \n \n",
            "referenceCount": 0,
            "citationCount": 188,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jair.org/index.php/jair/article/download/11396/26482",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-03-11",
            "journal": {
                "name": "J. Artif. Intell. Res.",
                "volume": "64"
            },
            "citationStyles": {
                "bibtex": "@Article{Silva2019ASO,\n author = {Felipe Leno da Silva and Anna Helena Reali Costa},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {J. Artif. Intell. Res.},\n pages = {645-703},\n title = {A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems},\n volume = {64},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:232bd009964a9bbf4757227aae94d1a139350e04",
            "@type": "ScholarlyArticle",
            "paperId": "232bd009964a9bbf4757227aae94d1a139350e04",
            "corpusId": 201666696,
            "url": "https://www.semanticscholar.org/paper/232bd009964a9bbf4757227aae94d1a139350e04",
            "title": "OpenSpiel: A Framework for Reinforcement Learning in Games",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1908.09453",
                "MAG": "2969287672",
                "DBLP": "journals/corr/abs-1908-09453",
                "CorpusId": 201666696
            },
            "abstract": "OpenSpiel is a collection of environments and algorithms for research in general reinforcement learning and search/planning in games. OpenSpiel supports n-player (single- and multi- agent) zero-sum, cooperative and general-sum, one-shot and sequential, strictly turn-taking and simultaneous-move, perfect and imperfect information games, as well as traditional multiagent environments such as (partially- and fully- observable) grid worlds and social dilemmas. OpenSpiel also includes tools to analyze learning dynamics and other common evaluation metrics. This document serves both as an overview of the code base and an introduction to the terminology, core concepts, and algorithms across the fields of reinforcement learning, computational game theory, and search.",
            "referenceCount": 89,
            "citationCount": 188,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-08-26",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1908.09453"
            },
            "citationStyles": {
                "bibtex": "@Article{Lanctot2019OpenSpielAF,\n author = {Marc Lanctot and Edward Lockhart and Jean-Baptiste Lespiau and V. Zambaldi and Satyaki Upadhyay and J. P\u00e9rolat and S. Srinivasan and Finbarr Timbers and K. Tuyls and Shayegan Omidshafiei and Daniel Hennes and Dustin Morrill and Paul Muller and Timo Ewalds and Ryan Faulkner and J\u00e1nos Kram\u00e1r and B. D. Vylder and Brennan Saeta and James Bradbury and David Ding and Sebastian Borgeaud and Matthew Lai and Julian Schrittwieser and Thomas W. Anthony and Edward Hughes and Ivo Danihelka and Jonah Ryan-Davis},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {OpenSpiel: A Framework for Reinforcement Learning in Games},\n volume = {abs/1908.09453},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:486c93a171650cdf1fff68cbbe646393517fca36",
            "@type": "ScholarlyArticle",
            "paperId": "486c93a171650cdf1fff68cbbe646393517fca36",
            "corpusId": 195767325,
            "url": "https://www.semanticscholar.org/paper/486c93a171650cdf1fff68cbbe646393517fca36",
            "title": "Variational Quantum Circuits for Deep Reinforcement Learning",
            "venue": "IEEE Access",
            "publicationVenue": {
                "id": "urn:research:2633f5b2-c15c-49fe-80f5-07523e770c26",
                "name": "IEEE Access",
                "alternate_names": null,
                "issn": "2169-3536",
                "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1907.00397",
                "MAG": "3045093737",
                "DBLP": "journals/access/ChenYQCMG20",
                "DOI": "10.1109/access.2020.3010470",
                "CorpusId": 195767325
            },
            "abstract": "The state-of-the-art machine learning approaches are based on classical von Neumann computing architectures and have been widely used in many industrial and academic domains. With the recent development of quantum computing, researchers and tech-giants have attempted new quantum circuits for machine learning tasks. However, the existing quantum computing platforms are hard to simulate classical deep learning models or problems because of the intractability of deep quantum circuits. Thus, it is necessary to design feasible quantum algorithms for quantum machine learning for noisy intermediate scale quantum (NISQ) devices. This work explores variational quantum circuits for deep reinforcement learning. Specifically, we reshape classical deep reinforcement learning algorithms like experience replay and target network into a representation of variational quantum circuits. Moreover, we use a quantum information encoding scheme to reduce the number of model parameters compared to classical neural networks. To the best of our knowledge, this work is the first proof-of-principle demonstration of variational quantum circuits to approximate the deep $Q$ -value function for decision-making and policy-selection reinforcement learning with experience replay and target network. Besides, our variational quantum circuits can be deployed in many near-term NISQ machines.",
            "referenceCount": 57,
            "citationCount": 181,
            "influentialCitationCount": 18,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09144562.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-06-30",
            "journal": {
                "name": "IEEE Access",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2019VariationalQC,\n author = {Samuel Yen-Chi Chen and C. Yang and Jun Qi and Pin-Yu Chen and Xiaoli Ma and H. Goan},\n booktitle = {IEEE Access},\n journal = {IEEE Access},\n pages = {141007-141024},\n title = {Variational Quantum Circuits for Deep Reinforcement Learning},\n volume = {8},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8e98719529f8e029210b6c31d54e7486ee00d0af",
            "@type": "ScholarlyArticle",
            "paperId": "8e98719529f8e029210b6c31d54e7486ee00d0af",
            "corpusId": 203902511,
            "url": "https://www.semanticscholar.org/paper/8e98719529f8e029210b6c31d54e7486ee00d0af",
            "title": "Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2995638039",
                "DBLP": "journals/corr/abs-1910-03016",
                "ArXiv": "1910.03016",
                "CorpusId": 203902511
            },
            "abstract": "Modern deep learning methods provide effective means to learn good representations. However, is a good representation itself sufficient for sample efficient reinforcement learning? This question has largely been studied only with respect to (worst-case) approximation error, in the more classical approximate dynamic programming literature. With regards to the statistical viewpoint, this question is largely unexplored, and the extant body of literature mainly focuses on conditions which permit sample efficient reinforcement learning with little understanding of what are necessary conditions for efficient reinforcement learning. \nThis work shows that, from the statistical viewpoint, the situation is far subtler than suggested by the more traditional approximation viewpoint, where the requirements on the representation that suffice for sample efficient RL are even more stringent. Our main results provide sharp thresholds for reinforcement learning methods, showing that there are hard limitations on what constitutes good function approximation (in terms of the dimensionality of the representation), where we focus on natural representational conditions relevant to value-based, model-based, and policy-based learning. These lower bounds highlight that having a good (value-based, model-based, or policy-based) representation in and of itself is insufficient for efficient reinforcement learning, unless the quality of this approximation passes certain hard thresholds. Furthermore, our lower bounds also imply exponential separations on the sample complexity between 1) value-based learning with perfect representation and value-based learning with a good-but-not-perfect representation, 2) value-based learning and policy-based learning, 3) policy-based learning and supervised learning and 4) reinforcement learning and imitation learning.",
            "referenceCount": 55,
            "citationCount": 170,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-10-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1910.03016"
            },
            "citationStyles": {
                "bibtex": "@Article{Du2019IsAG,\n author = {S. Du and S. Kakade and Ruosong Wang and Lin F. Yang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?},\n volume = {abs/1910.03016},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:53f8181a5a414f77e3d887bb20878178f3e8f859",
            "@type": "ScholarlyArticle",
            "paperId": "53f8181a5a414f77e3d887bb20878178f3e8f859",
            "corpusId": 204904886,
            "url": "https://www.semanticscholar.org/paper/53f8181a5a414f77e3d887bb20878178f3e8f859",
            "title": "Entity Abstraction in Visual Model-Based Reinforcement Learning",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2982246413",
                "DBLP": "journals/corr/abs-1910-12827",
                "ArXiv": "1910.12827",
                "CorpusId": 204904886
            },
            "abstract": "This paper tests the hypothesis that modeling a scene in terms of entities and their local interactions, as opposed to modeling the scene globally, provides a significant benefit in generalizing to physical tasks in a combinatorial space the learner has not encountered before. We present object-centric perception, prediction, and planning (OP3), which to the best of our knowledge is the first fully probabilistic entity-centric dynamic latent variable framework for model-based reinforcement learning that acquires entity representations from raw visual observations without supervision and uses them to predict and plan. OP3 enforces entity-abstraction -- symmetric processing of each entity representation with the same locally-scoped function -- which enables it to scale to model different numbers and configurations of objects from those in training. Our approach to solving the key technical challenge of grounding these entity representations to actual objects in the environment is to frame this variable binding problem as an inference problem, and we develop an interactive inference algorithm that uses temporal continuity and interactive feedback to bind information about object properties to the entity variables. On block-stacking tasks, OP3 generalizes to novel block configurations and more objects than observed during training, outperforming an oracle model that assumes access to object supervision and achieving two to three times better accuracy than a state-of-the-art video prediction model that does not exhibit entity abstraction.",
            "referenceCount": 69,
            "citationCount": 157,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-10-28",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1910.12827"
            },
            "citationStyles": {
                "bibtex": "@Article{Veerapaneni2019EntityAI,\n author = {Rishi Veerapaneni and John D. Co-Reyes and Michael Chang and Michael Janner and Chelsea Finn and Jiajun Wu and J. Tenenbaum and S. Levine},\n booktitle = {Conference on Robot Learning},\n journal = {ArXiv},\n title = {Entity Abstraction in Visual Model-Based Reinforcement Learning},\n volume = {abs/1910.12827},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f844925066c3de9cd9ad662059e25a9d47a59843",
            "@type": "ScholarlyArticle",
            "paperId": "f844925066c3de9cd9ad662059e25a9d47a59843",
            "corpusId": 184486852,
            "url": "https://www.semanticscholar.org/paper/f844925066c3de9cd9ad662059e25a9d47a59843",
            "title": "Causal Discovery with Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1906.04477",
                "DBLP": "conf/iclr/ZhuNC20",
                "MAG": "2949784712",
                "CorpusId": 184486852
            },
            "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are usually less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows a flexible score function under the acyclicity constraint.",
            "referenceCount": 55,
            "citationCount": 175,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-06-11",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1906.04477"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhu2019CausalDW,\n author = {Shengyu Zhu and Ignavier Ng and Zhitang Chen},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Causal Discovery with Reinforcement Learning},\n volume = {abs/1906.04477},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:86a1a83d5ffc3875a523cb8f6ce416b70c38e984",
            "@type": "ScholarlyArticle",
            "paperId": "86a1a83d5ffc3875a523cb8f6ce416b70c38e984",
            "corpusId": 153312553,
            "url": "https://www.semanticscholar.org/paper/86a1a83d5ffc3875a523cb8f6ce416b70c38e984",
            "title": "CityFlow: A Multi-Agent Reinforcement Learning Environment for Large Scale City Traffic Scenario",
            "venue": "The Web Conference",
            "publicationVenue": {
                "id": "urn:research:e07422f9-c065-40c3-a37b-75e98dce79fe",
                "name": "The Web Conference",
                "alternate_names": [
                    "Web Conf",
                    "WWW"
                ],
                "issn": null,
                "url": "http://www.iw3c2.org/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1905-05217",
                "MAG": "2945991855",
                "ArXiv": "1905.05217",
                "DOI": "10.1145/3308558.3314139",
                "CorpusId": 153312553
            },
            "abstract": "Traffic signal control is an emerging application scenario for reinforcement learning. Besides being as an important problem that affects people's daily life in commuting, traffic signal control poses its unique challenges for reinforcement learning in terms of adapting to dynamic traffic environment and coordinating thousands of agents including vehicles and pedestrians. A key factor in the success of modern reinforcement learning relies on a good simulator to generate a large number of data samples for learning. The most commonly used open-source traffic simulator SUMO is, however, not scalable to large road network and large traffic flow, which hinders the study of reinforcement learning on traffic scenarios. This motivates us to create a new traffic simulator CityFlow with fundamentally optimized data structures and efficient algorithms. CityFlow can support flexible definitions for road network and traffic flow based on synthetic and real-world data. It also provides user-friendly interface for reinforcement learning. Most importantly, CityFlow is more than twenty times faster than SUMO and is capable of supporting city-wide traffic simulation with an interactive render for monitoring. Besides traffic signal control, CityFlow could serve as the base for other transportation studies and can create new possibilities to test machine learning methods in the intelligent transportation domain.",
            "referenceCount": 14,
            "citationCount": 171,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1905.05217",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2019-05-13",
            "journal": {
                "name": "The World Wide Web Conference",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2019CityFlowAM,\n author = {Huichu Zhang and Siyuan Feng and Chang Liu and Yaoyao Ding and Yichen Zhu and Zihan Zhou and Weinan Zhang and Yong Yu and Haiming Jin and Z. Li},\n booktitle = {The Web Conference},\n journal = {The World Wide Web Conference},\n title = {CityFlow: A Multi-Agent Reinforcement Learning Environment for Large Scale City Traffic Scenario},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:765dcaf34e182df21c2f4361aa073691e5902df0",
            "@type": "ScholarlyArticle",
            "paperId": "765dcaf34e182df21c2f4361aa073691e5902df0",
            "corpusId": 58981458,
            "url": "https://www.semanticscholar.org/paper/765dcaf34e182df21c2f4361aa073691e5902df0",
            "title": "Lifelong Federated Reinforcement Learning: A Learning Architecture for Navigation in Cloud Robotic Systems",
            "venue": "IEEE Robotics and Automation Letters",
            "publicationVenue": {
                "id": "urn:research:93c335b7-edf4-45f5-8ddc-7c5835154945",
                "name": "IEEE Robotics and Automation Letters",
                "alternate_names": [
                    "IEEE Robot Autom Lett"
                ],
                "issn": "2377-3766",
                "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "3105018016",
                "ArXiv": "1901.06455",
                "DBLP": "journals/ral/LiuWL19",
                "DOI": "10.1109/LRA.2019.2931179",
                "CorpusId": 58981458
            },
            "abstract": "This letter was motivated by the problem of how to make robots fuse and transfer their experience so that they can effectively use prior knowledge and quickly adapt to new environments. To address the problem, we present a learning architecture for navigation in cloud robotic systems: Lifelong Federated Reinforcement Learning (LFRL). In the letter, we propose a knowledge fusion algorithm for upgrading a shared model deployed on the cloud. Then, effective transfer learning methods in LFRL are introduced. LFRL is consistent with human cognitive science and fits well in cloud robotic systems. Experiments show that LFRL greatly improves the efficiency of reinforcement learning for robot navigation. The cloud robotic system deployment also shows that LFRL is capable of fusing prior knowledge. In addition, we release a cloud robotic navigation-learning website to provide the service based on LFRL: www.shared-robotics.com.",
            "referenceCount": 34,
            "citationCount": 178,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1901.06455",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-01-19",
            "journal": {
                "name": "IEEE Robotics and Automation Letters",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2019LifelongFR,\n author = {Boyi Liu and Lujia Wang and Ming Liu},\n booktitle = {IEEE Robotics and Automation Letters},\n journal = {IEEE Robotics and Automation Letters},\n pages = {4555-4562},\n title = {Lifelong Federated Reinforcement Learning: A Learning Architecture for Navigation in Cloud Robotic Systems},\n volume = {4},\n year = {2019}\n}\n"
            }
        }
    }
]