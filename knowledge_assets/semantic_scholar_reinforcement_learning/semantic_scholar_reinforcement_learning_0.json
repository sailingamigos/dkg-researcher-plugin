[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:811df72e210e20de99719539505da54762a11c6d",
            "@type": "ScholarlyArticle",
            "paperId": "811df72e210e20de99719539505da54762a11c6d",
            "corpusId": 28202810,
            "url": "https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d",
            "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2932819148",
                "ArXiv": "1801.01290",
                "DBLP": "conf/icml/HaarnojaZAL18",
                "CorpusId": 28202810
            },
            "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
            "referenceCount": 42,
            "citationCount": 5264,
            "influentialCitationCount": 1318,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-01-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1801.01290"
            },
            "citationStyles": {
                "bibtex": "@Article{Haarnoja2018SoftAO,\n author = {Tuomas Haarnoja and Aurick Zhou and P. Abbeel and S. Levine},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},\n volume = {abs/1801.01290},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:69e76e16740ed69f4dc55361a3d319ac2f1293dd",
            "@type": "ScholarlyArticle",
            "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
            "corpusId": 6875312,
            "url": "https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd",
            "title": "Asynchronous Methods for Deep Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/MnihBMGLHSK16",
                "MAG": "2964043796",
                "ArXiv": "1602.01783",
                "CorpusId": 6875312
            },
            "abstract": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",
            "referenceCount": 43,
            "citationCount": 7254,
            "influentialCitationCount": 1303,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-02-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Mnih2016AsynchronousMF,\n author = {Volodymyr Mnih and Adri\u00e0 Puigdom\u00e8nech Badia and Mehdi Mirza and Alex Graves and T. Lillicrap and Tim Harley and David Silver and K. Kavukcuoglu},\n booktitle = {International Conference on Machine Learning},\n pages = {1928-1937},\n title = {Asynchronous Methods for Deep Reinforcement Learning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:340f48901f72278f6bf78a04ee5b01df208cc508",
            "@type": "ScholarlyArticle",
            "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
            "corpusId": 205242740,
            "url": "https://www.semanticscholar.org/paper/340f48901f72278f6bf78a04ee5b01df208cc508",
            "title": "Human-level control through deep reinforcement learning",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/nature/MnihKSRVBGRFOPB15",
                "MAG": "2145339207",
                "DOI": "10.1038/nature14236",
                "CorpusId": 205242740,
                "PubMed": "25719670"
            },
            "abstract": null,
            "referenceCount": 38,
            "citationCount": 22188,
            "influentialCitationCount": 3289,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-02-25",
            "journal": {
                "name": "Nature",
                "volume": "518"
            },
            "citationStyles": {
                "bibtex": "@Article{Mnih2015HumanlevelCT,\n author = {Volodymyr Mnih and K. Kavukcuoglu and David Silver and Andrei A. Rusu and J. Veness and Marc G. Bellemare and Alex Graves and Martin A. Riedmiller and A. Fidjeland and Georg Ostrovski and Stig Petersen and Charlie Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and D. Kumaran and Daan Wierstra and S. Legg and D. Hassabis},\n booktitle = {Nature},\n journal = {Nature},\n pages = {529-533},\n title = {Human-level control through deep reinforcement learning},\n volume = {518},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:67d968c7450878190e45ac7886746de867bf673d",
            "@type": "ScholarlyArticle",
            "paperId": "67d968c7450878190e45ac7886746de867bf673d",
            "corpusId": 12713052,
            "url": "https://www.semanticscholar.org/paper/67d968c7450878190e45ac7886746de867bf673d",
            "title": "Neural Architecture Search with Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2952431534",
                "ArXiv": "1611.01578",
                "DBLP": "journals/corr/ZophL16",
                "CorpusId": 12713052
            },
            "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.",
            "referenceCount": 74,
            "citationCount": 4573,
            "influentialCitationCount": 593,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.01578"
            },
            "citationStyles": {
                "bibtex": "@Article{Zoph2016NeuralAS,\n author = {Barret Zoph and Quoc V. Le},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Neural Architecture Search with Reinforcement Learning},\n volume = {abs/1611.01578},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2319a491378867c7049b3da055c5df60e1671158",
            "@type": "ScholarlyArticle",
            "paperId": "2319a491378867c7049b3da055c5df60e1671158",
            "corpusId": 15238391,
            "url": "https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158",
            "title": "Playing Atari with Deep Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "journals/corr/MnihKSGAWR13",
                "MAG": "1757796397",
                "ArXiv": "1312.5602",
                "CorpusId": 15238391
            },
            "abstract": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.",
            "referenceCount": 32,
            "citationCount": 9735,
            "influentialCitationCount": 1495,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-12-19",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1312.5602"
            },
            "citationStyles": {
                "bibtex": "@Article{Mnih2013PlayingAW,\n author = {Volodymyr Mnih and K. Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin A. Riedmiller},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Playing Atari with Deep Reinforcement Learning},\n volume = {abs/1312.5602},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6337bd57197c818fb5ab539004d64b0cf51ce2d5",
            "@type": "ScholarlyArticle",
            "paperId": "6337bd57197c818fb5ab539004d64b0cf51ce2d5",
            "corpusId": 2535201,
            "url": "https://www.semanticscholar.org/paper/6337bd57197c818fb5ab539004d64b0cf51ce2d5",
            "title": "Reinforcement learning",
            "venue": "Scholarpedia",
            "publicationVenue": {
                "id": "urn:research:856e61df-ae80-4713-a1f1-6afd81e6e2b1",
                "name": "Scholarpedia",
                "alternate_names": null,
                "issn": "1941-6016",
                "url": "http://www.scholarpedia.org/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "reference/ml/Stone10a",
                "DOI": "10.4249/scholarpedia.1448",
                "CorpusId": 2535201
            },
            "abstract": "The discussion here considers a much more common learning condition where an agent, such as a human or a robot, has to learn to make decisions in the environment from simple feedback. Such feedback is provided only after periods of actions in the form of reward or punishment without detailing which of the actions has contributed to the outcome. This type of learning scenario is called reinforcement learning. This learning problem is formalized in a Markov decision-making process with a variety of related algorithms. The second part of this chapter will use function approximators with neural networks which have made recent progress as deep reinforcement learning.",
            "referenceCount": 114,
            "citationCount": 3264,
            "influentialCitationCount": 223,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-08-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{W\u00f6rg\u00f6tter2019ReinforcementL,\n author = {F. W\u00f6rg\u00f6tter and B. Porr},\n booktitle = {Scholarpedia},\n pages = {1088-1090},\n title = {Reinforcement learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:024006d4c2a89f7acacc6e4438d156525b60a98f",
            "@type": "ScholarlyArticle",
            "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
            "corpusId": 16326763,
            "url": "https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f",
            "title": "Continuous control with deep reinforcement learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2173248099",
                "ArXiv": "1509.02971",
                "DBLP": "journals/corr/LillicrapHPHETS15",
                "CorpusId": 16326763
            },
            "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
            "referenceCount": 34,
            "citationCount": 10207,
            "influentialCitationCount": 2177,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-09-09",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1509.02971"
            },
            "citationStyles": {
                "bibtex": "@Article{Lillicrap2015ContinuousCW,\n author = {T. Lillicrap and Jonathan J. Hunt and A. Pritzel and N. Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Continuous control with deep reinforcement learning},\n volume = {abs/1509.02971},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
            "@type": "ScholarlyArticle",
            "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
            "corpusId": 6208256,
            "url": "https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
            "title": "Deep Reinforcement Learning with Double Q-Learning",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/corr/HasseltGS15",
                "MAG": "2952523895",
                "ArXiv": "1509.06461",
                "DOI": "10.1609/aaai.v30i1.10295",
                "CorpusId": 6208256
            },
            "abstract": "\n \n The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.\n \n",
            "referenceCount": 29,
            "citationCount": 5634,
            "influentialCitationCount": 953,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10295/10154",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-09-22",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hasselt2015DeepRL,\n author = {H. V. Hasselt and A. Guez and David Silver},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {2094-2100},\n title = {Deep Reinforcement Learning with Double Q-Learning},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:361c00b22e29d0816ca896513d2c165e26399821",
            "@type": "ScholarlyArticle",
            "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
            "corpusId": 204972004,
            "url": "https://www.semanticscholar.org/paper/361c00b22e29d0816ca896513d2c165e26399821",
            "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2982316857",
                "DBLP": "journals/nature/VinyalsBCMDCCPE19",
                "DOI": "10.1038/s41586-019-1724-z",
                "CorpusId": 204972004,
                "PubMed": "31666705"
            },
            "abstract": null,
            "referenceCount": 59,
            "citationCount": 2709,
            "influentialCitationCount": 135,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-10-30",
            "journal": {
                "name": "Nature",
                "volume": "575"
            },
            "citationStyles": {
                "bibtex": "@Article{Vinyals2019GrandmasterLI,\n author = {Oriol Vinyals and Igor Babuschkin and Wojciech M. Czarnecki and Micha\u00ebl Mathieu and Andrew Dudzik and Junyoung Chung and David H. Choi and Richard Powell and Timo Ewalds and Petko Georgiev and Junhyuk Oh and Dan Horgan and M. Kroiss and Ivo Danihelka and Aja Huang and L. Sifre and Trevor Cai and J. Agapiou and Max Jaderberg and A. Vezhnevets and R\u00e9mi Leblond and Tobias Pohlen and Valentin Dalibard and D. Budden and Yury Sulsky and James Molloy and T. Paine and Caglar Gulcehre and Ziyun Wang and T. Pfaff and Yuhuai Wu and Roman Ring and Dani Yogatama and Dario W\u00fcnsch and Katrina McKinney and Oliver Smith and T. Schaul and T. Lillicrap and K. Kavukcuoglu and D. Hassabis and C. Apps and David Silver},\n booktitle = {Nature},\n journal = {Nature},\n pages = {350 - 354},\n title = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},\n volume = {575},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0286b2736a114198b25fb5553c671c33aed5d477",
            "@type": "ScholarlyArticle",
            "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
            "corpusId": 248118878,
            "url": "https://www.semanticscholar.org/paper/0286b2736a114198b25fb5553c671c33aed5d477",
            "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2022,
            "externalIds": {
                "ArXiv": "2204.05862",
                "DBLP": "journals/corr/abs-2204-05862",
                "DOI": "10.48550/arXiv.2204.05862",
                "CorpusId": 248118878
            },
            "abstract": "We apply preference modeling and reinforcement learning from human feedback (RLHF) to \ufb01netune language models to act as helpful and harmless assistants. We \ufb01nd this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, ef\ufb01ciently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work. Figure These plots show that PM accuracy decreases as we focus exclusively on comparisons between pairs of samples with high score. We have normalized all preference models to have the same mean score on a held-out dataset so that they\u2019re directly comparable, and then plotted accuracy for the comparisons where both samples have scores above a speci\ufb01c threshold.",
            "referenceCount": 72,
            "citationCount": 528,
            "influentialCitationCount": 69,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2204.05862",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-04-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2204.05862"
            },
            "citationStyles": {
                "bibtex": "@Article{Bai2022TrainingAH,\n author = {Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and T. Henighan and Nicholas Joseph and Saurav Kadavath and John Kernion and Tom Conerly and S. El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and S. Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom B. Brown and Jack Clark and Sam McCandlish and C. Olah and Benjamin Mann and Jared Kaplan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},\n volume = {abs/2204.05862},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:aee6d6b3282662b69a1020c95be725e0075428bd",
            "@type": "ScholarlyArticle",
            "paperId": "aee6d6b3282662b69a1020c95be725e0075428bd",
            "corpusId": 60035920,
            "url": "https://www.semanticscholar.org/paper/aee6d6b3282662b69a1020c95be725e0075428bd",
            "title": "Reinforcement Learning: An Introduction",
            "venue": "IEEE Transactions on Neural Networks",
            "publicationVenue": {
                "id": "urn:research:2ac50919-507e-41c7-93a8-721c4b804757",
                "name": "IEEE Transactions on Neural Networks",
                "alternate_names": [
                    "IEEE Trans Neural Netw"
                ],
                "issn": "1045-9227",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=72"
            },
            "year": 2005,
            "externalIds": {
                "DOI": "10.1109/tnn.2004.842673",
                "CorpusId": 60035920
            },
            "abstract": "Overview of the volume In the research area of artificial intelligence (AI) a branch that is becoming more and more important is reinforcement learning (RL). RL can be defined as learning how to map situations into actions interacting with the environment, so as to maximize a reward. Written by two pioneers in this field, this book aims at supplying the basic RL ideas and algorithms. Even if the main point of view is the AI and engineering perspective, the Sutton\u2013Barto book was designed to be accessible to readers of different disciplines. It turns out that the level of mathematical knowledge required to understand the material is not too deep and requires familiarity only with elementary notions of probability. The book has been divided into three parts. The Problem (three chapters) is the introductory part devoted to the problem description. Elementary Solution Methods (three chapters) describes the most important elementary solution methods in authors' opinion: dynamic programming (DP), simple Monte Carlo (MC) methods, and temporal-difference (TD) learning. A Unified View (five chapters) concerns a generalization of the previous methods, gives a unified view of RL, and provides some examples of real RL applications. Each chapter has many examples and exercises. Sections and exercises marked with a star (*) can be skipped during a first reading. At the end of every chapter there is a very interesting section dedicated to bibliographical and historical remarks. Chapter 1: Introduction The first chapter, an introduction to RL, emphasizes its main characteristics: interaction with environment, goal-directed learning, rewards, trade-off between exploration and exploitation. The four subelements into which a RL system can be subdivided are well explained: a policy, a reward function, a value function and, optionally, an environment model. Chapter 2: Evaluative Feedback One of the main characteristics of RL is that it uses training information that evaluates actions. This is well exemplified by the n-armed bandit problem described in this chapter, which in addition introduces the basic learning methods that will be used in the rest of the book. Concepts like action estimation, update rule, and greedy action selection become familiar. Chapter 3: The Reinforcement Learning Problem This is certainly the most important chapter of the first part, since it describes the RL problem and gives its mathematical formalization. For simplicity, the authors expressly consider discrete time steps and do not extend the treatment to the continuous time case. The Markov decision processes (MDPs), \u2026",
            "referenceCount": 0,
            "citationCount": 13852,
            "influentialCitationCount": 2318,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "IEEE Transactions on Neural Networks",
                "volume": "16"
            },
            "citationStyles": {
                "bibtex": "@Article{Sutton2005ReinforcementLA,\n author = {R. S. Sutton},\n booktitle = {IEEE Transactions on Neural Networks},\n journal = {IEEE Transactions on Neural Networks},\n pages = {285-286},\n title = {Reinforcement Learning: An Introduction},\n volume = {16},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f9717d29840f4d8f1cc19d1b1e80c5d12ec40608",
            "@type": "ScholarlyArticle",
            "paperId": "f9717d29840f4d8f1cc19d1b1e80c5d12ec40608",
            "corpusId": 54457125,
            "url": "https://www.semanticscholar.org/paper/f9717d29840f4d8f1cc19d1b1e80c5d12ec40608",
            "title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play",
            "venue": "Science",
            "publicationVenue": {
                "id": "urn:research:f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
                "name": "Science",
                "alternate_names": null,
                "issn": "0193-4511",
                "url": "https://www.jstor.org/journal/science"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2902907165",
                "DOI": "10.1126/science.aar6404",
                "CorpusId": 54457125,
                "PubMed": "30523106"
            },
            "abstract": "One program to rule them all Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system. Science, this issue p. 1140; see also pp. 1087 and 1118 AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each. The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.",
            "referenceCount": 54,
            "citationCount": 2657,
            "influentialCitationCount": 152,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://science.sciencemag.org/content/sci/362/6419/1140.full.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-12-07",
            "journal": {
                "name": "Science",
                "volume": "362"
            },
            "citationStyles": {
                "bibtex": "@Article{Silver2018AGR,\n author = {David Silver and T. Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and A. Guez and Marc Lanctot and L. Sifre and D. Kumaran and T. Graepel and T. Lillicrap and K. Simonyan and D. Hassabis},\n booktitle = {Science},\n journal = {Science},\n pages = {1140 - 1144},\n title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},\n volume = {362},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
            "@type": "ScholarlyArticle",
            "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
            "corpusId": 235294299,
            "url": "https://www.semanticscholar.org/paper/c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
            "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2106-01345",
                "MAG": "3169291081",
                "ArXiv": "2106.01345",
                "CorpusId": 235294299
            },
            "abstract": "We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.",
            "referenceCount": 90,
            "citationCount": 767,
            "influentialCitationCount": 158,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-06-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2021DecisionTR,\n author = {Lili Chen and Kevin Lu and A. Rajeswaran and Kimin Lee and Aditya Grover and M. Laskin and P. Abbeel and A. Srinivas and Igor Mordatch},\n booktitle = {Neural Information Processing Systems},\n pages = {15084-15097},\n title = {Decision Transformer: Reinforcement Learning via Sequence Modeling},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c879b25308026d6538e52b27bcf4fd3cb60855f3",
            "@type": "ScholarlyArticle",
            "paperId": "c879b25308026d6538e52b27bcf4fd3cb60855f3",
            "corpusId": 235422620,
            "url": "https://www.semanticscholar.org/paper/c879b25308026d6538e52b27bcf4fd3cb60855f3",
            "title": "A Minimalist Approach to Offline Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "conf/nips/FujimotoG21",
                "ArXiv": "2106.06860",
                "CorpusId": 235422620
            },
            "abstract": "Offline reinforcement learning (RL) defines the task of learning from a fixed batch of data. Due to errors in value estimation from out-of-distribution actions, most offline RL algorithms take the approach of constraining or regularizing the policy with the actions contained in the dataset. Built on pre-existing RL algorithms, modifications to make an RL algorithm work offline comes at the cost of additional complexity. Offline RL algorithms introduce new hyperparameters and often leverage secondary components such as generative models, while adjusting the underlying RL algorithm. In this paper we aim to make a deep RL algorithm work while making minimal changes. We find that we can match the performance of state-of-the-art offline RL algorithms by simply adding a behavior cloning term to the policy update of an online RL algorithm and normalizing the data. The resulting algorithm is a simple to implement and tune baseline, while more than halving the overall run time by removing the additional computational overhead of previous methods.",
            "referenceCount": 64,
            "citationCount": 387,
            "influentialCitationCount": 137,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-06-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2106.06860"
            },
            "citationStyles": {
                "bibtex": "@Article{Fujimoto2021AMA,\n author = {Scott Fujimoto and S. Gu},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {A Minimalist Approach to Offline Reinforcement Learning},\n volume = {abs/2106.06860},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:348a855fe01f3f4273bf0ecf851ca688686dbfcc",
            "@type": "ScholarlyArticle",
            "paperId": "348a855fe01f3f4273bf0ecf851ca688686dbfcc",
            "corpusId": 238634325,
            "url": "https://www.semanticscholar.org/paper/348a855fe01f3f4273bf0ecf851ca688686dbfcc",
            "title": "Offline Reinforcement Learning with Implicit Q-Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2110.06169",
                "DBLP": "conf/iclr/KostrikovNL22",
                "CorpusId": 238634325
            },
            "abstract": "Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. This trade-off is critical, because most current offline reinforcement learning methods need to query the value of unseen actions during training to improve the policy, and therefore need to either constrain these actions to be in-distribution, or else regularize their values. We propose an offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization. The main insight in our work is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the dynamics to avoid excessive optimism), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state. This leverages the generalization capacity of the function approximator to estimate the value of the best available action at a given state without ever directly querying a Q-function with this unseen action. Our algorithm alternates between fitting this upper expectile value function and backing it up into a Q-function. Then, we extract the policy via advantage-weighted behavioral cloning. We dub our method implicit Q-learning (IQL). IQL demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline reinforcement learning. We also demonstrate that IQL achieves strong performance fine-tuning using online interaction after offline initialization.",
            "referenceCount": 35,
            "citationCount": 356,
            "influentialCitationCount": 141,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-10-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2110.06169"
            },
            "citationStyles": {
                "bibtex": "@Article{Kostrikov2021OfflineRL,\n author = {Ilya Kostrikov and Ashvin Nair and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Offline Reinforcement Learning with Implicit Q-Learning},\n volume = {abs/2110.06169},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:442ab95eb9cfbc03bb17a27b52313b5d25eaa738",
            "@type": "ScholarlyArticle",
            "paperId": "442ab95eb9cfbc03bb17a27b52313b5d25eaa738",
            "corpusId": 252717185,
            "url": "https://www.semanticscholar.org/paper/442ab95eb9cfbc03bb17a27b52313b5d25eaa738",
            "title": "Discovering faster matrix multiplication algorithms with reinforcement learning",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "journals/nature/FawziBHHRB0RSSS22",
                "PubMedCentral": "9534758",
                "DOI": "10.1038/s41586-022-05172-4",
                "CorpusId": 252717185,
                "PubMed": "36198780"
            },
            "abstract": null,
            "referenceCount": 71,
            "citationCount": 224,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41586-022-05172-4.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-10-01",
            "journal": {
                "name": "Nature",
                "volume": "610"
            },
            "citationStyles": {
                "bibtex": "@Article{Fawzi2022DiscoveringFM,\n author = {Alhussein Fawzi and Matej Balog and Aja Huang and T. Hubert and Bernardino Romera-Paredes and M. Barekatain and Alexander Novikov and Francisco J. R. Ruiz and Julian Schrittwieser and Grzegorz Swirszcz and David Silver and D. Hassabis and Pushmeet Kohli},\n booktitle = {Nature},\n journal = {Nature},\n pages = {47 - 53},\n title = {Discovering faster matrix multiplication algorithms with reinforcement learning},\n volume = {610},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e3fc5b5627af62ee6981a02090cf6bae368202d7",
            "@type": "ScholarlyArticle",
            "paperId": "e3fc5b5627af62ee6981a02090cf6bae368202d7",
            "corpusId": 246432496,
            "url": "https://www.semanticscholar.org/paper/e3fc5b5627af62ee6981a02090cf6bae368202d7",
            "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/jmlr/RaffinHGKED21",
                "CorpusId": 246432496
            },
            "abstract": "Stable-Baselines3 provides open-source implementations of deep reinforcement learning (RL) algorithms in Python. The implementations have been benchmarked against reference codebases, and automated unit tests cover 95% of the code. The algorithms follow a consistent interface and are accompanied by extensive documentation, making it simple to train and compare di\ufb00erent RL algorithms. Our documentation, examples, and source-code are available at https://github.com/DLR-RM/stable-baselines3 .",
            "referenceCount": 33,
            "citationCount": 738,
            "influentialCitationCount": 58,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Raffin2021StableBaselines3RR,\n author = {A. Raffin and Ashley Hill and A. Gleave and A. Kanervisto and M. Ernestus and Noah Dormann},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {268:1-268:8},\n title = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},\n volume = {22},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e7bc93622416f14e6948a500278bfbe58cd3890",
            "@type": "ScholarlyArticle",
            "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
            "corpusId": 218486979,
            "url": "https://www.semanticscholar.org/paper/5e7bc93622416f14e6948a500278bfbe58cd3890",
            "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2005-01643",
                "ArXiv": "2005.01643",
                "MAG": "3022566517",
                "CorpusId": 218486979
            },
            "abstract": "In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.",
            "referenceCount": 193,
            "citationCount": 1233,
            "influentialCitationCount": 118,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-05-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2005.01643"
            },
            "citationStyles": {
                "bibtex": "@Article{Levine2020OfflineRL,\n author = {S. Levine and Aviral Kumar and G. Tucker and Justin Fu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems},\n volume = {abs/2005.01643},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:558ca2e8c7eb56edd77a52b084e6cc24dffe5bcd",
            "@type": "ScholarlyArticle",
            "paperId": "558ca2e8c7eb56edd77a52b084e6cc24dffe5bcd",
            "corpusId": 237353084,
            "url": "https://www.semanticscholar.org/paper/558ca2e8c7eb56edd77a52b084e6cc24dffe5bcd",
            "title": "Deep Reinforcement Learning at the Edge of the Statistical Precipice",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "conf/nips/AgarwalSCCB21",
                "ArXiv": "2108.13264",
                "CorpusId": 237353084
            },
            "abstract": "Deep reinforcement learning (RL) algorithms are predominantly evaluated by comparing their relative performance on a large suite of tasks. Most published results on deep RL benchmarks compare point estimates of aggregate performance such as mean and median scores across tasks, ignoring the statistical uncertainty implied by the use of a finite number of training runs. Beginning with the Arcade Learning Environment (ALE), the shift towards computationally-demanding benchmarks has led to the practice of evaluating only a small number of runs per task, exacerbating the statistical uncertainty in point estimates. In this paper, we argue that reliable evaluation in the few run deep RL regime cannot ignore the uncertainty in results without running the risk of slowing down progress in the field. We illustrate this point using a case study on the Atari 100k benchmark, where we find substantial discrepancies between conclusions drawn from point estimates alone versus a more thorough statistical analysis. With the aim of increasing the field's confidence in reported results with a handful of runs, we advocate for reporting interval estimates of aggregate performance and propose performance profiles to account for the variability in results, as well as present more robust and efficient aggregate metrics, such as interquartile mean scores, to achieve small uncertainty in results. Using such statistical tools, we scrutinize performance evaluations of existing algorithms on other widely used RL benchmarks including the ALE, Procgen, and the DeepMind Control Suite, again revealing discrepancies in prior comparisons. Our findings call for a change in how we evaluate performance in deep RL, for which we present a more rigorous evaluation methodology, accompanied with an open-source library rliable, to prevent unreliable results from stagnating the field.",
            "referenceCount": 120,
            "citationCount": 332,
            "influentialCitationCount": 59,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-08-30",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Agarwal2021DeepRL,\n author = {Rishabh Agarwal and Max Schwarzer and P. S. Castro and Aaron C. Courville and Marc G. Bellemare},\n booktitle = {Neural Information Processing Systems},\n pages = {29304-29320},\n title = {Deep Reinforcement Learning at the Edge of the Statistical Precipice},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:88e488a08dcd629c8ce90099bd8ab0a87f10cafb",
            "@type": "ScholarlyArticle",
            "paperId": "88e488a08dcd629c8ce90099bd8ab0a87f10cafb",
            "corpusId": 246904229,
            "url": "https://www.semanticscholar.org/paper/88e488a08dcd629c8ce90099bd8ab0a87f10cafb",
            "title": "Magnetic control of tokamak plasmas through deep reinforcement learning",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2022,
            "externalIds": {
                "PubMedCentral": "8850200",
                "DBLP": "journals/nature/DegraveFBNTCEHA22",
                "DOI": "10.1038/s41586-021-04301-9",
                "CorpusId": 246904229,
                "PubMed": "35173339"
            },
            "abstract": null,
            "referenceCount": 47,
            "citationCount": 350,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41586-021-04301-9.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-02-01",
            "journal": {
                "name": "Nature",
                "volume": "602"
            },
            "citationStyles": {
                "bibtex": "@Article{Degrave2022MagneticCO,\n author = {Jonas Degrave and F. Felici and J. Buchli and Michael Neunert and Brendan D. Tracey and Francesco Carpanese and Timo Ewalds and Roland Hafner and A. Abdolmaleki and Diego de Las Casas and Craig Donner and Leslie Fritz and C. Galperti and Andrea Huber and James Keeling and Maria Tsimpoukelli and Jackie Kay and A. Merle and J. Moret and Seb Noury and F. Pesamosca and D. Pfau and O. Sauter and C. Sommariva and S. Coda and B. Duval and A. Fasoli and Pushmeet Kohli and K. Kavukcuoglu and D. Hassabis and Martin A. Riedmiller},\n booktitle = {Nature},\n journal = {Nature},\n pages = {414 - 419},\n title = {Magnetic control of tokamak plasmas through deep reinforcement learning},\n volume = {602},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:28db20a81eec74a50204686c3cf796c42a020d2e",
            "@type": "ScholarlyArticle",
            "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
            "corpusId": 219530894,
            "url": "https://www.semanticscholar.org/paper/28db20a81eec74a50204686c3cf796c42a020d2e",
            "title": "Conservative Q-Learning for Offline Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2006.04779",
                "DBLP": "journals/corr/abs-2006-04779",
                "MAG": "3102848167",
                "CorpusId": 219530894
            },
            "abstract": "Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with theoretical improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.",
            "referenceCount": 64,
            "citationCount": 996,
            "influentialCitationCount": 358,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-06-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2006.04779"
            },
            "citationStyles": {
                "bibtex": "@Article{Kumar2020ConservativeQF,\n author = {Aviral Kumar and Aurick Zhou and G. Tucker and S. Levine},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Conservative Q-Learning for Offline Reinforcement Learning},\n volume = {abs/2006.04779},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
            "@type": "ScholarlyArticle",
            "paperId": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
            "corpusId": 5389801,
            "url": "https://www.semanticscholar.org/paper/4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
            "title": "Dueling Network Architectures for Deep Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/icml/WangSHHLF16",
                "ArXiv": "1511.06581",
                "MAG": "2951799221",
                "CorpusId": 5389801
            },
            "abstract": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.",
            "referenceCount": 28,
            "citationCount": 2929,
            "influentialCitationCount": 485,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-11-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2015DuelingNA,\n author = {Ziyun Wang and T. Schaul and Matteo Hessel and H. V. Hasselt and Marc Lanctot and Nando de Freitas},\n booktitle = {International Conference on Machine Learning},\n pages = {1995-2003},\n title = {Dueling Network Architectures for Deep Reinforcement Learning},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b19729b27a1b4c24b52f87308c907653300afa7f",
            "@type": "ScholarlyArticle",
            "paperId": "b19729b27a1b4c24b52f87308c907653300afa7f",
            "corpusId": 209376771,
            "url": "https://www.semanticscholar.org/paper/b19729b27a1b4c24b52f87308c907653300afa7f",
            "title": "Dota 2 with Large Scale Deep Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2996037775",
                "DBLP": "journals/corr/abs-1912-06680",
                "ArXiv": "1912.06680",
                "CorpusId": 209376771
            },
            "abstract": "On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",
            "referenceCount": 50,
            "citationCount": 1314,
            "influentialCitationCount": 72,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-12-13",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1912.06680"
            },
            "citationStyles": {
                "bibtex": "@Article{Berner2019Dota2W,\n author = {Christopher Berner and Greg Brockman and Brooke Chan and Vicki Cheung and Przemyslaw Debiak and Christy Dennison and David Farhi and Quirin Fischer and Shariq Hashme and Christopher Hesse and R. J\u00f3zefowicz and S. Gray and Catherine Olsson and J. Pachocki and Michael Petrov and Henrique Pond\u00e9 de Oliveira Pinto and Jonathan Raiman and Tim Salimans and Jeremy Schlatter and Jonas Schneider and Szymon Sidor and Ilya Sutskever and Jie Tang and F. Wolski and Susan Zhang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Dota 2 with Large Scale Deep Reinforcement Learning},\n volume = {abs/1912.06680},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a326d9f2d2d351001fece788165dbcbb524da2e4",
            "@type": "ScholarlyArticle",
            "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
            "corpusId": 215827910,
            "url": "https://www.semanticscholar.org/paper/a326d9f2d2d351001fece788165dbcbb524da2e4",
            "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3016525976",
                "ArXiv": "2004.07219",
                "DBLP": "journals/corr/abs-2004-07219",
                "CorpusId": 215827910
            },
            "abstract": "The offline reinforcement learning (RL) problem, also known as batch RL, refers to the setting where a policy must be learned from a static dataset, without additional online data collection. This setting is compelling as potentially it allows RL methods to take advantage of large, pre-collected datasets, much like how the rise of large datasets has fueled results in supervised learning in recent years. However, existing online RL benchmarks are not tailored towards the offline setting, making progress in offline RL difficult to measure. In this work, we introduce benchmarks specifically designed for the offline setting, guided by key properties of datasets relevant to real-world applications of offline RL. Examples of such properties include: datasets generated via hand-designed controllers and human demonstrators, multi-objective datasets where an agent can perform different tasks in the same environment, and datasets consisting of a mixtures of policies. To facilitate research, we release our benchmark tasks and datasets with a comprehensive evaluation of existing algorithms and an evaluation protocol together with an open-source codebase. We hope that our benchmark will focus research effort on methods that drive improvements not just on simulated tasks, but ultimately on the kinds of real-world problems where offline RL will have the largest impact.",
            "referenceCount": 41,
            "citationCount": 753,
            "influentialCitationCount": 275,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-04-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2004.07219"
            },
            "citationStyles": {
                "bibtex": "@Article{Fu2020D4RLDF,\n author = {Justin Fu and Aviral Kumar and Ofir Nachum and G. Tucker and S. Levine},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {D4RL: Datasets for Deep Data-Driven Reinforcement Learning},\n volume = {abs/2004.07219},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:362cc80481b288874af0428107ab31e955dcf09f",
            "@type": "ScholarlyArticle",
            "paperId": "362cc80481b288874af0428107ab31e955dcf09f",
            "corpusId": 232233412,
            "url": "https://www.semanticscholar.org/paper/362cc80481b288874af0428107ab31e955dcf09f",
            "title": "Offline Reinforcement Learning with Fisher Divergence Critic Regularization",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2103-08050",
                "ArXiv": "2103.08050",
                "CorpusId": 232233412
            },
            "abstract": "Many modern approaches to offline Reinforcement Learning (RL) utilize behavior regularization, typically augmenting a model-free actor critic algorithm with a penalty measuring divergence of the policy from the offline data. In this work, we propose an alternative approach to encouraging the learned policy to stay close to the data, namely parameterizing the critic as the log-behavior-policy, which generated the offline data, plus a state-action value offset term, which can be learned using a neural network. Behavior regularization then corresponds to an appropriate regularizer on the offset term. We propose using a gradient penalty regularizer for the offset term and demonstrate its equivalence to Fisher divergence regularization, suggesting connections to the score matching and generative energy-based model literature. We thus term our resulting algorithm Fisher-BRC (Behavior Regularized Critic). On standard offline RL benchmarks, Fisher-BRC achieves both improved performance and faster convergence over existing state-of-the-art methods.",
            "referenceCount": 36,
            "citationCount": 189,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-03-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2103.08050"
            },
            "citationStyles": {
                "bibtex": "@Article{Kostrikov2021OfflineRL,\n author = {Ilya Kostrikov and Jonathan Tompson and R. Fergus and Ofir Nachum},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Offline Reinforcement Learning with Fisher Divergence Critic Regularization},\n volume = {abs/2103.08050},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d193b76178412e99e310e90668a498fa8fe11a5c",
            "@type": "ScholarlyArticle",
            "paperId": "d193b76178412e99e310e90668a498fa8fe11a5c",
            "corpusId": 61181574,
            "url": "https://www.semanticscholar.org/paper/d193b76178412e99e310e90668a498fa8fe11a5c",
            "title": "An Introduction to Reinforcement Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2257420332",
                "CorpusId": 61181574
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 3858,
            "influentialCitationCount": 380,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Carden2013AnIT,\n author = {S. Carden},\n title = {An Introduction to Reinforcement Learning},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:79e14a09ff070e06ab9df598ccd885b929164ef9",
            "@type": "ScholarlyArticle",
            "paperId": "79e14a09ff070e06ab9df598ccd885b929164ef9",
            "corpusId": 215415964,
            "url": "https://www.semanticscholar.org/paper/79e14a09ff070e06ab9df598ccd885b929164ef9",
            "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2004-04136",
                "MAG": "3015437096",
                "ArXiv": "2004.04136",
                "CorpusId": 215415964
            },
            "abstract": "We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at this https URL.",
            "referenceCount": 65,
            "citationCount": 767,
            "influentialCitationCount": 150,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-04-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2004.04136"
            },
            "citationStyles": {
                "bibtex": "@Article{Srinivas2020CURLCU,\n author = {A. Srinivas and M. Laskin and P. Abbeel},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {CURL: Contrastive Unsupervised Representations for Reinforcement Learning},\n volume = {abs/2004.04136},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4c915c1eecb217c123a36dc6d3ce52d12c742614",
            "@type": "ScholarlyArticle",
            "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
            "corpusId": 2332513,
            "url": "https://www.semanticscholar.org/paper/4c915c1eecb217c123a36dc6d3ce52d12c742614",
            "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1992,
            "externalIds": {
                "DBLP": "journals/ml/Williams92",
                "MAG": "2119717200",
                "DOI": "10.1007/BF00992696",
                "CorpusId": 2332513
            },
            "abstract": null,
            "referenceCount": 37,
            "citationCount": 7565,
            "influentialCitationCount": 1103,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/BF00992696.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1992-05-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Williams1992SimpleSG,\n author = {Ronald J. Williams},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {229-256},\n title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},\n volume = {8},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ca6096142016a2ba8133f6cb2c04ad30f5eae730",
            "@type": "ScholarlyArticle",
            "paperId": "ca6096142016a2ba8133f6cb2c04ad30f5eae730",
            "corpusId": 237635100,
            "url": "https://www.semanticscholar.org/paper/ca6096142016a2ba8133f6cb2c04ad30f5eae730",
            "title": "Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2109-11978",
                "ArXiv": "2109.11978",
                "CorpusId": 237635100
            },
            "abstract": "In this work, we present and study a training set-up that achieves fast policy generation for real-world robotic tasks by using massive parallelism on a single workstation GPU. We analyze and discuss the impact of different training algorithm components in the massively parallel regime on the final policy performance and training times. In addition, we present a novel game-inspired curriculum that is well suited for training with thousands of simulated robots in parallel. We evaluate the approach by training the quadrupedal robot ANYmal to walk on challenging terrain. The parallel approach allows training policies for flat terrain in under four minutes, and in twenty minutes for uneven terrain. This represents a speedup of multiple orders of magnitude compared to previous work. Finally, we transfer the policies to the real robot to validate the approach. We open-source our training code to help accelerate further research in the field of learned legged locomotion.",
            "referenceCount": 29,
            "citationCount": 205,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-09-24",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2109.11978"
            },
            "citationStyles": {
                "bibtex": "@Article{Rudin2021LearningTW,\n author = {N. Rudin and David Hoeller and Philipp Reist and M. Hutter},\n booktitle = {Conference on Robot Learning},\n journal = {ArXiv},\n title = {Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning},\n volume = {abs/2109.11978},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ffc211476f2e40e79466ffc198c919a97da3bb76",
            "@type": "ScholarlyArticle",
            "paperId": "ffc211476f2e40e79466ffc198c919a97da3bb76",
            "corpusId": 4533648,
            "url": "https://www.semanticscholar.org/paper/ffc211476f2e40e79466ffc198c919a97da3bb76",
            "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2951109555",
                "DBLP": "journals/corr/abs-1803-11485",
                "ArXiv": "1803.11485",
                "CorpusId": 4533648
            },
            "abstract": "In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.",
            "referenceCount": 77,
            "citationCount": 1303,
            "influentialCitationCount": 387,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-03-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1803.11485"
            },
            "citationStyles": {
                "bibtex": "@Article{Rashid2018QMIXMV,\n author = {Tabish Rashid and Mikayel Samvelyan and C. S. D. Witt and Gregory Farquhar and Jakob N. Foerster and Shimon Whiteson},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},\n volume = {abs/1803.11485},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6568423cfaca7e24c88ea208cb0e67129e43aa9b",
            "@type": "ScholarlyArticle",
            "paperId": "6568423cfaca7e24c88ea208cb0e67129e43aa9b",
            "corpusId": 216562627,
            "url": "https://www.semanticscholar.org/paper/6568423cfaca7e24c88ea208cb0e67129e43aa9b",
            "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3023640063",
                "ArXiv": "2004.13649",
                "DBLP": "journals/corr/abs-2004-13649",
                "CorpusId": 216562627
            },
            "abstract": "We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at this https URL.",
            "referenceCount": 71,
            "citationCount": 529,
            "influentialCitationCount": 122,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-04-28",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2004.13649"
            },
            "citationStyles": {
                "bibtex": "@Article{Kostrikov2020ImageAI,\n author = {Ilya Kostrikov and Denis Yarats and R. Fergus},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels},\n volume = {abs/2004.13649},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a50f7bcf8a998f3de11bc085b0f4dea32be19783",
            "@type": "ScholarlyArticle",
            "paperId": "a50f7bcf8a998f3de11bc085b0f4dea32be19783",
            "corpusId": 232014612,
            "url": "https://www.semanticscholar.org/paper/a50f7bcf8a998f3de11bc085b0f4dea32be19783",
            "title": "Reinforcement Learning with Prototypical Representations",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2102-11271",
                "ArXiv": "2102.11271",
                "CorpusId": 232014612
            },
            "abstract": "Learning effective representations in image-based environments is crucial for sample efficient Reinforcement Learning (RL). Unfortunately, in RL, representation learning is confounded with the exploratory experience of the agent -- learning a useful representation requires diverse data, while effective exploration is only possible with coherent representations. Furthermore, we would like to learn representations that not only generalize across tasks but also accelerate downstream exploration for efficient task-specific training. To address these challenges we propose Proto-RL, a self-supervised framework that ties representation learning with exploration through prototypical representations. These prototypes simultaneously serve as a summarization of the exploratory experience of an agent as well as a basis for representing observations. We pre-train these task-agnostic representations and prototypes on environments without downstream task information. This enables state-of-the-art downstream policy learning on a set of difficult continuous control tasks.",
            "referenceCount": 60,
            "citationCount": 150,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-02-22",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yarats2021ReinforcementLW,\n author = {Denis Yarats and R. Fergus and A. Lazaric and Lerrel Pinto},\n booktitle = {International Conference on Machine Learning},\n pages = {11920-11931},\n title = {Reinforcement Learning with Prototypical Representations},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5285cb8faada5de8a92a47622950f6cfd476ac1d",
            "@type": "ScholarlyArticle",
            "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
            "corpusId": 54457299,
            "url": "https://www.semanticscholar.org/paper/5285cb8faada5de8a92a47622950f6cfd476ac1d",
            "title": "Off-Policy Deep Reinforcement Learning without Exploration",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1812.02900",
                "MAG": "2904453761",
                "DBLP": "conf/icml/FujimotoMP19",
                "CorpusId": 54457299
            },
            "abstract": "Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.",
            "referenceCount": 88,
            "citationCount": 1038,
            "influentialCitationCount": 287,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-12-07",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Fujimoto2018OffPolicyDR,\n author = {Scott Fujimoto and D. Meger and Doina Precup},\n booktitle = {International Conference on Machine Learning},\n pages = {2052-2062},\n title = {Off-Policy Deep Reinforcement Learning without Exploration},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:744139d65c3bf6da6a6acd384a32d94a06f44f62",
            "@type": "ScholarlyArticle",
            "paperId": "744139d65c3bf6da6a6acd384a32d94a06f44f62",
            "corpusId": 216868834,
            "url": "https://www.semanticscholar.org/paper/744139d65c3bf6da6a6acd384a32d94a06f44f62",
            "title": "Reinforcement Learning with Augmented Data",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3021708257",
                "DBLP": "conf/nips/LaskinLSPAS20",
                "ArXiv": "2004.14990",
                "CorpusId": 216868834
            },
            "abstract": "Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efficiency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the first extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. Our RAD module and training code are available at this https URL.",
            "referenceCount": 56,
            "citationCount": 461,
            "influentialCitationCount": 90,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-04-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2004.14990"
            },
            "citationStyles": {
                "bibtex": "@Article{Laskin2020ReinforcementLW,\n author = {M. Laskin and Kimin Lee and Adam Stooke and Lerrel Pinto and P. Abbeel and A. Srinivas},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Reinforcement Learning with Augmented Data},\n volume = {abs/2004.14990},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0272b14dd471fe7b81df703af1b71d7600b77215",
            "@type": "ScholarlyArticle",
            "paperId": "0272b14dd471fe7b81df703af1b71d7600b77215",
            "corpusId": 219708452,
            "url": "https://www.semanticscholar.org/paper/0272b14dd471fe7b81df703af1b71d7600b77215",
            "title": "Accelerating Online Reinforcement Learning with Offline Datasets",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2006.09359",
                "MAG": "3034786558",
                "DBLP": "journals/corr/abs-2006-09359",
                "CorpusId": 219708452
            },
            "abstract": "Reinforcement learning provides an appealing formalism for learning control policies from experience. However, the classic active formulation of reinforcement learning necessitates a lengthy active exploration process for each behavior, making it difficult to apply in real-world settings. If we can instead allow reinforcement learning to effectively use previously collected data to aid the online learning process, where the data could be expert demonstrations or more generally any prior experience, we could make reinforcement learning a substantially more practical tool. While a number of recent methods have sought to learn offline from previously collected data, it remains exceptionally difficult to train a policy with offline data and improve it further with online reinforcement learning. In this paper we systematically analyze why this problem is so challenging, and propose a novel algorithm that combines sample-efficient dynamic programming with maximum likelihood policy updates, providing a simple and effective framework that is able to leverage large amounts of offline data and then quickly perform online fine-tuning of reinforcement learning policies. We show that our method enables rapid learning of skills with a combination of prior demonstration data and online experience across a suite of difficult dexterous manipulation and benchmark tasks.",
            "referenceCount": 55,
            "citationCount": 363,
            "influentialCitationCount": 73,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-06-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2006.09359"
            },
            "citationStyles": {
                "bibtex": "@Article{Nair2020AcceleratingOR,\n author = {Ashvin Nair and Murtaza Dalal and Abhishek Gupta and S. Levine},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Accelerating Online Reinforcement Learning with Offline Datasets},\n volume = {abs/2006.09359},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5f1adc14a77fb61aa463fac728397bd32e00b617",
            "@type": "ScholarlyArticle",
            "paperId": "5f1adc14a77fb61aa463fac728397bd32e00b617",
            "corpusId": 234868359,
            "url": "https://www.semanticscholar.org/paper/5f1adc14a77fb61aa463fac728397bd32e00b617",
            "title": "Challenges of real-world reinforcement learning: definitions, benchmarks and analysis",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/ml/Dulac-ArnoldLML21",
                "MAG": "3121342653",
                "DOI": "10.1007/s10994-021-05961-4",
                "CorpusId": 234868359
            },
            "abstract": null,
            "referenceCount": 135,
            "citationCount": 224,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10994-021-05961-4.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-04-22",
            "journal": {
                "name": "Machine Learning",
                "volume": "110"
            },
            "citationStyles": {
                "bibtex": "@Article{Paduraru2021ChallengesOR,\n author = {Cosmin Paduraru and D. Mankowitz and Gabriel Dulac-Arnold and J. Li and Nir Levine and Sven Gowal and Todd Hester},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {2419 - 2468},\n title = {Challenges of real-world reinforcement learning: definitions, benchmarks and analysis},\n volume = {110},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d6e783bce3b8e3ad082c2757235c34cb86c4e653",
            "@type": "ScholarlyArticle",
            "paperId": "d6e783bce3b8e3ad082c2757235c34cb86c4e653",
            "corpusId": 237048469,
            "url": "https://www.semanticscholar.org/paper/d6e783bce3b8e3ad082c2757235c34cb86c4e653",
            "title": "Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning",
            "venue": "Annu. Rev. Control. Robotics Auton. Syst.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2108-06266",
                "ArXiv": "2108.06266",
                "DOI": "10.1146/annurev-control-042920-020211",
                "CorpusId": 237048469
            },
            "abstract": "The last half decade has seen a steep rise in the number of contributions on safe learning methods for real-world robotic deployments from both the control and reinforcement learning communities. This article provides a concise but holistic review of the recent advances made in using machine learning to achieve safe decision-making under uncertainties, with a focus on unifying the language and frameworks used in control theory and reinforcement learning research. It includes learning-based control approaches that safely improve performance by learning the uncertain dynamics, reinforcement learning approaches that encourage safety or robustness, and methods that can formally certify the safety of a learned control policy. As data- and learning-based robot control methods continue to gain traction, researchers must understand when and how to best leverage them in real-world scenarios where safety is imperative, such as when operating in close proximity to humans. We highlight some of the open challenges that will drive the field of robot learning in the coming years, and emphasize the need for realistic physics-based benchmarks to facilitate fair comparisons between control and reinforcement learning approaches. Expected final online publication date for the Annual Review of Control, Robotics, and Autonomous Systems, Volume 5 is May 2022. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",
            "referenceCount": 157,
            "citationCount": 262,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.annualreviews.org/doi/pdf/10.1146/annurev-control-042920-020211",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-08-13",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2108.06266"
            },
            "citationStyles": {
                "bibtex": "@Article{Brunke2021SafeLI,\n author = {Lukas Brunke and Melissa Greeff and Adam W. Hall and Zhaocong Yuan and Siqi Zhou and Jacopo Panerati and Angela P. Schoellig},\n booktitle = {Annu. Rev. Control. Robotics Auton. Syst.},\n journal = {ArXiv},\n title = {Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning},\n volume = {abs/2108.06266},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:98cb29c03a0882fde368db28ade214c57c3239d6",
            "@type": "ScholarlyArticle",
            "paperId": "98cb29c03a0882fde368db28ade214c57c3239d6",
            "corpusId": 234833859,
            "url": "https://www.semanticscholar.org/paper/98cb29c03a0882fde368db28ade214c57c3239d6",
            "title": "Multi-agent deep reinforcement learning: a survey",
            "venue": "Artificial Intelligence Review",
            "publicationVenue": {
                "id": "urn:research:ea8553fe-2467-4367-afee-c4deb3754820",
                "name": "Artificial Intelligence Review",
                "alternate_names": [
                    "Artif Intell Rev"
                ],
                "issn": "0269-2821",
                "url": "https://link.springer.com/journal/10462"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/air/GronauerD22",
                "MAG": "3156295478",
                "DOI": "10.1007/s10462-021-09996-w",
                "CorpusId": 234833859
            },
            "abstract": null,
            "referenceCount": 260,
            "citationCount": 226,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10462-021-09996-w.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-04-15",
            "journal": {
                "name": "Artificial Intelligence Review",
                "volume": "55"
            },
            "citationStyles": {
                "bibtex": "@Article{Gronauer2021MultiagentDR,\n author = {Sven Gronauer and K. Diepold},\n booktitle = {Artificial Intelligence Review},\n journal = {Artificial Intelligence Review},\n pages = {895 - 943},\n title = {Multi-agent deep reinforcement learning: a survey},\n volume = {55},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:131007c8d7f79004f52599e9279936c333b3c083",
            "@type": "ScholarlyArticle",
            "paperId": "131007c8d7f79004f52599e9279936c333b3c083",
            "corpusId": 219792420,
            "url": "https://www.semanticscholar.org/paper/131007c8d7f79004f52599e9279936c333b3c083",
            "title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3036185205",
                "ArXiv": "2006.10742",
                "DBLP": "journals/corr/abs-2006-10742",
                "CorpusId": 219792420
            },
            "abstract": "We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that both provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.",
            "referenceCount": 39,
            "citationCount": 318,
            "influentialCitationCount": 66,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-06-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2006.10742"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2020LearningIR,\n author = {Amy Zhang and R. McAllister and R. Calandra and Y. Gal and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning Invariant Representations for Reinforcement Learning without Reconstruction},\n volume = {abs/2006.10742},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:175a9a3f0bb4f31fa235386aff52ad18c67275d3",
            "@type": "ScholarlyArticle",
            "paperId": "175a9a3f0bb4f31fa235386aff52ad18c67275d3",
            "corpusId": 219177541,
            "url": "https://www.semanticscholar.org/paper/175a9a3f0bb4f31fa235386aff52ad18c67275d3",
            "title": "Model-Based Reinforcement Learning with Value-Targeted Regression",
            "venue": "Conference on Learning for Dynamics & Control",
            "publicationVenue": {
                "id": "urn:research:20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                "name": "Conference on Learning for Dynamics & Control",
                "alternate_names": [
                    "L4DC",
                    "Conf Learn Dyn  Control"
                ],
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3109462044",
                "DBLP": "journals/corr/abs-2006-01107",
                "ArXiv": "2006.01107",
                "CorpusId": 219177541
            },
            "abstract": "This paper studies model-based reinforcement learning (RL) for regret minimization. We focus on finite-horizon episodic RL where the transition model $P$ belongs to a known family of models $\\mathcal{P}$, a special case of which is when models in $\\mathcal{P}$ take the form of linear mixtures: $P_{\\theta} = \\sum_{i=1}^{d} \\theta_{i}P_{i}$. We propose a model based RL algorithm that is based on optimism principle: In each episode, the set of models that are `consistent' with the data collected is constructed. The criterion of consistency is based on the total squared error of that the model incurs on the task of predicting \\emph{values} as determined by the last value estimate along the transitions. The next value function is then chosen by solving the optimistic planning problem with the constructed set of models. We derive a bound on the regret, which, in the special case of linear mixtures, the regret bound takes the form $\\tilde{\\mathcal{O}}(d\\sqrt{H^{3}T})$, where $H$, $T$ and $d$ are the horizon, total number of steps and dimension of $\\theta$, respectively. In particular, this regret bound is independent of the total number of states or actions, and is close to a lower bound $\\Omega(\\sqrt{HdT})$. For a general model family $\\mathcal{P}$, the regret bound is derived using the notion of the so-called Eluder dimension proposed by Russo & Van Roy (2014).",
            "referenceCount": 69,
            "citationCount": 241,
            "influentialCitationCount": 72,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-06-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ayoub2020ModelBasedRL,\n author = {Alex Ayoub and Zeyu Jia and Csaba Szepesvari and Mengdi Wang and Lin F. Yang},\n booktitle = {Conference on Learning for Dynamics & Control},\n pages = {666-686},\n title = {Model-Based Reinforcement Learning with Value-Targeted Regression},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:12d1d070a53d4084d88a77b8b143bad51c40c38f",
            "@type": "ScholarlyArticle",
            "paperId": "12d1d070a53d4084d88a77b8b143bad51c40c38f",
            "corpusId": 1708582,
            "url": "https://www.semanticscholar.org/paper/12d1d070a53d4084d88a77b8b143bad51c40c38f",
            "title": "Reinforcement Learning: A Survey",
            "venue": "Journal of Artificial Intelligence Research",
            "publicationVenue": {
                "id": "urn:research:aef12dca-60a0-4ca3-819b-cad26d309d4e",
                "name": "Journal of Artificial Intelligence Research",
                "alternate_names": [
                    "JAIR",
                    "J Artif Intell Res",
                    "The Journal of Artificial Intelligence Research"
                ],
                "issn": "1076-9757",
                "url": "http://www.jair.org/"
            },
            "year": 1996,
            "externalIds": {
                "DBLP": "journals/jair/KaelblingLM96",
                "MAG": "2107726111",
                "ArXiv": "cs/9605103",
                "DOI": "10.1613/jair.301",
                "CorpusId": 1708582
            },
            "abstract": "This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word \"reinforcement.\" The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.",
            "referenceCount": 131,
            "citationCount": 8266,
            "influentialCitationCount": 386,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.jair.org/index.php/jair/article/download/10166/24110",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1996-04-30",
            "journal": {
                "name": "J. Artif. Intell. Res.",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Kaelbling1996ReinforcementLA,\n author = {L. Kaelbling and M. Littman and A. Moore},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {J. Artif. Intell. Res.},\n pages = {237-285},\n title = {Reinforcement Learning: A Survey},\n volume = {4},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:309c2c5ee60e725244da09180f913cd8d4b8d4e9",
            "@type": "ScholarlyArticle",
            "paperId": "309c2c5ee60e725244da09180f913cd8d4b8d4e9",
            "corpusId": 218595964,
            "url": "https://www.semanticscholar.org/paper/309c2c5ee60e725244da09180f913cd8d4b8d4e9",
            "title": "MOReL : Model-Based Offline Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2005-05951",
                "MAG": "3106426955",
                "ArXiv": "2005.05951",
                "CorpusId": 218595964
            },
            "abstract": "In offline reinforcement learning (RL), the goal is to learn a highly rewarding policy based solely on a dataset of historical interactions with the environment. The ability to train RL policies offline can greatly expand the applicability of RL, its data efficiency, and its experimental velocity. Prior work in offline RL has been confined almost exclusively to model-free RL approaches. In this work, we present MOReL, an algorithmic framework for model-based offline RL. This framework consists of two steps: (a) learning a pessimistic MDP (P-MDP) using the offline dataset; and (b) learning a near-optimal policy in this P-MDP. The learned P-MDP has the property that for any policy, the performance in the real environment is approximately lower-bounded by the performance in the P-MDP. This enables it to serve as a good surrogate for purposes of policy evaluation and learning, and overcome common pitfalls of model-based RL like model exploitation. Theoretically, we show that MOReL is minimax optimal (up to log factors) for offline RL. Through experiments, we show that MOReL matches or exceeds state-of-the-art results in widely studied offline RL benchmarks. Moreover, the modular design of MOReL enables future advances in its components (e.g. generative modeling, uncertainty estimation, planning etc.) to directly translate into advances for offline RL.",
            "referenceCount": 99,
            "citationCount": 442,
            "influentialCitationCount": 50,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-05-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2005.05951"
            },
            "citationStyles": {
                "bibtex": "@Article{Kidambi2020MOReLM,\n author = {Rahul Kidambi and A. Rajeswaran and Praneeth Netrapalli and T. Joachims},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {MOReL : Model-Based Offline Reinforcement Learning},\n volume = {abs/2005.05951},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:42edbc3c29af476c27f102b3de9f04e56b5c642d",
            "@type": "ScholarlyArticle",
            "paperId": "42edbc3c29af476c27f102b3de9f04e56b5c642d",
            "corpusId": 244346227,
            "url": "https://www.semanticscholar.org/paper/42edbc3c29af476c27f102b3de9f04e56b5c642d",
            "title": "A Survey of Generalisation in Deep Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2111-09794",
                "CorpusId": 244346227
            },
            "abstract": "The study of generalisation in deep Reinforcement Learning (RL) aims to produce RL algorithms whose policies generalise well to novel unseen situations at deployment time, avoiding over\ufb01tting to their training environments. Tackling this is vital if we are to deploy reinforcement learning algorithms in real world scenarios, where the environment will be diverse, dynamic and unpredictable. This survey is an overview of this nascent \ufb01eld. We provide a unifying formalism and terminology for discussing different generalisation problems, building upon previous works. We go on to categorise existing benchmarks for generalisation, as well as current methods for tackling the generalisation problem. Finally, we provide a critical discussion of the current state of the \ufb01eld, including recommendations for future work. Among other conclusions, we argue that taking a purely procedural content generation approach to benchmark design is not conducive to progress in generalisation, we suggest fast online adaptation and tackling RL-speci\ufb01c problems as some areas for future work on methods for generalisation, and we recommend building benchmarks in underexplored problem settings such as of\ufb02ine RL generalisation and reward-function variation.",
            "referenceCount": 203,
            "citationCount": 155,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2111.09794"
            },
            "citationStyles": {
                "bibtex": "@Article{Kirk2021ASO,\n author = {Robert Kirk and Amy Zhang and Edward Grefenstette and Tim Rocktaschel},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Survey of Generalisation in Deep Reinforcement Learning},\n volume = {abs/2111.09794},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0ab3f7ecbdc5a33565a234215604a6ca9d155a33",
            "@type": "ScholarlyArticle",
            "paperId": "0ab3f7ecbdc5a33565a234215604a6ca9d155a33",
            "corpusId": 19135734,
            "url": "https://www.semanticscholar.org/paper/0ab3f7ecbdc5a33565a234215604a6ca9d155a33",
            "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/aaai/HesselMHSODHPAS18",
                "MAG": "2761873684",
                "ArXiv": "1710.02298",
                "DOI": "10.1609/aaai.v32i1.11796",
                "CorpusId": 19135734
            },
            "abstract": "\n \n The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.\n \n",
            "referenceCount": 32,
            "citationCount": 1743,
            "influentialCitationCount": 327,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11796/11655",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-10-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hessel2017RainbowCI,\n author = {Matteo Hessel and Joseph Modayil and H. V. Hasselt and T. Schaul and Georg Ostrovski and Will Dabney and Dan Horgan and Bilal Piot and M. G. Azar and David Silver},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {3215-3222},\n title = {Rainbow: Combining Improvements in Deep Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a20f0ce0616def7cc9a87446c228906cd5da093b",
            "@type": "ScholarlyArticle",
            "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
            "corpusId": 1211821,
            "url": "https://www.semanticscholar.org/paper/a20f0ce0616def7cc9a87446c228906cd5da093b",
            "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1999,
            "externalIds": {
                "DBLP": "conf/nips/SuttonMSM99",
                "MAG": "2155027007",
                "CorpusId": 1211821
            },
            "abstract": "Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.",
            "referenceCount": 20,
            "citationCount": 5733,
            "influentialCitationCount": 595,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1999-11-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sutton1999PolicyGM,\n author = {R. Sutton and David A. McAllester and Satinder Singh and Y. Mansour},\n booktitle = {Neural Information Processing Systems},\n pages = {1057-1063},\n title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:82f7073b503143cf3ed4d59dca2f206ba18a31b9",
            "@type": "ScholarlyArticle",
            "paperId": "82f7073b503143cf3ed4d59dca2f206ba18a31b9",
            "corpusId": 220903131,
            "url": "https://www.semanticscholar.org/paper/82f7073b503143cf3ed4d59dca2f206ba18a31b9",
            "title": "Optimizing Federated Learning on Non-IID Data with Reinforcement Learning",
            "venue": "IEEE Conference on Computer Communications",
            "publicationVenue": {
                "id": "urn:research:7f92b1d2-f2b3-454d-adbe-ff02c83fe404",
                "name": "IEEE Conference on Computer Communications",
                "alternate_names": [
                    "INFOCOM",
                    "IEEE Conf Comput Commun"
                ],
                "issn": null,
                "url": "http://www.ieee-infocom.org/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/infocom/WangKNL20",
                "MAG": "3047304572",
                "DOI": "10.1109/infocom41043.2020.9155494",
                "CorpusId": 220903131
            },
            "abstract": "The widespread deployment of machine learning applications in ubiquitous environments has sparked interests in exploiting the vast amount of data stored on mobile devices. To preserve data privacy, Federated Learning has been proposed to learn a shared model by performing distributed training locally on participating devices and aggregating the local models into a global one. However, due to the limited network connectivity of mobile devices, it is not practical for federated learning to perform model updates and aggregation on all participating devices in parallel. Besides, data samples across all devices are usually not independent and identically distributed (IID), posing additional challenges to the convergence and speed of federated learning. In this paper, we propose Favor, an experience-driven control framework that intelligently chooses the client devices to participate in each round of federated learning to counterbalance the bias introduced by non-IID data and to speed up convergence. Through both empirical and mathematical analysis, we observe an implicit connection between the distribution of training data on a device and the model weights trained based on those data, which enables us to profile the data distribution on that device based on its uploaded model weights. We then propose a mechanism based on deep Q-learning that learns to select a subset of devices in each communication round to maximize a reward that encourages the increase of validation accuracy and penalizes the use of more communication rounds. With extensive experiments performed in PyTorch, we show that the number of communication rounds required in federated learning can be reduced by up to 49% on the MNIST dataset, 23% on FashionMNIST, and 42% on CIFAR-10, as compared to the Federated Averaging algorithm.",
            "referenceCount": 17,
            "citationCount": 444,
            "influentialCitationCount": 48,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-07-01",
            "journal": {
                "name": "IEEE INFOCOM 2020 - IEEE Conference on Computer Communications",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2020OptimizingFL,\n author = {Hao Wang and Zakhary Kaplan and Di Niu and Baochun Li},\n booktitle = {IEEE Conference on Computer Communications},\n journal = {IEEE INFOCOM 2020 - IEEE Conference on Computer Communications},\n pages = {1698-1707},\n title = {Optimizing Federated Learning on Non-IID Data with Reinforcement Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3f5f13c6f5c659754086a7faa51d6bc60f577cd1",
            "@type": "ScholarlyArticle",
            "paperId": "3f5f13c6f5c659754086a7faa51d6bc60f577cd1",
            "corpusId": 213176860,
            "url": "https://www.semanticscholar.org/paper/3f5f13c6f5c659754086a7faa51d6bc60f577cd1",
            "title": "Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2003.08839",
                "DBLP": "journals/jmlr/RashidSWFFW20",
                "MAG": "2963762747",
                "CorpusId": 213176860
            },
            "abstract": "In many real-world settings, a team of agents must coordinate its behaviour while acting in a decentralised fashion. At the same time, it is often possible to train the agents in a centralised fashion where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a mixing network that estimates joint action-values as a monotonic combination of per-agent values. We structurally enforce that the joint-action value is monotonic in the per-agent values, through the use of non-negative weights in the mixing network, which guarantees consistency between the centralised and decentralised policies. To evaluate the performance of QMIX, we propose the StarCraft Multi-Agent Challenge (SMAC) as a new benchmark for deep multi-agent reinforcement learning. We evaluate QMIX on a challenging set of SMAC scenarios and show that it significantly outperforms existing multi-agent reinforcement learning methods.",
            "referenceCount": 86,
            "citationCount": 279,
            "influentialCitationCount": 47,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-03-19",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "21"
            },
            "citationStyles": {
                "bibtex": "@Article{Rashid2020MonotonicVF,\n author = {Tabish Rashid and Mikayel Samvelyan and C. S. D. Witt and Gregory Farquhar and J. Foerster and Shimon Whiteson},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {178:1-178:51},\n title = {Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},\n volume = {21},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0bc855f84668b35cb65618d996d09f6e434d28c9",
            "@type": "ScholarlyArticle",
            "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
            "corpusId": 204852201,
            "url": "https://www.semanticscholar.org/paper/0bc855f84668b35cb65618d996d09f6e434d28c9",
            "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2981344907",
                "DBLP": "journals/corr/abs-1910-10897",
                "ArXiv": "1910.10897",
                "CorpusId": 204852201
            },
            "abstract": "Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 6 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods.",
            "referenceCount": 68,
            "citationCount": 663,
            "influentialCitationCount": 144,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-10-24",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1910.10897"
            },
            "citationStyles": {
                "bibtex": "@Article{Yu2019MetaWorldAB,\n author = {Tianhe Yu and Deirdre Quillen and Zhanpeng He and Ryan C. Julian and Karol Hausman and Chelsea Finn and S. Levine},\n booktitle = {Conference on Robot Learning},\n journal = {ArXiv},\n title = {Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning},\n volume = {abs/1910.10897},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:56ca9b304e0476feaa6dd4fd1cccc8c0a1a9d8eb",
            "@type": "ScholarlyArticle",
            "paperId": "56ca9b304e0476feaa6dd4fd1cccc8c0a1a9d8eb",
            "corpusId": 153312464,
            "url": "https://www.semanticscholar.org/paper/56ca9b304e0476feaa6dd4fd1cccc8c0a1a9d8eb",
            "title": "QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1905-05408",
                "ArXiv": "1905.05408",
                "MAG": "2946606218",
                "CorpusId": 153312464
            },
            "abstract": "We explore value-based solutions for multi-agent reinforcement learning (MARL) tasks in the centralized training with decentralized execution (CTDE) regime popularized recently. However, VDN and QMIX are representative examples that use the idea of factorization of the joint action-value function into individual ones for decentralized execution. VDN and QMIX address only a fraction of factorizable MARL tasks due to their structural constraint in factorization such as additivity and monotonicity. In this paper, we propose a new factorization method for MARL, QTRAN, which is free from such structural constraints and takes on a new approach to transforming the original joint action-value function into an easily factorizable one, with the same optimal actions. QTRAN guarantees more general factorization than VDN or QMIX, thus covering a much wider class of MARL tasks than does previous methods. Our experiments for the tasks of multi-domain Gaussian-squeeze and modified predator-prey demonstrate QTRAN's superior performance with especially larger margins in games whose payoffs penalize non-cooperative behavior more aggressively.",
            "referenceCount": 33,
            "citationCount": 520,
            "influentialCitationCount": 134,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-05-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1905.05408"
            },
            "citationStyles": {
                "bibtex": "@Article{Son2019QTRANLT,\n author = {Kyunghwan Son and Daewoo Kim and Wan Ju Kang and D. Hostallero and Yung Yi},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning},\n volume = {abs/1905.05408},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4625628163a2ee0e6cd320cd7a14b4ccded2a631",
            "@type": "ScholarlyArticle",
            "paperId": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
            "corpusId": 84187276,
            "url": "https://www.semanticscholar.org/paper/4625628163a2ee0e6cd320cd7a14b4ccded2a631",
            "title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2923504512",
                "DBLP": "journals/corr/abs-1903-08254",
                "ArXiv": "1903.08254",
                "CorpusId": 84187276
            },
            "abstract": "Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While in principle meta-reinforcement learning (meta-RL) algorithms enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. The also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness in sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.",
            "referenceCount": 47,
            "citationCount": 478,
            "influentialCitationCount": 117,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-03-19",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rakelly2019EfficientOM,\n author = {Kate Rakelly and Aurick Zhou and Deirdre Quillen and Chelsea Finn and S. Levine},\n booktitle = {International Conference on Machine Learning},\n pages = {5331-5340},\n title = {Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:129983331ca874142a3e8eb2d93d820bdf1f9aca",
            "@type": "ScholarlyArticle",
            "paperId": "129983331ca874142a3e8eb2d93d820bdf1f9aca",
            "corpusId": 211011033,
            "url": "https://www.semanticscholar.org/paper/129983331ca874142a3e8eb2d93d820bdf1f9aca",
            "title": "Deep Reinforcement Learning for Autonomous Driving: A Survey",
            "venue": "IEEE transactions on intelligent transportation systems (Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2002.00444",
                "DBLP": "journals/tits/KiranSTMSYP22",
                "MAG": "3003533476",
                "DOI": "10.1109/TITS.2021.3054625",
                "CorpusId": 211011033
            },
            "abstract": "With the development of deep representation learning, the domain of reinforcement learning (RL) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving tasks where (D)RL methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical RL algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in RL are discussed.",
            "referenceCount": 156,
            "citationCount": 863,
            "influentialCitationCount": 30,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2002.00444",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-02-02",
            "journal": {
                "name": "IEEE Transactions on Intelligent Transportation Systems",
                "volume": "23"
            },
            "citationStyles": {
                "bibtex": "@Article{Kiran2020DeepRL,\n author = {B. R. Kiran and Ibrahim Sobh and V. Talpaert and P. Mannion and A. A. Sallab and S. Yogamani and P. P'erez},\n booktitle = {IEEE transactions on intelligent transportation systems (Print)},\n journal = {IEEE Transactions on Intelligent Transportation Systems},\n pages = {4909-4926},\n title = {Deep Reinforcement Learning for Autonomous Driving: A Survey},\n volume = {23},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:af4c100d7b32e6173cd567776e798529570a1c6a",
            "@type": "ScholarlyArticle",
            "paperId": "af4c100d7b32e6173cd567776e798529570a1c6a",
            "corpusId": 241489836,
            "url": "https://www.semanticscholar.org/paper/af4c100d7b32e6173cd567776e798529570a1c6a",
            "title": "Handbook of Reinforcement Learning and Control",
            "venue": "Studies in Systems, Decision and Control",
            "publicationVenue": {
                "id": "urn:research:ac3a598f-20ad-4833-aea7-3ca238d67bde",
                "name": "Studies in Systems, Decision and Control",
                "alternate_names": [
                    "Stud Syst Decis Control"
                ],
                "issn": "2198-4182",
                "url": "https://www.springer.com/series/13304?detailsPage=titles"
            },
            "year": 2021,
            "externalIds": {
                "DOI": "10.1007/978-3-030-60990-0",
                "CorpusId": 241489836
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 157,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm%3A978-3-030-60990-0%2F1",
                "status": "GREEN"
            },
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Studies in Systems, Decision and Control",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{None,\n booktitle = {Studies in Systems, Decision and Control},\n journal = {Studies in Systems, Decision and Control},\n title = {Handbook of Reinforcement Learning and Control},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7428f65393c19a6ca6381693767cb4f643a49a5c",
            "@type": "ScholarlyArticle",
            "paperId": "7428f65393c19a6ca6381693767cb4f643a49a5c",
            "corpusId": 231592776,
            "url": "https://www.semanticscholar.org/paper/7428f65393c19a6ca6381693767cb4f643a49a5c",
            "title": "Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "conf/iclr/AgarwalMCB21",
                "ArXiv": "2101.05265",
                "CorpusId": 231592776
            },
            "abstract": "Reinforcement learning methods trained on few environments rarely learn policies that generalize to unseen environments. To improve generalization, we incorporate the inherent sequential structure in reinforcement learning into the representation learning process. This approach is orthogonal to recent approaches, which rarely exploit this structure explicitly. Specifically, we introduce a theoretically motivated policy similarity metric (PSM) for measuring behavioral similarity between states. PSM assigns high similarity to states for which the optimal policies in those states as well as in future states are similar. We also present a contrastive representation learning procedure to embed any state similarity metric, which we instantiate with PSM to obtain policy similarity embeddings (PSEs). We demonstrate that PSEs improve generalization on diverse benchmarks, including LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite.",
            "referenceCount": 67,
            "citationCount": 126,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-01-13",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2101.05265"
            },
            "citationStyles": {
                "bibtex": "@Article{Agarwal2021ContrastiveBS,\n author = {Rishabh Agarwal and Marlos C. Machado and P. S. Castro and Marc G. Bellemare},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning},\n volume = {abs/2101.05265},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8d814620a1ca77e745bc8a33b96b86148f2804fe",
            "@type": "ScholarlyArticle",
            "paperId": "8d814620a1ca77e745bc8a33b96b86148f2804fe",
            "corpusId": 208547624,
            "url": "https://www.semanticscholar.org/paper/8d814620a1ca77e745bc8a33b96b86148f2804fe",
            "title": "Leveraging Procedural Generation to Benchmark Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1912-01588",
                "ArXiv": "1912.01588",
                "MAG": "2994073215",
                "CorpusId": 208547624
            },
            "abstract": "We introduce Procgen Benchmark, a suite of 16 procedurally generated game-like environments designed to benchmark both sample efficiency and generalization in reinforcement learning. We believe that the community will benefit from increased access to high quality training environments, and we provide detailed experimental protocols for using this benchmark. We empirically demonstrate that diverse environment distributions are essential to adequately train and evaluate RL agents, thereby motivating the extensive use of procedural content generation. We then use this benchmark to investigate the effects of scaling model size, finding that larger models significantly improve both sample efficiency and generalization.",
            "referenceCount": 30,
            "citationCount": 392,
            "influentialCitationCount": 108,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-12-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Cobbe2019LeveragingPG,\n author = {Karl Cobbe and Christopher Hesse and Jacob Hilton and J. Schulman},\n booktitle = {International Conference on Machine Learning},\n pages = {2048-2056},\n title = {Leveraging Procedural Generation to Benchmark Reinforcement Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9be492858863c8c7c24be1ecb75724de5086bd8e",
            "@type": "ScholarlyArticle",
            "paperId": "9be492858863c8c7c24be1ecb75724de5086bd8e",
            "corpusId": 208291277,
            "url": "https://www.semanticscholar.org/paper/9be492858863c8c7c24be1ecb75724de5086bd8e",
            "title": "Behavior Regularized Offline Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1911-11361",
                "ArXiv": "1911.11361",
                "MAG": "2991355586",
                "CorpusId": 208291277
            },
            "abstract": "In reinforcement learning (RL) research, it is common to assume access to direct online interactions with the environment. However in many real-world applications, access to the environment is limited to a fixed offline dataset of logged experience. In such settings, standard RL algorithms have been shown to diverge or otherwise yield poor performance. Accordingly, recent work has suggested a number of remedies to these issues. In this work, we introduce a general framework, behavior regularized actor critic (BRAC), to empirically evaluate recently proposed methods as well as a number of simple baselines across a variety of offline continuous control tasks. Surprisingly, we find that many of the technical complexities introduced in recent methods are unnecessary to achieve strong performance. Additional ablations provide insights into which design choices matter most in the offline RL setting.",
            "referenceCount": 38,
            "citationCount": 475,
            "influentialCitationCount": 94,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1911.11361"
            },
            "citationStyles": {
                "bibtex": "@Article{Wu2019BehaviorRO,\n author = {Yifan Wu and G. Tucker and Ofir Nachum},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Behavior Regularized Offline Reinforcement Learning},\n volume = {abs/1911.11361},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1fd4694e7c2d9c872a427d50e81b5475056de6bc",
            "@type": "ScholarlyArticle",
            "paperId": "1fd4694e7c2d9c872a427d50e81b5475056de6bc",
            "corpusId": 67856232,
            "url": "https://www.semanticscholar.org/paper/1fd4694e7c2d9c872a427d50e81b5475056de6bc",
            "title": "Model-Based Reinforcement Learning for Atari",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2994714051",
                "ArXiv": "1903.00374",
                "DBLP": "journals/corr/abs-1903-00374",
                "CorpusId": 67856232
            },
            "abstract": "Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.",
            "referenceCount": 57,
            "citationCount": 634,
            "influentialCitationCount": 64,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-03-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1903.00374"
            },
            "citationStyles": {
                "bibtex": "@Article{Kaiser2019ModelBasedRL,\n author = {Lukasz Kaiser and M. Babaeizadeh and Piotr Milos and B. Osinski and R. Campbell and K. Czechowski and D. Erhan and Chelsea Finn and Piotr Kozakowski and S. Levine and Afroz Mohiuddin and Ryan Sepassi and G. Tucker and H. Michalewski},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Model-Based Reinforcement Learning for Atari},\n volume = {abs/1903.00374},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4d0f6a6ffcd6ab04732ff76420fd9f8a7bb649c3",
            "@type": "ScholarlyArticle",
            "paperId": "4d0f6a6ffcd6ab04732ff76420fd9f8a7bb649c3",
            "corpusId": 208283920,
            "url": "https://www.semanticscholar.org/paper/4d0f6a6ffcd6ab04732ff76420fd9f8a7bb649c3",
            "title": "Benchmarking Safe Exploration in Deep Reinforcement Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "CorpusId": 208283920
            },
            "abstract": "Reinforcement learning (RL) agents need to explore their environments in order to learn optimal policies by trial and error. In many environments, safety is a critical concern and certain errors are unacceptable: for example, robotics systems that interact with humans should never cause injury to the humans while exploring. While it is currently typical to train RL agents mostly or entirely in simulation, where safety concerns are minimal, we anticipate that challenges in simulating the complexities of the real world (such as human-AI interactions) will cause a shift towards training RL agents directly in the real world, where safety concerns are paramount. Consequently we take the position that safe exploration should be viewed as a critical focus area for RL research, and in this work we make three contributions to advance the study of safe exploration. First, building on a wide range of prior work on safe reinforcement learning, we propose to standardize constrained RL as the main formalism for safe exploration. Second, we present the Safety Gym benchmark suite, a new slate of high-dimensional continuous control environments for measuring research progress on constrained RL. Finally, we benchmark several constrained deep RL algorithms on Safety Gym environments to establish baselines that future work can build on.",
            "referenceCount": 50,
            "citationCount": 312,
            "influentialCitationCount": 83,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Achiam2019BenchmarkingSE,\n author = {Joshua Achiam and Dario Amodei},\n title = {Benchmarking Safe Exploration in Deep Reinforcement Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:54d4a221db5a91a2487b1610374843fafff5a23d",
            "@type": "ScholarlyArticle",
            "paperId": "54d4a221db5a91a2487b1610374843fafff5a23d",
            "corpusId": 208268127,
            "url": "https://www.semanticscholar.org/paper/54d4a221db5a91a2487b1610374843fafff5a23d",
            "title": "Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms",
            "venue": "Handbook of Reinforcement Learning and Control",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1911-10635",
                "MAG": "2991046523",
                "ArXiv": "1911.10635",
                "DOI": "10.1007/978-3-030-60990-0_12",
                "CorpusId": 208268127
            },
            "abstract": null,
            "referenceCount": 398,
            "citationCount": 760,
            "influentialCitationCount": 56,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1911.10635",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-11-24",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1911.10635"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2019MultiAgentRL,\n author = {K. Zhang and Zhuoran Yang and T. Ba\u015far},\n booktitle = {Handbook of Reinforcement Learning and Control},\n journal = {ArXiv},\n title = {Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms},\n volume = {abs/1911.10635},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:90f9ef793c1e4000ee523e1f8cea501fb8806fa0",
            "@type": "ScholarlyArticle",
            "paperId": "90f9ef793c1e4000ee523e1f8cea501fb8806fa0",
            "corpusId": 211066437,
            "url": "https://www.semanticscholar.org/paper/90f9ef793c1e4000ee523e1f8cea501fb8806fa0",
            "title": "Reward-Free Exploration for Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3035599863",
                "ArXiv": "2002.02794",
                "DBLP": "conf/icml/JinKSY20",
                "CorpusId": 211066437
            },
            "abstract": "Exploration is widely regarded as one of the most challenging aspects of reinforcement learning (RL), with many naive approaches succumbing to exponential sample complexity. To isolate the challenges of exploration, we propose a new \"reward-free RL\" framework. In the exploration phase, the agent first collects trajectories from an MDP $\\mathcal{M}$ without a pre-specified reward function. After exploration, it is tasked with computing near-optimal policies under for $\\mathcal{M}$ for a collection of given reward functions. This framework is particularly suitable when there are many reward functions of interest, or when the reward function is shaped by an external agent to elicit desired behavior. \nWe give an efficient algorithm that conducts $\\tilde{\\mathcal{O}}(S^2A\\mathrm{poly}(H)/\\epsilon^2)$ episodes of exploration and returns $\\epsilon$-suboptimal policies for an arbitrary number of reward functions. We achieve this by finding exploratory policies that visit each \"significant\" state with probability proportional to its maximum visitation probability under any possible policy. Moreover, our planning procedure can be instantiated by any black-box approximate planner, such as value iteration or natural policy gradient. We also give a nearly-matching $\\Omega(S^2AH^2/\\epsilon^2)$ lower bound, demonstrating the near-optimality of our algorithm in this setting.",
            "referenceCount": 34,
            "citationCount": 157,
            "influentialCitationCount": 57,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-02-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2002.02794"
            },
            "citationStyles": {
                "bibtex": "@Article{Jin2020RewardFreeEF,\n author = {Chi Jin and A. Krishnamurthy and Max Simchowitz and Tiancheng Yu},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Reward-Free Exploration for Reinforcement Learning},\n volume = {abs/2002.02794},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4012d4ab621f3f5f04b0f91849a60c6eaabe64b4",
            "@type": "ScholarlyArticle",
            "paperId": "4012d4ab621f3f5f04b0f91849a60c6eaabe64b4",
            "corpusId": 212628904,
            "url": "https://www.semanticscholar.org/paper/4012d4ab621f3f5f04b0f91849a60c6eaabe64b4",
            "title": "An Optimistic Perspective on Offline Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/icml/AgarwalS020",
                "MAG": "3009584650",
                "CorpusId": 212628904
            },
            "abstract": "Off-policy reinforcement learning (RL) using a fixed offline dataset of logged interactions is an important consideration in real world applications. This paper studies offline RL using the DQN replay dataset comprising the entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate that recent off-policy deep RL algorithms, even when trained solely on this replay dataset, outperform the fully trained DQN agent. To enhance generalization in the offline setting, we present Random Ensemble Mixture (REM), a robust Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM trained on the DQN replay dataset surpasses strong RL baselines. The results here present an optimistic view that robust RL algorithms trained on sufficiently large and diverse offline datasets can lead to high quality policies. The DQN replay dataset can serve as an offline RL benchmark and is open-sourced.",
            "referenceCount": 72,
            "citationCount": 369,
            "influentialCitationCount": 60,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-07-10",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Agarwal2019AnOP,\n author = {Rishabh Agarwal and D. Schuurmans and Mohammad Norouzi},\n booktitle = {International Conference on Machine Learning},\n pages = {104-114},\n title = {An Optimistic Perspective on Offline Reinforcement Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:56136aa0b2c347cbcf3d50821f310c4253155026",
            "@type": "ScholarlyArticle",
            "paperId": "56136aa0b2c347cbcf3d50821f310c4253155026",
            "corpusId": 44177328,
            "url": "https://www.semanticscholar.org/paper/56136aa0b2c347cbcf3d50821f310c4253155026",
            "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963960193",
                "ArXiv": "1805.12114",
                "DBLP": "conf/nips/ChuaCML18",
                "CorpusId": 44177328
            },
            "abstract": "Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).",
            "referenceCount": 62,
            "citationCount": 946,
            "influentialCitationCount": 198,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-05-30",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chua2018DeepRL,\n author = {Kurtland Chua and R. Calandra and R. McAllister and S. Levine},\n booktitle = {Neural Information Processing Systems},\n pages = {4759-4770},\n title = {Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b17e0b5b5c7e632ecf5f35b0616cbf820382fcf7",
            "@type": "ScholarlyArticle",
            "paperId": "b17e0b5b5c7e632ecf5f35b0616cbf820382fcf7",
            "corpusId": 75136065,
            "url": "https://www.semanticscholar.org/paper/b17e0b5b5c7e632ecf5f35b0616cbf820382fcf7",
            "title": "Multi-Agent Deep Reinforcement Learning for Large-Scale Traffic Signal Control",
            "venue": "IEEE transactions on intelligent transportation systems (Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1903.04527",
                "DBLP": "journals/tits/ChuWCL20",
                "MAG": "2952265723",
                "DOI": "10.1109/TITS.2019.2901791",
                "CorpusId": 75136065
            },
            "abstract": "Reinforcement learning (RL) is a promising data-driven approach for adaptive traffic signal control (ATSC) in complex urban traffic networks, and deep neural networks further enhance its learning power. However, the centralized RL is infeasible for large-scale ATSC due to the extremely high dimension of the joint action space. The multi-agent RL (MARL) overcomes the scalability issue by distributing the global control to each local RL agent, but it introduces new challenges: now, the environment becomes partially observable from the viewpoint of each local agent due to limited communication among agents. Most existing studies in MARL focus on designing efficient communication and coordination among traditional Q-learning agents. This paper presents, for the first time, a fully scalable and decentralized MARL algorithm for the state-of-the-art deep RL agent, advantage actor critic (A2C), within the context of ATSC. In particular, two methods are proposed to stabilize the learning procedure, by improving the observability and reducing the learning difficulty of each local agent. The proposed multi-agent A2C is compared against independent A2C and independent Q-learning algorithms, in both a large synthetic traffic grid and a large real-world traffic network of Monaco city, under simulated peak-hour traffic dynamics. The results demonstrate its optimality, robustness, and sample efficiency over the other state-of-the-art decentralized MARL algorithms.",
            "referenceCount": 47,
            "citationCount": 438,
            "influentialCitationCount": 42,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1903.04527",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-03-11",
            "journal": {
                "name": "IEEE Transactions on Intelligent Transportation Systems",
                "volume": "21"
            },
            "citationStyles": {
                "bibtex": "@Article{Chu2019MultiAgentDR,\n author = {Tianshu Chu and Jie Wang and Lara Codec\u00e0 and Zhaojian Li},\n booktitle = {IEEE transactions on intelligent transportation systems (Print)},\n journal = {IEEE Transactions on Intelligent Transportation Systems},\n pages = {1086-1095},\n title = {Multi-Agent Deep Reinforcement Learning for Large-Scale Traffic Signal Control},\n volume = {21},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:69d1ee8a99f55e9228f33fdb3a0339541ad1201c",
            "@type": "ScholarlyArticle",
            "paperId": "69d1ee8a99f55e9228f33fdb3a0339541ad1201c",
            "corpusId": 195767454,
            "url": "https://www.semanticscholar.org/paper/69d1ee8a99f55e9228f33fdb3a0339541ad1201c",
            "title": "Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1907.00953",
                "DBLP": "journals/corr/abs-1907-00953",
                "MAG": "2954974210",
                "CorpusId": 195767454
            },
            "abstract": "Deep reinforcement learning (RL) algorithms can use high-capacity deep networks to learn directly from image observations. However, these high-dimensional observation spaces present a number of challenges in practice, since the policy must now solve two problems: representation learning and task learning. In this work, we tackle these two problems separately, by explicitly learning latent representations that can accelerate reinforcement learning from images. We propose the stochastic latent actor-critic (SLAC) algorithm: a sample-efficient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. SLAC provides a novel and principled approach for unifying stochastic sequential models and RL into a single method, by learning a compact latent representation and then performing RL in the model's learned latent space. Our experimental evaluation demonstrates that our method outperforms both model-free and model-based alternatives in terms of final performance and sample efficiency, on a range of difficult image-based control tasks. Our code and videos of our results are available at our website.",
            "referenceCount": 63,
            "citationCount": 295,
            "influentialCitationCount": 44,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-07-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1907.00953"
            },
            "citationStyles": {
                "bibtex": "@Article{Lee2019StochasticLA,\n author = {Alex X. Lee and Anusha Nagabandi and P. Abbeel and S. Levine},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model},\n volume = {abs/1907.00953},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
            "@type": "ScholarlyArticle",
            "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
            "corpusId": 4787508,
            "url": "https://www.semanticscholar.org/paper/5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
            "title": "Deep Reinforcement Learning from Human Preferences",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2626804490",
                "DBLP": "journals/corr/abs-1706-03741",
                "ArXiv": "1706.03741",
                "CorpusId": 4787508
            },
            "abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.",
            "referenceCount": 44,
            "citationCount": 1323,
            "influentialCitationCount": 184,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-06-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1706.03741"
            },
            "citationStyles": {
                "bibtex": "@Article{Christiano2017DeepRL,\n author = {P. Christiano and J. Leike and Tom B. Brown and Miljan Martic and S. Legg and Dario Amodei},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Deep Reinforcement Learning from Human Preferences},\n volume = {abs/1706.03741},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9f2e34581ca03160e8fd8b770203a5e7c2a902d3",
            "@type": "ScholarlyArticle",
            "paperId": "9f2e34581ca03160e8fd8b770203a5e7c2a902d3",
            "corpusId": 235755271,
            "url": "https://www.semanticscholar.org/paper/9f2e34581ca03160e8fd8b770203a5e7c2a902d3",
            "title": "RRL: Resnet as representation for Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2107-03380",
                "ArXiv": "2107.03380",
                "CorpusId": 235755271
            },
            "abstract": "The ability to autonomously learn behaviors via direct interactions in uninstrumented environments can lead to generalist robots capable of enhancing productivity or providing care in unstructured settings like homes. Such uninstrumented settings warrant operations only using the robot's proprioceptive sensor such as onboard cameras, joint encoders, etc which can be challenging for policy learning owing to the high dimensionality and partial observability issues. We propose RRL: Resnet as representation for Reinforcement Learning -- a straightforward yet effective approach that can learn complex behaviors directly from proprioceptive inputs. RRL fuses features extracted from pre-trained Resnet into the standard reinforcement learning pipeline and delivers results comparable to learning directly from the state. In a simulated dexterous manipulation benchmark, where the state of the art methods fail to make significant progress, RRL delivers contact rich behaviors. The appeal of RRL lies in its simplicity in bringing together progress from the fields of Representation Learning, Imitation Learning, and Reinforcement Learning. Its effectiveness in learning behaviors directly from visual inputs with performance and sample efficiency matching learning directly from the state, even in complex high dimensional domains, is far from obvious.",
            "referenceCount": 67,
            "citationCount": 70,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-07-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2107.03380"
            },
            "citationStyles": {
                "bibtex": "@Article{Shah2021RRLRA,\n author = {Rutav Shah and Vikash Kumar},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {RRL: Resnet as representation for Reinforcement Learning},\n volume = {abs/2107.03380},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b32ffe8f86bf0c36d85f8274ff0b6e2c9690a133",
            "@type": "ScholarlyArticle",
            "paperId": "b32ffe8f86bf0c36d85f8274ff0b6e2c9690a133",
            "corpusId": 141460093,
            "url": "https://www.semanticscholar.org/paper/b32ffe8f86bf0c36d85f8274ff0b6e2c9690a133",
            "title": "Information-Theoretic Considerations in Batch Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/icml/ChenJ19",
                "ArXiv": "1905.00360",
                "MAG": "2942709080",
                "CorpusId": 141460093
            },
            "abstract": "Value-function approximation methods that operate in batch mode have foundational importance to reinforcement learning (RL). Finite sample guarantees for these methods often crucially rely on two types of assumptions: (1) mild distribution shift, and (2) representation conditions that are stronger than realizability. However, the necessity (\"why do we need them?\") and the naturalness (\"when do they hold?\") of such assumptions have largely eluded the literature. In this paper, we revisit these assumptions and provide theoretical results towards answering the above questions, and make steps towards a deeper understanding of value-function approximation.",
            "referenceCount": 58,
            "citationCount": 273,
            "influentialCitationCount": 63,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-05-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2019InformationTheoreticCI,\n author = {Jinglin Chen and Nan Jiang},\n booktitle = {International Conference on Machine Learning},\n pages = {1042-1051},\n title = {Information-Theoretic Considerations in Batch Reinforcement Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:17985b57240bfaea02a6098a7a34e71e780180eb",
            "@type": "ScholarlyArticle",
            "paperId": "17985b57240bfaea02a6098a7a34e71e780180eb",
            "corpusId": 221761383,
            "url": "https://www.semanticscholar.org/paper/17985b57240bfaea02a6098a7a34e71e780180eb",
            "title": "Decoupling Representation Learning from Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3085605093",
                "DBLP": "journals/corr/abs-2009-08319",
                "ArXiv": "2009.08319",
                "CorpusId": 221761383
            },
            "abstract": "In an effort to overcome limitations of reward-driven feature learning in deep reinforcement learning (RL) from images, we propose decoupling representation learning from policy learning. To this end, we introduce a new unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC), which trains a convolutional encoder to associate pairs of observations separated by a short time difference, under image augmentations and using a contrastive loss. In online RL experiments, we show that training the encoder exclusively using ATC matches or outperforms end-to-end RL in most environments. Additionally, we benchmark several leading UL algorithms by pre-training encoders on expert demonstrations and using them, with weights frozen, in RL agents; we find that agents using ATC-trained encoders outperform all others. We also train multi-task encoders on data from multiple environments and show generalization to different downstream RL tasks. Finally, we ablate components of ATC, and introduce a new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when RL requires augmentation. Our experiments span visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and our complete code is available at this https URL.",
            "referenceCount": 42,
            "citationCount": 242,
            "influentialCitationCount": 29,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-09-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2009.08319"
            },
            "citationStyles": {
                "bibtex": "@Article{Stooke2020DecouplingRL,\n author = {Adam Stooke and Kimin Lee and P. Abbeel and M. Laskin},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Decoupling Representation Learning from Reinforcement Learning},\n volume = {abs/2009.08319},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:eb37e7b76d26b75463df22b2a3aa32b6a765c672",
            "@type": "ScholarlyArticle",
            "paperId": "eb37e7b76d26b75463df22b2a3aa32b6a765c672",
            "corpusId": 49470584,
            "url": "https://www.semanticscholar.org/paper/eb37e7b76d26b75463df22b2a3aa32b6a765c672",
            "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1806.10293",
                "DBLP": "journals/corr/abs-1806-10293",
                "MAG": "2951747857",
                "CorpusId": 49470584
            },
            "abstract": "In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.",
            "referenceCount": 49,
            "citationCount": 1112,
            "influentialCitationCount": 62,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.10293"
            },
            "citationStyles": {
                "bibtex": "@Article{Kalashnikov2018QTOptSD,\n author = {Dmitry Kalashnikov and A. Irpan and P. Pastor and Julian Ibarz and Alexander Herzog and Eric Jang and Deirdre Quillen and E. Holly and Mrinal Kalakrishnan and Vincent Vanhoucke and S. Levine},\n booktitle = {Conference on Robot Learning},\n journal = {ArXiv},\n title = {QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation},\n volume = {abs/1806.10293},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:498238a3bd5fd322fc3ce1572e33bbe3853a356f",
            "@type": "ScholarlyArticle",
            "paperId": "498238a3bd5fd322fc3ce1572e33bbe3853a356f",
            "corpusId": 4884302,
            "url": "https://www.semanticscholar.org/paper/498238a3bd5fd322fc3ce1572e33bbe3853a356f",
            "title": "Deep Reinforcement Learning: A Brief Survey",
            "venue": "IEEE Signal Processing Magazine",
            "publicationVenue": {
                "id": "urn:research:f62e5eab-173a-4e0a-a963-ed8de9835d22",
                "name": "IEEE Signal Processing Magazine",
                "alternate_names": [
                    "IEEE Signal Process Mag"
                ],
                "issn": "1053-5888",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=79"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "3100789280",
                "ArXiv": "1708.05866",
                "DBLP": "journals/spm/ArulkumaranDBB17",
                "DOI": "10.1109/MSP.2017.2743240",
                "CorpusId": 4884302
            },
            "abstract": "Deep reinforcement learning (DRL) is poised to revolutionize the field of artificial intelligence (AI) and represents a step toward building autonomous systems with a higherlevel understanding of the visual world. Currently, deep learning is enabling reinforcement learning (RL) to scale to problems that were previously intractable, such as learning to play video games directly from pixels. DRL algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of RL, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep RL, including the deep Q-network (DQN), trust region policy optimization (TRPO), and asynchronous advantage actor critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via RL. To conclude, we describe several current areas of research within the field.",
            "referenceCount": 181,
            "citationCount": 1894,
            "influentialCitationCount": 119,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1708.05866",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-08-19",
            "journal": {
                "name": "IEEE Signal Processing Magazine",
                "volume": "34"
            },
            "citationStyles": {
                "bibtex": "@Article{Arulkumaran2017DeepRL,\n author = {Kai Arulkumaran and M. Deisenroth and Miles Brundage and A. Bharath},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Processing Magazine},\n pages = {26-38},\n title = {Deep Reinforcement Learning: A Brief Survey},\n volume = {34},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8fe44af15f0e31c090c0dde4b606e91360a6fb74",
            "@type": "ScholarlyArticle",
            "paperId": "8fe44af15f0e31c090c0dde4b606e91360a6fb74",
            "corpusId": 237303796,
            "url": "https://www.semanticscholar.org/paper/8fe44af15f0e31c090c0dde4b606e91360a6fb74",
            "title": "Deep reinforcement learning in computer vision: a comprehensive survey",
            "venue": "Artificial Intelligence Review",
            "publicationVenue": {
                "id": "urn:research:ea8553fe-2467-4367-afee-c4deb3754820",
                "name": "Artificial Intelligence Review",
                "alternate_names": [
                    "Artif Intell Rev"
                ],
                "issn": "0269-2821",
                "url": "https://link.springer.com/journal/10462"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2108.11510",
                "DBLP": "journals/corr/abs-2108-11510",
                "DOI": "10.1007/s10462-021-10061-9",
                "CorpusId": 237303796
            },
            "abstract": null,
            "referenceCount": 140,
            "citationCount": 77,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2108.11510",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-08-25",
            "journal": {
                "name": "Artificial Intelligence Review",
                "volume": "55"
            },
            "citationStyles": {
                "bibtex": "@Article{Le2021DeepRL,\n author = {Ngan T. H. Le and V. Rathour and Kashu Yamazaki and Khoa Luu and M. Savvides},\n booktitle = {Artificial Intelligence Review},\n journal = {Artificial Intelligence Review},\n pages = {2733 - 2819},\n title = {Deep reinforcement learning in computer vision: a comprehensive survey},\n volume = {55},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7c4356ec0dca6e6df0af7a882e2cd1571c8bf3dc",
            "@type": "ScholarlyArticle",
            "paperId": "7c4356ec0dca6e6df0af7a882e2cd1571c8bf3dc",
            "corpusId": 222163237,
            "url": "https://www.semanticscholar.org/paper/7c4356ec0dca6e6df0af7a882e2cd1571c8bf3dc",
            "title": "Data-Efficient Reinforcement Learning with Self-Predictive Representations",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/iclr/SchwarzerAGHCB21",
                "ArXiv": "2007.05929",
                "CorpusId": 222163237
            },
            "abstract": "While deep reinforcement learning excels at solving tasks where large amounts of data can be collected through virtually unlimited interaction with the environment, learning from limited interaction remains a key challenge. We posit that an agent can learn more efficiently if we augment reward maximization with self-supervised objectives based on structure in its visual input and sequential interaction with the environment. Our method, Self-Predictive Representations(SPR), trains an agent to predict its own latent state representations multiple steps into the future. We compute target representations for future states using an encoder which is an exponential moving average of the agent's parameters and we make predictions using a learned transition model. On its own, this future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. We further improve performance by adding data augmentation to the future prediction loss, which forces the agent's representations to be consistent across multiple views of an observation. Our full self-supervised objective, which combines future prediction and data augmentation, achieves a median human-normalized score of 0.415 on Atari in a setting limited to 100k steps of environment interaction, which represents a 55% relative improvement over the previous state-of-the-art. Notably, even in this limited data regime, SPR exceeds expert human scores on 7 out of 26 games. The code associated with this work is available at https://github.com/mila-iqia/spr",
            "referenceCount": 58,
            "citationCount": 199,
            "influentialCitationCount": 38,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-07-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Schwarzer2020DataEfficientRL,\n author = {Max Schwarzer and Ankesh Anand and Rishab Goel and R. Devon Hjelm and Aaron C. Courville and Philip Bachman},\n booktitle = {International Conference on Learning Representations},\n title = {Data-Efficient Reinforcement Learning with Self-Predictive Representations},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:680c260ccdba6676a3ee0b9f43979f3b38b26888",
            "@type": "ScholarlyArticle",
            "paperId": "680c260ccdba6676a3ee0b9f43979f3b38b26888",
            "corpusId": 2732810,
            "url": "https://www.semanticscholar.org/paper/680c260ccdba6676a3ee0b9f43979f3b38b26888",
            "title": "The neural basis of human error processing: reinforcement learning, dopamine, and the error-related negativity.",
            "venue": "Psychology Review",
            "publicationVenue": {
                "id": "urn:research:dfb7f114-609c-4c34-9ee7-8cb1fc3e4a6b",
                "name": "Psychology Review",
                "alternate_names": [
                    "Psychol rev",
                    "Psychological review",
                    "Psychol Rev",
                    "Psychological Review"
                ],
                "issn": "1354-1129",
                "url": "http://www.apa.org/journals/rev/"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "1513825544",
                "DOI": "10.1037/0033-295X.109.4.679",
                "CorpusId": 2732810,
                "PubMed": "12374324"
            },
            "abstract": "The authors present a unified account of 2 neural systems concerned with the development and expression of adaptive behaviors: a mesencephalic dopamine system for reinforcement learning and a \"generic\" error-processing system associated with the anterior cingulate cortex. The existence of the error-processing system has been inferred from the error-related negativity (ERN), a component of the event-related brain potential elicited when human participants commit errors in reaction-time tasks. The authors propose that the ERN is generated when a negative reinforcement learning signal is conveyed to the anterior cingulate cortex via the mesencephalic dopamine system and that this signal is used by the anterior cingulate cortex to modify performance on the task at hand. They provide support for this proposal using both computational modeling and psychophysiological experimentation.",
            "referenceCount": 310,
            "citationCount": 3742,
            "influentialCitationCount": 379,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://brainvitge.org/papers/Holroyd_%26_Coles_2002.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "ClinicalTrial"
            ],
            "publicationDate": "2002-09-29",
            "journal": {
                "name": "Psychological review",
                "volume": "109 4"
            },
            "citationStyles": {
                "bibtex": "@Article{Holroyd2002TheNB,\n author = {Clay B. Holroyd and M. Coles},\n booktitle = {Psychology Review},\n journal = {Psychological review},\n pages = {\n          679-709\n        },\n title = {The neural basis of human error processing: reinforcement learning, dopamine, and the error-related negativity.},\n volume = {109 4},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:38fb1902c6a2ab4f767d4532b28a92473ea737aa",
            "@type": "ScholarlyArticle",
            "paperId": "38fb1902c6a2ab4f767d4532b28a92473ea737aa",
            "corpusId": 33081038,
            "url": "https://www.semanticscholar.org/paper/38fb1902c6a2ab4f767d4532b28a92473ea737aa",
            "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/abs-1712-01815",
                "ArXiv": "1712.01815",
                "MAG": "2772709170",
                "CorpusId": 33081038
            },
            "abstract": "The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.",
            "referenceCount": 48,
            "citationCount": 1348,
            "influentialCitationCount": 115,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-12-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1712.01815"
            },
            "citationStyles": {
                "bibtex": "@Article{Silver2017MasteringCA,\n author = {David Silver and T. Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and A. Guez and Marc Lanctot and L. Sifre and D. Kumaran and T. Graepel and T. Lillicrap and K. Simonyan and D. Hassabis},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},\n volume = {abs/1712.01815},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d85623ffae865f9ef386644dd02d0ea2d6a8c8de",
            "@type": "ScholarlyArticle",
            "paperId": "d85623ffae865f9ef386644dd02d0ea2d6a8c8de",
            "corpusId": 49309248,
            "url": "https://www.semanticscholar.org/paper/d85623ffae865f9ef386644dd02d0ea2d6a8c8de",
            "title": "Implicit Quantile Networks for Distributional Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2803308811",
                "ArXiv": "1806.06923",
                "DBLP": "journals/corr/abs-1806-06923",
                "CorpusId": 49309248
            },
            "abstract": "In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.",
            "referenceCount": 48,
            "citationCount": 389,
            "influentialCitationCount": 125,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-06-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.06923"
            },
            "citationStyles": {
                "bibtex": "@Article{Dabney2018ImplicitQN,\n author = {Will Dabney and Georg Ostrovski and David Silver and R. Munos},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Implicit Quantile Networks for Distributional Reinforcement Learning},\n volume = {abs/1806.06923},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:33690ff21ef1efb576410e656f2e60c89d0307d6",
            "@type": "ScholarlyArticle",
            "paperId": "33690ff21ef1efb576410e656f2e60c89d0307d6",
            "corpusId": 4674781,
            "url": "https://www.semanticscholar.org/paper/33690ff21ef1efb576410e656f2e60c89d0307d6",
            "title": "Deep Reinforcement Learning that Matters",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/aaai/0002IBPPM18",
                "MAG": "2963120839",
                "ArXiv": "1709.06560",
                "DOI": "10.1609/aaai.v32i1.11694",
                "CorpusId": 4674781
            },
            "abstract": "\n \n In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.\n \n",
            "referenceCount": 43,
            "citationCount": 1553,
            "influentialCitationCount": 100,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11694/11553",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-09-19",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Henderson2017DeepRL,\n author = {Peter Henderson and Riashat Islam and Philip Bachman and Joelle Pineau and Doina Precup and D. Meger},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {3207-3214},\n title = {Deep Reinforcement Learning that Matters},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a4af0eb16016c5e1f36bafb7291a8a4932c0f17c",
            "@type": "ScholarlyArticle",
            "paperId": "a4af0eb16016c5e1f36bafb7291a8a4932c0f17c",
            "corpusId": 166228069,
            "url": "https://www.semanticscholar.org/paper/a4af0eb16016c5e1f36bafb7291a8a4932c0f17c",
            "title": "Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1905.10389",
                "DBLP": "conf/icml/YangW20",
                "MAG": "2946300093",
                "CorpusId": 166228069
            },
            "abstract": "Exploration in reinforcement learning (RL) suffers from the curse of dimensionality when the state-action space is large. A common practice is to parameterize the high-dimensional value and policy functions using given features. However existing methods either have no theoretical guarantee or suffer a regret that is exponential in the planning horizon $H$. In this paper, we propose an online RL algorithm, namely the MatrixRL, that leverages ideas from linear bandit to learn a low-dimensional representation of the probability transition model while carefully balancing the exploitation-exploration tradeoff. We show that MatrixRL achieves a regret bound ${O}\\big(H^2d\\log T\\sqrt{T}\\big)$ where $d$ is the number of features. MatrixRL has an equivalent kernelized version, which is able to work with an arbitrary kernel Hilbert space without using explicit features. In this case, the kernelized MatrixRL satisfies a regret bound ${O}\\big(H^2\\widetilde{d}\\log T\\sqrt{T}\\big)$, where $\\widetilde{d}$ is the effective dimension of the kernel space. To our best knowledge, for RL using features or kernels, our results are the first regret bounds that are near-optimal in time $T$ and dimension $d$ (or $\\widetilde{d}$) and polynomial in the planning horizon $H$.",
            "referenceCount": 49,
            "citationCount": 244,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-05-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2019ReinforcementLI,\n author = {Lin F. Yang and Mengdi Wang},\n booktitle = {International Conference on Machine Learning},\n pages = {10746-10756},\n title = {Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d423fa6cd0f4d088941d9fe4bebd834d0137c9b9",
            "@type": "ScholarlyArticle",
            "paperId": "d423fa6cd0f4d088941d9fe4bebd834d0137c9b9",
            "corpusId": 221687655,
            "url": "https://www.semanticscholar.org/paper/d423fa6cd0f4d088941d9fe4bebd834d0137c9b9",
            "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation",
            "venue": "Annual Conference Computational Learning Theory",
            "publicationVenue": {
                "id": "urn:research:24b0721b-0592-414a-ac79-7271515aaab0",
                "name": "Annual Conference Computational Learning Theory",
                "alternate_names": [
                    "Conf Learn Theory",
                    "COLT",
                    "Conference on Learning Theory",
                    "Annu Conf Comput Learn Theory"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=536"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/colt/JinYWJ20",
                "MAG": "3046395471",
                "ArXiv": "1907.05388",
                "DOI": "10.1287/moor.2022.1309",
                "CorpusId": 221687655
            },
            "abstract": "Modern reinforcement learning (RL) is commonly applied to practical problems with an enormous number of states, where function approximation must be deployed to approximate either the value function or the policy. The introduction of function approximation raises a fundamental set of challenges involving computational and statistical efficiency, especially given the need to manage the exploration/exploitation trade-off. As a result, a core RL question remains open: how can we design provably efficient RL algorithms that incorporate function approximation? This question persists even in a basic setting with linear dynamics and linear rewards, for which only linear function approximation is needed. This paper presents the first provable RL algorithm with both polynomial run time and polynomial sample complexity in this linear setting, without requiring a \u201csimulator\u201d or additional assumptions. Concretely, we prove that an optimistic modification of least-squares value iteration\u2014a classical algorithm frequently studied in the linear setting\u2014achieves [Formula: see text] regret, where d is the ambient dimension of feature space, H is the length of each episode, and T is the total number of steps. Importantly, such regret is independent of the number of states and actions. Funding: This work was supported by the Defense Advanced Research Projects Agency program on Lifelong Learning Machines.",
            "referenceCount": 52,
            "citationCount": 242,
            "influentialCitationCount": 76,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1907.05388",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-07-11",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Jin2019ProvablyER,\n author = {Chi Jin and Zhuoran Yang and Zhaoran Wang and Michael I. Jordan},\n booktitle = {Annual Conference Computational Learning Theory},\n pages = {2137-2143},\n title = {Provably Efficient Reinforcement Learning with Linear Function Approximation},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4ee802a58d32aa049d549d06be440ac947b53987",
            "@type": "ScholarlyArticle",
            "paperId": "4ee802a58d32aa049d549d06be440ac947b53987",
            "corpusId": 11410889,
            "url": "https://www.semanticscholar.org/paper/4ee802a58d32aa049d549d06be440ac947b53987",
            "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/SalimansHCS17",
                "MAG": "2596367596",
                "ArXiv": "1703.03864",
                "CorpusId": 11410889
            },
            "abstract": "We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.",
            "referenceCount": 47,
            "citationCount": 1286,
            "influentialCitationCount": 221,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-03-10",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1703.03864"
            },
            "citationStyles": {
                "bibtex": "@Article{Salimans2017EvolutionSA,\n author = {Tim Salimans and Jonathan Ho and Xi Chen and Ilya Sutskever},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Evolution Strategies as a Scalable Alternative to Reinforcement Learning},\n volume = {abs/1703.03864},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a89d9eed2cc66be4645469cc7086b6e8a3cf6a96",
            "@type": "ScholarlyArticle",
            "paperId": "a89d9eed2cc66be4645469cc7086b6e8a3cf6a96",
            "corpusId": 198899862,
            "url": "https://www.semanticscholar.org/paper/a89d9eed2cc66be4645469cc7086b6e8a3cf6a96",
            "title": "Google Research Football: A Novel Reinforcement Learning Environment",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1907.11180",
                "MAG": "2997502221",
                "DBLP": "conf/aaai/KurachRSZBERVMB20",
                "DOI": "10.1609/AAAI.V34I04.5878",
                "CorpusId": 198899862
            },
            "abstract": "Recent progress in the field of reinforcement learning has been accelerated by virtual learning environments such as video games, where novel algorithms and ideas can be quickly tested in a safe and reproducible manner. We introduce the Google Research Football Environment, a new reinforcement learning environment where agents are trained to play football in an advanced, physics-based 3D simulator. The resulting environment is challenging, easy to use and customize, and it is available under a permissive open-source license. In addition, it provides support for multiplayer and multi-agent experiments. We propose three full-game scenarios of varying difficulty with the Football Benchmarks and report baseline results for three commonly used reinforcement algorithms (IMPALA, PPO, and Ape-X DQN). We also provide a diverse set of simpler scenarios with the Football Academy and showcase several promising research directions.",
            "referenceCount": 32,
            "citationCount": 237,
            "influentialCitationCount": 59,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/5878/5734",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-07-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1907.11180"
            },
            "citationStyles": {
                "bibtex": "@Article{Kurach2019GoogleRF,\n author = {Karol Kurach and Anton Raichuk and P. Sta\u0144czyk and Michal Zajac and Olivier Bachem and Lasse Espeholt and C. Riquelme and Damien Vincent and Marcin Michalski and O. Bousquet and S. Gelly},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Google Research Football: A Novel Reinforcement Learning Environment},\n volume = {abs/1907.11180},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:79c8f930bb66c82421b84617e4b6c0b2855cd063",
            "@type": "ScholarlyArticle",
            "paperId": "79c8f930bb66c82421b84617e4b6c0b2855cd063",
            "corpusId": 53018231,
            "url": "https://www.semanticscholar.org/paper/79c8f930bb66c82421b84617e4b6c0b2855cd063",
            "title": "Applications of Deep Reinforcement Learning in Communications and Networking: A Survey",
            "venue": "IEEE Communications Surveys and Tutorials",
            "publicationVenue": {
                "id": "urn:research:95d0dda7-5d58-4afd-b59f-315447b81992",
                "name": "IEEE Communications Surveys and Tutorials",
                "alternate_names": [
                    "IEEE Commun Surv Tutor"
                ],
                "issn": "1553-877X",
                "url": "http://www.comsoc.org/cst"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2952941635",
                "DBLP": "journals/corr/abs-1810-07862",
                "ArXiv": "1810.07862",
                "DOI": "10.1109/COMST.2019.2916583",
                "CorpusId": 53018231
            },
            "abstract": "This paper presents a comprehensive literature review on applications of deep reinforcement learning (DRL) in communications and networking. Modern networks, e.g., Internet of Things (IoT) and unmanned aerial vehicle (UAV) networks, become more decentralized and autonomous. In such networks, network entities need to make decisions locally to maximize the network performance under uncertainty of network environment. Reinforcement learning has been efficiently used to enable the network entities to obtain the optimal policy including, e.g., decisions or actions, given their states when the state and action spaces are small. However, in complex and large-scale networks, the state and action spaces are usually large, and the reinforcement learning may not be able to find the optimal policy in reasonable time. Therefore, DRL, a combination of reinforcement learning with deep learning, has been developed to overcome the shortcomings. In this survey, we first give a tutorial of DRL from fundamental concepts to advanced models. Then, we review DRL approaches proposed to address emerging issues in communications and networking. The issues include dynamic network access, data rate control, wireless caching, data offloading, network security, and connectivity preservation which are all important to next generation networks, such as 5G and beyond. Furthermore, we present applications of DRL for traffic routing, resource sharing, and data collection. Finally, we highlight important challenges, open issues, and future research directions of applying DRL.",
            "referenceCount": 214,
            "citationCount": 1023,
            "influentialCitationCount": 52,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://opus.lib.uts.edu.au/bitstream/10453/138762/4/draft1.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-10-18",
            "journal": {
                "name": "IEEE Communications Surveys & Tutorials",
                "volume": "21"
            },
            "citationStyles": {
                "bibtex": "@Article{Luong2018ApplicationsOD,\n author = {Nguyen Cong Luong and D. Hoang and Shimin Gong and D. Niyato and Ping Wang and Ying-Chang Liang and Dong In Kim},\n booktitle = {IEEE Communications Surveys and Tutorials},\n journal = {IEEE Communications Surveys & Tutorials},\n pages = {3133-3174},\n title = {Applications of Deep Reinforcement Learning in Communications and Networking: A Survey},\n volume = {21},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:adcd4bfd88213da0c33d3cb7057411dd15b72c7a",
            "@type": "ScholarlyArticle",
            "paperId": "adcd4bfd88213da0c33d3cb7057411dd15b72c7a",
            "corpusId": 58097250,
            "url": "https://www.semanticscholar.org/paper/adcd4bfd88213da0c33d3cb7057411dd15b72c7a",
            "title": "End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/aaai/ChengOMB19",
                "ArXiv": "1903.08792",
                "MAG": "2924156739",
                "DOI": "10.1609/aaai.v33i01.33013387",
                "CorpusId": 58097250
            },
            "abstract": "Reinforcement Learning (RL) algorithms have found limited success beyond simulated applications, and one main reason is the absence of safety guarantees during the learning process. Real world systems would realistically fail or break before an optimal controller can be learned. To address this issue, we propose a controller architecture that combines (1) a model-free RL-based controller with (2) model-based controllers utilizing control barrier functions (CBFs) and (3) online learning of the unknown system dynamics, in order to ensure safety during learning. Our general framework leverages the success of RL algorithms to learn high-performance controllers, while the CBF-based controllers both guarantee safety and guide the learning process by constraining the set of explorable polices. We utilize Gaussian Processes (GPs) to model the system dynamics and its uncertainties. \nOur novel controller synthesis algorithm, RL-CBF, guarantees safety with high probability during the learning process, regardless of the RL algorithm used, and demonstrates greater policy exploration efficiency. We test our algorithm on (1) control of an inverted pendulum and (2) autonomous carfollowing with wireless vehicle-to-vehicle communication, and show that our algorithm attains much greater sample efficiency in learning than other state-of-the-art algorithms and maintains safety during the entire learning process.",
            "referenceCount": 35,
            "citationCount": 449,
            "influentialCitationCount": 34,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4213/4091",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-03-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Cheng2019EndtoEndSR,\n author = {Richard Cheng and G. Orosz and R. Murray and J. Burdick},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {3387-3395},\n title = {End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3a5ac09e759f3223ee78b995ae2b519efc0f9292",
            "@type": "ScholarlyArticle",
            "paperId": "3a5ac09e759f3223ee78b995ae2b519efc0f9292",
            "corpusId": 20718228,
            "url": "https://www.semanticscholar.org/paper/3a5ac09e759f3223ee78b995ae2b519efc0f9292",
            "title": "Introduction to Reinforcement Learning",
            "venue": "Deep Reinforcement Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "MAG": "3038962357",
                "DOI": "10.1007/978-981-13-8285-7_1",
                "CorpusId": 20718228
            },
            "abstract": null,
            "referenceCount": 26,
            "citationCount": 472,
            "influentialCitationCount": 36,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Deep Reinforcement Learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sewak2019IntroductionTR,\n author = {Mohit Sewak},\n booktitle = {Deep Reinforcement Learning},\n journal = {Deep Reinforcement Learning},\n title = {Introduction to Reinforcement Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4b61c25a86083c20730c9b12737ac6ac4178c364",
            "@type": "ScholarlyArticle",
            "paperId": "4b61c25a86083c20730c9b12737ac6ac4178c364",
            "corpusId": 54434537,
            "url": "https://www.semanticscholar.org/paper/4b61c25a86083c20730c9b12737ac6ac4178c364",
            "title": "An Introduction to Deep Reinforcement Learning",
            "venue": "Found. Trends Mach. Learn.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "3100366369",
                "DBLP": "journals/corr/abs-1811-12560",
                "ArXiv": "1811.12560",
                "DOI": "10.1561/2200000071",
                "CorpusId": 54434537
            },
            "abstract": "Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.",
            "referenceCount": 352,
            "citationCount": 874,
            "influentialCitationCount": 50,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://repositorio.unal.edu.co/bitstream/unal/80758/2/98554412.2021.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-11-30",
            "journal": {
                "name": "Found. Trends Mach. Learn.",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Fran\u00e7ois-Lavet2018AnIT,\n author = {Vincent Fran\u00e7ois-Lavet and Peter Henderson and Riashat Islam and Marc G. Bellemare and Joelle Pineau},\n booktitle = {Found. Trends Mach. Learn.},\n journal = {Found. Trends Mach. Learn.},\n pages = {219-354},\n title = {An Introduction to Deep Reinforcement Learning},\n volume = {11},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:39b7007e6f3dd0744833f292f07ed77973503bfd",
            "@type": "ScholarlyArticle",
            "paperId": "39b7007e6f3dd0744833f292f07ed77973503bfd",
            "corpusId": 43920938,
            "url": "https://www.semanticscholar.org/paper/39b7007e6f3dd0744833f292f07ed77973503bfd",
            "title": "Data-Efficient Hierarchical Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/nips/NachumGLL18",
                "MAG": "2950614095",
                "ArXiv": "1805.08296",
                "CorpusId": 43920938
            },
            "abstract": "Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher- and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We find that our resulting HRL agent is generally applicable and highly sample-efficient. Our experiments show that our method can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.",
            "referenceCount": 48,
            "citationCount": 614,
            "influentialCitationCount": 90,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-05-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nachum2018DataEfficientHR,\n author = {Ofir Nachum and S. Gu and Honglak Lee and S. Levine},\n booktitle = {Neural Information Processing Systems},\n pages = {3307-3317},\n title = {Data-Efficient Hierarchical Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ad14227e4f51276892ffc37aa43fd8750bb5eba8",
            "@type": "ScholarlyArticle",
            "paperId": "ad14227e4f51276892ffc37aa43fd8750bb5eba8",
            "corpusId": 203610423,
            "url": "https://www.semanticscholar.org/paper/ad14227e4f51276892ffc37aa43fd8750bb5eba8",
            "title": "Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2978455699",
                "ArXiv": "1910.00177",
                "DBLP": "journals/corr/abs-1910-00177",
                "CorpusId": 203610423
            },
            "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.",
            "referenceCount": 40,
            "citationCount": 315,
            "influentialCitationCount": 38,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-10-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1910.00177"
            },
            "citationStyles": {
                "bibtex": "@Article{Peng2019AdvantageWeightedRS,\n author = {X. B. Peng and Aviral Kumar and Grace Zhang and S. Levine},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},\n volume = {abs/1910.00177},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6ff50528f3d7c72772f8c0e3f8398f9dd8e06575",
            "@type": "ScholarlyArticle",
            "paperId": "6ff50528f3d7c72772f8c0e3f8398f9dd8e06575",
            "corpusId": 166228022,
            "url": "https://www.semanticscholar.org/paper/6ff50528f3d7c72772f8c0e3f8398f9dd8e06575",
            "title": "Adversarial Policies: Attacking Deep Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1905.10615",
                "MAG": "2945924974",
                "DBLP": "conf/iclr/GleaveDWKLR20",
                "CorpusId": 166228022
            },
            "abstract": "Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at this https URL.",
            "referenceCount": 39,
            "citationCount": 275,
            "influentialCitationCount": 38,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-05-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1905.10615"
            },
            "citationStyles": {
                "bibtex": "@Article{Gleave2019AdversarialPA,\n author = {A. Gleave and Michael Dennis and Neel Kant and Cody Wild and S. Levine and Stuart J. Russell},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Adversarial Policies: Attacking Deep Reinforcement Learning},\n volume = {abs/1905.10615},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5a1b92aa50797a7c1e99b8840ff01aad66038596",
            "@type": "ScholarlyArticle",
            "paperId": "5a1b92aa50797a7c1e99b8840ff01aad66038596",
            "corpusId": 221971078,
            "url": "https://www.semanticscholar.org/paper/5a1b92aa50797a7c1e99b8840ff01aad66038596",
            "title": "Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey",
            "venue": "IEEE Symposium Series on Computational Intelligence",
            "publicationVenue": {
                "id": "urn:research:8a9e9f3b-a025-473d-801e-72cdb0653d22",
                "name": "IEEE Symposium Series on Computational Intelligence",
                "alternate_names": [
                    "IEEE Symp Ser Comput Intell",
                    "SSCI"
                ],
                "issn": null,
                "url": "http://www.ieee-ssci.org/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3088310808",
                "DBLP": "conf/ssci/ZhaoQW20",
                "ArXiv": "2009.13303",
                "DOI": "10.1109/SSCI47803.2020.9308468",
                "CorpusId": 221971078
            },
            "abstract": "Deep} reinforcement learning has recently seen huge success across multiple areas in the robotics domain. Owing to the limitations of gathering real-world data, i.e., sample inefficiency and the cost of collecting it, simulation environments are utilized for training the different agents. This not only aids in providing a potentially infinite data source, but also alleviates safety concerns with real robots. Nonetheless, the gap between the simulated and real worlds degrades the performance of the policies once the models are transferred into real robots. Multiple research efforts are therefore now being directed towards closing this sim-toreal gap and accomplish more efficient policy transfer. Recent years have seen the emergence of multiple methods applicable to different domains, but there is a lack, to the best of our knowledge, of a comprehensive review summarizing and putting into context the different methods. In this survey paper, we cover the fundamental background behind sim-to-real transfer in deep reinforcement learning and overview the main methods being utilized at the moment: domain randomization, domain adaptation, imitation learning, meta-learning and knowledge distillation. We categorize some of the most relevant recent works, and outline the main application scenarios. Finally, we discuss the main opportunities and challenges of the different approaches and point to the most promising directions.",
            "referenceCount": 81,
            "citationCount": 374,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2009.13303",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-09-24",
            "journal": {
                "name": "2020 IEEE Symposium Series on Computational Intelligence (SSCI)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhao2020SimtoRealTI,\n author = {Wenshuai Zhao and J. P. Queralta and Tomi Westerlund},\n booktitle = {IEEE Symposium Series on Computational Intelligence},\n journal = {2020 IEEE Symposium Series on Computational Intelligence (SSCI)},\n pages = {737-744},\n title = {Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:88c831ce56b8fcdc47ab5a8f133743abf58c8883",
            "@type": "ScholarlyArticle",
            "paperId": "88c831ce56b8fcdc47ab5a8f133743abf58c8883",
            "corpusId": 174800601,
            "url": "https://www.semanticscholar.org/paper/88c831ce56b8fcdc47ab5a8f133743abf58c8883",
            "title": "A Deep Reinforcement Learning Perspective on Internet Congestion Control",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2946348792",
                "DBLP": "conf/icml/JayRGST19",
                "CorpusId": 174800601
            },
            "abstract": "We present and investigate a novel and timely application domain for deep reinforcement learning (RL): Internet congestion control. Congestion control is the core networking task of modulat-ing traf\ufb01c sources\u2019 data-transmission rates to ef\ufb01ciently utilize network capacity, and is the subject of extensive attention in light of the advent of Internet services such as live video, virtual reality, Internet-of-Things, and more. We show that casting congestion control as RL enables training deep network policies that capture intricate patterns in data traf\ufb01c and network conditions, and leverage this to outperform the state-of-the-art. We also highlight signi\ufb01cant challenges facing real-world adoption of RL-based congestion control, including fairness, safety, and generalization, which are not trivial to address within conventional RL formalism. To facilitate further research and reproducibility of our results, we present a test suite for RL-guided congestion control based on the OpenAI Gym interface.",
            "referenceCount": 37,
            "citationCount": 213,
            "influentialCitationCount": 41,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-05-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Jay2019ADR,\n author = {Nathan Jay and Noga H. Rotman and Brighten Godfrey and Michael Schapira and Aviv Tamar},\n booktitle = {International Conference on Machine Learning},\n pages = {3050-3059},\n title = {A Deep Reinforcement Learning Perspective on Internet Congestion Control},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1464776f20e2bccb6182f183b5ff2e15b0ae5e56",
            "@type": "ScholarlyArticle",
            "paperId": "1464776f20e2bccb6182f183b5ff2e15b0ae5e56",
            "corpusId": 12296499,
            "url": "https://www.semanticscholar.org/paper/1464776f20e2bccb6182f183b5ff2e15b0ae5e56",
            "title": "Benchmarking Deep Reinforcement Learning for Continuous Control",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/icml/DuanCHSA16",
                "MAG": "2963641140",
                "ArXiv": "1604.06778",
                "CorpusId": 12296499
            },
            "abstract": "Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at this https URL in order to facilitate experimental reproducibility and to encourage adoption by other researchers.",
            "referenceCount": 86,
            "citationCount": 1509,
            "influentialCitationCount": 122,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-04-22",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Duan2016BenchmarkingDR,\n author = {Yan Duan and Xi Chen and Rein Houthooft and J. Schulman and P. Abbeel},\n booktitle = {International Conference on Machine Learning},\n pages = {1329-1338},\n title = {Benchmarking Deep Reinforcement Learning for Continuous Control},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0366b6396610708a77540564050a90a761a28937",
            "@type": "ScholarlyArticle",
            "paperId": "0366b6396610708a77540564050a90a761a28937",
            "corpusId": 46892983,
            "url": "https://www.semanticscholar.org/paper/0366b6396610708a77540564050a90a761a28937",
            "title": "Reinforcement Learning for Solving the Vehicle Routing Problem",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2949922211",
                "ArXiv": "1802.04240",
                "DBLP": "conf/nips/NazariOST18",
                "CorpusId": 46892983
            },
            "abstract": "We present an end-to-end framework for solving the Vehicle Routing Problem (VRP) using reinforcement learning. In this approach, we train a single model that finds near-optimal solutions for problem instances sampled from a given distribution, only by observing the reward signals and following feasibility rules. Our model represents a parameterized stochastic policy, and by applying a policy gradient algorithm to optimize its parameters, the trained model produces the solution as a sequence of consecutive actions in real time, without the need to re-train for every new problem instance. On capacitated VRP, our approach outperforms classical heuristics and Google's OR-Tools on medium-sized instances in solution quality with comparable computation time (after training). We demonstrate how our approach can handle problems with split delivery and explore the effect of such deliveries on the solution quality. Our proposed framework can be applied to other variants of the VRP such as the stochastic VRP, and has the potential to be applied more generally to combinatorial optimization problems.",
            "referenceCount": 38,
            "citationCount": 621,
            "influentialCitationCount": 73,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nazari2018ReinforcementLF,\n author = {M. Nazari and Afshin Oroojlooy and L. Snyder and Martin Tak\u00e1c},\n booktitle = {Neural Information Processing Systems},\n pages = {9861-9871},\n title = {Reinforcement Learning for Solving the Vehicle Routing Problem},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:86a9a8cff082ddba64e00969274d4408db2b4811",
            "@type": "ScholarlyArticle",
            "paperId": "86a9a8cff082ddba64e00969274d4408db2b4811",
            "corpusId": 210222104,
            "url": "https://www.semanticscholar.org/paper/86a9a8cff082ddba64e00969274d4408db2b4811",
            "title": "A distributional code for value in dopamine-based reinforcement learning",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/nature/DabneyKUSHMB20",
                "MAG": "2999778183",
                "DOI": "10.1038/s41586-019-1924-6",
                "CorpusId": 210222104,
                "PubMed": "31942076"
            },
            "abstract": null,
            "referenceCount": 39,
            "citationCount": 276,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc7476215?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-01-01",
            "journal": {
                "name": "Nature",
                "volume": "577"
            },
            "citationStyles": {
                "bibtex": "@Article{Dabney2020ADC,\n author = {Will Dabney and Z. Kurth-Nelson and N. Uchida and C. Starkweather and D. Hassabis and R. Munos and M. Botvinick},\n booktitle = {Nature},\n journal = {Nature},\n pages = {671 - 675},\n title = {A distributional code for value in dopamine-based reinforcement learning},\n volume = {577},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:59aab88aba5115e6af18e7f0c32b692a17091c56",
            "@type": "ScholarlyArticle",
            "paperId": "59aab88aba5115e6af18e7f0c32b692a17091c56",
            "corpusId": 218487167,
            "url": "https://www.semanticscholar.org/paper/59aab88aba5115e6af18e7f0c32b692a17091c56",
            "title": "Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey",
            "venue": "IEEE transactions on intelligent transportation systems (Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3022861755",
                "DBLP": "journals/tits/HaydariY22",
                "ArXiv": "2005.00935",
                "DOI": "10.1109/tits.2020.3008612",
                "CorpusId": 218487167
            },
            "abstract": "Latest technological improvements increased the quality of transportation. New data-driven approaches bring out a new research direction for all control-based systems, e.g., in transportation, robotics, IoT and power systems. Combining data-driven applications with transportation systems plays a key role in recent transportation applications. In this paper, the latest deep reinforcement learning (RL) based traffic control applications are surveyed. Specifically, traffic signal control (TSC) applications based on (deep) RL, which have been studied extensively in the literature, are discussed in detail. Different problem formulations, RL parameters, and simulation environments for TSC are discussed comprehensively. In the literature, there are also several autonomous driving applications studied with deep RL models. Our survey extensively summarizes existing works in this field by categorizing them with respect to application types, control models and studied algorithms. In the end, we discuss the challenges and open questions regarding deep RL-based transportation applications.",
            "referenceCount": 161,
            "citationCount": 238,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2005.00935",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-05-02",
            "journal": {
                "name": "IEEE Transactions on Intelligent Transportation Systems",
                "volume": "23"
            },
            "citationStyles": {
                "bibtex": "@Article{Haydari2020DeepRL,\n author = {Ammar Haydari and Y. Yilmaz},\n booktitle = {IEEE transactions on intelligent transportation systems (Print)},\n journal = {IEEE Transactions on Intelligent Transportation Systems},\n pages = {11-32},\n title = {Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey},\n volume = {23},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e102cd42c402026a3862d2e60a75eed7c78860a2",
            "@type": "ScholarlyArticle",
            "paperId": "e102cd42c402026a3862d2e60a75eed7c78860a2",
            "corpusId": 212657666,
            "url": "https://www.semanticscholar.org/paper/e102cd42c402026a3862d2e60a75eed7c78860a2",
            "title": "Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2003.04960",
                "MAG": "3085438811",
                "DBLP": "journals/corr/abs-2003-04960",
                "CorpusId": 212657666
            },
            "abstract": "Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.",
            "referenceCount": 119,
            "citationCount": 271,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-03-10",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2003.04960"
            },
            "citationStyles": {
                "bibtex": "@Article{Narvekar2020CurriculumLF,\n author = {Sanmit Narvekar and Bei Peng and M. Leonetti and J. Sinapov and Matthew E. Taylor and P. Stone},\n booktitle = {Journal of machine learning research},\n journal = {ArXiv},\n title = {Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey},\n volume = {abs/2003.04960},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba",
            "@type": "ScholarlyArticle",
            "paperId": "6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba",
            "corpusId": 19077536,
            "url": "https://www.semanticscholar.org/paper/6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba",
            "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1805-00909",
                "MAG": "2799151646",
                "ArXiv": "1805.00909",
                "CorpusId": 19077536
            },
            "abstract": "The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.",
            "referenceCount": 64,
            "citationCount": 515,
            "influentialCitationCount": 82,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-05-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1805.00909"
            },
            "citationStyles": {
                "bibtex": "@Article{Levine2018ReinforcementLA,\n author = {S. Levine},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review},\n volume = {abs/1805.00909},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:929bef0066bad871ba971b673c053112d055d29f",
            "@type": "ScholarlyArticle",
            "paperId": "929bef0066bad871ba971b673c053112d055d29f",
            "corpusId": 52939327,
            "url": "https://www.semanticscholar.org/paper/929bef0066bad871ba971b673c053112d055d29f",
            "title": "Actor-Attention-Critic for Multi-Agent Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2962966033",
                "DBLP": "journals/corr/abs-1810-02912",
                "ArXiv": "1810.02912",
                "CorpusId": 52939327
            },
            "abstract": "Reinforcement learning in multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in single-agent settings. We present an actor-critic algorithm that trains decentralized policies in multi-agent settings, using centrally computed critics that share an attention mechanism which selects relevant information for each agent at every timestep. This attention mechanism enables more effective and scalable learning in complex multi-agent environments, when compared to recent approaches. Our approach is applicable not only to cooperative settings with shared rewards, but also individualized reward settings, including adversarial settings, as well as settings that do not provide global states, and it makes no assumptions about the action spaces of the agents. As such, it is flexible enough to be applied to most multi-agent learning problems.",
            "referenceCount": 43,
            "citationCount": 538,
            "influentialCitationCount": 80,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-09-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Iqbal2018ActorAttentionCriticFM,\n author = {Shariq Iqbal and Fei Sha},\n booktitle = {International Conference on Machine Learning},\n pages = {2961-2970},\n title = {Actor-Attention-Critic for Multi-Agent Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:59562be2cf8e01e8b7bb7560cef56158ea171227",
            "@type": "ScholarlyArticle",
            "paperId": "59562be2cf8e01e8b7bb7560cef56158ea171227",
            "corpusId": 3510042,
            "url": "https://www.semanticscholar.org/paper/59562be2cf8e01e8b7bb7560cef56158ea171227",
            "title": "Ranking Sentences for Extractive Summarization with Reinforcement Learning",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:01103732-3808-4930-b8e4-7e9e68d5c68d",
                "name": "North American Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "North Am Chapter Assoc Comput Linguistics",
                    "NAACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/naacl"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2788283780",
                "ArXiv": "1802.08636",
                "ACL": "N18-1158",
                "DBLP": "journals/corr/abs-1802-08636",
                "DOI": "10.18653/v1/N18-1158",
                "CorpusId": 3510042
            },
            "abstract": "Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans.",
            "referenceCount": 72,
            "citationCount": 493,
            "influentialCitationCount": 78,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/N18-1158.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-02-23",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Narayan2018RankingSF,\n author = {Shashi Narayan and Shay B. Cohen and Mirella Lapata},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {1747-1759},\n title = {Ranking Sentences for Extractive Summarization with Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ef2bc452812d6005ab0a66af6c3f97b6b0ba837e",
            "@type": "ScholarlyArticle",
            "paperId": "ef2bc452812d6005ab0a66af6c3f97b6b0ba837e",
            "corpusId": 54448010,
            "url": "https://www.semanticscholar.org/paper/ef2bc452812d6005ab0a66af6c3f97b6b0ba837e",
            "title": "Quantifying Generalization in Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2903181768",
                "ArXiv": "1812.02341",
                "DBLP": "conf/icml/CobbeKHKS19",
                "CorpusId": 54448010
            },
            "abstract": "In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.",
            "referenceCount": 20,
            "citationCount": 497,
            "influentialCitationCount": 72,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-12-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Cobbe2018QuantifyingGI,\n author = {Karl Cobbe and Oleg Klimov and Christopher Hesse and Taehoon Kim and J. Schulman},\n booktitle = {International Conference on Machine Learning},\n pages = {1282-1289},\n title = {Quantifying Generalization in Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f8492a321d66c381637b693a24af994af41b3cdf",
            "@type": "ScholarlyArticle",
            "paperId": "f8492a321d66c381637b693a24af994af41b3cdf",
            "corpusId": 221761694,
            "url": "https://www.semanticscholar.org/paper/f8492a321d66c381637b693a24af994af41b3cdf",
            "title": "Transfer Learning in Deep Reinforcement Learning: A Survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2009.07888",
                "DBLP": "journals/corr/abs-2009-07888",
                "MAG": "3085267010",
                "DOI": "10.1109/TPAMI.2023.3292075",
                "CorpusId": 221761694,
                "PubMed": "37402188"
            },
            "abstract": "Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw connections between transfer learning and other relevant topics from the reinforcement learning perspective and explore their potential challenges that await future research progress.",
            "referenceCount": 234,
            "citationCount": 224,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2009.07888",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-09-16",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "45"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhu2020TransferLI,\n author = {Zhuangdi Zhu and Kaixiang Lin and Anil K. Jain and Jiayu Zhou},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {13344-13362},\n title = {Transfer Learning in Deep Reinforcement Learning: A Survey},\n volume = {45},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0881655dcdf891f529ebe7ac18301e138a5e265b",
            "@type": "ScholarlyArticle",
            "paperId": "0881655dcdf891f529ebe7ac18301e138a5e265b",
            "corpusId": 211204780,
            "url": "https://www.semanticscholar.org/paper/0881655dcdf891f529ebe7ac18301e138a5e265b",
            "title": "Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/iclr/SiegelSBANLHHR20",
                "ArXiv": "2002.08396",
                "MAG": "2995706821",
                "CorpusId": 211204780
            },
            "abstract": "Off-policy reinforcement learning algorithms promise to be applicable in settings where only a fixed data-set (batch) of environment interactions is available and no new experience can be acquired. This property makes these algorithms appealing for real world problems such as robot control. In practice, however, standard off-policy algorithms fail in the batch setting for continuous control. In this paper, we propose a simple solution to this problem. It admits the use of data generated by arbitrary behavior policies and uses a learned prior -- the advantage-weighted behavior model (ABM) -- to bias the RL policy towards actions that have previously been executed and are likely to be successful on the new task. Our method can be seen as an extension of recent work on batch-RL that enables stable learning from conflicting data-sources. We find improvements on competitive baselines in a variety of RL tasks -- including standard continuous control benchmarks and multi-task learning for simulated and real-world robots.",
            "referenceCount": 33,
            "citationCount": 230,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-02-19",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2002.08396"
            },
            "citationStyles": {
                "bibtex": "@Article{Siegel2020KeepDW,\n author = {Noah Siegel and J. T. Springenberg and Felix Berkenkamp and A. Abdolmaleki and Michael Neunert and Thomas Lampe and Roland Hafner and Martin A. Riedmiller},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning},\n volume = {abs/2002.08396},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:633870d249f03d39849224adc3381712fbb23ed8",
            "@type": "ScholarlyArticle",
            "paperId": "633870d249f03d39849224adc3381712fbb23ed8",
            "corpusId": 3318551,
            "url": "https://www.semanticscholar.org/paper/633870d249f03d39849224adc3381712fbb23ed8",
            "title": "Mean Field Multi-Agent Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1802-05438",
                "ArXiv": "1802.05438",
                "MAG": "2785315072",
                "CorpusId": 3318551
            },
            "abstract": "Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of user interactions. In this paper, we present Mean Field Reinforcement Learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent's optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies. We develop practical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution. Experiments on resource allocation, Ising model estimation, and battle game tasks verify the learning effectiveness of our mean field approaches in handling many-agent interactions in population.",
            "referenceCount": 55,
            "citationCount": 442,
            "influentialCitationCount": 61,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-02-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.05438"
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2018MeanFM,\n author = {Yaodong Yang and Rui Luo and Minne Li and M. Zhou and Weinan Zhang and Jun Wang},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Mean Field Multi-Agent Reinforcement Learning},\n volume = {abs/1802.05438},\n year = {2018}\n}\n"
            }
        }
    }
]