[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bf807c251962a694499b29938bdc54e716ca2dda",
            "@type": "ScholarlyArticle",
            "paperId": "bf807c251962a694499b29938bdc54e716ca2dda",
            "corpusId": 3138852,
            "url": "https://www.semanticscholar.org/paper/bf807c251962a694499b29938bdc54e716ca2dda",
            "title": "Reinforcement learning improves behaviour from evaluative feedback",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/nature/Littman15",
                "MAG": "1589747210",
                "DOI": "10.1038/nature14540",
                "CorpusId": 3138852,
                "PubMed": "26017443"
            },
            "abstract": null,
            "referenceCount": 65,
            "citationCount": 273,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2015-05-27",
            "journal": {
                "name": "Nature",
                "volume": "521"
            },
            "citationStyles": {
                "bibtex": "@Article{Littman2015ReinforcementLI,\n author = {M. Littman},\n booktitle = {Nature},\n journal = {Nature},\n pages = {445-451},\n title = {Reinforcement learning improves behaviour from evaluative feedback},\n volume = {521},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:382f2d3c7e318c3ad2de028c6598a9700899ce80",
            "@type": "ScholarlyArticle",
            "paperId": "382f2d3c7e318c3ad2de028c6598a9700899ce80",
            "corpusId": 261579713,
            "url": "https://www.semanticscholar.org/paper/382f2d3c7e318c3ad2de028c6598a9700899ce80",
            "title": "Introduction to Reinforcement Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1515851193",
                "CorpusId": 261579713
            },
            "abstract": "From the Publisher: \nIn Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.",
            "referenceCount": 0,
            "citationCount": 2718,
            "influentialCitationCount": 72,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1998-03-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Sutton1998IntroductionTR,\n author = {R. S. Sutton and A. Barto},\n title = {Introduction to Reinforcement Learning},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f28349edf0cd78d89d2e00ae5e4fcf589774826c",
            "@type": "ScholarlyArticle",
            "paperId": "f28349edf0cd78d89d2e00ae5e4fcf589774826c",
            "corpusId": 14147627,
            "url": "https://www.semanticscholar.org/paper/f28349edf0cd78d89d2e00ae5e4fcf589774826c",
            "title": "Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control",
            "venue": "IEEE International Conference on Robotics and Automation",
            "publicationVenue": {
                "id": "urn:research:3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                "name": "IEEE International Conference on Robotics and Automation",
                "alternate_names": [
                    "International Conference on Robotics and Automation",
                    "Int Conf Robot Autom",
                    "ICRA",
                    "IEEE Int Conf Robot Autom"
                ],
                "issn": "2152-4092",
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/corr/ZhangLMUC15",
                "MAG": "2963513913",
                "ArXiv": "1511.03791",
                "CorpusId": 14147627
            },
            "abstract": "This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only. The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time. We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation. A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.",
            "referenceCount": 21,
            "citationCount": 252,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-09-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1511.03791"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2015TowardsVD,\n author = {Fangyi Zhang and J. Leitner and Michael Milford and B. Upcroft and Peter Corke},\n booktitle = {IEEE International Conference on Robotics and Automation},\n journal = {ArXiv},\n title = {Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control},\n volume = {abs/1511.03791},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6118910c4014cc6c061198a2d88c080ab56ea452",
            "@type": "ScholarlyArticle",
            "paperId": "6118910c4014cc6c061198a2d88c080ab56ea452",
            "corpusId": 15986631,
            "url": "https://www.semanticscholar.org/paper/6118910c4014cc6c061198a2d88c080ab56ea452",
            "title": "Deep Reinforcement Learning with a Natural Language Action Space",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/acl/HeCHGLDO16",
                "ACL": "P16-1153",
                "MAG": "2950652962",
                "ArXiv": "1511.04636",
                "DOI": "10.18653/v1/P16-1153",
                "CorpusId": 15986631
            },
            "abstract": "This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text.",
            "referenceCount": 31,
            "citationCount": 196,
            "influentialCitationCount": 30,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/P16-1153.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-11-14",
            "journal": {
                "name": "arXiv: Artificial Intelligence",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{He2015DeepRL,\n author = {Ji He and Jianshu Chen and Xiaodong He and Jianfeng Gao and Lihong Li and L. Deng and Mari Ostendorf},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {arXiv: Artificial Intelligence},\n title = {Deep Reinforcement Learning with a Natural Language Action Space},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f926d3bf410875857effe1b5000a4fbc25397b74",
            "@type": "ScholarlyArticle",
            "paperId": "f926d3bf410875857effe1b5000a4fbc25397b74",
            "corpusId": 1557568,
            "url": "https://www.semanticscholar.org/paper/f926d3bf410875857effe1b5000a4fbc25397b74",
            "title": "Reinforcement Learning from Demonstration through Shaping",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2397581010",
                "DBLP": "conf/ijcai/BrysHSCTN15",
                "CorpusId": 1557568
            },
            "abstract": "Reinforcement learning describes how a learning agent can achieve optimal behaviour based on interactions with its environment and reward feedback. A limiting factor in reinforcement learning as employed in artificial intelligence is the need for an often prohibitively large number of environment samples before the agent reaches a desirable level of performance. Learning from demonstration is an approach that provides the agent with demonstrations by a supposed expert, from which it should derive suitable behaviour. Yet, one of the challenges of learning from demonstration is that no guarantees can be provided for the quality of the demonstrations, and thus the learned behavior. In this paper, we investigate the intersection of these two approaches, leveraging the theoretical guarantees provided by reinforcement learning, and using expert demonstrations to speed up this learning by biasing exploration through a process called reward shaping. This approach allows us to leverage human input without making an erroneous assumption regarding demonstration optimality. We show experimentally that this approach requires significantly fewer demonstrations, is more robust against suboptimality of demonstrations, and achieves much faster learning than the recently developed HAT algorithm.",
            "referenceCount": 37,
            "citationCount": 192,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-07-25",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Brys2015ReinforcementLF,\n author = {Tim Brys and A. Harutyunyan and Halit Bener Suay and S. Chernova and Matthew E. Taylor and A. Now\u00e9},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {3352-3358},\n title = {Reinforcement Learning from Demonstration through Shaping},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea59331b65129d8ac47bf159a0d0f43831738a14",
            "@type": "ScholarlyArticle",
            "paperId": "ea59331b65129d8ac47bf159a0d0f43831738a14",
            "corpusId": 62439933,
            "url": "https://www.semanticscholar.org/paper/ea59331b65129d8ac47bf159a0d0f43831738a14",
            "title": "Safe Reinforcement Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2121506959",
                "CorpusId": 62439933
            },
            "abstract": "SAFE REINFORCEMENT LEARNING",
            "referenceCount": 78,
            "citationCount": 113,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Thomas2015SafeRL,\n author = {Philip Thomas},\n title = {Safe Reinforcement Learning},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:467568f1777bc51a15a5100516cd4fe8de62b9ab",
            "@type": "ScholarlyArticle",
            "paperId": "467568f1777bc51a15a5100516cd4fe8de62b9ab",
            "corpusId": 17216004,
            "url": "https://www.semanticscholar.org/paper/467568f1777bc51a15a5100516cd4fe8de62b9ab",
            "title": "Transfer Learning for Reinforcement Learning Domains: A Survey",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "journals/jmlr/TaylorS09",
                "MAG": "2097381042",
                "DOI": "10.5555/1577069.1755839",
                "CorpusId": 17216004
            },
            "abstract": "The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.",
            "referenceCount": 131,
            "citationCount": 1728,
            "influentialCitationCount": 110,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2009-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Taylor2009TransferLF,\n author = {Matthew E. Taylor and P. Stone},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1633-1685},\n title = {Transfer Learning for Reinforcement Learning Domains: A Survey},\n volume = {10},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d356a5603f14c7a6873272774782d7812871f952",
            "@type": "ScholarlyArticle",
            "paperId": "d356a5603f14c7a6873272774782d7812871f952",
            "corpusId": 3582900,
            "url": "https://www.semanticscholar.org/paper/d356a5603f14c7a6873272774782d7812871f952",
            "title": "Reinforcement and Imitation Learning for Diverse Visuomotor Skills",
            "venue": "Robotics: Science and Systems",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2952854262",
                "DBLP": "conf/rss/Zhu0MRECTKHFH18",
                "ArXiv": "1802.09564",
                "DOI": "10.15607/RSS.2018.XIV.009",
                "CorpusId": 3582900
            },
            "abstract": "We propose a model-free deep reinforcement learning method that leverages a small amount of demonstration data to assist a reinforcement learning agent. We apply this approach to robotic manipulation tasks and train end-to-end visuomotor policies that map directly from RGB camera inputs to joint velocities. We demonstrate that our approach can solve a wide variety of visuomotor tasks, for which engineering a scripted controller would be laborious. In experiments, our reinforcement and imitation agent achieves significantly better performances than agents trained with reinforcement learning or imitation learning alone. We also illustrate that these policies, trained with large visual and dynamics variations, can achieve preliminary successes in zero-shot sim2real transfer. A brief visual description of this work can be viewed in this https URL",
            "referenceCount": 54,
            "citationCount": 273,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.15607/rss.2018.xiv.009",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-26",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.09564"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhu2018ReinforcementAI,\n author = {Yuke Zhu and Ziyun Wang and J. Merel and Andrei A. Rusu and Tom Erez and Serkan Cabi and S. Tunyasuvunakool and J\u00e1nos Kram\u00e1r and R. Hadsell and Nando de Freitas and N. Heess},\n booktitle = {Robotics: Science and Systems},\n journal = {ArXiv},\n title = {Reinforcement and Imitation Learning for Diverse Visuomotor Skills},\n volume = {abs/1802.09564},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ab68ddbdd8d0b61d9f9c8fa500a4c13d06158060",
            "@type": "ScholarlyArticle",
            "paperId": "ab68ddbdd8d0b61d9f9c8fa500a4c13d06158060",
            "corpusId": 12860852,
            "url": "https://www.semanticscholar.org/paper/ab68ddbdd8d0b61d9f9c8fa500a4c13d06158060",
            "title": "Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2949186857",
                "DBLP": "conf/nips/MohamedR15",
                "ArXiv": "1509.08731",
                "CorpusId": 12860852
            },
            "abstract": "The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm --- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.",
            "referenceCount": 31,
            "citationCount": 357,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-09-29",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1509.08731"
            },
            "citationStyles": {
                "bibtex": "@Article{Mohamed2015VariationalIM,\n author = {S. Mohamed and Danilo Jimenez Rezende},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning},\n volume = {abs/1509.08731},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ec85eeb27f71f389416ebbf6e13c725ddef78bea",
            "@type": "ScholarlyArticle",
            "paperId": "ec85eeb27f71f389416ebbf6e13c725ddef78bea",
            "corpusId": 15364622,
            "url": "https://www.semanticscholar.org/paper/ec85eeb27f71f389416ebbf6e13c725ddef78bea",
            "title": "Multi-objective reinforcement learning using sets of pareto dominating policies",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2186820913",
                "DBLP": "journals/jmlr/MoffaertN14",
                "DOI": "10.5555/2627435.2750356",
                "CorpusId": 15364622
            },
            "abstract": "Many real-world problems involve the optimization of multiple, possibly conicting objectives. Multi-objective reinforcement learning (MORL) is a generalization of standard reinforcement learning where the scalar reward signal is extended to multiple feedback signals, in essence, one for each objective. MORL is the process of learning policies that optimize multiple criteria simultaneously. In this paper, we present a novel temporal dierence learning algorithm that integrates the Pareto dominance relation into a reinforcement learning approach. This algorithm is a multi-policy algorithm that learns a set of Pareto dominating policies in a single run. We name this algorithm Pareto Q-learning and it is applicable in episodic environments with deterministic as well as stochastic transition functions. A crucial aspect of Pareto Q-learning is the updating mechanism that bootstraps sets of Q-vectors. One of our main contributions in this paper is a mechanism that separates the expected immediate reward vector from the set of expected future discounted reward vectors. This decomposition allows us to update the sets and to exploit the learned policies consistently throughout the state space. To balance exploration and exploitation during learning, we also propose three set evaluation mechanisms. These three mechanisms evaluate the sets of vectors to accommodate for standard action selection strategies, such as -greedy. More precisely, these mechanisms use multi-objective evaluation principles such as the hypervolume measure, the cardinality indicator and the Pareto dominance relation to select the most promising actions. We experimentally validate the algorithm on multiple environments with two and three objectives and we demonstrate that Pareto Q-learning outperforms current state-of-the-art MORL algorithms with respect to the hypervolume of the obtained policies. We note that (1) Pareto Q-learning is able to learn the entire Pareto front under the usual assumption that each state-action pair is suciently sampled, while (2) not being biased by the shape of the Pareto front. Furthermore, (3) the set evaluation mechanisms provide indicative measures for local action selection and (4) the learned policies can be retrieved throughout the state and action space.",
            "referenceCount": 32,
            "citationCount": 205,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "15"
            },
            "citationStyles": {
                "bibtex": "@Article{Moffaert2014MultiobjectiveRL,\n author = {Kristof Van Moffaert and A. Now\u00e9},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {3483-3512},\n title = {Multi-objective reinforcement learning using sets of pareto dominating policies},\n volume = {15},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8783688bfe249bd1cab13146a76ba50fe88128c7",
            "@type": "ScholarlyArticle",
            "paperId": "8783688bfe249bd1cab13146a76ba50fe88128c7",
            "corpusId": 7389645,
            "url": "https://www.semanticscholar.org/paper/8783688bfe249bd1cab13146a76ba50fe88128c7",
            "title": "Model-based Reinforcement Learning and the Eluder Dimension",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "conf/nips/OsbandR14a",
                "MAG": "2103708221",
                "ArXiv": "1406.1853",
                "CorpusId": 7389645
            },
            "abstract": "We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that, if the MDP can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system. We characterize this dependence explicitly as $\\tilde{O}(\\sqrt{d_K d_E T})$ where $T$ is time elapsed, $d_K$ is the Kolmogorov dimension and $d_E$ is the \\emph{eluder dimension}. These represent the first unified regret bounds for model-based reinforcement learning and provide state of the art guarantees in several important settings. Moreover, we present a simple and computationally efficient algorithm \\emph{posterior sampling for reinforcement learning} (PSRL) that satisfies these bounds.",
            "referenceCount": 26,
            "citationCount": 163,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-06-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1406.1853"
            },
            "citationStyles": {
                "bibtex": "@Article{Osband2014ModelbasedRL,\n author = {Ian Osband and Benjamin Van Roy},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Model-based Reinforcement Learning and the Eluder Dimension},\n volume = {abs/1406.1853},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4aece8df7bd59e2fbfedbf5729bba41abc56d870",
            "@type": "ScholarlyArticle",
            "paperId": "4aece8df7bd59e2fbfedbf5729bba41abc56d870",
            "corpusId": 206794869,
            "url": "https://www.semanticscholar.org/paper/4aece8df7bd59e2fbfedbf5729bba41abc56d870",
            "title": "A Comprehensive Survey of Multiagent Reinforcement Learning",
            "venue": "IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)",
            "publicationVenue": {
                "id": "urn:research:ecb11fdd-9e59-482f-a3b6-0cb14372306c",
                "name": "IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)",
                "alternate_names": [
                    "IEEE Trans Syst Man Cybern Part C (applications Rev"
                ],
                "issn": "1094-6977",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5326"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "journals/tsmc/BusoniuBS08",
                "MAG": "2099618002",
                "DOI": "10.1109/TSMCC.2007.913919",
                "CorpusId": 206794869
            },
            "abstract": "Multiagent systems are rapidly finding applications in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must, instead, discover a solution on their own, using learning. A significant part of the research on multiagent learning concerns reinforcement learning techniques. This paper provides a comprehensive survey of multiagent reinforcement learning (MARL). A central issue in the field is the formal statement of the multiagent learning goal. Different viewpoints on this issue have led to the proposal of many different goals, among which two focal points can be distinguished: stability of the agents' learning dynamics, and adaptation to the changing behavior of the other agents. The MARL algorithms described in the literature aim---either explicitly or implicitly---at one of these two goals or at a combination of both, in a fully cooperative, fully competitive, or more general setting. A representative selection of these algorithms is discussed in detail in this paper, together with the specific issues that arise in each category. Additionally, the benefits and challenges of MARL are described along with some of the problem domains where the MARL techniques have been applied. Finally, an outlook for the field is provided.",
            "referenceCount": 136,
            "citationCount": 1752,
            "influentialCitationCount": 126,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://repository.tudelft.nl/islandora/object/uuid%3A4c7d3b49-06fc-400c-923e-3903b8d230fe/datastream/OBJ/download",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2008-03-01",
            "journal": {
                "name": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",
                "volume": "38"
            },
            "citationStyles": {
                "bibtex": "@Article{Bu\u015foniu2008ACS,\n author = {L. Bu\u015foniu and Robert Babu\u0161ka and B. Schutter},\n booktitle = {IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},\n pages = {156-172},\n title = {A Comprehensive Survey of Multiagent Reinforcement Learning},\n volume = {38},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:67de20fb9d3d71aa37fb806deff35cefaf77081e",
            "@type": "ScholarlyArticle",
            "paperId": "67de20fb9d3d71aa37fb806deff35cefaf77081e",
            "corpusId": 12002001,
            "url": "https://www.semanticscholar.org/paper/67de20fb9d3d71aa37fb806deff35cefaf77081e",
            "title": "Role of Dopamine D2 Receptors in Human Reinforcement Learning",
            "venue": "Neuropsychopharmacology",
            "publicationVenue": {
                "id": "urn:research:e696c44c-baef-439c-bb1f-12bfe905eb77",
                "name": "Neuropsychopharmacology",
                "alternate_names": null,
                "issn": "0893-133X",
                "url": "https://www.nature.com/npp/"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2036976328",
                "DOI": "10.1038/npp.2014.84",
                "CorpusId": 12002001,
                "PubMed": "24713613"
            },
            "abstract": null,
            "referenceCount": 73,
            "citationCount": 123,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/npp201484.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2014-04-09",
            "journal": {
                "name": "Neuropsychopharmacology",
                "volume": "39"
            },
            "citationStyles": {
                "bibtex": "@Article{Eisenegger2014RoleOD,\n author = {C. Eisenegger and Michael Naef and A. Linssen and L. Clark and P. K. Gandamaneni and U. M\u00fcller and T. Robbins},\n booktitle = {Neuropsychopharmacology},\n journal = {Neuropsychopharmacology},\n pages = {2366-2375},\n title = {Role of Dopamine D2 Receptors in Human Reinforcement Learning},\n volume = {39},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f8f03a2b287aaa712b8dbd14024b452da6b04956",
            "@type": "ScholarlyArticle",
            "paperId": "f8f03a2b287aaa712b8dbd14024b452da6b04956",
            "corpusId": 18070108,
            "url": "https://www.semanticscholar.org/paper/f8f03a2b287aaa712b8dbd14024b452da6b04956",
            "title": "RLPy: a value-function-based reinforcement learning framework for education and research",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2591283724",
                "DBLP": "journals/jmlr/GeramifardDKDH15",
                "DOI": "10.5555/2789272.2886799",
                "CorpusId": 18070108
            },
            "abstract": "RLPy is an object-oriented reinforcement learning software package with a focus on value-function-based methods using linear function approximation and discrete actions. The framework was designed for both educational and research purposes. It provides a rich library of fine-grained, easily exchangeable components for learning agents (e.g., policies or representations of value functions), facilitating recently increased specialization in reinforcement learning. RLPy is written in Python to allow fast prototyping, but is also suitable for large-scale experiments through its built-in support for optimized numerical libraries and parallelization. Code profiling, domain visualizations, and data analysis are integrated in a self-contained package available under the Modified BSD License at http://github.com/rlpy/rlpy. All of these properties allow users to compare various reinforcement learning algorithms with little effort.",
            "referenceCount": 31,
            "citationCount": 69,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-08-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "16"
            },
            "citationStyles": {
                "bibtex": "@Article{Geramifard2015RLPyAV,\n author = {A. Geramifard and Christoph Dann and Robert H. Klein and Will Dabney and J. How},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1573-1578},\n title = {RLPy: a value-function-based reinforcement learning framework for education and research},\n volume = {16},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d163ae2ae7ee2b7991ab017113f13f54fc5c8c5f",
            "@type": "ScholarlyArticle",
            "paperId": "d163ae2ae7ee2b7991ab017113f13f54fc5c8c5f",
            "corpusId": 18671739,
            "url": "https://www.semanticscholar.org/paper/d163ae2ae7ee2b7991ab017113f13f54fc5c8c5f",
            "title": "PAC-inspired Option Discovery in Lifelong Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2133458291",
                "DBLP": "conf/icml/BrunskillL14",
                "CorpusId": 18671739
            },
            "abstract": "A key goal of AI is to create lifelong learning agents that can leverage prior experience to improve performance on later tasks. In reinforcement-learning problems, one way to summarize prior experience for future use is through options, which are temporally extended actions (subpolicies) for how to behave. Options can then be used to potentially accelerate learning in new reinforcement learning tasks. In this work, we provide the first formal analysis of the sample complexity, a measure of learning speed, of reinforcement learning with options. This analysis helps shed light on some interesting prior empirical results on when and how options may accelerate learning. We then quantify the benefit of options in reducing sample complexity of a lifelong learning agent. Finally, the new theoretical insights inspire a novel option-discovery algorithm that aims at minimizing overall sample complexity in lifelong reinforcement learning.",
            "referenceCount": 26,
            "citationCount": 120,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-06-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Brunskill2014PACinspiredOD,\n author = {E. Brunskill and Lihong Li},\n booktitle = {International Conference on Machine Learning},\n pages = {316-324},\n title = {PAC-inspired Option Discovery in Lifelong Reinforcement Learning},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e3465ce511059dbf449b15054d4ab9b1281345b4",
            "@type": "ScholarlyArticle",
            "paperId": "e3465ce511059dbf449b15054d4ab9b1281345b4",
            "corpusId": 1219941,
            "url": "https://www.semanticscholar.org/paper/e3465ce511059dbf449b15054d4ab9b1281345b4",
            "title": "Effective reinforcement learning following cerebellar damage requires a balance between exploration and motor noise",
            "venue": "Brain : a journal of neurology",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2175592801",
                "PubMedCentral": "4949390",
                "DOI": "10.1093/brain/awv329",
                "CorpusId": 1219941,
                "PubMed": "26626368"
            },
            "abstract": "See Miall and Galea (doi: 10.1093/awv343 ) for a scientific commentary on this article. Cerebellar lesions impair both coordination and motor learning. Therrien et al. show that affected individuals can learn using a reinforcement mechanism despite a deficit in error-based motor learning. They also identify a critical feature of cerebellar patients\u2019 movements (motor noise), which determines the effectiveness of learning under reinforcement.",
            "referenceCount": 36,
            "citationCount": 139,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://academic.oup.com/brain/article-pdf/139/1/101/24173336/awv329.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-12-01",
            "journal": {
                "name": "Brain",
                "volume": "139"
            },
            "citationStyles": {
                "bibtex": "@Article{Therrien2015EffectiveRL,\n author = {A. Therrien and D. Wolpert and A. Bastian},\n booktitle = {Brain : a journal of neurology},\n journal = {Brain},\n pages = {101 - 114},\n title = {Effective reinforcement learning following cerebellar damage requires a balance between exploration and motor noise},\n volume = {139},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3f59bce00434b432dfd0b9ab20903acadaefd456",
            "@type": "ScholarlyArticle",
            "paperId": "3f59bce00434b432dfd0b9ab20903acadaefd456",
            "corpusId": 2899283,
            "url": "https://www.semanticscholar.org/paper/3f59bce00434b432dfd0b9ab20903acadaefd456",
            "title": "Reinforcement Learning with Parameterized Actions",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1509.01644",
                "DBLP": "conf/aaai/MassonRK16",
                "MAG": "2951720942",
                "DOI": "10.1609/aaai.v30i1.10226",
                "CorpusId": 2899283
            },
            "abstract": "\n \n We introduce a model-free algorithm for learning in Markov decision processes with parameterized actions\u2014discrete actions with continuous parameters. At each step the agent must select both which action to use and which parameters to use with that action. We introduce the Q-PAMDP algorithm for learning in these domains, show that it converges to a local optimum, and compare it to direct policy search in the goal-scoring and Platform domains.\n \n",
            "referenceCount": 23,
            "citationCount": 155,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10226/10085",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-09-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Masson2015ReinforcementLW,\n author = {W. Masson and Pravesh Ranchod and G. Konidaris},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {1934-1940},\n title = {Reinforcement Learning with Parameterized Actions},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:789783016fb708abbc061790612ebe91273c05d3",
            "@type": "ScholarlyArticle",
            "paperId": "789783016fb708abbc061790612ebe91273c05d3",
            "corpusId": 674335,
            "url": "https://www.semanticscholar.org/paper/789783016fb708abbc061790612ebe91273c05d3",
            "title": "(More) Efficient Reinforcement Learning via Posterior Sampling",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2951592473",
                "DBLP": "conf/nips/OsbandRR13",
                "ArXiv": "1306.0940",
                "CorpusId": 674335
            },
            "abstract": "Most provably-efficient reinforcement learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration: posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an O(\u03c4S/\u221aAT) bound on expected regret, where T is time, \u03c4 is the episode length and S and A are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.",
            "referenceCount": 24,
            "citationCount": 461,
            "influentialCitationCount": 85,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-06-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Osband2013MoreER,\n author = {Ian Osband and Daniel Russo and Benjamin Van Roy},\n booktitle = {Neural Information Processing Systems},\n pages = {3003-3011},\n title = {(More) Efficient Reinforcement Learning via Posterior Sampling},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c9d7afd65b2127e6f0651bc7e13eeec1897ac8dd",
            "@type": "ScholarlyArticle",
            "paperId": "c9d7afd65b2127e6f0651bc7e13eeec1897ac8dd",
            "corpusId": 16228924,
            "url": "https://www.semanticscholar.org/paper/c9d7afd65b2127e6f0651bc7e13eeec1897ac8dd",
            "title": "Reinforcement Learning Neural Turing Machines",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2125308790",
                "DBLP": "journals/corr/ZarembaS15",
                "ArXiv": "1505.00521",
                "CorpusId": 16228924
            },
            "abstract": "The expressive power of a machine learning model is closely related to the number of sequential computational steps it can learn. For example, Deep Neural Networks have been more successful than shallow networks because they can perform a greater number of sequential computational steps (each highly parallel). The Neural Turing Machine (NTM) [8] is a model that can compactly express an even greater number of sequential computational steps, so it is even more powerful than a DNN. Its memory addressing operations are designed to be differentiable; thus the NTM can be trained with backpropagation. While differentiable memory is relatively easy to implement and train, it necessitates accessing the entire memory content at each computational step. This makes it difficult to implement a fast NTM. In this work, we use the Re inforce algorithm to learn where to access the memory, while using backpropagation to learn what to write to the memory. We call this model the RL-NTM. Reinforce allows our model to access a constant number of memory cells at each computational step, so its implementation can be faster. The RL-NTM is the first mo del that can, in principle, learn programs of unbounded running time. We successfully trained the RL-NTM to solve a number of algorithmic tasks that are simpler than the ones solvable by the fully differentiable NTM. As the RL-NTM is a fairly intricate model, we needed a method for verifying the correctness of our implementation. To do so, we developed a simple technique for numerically checking arbitrary implementations of models that use Reinforce, which may be of independent interest.",
            "referenceCount": 23,
            "citationCount": 165,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-05-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1505.00521"
            },
            "citationStyles": {
                "bibtex": "@Article{Zaremba2015ReinforcementLN,\n author = {Wojciech Zaremba and Ilya Sutskever},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Reinforcement Learning Neural Turing Machines},\n volume = {abs/1505.00521},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9f5b82d9915d0752957602224c5056be7e749c83",
            "@type": "ScholarlyArticle",
            "paperId": "9f5b82d9915d0752957602224c5056be7e749c83",
            "corpusId": 38553870,
            "url": "https://www.semanticscholar.org/paper/9f5b82d9915d0752957602224c5056be7e749c83",
            "title": "Foundations of Machine Learning",
            "venue": "Introduction to AI Techniques for Renewable Energy Systems",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "DOI": "10.2139/ssrn.3399990",
                "CorpusId": 38553870
            },
            "abstract": "Understanding Machine LearningProbabilistic Machine LearningHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlowFundamentals of Machine LearningReinforcement Learning, second editionDeep LearningIntroducing Machine LearningFoundations of Data ScienceFundamentals of Deep LearningIntelligent SystemsMachine Learning RefinedAn Introduction to Deep Reinforcement LearningDeep Learning: Fundamentals, Theory and ApplicationsDeep LearningDeep Learning for Coders with fastai and PyTorchMachine LearningA Brief Introduction to Machine Learning for EngineersElements of Causal InferenceFundamentals of Machine Learning for Predictive Data Analytics, second editionMachine Learning in Clinical NeuroscienceLearning Deep Architectures for AIArtificial IntelligenceStatistical Foundations of Data ScienceThe Mathematical Foundations of Learning MachinesFoundations of Machine LearningMachine Learning FoundationsBoostingThe Algorithmic Foundations of Differential PrivacyMathematics for Machine LearningFoundations of Rule LearningDeep Learning with PyTorchNeural Network LearningDeep Learning IllustratedFoundations of Deep Reinforcement LearningFoundations of Machine Learning, second editionImbalanced LearningFoundations of Knowledge AcquisitionOn the Path to AIMachine Learning: Theoretical Foundations and Practical ApplicationsArtificial Intelligence and Machine Learning Fundamentals",
            "referenceCount": 5,
            "citationCount": 2691,
            "influentialCitationCount": 392,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2021-10-07",
            "journal": {
                "name": "Introduction to AI Techniques for Renewable Energy Systems",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nathani2021FoundationsOM,\n author = {N. Nathani and Abhishek Singh},\n booktitle = {Introduction to AI Techniques for Renewable Energy Systems},\n journal = {Introduction to AI Techniques for Renewable Energy Systems},\n title = {Foundations of Machine Learning},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0cafe2903b097fc042782c359cb231ea34ef7ed3",
            "@type": "ScholarlyArticle",
            "paperId": "0cafe2903b097fc042782c359cb231ea34ef7ed3",
            "corpusId": 247490,
            "url": "https://www.semanticscholar.org/paper/0cafe2903b097fc042782c359cb231ea34ef7ed3",
            "title": "Near-optimal Regret Bounds for Reinforcement Learning",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/nips/AuerJO08",
                "MAG": "2132908009",
                "DOI": "10.5555/1756006.1859902",
                "CorpusId": 247490
            },
            "abstract": "For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s,s' there is a policy which moves from s to s' in at most D steps (on average). We present a reinforcement learning algorithm with total regret O(DS\u221aAT) after T steps for any unknown MDP with S states, A actions per state, and diameter D. A corresponding lower bound of \u03a9(\u221aDSAT) on the total regret of any learning algorithm is given as well. \n \nThese results are complemented by a sample complexity bound on the number of suboptimal steps taken by our algorithm. This bound can be used to achieve a (gap-dependent) regret bound that is logarithmic in T. \n \nFinally, we also consider a setting where the MDP is allowed to change a fixed number of l times. We present a modification of our algorithm that is able to deal with this setting and show a regret bound of O(l1/3T2/3DS\u221aA).",
            "referenceCount": 31,
            "citationCount": 1245,
            "influentialCitationCount": 303,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-12-08",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Jaksch2008NearoptimalRB,\n author = {Thomas Jaksch and R. Ortner and P. Auer},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1563-1600},\n title = {Near-optimal Regret Bounds for Reinforcement Learning},\n volume = {11},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:764fd3bc54709bc77c35df7d081538809065bd19",
            "@type": "ScholarlyArticle",
            "paperId": "764fd3bc54709bc77c35df7d081538809065bd19",
            "corpusId": 6089047,
            "url": "https://www.semanticscholar.org/paper/764fd3bc54709bc77c35df7d081538809065bd19",
            "title": "By Carrot or by Stick: Cognitive Reinforcement Learning in Parkinsonism",
            "venue": "Science",
            "publicationVenue": {
                "id": "urn:research:f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
                "name": "Science",
                "alternate_names": null,
                "issn": "0193-4511",
                "url": "https://www.jstor.org/journal/science"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2163647009",
                "DOI": "10.1126/SCIENCE.1102941",
                "CorpusId": 6089047,
                "PubMed": "15528409"
            },
            "abstract": "To what extent do we learn from the positive versus negative outcomes of our decisions? The neuromodulator dopamine plays a key role in these reinforcement learning processes. Patients with Parkinson's disease, who have depleted dopamine in the basal ganglia, are impaired in tasks that require learning from trial and error. Here, we show, using two cognitive procedural learning tasks, that Parkinson's patients off medication are better at learning to avoid choices that lead to negative outcomes than they are at learning from positive outcomes. Dopamine medication reverses this bias, making patients more sensitive to positive than negative outcomes. This pattern was predicted by our biologically based computational model of basal ganglia\u2013dopamine interactions in cognition, which has separate pathways for \u201cGo\u201d and \u201cNoGo\u201d responses that are differentially modulated by positive and negative reinforcement.",
            "referenceCount": 73,
            "citationCount": 1868,
            "influentialCitationCount": 176,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.science.org/cms/asset/07f07a99-ec1a-41fa-8418-043e15afd1fc/pap.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2004-12-10",
            "journal": {
                "name": "Science",
                "volume": "306"
            },
            "citationStyles": {
                "bibtex": "@Article{Frank2004ByCO,\n author = {M. Frank and L. Seeberger and R. O\u2019Reilly},\n booktitle = {Science},\n journal = {Science},\n pages = {1940 - 1943},\n title = {By Carrot or by Stick: Cognitive Reinforcement Learning in Parkinsonism},\n volume = {306},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:55b5162e8401df8812db192a609361ae42c6216f",
            "@type": "ScholarlyArticle",
            "paperId": "55b5162e8401df8812db192a609361ae42c6216f",
            "corpusId": 29204602,
            "url": "https://www.semanticscholar.org/paper/55b5162e8401df8812db192a609361ae42c6216f",
            "title": "Reinforcement Learning",
            "venue": "Encyclopedia of Algorithms",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "reference/algo/Even-Dar16",
                "DOI": "10.1007/978-1-4939-2864-4_341",
                "CorpusId": 29204602
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{None,\n booktitle = {Encyclopedia of Algorithms},\n pages = {1816-1820},\n title = {Reinforcement Learning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8f2d9719c48a6760ce5e5823f71b68751fb7e11b",
            "@type": "ScholarlyArticle",
            "paperId": "8f2d9719c48a6760ce5e5823f71b68751fb7e11b",
            "corpusId": 6662928,
            "url": "https://www.semanticscholar.org/paper/8f2d9719c48a6760ce5e5823f71b68751fb7e11b",
            "title": "Reinforcement Learning Output Feedback NN Control Using Deterministic Learning Technique",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "1970127232",
                "DBLP": "journals/tnn/XuYS14",
                "DOI": "10.1109/TNNLS.2013.2292704",
                "CorpusId": 6662928,
                "PubMed": "24807456"
            },
            "abstract": "In this brief, a novel adaptive-critic-based neural network (NN) controller is investigated for nonlinear pure-feedback systems. The controller design is based on the transformed predictor form, and the actor-critic NN control architecture includes two NNs, whereas the critic NN is used to approximate the strategic utility function, and the action NN is employed to minimize both the strategic utility function and the tracking error. A deterministic learning technique has been employed to guarantee that the partial persistent excitation condition of internal states is satisfied during tracking control to a periodic reference orbit. The uniformly ultimate boundedness of closed-loop signals is shown via Lyapunov stability analysis. Simulation results are presented to demonstrate the effectiveness of the proposed control.",
            "referenceCount": 26,
            "citationCount": 240,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2014-03-01",
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "25"
            },
            "citationStyles": {
                "bibtex": "@Article{Xu2014ReinforcementLO,\n author = {B. Xu and Chenguang Yang and Zhong-ke Shi},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {635-641},\n title = {Reinforcement Learning Output Feedback NN Control Using Deterministic Learning Technique},\n volume = {25},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0298f62cc9aa9bd51596adae4fced24320c401fd",
            "@type": "ScholarlyArticle",
            "paperId": "0298f62cc9aa9bd51596adae4fced24320c401fd",
            "corpusId": 210326100,
            "url": "https://www.semanticscholar.org/paper/0298f62cc9aa9bd51596adae4fced24320c401fd",
            "title": "Predicting How People Play Games: Reinforcement Learning in Experimental Games with Unique, Mixed Strategy Equilibria",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1564229172",
                "CorpusId": 210326100
            },
            "abstract": "The authors examine learning in all experiments they could locate involving one hundred periods or more of games with a unique equilibrium in mixed strategies, and in a new experiment. They study both the ex post ('best fit') descriptive power of learning models, and their ex ante predictive power, by simulating each experiment using parameters estimated from the other experiments. Even a one-parameter reinforcement learning model robustly outperforms the equilibrium predictions. Predictive power is improved by adding 'forgetting' and 'experimentation,' or by allowing greater rationality as in probabilistic fictitious play. Implications for developing a low-rationality, cognitive game theory are discussed. Copyright 1998 by American Economic Association.",
            "referenceCount": 44,
            "citationCount": 2004,
            "influentialCitationCount": 134,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Economics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Economics",
                    "source": "external"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1998-09-01",
            "journal": {
                "name": "The American Economic Review",
                "volume": "88"
            },
            "citationStyles": {
                "bibtex": "@Article{Erev1998PredictingHP,\n author = {Ido Erev and A. Roth},\n journal = {The American Economic Review},\n pages = {848-881},\n title = {Predicting How People Play Games: Reinforcement Learning in Experimental Games with Unique, Mixed Strategy Equilibria},\n volume = {88},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a7c31b6203949bd8ee6db0ebd4e075b980e9569f",
            "@type": "ScholarlyArticle",
            "paperId": "a7c31b6203949bd8ee6db0ebd4e075b980e9569f",
            "corpusId": 14678361,
            "url": "https://www.semanticscholar.org/paper/a7c31b6203949bd8ee6db0ebd4e075b980e9569f",
            "title": "Policy Shaping: Integrating Human Feedback with Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2098441518",
                "DBLP": "conf/nips/GriffithSSIT13",
                "CorpusId": 14678361
            },
            "abstract": "A long term goal of Interactive Reinforcement Learning is to incorporate nonexpert human feedback to solve complex tasks. Some state-of-the-art methods have approached this problem by mapping human information to rewards and values and iterating over them to compute better control policies. In this paper we argue for an alternate, more effective characterization of human feedback: Policy Shaping. We introduce Advise, a Bayesian approach that attempts to maximize the information gained from human feedback by utilizing it as direct policy labels. We compare Advise to state-of-the-art approaches and show that it can outperform them and is robust to infrequent and inconsistent human feedback.",
            "referenceCount": 24,
            "citationCount": 342,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-12-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Griffith2013PolicySI,\n author = {Shane Griffith and K. Subramanian and Jonathan Scholz and C. Isbell and A. Thomaz},\n booktitle = {Neural Information Processing Systems},\n pages = {2625-2633},\n title = {Policy Shaping: Integrating Human Feedback with Reinforcement Learning},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e3cc22208b493a8b23a969e3e121008cb210b455",
            "@type": "ScholarlyArticle",
            "paperId": "e3cc22208b493a8b23a969e3e121008cb210b455",
            "corpusId": 1854651,
            "url": "https://www.semanticscholar.org/paper/e3cc22208b493a8b23a969e3e121008cb210b455",
            "title": "Mapping anhedonia onto reinforcement learning: a behavioural meta-analysis",
            "venue": "Biology of Mood & Anxiety Disorders",
            "publicationVenue": {
                "id": "urn:research:63dbf8f8-1b66-447c-bd0e-ea5ef8f97321",
                "name": "Biology of Mood & Anxiety Disorders",
                "alternate_names": [
                    "Biology Mood  Anxiety Disord"
                ],
                "issn": "2045-5380",
                "url": "http://www.biolmoodanxietydisord.com/"
            },
            "year": 2013,
            "externalIds": {
                "PubMedCentral": "3701611",
                "MAG": "2157813419",
                "DOI": "10.1186/2045-5380-3-12",
                "CorpusId": 1854651,
                "PubMed": "23782813"
            },
            "abstract": null,
            "referenceCount": 135,
            "citationCount": 367,
            "influentialCitationCount": 17,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://biolmoodanxietydisord.biomedcentral.com/counter/pdf/10.1186/2045-5380-3-12",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-06-19",
            "journal": {
                "name": "Biology of Mood & Anxiety Disorders",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Huys2013MappingAO,\n author = {Q. Huys and D. Pizzagalli and R. Bogdan and P. Dayan},\n booktitle = {Biology of Mood & Anxiety Disorders},\n journal = {Biology of Mood & Anxiety Disorders},\n pages = {12 - 12},\n title = {Mapping anhedonia onto reinforcement learning: a behavioural meta-analysis},\n volume = {3},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e60f3c1cb857daa3233f2c5b17b6f111ff86698c",
            "@type": "ScholarlyArticle",
            "paperId": "e60f3c1cb857daa3233f2c5b17b6f111ff86698c",
            "corpusId": 5928833,
            "url": "https://www.semanticscholar.org/paper/e60f3c1cb857daa3233f2c5b17b6f111ff86698c",
            "title": "Algorithms for Reinforcement Learning",
            "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning",
            "publicationVenue": {
                "id": "urn:research:84e95d47-8c6e-4f56-b8c8-2fc3088cfb6b",
                "name": "Synthesis Lectures on Artificial Intelligence and Machine Learning",
                "alternate_names": [
                    "Synth Lect Artif Intell Mach Learn"
                ],
                "issn": "1939-4608",
                "url": "https://www.morganclaypool.com/toc/aim/1/1"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2073384958",
                "DBLP": "series/synthesis/2010Szepesvari",
                "DOI": "10.1007/978-3-031-01551-9",
                "CorpusId": 5928833
            },
            "abstract": null,
            "referenceCount": 199,
            "citationCount": 1227,
            "influentialCitationCount": 91,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm%3A978-3-031-01551-9%2F1",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2010-06-25",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Szepesvari2010AlgorithmsFR,\n author = {Csaba Szepesvari},\n booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},\n title = {Algorithms for Reinforcement Learning},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f0c4b7568f378e652645232e66a1dab4c5b5293f",
            "@type": "ScholarlyArticle",
            "paperId": "f0c4b7568f378e652645232e66a1dab4c5b5293f",
            "corpusId": 9798309,
            "url": "https://www.semanticscholar.org/paper/f0c4b7568f378e652645232e66a1dab4c5b5293f",
            "title": "Risk-Sensitive Reinforcement Learning",
            "venue": "Neural Computation",
            "publicationVenue": {
                "id": "urn:research:69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                "name": "Neural Computation",
                "alternate_names": [
                    "Neural Comput"
                ],
                "issn": "0899-7667",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "journals/corr/ShenTSO13",
                "MAG": "2169206416",
                "ArXiv": "1311.2097",
                "DOI": "10.1162/NECO_a_00600",
                "CorpusId": 9798309,
                "PubMed": "24708369"
            },
            "abstract": "We derive a family of risk-sensitive reinforcement learning methods for agents, who face sequential decision-making tasks in uncertain environments. By applying a utility function to the temporal difference (TD) error, nonlinear transformations are effectively applied not only to the received rewards but also to the true transition probabilities of the underlying Markov decision process. When appropriate utility functions are chosen, the agents\u2019 behaviors express key features of human behavior as predicted by prospect theory (Kahneman & Tversky, 1979), for example, different risk preferences for gains and losses, as well as the shape of subjective probability curves. We derive a risk-sensitive Q-learning algorithm, which is necessary for modeling human behavior when transition probabilities are unknown, and prove its convergence. As a proof of principle for the applicability of the new framework, we apply it to quantify human behavior in a sequential investment task. We find that the risk-sensitive variant provides a significantly better fit to the behavioral data and that it leads to an interpretation of the subject's responses that is indeed consistent with prospect theory. The analysis of simultaneously measured fMRI signals shows a significant correlation of the risk-sensitive TD error with BOLD signal change in the ventral striatum. In addition we find a significant correlation of the risk-sensitive Q-values with neural activity in the striatum, cingulate cortex, and insula that is not present if standard Q-values are used.",
            "referenceCount": 37,
            "citationCount": 274,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1311.2097",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "LettersAndComments",
                "JournalArticle"
            ],
            "publicationDate": "2013-11-08",
            "journal": {
                "name": "Neural Computation",
                "volume": "26"
            },
            "citationStyles": {
                "bibtex": "@Article{Shen2013RiskSensitiveRL,\n author = {Yun Shen and Michael J. Tobia and T. Sommer and K. Obermayer},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {1298-1328},\n title = {Risk-Sensitive Reinforcement Learning},\n volume = {26},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:145a42e83ec142a125da3ad845ee95027ef702e5",
            "@type": "ScholarlyArticle",
            "paperId": "145a42e83ec142a125da3ad845ee95027ef702e5",
            "corpusId": 1185580,
            "url": "https://www.semanticscholar.org/paper/145a42e83ec142a125da3ad845ee95027ef702e5",
            "title": "A Survey of Actor-Critic Reinforcement Learning: Standard and Natural Policy Gradients",
            "venue": "IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)",
            "publicationVenue": {
                "id": "urn:research:ecb11fdd-9e59-482f-a3b6-0cb14372306c",
                "name": "IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)",
                "alternate_names": [
                    "IEEE Trans Syst Man Cybern Part C (applications Rev"
                ],
                "issn": "1094-6977",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5326"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2046376809",
                "DBLP": "journals/tsmc/GrondmanBLB12",
                "DOI": "10.1109/TSMCC.2012.2218595",
                "CorpusId": 1185580
            },
            "abstract": "Policy-gradient-based actor-critic algorithms are amongst the most popular algorithms in the reinforcement learning framework. Their advantage of being able to search for optimal policies using low-variance gradient estimates has made them useful in several real-life applications, such as robotics, power control, and finance. Although general surveys on reinforcement learning techniques already exist, no survey is specifically dedicated to actor-critic algorithms in particular. This paper, therefore, describes the state of the art of actor-critic algorithms, with a focus on methods that can work in an online setting and use function approximation in order to deal with continuous state and action spaces. After starting with a discussion on the concepts of reinforcement learning and the origins of actor-critic algorithms, this paper describes the workings of the natural gradient, which has made its way into many actor-critic algorithms over the past few years. A review of several standard and natural actor-critic algorithms is given, and the paper concludes with an overview of application areas and a discussion on open issues.",
            "referenceCount": 90,
            "citationCount": 707,
            "influentialCitationCount": 42,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://hal.archives-ouvertes.fr/hal-00756747/file/ivo_smcc12_survey.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2012-11-01",
            "journal": {
                "name": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",
                "volume": "42"
            },
            "citationStyles": {
                "bibtex": "@Article{Grondman2012ASO,\n author = {I. Grondman and L. Bu\u015foniu and G. D. Lopes and Robert Babu\u0161ka},\n booktitle = {IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},\n pages = {1291-1307},\n title = {A Survey of Actor-Critic Reinforcement Learning: Standard and Natural Policy Gradients},\n volume = {42},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fae8bbf868681b83d91b2fec6c840d4d2b32005b",
            "@type": "ScholarlyArticle",
            "paperId": "fae8bbf868681b83d91b2fec6c840d4d2b32005b",
            "corpusId": 2326055,
            "url": "https://www.semanticscholar.org/paper/fae8bbf868681b83d91b2fec6c840d4d2b32005b",
            "title": "Intrinsic Motivation and Reinforcement Learning",
            "venue": "Intrinsically Motivated Learning in Natural and Artificial Systems",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2188721763",
                "DBLP": "books/sp/13/Barto13",
                "DOI": "10.1007/978-3-642-32375-1_2",
                "CorpusId": 2326055
            },
            "abstract": null,
            "referenceCount": 118,
            "citationCount": 207,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Barto2013IntrinsicMA,\n author = {A. Barto},\n booktitle = {Intrinsically Motivated Learning in Natural and Artificial Systems},\n pages = {17-47},\n title = {Intrinsic Motivation and Reinforcement Learning},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:902fccdc900a06246ee0f3f1ee7b58b68ca41916",
            "@type": "ScholarlyArticle",
            "paperId": "902fccdc900a06246ee0f3f1ee7b58b68ca41916",
            "corpusId": 14675976,
            "url": "https://www.semanticscholar.org/paper/902fccdc900a06246ee0f3f1ee7b58b68ca41916",
            "title": "Teaching on a budget: agents advising agents in reinforcement learning",
            "venue": "Adaptive Agents and Multi-Agent Systems",
            "publicationVenue": {
                "id": "urn:research:6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                "name": "Adaptive Agents and Multi-Agent Systems",
                "alternate_names": [
                    "Adapt Agent Multi-agent Syst",
                    "International Joint Conference on Autonomous Agents & Multiagent Systems",
                    "Adapt Agent Multi-agents Syst",
                    "AAMAS",
                    "Adaptive Agents and Multi-Agents Systems",
                    "Int Jt Conf Auton Agent  Multiagent Syst"
                ],
                "issn": null,
                "url": "http://www.ifaamas.org/"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "1529399279",
                "DBLP": "conf/atal/TorreyT13",
                "DOI": "10.5555/2484920.2485086",
                "CorpusId": 14675976
            },
            "abstract": "This paper introduces a teacher-student framework for reinforcement learning. In this framework, a teacher agent instructs a student agent by suggesting actions the student should take as it learns. However, the teacher may only give such advice a limited number of times. We present several novel algorithms that teachers can use to budget their advice effectively, and we evaluate them in two experimental domains: Mountain Car and Pac-Man. Our results show that the same amount of advice, given at different moments, can have different effects on student learning, and that teachers can significantly affect student learning even when students use different learning methods and state representations.",
            "referenceCount": 13,
            "citationCount": 162,
            "influentialCitationCount": 30,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-05-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Torrey2013TeachingOA,\n author = {Lisa A. Torrey and Matthew E. Taylor},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {1053-1060},\n title = {Teaching on a budget: agents advising agents in reinforcement learning},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f51265b88ce01e7e3c12fa9b8dc84dfd0a73975c",
            "@type": "ScholarlyArticle",
            "paperId": "f51265b88ce01e7e3c12fa9b8dc84dfd0a73975c",
            "corpusId": 15980735,
            "url": "https://www.semanticscholar.org/paper/f51265b88ce01e7e3c12fa9b8dc84dfd0a73975c",
            "title": "Scalarized multi-objective reinforcement learning: Novel design techniques",
            "venue": "IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning",
            "publicationVenue": {
                "id": "urn:research:1a13ae1e-223b-4ccd-821b-6a9e430564dc",
                "name": "IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning",
                "alternate_names": [
                    "ADPRL",
                    "IEEE Symp Adapt Dyn Program Reinf Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "conf/adprl/MoffaertDN13",
                "MAG": "2060846151",
                "DOI": "10.1109/ADPRL.2013.6615007",
                "CorpusId": 15980735
            },
            "abstract": "In multi-objective problems, it is key to find compromising solutions that balance different objectives. The linear scalarization function is often utilized to translate the multi-objective nature of a problem into a standard, single-objective problem. Generally, it is noted that such as linear combination can only find solutions in convex areas of the Pareto front, therefore making the method inapplicable in situations where the shape of the front is not known beforehand, as is often the case. We propose a non-linear scalarization function, called the Chebyshev scalarization function, as a basis for action selection strategies in multi-objective reinforcement learning. The Chebyshev scalarization method overcomes the flaws of the linear scalarization function as it can (i) discover Pareto optimal solutions regardless of the shape of the front, i.e. convex as well as non-convex , (ii) obtain a better spread amongst the set of Pareto optimal solutions and (iii) is not particularly dependent on the actual weights used.",
            "referenceCount": 12,
            "citationCount": 182,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-04-16",
            "journal": {
                "name": "2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Moffaert2013ScalarizedMR,\n author = {Kristof Van Moffaert and M\u0103d\u0103lina M. Drugan and A. Now\u00e9},\n booktitle = {IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning},\n journal = {2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)},\n pages = {191-199},\n title = {Scalarized multi-objective reinforcement learning: Novel design techniques},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1f7f6b462ac16543e32934336b93732bc2588188",
            "@type": "ScholarlyArticle",
            "paperId": "1f7f6b462ac16543e32934336b93732bc2588188",
            "corpusId": 8717841,
            "url": "https://www.semanticscholar.org/paper/1f7f6b462ac16543e32934336b93732bc2588188",
            "title": "Reinforcement Learning in Robotics: Applications and Real-World Challenges",
            "venue": "Robotics",
            "publicationVenue": {
                "id": "urn:research:14c9abdd-4a8a-49e3-bd38-8a685dac7977",
                "name": "Robotics",
                "alternate_names": null,
                "issn": "0167-8493",
                "url": "http://www.sciencedirect.com/science/journal/01678493"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "journals/robotics/KormushevCC13",
                "MAG": "2124267516",
                "DOI": "10.3390/robotics2030122",
                "CorpusId": 8717841
            },
            "abstract": "In robotics, the ultimate goal of reinforcement learning is to endow robots with the ability to learn, improve, adapt and reproduce tasks with dynamically changing constraints based on exploration and autonomous learning. We give a summary of the state-of-the-art of reinforcement learning in the context of robotics, in terms of both algorithms and policy representations. Numerous challenges faced by the policy representation in robotics are identified. Three recent examples for the application of reinforcement learning to real-world robots are described: a pancake flipping task, a bipedal walking energy minimization task and an archery-based aiming task. In all examples, a state-of-the-art expectation-maximization-based reinforcement learning is used, and different policy representations are proposed and evaluated for each task. The proposed policy representations offer viable solutions to six rarely-addressed challenges in policy representations: correlations, adaptability, multi-resolution, globality, multi-dimensionality and convergence. Both the successes and the practical difficulties encountered in these examples are discussed. Based on insights from these particular cases, conclusions are drawn about the state-of-the-art and the future perspective directions for reinforcement learning in robotics.",
            "referenceCount": 48,
            "citationCount": 201,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2218-6581/2/3/122/pdf?version=1373014977",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-07-05",
            "journal": {
                "name": "Robotics",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Kormushev2013ReinforcementLI,\n author = {Petar Kormushev and S. Calinon and D. Caldwell},\n booktitle = {Robotics},\n journal = {Robotics},\n pages = {122-148},\n title = {Reinforcement Learning in Robotics: Applications and Real-World Challenges},\n volume = {2},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4c96ca25d889251e20e33d01f24eec175301ab94",
            "@type": "ScholarlyArticle",
            "paperId": "4c96ca25d889251e20e33d01f24eec175301ab94",
            "corpusId": 57341,
            "url": "https://www.semanticscholar.org/paper/4c96ca25d889251e20e33d01f24eec175301ab94",
            "title": "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition",
            "venue": "Journal of Artificial Intelligence Research",
            "publicationVenue": {
                "id": "urn:research:aef12dca-60a0-4ca3-819b-cad26d309d4e",
                "name": "Journal of Artificial Intelligence Research",
                "alternate_names": [
                    "JAIR",
                    "J Artif Intell Res",
                    "The Journal of Artificial Intelligence Research"
                ],
                "issn": "1076-9757",
                "url": "http://www.jair.org/"
            },
            "year": 1999,
            "externalIds": {
                "MAG": "2951774643",
                "DBLP": "journals/corr/cs-LG-9905014",
                "ArXiv": "cs/9905014",
                "DOI": "10.1613/jair.639",
                "CorpusId": 57341
            },
            "abstract": "This paper presents a new approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The decomposition, known as the MAXQ decomposition, has both a procedural semantics--as a subroutine hierarchy--and a declarative semantics--as a representation of the value function of a hierarchical policy. MAXQ unifies and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. It is based on the assumption that the programmer can identify useful subgoals and define subtasks that achieve these subgoals. By defining such subgoals, the programmer constrains the set of policies that need to be considered during reinforcement learning. The MAXQ value function decomposition can represent the value function of any policy that is consistent with the given hierarchy. The decomposition also creates opportunities to exploit state abstractions, so that individual MDPs within the hierarchy can ignore large parts of the state space. This is important for the practical application of the method. This paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges with probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this nonhierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.",
            "referenceCount": 42,
            "citationCount": 1595,
            "influentialCitationCount": 194,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jair.org/index.php/jair/article/download/10266/24463",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1999-05-21",
            "journal": {
                "name": "ArXiv",
                "volume": "cs.LG/9905014"
            },
            "citationStyles": {
                "bibtex": "@Article{Dietterich1999HierarchicalRL,\n author = {Thomas G. Dietterich},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {ArXiv},\n title = {Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition},\n volume = {cs.LG/9905014},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4649eecad2473e98ce915825ceabef712ff10819",
            "@type": "ScholarlyArticle",
            "paperId": "4649eecad2473e98ce915825ceabef712ff10819",
            "corpusId": 265974965,
            "url": "https://www.semanticscholar.org/paper/4649eecad2473e98ce915825ceabef712ff10819",
            "title": "Reinforcement learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2014,
            "externalIds": {
                "DOI": "10.1201/b17476-11",
                "CorpusId": 265974965
            },
            "abstract": "For some real-world optimization problems where the best behavior is sought, it is infeasible to search for a solution by making a model of the problem and performing calculations on it. When this is the case, good solutions can sometimes be found by trial and error. Reinforcement learning is a way of finding optimal behavior by systematic trial and error. This thesis aims to compare different reinforcement learning techniques and evaluate them. Model-based interval estimation (MBIE) and Explicit Explore or Exploit using dynamic bayesian networks (DBN-E) are two algorithms that are evaluated. To evaluate the techniques, learning agents were constructed using the algorithms and then simulated in the environment Invasive Species from the Reinforcement Learning Competition. The results of the study show that an optimized version of DBN-E is better than MBIE at finding an optimal or near optimal behavior policy in Invasive Species for a selection of environment parameters. Using a factored model like a DBN shows certain advantages operating in Invasive Species, which is a factored environment. For example it achieves a near optimal policy within fewer episodes than MBIE.",
            "referenceCount": 18,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{ANDERSSON2014ReinforcementL,\n author = {JOHAN ANDERSSON and EMIL KRISTIANSSON and JOAKIM PERSSON and A. Eriksson and DANIEL TOOM and JOPPE WIDSTAM},\n title = {Reinforcement learning},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5c65d095600d6c647426fa3bc45031b208882d5f",
            "@type": "ScholarlyArticle",
            "paperId": "5c65d095600d6c647426fa3bc45031b208882d5f",
            "corpusId": 18760634,
            "url": "https://www.semanticscholar.org/paper/5c65d095600d6c647426fa3bc45031b208882d5f",
            "title": "Batch Reinforcement Learning",
            "venue": "Reinforcement Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "books/sp/12/LangeGR12",
                "MAG": "192920577",
                "DOI": "10.1007/978-3-642-27645-3_2",
                "CorpusId": 18760634
            },
            "abstract": null,
            "referenceCount": 47,
            "citationCount": 515,
            "influentialCitationCount": 34,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://ml.informatik.uni-freiburg.de/_media/publications/langegabelriedmiller2011chapter.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Lange2012BatchRL,\n author = {S. Lange and T. Gabel and Martin A. Riedmiller},\n booktitle = {Reinforcement Learning},\n pages = {45-73},\n title = {Batch Reinforcement Learning},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7872f34e2a164c5cf3c34a7a7433dc3342b6c7ea",
            "@type": "ScholarlyArticle",
            "paperId": "7872f34e2a164c5cf3c34a7a7433dc3342b6c7ea",
            "corpusId": 232322114,
            "url": "https://www.semanticscholar.org/paper/7872f34e2a164c5cf3c34a7a7433dc3342b6c7ea",
            "title": "Machine Learning: Algorithms, Real-World Applications and Research Directions",
            "venue": "SN Computer Science",
            "publicationVenue": {
                "id": "urn:research:7a7dc89b-e1a6-44df-a496-46c330a87840",
                "name": "SN Computer Science",
                "alternate_names": [
                    "SN Comput Sci"
                ],
                "issn": "2661-8907",
                "url": "https://link.springer.com/journal/42979"
            },
            "year": 2021,
            "externalIds": {
                "MAG": "3146760956",
                "PubMedCentral": "7983091",
                "DBLP": "journals/sncs/Sarker21a",
                "DOI": "10.1007/s42979-021-00592-x",
                "CorpusId": 232322114,
                "PubMed": "33778771"
            },
            "abstract": null,
            "referenceCount": 146,
            "citationCount": 1049,
            "influentialCitationCount": 36,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s42979-021-00592-x.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2021-03-08",
            "journal": {
                "name": "Sn Computer Science",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Sarker2021MachineLA,\n author = {Iqbal H. Sarker},\n booktitle = {SN Computer Science},\n journal = {Sn Computer Science},\n title = {Machine Learning: Algorithms, Real-World Applications and Research Directions},\n volume = {2},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ef48fea818706f109cc7c59ee61551d72f0bf2bc",
            "@type": "ScholarlyArticle",
            "paperId": "ef48fea818706f109cc7c59ee61551d72f0bf2bc",
            "corpusId": 109391519,
            "url": "https://www.semanticscholar.org/paper/ef48fea818706f109cc7c59ee61551d72f0bf2bc",
            "title": "Reinforcement learning for microgrid energy management",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2022185333",
                "DOI": "10.1016/J.ENERGY.2013.05.060",
                "CorpusId": 109391519
            },
            "abstract": null,
            "referenceCount": 47,
            "citationCount": 183,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2013-09-15",
            "journal": {
                "name": "Energy",
                "volume": "59"
            },
            "citationStyles": {
                "bibtex": "@Article{Kuznetsova2013ReinforcementLF,\n author = {E. Kuznetsova and Yanfu Li and C. Ruiz and E. Zio and G. Ault and K. Bell},\n journal = {Energy},\n pages = {133-146},\n title = {Reinforcement learning for microgrid energy management},\n volume = {59},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6fbe7b4dec82619923caa1d0fc3b35817b1c93c1",
            "@type": "ScholarlyArticle",
            "paperId": "6fbe7b4dec82619923caa1d0fc3b35817b1c93c1",
            "corpusId": 14166877,
            "url": "https://www.semanticscholar.org/paper/6fbe7b4dec82619923caa1d0fc3b35817b1c93c1",
            "title": "Dopamine and performance in a reinforcement learning task: evidence from Parkinson\u2019s disease",
            "venue": "Brain : a journal of neurology",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2170143116",
                "PubMedCentral": "3359751",
                "DOI": "10.1093/brain/aws083",
                "CorpusId": 14166877,
                "PubMed": "22508958"
            },
            "abstract": "The role dopamine plays in decision-making has important theoretical, empirical and clinical implications. Here, we examined its precise contribution by exploiting the lesion deficit model afforded by Parkinson\u2019s disease. We studied patients in a two-stage reinforcement learning task, while they were ON and OFF dopamine replacement medication. Contrary to expectation, we found that dopaminergic drug state (ON or OFF) did not impact learning. Instead, the critical factor was drug state during the performance phase, with patients ON medication choosing correctly significantly more frequently than those OFF medication. This effect was independent of drug state during initial learning and appears to reflect a facilitation of generalization for learnt information. This inference is bolstered by our observation that neural activity in nucleus accumbens and ventromedial prefrontal cortex, measured during simultaneously acquired functional magnetic resonance imaging, represented learnt stimulus values during performance. This effect was expressed solely during the ON state with activity in these regions correlating with better performance. Our data indicate that dopamine modulation of nucleus accumbens and ventromedial prefrontal cortex exerts a specific effect on choice behaviour distinct from pure learning. The findings are in keeping with the substantial other evidence that certain aspects of learning are unaffected by dopamine lesions or depletion, and that dopamine plays a key role in performance that may be distinct from its role in learning.",
            "referenceCount": 92,
            "citationCount": 525,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://academic.oup.com/brain/article-pdf/135/6/1871/13797015/aws083.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "ClinicalTrial"
            ],
            "publicationDate": "2012-04-14",
            "journal": {
                "name": "Brain",
                "volume": "135"
            },
            "citationStyles": {
                "bibtex": "@Article{Helmich2012DopamineAP,\n author = {R. Helmich and M. Hallett and G. Deuschl and I. Toni and B. Bloem and N. Granger and H. Blamires and R. J. M. Franklin and N. D. Jeffery and V. Failli and M. A. Kopp and C. Gericke and P. Martus and S. Klingbeil and B. Brommer and I. Laginha and Y. Chen and M. DeVivo and U. Dirnagl and J. M. Schwab and M. Sashindranath and E. Sales and M. Daglas and R. Freeman and A. L. Samson and E. Cops and S. Beckham and A. Galle and C. Mclean and C. Morganti-Kossmann and J. V. Rosenfeld and R. Madani and J. Vassalli and Enming J Su and D. A. Lawrence and R. Medcalf and M. Starkey and C. Bleul and B. Z\u00f6rner and N. T. Lindau and T. Mueggler and M. Rudin and M. Schwab and T. Doeppner and I. M\u0142ynarczuk-Bia\u0142y and U. Kuckelkorn and B. Kaltwasser and J. Herz and M. R. Hasan and D. Hermann and M. B\u00e4hr and H. Sakata and P. Narasimhan and K. Niizuma and C. Maier and T. Wakai and P. H. Chan and M. M. Helmy and E. Ruusuvuori and P. V. Watkins and J. Voipio and P. Kanold and K. Kaila and E. Butti and M. Bacigaluppi and S. Rossi and M. Cambiaghi and M. Bari and A. C. Silla and E. Brambilla and A. Musella and R. de Ceglia and L. Teneud and V. De Chiara and P. D 'adamo and J. Garc\u00eda-Verdugo and G. Comi and L. Muzio and A. Quattrini and L. Leocani and M. Maccarrone and D. Centonze and G. Martino and K. Sathe and W. Maetzler and J. D. Lang and R. B. Mounsey and C. Fleckenstein and H. L. Martin and C. Schulte and S. Mustafa and M. Synofzik and Z. Vukovic and S. Itohara and D. Berg and P. Teismann and C. Scherfler and K. Seppi and K. J. Mair and E. Donnemiller and I. Virgolini and G. Wenning and W. Poewe and L. T\u00f6nges and T. Frank and L. Tatenhorst and K. Saal and J. Koch and \u00c9. Szeg\u00f5 and Jochen H Weishaupt and P. Lingor and J. Ko\u0144czak and A. Sciutti and L. Avanzino and V. Squeri and M. Gori and L. Masia and G. Abbruzzese and G. Sandini and H. Tsuji and T. Arai and F. Kametani and T. Nonaka and M. Yamashita and M. Suzukake and M. Hosokawa and M. Yoshida and H. Hatsuta and M. Takao and Y. Saito and S. Murayama and H. Akiyama and M. Hasegawa and D. M. A. Mann and A. Tamaoka and D. S. Pitceathly and C. Smith and C. Fratter and C. Alston and L. He and K. Craig and E. Blakely and J. Evans and J. Taylor and Z. Shabbir and M. Deschauer and U. Pohl and M. Roberts and M. C. Jackson and C. Halfpenny and P. Turnpenny and P. W. Lunt and M. Hanna and A. Schaefer and R. Mcfarland and R. Horvath and P. Chinnery and D. Turnbull and J. Poulton and R. Taylor and G. Gorman and D. Ronchi and C. Garone and A. Bordoni and P. G. Rios and S. E. Calvo and M. Ripolone and M. Ranieri and M. Rizzuti and L. Villa and F. Magri and S. Corti and N. Bresolin and V. Mootha and M. Moggio and S. Dimauro and G. Comi and M. Sciacco and D. Cazzoli and R. M\u00fcri and R. Schumacher and S. von Arx and S. Chaves and K. Gutbrod and S. Bohlhalter and D. Bauer and T. Vanbellingen and M. Bertschi and S. Kipfer and C. R. Rosenthal and C. Kennard and C. Bassetti and T. Nyffeler and Brain},\n booktitle = {Brain : a journal of neurology},\n journal = {Brain},\n pages = {1871 - 1883},\n title = {Dopamine and performance in a reinforcement learning task: evidence from Parkinson\u2019s disease},\n volume = {135},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bdb8c8bcc54b02d037ffef2dfffcb1a55a85848e",
            "@type": "ScholarlyArticle",
            "paperId": "bdb8c8bcc54b02d037ffef2dfffcb1a55a85848e",
            "corpusId": 18627501,
            "url": "https://www.semanticscholar.org/paper/bdb8c8bcc54b02d037ffef2dfffcb1a55a85848e",
            "title": "Reinforcement Learning and Markov Decision Processes",
            "venue": "Reinforcement Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "169931978",
                "DBLP": "books/sp/12/OtterloW12",
                "DOI": "10.1007/978-3-642-27645-3_1",
                "CorpusId": 18627501
            },
            "abstract": null,
            "referenceCount": 78,
            "citationCount": 340,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Otterlo2012ReinforcementLA,\n author = {M. V. Otterlo and M. Wiering},\n booktitle = {Reinforcement Learning},\n pages = {3-42},\n title = {Reinforcement Learning and Markov Decision Processes},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:16c97a8a29b0d63fdb119daefabc47df92ff6c24",
            "@type": "ScholarlyArticle",
            "paperId": "16c97a8a29b0d63fdb119daefabc47df92ff6c24",
            "corpusId": 10087812,
            "url": "https://www.semanticscholar.org/paper/16c97a8a29b0d63fdb119daefabc47df92ff6c24",
            "title": "Transfer in Reinforcement Learning: A Framework and a Survey",
            "venue": "Reinforcement Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "books/sp/12/Lazaric12",
                "MAG": "158722652",
                "DOI": "10.1007/978-3-642-27645-3_5",
                "CorpusId": 10087812
            },
            "abstract": null,
            "referenceCount": 85,
            "citationCount": 270,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://hal.inria.fr/docs/00/77/26/26/PDF/transfer.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Lazaric2012TransferIR,\n author = {A. Lazaric},\n booktitle = {Reinforcement Learning},\n pages = {143-173},\n title = {Transfer in Reinforcement Learning: A Framework and a Survey},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6caac247905c9ed2bb7fcc6697a5c00205b98cc8",
            "@type": "ScholarlyArticle",
            "paperId": "6caac247905c9ed2bb7fcc6697a5c00205b98cc8",
            "corpusId": 6957503,
            "url": "https://www.semanticscholar.org/paper/6caac247905c9ed2bb7fcc6697a5c00205b98cc8",
            "title": "Understanding dopamine and reinforcement learning: The dopamine reward prediction error hypothesis",
            "venue": "Proceedings of the National Academy of Sciences of the United States of America",
            "publicationVenue": {
                "id": "urn:research:bb95bf2e-8383-4748-bf9d-d6906d091085",
                "name": "Proceedings of the National Academy of Sciences of the United States of America",
                "alternate_names": [
                    "PNAS",
                    "PNAS online",
                    "Proceedings of the National Academy of Sciences of the United States of America.",
                    "Proc National Acad Sci",
                    "Proceedings of the National Academy of Sciences",
                    "Proc National Acad Sci u s Am"
                ],
                "issn": "0027-8424",
                "url": "https://www.jstor.org/journal/procnatiacadscie"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2050233777",
                "DOI": "10.1073/pnas.1014269108",
                "CorpusId": 6957503,
                "PubMed": "21389268"
            },
            "abstract": "A number of recent advances have been achieved in the study of midbrain dopaminergic neurons. Understanding these advances and how they relate to one another requires a deep understanding of the computational models that serve as an explanatory framework and guide ongoing experimental inquiry. This intertwining of theory and experiment now suggests very clearly that the phasic activity of the midbrain dopamine neurons provides a global mechanism for synaptic modification. These synaptic modifications, in turn, provide the mechanistic underpinning for a specific class of reinforcement learning mechanisms that now seem to underlie much of human and animal behavior. This review describes both the critical empirical findings that are at the root of this conclusion and the fantastic theoretical advances from which this conclusion is drawn.",
            "referenceCount": 73,
            "citationCount": 791,
            "influentialCitationCount": 48,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2011-03-09",
            "journal": {
                "name": "Proceedings of the National Academy of Sciences",
                "volume": "108"
            },
            "citationStyles": {
                "bibtex": "@Article{Glimcher2011UnderstandingDA,\n author = {P. Glimcher},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {15647 - 15654},\n title = {Understanding dopamine and reinforcement learning: The dopamine reward prediction error hypothesis},\n volume = {108},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9a3bbdc091d9feb7b79b3b1ac476ea338944bd7d",
            "@type": "ScholarlyArticle",
            "paperId": "9a3bbdc091d9feb7b79b3b1ac476ea338944bd7d",
            "corpusId": 40981992,
            "url": "https://www.semanticscholar.org/paper/9a3bbdc091d9feb7b79b3b1ac476ea338944bd7d",
            "title": "Neural basis of reinforcement learning and decision making.",
            "venue": "Annual Review of Neuroscience",
            "publicationVenue": {
                "id": "urn:research:d9caa671-3be6-48bc-9710-f96334848b4c",
                "name": "Annual Review of Neuroscience",
                "alternate_names": [
                    "Annu Rev Neurosci"
                ],
                "issn": "0147-006X",
                "url": "https://www.annualreviews.org/journal/neuro"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2104148727",
                "DOI": "10.1146/annurev-neuro-062111-150512",
                "CorpusId": 40981992,
                "PubMed": "22462543"
            },
            "abstract": "Reinforcement learning is an adaptive process in which an animal utilizes its previous experience to improve the outcomes of future choices. Computational theories of reinforcement learning play a central role in the newly emerging areas of neuroeconomics and decision neuroscience. In this framework, actions are chosen according to their value functions, which describe how much future reward is expected from each action. Value functions can be adjusted not only through reward and penalty, but also by the animal's knowledge of its current environment. Studies have revealed that a large proportion of the brain is involved in representing and updating value functions and using them to choose an action. However, how the nature of a behavioral task affects the neural mechanisms of reinforcement learning remains incompletely understood. Future studies should uncover the principles by which different computational elements of reinforcement learning are dynamically coordinated across the entire brain.",
            "referenceCount": 161,
            "citationCount": 396,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc3490621?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2012-06-20",
            "journal": {
                "name": "Annual review of neuroscience",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{Lee2012NeuralBO,\n author = {Daeyeol Lee and H. Seo and M. Jung},\n booktitle = {Annual Review of Neuroscience},\n journal = {Annual review of neuroscience},\n pages = {\n          287-308\n        },\n title = {Neural basis of reinforcement learning and decision making.},\n volume = {35},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e28b84c2800d81532c89997ee3120ae3bf32977d",
            "@type": "ScholarlyArticle",
            "paperId": "e28b84c2800d81532c89997ee3120ae3bf32977d",
            "corpusId": 1685197,
            "url": "https://www.semanticscholar.org/paper/e28b84c2800d81532c89997ee3120ae3bf32977d",
            "title": "Reinforcement and Imitation Learning via Interactive No-Regret Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2014,
            "externalIds": {
                "ArXiv": "1406.5979",
                "MAG": "112666333",
                "DBLP": "journals/corr/RossB14",
                "CorpusId": 1685197
            },
            "abstract": "Recent work has demonstrated that problems-- particularly imitation learning and structured prediction-- where a learner's predictions influence the input-distribution it is tested on can be naturally addressed by an interactive approach and analyzed using no-regret online learning. These approaches to imitation learning, however, neither require nor benefit from information about the cost of actions. We extend existing results in two directions: first, we develop an interactive imitation learning approach that leverages cost information; second, we extend the technique to address reinforcement learning. The results provide theoretical support to the commonly observed successes of online approximate policy iteration. Our approach suggests a broad new family of algorithms and provides a unifying view of existing techniques for imitation and reinforcement learning.",
            "referenceCount": 29,
            "citationCount": 234,
            "influentialCitationCount": 34,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2014-06-23",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1406.5979"
            },
            "citationStyles": {
                "bibtex": "@Article{Ross2014ReinforcementAI,\n author = {St\u00e9phane Ross and J. Bagnell},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Reinforcement and Imitation Learning via Interactive No-Regret Learning},\n volume = {abs/1406.5979},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:26ed91408c3fa8604ac3c40b2931a131a23f8368",
            "@type": "ScholarlyArticle",
            "paperId": "26ed91408c3fa8604ac3c40b2931a131a23f8368",
            "corpusId": 53838315,
            "url": "https://www.semanticscholar.org/paper/26ed91408c3fa8604ac3c40b2931a131a23f8368",
            "title": "Game Theory and Multi-agent Reinforcement Learning",
            "venue": "Reinforcement Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "books/sp/12/NoweVH12",
                "MAG": "103885025",
                "DOI": "10.1007/978-3-642-27645-3_14",
                "CorpusId": 53838315
            },
            "abstract": null,
            "referenceCount": 61,
            "citationCount": 139,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Now\u00e92012GameTA,\n author = {A. Now\u00e9 and Peter Vrancx and Y. D. Hauwere},\n booktitle = {Reinforcement Learning},\n pages = {441-470},\n title = {Game Theory and Multi-agent Reinforcement Learning},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e5d53eee2d3b2fb13bfe0a4730f9f6756555d6c6",
            "@type": "ScholarlyArticle",
            "paperId": "e5d53eee2d3b2fb13bfe0a4730f9f6756555d6c6",
            "corpusId": 8078501,
            "url": "https://www.semanticscholar.org/paper/e5d53eee2d3b2fb13bfe0a4730f9f6756555d6c6",
            "title": "How much of reinforcement learning is working memory, not reinforcement learning? A behavioral, computational, and neurogenetic analysis",
            "venue": "European Journal of Neuroscience",
            "publicationVenue": {
                "id": "urn:research:a705f154-b548-498a-aa86-74f81b021f9e",
                "name": "European Journal of Neuroscience",
                "alternate_names": [
                    "Eur J Neurosci"
                ],
                "issn": "0953-816X",
                "url": "http://www.wiley.com/bw/journal.asp?ref=0953-816X&site=1"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2111971565",
                "DOI": "10.1111/j.1460-9568.2011.07980.x",
                "CorpusId": 8078501,
                "PubMed": "22487033"
            },
            "abstract": "Instrumental learning involves corticostriatal circuitry and the dopaminergic system. This system is typically modeled in the reinforcement learning (RL) framework by incrementally accumulating reward values of states and actions. However, human learning also implicates prefrontal cortical mechanisms involved in higher level cognitive functions. The interaction of these systems remains poorly understood, and models of human behavior often ignore working memory (WM) and therefore incorrectly assign behavioral variance to the RL system. Here we designed a task that highlights the profound entanglement of these two processes, even in simple learning problems. By systematically varying the size of the learning problem and delay between stimulus repetitions, we separately extracted WM\u2010specific effects of load and delay on learning. We propose a new computational model that accounts for the dynamic integration of RL and WM processes observed in subjects\u2019 behavior. Incorporating capacity\u2010limited WM into the model allowed us to capture behavioral variance that could not be captured in a pure RL framework even if we (implausibly) allowed separate RL systems for each set size. The WM component also allowed for a more reasonable estimation of a single RL process. Finally, we report effects of two genetic polymorphisms having relative specificity for prefrontal and basal ganglia functions. Whereas the COMT gene coding for catechol\u2010O\u2010methyl transferase selectively influenced model estimates of WM capacity, the GPR6 gene coding for G\u2010protein\u2010coupled receptor 6 influenced the RL learning rate. Thus, this study allowed us to specify distinct influences of the high\u2010level and low\u2010level cognitive functions on instrumental learning, beyond the possibilities offered by simple RL models.",
            "referenceCount": 73,
            "citationCount": 298,
            "influentialCitationCount": 24,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc3390186?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Study"
            ],
            "publicationDate": "2012-04-01",
            "journal": {
                "name": "European Journal of Neuroscience",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{Collins2012HowMO,\n author = {A. Collins and M. Frank},\n booktitle = {European Journal of Neuroscience},\n journal = {European Journal of Neuroscience},\n title = {How much of reinforcement learning is working memory, not reinforcement learning? A behavioral, computational, and neurogenetic analysis},\n volume = {35},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c5fa00d361e9e4d4344235ad4e354459f3f24e1e",
            "@type": "ScholarlyArticle",
            "paperId": "c5fa00d361e9e4d4344235ad4e354459f3f24e1e",
            "corpusId": 175713,
            "url": "https://www.semanticscholar.org/paper/c5fa00d361e9e4d4344235ad4e354459f3f24e1e",
            "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2001,
            "externalIds": {
                "DBLP": "conf/ijcai/BrafmanT01",
                "MAG": "2161966552",
                "DOI": "10.1162/153244303765208377",
                "CorpusId": 175713
            },
            "abstract": "R-MAX is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-MAX, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward (hence the name). During execution, it is updated based on the agent's observations. R-MAX improves upon several previous algorithms: (1) It is simpler and more general than Kearns and Singh's E3 algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism for resolving the exploration vs. exploitation dilemma. (3) It formally justifies the ``optimism under uncertainty'' bias used in many RL algorithms. (4) It is simpler, more general, and more efficient than Brafman and Tennenholtz's LSG algorithm for learning in single controller stochastic games. (5) It generalizes the algorithm by Monderer and Tennenholtz for learning in repeated games. (6) It is the only algorithm for learning in repeated games, to date, which is provably efficient, considerably improving and simplifying previous algorithms by Banos and by Megiddo.",
            "referenceCount": 20,
            "citationCount": 1296,
            "influentialCitationCount": 163,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-08-04",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Brafman2001RMAXA,\n author = {R. Brafman and Moshe Tennenholtz},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {213-231},\n title = {R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning},\n volume = {3},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f4c796ae4f735ddcb6545312dafc104d723a6a03",
            "@type": "ScholarlyArticle",
            "paperId": "f4c796ae4f735ddcb6545312dafc104d723a6a03",
            "corpusId": 7450323,
            "url": "https://www.semanticscholar.org/paper/f4c796ae4f735ddcb6545312dafc104d723a6a03",
            "title": "Neural Prediction Errors Reveal a Risk-Sensitive Reinforcement-Learning Process in the Human Brain",
            "venue": "Journal of Neuroscience",
            "publicationVenue": {
                "id": "urn:research:52edfc09-8d65-4876-8560-84530d7259b9",
                "name": "Journal of Neuroscience",
                "alternate_names": [
                    "The Journal of Neuroscience",
                    "J Neurosci"
                ],
                "issn": "0270-6474",
                "url": "https://www.jneurosci.org/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2033271727",
                "DOI": "10.1523/JNEUROSCI.5498-10.2012",
                "CorpusId": 7450323,
                "PubMed": "22238090"
            },
            "abstract": "Humans and animals are exquisitely, though idiosyncratically, sensitive to risk or variance in the outcomes of their actions. Economic, psychological, and neural aspects of this are well studied when information about risk is provided explicitly. However, we must normally learn about outcomes from experience, through trial and error. Traditional models of such reinforcement learning focus on learning about the mean reward value of cues and ignore higher order moments such as variance. We used fMRI to test whether the neural correlates of human reinforcement learning are sensitive to experienced risk. Our analysis focused on anatomically delineated regions of a priori interest in the nucleus accumbens, where blood oxygenation level-dependent (BOLD) signals have been suggested as correlating with quantities derived from reinforcement learning. We first provide unbiased evidence that the raw BOLD signal in these regions corresponds closely to a reward prediction error. We then derive from this signal the learned values of cues that predict rewards of equal mean but different variance and show that these values are indeed modulated by experienced risk. Moreover, a close neurometric\u2013psychometric coupling exists between the fluctuations of the experience-based evaluations of risky options that we measured neurally and the fluctuations in behavioral risk aversion. This suggests that risk sensitivity is integral to human learning, illuminating economic models of choice, neuroscientific models of affective learning, and the workings of the underlying neural mechanisms.",
            "referenceCount": 74,
            "citationCount": 300,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.jneurosci.org/content/jneuro/32/2/551.full.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-01-11",
            "journal": {
                "name": "The Journal of Neuroscience",
                "volume": "32"
            },
            "citationStyles": {
                "bibtex": "@Article{Niv2012NeuralPE,\n author = {Y. Niv and J. Edlund and P. Dayan and J. O\u2019Doherty},\n booktitle = {Journal of Neuroscience},\n journal = {The Journal of Neuroscience},\n pages = {551 - 562},\n title = {Neural Prediction Errors Reveal a Risk-Sensitive Reinforcement-Learning Process in the Human Brain},\n volume = {32},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b0f16acfa4efce9c24100ec330b82fb8a28feeec",
            "@type": "ScholarlyArticle",
            "paperId": "b0f16acfa4efce9c24100ec330b82fb8a28feeec",
            "corpusId": 21557823,
            "url": "https://www.semanticscholar.org/paper/b0f16acfa4efce9c24100ec330b82fb8a28feeec",
            "title": "Reinforcement Learning in Continuous State and Action Spaces",
            "venue": "Reinforcement Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "books/sp/12/Hasselt12",
                "MAG": "755046805",
                "DOI": "10.1007/978-3-642-27645-3_7",
                "CorpusId": 21557823
            },
            "abstract": null,
            "referenceCount": 189,
            "citationCount": 106,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2012-04-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Hasselt2012ReinforcementLI,\n author = {Hado Philip van Hasselt},\n booktitle = {Reinforcement Learning},\n pages = {207-251},\n title = {Reinforcement Learning in Continuous State and Action Spaces},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:92808faf1694ff4125a923925cd204e8f96a5f55",
            "@type": "ScholarlyArticle",
            "paperId": "92808faf1694ff4125a923925cd204e8f96a5f55",
            "corpusId": 6536428,
            "url": "https://www.semanticscholar.org/paper/92808faf1694ff4125a923925cd204e8f96a5f55",
            "title": "Evolutionary Computation for Reinforcement Learning",
            "venue": "Reinforcement Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "books/sp/12/Whiteson12",
                "MAG": "2174817438",
                "DOI": "10.1007/978-3-642-27645-3_10",
                "CorpusId": 6536428
            },
            "abstract": null,
            "referenceCount": 161,
            "citationCount": 54,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.bham.ac.uk/~wbl/biblio/gecco2006/docs/p1577.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Whiteson2012EvolutionaryCF,\n author = {Shimon Whiteson},\n booktitle = {Reinforcement Learning},\n pages = {325-355},\n title = {Evolutionary Computation for Reinforcement Learning},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:18cdb859f66113e582383fdfd13d267f86c89dbd",
            "@type": "ScholarlyArticle",
            "paperId": "18cdb859f66113e582383fdfd13d267f86c89dbd",
            "corpusId": 58330445,
            "url": "https://www.semanticscholar.org/paper/18cdb859f66113e582383fdfd13d267f86c89dbd",
            "title": "Reinforcement Learning in Games",
            "venue": "Reinforcement Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "113028485",
                "DBLP": "books/sp/12/Szita12",
                "DOI": "10.1007/978-3-642-27645-3_17",
                "CorpusId": 58330445
            },
            "abstract": null,
            "referenceCount": 92,
            "citationCount": 55,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Szita2012ReinforcementLI,\n author = {I. Szita},\n booktitle = {Reinforcement Learning},\n pages = {539-577},\n title = {Reinforcement Learning in Games},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
            "@type": "ScholarlyArticle",
            "paperId": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
            "corpusId": 215744839,
            "url": "https://www.semanticscholar.org/paper/020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
            "title": "Meta-Learning in Neural Networks: A Survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/pami/HospedalesAMS22",
                "MAG": "3015606043",
                "ArXiv": "2004.05439",
                "DOI": "10.1109/TPAMI.2021.3079209",
                "CorpusId": 215744839,
                "PubMed": "33974543"
            },
            "abstract": "The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.",
            "referenceCount": 332,
            "citationCount": 1184,
            "influentialCitationCount": 66,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/34/4359286/09428530.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-04-11",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "44"
            },
            "citationStyles": {
                "bibtex": "@Article{Hospedales2020MetaLearningIN,\n author = {Timothy M. Hospedales and Antreas Antoniou and P. Micaelli and A. Storkey},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {5149-5169},\n title = {Meta-Learning in Neural Networks: A Survey},\n volume = {44},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ed156c39a0d3a5f58660b571decbf3f46da5d752",
            "@type": "ScholarlyArticle",
            "paperId": "ed156c39a0d3a5f58660b571decbf3f46da5d752",
            "corpusId": 13328386,
            "url": "https://www.semanticscholar.org/paper/ed156c39a0d3a5f58660b571decbf3f46da5d752",
            "title": "Habits, action sequences and reinforcement learning",
            "venue": "European Journal of Neuroscience",
            "publicationVenue": {
                "id": "urn:research:a705f154-b548-498a-aa86-74f81b021f9e",
                "name": "European Journal of Neuroscience",
                "alternate_names": [
                    "Eur J Neurosci"
                ],
                "issn": "0953-816X",
                "url": "http://www.wiley.com/bw/journal.asp?ref=0953-816X&site=1"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "1583695788",
                "DOI": "10.1111/j.1460-9568.2012.08050.x",
                "CorpusId": 13328386,
                "PubMed": "22487034"
            },
            "abstract": "It is now widely accepted that instrumental actions can be either goal\u2010directed or habitual; whereas the former are rapidly acquired and regulated by their outcome, the latter are reflexive, elicited by antecedent stimuli rather than their consequences. Model\u2010based reinforcement learning (RL) provides an elegant description of goal\u2010directed action. Through exposure to states, actions and rewards, the agent rapidly constructs a model of the world and can choose an appropriate action based on quite abstract changes in environmental and evaluative demands. This model is powerful but has a problem explaining the development of habitual actions. To account for habits, theorists have argued that another action controller is required, called model\u2010free RL, that does not form a model of the world but rather caches action values within states allowing a state to select an action based on its reward history rather than its consequences. Nevertheless, there are persistent problems with important predictions from the model; most notably the failure of model\u2010free RL correctly to predict the insensitivity of habitual actions to changes in the action\u2013reward contingency. Here, we suggest that introducing model\u2010free RL in instrumental conditioning is unnecessary, and demonstrate that reconceptualizing habits as action sequences allows model\u2010based RL to be applied to both goal\u2010directed and habitual actions in a manner consistent with what real animals do. This approach has significant implications for the way habits are currently investigated and generates new experimental predictions.",
            "referenceCount": 123,
            "citationCount": 244,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc3325518?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle",
                "Study"
            ],
            "publicationDate": "2012-04-01",
            "journal": {
                "name": "European Journal of Neuroscience",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{Dezfouli2012HabitsAS,\n author = {A. Dezfouli and B. Balleine},\n booktitle = {European Journal of Neuroscience},\n journal = {European Journal of Neuroscience},\n title = {Habits, action sequences and reinforcement learning},\n volume = {35},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:47031c5b869ff9fe823d5d4bfa0f1ac3620714f0",
            "@type": "ScholarlyArticle",
            "paperId": "47031c5b869ff9fe823d5d4bfa0f1ac3620714f0",
            "corpusId": 5026912,
            "url": "https://www.semanticscholar.org/paper/47031c5b869ff9fe823d5d4bfa0f1ac3620714f0",
            "title": "Striatum-medial prefrontal cortex connectivity predicts developmental changes in reinforcement learning.",
            "venue": "Cerebral Cortex",
            "publicationVenue": {
                "id": "urn:research:388efbe2-cd51-4399-93f6-ba85b8300840",
                "name": "Cerebral Cortex",
                "alternate_names": [
                    "Cereb Cortex"
                ],
                "issn": "1047-3211",
                "url": "https://academic.oup.com/cercor"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2167747364",
                "DOI": "10.1093/cercor/bhr198",
                "CorpusId": 5026912,
                "PubMed": "21817091"
            },
            "abstract": "During development, children improve in learning from feedback to adapt their behavior. However, it is still unclear which neural mechanisms might underlie these developmental changes. In the current study, we used a reinforcement learning model to investigate neurodevelopmental changes in the representation and processing of learning signals. Sixty-seven healthy volunteers between ages 8 and 22 (children: 8-11 years, adolescents: 13-16 years, and adults: 18-22 years) performed a probabilistic learning task while in a magnetic resonance imaging scanner. The behavioral data demonstrated age differences in learning parameters with a stronger impact of negative feedback on expected value in children. Imaging data revealed that the neural representation of prediction errors was similar across age groups, but functional connectivity between the ventral striatum and the medial prefrontal cortex changed as a function of age. Furthermore, the connectivity strength predicted the tendency to alter expectations after receiving negative feedback. These findings suggest that the underlying mechanisms of developmental changes in learning are not related to differences in the neural representation of learning signals per se but rather in how learning signals are used to guide behavior and expectations.",
            "referenceCount": 62,
            "citationCount": 236,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc6283353?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-06-01",
            "journal": {
                "name": "Cerebral cortex",
                "volume": "22 6"
            },
            "citationStyles": {
                "bibtex": "@Article{Bos2012StriatummedialPC,\n author = {W. van den Bos and Michael X. Cohen and T. Kahnt and E. Crone},\n booktitle = {Cerebral Cortex},\n journal = {Cerebral cortex},\n pages = {\n          1247-55\n        },\n title = {Striatum-medial prefrontal cortex connectivity predicts developmental changes in reinforcement learning.},\n volume = {22 6},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:52449f97e09c7465adbc1d4f16e063802d392530",
            "@type": "ScholarlyArticle",
            "paperId": "52449f97e09c7465adbc1d4f16e063802d392530",
            "corpusId": 17023340,
            "url": "https://www.semanticscholar.org/paper/52449f97e09c7465adbc1d4f16e063802d392530",
            "title": "Autonomous reinforcement learning on raw visual input data in a real world application",
            "venue": "IEEE International Joint Conference on Neural Network",
            "publicationVenue": {
                "id": "urn:research:f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                "name": "IEEE International Joint Conference on Neural Network",
                "alternate_names": [
                    "IJCNN",
                    "IEEE Int Jt Conf Neural Netw",
                    "Int Jt Conf Neural Netw",
                    "International Joint Conference on Neural Network"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=1573"
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "conf/ijcnn/LangeRV12",
                "MAG": "1968962398",
                "DOI": "10.1109/IJCNN.2012.6252823",
                "CorpusId": 17023340
            },
            "abstract": "We propose a learning architecture, that is able to do reinforcement learning based on raw visual input data. In contrast to previous approaches, not only the control policy is learned. In order to be successful, the system must also autonomously learn, how to extract relevant information out of a high-dimensional stream of input information, for which the semantics are not provided to the learning system. We give a first proof-of-concept of this novel learning architecture on a challenging benchmark, namely visual control of a racing slot car. The resulting policy, learned only by success or failure, is hardly beaten by an experienced human player.",
            "referenceCount": 39,
            "citationCount": 232,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://zenodo.org/record/1273449/files/article.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2012-06-10",
            "journal": {
                "name": "The 2012 International Joint Conference on Neural Networks (IJCNN)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lange2012AutonomousRL,\n author = {S. Lange and Martin A. Riedmiller and Arne Voigtl\u00e4nder},\n booktitle = {IEEE International Joint Conference on Neural Network},\n journal = {The 2012 International Joint Conference on Neural Networks (IJCNN)},\n pages = {1-8},\n title = {Autonomous reinforcement learning on raw visual input data in a real world application},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b06d433ea090194d071a1171ce57d1ccf8b7c010",
            "@type": "ScholarlyArticle",
            "paperId": "b06d433ea090194d071a1171ce57d1ccf8b7c010",
            "corpusId": 58404165,
            "url": "https://www.semanticscholar.org/paper/b06d433ea090194d071a1171ce57d1ccf8b7c010",
            "title": "Reinforcement Learning: State-of-the-Art",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2760506156",
                "CorpusId": 58404165
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 514,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Now\u00e92012ReinforcementLS,\n author = {A. Now\u00e9 and Peter Vrancx and Y. D. Hauwere},\n title = {Reinforcement Learning: State-of-the-Art},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b1d730ad00d877b5ac6a1abb720483945a1904fd",
            "@type": "ScholarlyArticle",
            "paperId": "b1d730ad00d877b5ac6a1abb720483945a1904fd",
            "corpusId": 14302893,
            "url": "https://www.semanticscholar.org/paper/b1d730ad00d877b5ac6a1abb720483945a1904fd",
            "title": "Experience Replay for Real-Time Reinforcement Learning Control",
            "venue": "IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)",
            "publicationVenue": {
                "id": "urn:research:ecb11fdd-9e59-482f-a3b6-0cb14372306c",
                "name": "IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)",
                "alternate_names": [
                    "IEEE Trans Syst Man Cybern Part C (applications Rev"
                ],
                "issn": "1094-6977",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5326"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "1982262386",
                "DBLP": "journals/tsmc/AdamBB12",
                "DOI": "10.1109/TSMCC.2011.2106494",
                "CorpusId": 14302893
            },
            "abstract": "Reinforcement-learning (RL) algorithms can automatically learn optimal control strategies for nonlinear, possibly stochastic systems. A promising approach for RL control is experience replay (ER), which learns quickly from a limited amount of data, by repeatedly presenting these data to an underlying RL algorithm. Despite its benefits, ER RL has been studied only sporadically in the literature, and its applications have largely been confined to simulated systems. Therefore, in this paper, we evaluate ER RL on real-time control experiments that involve a pendulum swing-up problem and the vision-based control of a goalkeeper robot. These real-time experiments are complemented by simulation studies and comparisons with traditional RL. As a preliminary, we develop a general ER framework that can be combined with essentially any incremental RL technique, and instantiate this framework for the approximate Q-learning and SARSA algorithms. The successful real-time learning results that are presented here are highly encouraging for the applicability of ER RL in practice.",
            "referenceCount": 39,
            "citationCount": 217,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-03-01",
            "journal": {
                "name": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",
                "volume": "42"
            },
            "citationStyles": {
                "bibtex": "@Article{Adam2012ExperienceRF,\n author = {Sander Adam and L. Bu\u015foniu and Robert Babu\u0161ka},\n booktitle = {IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},\n pages = {201-212},\n title = {Experience Replay for Real-Time Reinforcement Learning Control},\n volume = {42},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3032844d6ac6882ccb03e7a2c22a0026b210ac05",
            "@type": "ScholarlyArticle",
            "paperId": "3032844d6ac6882ccb03e7a2c22a0026b210ac05",
            "corpusId": 236956615,
            "url": "https://www.semanticscholar.org/paper/3032844d6ac6882ccb03e7a2c22a0026b210ac05",
            "title": "What Matters in Learning from Offline Human Demonstrations for Robot Manipulation",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2108.03298",
                "DBLP": "journals/corr/abs-2108-03298",
                "CorpusId": 236956615
            },
            "abstract": "Imitating human demonstrations is a promising approach to endow robots with various manipulation capabilities. While recent advances have been made in imitation learning and batch (offline) reinforcement learning, a lack of open-source human datasets and reproducible learning methods make assessing the state of the field difficult. In this paper, we conduct an extensive study of six offline learning algorithms for robot manipulation on five simulated and three real-world multi-stage manipulation tasks of varying complexity, and with datasets of varying quality. Our study analyzes the most critical challenges when learning from offline human data for manipulation. Based on the study, we derive a series of lessons including the sensitivity to different algorithmic design choices, the dependence on the quality of the demonstrations, and the variability based on the stopping criteria due to the different objectives in training and evaluation. We also highlight opportunities for learning from human datasets, such as the ability to learn proficient policies on challenging, multi-stage tasks beyond the scope of current reinforcement learning methods, and the ability to easily scale to natural, real-world manipulation scenarios where only raw sensory signals are available. We have open-sourced our datasets and all algorithm implementations to facilitate future research and fair comparisons in learning from human demonstration data. Codebase, datasets, trained models, and more available at https://arise-initiative.github.io/robomimic-web/",
            "referenceCount": 90,
            "citationCount": 184,
            "influentialCitationCount": 33,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-08-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Mandlekar2021WhatMI,\n author = {Ajay Mandlekar and Danfei Xu and J. Wong and Soroush Nasiriany and Chen Wang and Rohun Kulkarni and Li Fei-Fei and S. Savarese and Yuke Zhu and Roberto Mart'in-Mart'in},\n booktitle = {Conference on Robot Learning},\n pages = {1678-1690},\n title = {What Matters in Learning from Offline Human Demonstrations for Robot Manipulation},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:44e1dd74f0446ec91221189ad3a65edb1a0208fe",
            "@type": "ScholarlyArticle",
            "paperId": "44e1dd74f0446ec91221189ad3a65edb1a0208fe",
            "corpusId": 205572964,
            "url": "https://www.semanticscholar.org/paper/44e1dd74f0446ec91221189ad3a65edb1a0208fe",
            "title": "A guide to deep learning in healthcare",
            "venue": "Nature Network Boston",
            "publicationVenue": {
                "id": "urn:research:9e995b6d-f30b-4ab4-a13b-3dc2cc992f47",
                "name": "Nature Network Boston",
                "alternate_names": [
                    "Nat Netw Boston",
                    "Nat Med",
                    "Nature Medicine"
                ],
                "issn": "1744-7933",
                "url": "https://www.nature.com/nature/articles?code=archive_news"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2905810301",
                "DOI": "10.1038/s41591-018-0316-z",
                "CorpusId": 205572964,
                "PubMed": "30617335"
            },
            "abstract": null,
            "referenceCount": 65,
            "citationCount": 1773,
            "influentialCitationCount": 41,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2019-01-01",
            "journal": {
                "name": "Nature Medicine",
                "volume": "25"
            },
            "citationStyles": {
                "bibtex": "@Article{Esteva2019AGT,\n author = {A. Esteva and Alexandre Robicquet and Bharath Ramsundar and Volodymyr Kuleshov and M. DePristo and Katherine Chou and Claire Cui and Greg S. Corrado and S. Thrun and J. Dean},\n booktitle = {Nature Network Boston},\n journal = {Nature Medicine},\n pages = {24 - 29},\n title = {A guide to deep learning in healthcare},\n volume = {25},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cc22d04377d75c35fd806620687143e7a120db5d",
            "@type": "ScholarlyArticle",
            "paperId": "cc22d04377d75c35fd806620687143e7a120db5d",
            "corpusId": 60747940,
            "url": "https://www.semanticscholar.org/paper/cc22d04377d75c35fd806620687143e7a120db5d",
            "title": "Reinforcement Learning and Dynamic Programming Using Function Approximators",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "1626155273",
                "DOI": "10.1201/9781439821091",
                "CorpusId": 60747940
            },
            "abstract": "From household appliances to applications in robotics, engineered systems involving complex dynamics can only be as effective as the algorithms that control them. While Dynamic Programming (DP) has provided researchers with a way to optimally solve decision and control problems involving complex dynamic systems, its practical value was limited by algorithms that lacked the capacity to scale up to realistic problems. However, in recent years, dramatic developments in Reinforcement Learning (RL), the model-free counterpart of DP, changed our understanding of what is possible. Those developments led to the creation of reliable methods that can be applied even when a mathematical model of the system is unavailable, allowing researchers to solve challenging control problems in engineering, as well as in a variety of other disciplines, including economics, medicine, and artificial intelligence. Reinforcement Learning and Dynamic Programming Using Function Approximators provides a comprehensive and unparalleled exploration of the field of RL and DP. With a focus on continuous-variable problems, this seminal text details essential developments that have substantially altered the field over the past decade. In its pages, pioneering experts provide a concise introduction to classical RL and DP, followed by an extensive presentation of the state-of-the-art and novel methods in RL and DP with approximation. Combining algorithm development with theoretical guarantees, they elaborate on their work with illustrative examples and insightful comparisons. Three individual chapters are dedicated to representative algorithms from each of the major classes of techniques: value iteration, policy iteration, and policy search. The features and performance of these algorithms are highlighted in extensive experimental studies on a range of control applications. The recent development of applications involving complex systems has led to a surge of interest in RL and DP methods and the subsequent need for a quality resource on the subject. For graduate students and others new to the field, this book offers a thorough introduction to both the basics and emerging methods. And for those researchers and practitioners working in the fields of optimal and adaptive control, machine learning, artificial intelligence, and operations research, this resource offers a combination of practical algorithms, theoretical analysis, and comprehensive examples that they will be able to adapt and apply to their own work. Access the authors' website at www.dcsc.tudelft.nl/rlbook/ for additional material, including computer code used in the studies and information concerning new developments.",
            "referenceCount": 73,
            "citationCount": 868,
            "influentialCitationCount": 66,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://orbi.uliege.be/bitstream/2268/27963/1/book-FA-RL-DP.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2010-04-29",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Bu\u015foniu2010ReinforcementLA,\n author = {L. Bu\u015foniu and Robert Babu\u0161ka and B. Schutter and D. Ernst},\n title = {Reinforcement Learning and Dynamic Programming Using Function Approximators},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:18a4cb69dd42d5b220c5443ad342a134b101158c",
            "@type": "ScholarlyArticle",
            "paperId": "18a4cb69dd42d5b220c5443ad342a134b101158c",
            "corpusId": 47518713,
            "url": "https://www.semanticscholar.org/paper/18a4cb69dd42d5b220c5443ad342a134b101158c",
            "title": "APRIL: Active Preference-learning based Reinforcement Learning",
            "venue": "ECML/PKDD",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "conf/pkdd/AkrourSS12",
                "MAG": "1583953806",
                "ArXiv": "1208.0984",
                "DOI": "10.1007/978-3-642-33486-3_8",
                "CorpusId": 47518713
            },
            "abstract": null,
            "referenceCount": 34,
            "citationCount": 120,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-642-33486-3_8.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-08-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1208.0984"
            },
            "citationStyles": {
                "bibtex": "@Article{Akrour2012APRILAP,\n author = {R. Akrour and Marc Schoenauer and M. Sebag},\n booktitle = {ECML/PKDD},\n journal = {ArXiv},\n title = {APRIL: Active Preference-learning based Reinforcement Learning},\n volume = {abs/1208.0984},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:af61e43831d35ec4f4db5e60f227629d2f39a437",
            "@type": "ScholarlyArticle",
            "paperId": "af61e43831d35ec4f4db5e60f227629d2f39a437",
            "corpusId": 124424860,
            "url": "https://www.semanticscholar.org/paper/af61e43831d35ec4f4db5e60f227629d2f39a437",
            "title": "On the Sample Complexity of Reinforcement Learning with a Generative Model",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2951326042",
                "DBLP": "conf/icml/AzarMK12",
                "ArXiv": "1206.6461",
                "CorpusId": 124424860
            },
            "abstract": "We consider the problem of learning the optimal action-value function in the discounted-reward Markov decision processes (MDPs). We prove a new PAC bound on the sample-complexity of model-based value iteration algorithm in the presence of the generative model, which indicates that for an MDP with N state-action pairs and the discount factor \u03b3 \u2208 [0, 1) only O(N log(N/\u03b4)/(1 - \u03b3)3e2)) samples are required to find an e-optimal estimation of the action-value function with the probability 1 - \u03b4. We also prove a matching lower bound of \u0398(N log(N/\u03b4)/((1 - \u03b3)3e2)) on the sample complexity of estimating the optimal action-value function by every RL algorithm. To the best of our knowledge, this is the first matching result on the sample complexity of estimating the optimal (action-) value function in which the upper bound matches the lower bound of RL in terms of N, e, \u03b4 and 1/(1-\u03b3). Also, both our lower bound and our upper bound significantly improve on the state-of-the-art in terms of 1/(1 - \u03b3).",
            "referenceCount": 19,
            "citationCount": 216,
            "influentialCitationCount": 43,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2012-06-26",
            "journal": {
                "name": "",
                "volume": "29"
            },
            "citationStyles": {
                "bibtex": "@Article{Azar2012OnTS,\n author = {M. G. Azar and R. Munos and H. Kappen},\n booktitle = {International Conference on Machine Learning},\n pages = {1707-1714},\n title = {On the Sample Complexity of Reinforcement Learning with a Generative Model},\n volume = {29},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fd30e9fcc821f819329cfa81174a2c163c215151",
            "@type": "ScholarlyArticle",
            "paperId": "fd30e9fcc821f819329cfa81174a2c163c215151",
            "corpusId": 17335492,
            "url": "https://www.semanticscholar.org/paper/fd30e9fcc821f819329cfa81174a2c163c215151",
            "title": "Generalization of value in reinforcement learning by humans",
            "venue": "European Journal of Neuroscience",
            "publicationVenue": {
                "id": "urn:research:a705f154-b548-498a-aa86-74f81b021f9e",
                "name": "European Journal of Neuroscience",
                "alternate_names": [
                    "Eur J Neurosci"
                ],
                "issn": "0953-816X",
                "url": "http://www.wiley.com/bw/journal.asp?ref=0953-816X&site=1"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "1535087671",
                "DOI": "10.1111/j.1460-9568.2012.08017.x",
                "CorpusId": 17335492,
                "PubMed": "22487039"
            },
            "abstract": "Research in decision\u2010making has focused on the role of dopamine and its striatal targets in guiding choices via learned stimulus\u2013reward or stimulus\u2013response associations, behavior that is well described by reinforcement learning theories. However, basic reinforcement learning is relatively limited in scope and does not explain how learning about stimulus regularities or relations may guide decision\u2010making. A candidate mechanism for this type of learning comes from the domain of memory, which has highlighted a role for the hippocampus in learning of stimulus\u2013stimulus relations, typically dissociated from the role of the striatum in stimulus\u2013response learning. Here, we used functional magnetic resonance imaging and computational model\u2010based analyses to examine the joint contributions of these mechanisms to reinforcement learning. Humans performed a reinforcement learning task with added relational structure, modeled after tasks used to isolate hippocampal contributions to memory. On each trial participants chose one of four options, but the reward probabilities for pairs of options were correlated across trials. This (uninstructed) relationship between pairs of options potentially enabled an observer to learn about option values based on experience with the other options and to generalize across them. We observed blood oxygen level\u2010dependent (BOLD) activity related to learning in the striatum and also in the hippocampus. By comparing a basic reinforcement learning model to one augmented to allow feedback to generalize between correlated options, we tested whether choice behavior and BOLD activity were influenced by the opportunity to generalize across correlated options. Although such generalization goes beyond standard computational accounts of reinforcement learning and striatal BOLD, both choices and striatal BOLD activity were better explained by the augmented model. Consistent with the hypothesized role for the hippocampus in this generalization, functional connectivity between the ventral striatum and hippocampus was modulated, across participants, by the ability of the augmented model to capture participants\u2019 choice. Our results thus point toward an interactive model in which striatal reinforcement learning systems may employ relational representations typically associated with the hippocampus.",
            "referenceCount": 113,
            "citationCount": 110,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/j.1460-9568.2012.08017.x",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Study"
            ],
            "publicationDate": "2012-04-01",
            "journal": {
                "name": "European Journal of Neuroscience",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{Wimmer2012GeneralizationOV,\n author = {G. E. Wimmer and N. Daw and D. Shohamy},\n booktitle = {European Journal of Neuroscience},\n journal = {European Journal of Neuroscience},\n title = {Generalization of value in reinforcement learning by humans},\n volume = {35},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:38d35d5581e58dca4e9458501e65c1f85ca754d5",
            "@type": "ScholarlyArticle",
            "paperId": "38d35d5581e58dca4e9458501e65c1f85ca754d5",
            "corpusId": 871178,
            "url": "https://www.semanticscholar.org/paper/38d35d5581e58dca4e9458501e65c1f85ca754d5",
            "title": "The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems",
            "venue": "AAAI/IAAI",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2104602264",
                "DBLP": "conf/aaai/ClausB98",
                "CorpusId": 871178
            },
            "abstract": "Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multi agent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-leaming in cooperative multi agent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium.",
            "referenceCount": 31,
            "citationCount": 1282,
            "influentialCitationCount": 130,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1998-07-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Claus1998TheDO,\n author = {C. Claus and Craig Boutilier},\n booktitle = {AAAI/IAAI},\n pages = {746-752},\n title = {The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:749744b5c797bbcff42dc27820f748ae94cc1932",
            "@type": "ScholarlyArticle",
            "paperId": "749744b5c797bbcff42dc27820f748ae94cc1932",
            "corpusId": 16032859,
            "url": "https://www.semanticscholar.org/paper/749744b5c797bbcff42dc27820f748ae94cc1932",
            "title": "Introduction to Deep Learning",
            "venue": "Advanced Deep Learning for Engineers and Scientists",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "MAG": "3186768639",
                "DOI": "10.1007/978-981-13-8285-7_6",
                "CorpusId": 16032859
            },
            "abstract": null,
            "referenceCount": 26,
            "citationCount": 210,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://hal.science/hal-01352061/document",
                "status": "CLOSED"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2021-12-10",
            "journal": {
                "name": "Advanced Deep Learning for Engineers and Scientists",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sewak2021IntroductionTD,\n author = {Mohit Sewak},\n booktitle = {Advanced Deep Learning for Engineers and Scientists},\n journal = {Advanced Deep Learning for Engineers and Scientists},\n title = {Introduction to Deep Learning},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:23a165c12b138cdaa726ef375b0930c8b56d4821",
            "@type": "ScholarlyArticle",
            "paperId": "23a165c12b138cdaa726ef375b0930c8b56d4821",
            "corpusId": 16837636,
            "url": "https://www.semanticscholar.org/paper/23a165c12b138cdaa726ef375b0930c8b56d4821",
            "title": "The ubiquity of model-based reinforcement learning",
            "venue": "Current Opinion in Neurobiology",
            "publicationVenue": {
                "id": "urn:research:d0db851a-b0a0-41e2-bef3-3861d0389a5b",
                "name": "Current Opinion in Neurobiology",
                "alternate_names": [
                    "Curr Opin Neurobiol"
                ],
                "issn": "0959-4388",
                "url": "http://www.current-opinion.com/journals/current-opinion-in-neurobiology/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "1977114987",
                "DOI": "10.1016/j.conb.2012.08.003",
                "CorpusId": 16837636,
                "PubMed": "22959354"
            },
            "abstract": null,
            "referenceCount": 65,
            "citationCount": 337,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc3513648?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2012-12-01",
            "journal": {
                "name": "Current Opinion in Neurobiology",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Doll2012TheUO,\n author = {B. B. Doll and Dylan A. Simon and N. Daw},\n booktitle = {Current Opinion in Neurobiology},\n journal = {Current Opinion in Neurobiology},\n pages = {1075-1081},\n title = {The ubiquity of model-based reinforcement learning},\n volume = {22},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cd15aee36d88704e7c923bd123bbc35788a98095",
            "@type": "ScholarlyArticle",
            "paperId": "cd15aee36d88704e7c923bd123bbc35788a98095",
            "corpusId": 17397350,
            "url": "https://www.semanticscholar.org/paper/cd15aee36d88704e7c923bd123bbc35788a98095",
            "title": "From reinforcement learning models to psychiatric and neurological disorders",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2058279623",
                "DOI": "10.1038/nn.2723",
                "CorpusId": 17397350,
                "PubMed": "21270784"
            },
            "abstract": null,
            "referenceCount": 106,
            "citationCount": 646,
            "influentialCitationCount": 25,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc4408000?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2011-02-01",
            "journal": {
                "name": "Nature Neuroscience",
                "volume": "14"
            },
            "citationStyles": {
                "bibtex": "@Article{Maia2011FromRL,\n author = {T. Maia and M. Frank},\n booktitle = {Nature Neuroscience},\n journal = {Nature Neuroscience},\n pages = {154-162},\n title = {From reinforcement learning models to psychiatric and neurological disorders},\n volume = {14},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:087874409722c38bbb643e3c7328e86a49a1aac6",
            "@type": "ScholarlyArticle",
            "paperId": "087874409722c38bbb643e3c7328e86a49a1aac6",
            "corpusId": 59972055,
            "url": "https://www.semanticscholar.org/paper/087874409722c38bbb643e3c7328e86a49a1aac6",
            "title": "Multi-Agent Machine Learning: A Reinforcement Approach",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2014,
            "externalIds": {
                "MAG": "568318363",
                "DOI": "10.1002/9781118884614",
                "CorpusId": 59972055
            },
            "abstract": "multi agent machine learning a reinforcement approach multi-agent machine learning: a reinforcement approach by multi agent machine learning a reinforcement approach multi agent machine learning a reinforcement approach user crowd simulation via multi-agent reinforcement learning multi-agent machine learning: a reinforcement approach by multi-agent reinforcement learning for planning and multi-agent machine download.e-bookshelf multi-agent machine learning buch cooperative multi-agent learning: the state of the art learning to communicate with deep multi-agent a modular approach to multi-agent reinforcement learning multi agent machine learning a reinforcement approach multi-agent machine learning: a reinforcement approach by multi-agent machine learning: a reinforcement approach multi agent machine learning a reinforcement approach interactive machine learning 2015w, se, 2.0 3.0 ects 45 06 online learning for multi-agent local navigation learning to communicate with deep multi-agent a mas reinforcement learning approach for indeterministic safe, multi-agent, reinforcement learning for arxiv multi-agent inverse reinforcement learning combining machine learning and multi-agent approach for robocup soccer simulation: a reinforcement learning approach multi-agent relational reinforcement learning a generic multi-agent reinforcement learning approach for supervised rule learning and reinforcement learning in a multi-agent reinforcement learning simulation for multi iensemble: a framework for committee machine based on simulation and investigation of multi-agent reinforcement hierarchical reinforcement learning in multi-agent environment learning to cooperate in multi-agent systems by combining traf?c light control by multiagent reinforcement learning multi-agent relational reinforcement learning cooperative reinforcement learning in topology-based multi multi-agent case-based reasoning for cooperative cooperative coevolution of multi-agent systems conditional random fields for multi-agent reinforcement",
            "referenceCount": 5,
            "citationCount": 98,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2014-08-11",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Schwartz2014MultiAgentML,\n author = {H. Schwartz},\n title = {Multi-Agent Machine Learning: A Reinforcement Approach},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d1d828ff8b9ed26e4e64c8ae89ac2b98d683a576",
            "@type": "ScholarlyArticle",
            "paperId": "d1d828ff8b9ed26e4e64c8ae89ac2b98d683a576",
            "corpusId": 35758336,
            "url": "https://www.semanticscholar.org/paper/d1d828ff8b9ed26e4e64c8ae89ac2b98d683a576",
            "title": "Hierarchical reinforcement learning and decision making",
            "venue": "Current Opinion in Neurobiology",
            "publicationVenue": {
                "id": "urn:research:d0db851a-b0a0-41e2-bef3-3861d0389a5b",
                "name": "Current Opinion in Neurobiology",
                "alternate_names": [
                    "Curr Opin Neurobiol"
                ],
                "issn": "0959-4388",
                "url": "http://www.current-opinion.com/journals/current-opinion-in-neurobiology/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2071302132",
                "DOI": "10.1016/j.conb.2012.05.008",
                "CorpusId": 35758336,
                "PubMed": "22695048"
            },
            "abstract": null,
            "referenceCount": 58,
            "citationCount": 189,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2012-12-01",
            "journal": {
                "name": "Current Opinion in Neurobiology",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Botvinick2012HierarchicalRL,\n author = {M. Botvinick},\n booktitle = {Current Opinion in Neurobiology},\n journal = {Current Opinion in Neurobiology},\n pages = {956-962},\n title = {Hierarchical reinforcement learning and decision making},\n volume = {22},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:642b89cdc165de1b9dac019c76e2cac6efdc5f37",
            "@type": "ScholarlyArticle",
            "paperId": "642b89cdc165de1b9dac019c76e2cac6efdc5f37",
            "corpusId": 5947874,
            "url": "https://www.semanticscholar.org/paper/642b89cdc165de1b9dac019c76e2cac6efdc5f37",
            "title": "Reinforcement learning and adaptive dynamic programming for feedback control",
            "venue": "IEEE Circuits and Systems Magazine",
            "publicationVenue": {
                "id": "urn:research:6c3888fc-6ac6-4924-b8b5-5a39d2a269b3",
                "name": "IEEE Circuits and Systems Magazine",
                "alternate_names": [
                    "IEEE Circuit Syst Mag",
                    "IEEE Circuits & Systems Magazine",
                    "IEEE Circuit  Syst Mag"
                ],
                "issn": "1531-636X",
                "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=7384"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2165726932",
                "DOI": "10.1109/MCAS.2009.933854",
                "CorpusId": 5947874
            },
            "abstract": "Living organisms learn by acting on their environment, observing the resulting reward stimulus, and adjusting their actions accordingly to improve the reward. This action-based or reinforcement learning can capture notions of optimal behavior occurring in natural systems. We describe mathematical formulations for reinforcement learning and a practical implementation method known as adaptive dynamic programming. These give us insight into the design of controllers for man-made engineered systems that both learn and exhibit optimal behavior.",
            "referenceCount": 98,
            "citationCount": 1011,
            "influentialCitationCount": 63,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2009-09-01",
            "journal": {
                "name": "IEEE Circuits and Systems Magazine",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Lewis2009ReinforcementLA,\n author = {F. Lewis and D. Vrabie},\n booktitle = {IEEE Circuits and Systems Magazine},\n journal = {IEEE Circuits and Systems Magazine},\n pages = {32-50},\n title = {Reinforcement learning and adaptive dynamic programming for feedback control},\n volume = {9},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b3fc56876ad1cdf35ad4af13b991bbb24d219bd9",
            "@type": "ScholarlyArticle",
            "paperId": "b3fc56876ad1cdf35ad4af13b991bbb24d219bd9",
            "corpusId": 260435822,
            "url": "https://www.semanticscholar.org/paper/b3fc56876ad1cdf35ad4af13b991bbb24d219bd9",
            "title": "Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1997,
            "externalIds": {
                "MAG": "1641379095",
                "DBLP": "conf/icml/Tan93",
                "DOI": "10.1016/b978-1-55860-307-3.50049-6",
                "CorpusId": 260435822
            },
            "abstract": null,
            "referenceCount": 13,
            "citationCount": 1357,
            "influentialCitationCount": 143,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1997-10-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tan1997MultiAgentRL,\n author = {Ming Tan},\n booktitle = {International Conference on Machine Learning},\n pages = {330-337},\n title = {Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4ab53de69372ec2cd2d90c126b6a100165dc8ed1",
            "@type": "ScholarlyArticle",
            "paperId": "4ab53de69372ec2cd2d90c126b6a100165dc8ed1",
            "corpusId": 16153365,
            "url": "https://www.semanticscholar.org/paper/4ab53de69372ec2cd2d90c126b6a100165dc8ed1",
            "title": "Generative Adversarial Imitation Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/HoE16",
                "MAG": "2949080919",
                "ArXiv": "1606.03476",
                "CorpusId": 16153365
            },
            "abstract": "Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.",
            "referenceCount": 32,
            "citationCount": 2326,
            "influentialCitationCount": 499,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-10",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ho2016GenerativeAI,\n author = {Jonathan Ho and Stefano Ermon},\n booktitle = {Neural Information Processing Systems},\n pages = {4565-4573},\n title = {Generative Adversarial Imitation Learning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0a8149fb5aa8a5684e7d530c264451a5cb9250f5",
            "@type": "ScholarlyArticle",
            "paperId": "0a8149fb5aa8a5684e7d530c264451a5cb9250f5",
            "corpusId": 386824,
            "url": "https://www.semanticscholar.org/paper/0a8149fb5aa8a5684e7d530c264451a5cb9250f5",
            "title": "Recent Advances in Hierarchical Reinforcement Learning",
            "venue": "Discrete event dynamic systems",
            "publicationVenue": {
                "id": "urn:research:20d20002-fb29-4a0e-84bb-89f05649d0ca",
                "name": "Discrete event dynamic systems",
                "alternate_names": [
                    "Discret Event Dyn Syst",
                    "Discret event dyn syst",
                    "Discrete Event Dynamic Systems"
                ],
                "issn": "0924-6703",
                "url": "https://link.springer.com/journal/10626"
            },
            "year": 2003,
            "externalIds": {
                "MAG": "1592847719",
                "DBLP": "journals/deds/BartoM03",
                "DOI": "10.1023/A:1022140919877",
                "CorpusId": 386824
            },
            "abstract": null,
            "referenceCount": 95,
            "citationCount": 1179,
            "influentialCitationCount": 88,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Discrete Event Dynamic Systems",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Barto2003RecentAI,\n author = {A. Barto and S. Mahadevan},\n booktitle = {Discrete event dynamic systems},\n journal = {Discrete Event Dynamic Systems},\n pages = {41-77},\n title = {Recent Advances in Hierarchical Reinforcement Learning},\n volume = {13},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c551e15f144e0549cad81729a00d4fa236b96c38",
            "@type": "ScholarlyArticle",
            "paperId": "c551e15f144e0549cad81729a00d4fa236b96c38",
            "corpusId": 519313,
            "url": "https://www.semanticscholar.org/paper/c551e15f144e0549cad81729a00d4fa236b96c38",
            "title": "Reinforcement Learning by Probability Matching",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1995,
            "externalIds": {
                "DBLP": "conf/nips/SabesJ95",
                "MAG": "2152726590",
                "CorpusId": 519313
            },
            "abstract": "We present a new algorithm for associative reinforcement learning. The algorithm is based upon the idea of matching a network's output probability with a probability distribution derived from the environment's reward signal. This Probability Matching algorithm is shown to perform faster and be less susceptible to local minima than previously existing algorithms. We use Probability Matching to train mixture of experts networks, an architecture for which other reinforcement learning rules fail to converge reliably on even simple problems. This architecture is particularly well suited for our algorithm as it can compute arbitrarily complex functions yet calculation of the output probability is simple.",
            "referenceCount": 4,
            "citationCount": 1226,
            "influentialCitationCount": 136,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1995-11-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ghahramani1995ReinforcementLB,\n author = {Zoubin Ghahramani and Michael I. Jordan},\n booktitle = {Neural Information Processing Systems},\n pages = {1080-1086},\n title = {Reinforcement Learning by Probability Matching},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:eb53b7c13156e3acacb47c1e51d93cefeabfaeb0",
            "@type": "ScholarlyArticle",
            "paperId": "eb53b7c13156e3acacb47c1e51d93cefeabfaeb0",
            "corpusId": 2198541,
            "url": "https://www.semanticscholar.org/paper/eb53b7c13156e3acacb47c1e51d93cefeabfaeb0",
            "title": "Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System",
            "venue": "Journal of Artificial Intelligence Research",
            "publicationVenue": {
                "id": "urn:research:aef12dca-60a0-4ca3-819b-cad26d309d4e",
                "name": "Journal of Artificial Intelligence Research",
                "alternate_names": [
                    "JAIR",
                    "J Artif Intell Res",
                    "The Journal of Artificial Intelligence Research"
                ],
                "issn": "1076-9757",
                "url": "http://www.jair.org/"
            },
            "year": 2011,
            "externalIds": {
                "ArXiv": "1106.0676",
                "DBLP": "journals/jair/SinghLKW02",
                "MAG": "1681299129",
                "DOI": "10.1613/jair.859",
                "CorpusId": 2198541
            },
            "abstract": "Designing the dialogue policy of a spoken dialogue system involves many nontrivial choices. This paper presents a reinforcement learning approach for automatically optimizing a dialogue policy, which addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users. We report on the design, construction and empirical evaluation of NJFun, an experimental spoken dialogue system that provides users with access to information about fun things to do in New Jersey. Our results show that by optimizing its performance via reinforcement learning, NJFun measurably improves system performance.",
            "referenceCount": 35,
            "citationCount": 417,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.jair.org/index.php/jair/article/download/10294/24560",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-06-03",
            "journal": {
                "name": "J. Artif. Intell. Res.",
                "volume": "16"
            },
            "citationStyles": {
                "bibtex": "@Article{Kearns2011OptimizingDM,\n author = {Michael Kearns and D. Litman and Satinder Singh and M. Walker},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {J. Artif. Intell. Res.},\n pages = {105-133},\n title = {Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System},\n volume = {16},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:41356b8998dd7ddf89429445320d82a269e3ab14",
            "@type": "ScholarlyArticle",
            "paperId": "41356b8998dd7ddf89429445320d82a269e3ab14",
            "corpusId": 724794,
            "url": "https://www.semanticscholar.org/paper/41356b8998dd7ddf89429445320d82a269e3ab14",
            "title": "Tree-Based Batch Mode Reinforcement Learning",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "journals/jmlr/ErnstGW05",
                "MAG": "2120346334",
                "CorpusId": 724794
            },
            "abstract": "Reinforcement learning aims to determine an optimal control policy from interaction with a system or from observations gathered from a system. In batch mode, it can be achieved by approximating the so-called Q-function based on a set of four-tuples (xt, ut , rt, xt+1) where xt denotes the system state at time t, ut the control action taken, rt the instantaneous reward obtained and xt+1 the successor state of the system, and by determining the control policy from this Q-function. The Q-function approximation may be obtained from the limit of a sequence of (batch mode) supervised learning problems. Within this framework we describe the use of several classical tree-based supervised learning methods (CART, Kd-tree, tree bagging) and two newly proposed ensemble algorithms, namely extremely and totally randomized trees. We study their performances on several examples and find that the ensemble methods based on regression trees perform well in extracting relevant information about the optimal control policy from sets of four-tuples. In particular, the totally randomized trees give good results while ensuring the convergence of the sequence, whereas by relaxing the convergence constraint even better accuracy results are provided by the extremely randomized trees.",
            "referenceCount": 47,
            "citationCount": 1129,
            "influentialCitationCount": 175,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2005-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Ernst2005TreeBasedBM,\n author = {D. Ernst and P. Geurts and L. Wehenkel},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {503-556},\n title = {Tree-Based Batch Mode Reinforcement Learning},\n volume = {6},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1e045f3447f69d9a7cac18ef23062ea8dd661285",
            "@type": "ScholarlyArticle",
            "paperId": "1e045f3447f69d9a7cac18ef23062ea8dd661285",
            "corpusId": 12063228,
            "url": "https://www.semanticscholar.org/paper/1e045f3447f69d9a7cac18ef23062ea8dd661285",
            "title": "Nonlinear Inverse Reinforcement Learning with Gaussian Processes",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "conf/nips/LevinePK11",
                "MAG": "2117675763",
                "CorpusId": 12063228
            },
            "abstract": "We present a probabilistic algorithm for nonlinear inverse reinforcement learning. The goal of inverse reinforcement learning is to learn the reward function in a Markov decision process from expert demonstrations. While most prior inverse reinforcement learning algorithms represent the reward as a linear combination of a set of features, we use Gaussian processes to learn the reward as a nonlinear function, while also determining the relevance of each feature to the expert's policy. Our probabilistic algorithm allows complex behaviors to be captured from suboptimal stochastic demonstrations, while automatically balancing the simplicity of the learned reward structure against its consistency with the observed actions.",
            "referenceCount": 16,
            "citationCount": 354,
            "influentialCitationCount": 44,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-12-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Levine2011NonlinearIR,\n author = {S. Levine and Zoran Popovic and V. Koltun},\n booktitle = {Neural Information Processing Systems},\n pages = {19-27},\n title = {Nonlinear Inverse Reinforcement Learning with Gaussian Processes},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b",
            "@type": "ScholarlyArticle",
            "paperId": "9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b",
            "corpusId": 3248358,
            "url": "https://www.semanticscholar.org/paper/9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b",
            "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1992,
            "externalIds": {
                "MAG": "2141559645",
                "DBLP": "journals/ml/Lin92",
                "DOI": "10.1007/BF00992699",
                "CorpusId": 3248358
            },
            "abstract": null,
            "referenceCount": 35,
            "citationCount": 1435,
            "influentialCitationCount": 81,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/BF00992699.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1992-05-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Lin1992SelfimprovingRA,\n author = {Longxin Lin},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {293-321},\n title = {Self-improving reactive agents based on reinforcement learning, planning and teaching},\n volume = {8},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:afaf65883ff75cc19926f61f181a687927789ad1",
            "@type": "ScholarlyArticle",
            "paperId": "afaf65883ff75cc19926f61f181a687927789ad1",
            "corpusId": 51139715,
            "url": "https://www.semanticscholar.org/paper/afaf65883ff75cc19926f61f181a687927789ad1",
            "title": "A theory of Pavlovian conditioning : Variations in the effectiveness of reinforcement and nonreinforcement",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1972,
            "externalIds": {
                "MAG": "2903532135",
                "CorpusId": 51139715
            },
            "abstract": "In several recent papers (Rescorla, 1969; Wagner, 1969a, 1969b) we have entertained similar theories of Pavlovian conditioning. The separate statements have in [act differed more in the language of their expression than in their substance. The major intent of the present paper is to explicate a more precise version of the form of theory involved, and to indicate how it may be usefully applied to a variety of phenomena in\u00b7 volving asso('iative learning. The impetus for a new theoretical model is not generally a new datum which dearly disconfirms existing theory. It is more likely to be the accumulation of a salient pattern of data, separate portions of which may be ade(luately handled by separate existing theories, but which apo pears to invite a more integrated theoretical account. Such, at least, is the better description of the background of the present work. In the sections which follow we will first describe certain data from our laboratories which exemplify the kind of observations which have encouraged the present theorizing. The theory will then be presented in sufficient detail to show how it may be applied to experimental situations involving a variety of Pavlovian conditioning arrangements. Finally, we will briefly discuss the theory in relationship to more conventional approaches.",
            "referenceCount": 10,
            "citationCount": 7555,
            "influentialCitationCount": 307,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Rescorla1972ATO,\n author = {R. Rescorla and A. R. Wagner},\n pages = {64-99},\n title = {A theory of Pavlovian conditioning : Variations in the effectiveness of reinforcement and nonreinforcement},\n year = {1972}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7bff4e9b01e9a79703ca1f3ec60cf000cd802501",
            "@type": "ScholarlyArticle",
            "paperId": "7bff4e9b01e9a79703ca1f3ec60cf000cd802501",
            "corpusId": 9640442,
            "url": "https://www.semanticscholar.org/paper/7bff4e9b01e9a79703ca1f3ec60cf000cd802501",
            "title": "Relative Entropy Inverse Reinforcement Learning",
            "venue": "International Conference on Artificial Intelligence and Statistics",
            "publicationVenue": {
                "id": "urn:research:2d136b11-c2b5-484b-b008-7f4a852fd61e",
                "name": "International Conference on Artificial Intelligence and Statistics",
                "alternate_names": [
                    "AISTATS",
                    "Int Conf Artif Intell Stat"
                ],
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2133068870",
                "DBLP": "journals/jmlr/BoulariasKP11",
                "CorpusId": 9640442
            },
            "abstract": "We consider the problem of imitation learning where the examples, demonstrated by an expert, cover only a small part of a large state space. Inverse Reinforcement Learning (IRL) provides an ecient tool for generalizing the demonstration, based on the assumption that the expert is optimally acting in a Markov Decision Process (MDP). Most of the past work on IRL requires that a (near)optimal policy can be computed for dierent reward functions. However, this requirement can hardly be satised in systems with a large, or continuous, state space. In this paper, we propose a model-free IRL algorithm, where the relative entropy between the empirical distribution of the state-action trajectories under a baseline policy and their distribution under the learned policy is minimized by stochastic gradient descent. We compare this new approach to well-known IRL algorithms using learned MDP models. Empirical results on simulated car racing, gridworld and ball-in-a-cup problems show that our approach is able to learn good policies from a small number of demonstrations.",
            "referenceCount": 15,
            "citationCount": 334,
            "influentialCitationCount": 32,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-06-14",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Boularias2011RelativeEI,\n author = {Abdeslam Boularias and J. Kober and Jan Peters},\n booktitle = {International Conference on Artificial Intelligence and Statistics},\n pages = {182-189},\n title = {Relative Entropy Inverse Reinforcement Learning},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bb0ee42d406f2361fee89cf1274073185a0e9eec",
            "@type": "ScholarlyArticle",
            "paperId": "bb0ee42d406f2361fee89cf1274073185a0e9eec",
            "corpusId": 58031572,
            "url": "https://www.semanticscholar.org/paper/bb0ee42d406f2361fee89cf1274073185a0e9eec",
            "title": "Learning agile and dynamic motor skills for legged robots",
            "venue": "Science Robotics",
            "publicationVenue": {
                "id": "urn:research:e09464be-9afe-4f80-961f-1f692839a54a",
                "name": "Science Robotics",
                "alternate_names": [
                    "Sci Robot"
                ],
                "issn": "2470-9476",
                "url": "http://www.sciencemag.org/journals/robotics"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1901.08652",
                "DBLP": "journals/scirobotics/LeeDBTKH19",
                "MAG": "3105522894",
                "DOI": "10.1126/scirobotics.aau5872",
                "CorpusId": 58031572,
                "PubMed": "33137755"
            },
            "abstract": "A method for learning agile control policies uses simulated data to enable precise, efficient movements in a complex physical robot. Legged robots pose one of the greatest challenges in robotics. Dynamic and agile maneuvers of animals cannot be imitated by existing methods that are crafted by humans. A compelling alternative is reinforcement learning, which requires minimal craftsmanship and promotes the natural evolution of a control policy. However, so far, reinforcement learning research for legged robots is mainly limited to simulation, and only few and comparably simple examples have been deployed on real systems. The primary reason is that training with real robots, particularly with dynamically balancing systems, is complicated and expensive. In the present work, we introduce a method for training a neural network policy in simulation and transferring it to a state-of-the-art legged system, thereby leveraging fast, automated, and cost-effective data generation schemes. The approach is applied to the ANYmal robot, a sophisticated medium-dog\u2013sized quadrupedal system. Using policies trained in simulation, the quadrupedal machine achieves locomotion skills that go beyond what had been achieved with prior methods: ANYmal is capable of precisely and energy-efficiently following high-level body velocity commands, running faster than before, and recovering from falling even in complex configurations.",
            "referenceCount": 67,
            "citationCount": 869,
            "influentialCitationCount": 57,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://robotics.sciencemag.org/content/robotics/4/26/eaau5872.full.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-01-16",
            "journal": {
                "name": "Science Robotics",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Hwangbo2019LearningAA,\n author = {Jemin Hwangbo and Joonho Lee and A. Dosovitskiy and Dario Bellicoso and Vassilios Tsounis and V. Koltun and M. Hutter},\n booktitle = {Science Robotics},\n journal = {Science Robotics},\n title = {Learning agile and dynamic motor skills for legged robots},\n volume = {4},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d37a34c204a8beefcaef4dddddb7a90c16e973d4",
            "@type": "ScholarlyArticle",
            "paperId": "d37a34c204a8beefcaef4dddddb7a90c16e973d4",
            "corpusId": 51894399,
            "url": "https://www.semanticscholar.org/paper/d37a34c204a8beefcaef4dddddb7a90c16e973d4",
            "title": "Learning dexterous in-hand manipulation",
            "venue": "Int. J. Robotics Res.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2990747716",
                "DBLP": "journals/ijrr/OpenAI20",
                "ArXiv": "1808.00177",
                "DOI": "10.1177/0278364919887447",
                "CorpusId": 51894399
            },
            "abstract": "We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies that can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system such as friction coefficients and an object\u2019s appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM.",
            "referenceCount": 76,
            "citationCount": 1465,
            "influentialCitationCount": 81,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journals.sagepub.com/doi/pdf/10.1177/0278364919887447",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-08-01",
            "journal": {
                "name": "The International Journal of Robotics Research",
                "volume": "39"
            },
            "citationStyles": {
                "bibtex": "@Article{Andrychowicz2018LearningDI,\n author = {Marcin Andrychowicz and Bowen Baker and Maciek Chociej and R. J\u00f3zefowicz and Bob McGrew and J. Pachocki and Arthur Petron and Matthias Plappert and Glenn Powell and Alex Ray and Jonas Schneider and Szymon Sidor and Joshua Tobin and P. Welinder and Lilian Weng and Wojciech Zaremba},\n booktitle = {Int. J. Robotics Res.},\n journal = {The International Journal of Robotics Research},\n pages = {20 - 3},\n title = {Learning dexterous in-hand manipulation},\n volume = {39},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c5ffd0bdbecbeee18232676d2e272ba24b025797",
            "@type": "ScholarlyArticle",
            "paperId": "c5ffd0bdbecbeee18232676d2e272ba24b025797",
            "corpusId": 13201358,
            "url": "https://www.semanticscholar.org/paper/c5ffd0bdbecbeee18232676d2e272ba24b025797",
            "title": "Empirical evaluation methods for multiobjective reinforcement learning algorithms",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "journals/ml/VamplewDBID11",
                "MAG": "2012612381",
                "DOI": "10.1007/s10994-010-5232-5",
                "CorpusId": 13201358
            },
            "abstract": null,
            "referenceCount": 39,
            "citationCount": 256,
            "influentialCitationCount": 36,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10994-010-5232-5.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-07-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "84"
            },
            "citationStyles": {
                "bibtex": "@Article{Vamplew2011EmpiricalEM,\n author = {P. Vamplew and Richard Dazeley and Adam Berry and Rustam Issabekov and Evan Dekker},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {51-80},\n title = {Empirical evaluation methods for multiobjective reinforcement learning algorithms},\n volume = {84},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f52fb7ca4bddd1d9479ebe5a8c9b83469c812869",
            "@type": "ScholarlyArticle",
            "paperId": "f52fb7ca4bddd1d9479ebe5a8c9b83469c812869",
            "corpusId": 10352232,
            "url": "https://www.semanticscholar.org/paper/f52fb7ca4bddd1d9479ebe5a8c9b83469c812869",
            "title": "Reinforcement Learning With Function Approximation for Traffic Signal Control",
            "venue": "IEEE transactions on intelligent transportation systems (Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2125001944",
                "DBLP": "journals/tits/PrashanthB11",
                "DOI": "10.1109/TITS.2010.2091408",
                "CorpusId": 10352232
            },
            "abstract": "We propose, for the first time, a reinforcement learning (RL) algorithm with function approximation for traffic signal control. Our algorithm incorporates state-action features and is easily implementable in high-dimensional settings. Prior work, e.g., the work of Abdulhai , on the application of RL to traffic signal control requires full-state representations and cannot be implemented, even in moderate-sized road networks, because the computational complexity exponentially grows in the numbers of lanes and junctions. We tackle this problem of the curse of dimensionality by effectively using feature-based state representations that use a broad characterization of the level of congestion as low, medium, or high. One advantage of our algorithm is that, unlike prior work based on RL, it does not require precise information on queue lengths and elapsed times at each lane but instead works with the aforementioned described features. The number of features that our algorithm requires is linear to the number of signaled lanes, thereby leading to several orders of magnitude reduction in the computational complexity. We perform implementations of our algorithm on various settings and show performance comparisons with other algorithms in the literature, including the works of Abdulhai and Cools , as well as the fixed-timing and the longest queue algorithms. For comparison, we also develop an RL algorithm that uses full-state representation and incorporates prioritization of traffic, unlike the work of Abdulhai We observe that our algorithm outperforms all the other algorithms on all the road network settings that we consider.",
            "referenceCount": 32,
            "citationCount": 267,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-06-01",
            "journal": {
                "name": "IEEE Transactions on Intelligent Transportation Systems",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{PrashanthL.2011ReinforcementLW,\n author = {A. PrashanthL. and S. Bhatnagar},\n booktitle = {IEEE transactions on intelligent transportation systems (Print)},\n journal = {IEEE Transactions on Intelligent Transportation Systems},\n pages = {412-421},\n title = {Reinforcement Learning With Function Approximation for Traffic Signal Control},\n volume = {12},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:28cbc3c1a89dc07e4a79999b7661fece4700613a",
            "@type": "ScholarlyArticle",
            "paperId": "28cbc3c1a89dc07e4a79999b7661fece4700613a",
            "corpusId": 15020439,
            "url": "https://www.semanticscholar.org/paper/28cbc3c1a89dc07e4a79999b7661fece4700613a",
            "title": "Dopamine-Mediated Reinforcement Learning Signals in the Striatum and Ventromedial Prefrontal Cortex Underlie Value-Based Choices",
            "venue": "Journal of Neuroscience",
            "publicationVenue": {
                "id": "urn:research:52edfc09-8d65-4876-8560-84530d7259b9",
                "name": "Journal of Neuroscience",
                "alternate_names": [
                    "The Journal of Neuroscience",
                    "J Neurosci"
                ],
                "issn": "0270-6474",
                "url": "https://www.jneurosci.org/"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2041053327",
                "DOI": "10.1523/JNEUROSCI.3904-10.2011",
                "CorpusId": 15020439,
                "PubMed": "21289169"
            },
            "abstract": "A large body of evidence exists on the role of dopamine in reinforcement learning. Less is known about how dopamine shapes the relative impact of positive and negative outcomes to guide value-based choices. We combined administration of the dopamine D2 receptor antagonist amisulpride with functional magnetic resonance imaging in healthy human volunteers. Amisulpride did not affect initial reinforcement learning. However, in a later transfer phase that involved novel choice situations requiring decisions between two symbols based on their previously learned values, amisulpride improved participants' ability to select the better of two highly rewarding options, while it had no effect on choices between two very poor options. During the learning phase, activity in the striatum encoded a reward prediction error. In the transfer phase, in the absence of any outcome, ventromedial prefrontal cortex (vmPFC) continually tracked the learned value of the available options on each trial. Both striatal prediction error coding and tracking of learned value in the vmPFC were predictive of subjects' choice performance in the transfer phase, and both were enhanced under amisulpride. These findings show that dopamine-dependent mechanisms enhance reinforcement learning signals in the striatum and sharpen representations of associative values in prefrontal cortex that are used to guide reinforcement-based decisions.",
            "referenceCount": 40,
            "citationCount": 245,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.jneurosci.org/content/jneuro/31/5/1606.full.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-02-02",
            "journal": {
                "name": "The Journal of Neuroscience",
                "volume": "31"
            },
            "citationStyles": {
                "bibtex": "@Article{Jocham2011DopamineMediatedRL,\n author = {G. Jocham and T. A. Klein and M. Ullsperger},\n booktitle = {Journal of Neuroscience},\n journal = {The Journal of Neuroscience},\n pages = {1606 - 1613},\n title = {Dopamine-Mediated Reinforcement Learning Signals in the Striatum and Ventromedial Prefrontal Cortex Underlie Value-Based Choices},\n volume = {31},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4f185ec16ce9c4e2d01d4acb0f9b46fe91b1b1eb",
            "@type": "ScholarlyArticle",
            "paperId": "4f185ec16ce9c4e2d01d4acb0f9b46fe91b1b1eb",
            "corpusId": 16758749,
            "url": "https://www.semanticscholar.org/paper/4f185ec16ce9c4e2d01d4acb0f9b46fe91b1b1eb",
            "title": "Learning to Control a Low-Cost Manipulator using Data-Efficient Reinforcement Learning",
            "venue": "Robotics: Science and Systems",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "1520597402",
                "DBLP": "conf/rss/DeisenrothRF11",
                "DOI": "10.15607/RSS.2011.VII.008",
                "CorpusId": 16758749
            },
            "abstract": "Over the last years, there has been substantial progress in robust manipulation in unstructured environments. The long-term goal of our work is to get away from precise, but very expensive robotic systems and to develop affordable, potentially imprecise, self-adaptive manipulator systems that can interactively perform tasks such as playing with children. In this paper, we demonstrate how a low-cost off-the-shelf robotic system can learn closed-loop policies for a stacking task in only a handful of trials-from scratch. Our manipulator is inaccurate and provides no pose feedback. For learning a controller in the work space of a Kinect-style depth camera, we use a model-based reinforcement learning technique. Our learning method is data efficient, reduces model bias, and deals with several noise sources in a principled way during long-term planning. We present a way of incorporating state-space constraints into the learning process and analyze the learning gain by exploiting the sequential structure of the stacking task.",
            "referenceCount": 39,
            "citationCount": 225,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-06-27",
            "journal": {
                "name": "",
                "volume": "07"
            },
            "citationStyles": {
                "bibtex": "@Article{Deisenroth2011LearningTC,\n author = {M. Deisenroth and C. Rasmussen and D. Fox},\n booktitle = {Robotics: Science and Systems},\n title = {Learning to Control a Low-Cost Manipulator using Data-Efficient Reinforcement Learning},\n volume = {07},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:50e48d011106c6ee94c04d8522fa1e97e2d8f5b6",
            "@type": "ScholarlyArticle",
            "paperId": "50e48d011106c6ee94c04d8522fa1e97e2d8f5b6",
            "corpusId": 2294756,
            "url": "https://www.semanticscholar.org/paper/50e48d011106c6ee94c04d8522fa1e97e2d8f5b6",
            "title": "Behavioral and Neural Properties of Social Reinforcement Learning",
            "venue": "Journal of Neuroscience",
            "publicationVenue": {
                "id": "urn:research:52edfc09-8d65-4876-8560-84530d7259b9",
                "name": "Journal of Neuroscience",
                "alternate_names": [
                    "The Journal of Neuroscience",
                    "J Neurosci"
                ],
                "issn": "0270-6474",
                "url": "https://www.jneurosci.org/"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2113411523",
                "DOI": "10.1523/JNEUROSCI.2972-11.2011",
                "CorpusId": 2294756,
                "PubMed": "21917787"
            },
            "abstract": "Social learning is critical for engaging in complex interactions with other individuals. Learning from positive social exchanges, such as acceptance from peers, may be similar to basic reinforcement learning. We formally test this hypothesis by developing a novel paradigm that is based on work in nonhuman primates and human imaging studies of reinforcement learning. The probability of receiving positive social reinforcement from three distinct peers was parametrically manipulated while brain activity was recorded in healthy adults using event-related functional magnetic resonance imaging. Over the course of the experiment, participants responded more quickly to faces of peers who provided more frequent positive social reinforcement, and rated them as more likeable. Modeling trial-by-trial learning showed ventral striatum and orbital frontal cortex activity correlated positively with forming expectations about receiving social reinforcement. Rostral anterior cingulate cortex activity tracked positively with modulations of expected value of the cues (peers). Together, the findings across three levels of analysis\u2014social preferences, response latencies, and modeling neural responses\u2014are consistent with reinforcement learning theory and nonhuman primate electrophysiological studies of reward. This work highlights the fundamental influence of acceptance by one's peers in altering subsequent behavior.",
            "referenceCount": 55,
            "citationCount": 154,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.jneurosci.org/content/jneuro/31/37/13039.full.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-09-14",
            "journal": {
                "name": "The Journal of Neuroscience",
                "volume": "31"
            },
            "citationStyles": {
                "bibtex": "@Article{Jones2011BehavioralAN,\n author = {R. Jones and L. Somerville and Jian Li and Erika J. Ruberry and Victoria Libby and G. Glover and H. Voss and D. Ballon and B. Casey},\n booktitle = {Journal of Neuroscience},\n journal = {The Journal of Neuroscience},\n pages = {13039 - 13045},\n title = {Behavioral and Neural Properties of Social Reinforcement Learning},\n volume = {31},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e84a5549b2a985bd1bc0c3535de96c448b8b8cb",
            "@type": "ScholarlyArticle",
            "paperId": "5e84a5549b2a985bd1bc0c3535de96c448b8b8cb",
            "corpusId": 207211627,
            "url": "https://www.semanticscholar.org/paper/5e84a5549b2a985bd1bc0c3535de96c448b8b8cb",
            "title": "Reinforcement learning in feedback control",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "journals/ml/HafnerR11",
                "MAG": "1825869920",
                "DOI": "10.1007/s10994-011-5235-x",
                "CorpusId": 207211627
            },
            "abstract": null,
            "referenceCount": 49,
            "citationCount": 183,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10994-011-5235-x.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-07-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "84"
            },
            "citationStyles": {
                "bibtex": "@Article{Hafner2011ReinforcementLI,\n author = {Roland Hafner and Martin A. Riedmiller},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {137-169},\n title = {Reinforcement learning in feedback control},\n volume = {84},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:aaab75af076bbdbd4d9c5cd7a2c332b03f9b58d4",
            "@type": "ScholarlyArticle",
            "paperId": "aaab75af076bbdbd4d9c5cd7a2c332b03f9b58d4",
            "corpusId": 10843053,
            "url": "https://www.semanticscholar.org/paper/aaab75af076bbdbd4d9c5cd7a2c332b03f9b58d4",
            "title": "Integrating reinforcement learning with human demonstrations of varying ability",
            "venue": "Adaptive Agents and Multi-Agent Systems",
            "publicationVenue": {
                "id": "urn:research:6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                "name": "Adaptive Agents and Multi-Agent Systems",
                "alternate_names": [
                    "Adapt Agent Multi-agent Syst",
                    "International Joint Conference on Autonomous Agents & Multiagent Systems",
                    "Adapt Agent Multi-agents Syst",
                    "AAMAS",
                    "Adaptive Agents and Multi-Agents Systems",
                    "Int Jt Conf Auton Agent  Multiagent Syst"
                ],
                "issn": null,
                "url": "http://www.ifaamas.org/"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2137375617",
                "DBLP": "conf/atal/TaylorSC11",
                "CorpusId": 10843053
            },
            "abstract": "This work introduces Human-Agent Transfer (HAT), an algorithm that combines transfer learning, learning from demonstration and reinforcement learning to achieve rapid learning and high performance in complex domains. Using experiments in a simulated robot soccer domain, we show that human demonstrations transferred into a baseline policy for an agent and refined using reinforcement learning significantly improve both learning time and policy performance. Our evaluation compares three algorithmic approaches to incorporating demonstration rule summaries into transfer learning, and studies the impact of demonstration quality and quantity, as well as the effect of combining demonstrations from multiple teachers. Our results show that all three transfer methods lead to statistically significant improvement in performance over learning without demonstration. The best performance was achieved by combining the best demonstrations from two teachers.",
            "referenceCount": 41,
            "citationCount": 153,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-05-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Taylor2011IntegratingRL,\n author = {Matthew E. Taylor and Halit Bener Suay and S. Chernova},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {617-624},\n title = {Integrating reinforcement learning with human demonstrations of varying ability},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:56580ccd71ec710f9c6bbbf4b22f86d1c3ea3256",
            "@type": "ScholarlyArticle",
            "paperId": "56580ccd71ec710f9c6bbbf4b22f86d1c3ea3256",
            "corpusId": 7209531,
            "url": "https://www.semanticscholar.org/paper/56580ccd71ec710f9c6bbbf4b22f86d1c3ea3256",
            "title": "Reinforcement Learning Strategies for Clinical Trials in Nonsmall Cell Lung Cancer",
            "venue": "Biometrics",
            "publicationVenue": {
                "id": "urn:research:777d7a41-df2c-4867-a5ec-63920572d28f",
                "name": "Biometrics",
                "alternate_names": null,
                "issn": "0006-341X",
                "url": "http://www3.interscience.wiley.com/journal/118538342/home"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2049655669",
                "DOI": "10.1111/j.1541-0420.2011.01572.x",
                "CorpusId": 7209531,
                "PubMed": "21385164"
            },
            "abstract": "Summary Typical regimens for advanced metastatic stage IIIB/IV nonsmall cell lung cancer (NSCLC) consist of multiple lines of treatment. We present an adaptive reinforcement learning approach to discover optimal individualized treatment regimens from a specially designed clinical trial (a \u201cclinical reinforcement trial\u201d) of an experimental treatment for patients with advanced NSCLC who have not been treated previously with systemic therapy. In addition to the complexity of the problem of selecting optimal compounds for first\u2010 and second\u2010line treatments based on prognostic factors, another primary goal is to determine the optimal time to initiate second\u2010line therapy, either immediately or delayed after induction therapy, yielding the longest overall survival time. A reinforcement learning method called\u2002Q\u2010learning is utilized, which involves learning an optimal regimen from patient data generated from the clinical reinforcement trial. Approximating the\u2002Q\u2010function with time\u2010indexed parameters can be achieved by using a modification of support vector regression that can utilize censored data. Within this framework, a simulation study shows that the procedure can extract optimal regimens for two lines of treatment directly from clinical data without prior knowledge of the treatment effect mechanism. In addition, we demonstrate that the design reliably selects the best initial time for second\u2010line therapy while taking into account the heterogeneity of NSCLC across patients.",
            "referenceCount": 23,
            "citationCount": 188,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc3138840?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-12-01",
            "journal": {
                "name": "Biometrics",
                "volume": "67"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhao2011ReinforcementLS,\n author = {Yufan Zhao and D. Zeng and M. Socinski and M. Kosorok},\n booktitle = {Biometrics},\n journal = {Biometrics},\n title = {Reinforcement Learning Strategies for Clinical Trials in Nonsmall Cell Lung Cancer},\n volume = {67},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:282001869bd502c7917db8b32b75593addfbbc68",
            "@type": "ScholarlyArticle",
            "paperId": "282001869bd502c7917db8b32b75593addfbbc68",
            "corpusId": 6921329,
            "url": "https://www.semanticscholar.org/paper/282001869bd502c7917db8b32b75593addfbbc68",
            "title": "Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method",
            "venue": "European Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:f9a81beb-9bcb-43bc-96a7-67cf23dba2d6",
                "name": "European Conference on Machine Learning",
                "alternate_names": [
                    "Eur Conf Mach Learn",
                    "European conference on Machine Learning",
                    "Eur conf Mach Learn",
                    "ECML"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/ecml"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "166862392",
                "DBLP": "conf/ecml/Riedmiller05",
                "DOI": "10.1007/11564096_32",
                "CorpusId": 6921329
            },
            "abstract": null,
            "referenceCount": 10,
            "citationCount": 986,
            "influentialCitationCount": 140,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/11564096_32.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2005-10-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Riedmiller2005NeuralFQ,\n author = {Martin A. Riedmiller},\n booktitle = {European Conference on Machine Learning},\n pages = {317-328},\n title = {Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0cc956565c7d249d4197eeb1dbab6523c648b2c9",
            "@type": "ScholarlyArticle",
            "paperId": "0cc956565c7d249d4197eeb1dbab6523c648b2c9",
            "corpusId": 208547755,
            "url": "https://www.semanticscholar.org/paper/0cc956565c7d249d4197eeb1dbab6523c648b2c9",
            "title": "Dream to Control: Learning Behaviors by Latent Imagination",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2995298643",
                "ArXiv": "1912.01603",
                "DBLP": "conf/iclr/HafnerLB020",
                "CorpusId": 208547755
            },
            "abstract": "To select effective actions in complex environments, intelligent agents need to generalize from past experience. World models can represent knowledge about the environment to facilitate such generalization. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks purely by latent imagination. We efficiently learn behaviors by backpropagating analytic gradients of learned state values through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.",
            "referenceCount": 71,
            "citationCount": 811,
            "influentialCitationCount": 161,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-12-03",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1912.01603"
            },
            "citationStyles": {
                "bibtex": "@Article{Hafner2019DreamTC,\n author = {Danijar Hafner and T. Lillicrap and Jimmy Ba and Mohammad Norouzi},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Dream to Control: Learning Behaviors by Latent Imagination},\n volume = {abs/1912.01603},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f7e59546d75b2fe71d1fdda2773f84bb04fcc6d2",
            "@type": "ScholarlyArticle",
            "paperId": "f7e59546d75b2fe71d1fdda2773f84bb04fcc6d2",
            "corpusId": 1797388,
            "url": "https://www.semanticscholar.org/paper/f7e59546d75b2fe71d1fdda2773f84bb04fcc6d2",
            "title": "A Generalized Path Integral Control Approach to Reinforcement Learning",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2182445582",
                "DBLP": "journals/jmlr/TheodorouBS10",
                "DOI": "10.5555/1756006.1953033",
                "CorpusId": 1797388
            },
            "abstract": "With the goal to generate more scalable algorithms with higher efficiency and fewer open parameters, reinforcement learning (RL) has recently moved towards combining classical techniques from optimal control and dynamic programming with modern learning techniques from statistical estimation theory. In this vein, this paper suggests to use the framework of stochastic optimal control with path integrals to derive a novel approach to RL with parameterized policies. While solidly grounded in value function estimation and optimal control based on the stochastic Hamilton-Jacobi-Bellman (HJB) equations, policy improvements can be transformed into an approximation problem of a path integral which has no open algorithmic parameters other than the exploration noise. The resulting algorithm can be conceived of as model-based, semi-model-based, or even model free, depending on how the learning problem is structured. The update equations have no danger of numerical instabilities as neither matrix inversions nor gradient learning rates are required. Our new algorithm demonstrates interesting similarities with previous RL research in the framework of probability matching and provides intuition why the slightly heuristically motivated probability matching approach can actually perform well. Empirical evaluations demonstrate significant performance improvements over gradient-based policy learning and scalability to high-dimensional control problems. Finally, a learning experiment on a simulated 12 degree-of-freedom robot dog illustrates the functionality of our algorithm in a complex robot learning scenario. We believe that Policy Improvement with Path Integrals (PI2) offers currently one of the most efficient, numerically robust, and easy to implement algorithms for RL based on trajectory roll-outs.",
            "referenceCount": 46,
            "citationCount": 598,
            "influentialCitationCount": 64,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-03-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Theodorou2010AGP,\n author = {Evangelos A. Theodorou and J. Buchli and S. Schaal},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {3137-3181},\n title = {A Generalized Path Integral Control Approach to Reinforcement Learning},\n volume = {11},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cd49acefc8d51e324aa562e5337e1c2aff067053",
            "@type": "ScholarlyArticle",
            "paperId": "cd49acefc8d51e324aa562e5337e1c2aff067053",
            "corpusId": 90063862,
            "url": "https://www.semanticscholar.org/paper/cd49acefc8d51e324aa562e5337e1c2aff067053",
            "title": "An Overview of Multi-task Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2753709519",
                "DOI": "10.1093/NSR/NWX105",
                "CorpusId": 90063862
            },
            "abstract": "As a promising area in machine learning, multi-task learning (MTL) aims to improve the performance of multiple related learning tasks by leveraging useful information among them. In this paper, we give an overview of MTL by first giving a definition of MTL. Then several different settings of MTL are introduced, including multi-task supervised learning, multi-task unsupervised learning, multi-task semi-supervised learning, multi-task active learning, multi-task reinforcement learning, multi-task online learning and multi-task multi-view learning. For each setting, representative MTL models are presented. In order to speed up the learning process, parallel and distributed MTL models are introduced. Many areas, including computer vision, bioinformatics, health informatics, speech, natural language processing, web applications and ubiquitous computing, use MTL to improve the performance of the applications involved and some representative works are reviewed. Finally, recent theoretical analyses for MTL are presented.",
            "referenceCount": 136,
            "citationCount": 1311,
            "influentialCitationCount": 31,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://academic.oup.com/nsr/article-pdf/5/1/30/24164435/nwx105.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "National Science Review",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2018AnOO,\n author = {Yu Zhang and Qiang Yang},\n journal = {National Science Review},\n pages = {30-43},\n title = {An Overview of Multi-task Learning},\n volume = {5},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f1c72d92275e460d142db923faa6a9ecbb0a8346",
            "@type": "ScholarlyArticle",
            "paperId": "f1c72d92275e460d142db923faa6a9ecbb0a8346",
            "corpusId": 1104128,
            "url": "https://www.semanticscholar.org/paper/f1c72d92275e460d142db923faa6a9ecbb0a8346",
            "title": "Effect of human guidance and state space size on Interactive Reinforcement Learning",
            "venue": "IEEE International Symposium on Robot and Human Interactive Communication",
            "publicationVenue": {
                "id": "urn:research:4076610c-1337-43ac-8695-42d7e74ee8f0",
                "name": "IEEE International Symposium on Robot and Human Interactive Communication",
                "alternate_names": [
                    "Robot and Human Interactive Communication",
                    "IEEE Int Symp Robot Hum Interact Commun",
                    "Robot Hum Interact Commun",
                    "RO-MAN"
                ],
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2129659607",
                "DBLP": "conf/ro-man/SuayC11",
                "DOI": "10.1109/ROMAN.2011.6005223",
                "CorpusId": 1104128
            },
            "abstract": "The Interactive Reinforcement Learning algorithm enables a human user to train a robot by providing rewards in response to past actions and anticipatory guidance to guide the selection of future actions. Past work with software agents has shown that incorporating user guidance into the policy learning process through Interactive Reinforcement Learning significantly improves the policy learning time by reducing the number of states the agent explores. We present the first study of Interactive Reinforcement Learning in real-world robotic systems. We report on four experiments that study the effects that teacher guidance and state space size have on policy learning performance. We discuss modifications made to apply Interactive Reinforcement Learning to a real-world system and show that guidance significantly reduces the learning rate, and that its positive effects increase with state space size.",
            "referenceCount": 22,
            "citationCount": 110,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-08-30",
            "journal": {
                "name": "2011 RO-MAN",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Suay2011EffectOH,\n author = {Halit Bener Suay and S. Chernova},\n booktitle = {IEEE International Symposium on Robot and Human Interactive Communication},\n journal = {2011 RO-MAN},\n pages = {1-6},\n title = {Effect of human guidance and state space size on Interactive Reinforcement Learning},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2832e4cdb1f927dca50422ff010bbd2a9934292e",
            "@type": "ScholarlyArticle",
            "paperId": "2832e4cdb1f927dca50422ff010bbd2a9934292e",
            "corpusId": 2298000,
            "url": "https://www.semanticscholar.org/paper/2832e4cdb1f927dca50422ff010bbd2a9934292e",
            "title": "Bayesian Multitask Inverse Reinforcement Learning",
            "venue": "European Workshop on Reinforcement Learning",
            "publicationVenue": {
                "id": "urn:research:6795cf27-8e9b-4c0f-9853-4b02b9428e13",
                "name": "European Workshop on Reinforcement Learning",
                "alternate_names": [
                    "Eur Workshop Reinf Learn",
                    "EWRL"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=998"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "1567876833",
                "DBLP": "conf/ewrl/DimitrakakisR11",
                "ArXiv": "1106.3655",
                "DOI": "10.1007/978-3-642-29946-9_27",
                "CorpusId": 2298000
            },
            "abstract": null,
            "referenceCount": 24,
            "citationCount": 104,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1106.3655",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-06-18",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dimitrakakis2011BayesianMI,\n author = {Christos Dimitrakakis and C. Rothkopf},\n booktitle = {European Workshop on Reinforcement Learning},\n pages = {273-284},\n title = {Bayesian Multitask Inverse Reinforcement Learning},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7a9487a57df4e47bd8a224d564373934fa63db0a",
            "@type": "ScholarlyArticle",
            "paperId": "7a9487a57df4e47bd8a224d564373934fa63db0a",
            "corpusId": 1545761,
            "url": "https://www.semanticscholar.org/paper/7a9487a57df4e47bd8a224d564373934fa63db0a",
            "title": "Accelerating Reinforcement Learning by Composing Solutions of Automatically Identified Subtasks",
            "venue": "Journal of Artificial Intelligence Research",
            "publicationVenue": {
                "id": "urn:research:aef12dca-60a0-4ca3-819b-cad26d309d4e",
                "name": "Journal of Artificial Intelligence Research",
                "alternate_names": [
                    "JAIR",
                    "J Artif Intell Res",
                    "The Journal of Artificial Intelligence Research"
                ],
                "issn": "1076-9757",
                "url": "http://www.jair.org/"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "1799762961",
                "ArXiv": "1106.1796",
                "DBLP": "journals/corr/abs-1106-1796",
                "DOI": "10.1613/jair.904",
                "CorpusId": 1545761
            },
            "abstract": "This paper discusses a system that accelerates reinforcement learning by using transfer from related tasks. Without such transfer, even if two tasks are very similar at some abstract level, an extensive re-learning effort is required. The system achieves much of its power by transferring parts of previously learned solutions rather than a single complete solution. The system exploits strong features in the multi-dimensional function produced by reinforcement learning in solving a particular task. These features are stable and easy to recognize early in the learning process. They generate a partitioning of the state space and thus the function. The partition is represented as a graph. This is used to index and compose functions stored in a case base to form a close approximation to the solution of the new task. Experiments demonstrate that function composition often produces more than an order of magnitude increase in learning rate compared to a basic reinforcement learning algorithm.",
            "referenceCount": 47,
            "citationCount": 104,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.jair.org/index.php/jair/article/download/10295/24564",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-06-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1106.1796"
            },
            "citationStyles": {
                "bibtex": "@Article{Drummond2011AcceleratingRL,\n author = {C. Drummond},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {ArXiv},\n title = {Accelerating Reinforcement Learning by Composing Solutions of Automatically Identified Subtasks},\n volume = {abs/1106.1796},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:625ea3b82f9c22d75b0113397620c5e1e7e06e68",
            "@type": "ScholarlyArticle",
            "paperId": "625ea3b82f9c22d75b0113397620c5e1e7e06e68",
            "corpusId": 18423456,
            "url": "https://www.semanticscholar.org/paper/625ea3b82f9c22d75b0113397620c5e1e7e06e68",
            "title": "Reinforcement learning model, algorithms and its application",
            "venue": "International Conference on Mechatronic Science, Electric Engineering and Computer",
            "publicationVenue": {
                "id": "urn:research:e8d21017-45c4-4ee8-928e-c64ab1940b0c",
                "name": "International Conference on Mechatronic Science, Electric Engineering and Computer",
                "alternate_names": [
                    "MEC",
                    "Int Conf Mechatron Sci Electr Eng Comput"
                ],
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2130711276",
                "DOI": "10.1109/MEC.2011.6025669",
                "CorpusId": 18423456
            },
            "abstract": "Reinforcement learning comes from the animal learning theory. RL does not need prior knowledge, it can autonomously get optional policy with the knowledge obtained by trial-and-error and continuously interacting with dynamic environment. Its characteristics of self improving and online learning make reinforcement learning become one of intelligent agent's core technologies. In this paper, we firstly survey the model and theory of reinforcement learning. Then, we roundly present the main reinforcement learning algorithms, including Sarsa, temporal difference, Q-learning and function approximation. Finally, we briefly introduce some applications of reinforcement learning and point out some future research directions of reinforcement learning.",
            "referenceCount": 19,
            "citationCount": 90,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference",
                "Review"
            ],
            "publicationDate": "2011-09-23",
            "journal": {
                "name": "2011 International Conference on Mechatronic Science, Electric Engineering and Computer (MEC)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Conference{Qiang2011ReinforcementLM,\n author = {Wang Qiang and Zhongli Zhan},\n booktitle = {International Conference on Mechatronic Science, Electric Engineering and Computer},\n journal = {2011 International Conference on Mechatronic Science, Electric Engineering and Computer (MEC)},\n pages = {1143-1146},\n title = {Reinforcement learning model, algorithms and its application},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:24e5346ca36abd2eeb47b21e223f493f3d4b722b",
            "@type": "ScholarlyArticle",
            "paperId": "24e5346ca36abd2eeb47b21e223f493f3d4b722b",
            "corpusId": 6231557,
            "url": "https://www.semanticscholar.org/paper/24e5346ca36abd2eeb47b21e223f493f3d4b722b",
            "title": "Protecting against evaluation overfitting in empirical reinforcement learning",
            "venue": "IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning",
            "publicationVenue": {
                "id": "urn:research:1a13ae1e-223b-4ccd-821b-6a9e430564dc",
                "name": "IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning",
                "alternate_names": [
                    "ADPRL",
                    "IEEE Symp Adapt Dyn Program Reinf Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2104228245",
                "DBLP": "conf/adprl/WhitesonTTS11",
                "DOI": "10.1109/ADPRL.2011.5967363",
                "CorpusId": 6231557
            },
            "abstract": "Empirical evaluations play an important role in machine learning. However, the usefulness of any evaluation depends on the empirical methodology employed. Designing good empirical methodologies is difficult in part because agents can overfit test evaluations and thereby obtain misleadingly high scores. We argue that reinforcement learning is particularly vulnerable to environment overfitting and propose as a remedy generalized methodologies, in which evaluations are based on multiple environments sampled from a distribution. In addition, we consider how to summarize performance when scores from different environments may not have commensurate values. Finally, we present proof-of-concept results demonstrating how these methodologies can validate an intuitively useful range-adaptive tile coding method.",
            "referenceCount": 33,
            "citationCount": 99,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-04-01",
            "journal": {
                "name": "2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Whiteson2011ProtectingAE,\n author = {Shimon Whiteson and B. Tanner and Matthew E. Taylor and P. Stone},\n booktitle = {IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning},\n journal = {2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)},\n pages = {120-127},\n title = {Protecting against evaluation overfitting in empirical reinforcement learning},\n year = {2011}\n}\n"
            }
        }
    }
]