[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f518bffb712a298bff18248c67f6fc0181018ae6",
            "@type": "ScholarlyArticle",
            "paperId": "f518bffb712a298bff18248c67f6fc0181018ae6",
            "corpusId": 621595,
            "url": "https://www.semanticscholar.org/paper/f518bffb712a298bff18248c67f6fc0181018ae6",
            "title": "Residual Algorithms: Reinforcement Learning with Function Approximation",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1995,
            "externalIds": {
                "MAG": "1646707810",
                "DBLP": "conf/icml/Baird95",
                "DOI": "10.1016/b978-1-55860-377-6.50013-x",
                "CorpusId": 621595
            },
            "abstract": null,
            "referenceCount": 14,
            "citationCount": 1171,
            "influentialCitationCount": 161,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.utsa.edu/~bylander/cs6243/baird95residual.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1995-07-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Baird1995ResidualAR,\n author = {L. Baird},\n booktitle = {International Conference on Machine Learning},\n pages = {30-37},\n title = {Residual Algorithms: Reinforcement Learning with Function Approximation},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:70c31a7e1240413fcd26670a551228a244a63791",
            "@type": "ScholarlyArticle",
            "paperId": "70c31a7e1240413fcd26670a551228a244a63791",
            "corpusId": 9140802,
            "url": "https://www.semanticscholar.org/paper/70c31a7e1240413fcd26670a551228a244a63791",
            "title": "Hierarchical reinforcement learning with movement primitives",
            "venue": "IEEE-RAS International Conference on Humanoid Robots",
            "publicationVenue": {
                "id": "urn:research:4c11f8e3-7783-495c-9c2a-91746ade3544",
                "name": "IEEE-RAS International Conference on Humanoid Robots",
                "alternate_names": [
                    "IEEE-RAS Int Conf Humanoid Robot",
                    "Humanoids"
                ],
                "issn": null,
                "url": "https://www.ieee-ras.org/conferences-workshops/fully-sponsored/humanoids"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2071444114",
                "DBLP": "conf/humanoids/StulpS11",
                "DOI": "10.1109/Humanoids.2011.6100841",
                "CorpusId": 9140802
            },
            "abstract": "Temporal abstraction and task decomposition drastically reduce the search space for planning and control, and are fundamental to making complex tasks amenable to learning. In the context of reinforcement learning, temporal abstractions are studied within the paradigm of hierarchical reinforcement learning.",
            "referenceCount": 19,
            "citationCount": 77,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-12-12",
            "journal": {
                "name": "2011 11th IEEE-RAS International Conference on Humanoid Robots",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Stulp2011HierarchicalRL,\n author = {F. Stulp and S. Schaal},\n booktitle = {IEEE-RAS International Conference on Humanoid Robots},\n journal = {2011 11th IEEE-RAS International Conference on Humanoid Robots},\n pages = {231-238},\n title = {Hierarchical reinforcement learning with movement primitives},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a0fefc14c3dd10a031c9567fa136a64146eda925",
            "@type": "ScholarlyArticle",
            "paperId": "a0fefc14c3dd10a031c9567fa136a64146eda925",
            "corpusId": 14173128,
            "url": "https://www.semanticscholar.org/paper/a0fefc14c3dd10a031c9567fa136a64146eda925",
            "title": "Transfer Learning in Multi-Agent Reinforcement Learning Domains",
            "venue": "European Workshop on Reinforcement Learning",
            "publicationVenue": {
                "id": "urn:research:6795cf27-8e9b-4c0f-9853-4b02b9428e13",
                "name": "European Workshop on Reinforcement Learning",
                "alternate_names": [
                    "Eur Workshop Reinf Learn",
                    "EWRL"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=998"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "1457482454",
                "DBLP": "conf/ewrl/BoutsioukisPV11",
                "DOI": "10.1007/978-3-642-29946-9_25",
                "CorpusId": 14173128
            },
            "abstract": null,
            "referenceCount": 16,
            "citationCount": 59,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://ewrl.files.wordpress.com/2011/08/ewrl2011_submission_19.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-09-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Boutsioukis2011TransferLI,\n author = {G. Boutsioukis and Ioannis Partalas and I. Vlahavas},\n booktitle = {European Workshop on Reinforcement Learning},\n pages = {249-260},\n title = {Transfer Learning in Multi-Agent Reinforcement Learning Domains},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8851953ef486615fce803bda2e40aec97cbb5547",
            "@type": "ScholarlyArticle",
            "paperId": "8851953ef486615fce803bda2e40aec97cbb5547",
            "corpusId": 17136625,
            "url": "https://www.semanticscholar.org/paper/8851953ef486615fce803bda2e40aec97cbb5547",
            "title": "Multi-agent Reinforcement Learning: An Overview",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "206679605",
                "DOI": "10.1007/978-3-642-14435-6_7",
                "CorpusId": 17136625
            },
            "abstract": null,
            "referenceCount": 161,
            "citationCount": 583,
            "influentialCitationCount": 44,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Bu\u015foniu2010MultiagentRL,\n author = {L. Bu\u015foniu and Robert Babu\u0161ka and B. Schutter},\n pages = {183-221},\n title = {Multi-agent Reinforcement Learning: An Overview},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5705887358c73c993e1d1c595bc04033c42dfc0f",
            "@type": "ScholarlyArticle",
            "paperId": "5705887358c73c993e1d1c595bc04033c42dfc0f",
            "corpusId": 6400766,
            "url": "https://www.semanticscholar.org/paper/5705887358c73c993e1d1c595bc04033c42dfc0f",
            "title": "Approximate reinforcement learning: An overview",
            "venue": "IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning",
            "publicationVenue": {
                "id": "urn:research:1a13ae1e-223b-4ccd-821b-6a9e430564dc",
                "name": "IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning",
                "alternate_names": [
                    "ADPRL",
                    "IEEE Symp Adapt Dyn Program Reinf Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "conf/adprl/BusoniuESB11",
                "MAG": "1980972971",
                "DOI": "10.1109/ADPRL.2011.5967353",
                "CorpusId": 6400766
            },
            "abstract": "Reinforcement learning (RL) allows agents to learn how to optimally interact with complex environments. Fueled by recent advances in approximation-based algorithms, RL has obtained impressive successes in robotics, artificial intelligence, control, operations research, etc. However, the scarcity of survey papers about approximate RL makes it difficult for newcomers to grasp this intricate field. With the present overview, we take a step toward alleviating this situation. We review methods for approximate RL, starting from their dynamic programming roots and organizing them into three major classes: approximate value iteration, policy iteration, and policy search. Each class is subdivided into representative categories, highlighting among others offline and online algorithms, policy gradient methods, and simulation-based techniques. We also compare the different categories of methods, and outline possible ways to enhance the reviewed algorithms.",
            "referenceCount": 82,
            "citationCount": 62,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://orbi.uliege.be/bitstream/2268/88933/1/adprl11_Busoniu_et_al.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2011-04-11",
            "journal": {
                "name": "2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bu\u015foniu2011ApproximateRL,\n author = {L. Bu\u015foniu and D. Ernst and B. Schutter and Robert Babu\u0161ka},\n booktitle = {IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning},\n journal = {2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)},\n pages = {1-8},\n title = {Approximate reinforcement learning: An overview},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e81de96fcf20ff5520c20e4ee93db59bbf644cc9",
            "@type": "ScholarlyArticle",
            "paperId": "e81de96fcf20ff5520c20e4ee93db59bbf644cc9",
            "corpusId": 10498694,
            "url": "https://www.semanticscholar.org/paper/e81de96fcf20ff5520c20e4ee93db59bbf644cc9",
            "title": "Cooperative Adaptive Cruise Control: A Reinforcement Learning Approach",
            "venue": "IEEE transactions on intelligent transportation systems (Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "journals/tits/DesjardinsC11",
                "MAG": "2129660761",
                "DOI": "10.1109/TITS.2011.2157145",
                "CorpusId": 10498694
            },
            "abstract": "Recently, improvements in sensing, communicating, and computing technologies have led to the development of driver-assistance systems (DASs). Such systems aim at helping drivers by either providing a warning to reduce crashes or doing some of the control tasks to relieve a driver from repetitive and boring tasks. Thus, for example, adaptive cruise control (ACC) aims at relieving a driver from manually adjusting his/her speed to maintain a constant speed or a safe distance from the vehicle in front of him/her. Currently, ACC can be improved through vehicle-to-vehicle communication, where the current speed and acceleration of a vehicle can be transmitted to the following vehicles by intervehicle communication. This way, vehicle-to-vehicle communication with ACC can be combined in one single system called cooperative adaptive cruise control (CACC). This paper investigates CACC by proposing a novel approach for the design of autonomous vehicle controllers based on modern machine-learning techniques. More specifically, this paper shows how a reinforcement-learning approach can be used to develop controllers for the secure longitudinal following of a front vehicle. This approach uses function approximation techniques along with gradient-descent learning algorithms as a means of directly modifying a control policy to optimize its performance. The experimental results, through simulation, show that this design approach can result in efficient behavior for CACC.",
            "referenceCount": 77,
            "citationCount": 271,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-12-01",
            "journal": {
                "name": "IEEE Transactions on Intelligent Transportation Systems",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Desjardins2011CooperativeAC,\n author = {Charles Desjardins and B. Chaib-draa},\n booktitle = {IEEE transactions on intelligent transportation systems (Print)},\n journal = {IEEE Transactions on Intelligent Transportation Systems},\n pages = {1248-1260},\n title = {Cooperative Adaptive Cruise Control: A Reinforcement Learning Approach},\n volume = {12},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5703c2643f7e8012b9bfb2ed53f34114acfc10f8",
            "@type": "ScholarlyArticle",
            "paperId": "5703c2643f7e8012b9bfb2ed53f34114acfc10f8",
            "corpusId": 12495013,
            "url": "https://www.semanticscholar.org/paper/5703c2643f7e8012b9bfb2ed53f34114acfc10f8",
            "title": "Relational Reinforcement Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": null,
            "externalIds": {
                "DOI": "10.1007/springerreference_179433",
                "CorpusId": 12495013
            },
            "abstract": null,
            "referenceCount": 112,
            "citationCount": 428,
            "influentialCitationCount": 41,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Misc{None,\n author = {De and M Bruynooghe},\n title = {Relational Reinforcement Learning}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:70ab5992627e457241c0648ebffc7ed2edc84303",
            "@type": "ScholarlyArticle",
            "paperId": "70ab5992627e457241c0648ebffc7ed2edc84303",
            "corpusId": 16941967,
            "url": "https://www.semanticscholar.org/paper/70ab5992627e457241c0648ebffc7ed2edc84303",
            "title": "Neural systems of reinforcement for drug addiction: from actions to habits to compulsion",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2329823113",
                "DOI": "10.1038/nn1579",
                "CorpusId": 16941967,
                "PubMed": "16251991"
            },
            "abstract": null,
            "referenceCount": 107,
            "citationCount": 3477,
            "influentialCitationCount": 187,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2005-11-01",
            "journal": {
                "name": "Nature Neuroscience",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Everitt2005NeuralSO,\n author = {B. Everitt and T. Robbins},\n booktitle = {Nature Neuroscience},\n journal = {Nature Neuroscience},\n pages = {1481-1489},\n title = {Neural systems of reinforcement for drug addiction: from actions to habits to compulsion},\n volume = {8},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7ad66cba3b7e3abae7ef33122588512a146f7f77",
            "@type": "ScholarlyArticle",
            "paperId": "7ad66cba3b7e3abae7ef33122588512a146f7f77",
            "corpusId": 11311635,
            "url": "https://www.semanticscholar.org/paper/7ad66cba3b7e3abae7ef33122588512a146f7f77",
            "title": "A Survey on Multi-Task Learning",
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "publicationVenue": {
                "id": "urn:research:c6840156-ee10-4d78-8832-7f8909811576",
                "name": "IEEE Transactions on Knowledge and Data Engineering",
                "alternate_names": [
                    "IEEE Trans Knowl Data Eng"
                ],
                "issn": "1041-4347",
                "url": "https://www.computer.org/web/tkde"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/ZhangY17aa",
                "ArXiv": "1707.08114",
                "MAG": "3141797743",
                "DOI": "10.1109/TKDE.2021.3070203",
                "CorpusId": 11311635
            },
            "abstract": "Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.",
            "referenceCount": 277,
            "citationCount": 1394,
            "influentialCitationCount": 87,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1707.08114",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-07-25",
            "journal": {
                "name": "IEEE Transactions on Knowledge and Data Engineering",
                "volume": "34"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2017ASO,\n author = {Yu Zhang and Qiang Yang},\n booktitle = {IEEE Transactions on Knowledge and Data Engineering},\n journal = {IEEE Transactions on Knowledge and Data Engineering},\n pages = {5586-5609},\n title = {A Survey on Multi-Task Learning},\n volume = {34},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:279beb46b49d01dd893bf942191f473cd37cde85",
            "@type": "ScholarlyArticle",
            "paperId": "279beb46b49d01dd893bf942191f473cd37cde85",
            "corpusId": 427785,
            "url": "https://www.semanticscholar.org/paper/279beb46b49d01dd893bf942191f473cd37cde85",
            "title": "A Neural Signature of Hierarchical Reinforcement Learning",
            "venue": "Neuron",
            "publicationVenue": {
                "id": "urn:research:7a61412a-9a9a-487d-9613-5a1fbd879c9d",
                "name": "Neuron",
                "alternate_names": null,
                "issn": "0896-6273",
                "url": "https://www.cell.com/neuron/home"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2127469248",
                "DOI": "10.1016/j.neuron.2011.05.042",
                "CorpusId": 427785,
                "PubMed": "21791294"
            },
            "abstract": null,
            "referenceCount": 69,
            "citationCount": 182,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cell.com/article/S0896627311004995/pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-07-28",
            "journal": {
                "name": "Neuron",
                "volume": "71"
            },
            "citationStyles": {
                "bibtex": "@Article{Ribas-Fernandes2011ANS,\n author = {Jos\u00e9 Ribas-Fernandes and Alec Solway and Carlos Diuk and Joseph T. McGuire and A. Barto and Y. Niv and M. Botvinick},\n booktitle = {Neuron},\n journal = {Neuron},\n pages = {370-379},\n title = {A Neural Signature of Hierarchical Reinforcement Learning},\n volume = {71},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8bc80ccc97902264e9d98c48f7ad36f7dcd61b3c",
            "@type": "ScholarlyArticle",
            "paperId": "8bc80ccc97902264e9d98c48f7ad36f7dcd61b3c",
            "corpusId": 16182497,
            "url": "https://www.semanticscholar.org/paper/8bc80ccc97902264e9d98c48f7ad36f7dcd61b3c",
            "title": "Reinforcement learning-based multi-agent system for network traffic signal control",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2074500080",
                "DOI": "10.1049/IET-ITS.2009.0070",
                "CorpusId": 16182497
            },
            "abstract": "A challenging application of artificial intelligence systems involves the scheduling of traffic signals in multi-intersection vehicular networks. This paper introduces a novel use of a multi-agent system and reinforcement learning (RL) framework to obtain an efficient traffic signal control policy. The latter is aimed at minimising the average delay, congestion and likelihood of intersection cross-blocking. A five-intersection traffic network has been studied in which each intersection is governed by an autonomous intelligent agent. Two types of agents, a central agent and an outbound agent, were employed. The outbound agents schedule traffic signals by following the longest-queue-first (LQF) algorithm, which has been proved to guarantee stability and fairness, and collaborate with the central agent by providing it local traffic statistics. The central agent learns a value function driven by its local and neighbours' traffic conditions. The novel methodology proposed here utilises the Q-Learning algorithm with a feedforward neural network for value function approximation. Experimental results clearly demonstrate the advantages of multi-agent RL-based control over LQF governed isolated single-intersection control, thus paving the way for efficient distributed traffic signal control in complex settings.",
            "referenceCount": 29,
            "citationCount": 450,
            "influentialCitationCount": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.ece.utk.edu/%7Eitamar/Papers/IET_ITS_2010.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2010-06-03",
            "journal": {
                "name": "Iet Intelligent Transport Systems",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Arel2010ReinforcementLM,\n author = {I. Arel and C. Liu and T. Urbanik and A. Kohls},\n journal = {Iet Intelligent Transport Systems},\n pages = {128-135},\n title = {Reinforcement learning-based multi-agent system for network traffic signal control},\n volume = {4},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:19de9ed920d9ed406274dd3595ef81e5e4398537",
            "@type": "ScholarlyArticle",
            "paperId": "19de9ed920d9ed406274dd3595ef81e5e4398537",
            "corpusId": 20185835,
            "url": "https://www.semanticscholar.org/paper/19de9ed920d9ed406274dd3595ef81e5e4398537",
            "title": "Transfer Learning",
            "venue": "Encyclopedia of Machine Learning and Data Mining",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "reference/ml/X17xq",
                "DOI": "10.1007/978-1-4899-7687-1_100487",
                "CorpusId": 20185835
            },
            "abstract": null,
            "referenceCount": 69,
            "citationCount": 390,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2020-11-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Pan2020TransferL,\n author = {Sinno Jialin Pan},\n booktitle = {Encyclopedia of Machine Learning and Data Mining},\n pages = {1342},\n title = {Transfer Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ed06643f750773ce6af6b29a6d0f465731c8e0a5",
            "@type": "ScholarlyArticle",
            "paperId": "ed06643f750773ce6af6b29a6d0f465731c8e0a5",
            "corpusId": 15454626,
            "url": "https://www.semanticscholar.org/paper/ed06643f750773ce6af6b29a6d0f465731c8e0a5",
            "title": "Reinforcement learning of motor skills with policy gradients",
            "venue": "Neural Networks",
            "publicationVenue": {
                "id": "urn:research:a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                "name": "Neural Networks",
                "alternate_names": [
                    "Neural Netw"
                ],
                "issn": "0893-6080",
                "url": "http://www.elsevier.com/locate/neunet"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "journals/nn/PetersS08",
                "MAG": "2125612430",
                "DOI": "10.1016/j.neunet.2008.02.003",
                "CorpusId": 15454626,
                "PubMed": "18482830"
            },
            "abstract": null,
            "referenceCount": 77,
            "citationCount": 913,
            "influentialCitationCount": 50,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www-clmc.usc.edu/publications/P/peters-NN2008.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-05-01",
            "journal": {
                "name": "Neural networks : the official journal of the International Neural Network Society",
                "volume": "21 4"
            },
            "citationStyles": {
                "bibtex": "@Article{Peters2008ReinforcementLO,\n author = {Jan Peters and S. Schaal},\n booktitle = {Neural Networks},\n journal = {Neural networks : the official journal of the International Neural Network Society},\n pages = {\n          682-97\n        },\n title = {Reinforcement learning of motor skills with policy gradients},\n volume = {21 4},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5b01eaef54a653ba03ddd5a978690380fbc19bfc",
            "@type": "ScholarlyArticle",
            "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
            "corpusId": 3521071,
            "url": "https://www.semanticscholar.org/paper/5b01eaef54a653ba03ddd5a978690380fbc19bfc",
            "title": "Diversity is All You Need: Learning Skills without a Reward Function",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1802-06070",
                "MAG": "2951020250",
                "ArXiv": "1802.06070",
                "CorpusId": 3521071
            },
            "abstract": "Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN (\"Diversity is All You Need\"), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. In these environments, some of the learned skills correspond to solving the task, and each skill that solves the task does so in a distinct manner. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning",
            "referenceCount": 48,
            "citationCount": 844,
            "influentialCitationCount": 153,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.06070"
            },
            "citationStyles": {
                "bibtex": "@Article{Eysenbach2018DiversityIA,\n author = {Benjamin Eysenbach and Abhishek Gupta and Julian Ibarz and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Diversity is All You Need: Learning Skills without a Reward Function},\n volume = {abs/1802.06070},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:30b38ca8151bbd5a5ff45bce94297d1248ff58b5",
            "@type": "ScholarlyArticle",
            "paperId": "30b38ca8151bbd5a5ff45bce94297d1248ff58b5",
            "corpusId": 54559476,
            "url": "https://www.semanticscholar.org/paper/30b38ca8151bbd5a5ff45bce94297d1248ff58b5",
            "title": "Deep Learning on Graphs: A Survey",
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "publicationVenue": {
                "id": "urn:research:c6840156-ee10-4d78-8832-7f8909811576",
                "name": "IEEE Transactions on Knowledge and Data Engineering",
                "alternate_names": [
                    "IEEE Trans Knowl Data Eng"
                ],
                "issn": "1041-4347",
                "url": "https://www.computer.org/web/tkde"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2904900486",
                "DBLP": "journals/corr/abs-1812-04202",
                "ArXiv": "1812.04202",
                "DOI": "10.1109/tkde.2020.2981333",
                "CorpusId": 54559476
            },
            "abstract": "Deep learning has been shown to be successful in a number of domains, ranging from acoustics, images, to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, substantial research efforts have been devoted to applying deep learning methods to graphs, resulting in beneficial advances in graph analysis techniques. In this survey, we comprehensively review the different types of deep learning methods on graphs. We divide the existing methods into five categories based on their model architectures and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods. We then provide a comprehensive overview of these methods in a systematic manner mainly by following their development history. We also analyze the differences and compositions of different methods. Finally, we briefly outline the applications in which they have been used and discuss potential future research directions.",
            "referenceCount": 201,
            "citationCount": 989,
            "influentialCitationCount": 41,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1812.04202",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-12-11",
            "journal": {
                "name": "IEEE Transactions on Knowledge and Data Engineering",
                "volume": "34"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2018DeepLO,\n author = {Ziwei Zhang and Peng Cui and Wenwu Zhu},\n booktitle = {IEEE Transactions on Knowledge and Data Engineering},\n journal = {IEEE Transactions on Knowledge and Data Engineering},\n pages = {249-270},\n title = {Deep Learning on Graphs: A Survey},\n volume = {34},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3c79e6ec5344181151905b20d4b4f4cca680a2ee",
            "@type": "ScholarlyArticle",
            "paperId": "3c79e6ec5344181151905b20d4b4f4cca680a2ee",
            "corpusId": 7449640,
            "url": "https://www.semanticscholar.org/paper/3c79e6ec5344181151905b20d4b4f4cca680a2ee",
            "title": "Intrinsically Motivated Reinforcement Learning: An Evolutionary Perspective",
            "venue": "IEEE Transactions on Autonomous Mental Development",
            "publicationVenue": {
                "id": "urn:research:ff02830d-21ea-43d8-8b9a-2d4532c92647",
                "name": "IEEE Transactions on Autonomous Mental Development",
                "alternate_names": [
                    "IEEE Trans Auton Ment Dev"
                ],
                "issn": "1943-0604",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=4563672"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2164424353",
                "DBLP": "journals/tamd/SinghLBS10",
                "DOI": "10.1109/TAMD.2010.2051031",
                "CorpusId": 7449640
            },
            "abstract": "There is great interest in building intrinsic motivation into artificial systems using the reinforcement learning framework. Yet, what intrinsic motivation may mean computationally, and how it may differ from extrinsic motivation, remains a murky and controversial subject. In this paper, we adopt an evolutionary perspective and define a new optimal reward framework that captures the pressure to design good primary reward functions that lead to evolutionary success across environments. The results of two computational experiments show that optimal primary reward signals may yield both emergent intrinsic and extrinsic motivation. The evolutionary perspective and the associated optimal reward framework thus lead to the conclusion that there are no hard and fast features distinguishing intrinsic and extrinsic reward computationally. Rather, the directness of the relationship between rewarding behavior and evolutionary success varies along a continuum.",
            "referenceCount": 64,
            "citationCount": 392,
            "influentialCitationCount": 17,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www-personal.umich.edu/%7Erickl/pubs/singh-lewis-barto-sorg-2010-ieee-draft.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-06-01",
            "journal": {
                "name": "IEEE Transactions on Autonomous Mental Development",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Singh2010IntrinsicallyMR,\n author = {Satinder Singh and Richard L. Lewis and A. Barto and Jonathan Sorg},\n booktitle = {IEEE Transactions on Autonomous Mental Development},\n journal = {IEEE Transactions on Autonomous Mental Development},\n pages = {70-82},\n title = {Intrinsically Motivated Reinforcement Learning: An Evolutionary Perspective},\n volume = {2},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:36086ff255207cc1adb818c4d0cd62287d437d38",
            "@type": "ScholarlyArticle",
            "paperId": "36086ff255207cc1adb818c4d0cd62287d437d38",
            "corpusId": 1240464,
            "url": "https://www.semanticscholar.org/paper/36086ff255207cc1adb818c4d0cd62287d437d38",
            "title": "Deep auto-encoder neural networks in reinforcement learning",
            "venue": "IEEE International Joint Conference on Neural Network",
            "publicationVenue": {
                "id": "urn:research:f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                "name": "IEEE International Joint Conference on Neural Network",
                "alternate_names": [
                    "IJCNN",
                    "IEEE Int Jt Conf Neural Netw",
                    "Int Jt Conf Neural Netw",
                    "International Joint Conference on Neural Network"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=1573"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "conf/ijcnn/LangeR10",
                "MAG": "2154997814",
                "DOI": "10.1109/IJCNN.2010.5596468",
                "CorpusId": 1240464
            },
            "abstract": "This paper discusses the effectiveness of deep auto-encoder neural networks in visual reinforcement learning (RL) tasks. We propose a framework for combining the training of deep auto-encoders (for learning compact feature spaces) with recently-proposed batch-mode RL algorithms (for learning policies). An emphasis is put on the data-efficiency of this combination and on studying the properties of the feature spaces automatically constructed by the deep auto-encoders. These feature spaces are empirically shown to adequately resemble existing similarities and spatial relations between observations and allow to learn useful policies. We propose several methods for improving the topology of the feature spaces making use of task-dependent information. Finally, we present first results on successfully learning good control policies directly on synthesized and real images.",
            "referenceCount": 22,
            "citationCount": 331,
            "influentialCitationCount": 20,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://ml.informatik.uni-freiburg.de/_media/publications/langeijcnn2010.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-07-01",
            "journal": {
                "name": "The 2010 International Joint Conference on Neural Networks (IJCNN)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lange2010DeepAN,\n author = {S. Lange and Martin A. Riedmiller},\n booktitle = {IEEE International Joint Conference on Neural Network},\n journal = {The 2010 International Joint Conference on Neural Networks (IJCNN)},\n pages = {1-8},\n title = {Deep auto-encoder neural networks in reinforcement learning},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ae7619604821adce52c28daa2aed14f5a191d975",
            "@type": "ScholarlyArticle",
            "paperId": "ae7619604821adce52c28daa2aed14f5a191d975",
            "corpusId": 21718339,
            "url": "https://www.semanticscholar.org/paper/ae7619604821adce52c28daa2aed14f5a191d975",
            "title": "Progress & Compress: A scalable framework for continual learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2804175194",
                "DBLP": "journals/corr/abs-1805-06370",
                "ArXiv": "1805.06370",
                "CorpusId": 21718339
            },
            "abstract": "We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to preserve performance on previously encountered tasks while accelerating learning progress on subsequent problems. This is achieved by training a network with two components: A knowledge base, capable of solving previously encountered problems, which is connected to an active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to protect any previously acquired skills. This cycle of active learning (progression) followed by consolidation (compression) requires no architecture growth, no access to or storing of previous data or tasks, and no task-specific parameters. We demonstrate the progress & compress approach on sequential classification of handwritten alphabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.",
            "referenceCount": 43,
            "citationCount": 655,
            "influentialCitationCount": 105,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-05-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1805.06370"
            },
            "citationStyles": {
                "bibtex": "@Article{Schwarz2018ProgressC,\n author = {Jonathan Schwarz and Wojciech M. Czarnecki and Jelena Luketina and A. Grabska-Barwinska and Y. Teh and Razvan Pascanu and R. Hadsell},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Progress & Compress: A scalable framework for continual learning},\n volume = {abs/1805.06370},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f5f323e62acb75f785e00b4c90ace16f1690076f",
            "@type": "ScholarlyArticle",
            "paperId": "f5f323e62acb75f785e00b4c90ace16f1690076f",
            "corpusId": 8696662,
            "url": "https://www.semanticscholar.org/paper/f5f323e62acb75f785e00b4c90ace16f1690076f",
            "title": "Deep Recurrent Q-Learning for Partially Observable MDPs",
            "venue": "AAAI Fall Symposia",
            "publicationVenue": {
                "id": "urn:research:5c1b6dba-7390-42e1-a873-2059f9f57ddd",
                "name": "AAAI Fall Symposia",
                "alternate_names": [
                    "AAAI Fall Symp"
                ],
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/corr/HausknechtS15",
                "ArXiv": "1507.06527",
                "MAG": "2952684340",
                "CorpusId": 8696662
            },
            "abstract": "Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting \\textit{Deep Recurrent Q-Network} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.",
            "referenceCount": 17,
            "citationCount": 1396,
            "influentialCitationCount": 207,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-07-23",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1507.06527"
            },
            "citationStyles": {
                "bibtex": "@Article{Hausknecht2015DeepRQ,\n author = {Matthew J. Hausknecht and P. Stone},\n booktitle = {AAAI Fall Symposia},\n journal = {ArXiv},\n title = {Deep Recurrent Q-Learning for Partially Observable MDPs},\n volume = {abs/1507.06527},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2b877889ac31b73d1ede70b00eb4c7118ef8eca2",
            "@type": "ScholarlyArticle",
            "paperId": "2b877889ac31b73d1ede70b00eb4c7118ef8eca2",
            "corpusId": 53100211,
            "url": "https://www.semanticscholar.org/paper/2b877889ac31b73d1ede70b00eb4c7118ef8eca2",
            "title": "Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2952093114",
                "ArXiv": "1810.11910",
                "DBLP": "conf/iclr/RiemerCALRTT19",
                "CorpusId": 53100211
            },
            "abstract": "Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.",
            "referenceCount": 63,
            "citationCount": 549,
            "influentialCitationCount": 73,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1810.11910"
            },
            "citationStyles": {
                "bibtex": "@Article{Riemer2018LearningTL,\n author = {M. Riemer and Ignacio Cases and R. Ajemian and Miao Liu and I. Rish and Y. Tu and G. Tesauro},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference},\n volume = {abs/1810.11910},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d33ad6a25264ba1747d8c93f6621c7f90a7ec601",
            "@type": "ScholarlyArticle",
            "paperId": "d33ad6a25264ba1747d8c93f6621c7f90a7ec601",
            "corpusId": 25316837,
            "url": "https://www.semanticscholar.org/paper/d33ad6a25264ba1747d8c93f6621c7f90a7ec601",
            "title": "Meta-SGD: Learning to Learn Quickly for Few Shot Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2742093937",
                "ArXiv": "1707.09835",
                "DBLP": "journals/corr/LiZCL17",
                "CorpusId": 25316837
            },
            "abstract": "Few-shot learning is challenging for learning algorithms that learn each task in isolation and from scratch. In contrast, meta-learning learns from many related tasks a meta-learner that can learn a new task more accurately and faster with fewer examples, where the choice of meta-learners is crucial. In this paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learner that can initialize and adapt any differentiable learner in just one step, on both supervised learning and reinforcement learning. Compared to the popular meta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, and can be learned more efficiently. Compared to the latest meta-learner MAML, Meta-SGD has a much higher capacity by learning to learn not just the learner initialization, but also the learner update direction and learning rate, all in a single meta-learning process. Meta-SGD shows highly competitive performance for few-shot learning on regression, classification, and reinforcement learning.",
            "referenceCount": 32,
            "citationCount": 895,
            "influentialCitationCount": 127,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-07-31",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1707.09835"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2017MetaSGDLT,\n author = {Zhenguo Li and Fengwei Zhou and Fei Chen and Hang Li},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Meta-SGD: Learning to Learn Quickly for Few Shot Learning},\n volume = {abs/1707.09835},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1e819f533ef2bf5ca50a6b2008d96eaea2a2706e",
            "@type": "ScholarlyArticle",
            "paperId": "1e819f533ef2bf5ca50a6b2008d96eaea2a2706e",
            "corpusId": 3486660,
            "url": "https://www.semanticscholar.org/paper/1e819f533ef2bf5ca50a6b2008d96eaea2a2706e",
            "title": "Learning Combinatorial Optimization Algorithms over Graphs",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1704.01665",
                "DBLP": "journals/corr/DaiKZDS17",
                "MAG": "2607264901",
                "CorpusId": 3486660
            },
            "abstract": "The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires significant specialized knowledge and trial-and-error. Can we automate this challenging, tedious process, and learn the algorithms instead? In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data. This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems. In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge. The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.",
            "referenceCount": 51,
            "citationCount": 1121,
            "influentialCitationCount": 125,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-04-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1704.01665"
            },
            "citationStyles": {
                "bibtex": "@Article{Khalil2017LearningCO,\n author = {Elias Boutros Khalil and H. Dai and Yuyu Zhang and B. Dilkina and Le Song},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Learning Combinatorial Optimization Algorithms over Graphs},\n volume = {abs/1704.01665},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d44ac0a7fd4734187bccafc4a2771027b8bb595e",
            "@type": "ScholarlyArticle",
            "paperId": "d44ac0a7fd4734187bccafc4a2771027b8bb595e",
            "corpusId": 201716327,
            "url": "https://www.semanticscholar.org/paper/d44ac0a7fd4734187bccafc4a2771027b8bb595e",
            "title": "Deep learning enables rapid identification of potent DDR1 kinase inhibitors",
            "venue": "Nature Biotechnology",
            "publicationVenue": {
                "id": "urn:research:458166b3-de17-4bf3-bbbb-e53782de2f0f",
                "name": "Nature Biotechnology",
                "alternate_names": [
                    "Nat Biotechnol"
                ],
                "issn": "1087-0156",
                "url": "http://www.nature.com/nbt/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2971690404",
                "DOI": "10.1038/s41587-019-0224-x",
                "CorpusId": 201716327,
                "PubMed": "31477924"
            },
            "abstract": null,
            "referenceCount": 28,
            "citationCount": 635,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Biology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-01",
            "journal": {
                "name": "Nature Biotechnology",
                "volume": "37"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhavoronkov2019DeepLE,\n author = {A. Zhavoronkov and Y. Ivanenkov and A. Aliper and M. Veselov and V. Aladinskiy and Anastasiya V Aladinskaya and V. Terentiev and Daniil Polykovskiy and Maksim Kuznetsov and Arip Asadulaev and Yury Volkov and Artem Zholus and Shayakhmetov Rim and Alexander Zhebrak and L. Minaeva and B. Zagribelnyy and Lennart H Lee and R. Soll and D. Madge and Li Xing and Tao Guo and Al\u00e1n Aspuru-Guzik},\n booktitle = {Nature Biotechnology},\n journal = {Nature Biotechnology},\n pages = {1038 - 1040},\n title = {Deep learning enables rapid identification of potent DDR1 kinase inhibitors},\n volume = {37},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:27f0504bb0ddb10249aeee4b0216cdb6f2a51dd8",
            "@type": "ScholarlyArticle",
            "paperId": "27f0504bb0ddb10249aeee4b0216cdb6f2a51dd8",
            "corpusId": 10837930,
            "url": "https://www.semanticscholar.org/paper/27f0504bb0ddb10249aeee4b0216cdb6f2a51dd8",
            "title": "Robot motor skill coordination with EM-based Reinforcement Learning",
            "venue": "2010 IEEE/RSJ International Conference on Intelligent Robots and Systems",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2016765487",
                "DBLP": "conf/iros/KormushevCC10",
                "DOI": "10.1109/IROS.2010.5649089",
                "CorpusId": 10837930
            },
            "abstract": "We present an approach allowing a robot to acquire new motor skills by learning the couplings across motor control variables. The demonstrated skill is first encoded in a compact form through a modified version of Dynamic Movement Primitives (DMP) which encapsulates correlation information. Expectation-Maximization based Reinforcement Learning is then used to modulate the mixture of dynamical systems initialized from the user's demonstration. The approach is evaluated on a torque-controlled 7 DOFs Barrett WAM robotic arm. Two skill learning experiments are conducted: a reaching task where the robot needs to adapt the learned movement to avoid an obstacle, and a dynamic pancake-flipping task.",
            "referenceCount": 29,
            "citationCount": 291,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://spiral.imperial.ac.uk/bitstream/10044/1/26071/2/Kormushev-IROS2010.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-12-03",
            "journal": {
                "name": "2010 IEEE/RSJ International Conference on Intelligent Robots and Systems",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kormushev2010RobotMS,\n author = {Petar Kormushev and S. Calinon and D. Caldwell},\n booktitle = {2010 IEEE/RSJ International Conference on Intelligent Robots and Systems},\n journal = {2010 IEEE/RSJ International Conference on Intelligent Robots and Systems},\n pages = {3232-3237},\n title = {Robot motor skill coordination with EM-based Reinforcement Learning},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0f32846ca519e1966de3dbfecacf7188347b8c7b",
            "@type": "ScholarlyArticle",
            "paperId": "0f32846ca519e1966de3dbfecacf7188347b8c7b",
            "corpusId": 15116370,
            "url": "https://www.semanticscholar.org/paper/0f32846ca519e1966de3dbfecacf7188347b8c7b",
            "title": "Reinforcement learning of motor skills in high dimensions: A path integral approach",
            "venue": "IEEE International Conference on Robotics and Automation",
            "publicationVenue": {
                "id": "urn:research:3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                "name": "IEEE International Conference on Robotics and Automation",
                "alternate_names": [
                    "International Conference on Robotics and Automation",
                    "Int Conf Robot Autom",
                    "ICRA",
                    "IEEE Int Conf Robot Autom"
                ],
                "issn": "2152-4092",
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "conf/icra/TheodorouBS10",
                "MAG": "2042882799",
                "DOI": "10.1109/ROBOT.2010.5509336",
                "CorpusId": 15116370
            },
            "abstract": "Reinforcement learning (RL) is one of the most general approaches to learning control. Its applicability to complex motor systems, however, has been largely impossible so far due to the computational difficulties that reinforcement learning encounters in high dimensional continuous state-action spaces. In this paper, we derive a novel approach to RL for parameterized control policies based on the framework of stochastic optimal control with path integrals. While solidly grounded in optimal control theory and estimation theory, the update equations for learning are surprisingly simple and have no danger of numerical instabilities as neither matrix inversions nor gradient learning rates are required. Empirical evaluations demonstrate significant performance improvements over gradient-based policy learning and scalability to high-dimensional control problems. Finally, a learning experiment on a robot dog illustrates the functionality of our algorithm in a real-world scenario. We believe that our new algorithm, Policy Improvement with Path Integrals (PI2), offers currently one of the most efficient, numerically robust, and easy to implement algorithms for RL in robotics.",
            "referenceCount": 41,
            "citationCount": 288,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-05-03",
            "journal": {
                "name": "2010 IEEE International Conference on Robotics and Automation",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Theodorou2010ReinforcementLO,\n author = {Evangelos A. Theodorou and J. Buchli and S. Schaal},\n booktitle = {IEEE International Conference on Robotics and Automation},\n journal = {2010 IEEE International Conference on Robotics and Automation},\n pages = {2397-2403},\n title = {Reinforcement learning of motor skills in high dimensions: A path integral approach},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:250bf5e1d40a619080dec553914c2905db6008c7",
            "@type": "ScholarlyArticle",
            "paperId": "250bf5e1d40a619080dec553914c2905db6008c7",
            "corpusId": 25026734,
            "url": "https://www.semanticscholar.org/paper/250bf5e1d40a619080dec553914c2905db6008c7",
            "title": "Value-Decomposition Networks For Cooperative Multi-Agent Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1706.05296",
                "DBLP": "journals/corr/SunehagLGCZJLSL17",
                "MAG": "2626637010",
                "CorpusId": 25026734
            },
            "abstract": "We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the \"lazy agent\" problem, which arises due to partial observability. We address these problems by training individual agents with a novel value decomposition network architecture, which learns to decompose the team value function into agent-wise value functions. We perform an experimental evaluation across a range of partially-observable multi-agent domains and show that learning such value-decompositions leads to superior results, in particular when combined with weight sharing, role information and information channels.",
            "referenceCount": 40,
            "citationCount": 821,
            "influentialCitationCount": 176,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-06-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1706.05296"
            },
            "citationStyles": {
                "bibtex": "@Article{Sunehag2017ValueDecompositionNF,\n author = {P. Sunehag and Guy Lever and A. Gruslys and Wojciech M. Czarnecki and V. Zambaldi and Max Jaderberg and Marc Lanctot and Nicolas Sonnerat and Joel Z. Leibo and K. Tuyls and T. Graepel},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Value-Decomposition Networks For Cooperative Multi-Agent Learning},\n volume = {abs/1706.05296},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:accb4b7a1e670ec1be3d5a2e784b8f524ff8b303",
            "@type": "ScholarlyArticle",
            "paperId": "accb4b7a1e670ec1be3d5a2e784b8f524ff8b303",
            "corpusId": 6676959,
            "url": "https://www.semanticscholar.org/paper/accb4b7a1e670ec1be3d5a2e784b8f524ff8b303",
            "title": "Combining manual feedback with subsequent MDP reward signals for reinforcement learning",
            "venue": "Adaptive Agents and Multi-Agent Systems",
            "publicationVenue": {
                "id": "urn:research:6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                "name": "Adaptive Agents and Multi-Agent Systems",
                "alternate_names": [
                    "Adapt Agent Multi-agent Syst",
                    "International Joint Conference on Autonomous Agents & Multiagent Systems",
                    "Adapt Agent Multi-agents Syst",
                    "AAMAS",
                    "Adaptive Agents and Multi-Agents Systems",
                    "Int Jt Conf Auton Agent  Multiagent Syst"
                ],
                "issn": null,
                "url": "http://www.ifaamas.org/"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "conf/atal/KnoxS10",
                "MAG": "2116157560",
                "DOI": "10.1145/1838206.1838208",
                "CorpusId": 6676959
            },
            "abstract": "As learning agents move from research labs to the real world, it is increasingly important that human users, including those without programming skills, be able to teach agents desired behaviors. Recently, the tamer framework was introduced for designing agents that can be interactively shaped by human trainers who give only positive and negative feedback signals. Past work on tamer showed that shaping can greatly reduce the sample complexity required to learn a good policy, can enable lay users to teach agents the behaviors they desire, and can allow agents to learn within a Markov Decision Process (MDP) in the absence of a coded reward function. However, tamer does not allow this human training to be combined with autonomous learning based on such a coded reward function. This paper leverages the fast learning exhibited within the tamer framework to hasten a reinforcement learning (RL) algorithm's climb up the learning curve, effectively demonstrating that human reinforcement and MDP reward can be used in conjunction with one another by an autonomous agent. We tested eight plausible tamer+rl methods for combining a previously learned human reinforcement function, H, with MDP reward in a reinforcement learning algorithm. This paper identifies which of these methods are most effective and analyzes their strengths and weaknesses. Results from these tamer+rl algorithms indicate better final performance and better cumulative performance than either a tamer agent or an RL agent alone.",
            "referenceCount": 22,
            "citationCount": 225,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-05-10",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Knox2010CombiningMF,\n author = {W. B. Knox and P. Stone},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {5-12},\n title = {Combining manual feedback with subsequent MDP reward signals for reinforcement learning},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:af64497a418cf62016251360d9ba537604f7622a",
            "@type": "ScholarlyArticle",
            "paperId": "af64497a418cf62016251360d9ba537604f7622a",
            "corpusId": 1738567,
            "url": "https://www.semanticscholar.org/paper/af64497a418cf62016251360d9ba537604f7622a",
            "title": "Efficient Exploration in Reinforcement Learning",
            "venue": "Encyclopedia of Machine Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "reference/ml/Langford17",
                "MAG": "1554233645",
                "DOI": "10.1007/978-0-387-30164-8_244",
                "CorpusId": 1738567
            },
            "abstract": null,
            "referenceCount": 9,
            "citationCount": 224,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Langford2010EfficientEI,\n author = {J. Langford},\n booktitle = {Encyclopedia of Machine Learning},\n pages = {389-392},\n title = {Efficient Exploration in Reinforcement Learning},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:176f1d608b918eec8dc4b75e7b6e0acaba84a447",
            "@type": "ScholarlyArticle",
            "paperId": "176f1d608b918eec8dc4b75e7b6e0acaba84a447",
            "corpusId": 98180,
            "url": "https://www.semanticscholar.org/paper/176f1d608b918eec8dc4b75e7b6e0acaba84a447",
            "title": "Adversarial Learning for Neural Dialogue Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1701.06547",
                "ACL": "D17-1230",
                "DBLP": "journals/corr/LiMSRJ17",
                "MAG": "2951520714",
                "DOI": "10.18653/v1/D17-1230",
                "CorpusId": 98180
            },
            "abstract": "We apply adversarial training to open-domain dialogue generation, training a system to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning problem where we jointly train two systems: a generative model to produce response sequences, and a discriminator\u2014analagous to the human evaluator in the Turing test\u2014 to distinguish between the human-generated dialogues and the machine-generated ones. In this generative adversarial network approach, the outputs from the discriminator are used to encourage the system towards more human-like dialogue. Further, we investigate models for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines",
            "referenceCount": 52,
            "citationCount": 862,
            "influentialCitationCount": 114,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D17-1230.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-01-23",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1701.06547"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2017AdversarialLF,\n author = {Jiwei Li and Will Monroe and Tianlin Shi and S\u00e9bastien Jean and Alan Ritter and Dan Jurafsky},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Adversarial Learning for Neural Dialogue Generation},\n volume = {abs/1701.06547},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8acc06ebabba26b831787c08cba3c9ab7caee850",
            "@type": "ScholarlyArticle",
            "paperId": "8acc06ebabba26b831787c08cba3c9ab7caee850",
            "corpusId": 5938812,
            "url": "https://www.semanticscholar.org/paper/8acc06ebabba26b831787c08cba3c9ab7caee850",
            "title": "Informing sequential clinical decision-making through\u00a0reinforcement learning: an empirical study",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "journals/ml/ShortreedLLSPM11",
                "MAG": "2151161180",
                "DOI": "10.1007/s10994-010-5229-0",
                "CorpusId": 5938812,
                "PubMed": "21799585"
            },
            "abstract": null,
            "referenceCount": 76,
            "citationCount": 182,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2Fs10994-010-5229-0.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-12-22",
            "journal": {
                "name": "Machine Learning",
                "volume": "84"
            },
            "citationStyles": {
                "bibtex": "@Article{Shortreed2010InformingSC,\n author = {S. Shortreed and Eric B. Laber and D. Lizotte and T. Stroup and Joelle Pineau and S. Murphy},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {109-136},\n title = {Informing sequential clinical decision-making through\u00a0reinforcement learning: an empirical study},\n volume = {84},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6f55f2234a002c69001df8ef9108cb86f1dfa506",
            "@type": "ScholarlyArticle",
            "paperId": "6f55f2234a002c69001df8ef9108cb86f1dfa506",
            "corpusId": 49559279,
            "url": "https://www.semanticscholar.org/paper/6f55f2234a002c69001df8ef9108cb86f1dfa506",
            "title": "Accurate Uncertainties for Deep Learning Using Calibrated Regression",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2810469995",
                "ArXiv": "1807.00263",
                "DBLP": "conf/icml/KuleshovFE18",
                "CorpusId": 49559279
            },
            "abstract": "Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate -- for example, a 90% credible interval may not contain the true outcome 90% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.",
            "referenceCount": 30,
            "citationCount": 485,
            "influentialCitationCount": 71,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-07-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1807.00263"
            },
            "citationStyles": {
                "bibtex": "@Article{Kuleshov2018AccurateUF,\n author = {Volodymyr Kuleshov and Nathan Fenner and Stefano Ermon},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Accurate Uncertainties for Deep Learning Using Calibrated Regression},\n volume = {abs/1807.00263},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:47632b66387d00d19b66e71560ba462847b78006",
            "@type": "ScholarlyArticle",
            "paperId": "47632b66387d00d19b66e71560ba462847b78006",
            "corpusId": 198950131,
            "url": "https://www.semanticscholar.org/paper/47632b66387d00d19b66e71560ba462847b78006",
            "title": "Coordinated Deep Reinforcement Learners for Traffic Light Control",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "CorpusId": 198950131
            },
            "abstract": "This paper investigates learning control policies for traffic lights. We introduce a new reward function for the traffic light control problem, and propose the combination of the popular Deep Q-learning algorithm with a coordination algorithm for a scalable approach to controlling coordinating traffic lights, without requiring the simplifying assumptions made in earlier work. We show that this approach reduces travel times compared to earlier work on reinforcement learning methods for traffic light control and investigate possible causes of instability in the single-agent case.",
            "referenceCount": 22,
            "citationCount": 286,
            "influentialCitationCount": 30,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Pol2016CoordinatedDR,\n author = {Elise van der Pol and F. Oliehoek},\n title = {Coordinated Deep Reinforcement Learners for Traffic Light Control},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d358d41c69450b171327ebd99462b6afef687269",
            "@type": "ScholarlyArticle",
            "paperId": "d358d41c69450b171327ebd99462b6afef687269",
            "corpusId": 890737,
            "url": "https://www.semanticscholar.org/paper/d358d41c69450b171327ebd99462b6afef687269",
            "title": "Continuous Deep Q-Learning with Model-based Acceleration",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1603.00748",
                "DBLP": "journals/corr/GuLSL16",
                "MAG": "2290354866",
                "CorpusId": 890737
            },
            "abstract": "Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of modelfree algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized advantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.",
            "referenceCount": 41,
            "citationCount": 894,
            "influentialCitationCount": 87,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-03-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Gu2016ContinuousDQ,\n author = {S. Gu and T. Lillicrap and Ilya Sutskever and S. Levine},\n booktitle = {International Conference on Machine Learning},\n pages = {2829-2838},\n title = {Continuous Deep Q-Learning with Model-based Acceleration},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:274f4ad2f9be649c297eba6bffa97540599569df",
            "@type": "ScholarlyArticle",
            "paperId": "274f4ad2f9be649c297eba6bffa97540599569df",
            "corpusId": 2988264,
            "url": "https://www.semanticscholar.org/paper/274f4ad2f9be649c297eba6bffa97540599569df",
            "title": "Bayesian Multi-Task Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2134197408",
                "DBLP": "conf/icml/LazaricG10",
                "CorpusId": 2988264
            },
            "abstract": "We consider the problem of multi-task reinforcement learning where the learner is provided with a set of tasks, for which only a small number of samples can be generated for any given policy. As the number of samples may not be enough to learn an accurate evaluation of the policy, it would be necessary to identify classes of tasks with similar structure and to learn them jointly. We consider the case where the tasks share structure in their value functions, and model this by assuming that the value functions are all sampled from a common prior. We adopt the Gaussian process temporal-difference value function model and use a hierarchical Bayesian approach to model the distribution over the value functions. We study two cases, where all the value functions belong to the same class and where they belong to an undefined number of classes. For each case, we present a hierarchical Bayesian model, and derive inference algorithms for (i) joint learning of the value functions, and (ii) efficient transfer of the information gained in (i) to assist learning the value function of a newly observed task.",
            "referenceCount": 20,
            "citationCount": 117,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-06-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lazaric2010BayesianMR,\n author = {A. Lazaric and M. Ghavamzadeh},\n booktitle = {International Conference on Machine Learning},\n pages = {599-606},\n title = {Bayesian Multi-Task Reinforcement Learning},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:07b2c10ad4d5f608eb0a535187d2b851cc1d2e15",
            "@type": "ScholarlyArticle",
            "paperId": "07b2c10ad4d5f608eb0a535187d2b851cc1d2e15",
            "corpusId": 1060023,
            "url": "https://www.semanticscholar.org/paper/07b2c10ad4d5f608eb0a535187d2b851cc1d2e15",
            "title": "Inverse Reinforcement Learning",
            "venue": "Encyclopedia of Machine Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "reference/ml/AbbeelN10",
                "DOI": "10.1007/978-0-387-30164-8_417",
                "CorpusId": 1060023
            },
            "abstract": null,
            "referenceCount": 6,
            "citationCount": 81,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.ias.informatik.tu-darmstadt.de/uploads/Teaching/AutonomousLearningSystems/Fischer_ALS_2012.pdf",
                "status": "CLOSED"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Abbeel2010InverseRL,\n author = {P. Abbeel and Andrew Y. Ng},\n booktitle = {Encyclopedia of Machine Learning},\n pages = {554-558},\n title = {Inverse Reinforcement Learning},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c3c5947ef7d92b9bb10a35f84368d624dbe837af",
            "@type": "ScholarlyArticle",
            "paperId": "c3c5947ef7d92b9bb10a35f84368d624dbe837af",
            "corpusId": 3440496,
            "url": "https://www.semanticscholar.org/paper/c3c5947ef7d92b9bb10a35f84368d624dbe837af",
            "title": "Multi-Agent Inverse Reinforcement Learning",
            "venue": "2010 Ninth International Conference on Machine Learning and Applications",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2124394479",
                "DBLP": "conf/icmla/NatarajanKJTKS10",
                "DOI": "10.1109/ICMLA.2010.65",
                "CorpusId": 3440496
            },
            "abstract": "Learning the reward function of an agent by observing its behavior is termed inverse reinforcement learning and has applications in learning from demonstration or apprenticeship learning. We introduce the problem of multi-agent inverse reinforcement learning, where reward functions of multiple agents are learned by observing their uncoordinated behavior. A centralized controller then learns to coordinate their behavior by optimizing a weighted sum of reward functions of all the agents. We evaluate our approach on a traffic-routing domain, in which a controller coordinates actions of multiple traffic signals to regulate traffic density. We show that the learner is not only able to match but even significantly outperform the expert.",
            "referenceCount": 11,
            "citationCount": 77,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://ftp.cs.wisc.edu/machine-learning/shavlik-group/natarajan.icmla10.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-12-12",
            "journal": {
                "name": "2010 Ninth International Conference on Machine Learning and Applications",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Natarajan2010MultiAgentIR,\n author = {Sriraam Natarajan and Gautam Kunapuli and Kshitij Judah and Prasad Tadepalli and K. Kersting and J. Shavlik},\n booktitle = {2010 Ninth International Conference on Machine Learning and Applications},\n journal = {2010 Ninth International Conference on Machine Learning and Applications},\n pages = {395-400},\n title = {Multi-Agent Inverse Reinforcement Learning},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:064dec29989aafd3b95aa0eff261556ba70dd363",
            "@type": "ScholarlyArticle",
            "paperId": "064dec29989aafd3b95aa0eff261556ba70dd363",
            "corpusId": 40735613,
            "url": "https://www.semanticscholar.org/paper/064dec29989aafd3b95aa0eff261556ba70dd363",
            "title": "Bayesian Reinforcement Learning",
            "venue": "Encyclopedia of Machine Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "books/sp/12/VlassisGMP12",
                "MAG": "1862757251",
                "DOI": "10.1007/978-0-387-30164-8_67",
                "CorpusId": 40735613
            },
            "abstract": null,
            "referenceCount": 84,
            "citationCount": 78,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://hal.inria.fr/hal-00840479/file/BRLchapter.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Vlassis2010BayesianRL,\n author = {N. Vlassis and M. Ghavamzadeh and Shie Mannor and P. Poupart},\n booktitle = {Encyclopedia of Machine Learning},\n pages = {359-386},\n title = {Bayesian Reinforcement Learning},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:59bf2a4efdd6fce10f9d0e37ffc8ef689e35f315",
            "@type": "ScholarlyArticle",
            "paperId": "59bf2a4efdd6fce10f9d0e37ffc8ef689e35f315",
            "corpusId": 9123356,
            "url": "https://www.semanticscholar.org/paper/59bf2a4efdd6fce10f9d0e37ffc8ef689e35f315",
            "title": "Reinforcement learning in the brain",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2081030963",
                "DOI": "10.1016/J.JMP.2008.12.005",
                "CorpusId": 9123356
            },
            "abstract": null,
            "referenceCount": 181,
            "citationCount": 625,
            "influentialCitationCount": 49,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.princeton.edu/%7Eyael/Publications/Niv2009.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2009-06-01",
            "journal": {
                "name": "Journal of Mathematical Psychology",
                "volume": "53"
            },
            "citationStyles": {
                "bibtex": "@Article{Niv2009ReinforcementLI,\n author = {Y. Niv},\n journal = {Journal of Mathematical Psychology},\n pages = {139-154},\n title = {Reinforcement learning in the brain},\n volume = {53},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ed0eb0ea2e3d8a5b266e213ee8f2677c1f5c05ea",
            "@type": "ScholarlyArticle",
            "paperId": "ed0eb0ea2e3d8a5b266e213ee8f2677c1f5c05ea",
            "corpusId": 14805649,
            "url": "https://www.semanticscholar.org/paper/ed0eb0ea2e3d8a5b266e213ee8f2677c1f5c05ea",
            "title": "Residential Demand Response Using Reinforcement Learning",
            "venue": "2010 First IEEE International Conference on Smart Grid Communications",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2012160694",
                "DOI": "10.1109/SMARTGRID.2010.5622078",
                "CorpusId": 14805649
            },
            "abstract": "We present a novel energy management system for residential demand response. The algorithm, named CAES, reduces residential energy costs and smooths energy usage. CAES is an online learning application that implicitly estimates the impact of future energy prices and of consumer decisions on long term costs and schedules residential device usage. CAES models both energy prices and residential device usage as Markov, but does not assume knowledge of the structure or transition probabilities of these Markov chains. CAES learns continuously and adapts to individual consumer preferences and pricing modifications over time. In numerical simulations CAES reduced average end-user financial costs from $16\\%$ to $40\\%$ with respect to a price-unaware energy allocation.",
            "referenceCount": 20,
            "citationCount": 291,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference"
            ],
            "publicationDate": "2010-11-04",
            "journal": {
                "name": "2010 First IEEE International Conference on Smart Grid Communications",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Conference{O'Neill2010ResidentialDR,\n author = {D. O'Neill and M. Levorato and A. Goldsmith and U. Mitra},\n booktitle = {2010 First IEEE International Conference on Smart Grid Communications},\n journal = {2010 First IEEE International Conference on Smart Grid Communications},\n pages = {409-414},\n title = {Residential Demand Response Using Reinforcement Learning},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7ca8ac34767d6e6cb389eeebcdabc4225b39edfe",
            "@type": "ScholarlyArticle",
            "paperId": "7ca8ac34767d6e6cb389eeebcdabc4225b39edfe",
            "corpusId": 10253791,
            "url": "https://www.semanticscholar.org/paper/7ca8ac34767d6e6cb389eeebcdabc4225b39edfe",
            "title": "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1995,
            "externalIds": {
                "MAG": "2124175081",
                "DBLP": "conf/nips/Sutton95",
                "CorpusId": 10253791
            },
            "abstract": "On large problems, reinforcement learning systems must use parameterized function approximators such as neural networks in order to generalize between similar situations and actions. In these cases there are no strong theoretical results on the accuracy of convergence, and computational results have been mixed. In particular, Boyan and Moore reported at last year's meeting a series of negative results in attempting to apply dynamic programming together with function approximation to simple control problems with continuous state spaces. In this paper, we present positive results for all the control tasks they attempted, and for one that is significantly larger. The most important differences are that we used sparse-coarse-coded function approximators (CMACs) whereas they used mostly global function approximators, and that we learned online whereas they learned offline. Boyan and Moore and others have suggested that the problems they encountered could be solved by using actual outcomes (\"rollouts\"), as in classical Monte Carlo methods, and as in the TD(\u03bb) algorithm when \u03bb = 1. However, in our experiments this always resulted in substantially poorer performance. We conclude that reinforcement learning can work robustly in conjunction with function approximators, and that there is little justification at present for avoiding the case of general \u03bb.",
            "referenceCount": 23,
            "citationCount": 1095,
            "influentialCitationCount": 113,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1995-11-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sutton1995GeneralizationIR,\n author = {Richard S. Sutton},\n booktitle = {Neural Information Processing Systems},\n pages = {1038-1044},\n title = {Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f82e4ff4f003581330338aaae71f60316e58dd26",
            "@type": "ScholarlyArticle",
            "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
            "corpusId": 1552061,
            "url": "https://www.semanticscholar.org/paper/f82e4ff4f003581330338aaae71f60316e58dd26",
            "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2404342324",
                "ArXiv": "1207.4708",
                "DBLP": "journals/corr/abs-1207-4708",
                "DOI": "10.1613/jair.3912",
                "CorpusId": 1552061
            },
            "abstract": "In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.",
            "referenceCount": 40,
            "citationCount": 2588,
            "influentialCitationCount": 457,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jair.org/index.php/jair/article/download/10819/25823",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2012-07-19",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bellemare2012TheAL,\n author = {Marc G. Bellemare and Yavar Naddaf and J. Veness and Michael Bowling},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {4148-4152},\n title = {The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dc649486b881e672eea6546da48c46e1f98daf32",
            "@type": "ScholarlyArticle",
            "paperId": "dc649486b881e672eea6546da48c46e1f98daf32",
            "corpusId": 2695116,
            "url": "https://www.semanticscholar.org/paper/dc649486b881e672eea6546da48c46e1f98daf32",
            "title": "Near-Optimal Reinforcement Learning in Polynomial Time",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1998,
            "externalIds": {
                "DBLP": "conf/icml/KearnsS98",
                "MAG": "3023407077",
                "DOI": "10.1023/A:1017984413808",
                "CorpusId": 2695116
            },
            "abstract": null,
            "referenceCount": 39,
            "citationCount": 1088,
            "influentialCitationCount": 86,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1017984413808.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1998-07-24",
            "journal": {
                "name": "Machine Learning",
                "volume": "49"
            },
            "citationStyles": {
                "bibtex": "@Article{Kearns1998NearOptimalRL,\n author = {Michael Kearns and Satinder Singh},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {209-232},\n title = {Near-Optimal Reinforcement Learning in Polynomial Time},\n volume = {49},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:04162cb8cfaa0f7e37586823ff4ad0bff09ed21d",
            "@type": "ScholarlyArticle",
            "paperId": "04162cb8cfaa0f7e37586823ff4ad0bff09ed21d",
            "corpusId": 8121626,
            "url": "https://www.semanticscholar.org/paper/04162cb8cfaa0f7e37586823ff4ad0bff09ed21d",
            "title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2963590100",
                "DBLP": "journals/corr/FinnLA16",
                "ArXiv": "1603.00448",
                "CorpusId": 8121626
            },
            "abstract": "Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.",
            "referenceCount": 32,
            "citationCount": 809,
            "influentialCitationCount": 87,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-03-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Finn2016GuidedCL,\n author = {Chelsea Finn and S. Levine and P. Abbeel},\n booktitle = {International Conference on Machine Learning},\n pages = {49-58},\n title = {Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3684aef466e06bfaf417c1641948bb6e670e158f",
            "@type": "ScholarlyArticle",
            "paperId": "3684aef466e06bfaf417c1641948bb6e670e158f",
            "corpusId": 37048217,
            "url": "https://www.semanticscholar.org/paper/3684aef466e06bfaf417c1641948bb6e670e158f",
            "title": "Deep Learning-Based Numerical Methods for High-Dimensional Parabolic Partial Differential Equations and Backward Stochastic Differential Equations",
            "venue": "Communications in Mathematics and Statistics",
            "publicationVenue": {
                "id": "urn:research:f2ccdde5-01ea-4801-8a2d-35d9889dac47",
                "name": "Communications in Mathematics and Statistics",
                "alternate_names": [
                    "Commun Math Stat"
                ],
                "issn": "2194-671X",
                "url": "https://link.springer.com/journal/40304"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2953091395",
                "DBLP": "journals/corr/EHJ17",
                "ArXiv": "1706.04702",
                "DOI": "10.1007/s40304-017-0117-6",
                "CorpusId": 37048217
            },
            "abstract": null,
            "referenceCount": 35,
            "citationCount": 636,
            "influentialCitationCount": 46,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1706.04702",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-06-15",
            "journal": {
                "name": "Communications in Mathematics and Statistics",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{E2017DeepLN,\n author = {W. E and Jiequn Han and Arnulf Jentzen},\n booktitle = {Communications in Mathematics and Statistics},\n journal = {Communications in Mathematics and Statistics},\n pages = {349 - 380},\n title = {Deep Learning-Based Numerical Methods for High-Dimensional Parabolic Partial Differential Equations and Backward Stochastic Differential Equations},\n volume = {5},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fd62bc380b66500c31a8f1a8b566bcaea25d1652",
            "@type": "ScholarlyArticle",
            "paperId": "fd62bc380b66500c31a8f1a8b566bcaea25d1652",
            "corpusId": 5649130,
            "url": "https://www.semanticscholar.org/paper/fd62bc380b66500c31a8f1a8b566bcaea25d1652",
            "title": "Bayesian Inverse Reinforcement Learning",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "conf/ijcai/RamachandranA07",
                "MAG": "1591675293",
                "CorpusId": 5649130
            },
            "abstract": "Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.",
            "referenceCount": 19,
            "citationCount": 739,
            "influentialCitationCount": 145,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2007-01-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ramachandran2007BayesianIR,\n author = {Deepak Ramachandran and Eyal Amir},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {2586-2591},\n title = {Bayesian Inverse Reinforcement Learning},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:770990076b4dd6bd9078ec6fe3d9680bcf766dde",
            "@type": "ScholarlyArticle",
            "paperId": "770990076b4dd6bd9078ec6fe3d9680bcf766dde",
            "corpusId": 52164670,
            "url": "https://www.semanticscholar.org/paper/770990076b4dd6bd9078ec6fe3d9680bcf766dde",
            "title": "IoT Security Techniques Based on Machine Learning: How Do IoT Devices Use AI to Enhance Security?",
            "venue": "IEEE Signal Processing Magazine",
            "publicationVenue": {
                "id": "urn:research:f62e5eab-173a-4e0a-a963-ed8de9835d22",
                "name": "IEEE Signal Processing Magazine",
                "alternate_names": [
                    "IEEE Signal Process Mag"
                ],
                "issn": "1053-5888",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=79"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1801-06275",
                "MAG": "2892077825",
                "ArXiv": "1801.06275",
                "DOI": "10.1109/MSP.2018.2825478",
                "CorpusId": 52164670
            },
            "abstract": "The Internet of things (IoT), which integrates a variety of devices into networks to provide advanced and intelligent services, has to protect user privacy and address attacks such as spoofing attacks, denial of service (DoS) attacks, jamming, and eavesdropping. We investigate the attack model for IoT systems and review the IoT security solutions based on machine-learning (ML) techniques including supervised learning, unsupervised learning, and reinforcement learning (RL). ML-based IoT authentication, access control, secure offloading, and malware detection schemes to protect data privacy are the focus of this article. We also discuss the challenges that need to be addressed to implement these ML-based security schemes in practical IoT systems.",
            "referenceCount": 37,
            "citationCount": 464,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-01-19",
            "journal": {
                "name": "IEEE Signal Processing Magazine",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{Xiao2018IoTST,\n author = {Liang Xiao and Xiaoyue Wan and Xiaozhen Lu and Yanyong Zhang and Di Wu},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Processing Magazine},\n pages = {41-49},\n title = {IoT Security Techniques Based on Machine Learning: How Do IoT Devices Use AI to Enhance Security?},\n volume = {35},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c09a440abdc92d187c53812d8aa29ed7de29a37b",
            "@type": "ScholarlyArticle",
            "paperId": "c09a440abdc92d187c53812d8aa29ed7de29a37b",
            "corpusId": 49558891,
            "url": "https://www.semanticscholar.org/paper/c09a440abdc92d187c53812d8aa29ed7de29a37b",
            "title": "Learning to Drive in a Day",
            "venue": "IEEE International Conference on Robotics and Automation",
            "publicationVenue": {
                "id": "urn:research:3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                "name": "IEEE International Conference on Robotics and Automation",
                "alternate_names": [
                    "International Conference on Robotics and Automation",
                    "Int Conf Robot Autom",
                    "ICRA",
                    "IEEE Int Conf Robot Autom"
                ],
                "issn": "2152-4092",
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2968983352",
                "ArXiv": "1807.00412",
                "DBLP": "journals/corr/abs-1807-00412",
                "DOI": "10.1109/ICRA.2019.8793742",
                "CorpusId": 49558891
            },
            "abstract": "We demonstrate the first application of deep reinforcement learning to autonomous driving. From randomly initialised parameters, our model is able to learn a policy for lane following in a handful of training episodes using a single monocular image as input. We provide a general and easy to obtain reward: the distance travelled by the vehicle without the safety driver taking control. We use a continuous, model-free deep reinforcement learning algorithm, with all exploration and optimisation performed on-vehicle. This demonstrates a new framework for autonomous driving which moves away from reliance on defined logical rules, mapping, and direct supervision. We discuss the challenges and opportunities to scale this approach to a broader range of autonomous driving tasks.",
            "referenceCount": 44,
            "citationCount": 453,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-07-01",
            "journal": {
                "name": "2019 International Conference on Robotics and Automation (ICRA)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kendall2018LearningTD,\n author = {Alex Kendall and Jeffrey Hawke and David Janz and Przemyslaw Mazur and Daniele Reda and John M. Allen and Vinh-Dieu Lam and A. Bewley and Amar Shah},\n booktitle = {IEEE International Conference on Robotics and Automation},\n journal = {2019 International Conference on Robotics and Automation (ICRA)},\n pages = {8248-8254},\n title = {Learning to Drive in a Day},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d35b05f440b5ba00d9429139edef7182bf9f7ce7",
            "@type": "ScholarlyArticle",
            "paperId": "d35b05f440b5ba00d9429139edef7182bf9f7ce7",
            "corpusId": 13298214,
            "url": "https://www.semanticscholar.org/paper/d35b05f440b5ba00d9429139edef7182bf9f7ce7",
            "title": "Learning to Navigate in Complex Environments",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2952791429",
                "DBLP": "journals/corr/MirowskiPVSBBDG16",
                "ArXiv": "1611.03673",
                "CorpusId": 13298214
            },
            "abstract": "Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.",
            "referenceCount": 34,
            "citationCount": 775,
            "influentialCitationCount": 46,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-11",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.03673"
            },
            "citationStyles": {
                "bibtex": "@Article{Mirowski2016LearningTN,\n author = {Piotr Wojciech Mirowski and Razvan Pascanu and Fabio Viola and Hubert Soyer and Andy Ballard and Andrea Banino and Misha Denil and Ross Goroshin and L. Sifre and K. Kavukcuoglu and D. Kumaran and R. Hadsell},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning to Navigate in Complex Environments},\n volume = {abs/1611.03673},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6f144fde2fed74566180b5d35f2c7ed00d920401",
            "@type": "ScholarlyArticle",
            "paperId": "6f144fde2fed74566180b5d35f2c7ed00d920401",
            "corpusId": 13090656,
            "url": "https://www.semanticscholar.org/paper/6f144fde2fed74566180b5d35f2c7ed00d920401",
            "title": "Reinforcement Learning Signal Predicts Social Conformity",
            "venue": "Neuron",
            "publicationVenue": {
                "id": "urn:research:7a61412a-9a9a-487d-9613-5a1fbd879c9d",
                "name": "Neuron",
                "alternate_names": null,
                "issn": "0896-6273",
                "url": "https://www.cell.com/neuron/home"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2114905176",
                "DOI": "10.1016/j.neuron.2008.11.027",
                "CorpusId": 13090656,
                "PubMed": "19146819"
            },
            "abstract": null,
            "referenceCount": 64,
            "citationCount": 509,
            "influentialCitationCount": 53,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cell.com/article/S0896627308010209/pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-01-15",
            "journal": {
                "name": "Neuron",
                "volume": "61"
            },
            "citationStyles": {
                "bibtex": "@Article{Klucharev2009ReinforcementLS,\n author = {V. Klucharev and Kaisa Hyt\u00f6nen and M. Rijpkema and A. Smidts and G. Fern\u00e1ndez},\n booktitle = {Neuron},\n journal = {Neuron},\n pages = {140-151},\n title = {Reinforcement Learning Signal Predicts Social Conformity},\n volume = {61},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:14b287bf01bc1cfc656be1493aadd21cca3ae8d3",
            "@type": "ScholarlyArticle",
            "paperId": "14b287bf01bc1cfc656be1493aadd21cca3ae8d3",
            "corpusId": 16771371,
            "url": "https://www.semanticscholar.org/paper/14b287bf01bc1cfc656be1493aadd21cca3ae8d3",
            "title": "Imitation and Reinforcement Learning",
            "venue": "IEEE robotics & automation magazine",
            "publicationVenue": {
                "id": "urn:research:bb803f8e-3f8e-4fd1-8192-391b7d4de1f1",
                "name": "IEEE robotics & automation magazine",
                "alternate_names": [
                    "IEEE robot  autom mag",
                    "IEEE Robotics & Automation Magazine",
                    "IEEE Robot  Autom Mag"
                ],
                "issn": "1070-9932",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=100"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "journals/ram/KoberP10",
                "MAG": "2071841410",
                "DOI": "10.1109/MRA.2010.936952",
                "CorpusId": 16771371
            },
            "abstract": "In this article, we present both novel learning algorithms and experiments using the dynamical system MPs. As such, we describe this MP representation in a way that it is straightforward to reproduce. We review an appropriate imitation learning method, i.e., locally weighted regression, and show how this method can be used both for initializing RL tasks as well as for modifying the start-up phase in a rhythmic task. We also show our current best-suited RL algorithm for this framework, i.e., PoWER. We present two complex motor tasks, i.e., ball-in-a-cup and ball paddling, learned on a real, physical Barrett WAM, using the methods presented in this article. Of particular interest is the ball-paddling application, as it requires a combination of both rhythmic and discrete dynamical systems MPs during the start-up phase to achieve a particular task.",
            "referenceCount": 30,
            "citationCount": 128,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2010-06-07",
            "journal": {
                "name": "IEEE Robotics & Automation Magazine",
                "volume": "17"
            },
            "citationStyles": {
                "bibtex": "@Article{Kober2010ImitationAR,\n author = {J. Kober and Jan Peters},\n booktitle = {IEEE robotics & automation magazine},\n journal = {IEEE Robotics & Automation Magazine},\n pages = {55-62},\n title = {Imitation and Reinforcement Learning},\n volume = {17},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5b53951625e4b39cef0fd9920ccd652fa3059db2",
            "@type": "ScholarlyArticle",
            "paperId": "5b53951625e4b39cef0fd9920ccd652fa3059db2",
            "corpusId": 6652458,
            "url": "https://www.semanticscholar.org/paper/5b53951625e4b39cef0fd9920ccd652fa3059db2",
            "title": "Reinforcement Learning in Continuous Time and Space",
            "venue": "Neural Computation",
            "publicationVenue": {
                "id": "urn:research:69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                "name": "Neural Computation",
                "alternate_names": [
                    "Neural Comput"
                ],
                "issn": "0899-7667",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2113501460",
                "DBLP": "journals/neco/Doya00",
                "DOI": "10.1162/089976600300015961",
                "CorpusId": 6652458,
                "PubMed": "10636940"
            },
            "abstract": "This article presents a reinforcement learning framework for continuous-time dynamical systems without a priori discretization of time, state, and action. Basedonthe Hamilton-Jacobi-Bellman (HJB) equation for infinite-horizon, discounted reward problems, we derive algorithms for estimating value functions and improving policies with the use of function approximators. The process of value function estimation is formulated as the minimization of a continuous-time form of the temporal difference (TD) error. Update methods based on backward Euler approximation and exponential eligibility traces are derived, and their correspondences with the conventional residual gradient, TD (0), and TD () algorithms are shown. For policy improvement, two methodsa continuous actor-critic method and a value-gradient-based greedy policyare formulated. As a special case of the latter, a nonlinear feedback control law using the value gradient and the model of the input gain is derived. The advantage updating, a model-free algorithm derived previously, is also formulated in the HJB-based framework. The performance of the proposed algorithms is first tested in a nonlinear control task of swinging a pendulum up with limited torque. It is shown in the simulations that (1) the task is accomplished by the continuous actor-critic method in a number of trials several times fewer than by the conventional discrete actor-critic method; (2) among the continuous policy update methods, the value-gradient-based policy with a known or learned dynamic model performs several times better than the actor-critic method; and (3) a value function update using exponential eligibility traces is more efficient and stable than that based on Euler approximation. The algorithms are then tested in a higher-dimensional task: cart-pole swing-up. This task is accomplished in several hundred trials using the value-gradient-based policy with a learned dynamic model.",
            "referenceCount": 47,
            "citationCount": 948,
            "influentialCitationCount": 94,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Neural Computation",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Doya2000ReinforcementLI,\n author = {K. Doya},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {219-245},\n title = {Reinforcement Learning in Continuous Time and Space},\n volume = {12},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:523b4ce1c2a1336962444abc1dec215756c2f3e6",
            "@type": "ScholarlyArticle",
            "paperId": "523b4ce1c2a1336962444abc1dec215756c2f3e6",
            "corpusId": 31442909,
            "url": "https://www.semanticscholar.org/paper/523b4ce1c2a1336962444abc1dec215756c2f3e6",
            "title": "Approximately Optimal Approximate Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2002,
            "externalIds": {
                "DBLP": "conf/icml/KakadeL02",
                "MAG": "1575592356",
                "CorpusId": 31442909
            },
            "abstract": null,
            "referenceCount": 1,
            "citationCount": 879,
            "influentialCitationCount": 160,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2002-07-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kakade2002ApproximatelyOA,\n author = {S. Kakade and J. Langford},\n booktitle = {International Conference on Machine Learning},\n pages = {267-274},\n title = {Approximately Optimal Approximate Reinforcement Learning},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e07da2914783a21980ecc1cea688d1333e6b6e4",
            "@type": "ScholarlyArticle",
            "paperId": "5e07da2914783a21980ecc1cea688d1333e6b6e4",
            "corpusId": 3853400,
            "url": "https://www.semanticscholar.org/paper/5e07da2914783a21980ecc1cea688d1333e6b6e4",
            "title": "Reinforcement Learning or Active Inference?",
            "venue": "PLoS ONE",
            "publicationVenue": {
                "id": "urn:research:0aed7a40-85f3-4c66-9e1b-c1556c57001b",
                "name": "PLoS ONE",
                "alternate_names": [
                    "Plo ONE",
                    "PLOS ONE",
                    "PLO ONE"
                ],
                "issn": "1932-6203",
                "url": "https://journals.plos.org/plosone/"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2137411342",
                "PubMedCentral": "2713351",
                "DOI": "10.1371/journal.pone.0006421",
                "CorpusId": 3853400,
                "PubMed": "19641614"
            },
            "abstract": "This paper questions the need for reinforcement learning or control theory when optimising behaviour. We show that it is fairly simple to teach an agent complicated and adaptive behaviours using a free-energy formulation of perception. In this formulation, agents adjust their internal states and sampling of the environment to minimize their free-energy. Such agents learn causal structure in the environment and sample it in an adaptive and self-supervised fashion. This results in behavioural policies that reproduce those optimised by reinforcement learning and dynamic programming. Critically, we do not need to invoke the notion of reward, value or utility. We illustrate these points by solving a benchmark problem in dynamic programming; namely the mountain-car problem, using active perception or inference under the free-energy principle. The ensuing proof-of-concept may be important because the free-energy formulation furnishes a unified account of both action and perception and may speak to a reappraisal of the role of dopamine in the brain.",
            "referenceCount": 68,
            "citationCount": 373,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0006421&type=printable",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Physics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-07-29",
            "journal": {
                "name": "PLoS ONE",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Friston2009ReinforcementLO,\n author = {Karl J. Friston and J. Daunizeau and S. Kiebel},\n booktitle = {PLoS ONE},\n journal = {PLoS ONE},\n title = {Reinforcement Learning or Active Inference?},\n volume = {4},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cab81775baae7ba2d056ebbc60437f2e03358ca3",
            "@type": "ScholarlyArticle",
            "paperId": "cab81775baae7ba2d056ebbc60437f2e03358ca3",
            "corpusId": 3562704,
            "url": "https://www.semanticscholar.org/paper/cab81775baae7ba2d056ebbc60437f2e03358ca3",
            "title": "Learning by Playing - Solving Sparse Reward Tasks from Scratch",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/icml/RiedmillerHLNDW18",
                "MAG": "2950462477",
                "ArXiv": "1802.10567",
                "CorpusId": 3562704
            },
            "abstract": "We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.",
            "referenceCount": 54,
            "citationCount": 376,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-02-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Riedmiller2018LearningBP,\n author = {Martin A. Riedmiller and Roland Hafner and Thomas Lampe and Michael Neunert and Jonas Degrave and T. Wiele and Volodymyr Mnih and N. Heess and J. T. Springenberg},\n booktitle = {International Conference on Machine Learning},\n pages = {4341-4350},\n title = {Learning by Playing - Solving Sparse Reward Tasks from Scratch},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:98ea4abc9bf0e30eb020db2075c9c8a039a848a3",
            "@type": "ScholarlyArticle",
            "paperId": "98ea4abc9bf0e30eb020db2075c9c8a039a848a3",
            "corpusId": 3130692,
            "url": "https://www.semanticscholar.org/paper/98ea4abc9bf0e30eb020db2075c9c8a039a848a3",
            "title": "Learning to Compose Neural Networks for Question Answering",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:01103732-3808-4930-b8e4-7e9e68d5c68d",
                "name": "North American Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "North Am Chapter Assoc Comput Linguistics",
                    "NAACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/naacl"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1601.01705",
                "MAG": "2963143606",
                "DBLP": "conf/naacl/AndreasRDK16",
                "ACL": "N16-1181",
                "DOI": "10.18653/v1/N16-1181",
                "CorpusId": 3130692
            },
            "abstract": "We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.",
            "referenceCount": 40,
            "citationCount": 536,
            "influentialCitationCount": 51,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/N16-1181.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-01-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1601.01705"
            },
            "citationStyles": {
                "bibtex": "@Article{Andreas2016LearningTC,\n author = {Jacob Andreas and Marcus Rohrbach and Trevor Darrell and D. Klein},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Learning to Compose Neural Networks for Question Answering},\n volume = {abs/1601.01705},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cc1648c91ffda21bbe6e5f08f69c683588fc384c",
            "@type": "ScholarlyArticle",
            "paperId": "cc1648c91ffda21bbe6e5f08f69c683588fc384c",
            "corpusId": 5249151,
            "url": "https://www.semanticscholar.org/paper/cc1648c91ffda21bbe6e5f08f69c683588fc384c",
            "title": "Reinforcement Learning for Mapping Instructions to Actions",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2122223050",
                "DBLP": "conf/acl/BranavanCZB09",
                "ACL": "P09-1010",
                "DOI": "10.3115/1687878.1687892",
                "CorpusId": 5249151
            },
            "abstract": "In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains --- Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.",
            "referenceCount": 23,
            "citationCount": 304,
            "influentialCitationCount": 20,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.5555/1687878.1687892",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2009-08-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Branavan2009ReinforcementLF,\n author = {S. Branavan and Harr Chen and Luke Zettlemoyer and R. Barzilay},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {82-90},\n title = {Reinforcement Learning for Mapping Instructions to Actions},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4a5f0c5c7d1b404cbb3a717e5581d86ff29025de",
            "@type": "ScholarlyArticle",
            "paperId": "4a5f0c5c7d1b404cbb3a717e5581d86ff29025de",
            "corpusId": 15929566,
            "url": "https://www.semanticscholar.org/paper/4a5f0c5c7d1b404cbb3a717e5581d86ff29025de",
            "title": "Reinforcement Learning: A Tutorial Survey and Recent Advances",
            "venue": "INFORMS journal on computing",
            "publicationVenue": {
                "id": "urn:research:b76a26bb-18f2-4155-bb35-7e7de85d01bb",
                "name": "INFORMS journal on computing",
                "alternate_names": [
                    "INFORMS j comput",
                    "Informs J Comput",
                    "Informs Journal on Computing"
                ],
                "issn": "1091-9856",
                "url": "https://www.informs.org/"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "journals/informs/Gosavi09",
                "MAG": "2095487261",
                "DOI": "10.1287/ijoc.1080.0305",
                "CorpusId": 15929566
            },
            "abstract": "In the last few years, reinforcement learning (RL), also called adaptive (or approximate) dynamic programming, has emerged as a powerful tool for solving complex sequential decision-making problems in control theory. Although seminal research in this area was performed in the artificial intelligence (AI) community, more recently it has attracted the attention of optimization theorists because of several noteworthy success stories from operations management. It is on large-scale and complex problems of dynamic optimization, in particular the Markov decision problem (MDP) and its variants, that the power of RL becomes more obvious. It has been known for many years that on large-scale MDPs, the curse of dimensionality and the curse of modeling render classical dynamic programming (DP) ineffective. The excitement in RL stems from its direct attack on these curses, which allows it to solve problems that were considered intractable via classical DP in the past. The success of RL is due to its strong mathematical roots in the principles of DP, Monte Carlo simulation, function approximation, and AI. Topics treated in some detail in this survey are temporal differences, Q-learning, semi-MDPs, and stochastic games. Several recent advances in RL, e.g., policy gradients and hierarchical RL, are covered along with references. Pointers to numerous examples of applications are provided. This overview is aimed at uncovering the mathematical roots of this science so that readers gain a clear understanding of the core concepts and are able to use them in their own research. The survey points to more than 100 references from the literature.",
            "referenceCount": 141,
            "citationCount": 296,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://web.mst.edu/~gosavia/joc.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2009-04-01",
            "journal": {
                "name": "INFORMS J. Comput.",
                "volume": "21"
            },
            "citationStyles": {
                "bibtex": "@Article{Gosavi2009ReinforcementLA,\n author = {A. Gosavi},\n booktitle = {INFORMS journal on computing},\n journal = {INFORMS J. Comput.},\n pages = {178-192},\n title = {Reinforcement Learning: A Tutorial Survey and Recent Advances},\n volume = {21},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bdc5a10aa5805808cfca58ac527ddc23e737bee8",
            "@type": "ScholarlyArticle",
            "paperId": "bdc5a10aa5805808cfca58ac527ddc23e737bee8",
            "corpusId": 2122274,
            "url": "https://www.semanticscholar.org/paper/bdc5a10aa5805808cfca58ac527ddc23e737bee8",
            "title": "Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "conf/nips/KonidarisB09",
                "MAG": "2108535023",
                "CorpusId": 2122274
            },
            "abstract": "We introduce a skill discovery method for reinforcement learning in continuous domains that constructs chains of skills leading to an end-of-task reward. We demonstrate experimentally that it creates appropriate skills and achieves performance benefits in a challenging continuous domain.",
            "referenceCount": 31,
            "citationCount": 295,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2009-12-07",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Konidaris2009SkillDI,\n author = {G. Konidaris and A. Barto},\n booktitle = {Neural Information Processing Systems},\n pages = {1015-1023},\n title = {Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d6a74ea198b8a00692d6b5aeee1a83da0c7ec6df",
            "@type": "ScholarlyArticle",
            "paperId": "d6a74ea198b8a00692d6b5aeee1a83da0c7ec6df",
            "corpusId": 5320751,
            "url": "https://www.semanticscholar.org/paper/d6a74ea198b8a00692d6b5aeee1a83da0c7ec6df",
            "title": "Reinforcement learning can account for associative and perceptual learning on a visual decision task",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2040414277",
                "PubMedCentral": "2674144",
                "DOI": "10.1038/nn.2304",
                "CorpusId": 5320751,
                "PubMed": "19377473"
            },
            "abstract": null,
            "referenceCount": 53,
            "citationCount": 279,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc2674144?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-03-17",
            "journal": {
                "name": "Nature neuroscience",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Law2009ReinforcementLC,\n author = {Chi-Tat Law and J. Gold},\n booktitle = {Nature Neuroscience},\n journal = {Nature neuroscience},\n pages = {655 - 663},\n title = {Reinforcement learning can account for associative and perceptual learning on a visual decision task},\n volume = {12},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e227e9f7a0ffc41bee929d2ee07a76f3b445bdb1",
            "@type": "ScholarlyArticle",
            "paperId": "e227e9f7a0ffc41bee929d2ee07a76f3b445bdb1",
            "corpusId": 17961083,
            "url": "https://www.semanticscholar.org/paper/e227e9f7a0ffc41bee929d2ee07a76f3b445bdb1",
            "title": "Reinforcement learning for robot soccer",
            "venue": "Auton. Robots",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2127412976",
                "DBLP": "journals/arobots/RiedmillerGHL09",
                "DOI": "10.1007/s10514-009-9120-4",
                "CorpusId": 17961083
            },
            "abstract": null,
            "referenceCount": 46,
            "citationCount": 271,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2009-07-01",
            "journal": {
                "name": "Autonomous Robots",
                "volume": "27"
            },
            "citationStyles": {
                "bibtex": "@Article{Riedmiller2009ReinforcementLF,\n author = {Martin A. Riedmiller and T. Gabel and Roland Hafner and S. Lange},\n booktitle = {Auton. Robots},\n journal = {Autonomous Robots},\n pages = {55-73},\n title = {Reinforcement learning for robot soccer},\n volume = {27},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:606c3108fe948d9a8a0da8759f88de4df53b5d94",
            "@type": "ScholarlyArticle",
            "paperId": "606c3108fe948d9a8a0da8759f88de4df53b5d94",
            "corpusId": 1696960,
            "url": "https://www.semanticscholar.org/paper/606c3108fe948d9a8a0da8759f88de4df53b5d94",
            "title": "A Bayesian Sampling Approach to Exploration in Reinforcement Learning",
            "venue": "Conference on Uncertainty in Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:f9af8000-42f8-410d-a622-e8811e41660a",
                "name": "Conference on Uncertainty in Artificial Intelligence",
                "alternate_names": [
                    "Uncertainty in Artificial Intelligence",
                    "UAI",
                    "Conf Uncertain Artif Intell",
                    "Uncertain Artif Intell"
                ],
                "issn": null,
                "url": "http://www.auai.org/"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "conf/uai/AsmuthLLNW09",
                "ArXiv": "1205.2664",
                "MAG": "2950270837",
                "CorpusId": 1696960
            },
            "abstract": "We present a modular approach to reinforcement learning that uses a Bayesian representation of the uncertainty over models. The approach, BOSS (Best of Sampled Set), drives exploration by sampling multiple models from the posterior and selecting actions optimistically. It extends previous work by providing a rule for deciding when to re-sample and how to combine the models. We show that our algorithm achieves near-optimal reward with high probability with a sample complexity that is low relative to the speed at which the posterior distribution converges during learning. We demonstrate that BOSS performs quite favorably compared to state-of-the-art reinforcement-learning approaches and illustrate its flexibility by pairing it with a non-parametric model that generalizes across states.",
            "referenceCount": 19,
            "citationCount": 183,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2009-06-18",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Asmuth2009ABS,\n author = {J. Asmuth and Lihong Li and M. Littman and A. Nouri and D. Wingate},\n booktitle = {Conference on Uncertainty in Artificial Intelligence},\n pages = {19-26},\n title = {A Bayesian Sampling Approach to Exploration in Reinforcement Learning},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:927acc246fcda5c31bc5249cb0f270423fda67df",
            "@type": "ScholarlyArticle",
            "paperId": "927acc246fcda5c31bc5249cb0f270423fda67df",
            "corpusId": 24710563,
            "url": "https://www.semanticscholar.org/paper/927acc246fcda5c31bc5249cb0f270423fda67df",
            "title": "Reinforcement learning design for cancer clinical trials",
            "venue": "Statistics in Medicine",
            "publicationVenue": {
                "id": "urn:research:6c8c2afc-7abf-4f41-a2ea-a673c67ab017",
                "name": "Statistics in Medicine",
                "alternate_names": [
                    "Stat Med"
                ],
                "issn": "0277-6715",
                "url": "http://www.interscience.wiley.com/jpages/0277-6715/"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "phd/basesearch/Zhao09b",
                "MAG": "2156682402",
                "DOI": "10.1002/sim.3720",
                "CorpusId": 24710563,
                "PubMed": "19750510"
            },
            "abstract": "We develop reinforcement learning trials for discovering individualized treatment regimens for life\u2010threatening diseases such as cancer. A temporal\u2010difference learning method called Q\u2010learning is utilized that involves learning an optimal policy from a single training set of finite longitudinal patient trajectories. Approximating the Q\u2010function with time\u2010indexed parameters can be achieved by using support vector regression or extremely randomized trees. Within this framework, we demonstrate that the procedure can extract optimal strategies directly from clinical data without relying on the identification of any accurate mathematical models, unlike approaches based on adaptive design. We show that reinforcement learning has tremendous potential in clinical research because it can select actions that improve outcomes by taking into account delayed effects even when the relationship between actions and outcomes is not fully known. To support our claims, the methodology's practical utility is illustrated in a simulation analysis. In the immediate future, we will apply this general strategy to studying and identifying new treatments for advanced metastatic stage IIIB/IV non\u2010small cell lung cancer, which usually includes multiple lines of chemotherapy treatment. Moreover, there is significant potential of the proposed methodology for developing personalized treatment strategies in other cancers, in cystic fibrosis, and in other life\u2010threatening diseases. Copyright \u00a9 2009 John Wiley & Sons, Ltd.",
            "referenceCount": 97,
            "citationCount": 243,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://cdr.lib.unc.edu/downloads/3x816n42d",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Statistics in Medicine",
                "volume": "28"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhao2009ReinforcementLD,\n author = {Yufan Zhao},\n booktitle = {Statistics in Medicine},\n journal = {Statistics in Medicine},\n title = {Reinforcement learning design for cancer clinical trials},\n volume = {28},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:64f134598245a279441dfe532e2277b0f6dd46fa",
            "@type": "ScholarlyArticle",
            "paperId": "64f134598245a279441dfe532e2277b0f6dd46fa",
            "corpusId": 22743214,
            "url": "https://www.semanticscholar.org/paper/64f134598245a279441dfe532e2277b0f6dd46fa",
            "title": "Reinforcement learning, conditioning, and the brain: Successes and challenges",
            "venue": "Cognitive, Affective, & Behavioral Neuroscience",
            "publicationVenue": {
                "id": "urn:research:9bab859b-5cf9-4dca-a5d7-021c0ad80137",
                "name": "Cognitive, Affective, & Behavioral Neuroscience",
                "alternate_names": [
                    "Cogn Affect  Behav Neurosci"
                ],
                "issn": "1530-7026",
                "url": "http://www.springer.com/psychology/cognitive+psychology/journal/13415"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2027600189",
                "DOI": "10.3758/CABN.9.4.343",
                "CorpusId": 22743214,
                "PubMed": "19897789"
            },
            "abstract": null,
            "referenceCount": 226,
            "citationCount": 185,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.3758/CABN.9.4.343.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2009-12-01",
            "journal": {
                "name": "Cognitive, Affective, & Behavioral Neuroscience",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Maia2009ReinforcementLC,\n author = {T. Maia},\n booktitle = {Cognitive, Affective, & Behavioral Neuroscience},\n journal = {Cognitive, Affective, & Behavioral Neuroscience},\n pages = {343-364},\n title = {Reinforcement learning, conditioning, and the brain: Successes and challenges},\n volume = {9},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8a743261fbebc016a3659494886a6e6b7bf8e0ac",
            "@type": "ScholarlyArticle",
            "paperId": "8a743261fbebc016a3659494886a6e6b7bf8e0ac",
            "corpusId": 3034934,
            "url": "https://www.semanticscholar.org/paper/8a743261fbebc016a3659494886a6e6b7bf8e0ac",
            "title": "Active Learning for Reward Estimation in Inverse Reinforcement Learning",
            "venue": "ECML/PKDD",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2156163138",
                "DBLP": "conf/pkdd/LopesMM09",
                "DOI": "10.1007/978-3-642-04174-7_3",
                "CorpusId": 3034934
            },
            "abstract": null,
            "referenceCount": 15,
            "citationCount": 193,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-642-04174-7_3.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-08-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lopes2009ActiveLF,\n author = {M. Lopes and Francisco S. Melo and L. Montesano},\n booktitle = {ECML/PKDD},\n pages = {31-46},\n title = {Active Learning for Reward Estimation in Inverse Reinforcement Learning},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3227b933d917c73902746ac2c36a72927064a3c0",
            "@type": "ScholarlyArticle",
            "paperId": "3227b933d917c73902746ac2c36a72927064a3c0",
            "corpusId": 10991044,
            "url": "https://www.semanticscholar.org/paper/3227b933d917c73902746ac2c36a72927064a3c0",
            "title": "Learning to Track: Online Multi-object Tracking by Decision Making",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/iccv/XiangAS15",
                "MAG": "2225887246",
                "DOI": "10.1109/ICCV.2015.534",
                "CorpusId": 10991044
            },
            "abstract": "Online Multi-Object Tracking (MOT) has wide applications in time-critical video analysis scenarios, such as robot navigation and autonomous driving. In tracking-by-detection, a major challenge of online MOT is how to robustly associate noisy object detections on a new video frame with previously tracked objects. In this work, we formulate the online MOT problem as decision making in Markov Decision Processes (MDPs), where the lifetime of an object is modeled with a MDP. Learning a similarity function for data association is equivalent to learning a policy for the MDP, and the policy learning is approached in a reinforcement learning fashion which benefits from both advantages of offline-learning and online-learning for data association. Moreover, our framework can naturally handle the birth/death and appearance/disappearance of targets by treating them as state transitions in the MDP while leveraging existing online single object tracking methods. We conduct experiments on the MOT Benchmark [24] to verify the effectiveness of our method.",
            "referenceCount": 40,
            "citationCount": 615,
            "influentialCitationCount": 65,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://infoscience.epfl.ch/record/230283/files/Xiang_Learning_to_Track_ICCV_2015_paper.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-12-07",
            "journal": {
                "name": "2015 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Xiang2015LearningTT,\n author = {Yu Xiang and Alexandre Alahi and S. Savarese},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2015 IEEE International Conference on Computer Vision (ICCV)},\n pages = {4705-4713},\n title = {Learning to Track: Online Multi-object Tracking by Decision Making},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5d8e1eeeb0e4b0e0846a355532d0f9452249e68a",
            "@type": "ScholarlyArticle",
            "paperId": "5d8e1eeeb0e4b0e0846a355532d0f9452249e68a",
            "corpusId": 719934,
            "url": "https://www.semanticscholar.org/paper/5d8e1eeeb0e4b0e0846a355532d0f9452249e68a",
            "title": "Reinforcement Learning in Finite MDPs: PAC Analysis",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "journals/jmlr/StrehlLL09",
                "MAG": "2123447947",
                "DOI": "10.5555/1577069.1755867",
                "CorpusId": 719934
            },
            "abstract": "We study the problem of learning near-optimal behavior in finite Markov Decision Processes (MDPs) with a polynomial number of samples. These \"PAC-MDP\" algorithms include the well-known E3 and R-MAX algorithms as well as the more recent Delayed Q-learning algorithm. We summarize the current state-of-the-art by presenting bounds for the problem in a unified theoretical framework. A more refined analysis for upper and lower bounds is presented to yield insight into the differences between the model-free Delayed Q-learning and the model-based R-MAX.",
            "referenceCount": 48,
            "citationCount": 299,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Strehl2009ReinforcementLI,\n author = {Alexander L. Strehl and Lihong Li and M. Littman},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {2413-2444},\n title = {Reinforcement Learning in Finite MDPs: PAC Analysis},\n volume = {10},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:54c4cf3a8168c1b70f91cf78a3dc98b671935492",
            "@type": "ScholarlyArticle",
            "paperId": "54c4cf3a8168c1b70f91cf78a3dc98b671935492",
            "corpusId": 60875658,
            "url": "https://www.semanticscholar.org/paper/54c4cf3a8168c1b70f91cf78a3dc98b671935492",
            "title": "Reinforcement learning for robots using neural networks",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1992,
            "externalIds": {
                "MAG": "1595483645",
                "CorpusId": 60875658
            },
            "abstract": "Reinforcement learning agents are adaptive, reactive, and self-supervised. The aim of this dissertation is to extend the state of the art of reinforcement learning and enable its applications to complex robot-learning problems. In particular, it focuses on two issues. First, learning from sparse and delayed reinforcement signals is hard and in general a slow process. Techniques for reducing learning time must be devised. Second, most existing reinforcement learning methods assume that the world is a Markov decision process. This assumption is too strong for many robot tasks of interest. \nThis dissertation demonstrates how we can possibly overcome the slow learning problem and tackle non-Markovian environments, making reinforcement learning more practical for realistic robot tasks: (1) Reinforcement learning can be naturally integrated with artificial neural networks to obtain high-quality generalization, resulting in a significant learning speedup. Neural networks are used in this dissertation, and they generalize effectively even in the presence of noise and a large of binary and real-valued inputs. (2) Reinforcement learning agents can save many learning trials by using an action model, which can be learned on-line. With a model, an agent can mentally experience the effects of its actions without actually executing them. Experience replay is a simple technique that implements this idea, and is shown to be effective in reducing the number of action executions required. (3) Reinforcement learning agents can take advantage of instructive training instances provided by human teachers, resulting in a significant learning speedup. Teaching can also help learning agents avoid local optima during the search for optimal control. Simulation experiments indicate that even a small amount of teaching can save agents many learning trials. (4) Reinforcement learning agents can significantly reduce learning time by hierarchical learning--they first solve elementary learning problems and then combine solutions to the elementary problems to solve a complex problem. Simulation experiments indicate that a robot with hierarchical learning can solve a complex problem, which otherwise is hardly solvable within a reasonable time. (5) Reinforcement learning agents can deal with a wide range of non-Markovian environments by having a memory of their past. Three memory architectures are discussed. They work reasonably well for a variety of simple problems. One of them is also successfully applied to a nontrivial non-Markovian robot task. \nThe results of this dissertation rely on computer simulation, including (1) an agent operating in a dynamic and hostile environment and (2) a mobile robot operating in a noisy and non-Markovian environment. The robot simulator is physically realistic. This dissertation concludes that it is possible to build artificial agents than can acquire complex control policies effectively by reinforcement learning.",
            "referenceCount": 73,
            "citationCount": 990,
            "influentialCitationCount": 64,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Lin1992ReinforcementLF,\n author = {Longxin Lin},\n title = {Reinforcement learning for robots using neural networks},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8696c71b3248c07f300bccb0976a73fb0eba86f6",
            "@type": "ScholarlyArticle",
            "paperId": "8696c71b3248c07f300bccb0976a73fb0eba86f6",
            "corpusId": 45970905,
            "url": "https://www.semanticscholar.org/paper/8696c71b3248c07f300bccb0976a73fb0eba86f6",
            "title": "Motivated Reinforcement Learning - Curious Characters for Multiuser Games",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "1584210162",
                "DBLP": "books/daglib/0023633",
                "DOI": "10.1007/978-3-540-89187-1",
                "CorpusId": 45970905
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 99,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2009-06-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Merrick2009MotivatedRL,\n author = {K. Merrick and M. Maher},\n pages = {I-XIV, 1-206},\n title = {Motivated Reinforcement Learning - Curious Characters for Multiuser Games},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6316daea76f71804a0f3cd1e150d4df77a1bf517",
            "@type": "ScholarlyArticle",
            "paperId": "6316daea76f71804a0f3cd1e150d4df77a1bf517",
            "corpusId": 3042664,
            "url": "https://www.semanticscholar.org/paper/6316daea76f71804a0f3cd1e150d4df77a1bf517",
            "title": "The adaptive k-meteorologists problem and its application to structure learning and feature selection in reinforcement learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2112899086",
                "DBLP": "conf/icml/DiukLL09",
                "DOI": "10.1145/1553374.1553406",
                "CorpusId": 3042664
            },
            "abstract": "The purpose of this paper is three-fold. First, we formalize and study a problem of learning probabilistic concepts in the recently proposed KWIK framework. We give details of an algorithm, known as the Adaptive k-Meteorologists Algorithm, analyze its sample-complexity upper bound, and give a matching lower bound. Second, this algorithm is used to create a new reinforcement-learning algorithm for factored-state problems that enjoys significant improvement over the previous state-of-the-art algorithm. Finally, we apply the Adaptive k-Meteorologists Algorithm to remove a limiting assumption in an existing reinforcement-learning algorithm. The effectiveness of our approaches is demonstrated empirically in a couple benchmark domains as well as a robotics navigation problem.",
            "referenceCount": 23,
            "citationCount": 95,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.research.rutgers.edu/~lihong/pub/Diuk09Adaptive.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-06-14",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Diuk2009TheAK,\n author = {Carlos Diuk and Lihong Li and Bethany R. Leffler},\n booktitle = {International Conference on Machine Learning},\n pages = {249-256},\n title = {The adaptive k-meteorologists problem and its application to structure learning and feature selection in reinforcement learning},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:30c6f7725bcd1502ac9de22eb1cfc66426b2855b",
            "@type": "ScholarlyArticle",
            "paperId": "30c6f7725bcd1502ac9de22eb1cfc66426b2855b",
            "corpusId": 774770,
            "url": "https://www.semanticscholar.org/paper/30c6f7725bcd1502ac9de22eb1cfc66426b2855b",
            "title": "Kernelized value function approximation for reinforcement learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "conf/icml/TaylorP09",
                "MAG": "2046513829",
                "DOI": "10.1145/1553374.1553504",
                "CorpusId": 774770
            },
            "abstract": "A recent surge in research in kernelized approaches to reinforcement learning has sought to bring the benefits of kernelized machine learning techniques to reinforcement learning. Kernelized reinforcement learning techniques are fairly new and different authors have approached the topic with different assumptions and goals. Neither a unifying view nor an understanding of the pros and cons of different approaches has yet emerged. In this paper, we offer a unifying view of the different approaches to kernelized value function approximation for reinforcement learning. We show that, except for different approaches to regularization, Kernelized LSTD (KLSTD) is equivalent to a modelbased approach that uses kernelized regression to find an approximate reward and transition model, and that Gaussian Process Temporal Difference learning (GPTD) returns a mean value function that is equivalent to these other approaches. We also discuss the relationship between our modelbased approach and the earlier Gaussian Processes in Reinforcement Learning (GPRL). Finally, we decompose the Bellman error into the sum of transition error and reward error terms, and demonstrate through experiments that this decomposition can be helpful in choosing regularization parameters.",
            "referenceCount": 16,
            "citationCount": 119,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-06-14",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Taylor2009KernelizedVF,\n author = {Gavin Taylor and Ronald E. Parr},\n booktitle = {International Conference on Machine Learning},\n pages = {1017-1024},\n title = {Kernelized value function approximation for reinforcement learning},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0393abe5f707af2fae809ad8dc6079c280a6e49d",
            "@type": "ScholarlyArticle",
            "paperId": "0393abe5f707af2fae809ad8dc6079c280a6e49d",
            "corpusId": 12018227,
            "url": "https://www.semanticscholar.org/paper/0393abe5f707af2fae809ad8dc6079c280a6e49d",
            "title": "Abstraction and Generalization in Reinforcement Learning: A Summary and Framework",
            "venue": "Adaptive and Learning Agents",
            "publicationVenue": {
                "id": "urn:research:49a9818c-00bf-488e-bcfb-9d3e08298a35",
                "name": "Adaptive and Learning Agents",
                "alternate_names": [
                    "ALA",
                    "Adapt Learn Agent"
                ],
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2584801203",
                "DBLP": "conf/atal/PonsenTT09",
                "DOI": "10.1007/978-3-642-11814-2_1",
                "CorpusId": 12018227
            },
            "abstract": null,
            "referenceCount": 57,
            "citationCount": 50,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2009-05-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ponsen2009AbstractionAG,\n author = {M. Ponsen and Matthew E. Taylor and K. Tuyls},\n booktitle = {Adaptive and Learning Agents},\n pages = {1-32},\n title = {Abstraction and Generalization in Reinforcement Learning: A Summary and Framework},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:505808c55b2d96ad72f4b7bca04572655742b87d",
            "@type": "ScholarlyArticle",
            "paperId": "505808c55b2d96ad72f4b7bca04572655742b87d",
            "corpusId": 876231,
            "url": "https://www.semanticscholar.org/paper/505808c55b2d96ad72f4b7bca04572655742b87d",
            "title": "Sim-to-Real Robot Learning from Pixels with Progressive Nets",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1610.04286",
                "MAG": "2534269850",
                "DBLP": "journals/corr/RusuVRHPH16",
                "CorpusId": 876231
            },
            "abstract": "Applying end-to-end learning to solve complex, interactive, pixel-driven control tasks on a robot is an unsolved problem. Deep Reinforcement Learning algorithms are too slow to achieve performance on a real robot, but their potential has been demonstrated in simulated environments. We propose using progressive networks to bridge the reality gap and transfer learned policies from simulation to the real world. The progressive net approach is a general framework that enables reuse of everything from low-level visual features to high-level policies for transfer to new tasks, enabling a compositional, yet simple, approach to building complex skills. We present an early demonstration of this approach with a number of experiments in the domain of robot manipulation that focus on bridging the reality gap. Unlike other proposed approaches, our real-world experiments demonstrate successful task learning from raw visual input on a fully actuated robot manipulator. Moreover, rather than relying on model-based trajectory optimisation, the task learning is accomplished using only deep reinforcement learning and sparse rewards.",
            "referenceCount": 26,
            "citationCount": 478,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-10-13",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1610.04286"
            },
            "citationStyles": {
                "bibtex": "@Article{Rusu2016SimtoRealRL,\n author = {Andrei A. Rusu and Matej Vecer\u00edk and Thomas Roth\u00f6rl and N. Heess and Razvan Pascanu and R. Hadsell},\n booktitle = {Conference on Robot Learning},\n journal = {ArXiv},\n title = {Sim-to-Real Robot Learning from Pixels with Progressive Nets},\n volume = {abs/1610.04286},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7b265d9fa9d4beeb699f592efd73d06694c72df7",
            "@type": "ScholarlyArticle",
            "paperId": "7b265d9fa9d4beeb699f592efd73d06694c72df7",
            "corpusId": 7671810,
            "url": "https://www.semanticscholar.org/paper/7b265d9fa9d4beeb699f592efd73d06694c72df7",
            "title": "Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1513468570",
                "DBLP": "conf/icml/HuW98",
                "CorpusId": 7671810
            },
            "abstract": "In this paper, we adopt general-sum stochastic games as a framework for multiagent reinforcement learning. Our work extends previous work by Littman on zero-sum stochastic games to a broader framework. We design a multiagent Q-learning method under this framework, and prove that it converges to a Nash equilibrium under speci ed conditions. This algorithm is useful for nding the optimal strategy when there exists a unique Nash equilibrium in the game. When there exist multiple Nash equilibria in the game, this algorithm should be combined with other learning techniques to nd optimal strategies.",
            "referenceCount": 15,
            "citationCount": 905,
            "influentialCitationCount": 70,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1998-07-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hu1998MultiagentRL,\n author = {Junling Hu and Michael P. Wellman},\n booktitle = {International Conference on Machine Learning},\n pages = {242-250},\n title = {Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ab53693219a3661acc25568417bcab60acd8a1ff",
            "@type": "ScholarlyArticle",
            "paperId": "ab53693219a3661acc25568417bcab60acd8a1ff",
            "corpusId": 9926020,
            "url": "https://www.semanticscholar.org/paper/ab53693219a3661acc25568417bcab60acd8a1ff",
            "title": "Reinforcement learning: The Good, The Bad and The Ugly",
            "venue": "Current Opinion in Neurobiology",
            "publicationVenue": {
                "id": "urn:research:d0db851a-b0a0-41e2-bef3-3861d0389a5b",
                "name": "Current Opinion in Neurobiology",
                "alternate_names": [
                    "Curr Opin Neurobiol"
                ],
                "issn": "0959-4388",
                "url": "http://www.current-opinion.com/journals/current-opinion-in-neurobiology/"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2133993682",
                "DOI": "10.1016/j.conb.2008.08.003",
                "CorpusId": 9926020,
                "PubMed": "18708140"
            },
            "abstract": null,
            "referenceCount": 170,
            "citationCount": 603,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2008-04-01",
            "journal": {
                "name": "Current Opinion in Neurobiology",
                "volume": "18"
            },
            "citationStyles": {
                "bibtex": "@Article{Dayan2008ReinforcementLT,\n author = {P. Dayan and Y. Niv},\n booktitle = {Current Opinion in Neurobiology},\n journal = {Current Opinion in Neurobiology},\n pages = {185-196},\n title = {Reinforcement learning: The Good, The Bad and The Ugly},\n volume = {18},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9219d4cca4af0df78ac8d02ff1cb8fca0c6524a3",
            "@type": "ScholarlyArticle",
            "paperId": "9219d4cca4af0df78ac8d02ff1cb8fca0c6524a3",
            "corpusId": 17955262,
            "url": "https://www.semanticscholar.org/paper/9219d4cca4af0df78ac8d02ff1cb8fca0c6524a3",
            "title": "A Theory of Attention: Variations in the Associability of Stimuli with Reinforcement",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1975,
            "externalIds": {
                "MAG": "1986264597",
                "DOI": "10.1037/H0076778",
                "CorpusId": 17955262
            },
            "abstract": "According to theories of selective attention, learning about a stimulus depends on attending to that stimulus; this is represented in two-stage models by saying that subjects switch in analyzers as well as learning stimulusresponse associations. This assumption, however, is equally well represented in a formal model by the incorporation of a stimulus-specific learning-rate parameter, a, into the equations describing changes in the associative strength of stimuli. Theories of selective attention have also assumed (a) that subjects learn to attend to and ignore relevant and irrelevant stimuli (i.e., that a may increase or decrease depending on the correlation of a stimulus with reinforcement) and (b) that there is an inverse relationship between the probabilities of attending to different stimuli (i.e., that an increase in a to one stimulus is accompanied by a decrease in a to others). The first assumption is used to explain the phenomena of acquired distinctiveness and dimensional transfer, the second those of overshadowing and blocking. Although the first assumption is justified by the data, the second is not: Overshadowing and blocking are better explained by the choice of an appropriate rule for changing a, such that a decreases to stimuli that signal no change from the probability of reinforcement predicted by other stimuli.",
            "referenceCount": 69,
            "citationCount": 2680,
            "influentialCitationCount": 197,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1975-07-01",
            "journal": {
                "name": "Psychological Review",
                "volume": "82"
            },
            "citationStyles": {
                "bibtex": "@Article{Mackintosh1975ATO,\n author = {N. Mackintosh},\n journal = {Psychological Review},\n pages = {276-298},\n title = {A Theory of Attention: Variations in the Associability of Stimuli with Reinforcement},\n volume = {82},\n year = {1975}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f70f3c48ea8c8c99d64db1daac4ce111d09205cd",
            "@type": "ScholarlyArticle",
            "paperId": "f70f3c48ea8c8c99d64db1daac4ce111d09205cd",
            "corpusId": 203149977,
            "url": "https://www.semanticscholar.org/paper/f70f3c48ea8c8c99d64db1daac4ce111d09205cd",
            "title": "Q-Learning Algorithms: A Comprehensive Classification and Applications",
            "venue": "IEEE Access",
            "publicationVenue": {
                "id": "urn:research:2633f5b2-c15c-49fe-80f5-07523e770c26",
                "name": "IEEE Access",
                "alternate_names": null,
                "issn": "2169-3536",
                "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2973184394",
                "DBLP": "journals/access/JangKHK19",
                "DOI": "10.1109/ACCESS.2019.2941229",
                "CorpusId": 203149977
            },
            "abstract": "Q-learning is arguably one of the most applied representative reinforcement learning approaches and one of the off-policy strategies. Since the emergence of Q-learning, many studies have described its uses in reinforcement learning and artificial intelligence problems. However, there is an information gap as to how these powerful algorithms can be leveraged and incorporated into general artificial intelligence workflow. Early Q-learning algorithms were unsatisfactory in several aspects and covered a narrow range of applications. It has also been observed that sometimes, this rather powerful algorithm learns unrealistically and overestimates the action values hence abating the overall performance. Recently with the general advances of machine learning, more variants of Q-learning like Deep Q-learning which combines basic Q learning with deep neural networks have been discovered and applied extensively. In this paper, we thoroughly explain how Q-learning evolved by unraveling the mathematical complexities behind it as well its flow from reinforcement learning family of algorithms. Improved variants are fully described, and we categorize Q-learning algorithms into single-agent and multi-agent approaches. Finally, we thoroughly investigate up-to-date research trends and key applications that leverage Q-learning algorithms.",
            "referenceCount": 0,
            "citationCount": 174,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/8600701/08836506.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-13",
            "journal": {
                "name": "IEEE Access",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Jang2019QLearningAA,\n author = {Beakcheol Jang and Myeonghwi Kim and Gaspard Harerimana and Jong Wook Kim},\n booktitle = {IEEE Access},\n journal = {IEEE Access},\n pages = {133653-133667},\n title = {Q-Learning Algorithms: A Comprehensive Classification and Applications},\n volume = {7},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c8f41160130980c1ffead5a812cf2b3c6b03049f",
            "@type": "ScholarlyArticle",
            "paperId": "c8f41160130980c1ffead5a812cf2b3c6b03049f",
            "corpusId": 1616437,
            "url": "https://www.semanticscholar.org/paper/c8f41160130980c1ffead5a812cf2b3c6b03049f",
            "title": "Deep spatial autoencoders for visuomotor learning",
            "venue": "IEEE International Conference on Robotics and Automation",
            "publicationVenue": {
                "id": "urn:research:3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                "name": "IEEE International Conference on Robotics and Automation",
                "alternate_names": [
                    "International Conference on Robotics and Automation",
                    "Int Conf Robot Autom",
                    "ICRA",
                    "IEEE Int Conf Robot Autom"
                ],
                "issn": "2152-4092",
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/icra/FinnTDDLA16",
                "MAG": "2210483910",
                "ArXiv": "1509.06113",
                "DOI": "10.1109/ICRA.2016.7487173",
                "CorpusId": 1616437
            },
            "abstract": "Reinforcement learning provides a powerful and flexible framework for automated acquisition of robotic motion skills. However, applying reinforcement learning requires a sufficiently detailed representation of the state, including the configuration of task-relevant objects. We present an approach that automates state-space construction by learning a state representation directly from camera images. Our method uses a deep spatial autoencoder to acquire a set of feature points that describe the environment for the current task, such as the positions of objects, and then learns a motion skill with these feature points using an efficient reinforcement learning method based on local linear models. The resulting controller reacts continuously to the learned feature points, allowing the robot to dynamically manipulate objects in the world with closed-loop control. We demonstrate our method with a PR2 robot on tasks that include pushing a free-standing toy block, picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions. In each task, our method automatically learns to track task-relevant objects and manipulate their configuration with the robot's arm.",
            "referenceCount": 40,
            "citationCount": 491,
            "influentialCitationCount": 28,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1509.06113",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-09-21",
            "journal": {
                "name": "2016 IEEE International Conference on Robotics and Automation (ICRA)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Finn2015DeepSA,\n author = {Chelsea Finn and X. Tan and Yan Duan and Trevor Darrell and S. Levine and P. Abbeel},\n booktitle = {IEEE International Conference on Robotics and Automation},\n journal = {2016 IEEE International Conference on Robotics and Automation (ICRA)},\n pages = {512-519},\n title = {Deep spatial autoencoders for visuomotor learning},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ecf5fd423c117ffb87730d75a473bc05beaae2b8",
            "@type": "ScholarlyArticle",
            "paperId": "ecf5fd423c117ffb87730d75a473bc05beaae2b8",
            "corpusId": 537702,
            "url": "https://www.semanticscholar.org/paper/ecf5fd423c117ffb87730d75a473bc05beaae2b8",
            "title": "Self-Optimizing Memory Controllers: A Reinforcement Learning Approach",
            "venue": "International Symposium on Computer Architecture",
            "publicationVenue": {
                "id": "urn:research:deedf64a-dd5c-4b33-b345-ff83bfb93d71",
                "name": "International Symposium on Computer Architecture",
                "alternate_names": [
                    "Int Symp Comput Archit",
                    "ISCA"
                ],
                "issn": null,
                "url": "http://www.cs.wisc.edu/~arch/www/"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2100787464",
                "DBLP": "conf/isca/IpekMMC08",
                "DOI": "10.1145/1394608.1382172",
                "CorpusId": 537702
            },
            "abstract": "Efficiently utilizing off-chip DRAM bandwidth is a critical issue in designing cost-effective, high-performance chip multiprocessors (CMPs). Conventional memory controllers deliver relatively low performance in part because they often employ fixed, rigid access scheduling policies designed for average-case application behavior. As a result, they cannot learn and optimize the long-term performance impact of their scheduling decisions,and cannot adapt their scheduling policies to dynamic workload behavior.We propose a new, self-optimizing memory controller design that operates using the principles of reinforcement learning (RL)to overcome these limitations. Our RL-based memory controller observes the system state and estimates the long-term performance impact of each action it can take. In this way, the controller learns to optimize its scheduling policy on the fly to maximize long-term performance. Our results show that an RL-based memory controller improves the performance of a set of parallel applications run on a 4-core CMP by 19% on average (upto 33%), and it improves DRAM bandwidth utilization by 22%compared to a state-of-the-art controller.",
            "referenceCount": 55,
            "citationCount": 480,
            "influentialCitationCount": 18,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.csl.cornell.edu/~martinez/doc/isca08.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-06-01",
            "journal": {
                "name": "2008 International Symposium on Computer Architecture",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ipek2008SelfOptimizingMC,\n author = {Engin Ipek and O. Mutlu and Jos\u00e9 F. Mart\u00ednez and R. Caruana},\n booktitle = {International Symposium on Computer Architecture},\n journal = {2008 International Symposium on Computer Architecture},\n pages = {39-50},\n title = {Self-Optimizing Memory Controllers: A Reinforcement Learning Approach},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4a5794c5aa33d6548c7c3f9c2d05d6602d325d6c",
            "@type": "ScholarlyArticle",
            "paperId": "4a5794c5aa33d6548c7c3f9c2d05d6602d325d6c",
            "corpusId": 2453139,
            "url": "https://www.semanticscholar.org/paper/4a5794c5aa33d6548c7c3f9c2d05d6602d325d6c",
            "title": "Genetic triple dissociation reveals multiple roles for dopamine in reinforcement learning",
            "venue": "Proceedings of the National Academy of Sciences of the United States of America",
            "publicationVenue": {
                "id": "urn:research:bb95bf2e-8383-4748-bf9d-d6906d091085",
                "name": "Proceedings of the National Academy of Sciences of the United States of America",
                "alternate_names": [
                    "PNAS",
                    "PNAS online",
                    "Proceedings of the National Academy of Sciences of the United States of America.",
                    "Proc National Acad Sci",
                    "Proceedings of the National Academy of Sciences",
                    "Proc National Acad Sci u s Am"
                ],
                "issn": "0027-8424",
                "url": "https://www.jstor.org/journal/procnatiacadscie"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2014979870",
                "DOI": "10.1073/pnas.0706111104",
                "CorpusId": 2453139,
                "PubMed": "17913879"
            },
            "abstract": "What are the genetic and neural components that support adaptive learning from positive and negative outcomes? Here, we show with genetic analyses that three independent dopaminergic mechanisms contribute to reward and avoidance learning in humans. A polymorphism in the DARPP-32 gene, associated with striatal dopamine function, predicted relatively better probabilistic reward learning. Conversely, the C957T polymorphism of the DRD2 gene, associated with striatal D2 receptor function, predicted the degree to which participants learned to avoid choices that had been probabilistically associated with negative outcomes. The Val/Met polymorphism of the COMT gene, associated with prefrontal cortical dopamine function, predicted participants' ability to rapidly adapt behavior on a trial-to-trial basis. These findings support a neurocomputational dissociation between striatal and prefrontal dopaminergic mechanisms in reinforcement learning. Computational maximum likelihood analyses reveal independent gene effects on three reinforcement learning parameters that can explain the observed dissociations.",
            "referenceCount": 63,
            "citationCount": 649,
            "influentialCitationCount": 72,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.pnas.org/content/pnas/104/41/16311.full.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-10-09",
            "journal": {
                "name": "Proceedings of the National Academy of Sciences",
                "volume": "104"
            },
            "citationStyles": {
                "bibtex": "@Article{Frank2007GeneticTD,\n author = {M. Frank and A. Moustafa and H. Haughey and T. Curran and K. Hutchison},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {16311 - 16316},\n title = {Genetic triple dissociation reveals multiple roles for dopamine in reinforcement learning},\n volume = {104},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5fff3cc67bc1e27fca6f92aba00056d17f52e5a7",
            "@type": "ScholarlyArticle",
            "paperId": "5fff3cc67bc1e27fca6f92aba00056d17f52e5a7",
            "corpusId": 17066353,
            "url": "https://www.semanticscholar.org/paper/5fff3cc67bc1e27fca6f92aba00056d17f52e5a7",
            "title": "Decision theory, reinforcement learning, and the brain",
            "venue": "Cognitive, Affective, & Behavioral Neuroscience",
            "publicationVenue": {
                "id": "urn:research:9bab859b-5cf9-4dca-a5d7-021c0ad80137",
                "name": "Cognitive, Affective, & Behavioral Neuroscience",
                "alternate_names": [
                    "Cogn Affect  Behav Neurosci"
                ],
                "issn": "1530-7026",
                "url": "http://www.springer.com/psychology/cognitive+psychology/journal/13415"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2037917209",
                "DOI": "10.3758/CABN.8.4.429",
                "CorpusId": 17066353,
                "PubMed": "19033240"
            },
            "abstract": null,
            "referenceCount": 117,
            "citationCount": 497,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.3758/CABN.8.4.429.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2008-12-01",
            "journal": {
                "name": "Cognitive, Affective, & Behavioral Neuroscience",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Dayan2008DecisionTR,\n author = {P. Dayan and N. Daw},\n booktitle = {Cognitive, Affective, & Behavioral Neuroscience},\n journal = {Cognitive, Affective, & Behavioral Neuroscience},\n pages = {429-453},\n title = {Decision theory, reinforcement learning, and the brain},\n volume = {8},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dba141eddbbaa86f86a9831c83641ff5a7a28861",
            "@type": "ScholarlyArticle",
            "paperId": "dba141eddbbaa86f86a9831c83641ff5a7a28861",
            "corpusId": 9715887,
            "url": "https://www.semanticscholar.org/paper/dba141eddbbaa86f86a9831c83641ff5a7a28861",
            "title": "Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "journals/jmlr/Even-DarMM06",
                "MAG": "2147967768",
                "CorpusId": 9715887
            },
            "abstract": "We incorporate statistical confidence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it suffices to pull the arms a total of O((n/e2)log(1/\u03b4)) times to find an e-optimal arm with probability of at least 1-\u03b4. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the confidence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over e-greedy Q-learning.",
            "referenceCount": 25,
            "citationCount": 671,
            "influentialCitationCount": 84,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Even-Dar2006ActionEA,\n author = {Eyal Even-Dar and Shie Mannor and Y. Mansour},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1079-1105},\n title = {Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems},\n volume = {7},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:25fd7e9ed8d1a669c7a8d28a8b620479899e6b53",
            "@type": "ScholarlyArticle",
            "paperId": "25fd7e9ed8d1a669c7a8d28a8b620479899e6b53",
            "corpusId": 207168200,
            "url": "https://www.semanticscholar.org/paper/25fd7e9ed8d1a669c7a8d28a8b620479899e6b53",
            "title": "An object-oriented representation for efficient reinforcement learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/icml/DiukCL08",
                "MAG": "2101355568",
                "DOI": "10.1145/1390156.1390187",
                "CorpusId": 207168200
            },
            "abstract": "Rich representations in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state spaces. We introduce Object-Oriented MDPs (OO-MDPs), a representation based on objects and their interactions, which is a natural way of modeling environments and offers important generalization opportunities. We introduce a learning algorithm for deterministic OO-MDPs and prove a polynomial bound on its sample complexity. We illustrate the performance gains of our representation and algorithm in the well-known Taxi domain, plus a real-life videogame.",
            "referenceCount": 101,
            "citationCount": 311,
            "influentialCitationCount": 32,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://rucore.libraries.rutgers.edu/rutgers-lib/30035/PDF/1/",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-07-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Diuk2008AnOR,\n author = {Carlos Diuk and A. Cohen and M. Littman},\n booktitle = {International Conference on Machine Learning},\n pages = {240-247},\n title = {An object-oriented representation for efficient reinforcement learning},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3e6cde685fdf321d7edf9319f7b07c01ff79c11a",
            "@type": "ScholarlyArticle",
            "paperId": "3e6cde685fdf321d7edf9319f7b07c01ff79c11a",
            "corpusId": 53424488,
            "url": "https://www.semanticscholar.org/paper/3e6cde685fdf321d7edf9319f7b07c01ff79c11a",
            "title": "Reward learning from human preferences and demonstrations in Atari",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2889659926",
                "DBLP": "journals/corr/abs-1811-06521",
                "ArXiv": "1811.06521",
                "CorpusId": 53424488
            },
            "abstract": "To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.",
            "referenceCount": 58,
            "citationCount": 235,
            "influentialCitationCount": 29,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-11-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1811.06521"
            },
            "citationStyles": {
                "bibtex": "@Article{Ibarz2018RewardLF,\n author = {Borja Ibarz and J. Leike and Tobias Pohlen and G. Irving and S. Legg and Dario Amodei},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Reward learning from human preferences and demonstrations in Atari},\n volume = {abs/1811.06521},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:12d6fde053e2c7174a76fe1bbdb97dd039a3b662",
            "@type": "ScholarlyArticle",
            "paperId": "12d6fde053e2c7174a76fe1bbdb97dd039a3b662",
            "corpusId": 7241207,
            "url": "https://www.semanticscholar.org/paper/12d6fde053e2c7174a76fe1bbdb97dd039a3b662",
            "title": "Intrinsically Motivated Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "conf/nips/SinghBC04",
                "MAG": "2139612737",
                "DOI": "10.21236/ada440280",
                "CorpusId": 7241207
            },
            "abstract": "Psychologists call behavior intrinsically motivated when it is engaged in for its own sake rather than as a step toward solving a specific problem of clear practical value. But what we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy.",
            "referenceCount": 18,
            "citationCount": 778,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-12-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Singh2004IntrinsicallyMR,\n author = {Satinder Singh and A. Barto and N. Chentanez},\n booktitle = {Neural Information Processing Systems},\n pages = {1281-1288},\n title = {Intrinsically Motivated Reinforcement Learning},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:86b2a1b37ae0d4591dc9902138e9db8ea303fabf",
            "@type": "ScholarlyArticle",
            "paperId": "86b2a1b37ae0d4591dc9902138e9db8ea303fabf",
            "corpusId": 17135168,
            "url": "https://www.semanticscholar.org/paper/86b2a1b37ae0d4591dc9902138e9db8ea303fabf",
            "title": "Quantum Reinforcement Learning",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "journals/corr/abs-0810-3828",
                "MAG": "2117941808",
                "ArXiv": "0810.3828",
                "DOI": "10.1109/TSMCB.2008.925743",
                "CorpusId": 17135168,
                "PubMed": "18784007"
            },
            "abstract": "The key approaches for machine learning, particularly learning in unknown probabilistic environments, are new representations and computation mechanisms. In this paper, a novel quantum reinforcement learning (QRL) method is proposed by combining quantum theory and reinforcement learning (RL). Inspired by the state superposition principle and quantum parallelism, a framework of a value-updating algorithm is introduced. The state (action) in traditional RL is identified as the eigen state (eigen action) in QRL. The state (action) set can be represented with a quantum superposition state, and the eigen state (eigen action) can be obtained by randomly observing the simulated quantum state according to the collapse postulate of quantum measurement. The probability of the eigen action is determined by the probability amplitude, which is updated in parallel according to rewards. Some related characteristics of QRL such as convergence, optimality, and balancing between exploration and exploitation are also analyzed, which shows that this approach makes a good tradeoff between exploration and exploitation using the probability amplitude and can speedup learning through the quantum parallelism. To evaluate the performance and practicability of QRL, several simulated experiments are given, and the results demonstrate the effectiveness and superiority of the QRL algorithm for some complex problems. This paper is also an effective exploration on the application of quantum computation to artificial intelligence.",
            "referenceCount": 57,
            "citationCount": 265,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/0810.3828",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-10-01",
            "journal": {
                "name": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",
                "volume": "38"
            },
            "citationStyles": {
                "bibtex": "@Article{Dong2008QuantumRL,\n author = {Daoyi Dong and Chunlin Chen and Han-Xiong Li and T. Tarn},\n booktitle = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},\n pages = {1207-1220},\n title = {Quantum Reinforcement Learning},\n volume = {38},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4d2c4cbb535801549371d9783a98d1e43bddf4e5",
            "@type": "ScholarlyArticle",
            "paperId": "4d2c4cbb535801549371d9783a98d1e43bddf4e5",
            "corpusId": 1369182,
            "url": "https://www.semanticscholar.org/paper/4d2c4cbb535801549371d9783a98d1e43bddf4e5",
            "title": "Meta Learning Shared Hierarchies",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2951513524",
                "DBLP": "journals/corr/abs-1710-09767",
                "ArXiv": "1710.09767",
                "CorpusId": 1369182
            },
            "abstract": "We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives---policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.",
            "referenceCount": 27,
            "citationCount": 307,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-10-26",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1710.09767"
            },
            "citationStyles": {
                "bibtex": "@Article{Frans2017MetaLS,\n author = {Kevin Frans and Jonathan Ho and Xi Chen and P. Abbeel and J. Schulman},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Meta Learning Shared Hierarchies},\n volume = {abs/1710.09767},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0bfbdafdfbcc268860fe54ae4d8f08d487bcc762",
            "@type": "ScholarlyArticle",
            "paperId": "0bfbdafdfbcc268860fe54ae4d8f08d487bcc762",
            "corpusId": 15279814,
            "url": "https://www.semanticscholar.org/paper/0bfbdafdfbcc268860fe54ae4d8f08d487bcc762",
            "title": "An Application of Reinforcement Learning to Aerobatic Helicopter Flight",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2540085067",
                "DBLP": "conf/nips/AbbeelCQN06",
                "DOI": "10.7551/mitpress/7503.003.0006",
                "CorpusId": 15279814
            },
            "abstract": "Autonomous helicopter flight is widely regarded to be a highly challenging control problem. This paper presents the first successful autonomous completion on a real RC helicopter of the following four aerobatic maneuvers: forward flip and sideways roll at low speed, tail-in funnel, and nose-in funnel. Our experimental results significantly extend the state of the art in autonomous helicopter flight. We used the following approach: First we had a pilot fly the helicopter to help us find a helicopter dynamics model and a reward (cost) function. Then we used a reinforcement learning (optimal control) algorithm to find a controller that is optimized for the resulting model and reward function. More specifically, we used differential dynamic programming (DDP), an extension of the linear quadratic regulator (LQR).",
            "referenceCount": 20,
            "citationCount": 665,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2006-12-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Abbeel2006AnAO,\n author = {P. Abbeel and Adam Coates and M. Quigley and A. Ng},\n booktitle = {Neural Information Processing Systems},\n pages = {1-8},\n title = {An Application of Reinforcement Learning to Aerobatic Helicopter Flight},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d0352057e2b99f65f8b5244a0b912026c86d7b21",
            "@type": "ScholarlyArticle",
            "paperId": "d0352057e2b99f65f8b5244a0b912026c86d7b21",
            "corpusId": 17234172,
            "url": "https://www.semanticscholar.org/paper/d0352057e2b99f65f8b5244a0b912026c86d7b21",
            "title": "Equivalence Between Policy Gradients and Soft Q-Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1704.06440",
                "DBLP": "journals/corr/SchulmanAC17",
                "MAG": "2609650878",
                "CorpusId": 17234172
            },
            "abstract": "Two of the leading approaches for model-free reinforcement learning are policy gradient methods and $Q$-learning methods. $Q$-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the $Q$-values they estimate are very inaccurate. A partial explanation may be that $Q$-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between $Q$-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that \"soft\" (entropy-regularized) $Q$-learning is exactly equivalent to a policy gradient method. We also point out a connection between $Q$-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of $Q$-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a $Q$-learning method that closely matches the learning dynamics of A3C without using a target network or $\\epsilon$-greedy exploration schedule.",
            "referenceCount": 13,
            "citationCount": 275,
            "influentialCitationCount": 53,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-04-21",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1704.06440"
            },
            "citationStyles": {
                "bibtex": "@Article{Schulman2017EquivalenceBP,\n author = {J. Schulman and P. Abbeel and Xi Chen},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Equivalence Between Policy Gradients and Soft Q-Learning},\n volume = {abs/1704.06440},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f8f4fb11740962311563872c4026f49195e75edc",
            "@type": "ScholarlyArticle",
            "paperId": "f8f4fb11740962311563872c4026f49195e75edc",
            "corpusId": 364332,
            "url": "https://www.semanticscholar.org/paper/f8f4fb11740962311563872c4026f49195e75edc",
            "title": "Packet Routing in Dynamically Changing Networks: A Reinforcement Learning Approach",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1993,
            "externalIds": {
                "MAG": "2156666755",
                "DBLP": "conf/nips/BoyanL93",
                "CorpusId": 364332
            },
            "abstract": "This paper describes the Q-routing algorithm for packet routing, in which a reinforcement learning module is embedded into each node of a switching network. Only local communication is used by each node to keep accurate statistics on which routing decisions lead to minimal delivery times. In simple experiments involving a 36-node, irregularly connected network, Q-routing proves superior to a nonadaptive algorithm based on precomputed shortest paths and is able to route efficiently even when critical aspects of the simulation, such as the network load, are allowed to vary dynamically. The paper concludes with a discussion of the tradeoff between discovering shortcuts and maintaining stable policies.",
            "referenceCount": 8,
            "citationCount": 846,
            "influentialCitationCount": 83,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1993-11-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Boyan1993PacketRI,\n author = {J. Boyan and M. Littman},\n booktitle = {Neural Information Processing Systems},\n pages = {671-678},\n title = {Packet Routing in Dynamically Changing Networks: A Reinforcement Learning Approach},\n year = {1993}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:117d0903a0dc0d78aacc8cbc84e6cd86f4530ef2",
            "@type": "ScholarlyArticle",
            "paperId": "117d0903a0dc0d78aacc8cbc84e6cd86f4530ef2",
            "corpusId": 295335,
            "url": "https://www.semanticscholar.org/paper/117d0903a0dc0d78aacc8cbc84e6cd86f4530ef2",
            "title": "Transfer of samples in batch reinforcement learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2004030284",
                "DBLP": "conf/icml/LazaricRB08",
                "DOI": "10.1145/1390156.1390225",
                "CorpusId": 295335
            },
            "abstract": "The main objective of transfer in reinforcement learning is to reduce the complexity of learning the solution of a target task by effectively reusing the knowledge retained from solving a set of source tasks. In this paper, we introduce a novel algorithm that transfers samples (i.e., tuples \u27e8s, a, s', r\u27e9) from source to target tasks. Under the assumption that tasks have similar transition models and reward functions, we propose a method to select samples from the source tasks that are mostly similar to the target task, and, then, to use them as input for batch reinforcement-learning algorithms. As a result, the number of samples an agent needs to collect from the target task to learn its solution is reduced. We empirically show that, following the proposed approach, the transfer of samples is effective in reducing the learning complexity, even when some source tasks are significantly different from the target task.",
            "referenceCount": 14,
            "citationCount": 185,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-07-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lazaric2008TransferOS,\n author = {A. Lazaric and Marcello Restelli and Andrea Bonarini},\n booktitle = {International Conference on Machine Learning},\n pages = {544-551},\n title = {Transfer of samples in batch reinforcement learning},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6d23073dbb68d353f30bb97f4803cfbd66546444",
            "@type": "ScholarlyArticle",
            "paperId": "6d23073dbb68d353f30bb97f4803cfbd66546444",
            "corpusId": 234450,
            "url": "https://www.semanticscholar.org/paper/6d23073dbb68d353f30bb97f4803cfbd66546444",
            "title": "Ensemble Algorithms in Reinforcement Learning",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2043806097",
                "DBLP": "journals/tsmc/WieringH08",
                "DOI": "10.1109/TSMCB.2008.920231",
                "CorpusId": 234450,
                "PubMed": "18632380"
            },
            "abstract": "This paper describes several ensemble methods that combine multiple different reinforcement learning (RL) algorithms in a single agent. The aim is to enhance learning speed and final performance by combining the chosen actions or action probabilities of different RL algorithms. We designed and implemented four different ensemble methods combining the following five different RL algorithms: Q-learning, Sarsa, actor-critic (AC), QV-learning, and AC learning automaton. The intuitively designed ensemble methods, namely, majority voting (MV), rank voting, Boltzmann multiplication (BM), and Boltzmann addition, combine the policies derived from the value functions of the different RL algorithms, in contrast to previous work where ensemble methods have been used in RL for representing and learning a single value function. We show experiments on five maze problems of varying complexity; the first problem is simple, but the other four maze tasks are of a dynamic or partially observable nature. The results indicate that the BM and MV ensembles significantly outperform the single RL algorithms.",
            "referenceCount": 24,
            "citationCount": 183,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.ai.rug.nl/~mwiering/GROUP/ARTICLES/ensemble_RL_final.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-08-01",
            "journal": {
                "name": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",
                "volume": "38"
            },
            "citationStyles": {
                "bibtex": "@Article{Wiering2008EnsembleAI,\n author = {M. Wiering and H. V. Hasselt},\n booktitle = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},\n pages = {930-936},\n title = {Ensemble Algorithms in Reinforcement Learning},\n volume = {38},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:249408527106d7595d45dd761dd53c83e5a02613",
            "@type": "ScholarlyArticle",
            "paperId": "249408527106d7595d45dd761dd53c83e5a02613",
            "corpusId": 65051725,
            "url": "https://www.semanticscholar.org/paper/249408527106d7595d45dd761dd53c83e5a02613",
            "title": "NerveNet: Learning Structured Policy with Graph Neural Networks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/iclr/WangLBF18",
                "MAG": "2785948534",
                "CorpusId": 65051725
            },
            "abstract": "We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by multi-layer perceptrons (MLPs) which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Speci\ufb01cally, serving as the agent\u2019s policy network, NerveNet \ufb01rst propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we \ufb01rst show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer , as well as multi-task learning . We demonstrate that policies learned by NerveNet are signi\ufb01cantly more transferable and generalizable than policies learned by other models and are able to transfer even in a zero-shot setting.",
            "referenceCount": 60,
            "citationCount": 218,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-15",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2018NerveNetLS,\n author = {Tingwu Wang and Renjie Liao and Jimmy Ba and S. Fidler},\n booktitle = {International Conference on Learning Representations},\n title = {NerveNet: Learning Structured Policy with Graph Neural Networks},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:52e2ac397f0c8d5f533959905df899bc328d9f85",
            "@type": "ScholarlyArticle",
            "paperId": "52e2ac397f0c8d5f533959905df899bc328d9f85",
            "corpusId": 6760236,
            "url": "https://www.semanticscholar.org/paper/52e2ac397f0c8d5f533959905df899bc328d9f85",
            "title": "Reinforcement Learning with Hierarchies of Machines",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1997,
            "externalIds": {
                "DBLP": "conf/nips/ParrR97",
                "MAG": "2158548602",
                "CorpusId": 6760236
            },
            "abstract": "We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and \"behavior-based\" or \"teleo-reactive\" approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states.",
            "referenceCount": 17,
            "citationCount": 838,
            "influentialCitationCount": 47,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1997-12-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Parr1997ReinforcementLW,\n author = {Ronald E. Parr and Stuart J. Russell},\n booktitle = {Neural Information Processing Systems},\n pages = {1043-1049},\n title = {Reinforcement Learning with Hierarchies of Machines},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d25bfa7bede6c24df75680dd447ef8dc3b7a7064",
            "@type": "ScholarlyArticle",
            "paperId": "d25bfa7bede6c24df75680dd447ef8dc3b7a7064",
            "corpusId": 8735198,
            "url": "https://www.semanticscholar.org/paper/d25bfa7bede6c24df75680dd447ef8dc3b7a7064",
            "title": "Transferring Instances for Model-Based Reinforcement Learning",
            "venue": "ECML/PKDD",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/pkdd/TaylorJS08",
                "MAG": "2110292307",
                "DOI": "10.1007/978-3-540-87481-2_32",
                "CorpusId": 8735198
            },
            "abstract": null,
            "referenceCount": 35,
            "citationCount": 137,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/978-3-540-87481-2_32.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-09-15",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Taylor2008TransferringIF,\n author = {Matthew E. Taylor and Nicholas K. Jong and P. Stone},\n booktitle = {ECML/PKDD},\n pages = {488-505},\n title = {Transferring Instances for Model-Based Reinforcement Learning},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ab131a2e397a3d5b987d78fef31c3b060a118619",
            "@type": "ScholarlyArticle",
            "paperId": "ab131a2e397a3d5b987d78fef31c3b060a118619",
            "corpusId": 67856451,
            "url": "https://www.semanticscholar.org/paper/ab131a2e397a3d5b987d78fef31c3b060a118619",
            "title": "Diagnosing Bottlenecks in Deep Q-learning Algorithms",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2949576341",
                "DBLP": "journals/corr/abs-1902-10250",
                "ArXiv": "1902.10250",
                "CorpusId": 67856451
            },
            "abstract": "Q-learning methods represent a commonly used class of algorithms in reinforcement learning: they are generally efficient and simple, and can be combined readily with function approximators for deep reinforcement learning (RL). However, the behavior of Q-learning methods with function approximation is poorly understood, both theoretically and empirically. In this work, we aim to experimentally investigate potential issues in Q-learning, by means of a \"unit testing\" framework where we can utilize oracles to disentangle sources of error. Specifically, we investigate questions related to function approximation, sampling error and nonstationarity, and where available, verify if trends found in oracle settings hold true with modern deep RL methods. We find that large neural network architectures have many benefits with regards to learning stability; offer several practical compensations for overfitting; and develop a novel sampling method based on explicitly compensating for function approximation error that yields fair improvement on high-dimensional continuous control domains.",
            "referenceCount": 46,
            "citationCount": 118,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-02-26",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Fu2019DiagnosingBI,\n author = {Justin Fu and Aviral Kumar and Matthew Soh and S. Levine},\n booktitle = {International Conference on Machine Learning},\n pages = {2021-2030},\n title = {Diagnosing Bottlenecks in Deep Q-learning Algorithms},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8a7acaf6469c06ae5876d92f013184db5897bb13",
            "@type": "ScholarlyArticle",
            "paperId": "8a7acaf6469c06ae5876d92f013184db5897bb13",
            "corpusId": 1522994,
            "url": "https://www.semanticscholar.org/paper/8a7acaf6469c06ae5876d92f013184db5897bb13",
            "title": "Neuronlike adaptive elements that can solve difficult learning control problems",
            "venue": "IEEE Transactions on Systems, Man and Cybernetics",
            "publicationVenue": {
                "id": "urn:research:336446b6-e859-4f7b-9121-d2d40357fe0a",
                "name": "IEEE Transactions on Systems, Man and Cybernetics",
                "alternate_names": [
                    "IEEE Trans Syst Man Cybern",
                    "IEEE Transactions on Systems, Man, and Cybernetics"
                ],
                "issn": "0018-9472",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=21"
            },
            "year": 1983,
            "externalIds": {
                "MAG": "1583833196",
                "DBLP": "journals/tsmc/BartoSA83",
                "DOI": "10.1109/TSMC.1983.6313077",
                "CorpusId": 1522994
            },
            "abstract": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.",
            "referenceCount": 27,
            "citationCount": 3388,
            "influentialCitationCount": 174,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1983-09-01",
            "journal": {
                "name": "IEEE Transactions on Systems, Man, and Cybernetics",
                "volume": "SMC-13"
            },
            "citationStyles": {
                "bibtex": "@Article{Barto1983NeuronlikeAE,\n author = {A. Barto and R. Sutton and C. Anderson},\n booktitle = {IEEE Transactions on Systems, Man and Cybernetics},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics},\n pages = {834-846},\n title = {Neuronlike adaptive elements that can solve difficult learning control problems},\n volume = {SMC-13},\n year = {1983}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:36fcc32aac99de1f67f83da33f134721258ea943",
            "@type": "ScholarlyArticle",
            "paperId": "36fcc32aac99de1f67f83da33f134721258ea943",
            "corpusId": 7794529,
            "url": "https://www.semanticscholar.org/paper/36fcc32aac99de1f67f83da33f134721258ea943",
            "title": "Transfer in variable-reward hierarchical reinforcement learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "journals/ml/MehtaNTF08",
                "MAG": "2132057084",
                "DOI": "10.1007/s10994-008-5061-y",
                "CorpusId": 7794529
            },
            "abstract": null,
            "referenceCount": 25,
            "citationCount": 104,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10994-008-5061-y.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-12-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "73"
            },
            "citationStyles": {
                "bibtex": "@Article{Mehta2008TransferIV,\n author = {N. Mehta and Sriraam Natarajan and Prasad Tadepalli and Alan Fern},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {289-312},\n title = {Transfer in variable-reward hierarchical reinforcement learning},\n volume = {73},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:77365e7d272bbd3fd8472d44054168dba46cb5fe",
            "@type": "ScholarlyArticle",
            "paperId": "77365e7d272bbd3fd8472d44054168dba46cb5fe",
            "corpusId": 6523949,
            "url": "https://www.semanticscholar.org/paper/77365e7d272bbd3fd8472d44054168dba46cb5fe",
            "title": "Autonomous transfer for reinforcement learning",
            "venue": "Adaptive Agents and Multi-Agent Systems",
            "publicationVenue": {
                "id": "urn:research:6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                "name": "Adaptive Agents and Multi-Agent Systems",
                "alternate_names": [
                    "Adapt Agent Multi-agent Syst",
                    "International Joint Conference on Autonomous Agents & Multiagent Systems",
                    "Adapt Agent Multi-agents Syst",
                    "AAMAS",
                    "Adaptive Agents and Multi-Agents Systems",
                    "Int Jt Conf Auton Agent  Multiagent Syst"
                ],
                "issn": null,
                "url": "http://www.ifaamas.org/"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/atal/TaylorKS08",
                "MAG": "2158150115",
                "DOI": "10.1145/1402383.1402427",
                "CorpusId": 6523949
            },
            "abstract": "Recent work in transfer learning has succeeded in making reinforcement learning algorithms more efficient by incorporating knowledge from previous tasks. However, such methods typically must be provided either a full model of the tasks or an explicit relation mapping one task into the other. An autonomous agent may not have access to such high-level information, but would be able to analyze its experience to find similarities between tasks. In this paper we introduce Modeling Approximate State Transitions by Exploiting Regression (MASTER), a method for automatically learning a mapping from one task to another through an agent's experience. We empirically demonstrate that such learned relationships can significantly improve the speed of a reinforcement learning algorithm in a series of Mountain Car tasks. Additionally, we demonstrate that our method may also assist with the difficult problem of task selection for transfer.",
            "referenceCount": 22,
            "citationCount": 123,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2008-05-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Taylor2008AutonomousTF,\n author = {Matthew E. Taylor and Gregory Kuhlmann and P. Stone},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {283-290},\n title = {Autonomous transfer for reinforcement learning},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5ee27e9db2ae248d1254107852311117c4cda1c9",
            "@type": "ScholarlyArticle",
            "paperId": "5ee27e9db2ae248d1254107852311117c4cda1c9",
            "corpusId": 13321704,
            "url": "https://www.semanticscholar.org/paper/5ee27e9db2ae248d1254107852311117c4cda1c9",
            "title": "Safe exploration for reinforcement learning",
            "venue": "The European Symposium on Artificial Neural Networks",
            "publicationVenue": {
                "id": "urn:research:93d6c444-c90a-48ee-a3ad-ef1f015bc28a",
                "name": "The European Symposium on Artificial Neural Networks",
                "alternate_names": [
                    "Eur Symp Artif Neural Netw",
                    "ESANN"
                ],
                "issn": null,
                "url": "https://www.esann.org/"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/esann/HansSSU08",
                "MAG": "15411808",
                "CorpusId": 13321704
            },
            "abstract": "In this paper we define and address the problem of safe exploration in the context of reinforcement learning. Our notion of safety is concerned with states or transitions that can lead to damage and thus must be avoided. We introduce the concepts of a safety function for determining a state\u2019s safety degree and that of a backup policy that is able to lead the system under control from a critical state back to a safe one. Moreover, we present a level-based exploration scheme that is able to generate a comprehensive base of observations while adhering safety constraints.We evaluate our approach on a simplified simulation of a gas turbine.",
            "referenceCount": 7,
            "citationCount": 123,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hans2008SafeEF,\n author = {A. Hans and Daniel Schneega\u00df and A. Sch\u00e4fer and S. Udluft},\n booktitle = {The European Symposium on Artificial Neural Networks},\n pages = {143-148},\n title = {Safe exploration for reinforcement learning},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d5cc152a8ab42d5a54ccc522b68479ee56d93b53",
            "@type": "ScholarlyArticle",
            "paperId": "d5cc152a8ab42d5a54ccc522b68479ee56d93b53",
            "corpusId": 8772168,
            "url": "https://www.semanticscholar.org/paper/d5cc152a8ab42d5a54ccc522b68479ee56d93b53",
            "title": "Anatomy of a decision: striato-orbitofrontal interactions in reinforcement learning, decision making, and reversal.",
            "venue": "Psychology Review",
            "publicationVenue": {
                "id": "urn:research:dfb7f114-609c-4c34-9ee7-8cb1fc3e4a6b",
                "name": "Psychology Review",
                "alternate_names": [
                    "Psychol rev",
                    "Psychological review",
                    "Psychol Rev",
                    "Psychological Review"
                ],
                "issn": "1354-1129",
                "url": "http://www.apa.org/journals/rev/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "1990068200",
                "DOI": "10.1037/0033-295X.113.2.300",
                "CorpusId": 8772168,
                "PubMed": "16637763"
            },
            "abstract": "The authors explore the division of labor between the basal ganglia-dopamine (BG-DA) system and the orbitofrontal cortex (OFC) in decision making. They show that a primitive neural network model of the BG-DA system slowly learns to make decisions on the basis of the relative probability of rewards but is not as sensitive to (a) recency or (b) the value of specific rewards. An augmented model that explores BG-OFC interactions is more successful at estimating the true expected value of decisions and is faster at switching behavior when reinforcement contingencies change. In the augmented model, OFC areas exert top-down control on the BG and premotor areas by representing reinforcement magnitudes in working memory. The model successfully captures patterns of behavior resulting from OFC damage in decision making, reversal learning, and devaluation paradigms and makes additional predictions for the underlying source of these deficits.",
            "referenceCount": 234,
            "citationCount": 598,
            "influentialCitationCount": 48,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://ski.cog.brown.edu/papers/FrankClaus06.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2006-04-01",
            "journal": {
                "name": "Psychological review",
                "volume": "113 2"
            },
            "citationStyles": {
                "bibtex": "@Article{Frank2006AnatomyOA,\n author = {M. Frank and E. Claus},\n booktitle = {Psychology Review},\n journal = {Psychological review},\n pages = {\n          300-326\n        },\n title = {Anatomy of a decision: striato-orbitofrontal interactions in reinforcement learning, decision making, and reversal.},\n volume = {113 2},\n year = {2006}\n}\n"
            }
        }
    }
]