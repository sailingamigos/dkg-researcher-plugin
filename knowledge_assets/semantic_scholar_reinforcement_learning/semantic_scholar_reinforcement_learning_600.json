[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5ee27e9db2ae248d1254107852311117c4cda1c9",
            "@type": "ScholarlyArticle",
            "paperId": "5ee27e9db2ae248d1254107852311117c4cda1c9",
            "corpusId": 13321704,
            "url": "https://www.semanticscholar.org/paper/5ee27e9db2ae248d1254107852311117c4cda1c9",
            "title": "Safe exploration for reinforcement learning",
            "venue": "The European Symposium on Artificial Neural Networks",
            "publicationVenue": {
                "id": "urn:research:93d6c444-c90a-48ee-a3ad-ef1f015bc28a",
                "name": "The European Symposium on Artificial Neural Networks",
                "alternate_names": [
                    "Eur Symp Artif Neural Netw",
                    "ESANN"
                ],
                "issn": null,
                "url": "https://www.esann.org/"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/esann/HansSSU08",
                "MAG": "15411808",
                "CorpusId": 13321704
            },
            "abstract": "In this paper we define and address the problem of safe exploration in the context of reinforcement learning. Our notion of safety is concerned with states or transitions that can lead to damage and thus must be avoided. We introduce the concepts of a safety function for determining a state\u2019s safety degree and that of a backup policy that is able to lead the system under control from a critical state back to a safe one. Moreover, we present a level-based exploration scheme that is able to generate a comprehensive base of observations while adhering safety constraints.We evaluate our approach on a simplified simulation of a gas turbine.",
            "referenceCount": 7,
            "citationCount": 123,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hans2008SafeEF,\n author = {A. Hans and Daniel Schneega\u00df and A. Sch\u00e4fer and S. Udluft},\n booktitle = {The European Symposium on Artificial Neural Networks},\n pages = {143-148},\n title = {Safe exploration for reinforcement learning},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d5cc152a8ab42d5a54ccc522b68479ee56d93b53",
            "@type": "ScholarlyArticle",
            "paperId": "d5cc152a8ab42d5a54ccc522b68479ee56d93b53",
            "corpusId": 8772168,
            "url": "https://www.semanticscholar.org/paper/d5cc152a8ab42d5a54ccc522b68479ee56d93b53",
            "title": "Anatomy of a decision: striato-orbitofrontal interactions in reinforcement learning, decision making, and reversal.",
            "venue": "Psychology Review",
            "publicationVenue": {
                "id": "urn:research:dfb7f114-609c-4c34-9ee7-8cb1fc3e4a6b",
                "name": "Psychology Review",
                "alternate_names": [
                    "Psychol rev",
                    "Psychological review",
                    "Psychol Rev",
                    "Psychological Review"
                ],
                "issn": "1354-1129",
                "url": "http://www.apa.org/journals/rev/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "1990068200",
                "DOI": "10.1037/0033-295X.113.2.300",
                "CorpusId": 8772168,
                "PubMed": "16637763"
            },
            "abstract": "The authors explore the division of labor between the basal ganglia-dopamine (BG-DA) system and the orbitofrontal cortex (OFC) in decision making. They show that a primitive neural network model of the BG-DA system slowly learns to make decisions on the basis of the relative probability of rewards but is not as sensitive to (a) recency or (b) the value of specific rewards. An augmented model that explores BG-OFC interactions is more successful at estimating the true expected value of decisions and is faster at switching behavior when reinforcement contingencies change. In the augmented model, OFC areas exert top-down control on the BG and premotor areas by representing reinforcement magnitudes in working memory. The model successfully captures patterns of behavior resulting from OFC damage in decision making, reversal learning, and devaluation paradigms and makes additional predictions for the underlying source of these deficits.",
            "referenceCount": 234,
            "citationCount": 598,
            "influentialCitationCount": 48,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://ski.cog.brown.edu/papers/FrankClaus06.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2006-04-01",
            "journal": {
                "name": "Psychological review",
                "volume": "113 2"
            },
            "citationStyles": {
                "bibtex": "@Article{Frank2006AnatomyOA,\n author = {M. Frank and E. Claus},\n booktitle = {Psychology Review},\n journal = {Psychological review},\n pages = {\n          300-326\n        },\n title = {Anatomy of a decision: striato-orbitofrontal interactions in reinforcement learning, decision making, and reversal.},\n volume = {113 2},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c221cc946425d85f93c86e3be2c31d4feb00faa1",
            "@type": "ScholarlyArticle",
            "paperId": "c221cc946425d85f93c86e3be2c31d4feb00faa1",
            "corpusId": 19115634,
            "url": "https://www.semanticscholar.org/paper/c221cc946425d85f93c86e3be2c31d4feb00faa1",
            "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2004,
            "externalIds": {
                "DOI": "10.1023/A:1022672621406",
                "CorpusId": 19115634
            },
            "abstract": null,
            "referenceCount": 30,
            "citationCount": 708,
            "influentialCitationCount": 93,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1022672621406.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Williams2004SimpleSG,\n author = {Ronald J. Williams},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {229-256},\n title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},\n volume = {8},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:22069cd4504656d3bb85748a4d43be7a4d7d5545",
            "@type": "ScholarlyArticle",
            "paperId": "22069cd4504656d3bb85748a4d43be7a4d7d5545",
            "corpusId": 60564875,
            "url": "https://www.semanticscholar.org/paper/22069cd4504656d3bb85748a4d43be7a4d7d5545",
            "title": "Temporal credit assignment in reinforcement learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1984,
            "externalIds": {
                "MAG": "1569296262",
                "CorpusId": 60564875
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 920,
            "influentialCitationCount": 62,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Sutton1984TemporalCA,\n author = {R. Sutton},\n title = {Temporal credit assignment in reinforcement learning},\n year = {1984}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6024d2494e6b918bfa9977c9b98283688a66740e",
            "@type": "ScholarlyArticle",
            "paperId": "6024d2494e6b918bfa9977c9b98283688a66740e",
            "corpusId": 8796344,
            "url": "https://www.semanticscholar.org/paper/6024d2494e6b918bfa9977c9b98283688a66740e",
            "title": "An analysis of reinforcement learning with function approximation",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2151661095",
                "DBLP": "conf/icml/MeloMR08",
                "DOI": "10.1145/1390156.1390240",
                "CorpusId": 8796344
            },
            "abstract": "We address the problem of computing the optimal Q-function in Markov decision problems with infinite state-space. We analyze the convergence properties of several variations of Q-learning when combined with function approximation, extending the analysis of TD-learning in (Tsitsiklis & Van Roy, 1996a) to stochastic control settings. We identify conditions under which such approximate methods converge with probability 1. We conclude with a brief discussion on the general applicability of our results and compare them with several related works.",
            "referenceCount": 28,
            "citationCount": 249,
            "influentialCitationCount": 32,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-07-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Melo2008AnAO,\n author = {Francisco S. Melo and Sean P. Meyn and M. Ribeiro},\n booktitle = {International Conference on Machine Learning},\n pages = {664-671},\n title = {An analysis of reinforcement learning with function approximation},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a9c8124b18c0e843878f2df4d49da6f250dc06e7",
            "@type": "ScholarlyArticle",
            "paperId": "a9c8124b18c0e843878f2df4d49da6f250dc06e7",
            "corpusId": 3461501,
            "url": "https://www.semanticscholar.org/paper/a9c8124b18c0e843878f2df4d49da6f250dc06e7",
            "title": "Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/icml/DepewegHDU18",
                "MAG": "2788755844",
                "CorpusId": 3461501
            },
            "abstract": "\u00a9 2018 35th International Conference on Machine Learning, ICML 2018. All rights reserved. Bayesian neural networks with latent variables are scalable and flexible probabilistic models: They account for uncertainty in the estimation of the network weights and, by making use of latent variables, can capture complex noise patterns in the : data. Using these models we show how to per- \u2217 form and utilize a decomposition of uncertainty in \u2022 aleatoric and epistemic components for decision making purposes. This allows us to successfully identify informative points for active learning of functions with heteroscedastic and bimodal noise. ' t Using the decomposition we further define a novel risk-sensitive criterion for reinforcement learning to identify policies that balance expected cost, model-bias and noise aversion.",
            "referenceCount": 40,
            "citationCount": 280,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-10-19",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Depeweg2017DecompositionOU,\n author = {Stefan Depeweg and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato and F. Doshi-Velez and S. Udluft},\n booktitle = {International Conference on Machine Learning},\n pages = {1192-1201},\n title = {Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e6134e7ae32e7b9b3337e47e8354dd43102850a4",
            "@type": "ScholarlyArticle",
            "paperId": "e6134e7ae32e7b9b3337e47e8354dd43102850a4",
            "corpusId": 18453286,
            "url": "https://www.semanticscholar.org/paper/e6134e7ae32e7b9b3337e47e8354dd43102850a4",
            "title": "Online Learning for Offloading and Autoscaling in Energy Harvesting Mobile Edge Computing",
            "venue": "IEEE Transactions on Cognitive Communications and Networking",
            "publicationVenue": {
                "id": "urn:research:65e58b80-9699-4da6-bd60-929b57b8533d",
                "name": "IEEE Transactions on Cognitive Communications and Networking",
                "alternate_names": [
                    "IEEE Trans Cogn Commun Netw"
                ],
                "issn": "2332-7731",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6687307"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/XuCR17",
                "MAG": "2599106590",
                "ArXiv": "1703.06060",
                "DOI": "10.1109/TCCN.2017.2725277",
                "CorpusId": 18453286
            },
            "abstract": "Mobile edge computing (also known as fog computing) has recently emerged to enable in-situ processing of delay-sensitive applications at the edge of mobile networks. Providing grid power supply in support of mobile edge computing, however, is costly and even infeasible (in certain rugged or under-developed areas), thus mandating on-site renewable energy as a major or even sole power supply in increasingly many scenarios. Nonetheless, the high intermittency and unpredictability of renewable energy make it very challenging to deliver a high quality of service to users in energy harvesting mobile edge computing systems. In this paper, we address the challenge of incorporating renewables into mobile edge computing and propose an efficient reinforcement learning-based resource management algorithm, which learns on-the-fly the optimal policy of dynamic workload offloading (to the centralized cloud) and edge server provisioning to minimize the long-term system cost (including both service delay and operational cost). Our online learning algorithm uses a decomposition of the (offline) value iteration and (online) reinforcement learning, thus achieving a significant improvement of learning rate and run-time performance when compared to standard reinforcement learning algorithms such as  ${Q}$ -learning. We prove the convergence of the proposed algorithm and analytically show that the learned policy has a simple monotone structure amenable to practical implementation. Our simulation results validate the efficacy of our algorithm, which significantly improves the edge computing performance compared to fixed or myopic optimization schemes and conventional reinforcement learning algorithms.",
            "referenceCount": 34,
            "citationCount": 279,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-03-17",
            "journal": {
                "name": "IEEE Transactions on Cognitive Communications and Networking",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Xu2017OnlineLF,\n author = {Jie Xu and Lixing Chen and Shaolei Ren},\n booktitle = {IEEE Transactions on Cognitive Communications and Networking},\n journal = {IEEE Transactions on Cognitive Communications and Networking},\n pages = {361-373},\n title = {Online Learning for Offloading and Autoscaling in Energy Harvesting Mobile Edge Computing},\n volume = {3},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4741b9f50c150506ba77201da630df193bc95441",
            "@type": "ScholarlyArticle",
            "paperId": "4741b9f50c150506ba77201da630df193bc95441",
            "corpusId": 43514760,
            "url": "https://www.semanticscholar.org/paper/4741b9f50c150506ba77201da630df193bc95441",
            "title": "The Role of Variability in Motor Learning.",
            "venue": "Annual Review of Neuroscience",
            "publicationVenue": {
                "id": "urn:research:d9caa671-3be6-48bc-9710-f96334848b4c",
                "name": "Annual Review of Neuroscience",
                "alternate_names": [
                    "Annu Rev Neurosci"
                ],
                "issn": "0147-006X",
                "url": "https://www.annualreviews.org/journal/neuro"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2613867659",
                "DOI": "10.1146/annurev-neuro-072116-031548",
                "CorpusId": 43514760,
                "PubMed": "28489490"
            },
            "abstract": "Trial-to-trial variability in the execution of movements and motor skills is ubiquitous and widely considered to be the unwanted consequence of a noisy nervous system. However, recent studies have suggested that motor variability may also be a feature of how sensorimotor systems operate and learn. This view, rooted in reinforcement learning theory, equates motor variability with purposeful exploration of motor space that, when coupled with reinforcement, can drive motor learning. Here we review studies that explore the relationship between motor variability and motor learning in both humans and animal models. We discuss neural circuit mechanisms that underlie the generation and regulation of motor variability and consider the implications that this work has for our understanding of motor learning.",
            "referenceCount": 142,
            "citationCount": 275,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.annualreviews.org/doi/pdf/10.1146/annurev-neuro-072116-031548",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2017-08-03",
            "journal": {
                "name": "Annual review of neuroscience",
                "volume": "40"
            },
            "citationStyles": {
                "bibtex": "@Article{Dhawale2017TheRO,\n author = {Ashesh K. Dhawale and Maurice A. Smith and B. \u00d6lveczky},\n booktitle = {Annual Review of Neuroscience},\n journal = {Annual review of neuroscience},\n pages = {\n          479-498\n        },\n title = {The Role of Variability in Motor Learning.},\n volume = {40},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b7437c88043f070b903ad1dfe2f14b21aa0ec6ab",
            "@type": "ScholarlyArticle",
            "paperId": "b7437c88043f070b903ad1dfe2f14b21aa0ec6ab",
            "corpusId": 17652130,
            "url": "https://www.semanticscholar.org/paper/b7437c88043f070b903ad1dfe2f14b21aa0ec6ab",
            "title": "Reinforcement learning in the robocup-soccer keepaway",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "MAG": "3008729385",
                "CorpusId": 17652130
            },
            "abstract": "Many researchers purpose reinforcement learning (RL) as a form of machine learning for robot learning. However, there are several issues that need to be considered when applying (RL) techniques to robot tasks. There are many different (RL) algorithms available such as Q-learning or Sarsa. These algorithms may produce different results. In complex domains with large states and action spaces is necessary to apply generalization techniques such as function approximation. Last, a right balance between exploration and exploitation is required. In this paper we review these issues in order to improve the learning process in the keepaway domain. We present some new combinations in the choice of the RL algorithm, the generalization method and the exploration-exploitation strategy.",
            "referenceCount": 7,
            "citationCount": 394,
            "influentialCitationCount": 49,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Garc\u00eda2007ReinforcementLI,\n author = {Javier Garc\u00eda and F. Fern\u00e1ndez and M. Veloso},\n pages = {357-366},\n title = {Reinforcement learning in the robocup-soccer keepaway},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9437ec02806f1de7b49cc5700b905a6bb8eb5994",
            "@type": "ScholarlyArticle",
            "paperId": "9437ec02806f1de7b49cc5700b905a6bb8eb5994",
            "corpusId": 1508545,
            "url": "https://www.semanticscholar.org/paper/9437ec02806f1de7b49cc5700b905a6bb8eb5994",
            "title": "Reinforcement learning with limited reinforcement: using Bayes risk for active learning in POMDPs",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2407441816",
                "DBLP": "conf/isaim/DoshiPR08",
                "DOI": "10.1145/1390156.1390189",
                "CorpusId": 1508545,
                "PubMed": "20467572"
            },
            "abstract": "Partially Observable Markov Decision Processes (POMDPs) have succeeded in planning domains that require balancing actions that increase an agent's knowledge and actions that increase an agent's reward. Unfortunately, most POMDPs are defined with a large number of parameters which are difficult to specify only from domain knowledge. In this paper, we present an approximation approach that allows us to treat the POMDP model parameters as additional hidden state in a \"model-uncertainty\" POMDP. Coupled with model-directed queries, our planner actively learns good policies. We demonstrate our approach on several POMDP problems.",
            "referenceCount": 54,
            "citationCount": 104,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-07-05",
            "journal": {
                "name": "Proceedings of the ... International Conference on Machine Learning. International Conference on Machine Learning",
                "volume": "301"
            },
            "citationStyles": {
                "bibtex": "@Article{Doshi-Velez2008ReinforcementLW,\n author = {F. Doshi-Velez and Joelle Pineau and N. Roy},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the ... International Conference on Machine Learning. International Conference on Machine Learning},\n pages = {\n          256-263\n        },\n title = {Reinforcement learning with limited reinforcement: using Bayes risk for active learning in POMDPs},\n volume = {301},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:88c882e835f3a08d4baf44869f64f1ad53cdf9b2",
            "@type": "ScholarlyArticle",
            "paperId": "88c882e835f3a08d4baf44869f64f1ad53cdf9b2",
            "corpusId": 264615888,
            "url": "https://www.semanticscholar.org/paper/88c882e835f3a08d4baf44869f64f1ad53cdf9b2",
            "title": "Distinct roles for direct and indirect pathway striatal neurons in reinforcement",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2012,
            "externalIds": {
                "PubMedCentral": "3410042",
                "MAG": "2022633426",
                "DOI": "10.1038/nn.3100",
                "CorpusId": 264615888,
                "PubMed": "22544310"
            },
            "abstract": null,
            "referenceCount": 24,
            "citationCount": 763,
            "influentialCitationCount": 43,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc3410042?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-04-05",
            "journal": {
                "name": "Nature neuroscience",
                "volume": "15"
            },
            "citationStyles": {
                "bibtex": "@Article{Kravitz2012DistinctRF,\n author = {Alexxai V. Kravitz and Lynne D. Tye and Anatol C. Kreitzer},\n booktitle = {Nature Neuroscience},\n journal = {Nature neuroscience},\n pages = {816 - 818},\n title = {Distinct roles for direct and indirect pathway striatal neurons in reinforcement},\n volume = {15},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5b69da724e0e2c0cfec7a81de3c117cf8983fb55",
            "@type": "ScholarlyArticle",
            "paperId": "5b69da724e0e2c0cfec7a81de3c117cf8983fb55",
            "corpusId": 16251063,
            "url": "https://www.semanticscholar.org/paper/5b69da724e0e2c0cfec7a81de3c117cf8983fb55",
            "title": "Reward, Motivation, and Reinforcement Learning",
            "venue": "Neuron",
            "publicationVenue": {
                "id": "urn:research:7a61412a-9a9a-487d-9613-5a1fbd879c9d",
                "name": "Neuron",
                "alternate_names": null,
                "issn": "0896-6273",
                "url": "https://www.cell.com/neuron/home"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2043615441",
                "DOI": "10.1016/S0896-6273(02)00963-7",
                "CorpusId": 16251063,
                "PubMed": "12383782"
            },
            "abstract": null,
            "referenceCount": 140,
            "citationCount": 745,
            "influentialCitationCount": 26,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cell.com/article/S0896627302009637/pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2002-10-10",
            "journal": {
                "name": "Neuron",
                "volume": "36"
            },
            "citationStyles": {
                "bibtex": "@Article{Dayan2002RewardMA,\n author = {P. Dayan and B. Balleine},\n booktitle = {Neuron},\n journal = {Neuron},\n pages = {285-298},\n title = {Reward, Motivation, and Reinforcement Learning},\n volume = {36},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a9e5a40b0ff5c40d2db7b73490922e115576adb5",
            "@type": "ScholarlyArticle",
            "paperId": "a9e5a40b0ff5c40d2db7b73490922e115576adb5",
            "corpusId": 260534783,
            "url": "https://www.semanticscholar.org/paper/a9e5a40b0ff5c40d2db7b73490922e115576adb5",
            "title": "On the sample complexity of reinforcement learning.",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2003,
            "externalIds": {
                "MAG": "107583932",
                "CorpusId": 260534783
            },
            "abstract": "This thesis is a detailed investigation into the following question: how much data must an agent collect in order to perform \u201creinforcement learning\u201d successfully? This question is analogous to the classical issue of the sample complexity in supervised learning, but is harder because of the increased realism of the reinforcement learning setting. This thesis summarizes recent sample complexity results in the reinforcement learning literature and builds on these results to provide novel algorithms with strong performance guarantees. We focus on a variety of reasonable performance criteria and sampling models by which agents may access the environment. For instance, in a policy search setting, we consider the problem of how much simulated experience is required to reliably choose a \u201cgood\u201d policy among a restricted class of policies II (as in Kearns, Mansour, and Ng [2000]). In a more online setting, we consider the case in which an agent is placed in an environment and must follow one unbroken chain of experience with no access to \u201coffline\u201d simulation (as in Kearns and Singh [1998]). We build on the sample based algorithms suggested by Kearns, Mansour, and Ng [2000]. Their sample complexity bounds have no dependence on the size of the state space, an exponential dependence on the planning horizon time, and linear dependence on the com\u00ad plexity of n. We suggest novel algorithms with more restricted guarantees whose sample complexities are again independent of the size of the state space and depend linearly on the complexity of the policy class II, but have only a polynomial dependence on the horizon time. We pay particular attention to the tradeoffs made by such algorithms.",
            "referenceCount": 64,
            "citationCount": 647,
            "influentialCitationCount": 92,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Kakade2003OnTS,\n author = {S. Kakade},\n title = {On the sample complexity of reinforcement learning.},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:187f3f984e6f375178f41827ab90c4e748773fa7",
            "@type": "ScholarlyArticle",
            "paperId": "187f3f984e6f375178f41827ab90c4e748773fa7",
            "corpusId": 207159693,
            "url": "https://www.semanticscholar.org/paper/187f3f984e6f375178f41827ab90c4e748773fa7",
            "title": "PAC model-free reinforcement learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "conf/icml/StrehlLWLL06",
                "MAG": "2129670787",
                "DOI": "10.1145/1143844.1143955",
                "CorpusId": 207159693
            },
            "abstract": "For a Markov Decision Process with finite state (size S) and action spaces (size A per state), we propose a new algorithm---Delayed Q-Learning. We prove it is PAC, achieving near optimal performance except for \u00d5(SA) timesteps using O(SA) space, improving on the \u00d5(S2 A) bounds of best previous algorithms. This result proves efficient reinforcement learning is possible without learning a model of the MDP from experience. Learning takes place from a single continuous thread of experience---no resets nor parallel sampling is used. Beyond its smaller storage and experience requirements, Delayed Q-learning's per-experience computation cost is much less than that of previous PAC algorithms.",
            "referenceCount": 13,
            "citationCount": 480,
            "influentialCitationCount": 35,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2006-06-25",
            "journal": {
                "name": "Proceedings of the 23rd international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Strehl2006PACMR,\n author = {Alexander L. Strehl and Lihong Li and Eric Wiewiora and J. Langford and M. Littman},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the 23rd international conference on Machine learning},\n title = {PAC model-free reinforcement learning},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1debadf7e6c580a280919df9b9cef5bc52b50100",
            "@type": "ScholarlyArticle",
            "paperId": "1debadf7e6c580a280919df9b9cef5bc52b50100",
            "corpusId": 14876236,
            "url": "https://www.semanticscholar.org/paper/1debadf7e6c580a280919df9b9cef5bc52b50100",
            "title": "Reinforcement Learning Through Modulation of Spike-Timing-Dependent Synaptic Plasticity",
            "venue": "Neural Computation",
            "publicationVenue": {
                "id": "urn:research:69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                "name": "Neural Computation",
                "alternate_names": [
                    "Neural Comput"
                ],
                "issn": "0899-7667",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667"
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "journals/neco/Florian07",
                "MAG": "2126404188",
                "DOI": "10.1162/neco.2007.19.6.1468",
                "CorpusId": 14876236,
                "PubMed": "17444757"
            },
            "abstract": "Abstract The persistent modification of synaptic efficacy as a function of the relative timing of pre- and postsynaptic spikes is a phenomenon known as spike-timing-dependent plasticity (STDP). Here we show that the modulation of STDP by a global reward signal leads to reinforcement learning. We first derive analytically learning rules involving reward-modulated spike-timing-dependent synaptic and intrinsic plasticity, by applying a reinforcement learning algorithm to the stochastic spike response model of spiking neurons. These rules have several features common to plasticity mechanisms experimentally found in the brain. We then demonstrate in simulations of networks of integrate-and-fire neurons the efficacy of two simple learning rules involving modulated STDP. One rule is a direct extension of the standard STDP model (modulated STDP), and the other one involves an eligibility trace stored at each synapse that keeps a decaying memory of the relationships between the recent pairs of pre- and postsynaptic spike pairs (modulated STDP with eligibility trace). This latter rule permits learning even if the reward signal is delayed. The proposed rules are able to solve the XOR problem with both rate coded and temporally coded input and to learn a target output firing-rate pattern. These learning rules are biologically plausible, may be used for training generic artificial spiking neural networks, regardless of the neural model used, and suggest the experimental investigation in animals of the existence of reward-modulated STDP.",
            "referenceCount": 91,
            "citationCount": 336,
            "influentialCitationCount": 38,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-06-01",
            "journal": {
                "name": "Neural Computation",
                "volume": "19"
            },
            "citationStyles": {
                "bibtex": "@Article{Florian2007ReinforcementLT,\n author = {Razvan V. Florian},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {1468-1502},\n title = {Reinforcement Learning Through Modulation of Spike-Timing-Dependent Synaptic Plasticity},\n volume = {19},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:638ab3eb4723add266e484f8f792cd6e14ce5e3e",
            "@type": "ScholarlyArticle",
            "paperId": "638ab3eb4723add266e484f8f792cd6e14ce5e3e",
            "corpusId": 6225453,
            "url": "https://www.semanticscholar.org/paper/638ab3eb4723add266e484f8f792cd6e14ce5e3e",
            "title": "Multi-task reinforcement learning: a hierarchical Bayesian approach",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2169743339",
                "DBLP": "conf/icml/WilsonFRT07",
                "DOI": "10.1145/1273496.1273624",
                "CorpusId": 6225453
            },
            "abstract": "We consider the problem of multi-task reinforcement learning, where the agent needs to solve a sequence of Markov Decision Processes (MDPs) chosen randomly from a fixed but unknown distribution. We model the distribution over MDPs using a hierarchical Bayesian infinite mixture model. For each novel MDP, we use the previously learned distribution as an informed prior for modelbased Bayesian reinforcement learning. The hierarchical Bayesian framework provides a strong prior that allows us to rapidly infer the characteristics of new environments based on previous environments, while the use of a nonparametric model allows us to quickly adapt to environments we have not encountered before. In addition, the use of infinite mixtures allows for the model to automatically learn the number of underlying MDP components. We evaluate our approach and show that it leads to significant speedups in convergence to an optimal policy after observing only a small number of tasks.",
            "referenceCount": 12,
            "citationCount": 324,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-06-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wilson2007MultitaskRL,\n author = {Aaron Wilson and Alan Fern and Soumya Ray and Prasad Tadepalli},\n booktitle = {International Conference on Machine Learning},\n pages = {1015-1022},\n title = {Multi-task reinforcement learning: a hierarchical Bayesian approach},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:670c954c0f151564b6c0caa4a6dbc8ee010bb8ba",
            "@type": "ScholarlyArticle",
            "paperId": "670c954c0f151564b6c0caa4a6dbc8ee010bb8ba",
            "corpusId": 15288363,
            "url": "https://www.semanticscholar.org/paper/670c954c0f151564b6c0caa4a6dbc8ee010bb8ba",
            "title": "Reconciling reinforcement learning models with behavioral extinction and renewal: implications for addiction, relapse, and problem gambling.",
            "venue": "Psychology Review",
            "publicationVenue": {
                "id": "urn:research:dfb7f114-609c-4c34-9ee7-8cb1fc3e4a6b",
                "name": "Psychology Review",
                "alternate_names": [
                    "Psychol rev",
                    "Psychological review",
                    "Psychol Rev",
                    "Psychological Review"
                ],
                "issn": "1354-1129",
                "url": "http://www.apa.org/journals/rev/"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2136820623",
                "DOI": "10.1037/0033-295X.114.3.784",
                "CorpusId": 15288363,
                "PubMed": "17638506"
            },
            "abstract": "Because learned associations are quickly renewed following extinction, the extinction process must include processes other than unlearning. However, reinforcement learning models, such as the temporal difference reinforcement learning (TDRL) model, treat extinction as an unlearning of associated value and are thus unable to capture renewal. TDRL models are based on the hypothesis that dopamine carries a reward prediction error signal; these models predict reward by driving that reward error to zero. The authors construct a TDRL model that can accommodate extinction and renewal through two simple processes: (a) a TDRL process that learns the value of situation-action pairs and (b) a situation recognition process that categorizes the observed cues into situations. This model has implications for dysfunctional states, including relapse after addiction and problem gambling.",
            "referenceCount": 235,
            "citationCount": 326,
            "influentialCitationCount": 20,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.gatsby.ucl.ac.uk/~beierh/neuro_jc/redish_etal_07.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-07-01",
            "journal": {
                "name": "Psychological review",
                "volume": "114 3"
            },
            "citationStyles": {
                "bibtex": "@Article{Redish2007ReconcilingRL,\n author = {A. Redish and Steve Jensen and Adam Johnson and Z. Kurth-Nelson and Erin B. Larson and J. Gewirtz and Y. Niv and Betsy Murray and Jadin C. Jackson and Mark J. Thomas and Daniel Smith and Karim Nader and Josh Gordon and Greg Quirk and Martin Paulus and S. Floresco and G. Schoenbaum and Steve and David Redish},\n booktitle = {Psychology Review},\n journal = {Psychological review},\n pages = {\n          784-805\n        },\n title = {Reconciling reinforcement learning models with behavioral extinction and renewal: implications for addiction, relapse, and problem gambling.},\n volume = {114 3},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1678bd32846b1aded5b1e80a617170812e80f562",
            "@type": "ScholarlyArticle",
            "paperId": "1678bd32846b1aded5b1e80a617170812e80f562",
            "corpusId": 2801572,
            "url": "https://www.semanticscholar.org/paper/1678bd32846b1aded5b1e80a617170812e80f562",
            "title": "Feudal Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1992,
            "externalIds": {
                "MAG": "2296330590",
                "DBLP": "conf/nips/DayanH92",
                "CorpusId": 2801572
            },
            "abstract": "One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a Q-learning managerial hierarchy in which high level managers learn how to set tasks to their submanagers who, in turn, learn how to satisfy them. Submanagers need not initially understand their managers' commands. They simply learn to maximise their reinforcement in the context of the current command. \n \nWe illustrate the system using a simple maze task. As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-learning and builds a more comprehensive map.",
            "referenceCount": 10,
            "citationCount": 752,
            "influentialCitationCount": 50,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Business",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1992-11-30",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dayan1992FeudalRL,\n author = {P. Dayan and Geoffrey E. Hinton},\n booktitle = {Neural Information Processing Systems},\n pages = {271-278},\n title = {Feudal Reinforcement Learning},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:74921bc8762812345b7010746faa22988b85252e",
            "@type": "ScholarlyArticle",
            "paperId": "74921bc8762812345b7010746faa22988b85252e",
            "corpusId": 29052647,
            "url": "https://www.semanticscholar.org/paper/74921bc8762812345b7010746faa22988b85252e",
            "title": "Prioritized sweeping: Reinforcement learning with less data and less time",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "journals/ml/MooreA93",
                "DOI": "10.1007/BF00993104",
                "CorpusId": 29052647
            },
            "abstract": null,
            "referenceCount": 27,
            "citationCount": 615,
            "influentialCitationCount": 43,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/BF00993104.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Moore2004PrioritizedSR,\n author = {A. Moore and C. Atkeson},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {103-130},\n title = {Prioritized sweeping: Reinforcement learning with less data and less time},\n volume = {13},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ad0d1149875291d9b1177bc47e53e09237beeca0",
            "@type": "ScholarlyArticle",
            "paperId": "ad0d1149875291d9b1177bc47e53e09237beeca0",
            "corpusId": 12698722,
            "url": "https://www.semanticscholar.org/paper/ad0d1149875291d9b1177bc47e53e09237beeca0",
            "title": "Quantum-enhanced machine learning",
            "venue": "Physical Review Letters",
            "publicationVenue": {
                "id": "urn:research:16c9f9d4-bee1-435d-8c85-22a3deba109d",
                "name": "Physical Review Letters",
                "alternate_names": [
                    "Phys Rev Lett"
                ],
                "issn": "0031-9007",
                "url": "https://journals.aps.org/prl/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/DunjkoTB16",
                "ArXiv": "1610.08251",
                "MAG": "2521267242",
                "DOI": "10.1103/PhysRevLett.117.130501",
                "CorpusId": 12698722,
                "PubMed": "27715099"
            },
            "abstract": "The emerging field of quantum machine learning has the potential to substantially aid in the problems and scope of artificial intelligence. This is only enhanced by recent successes in the field of classical machine learning. In this work we propose an approach for the systematic treatment of machine learning, from the perspective of quantum information. Our approach is general and covers all three main branches of machine learning: supervised, unsupervised, and reinforcement learning. While quantum improvements in supervised and unsupervised learning have been reported, reinforcement learning has received much less attention. Within our approach, we tackle the problem of quantum enhancements in reinforcement learning as well, and propose a systematic scheme for providing improvements. As an example, we show that quadratic improvements in learning efficiency, and exponential improvements in performance over limited time periods, can be obtained for a broad class of learning problems.",
            "referenceCount": 37,
            "citationCount": 284,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://link.aps.org/pdf/10.1103/PhysRevLett.117.130501",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Physics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-09-20",
            "journal": {
                "name": "Physical review letters",
                "volume": "117 13"
            },
            "citationStyles": {
                "bibtex": "@Article{Dunjko2016QuantumenhancedML,\n author = {V. Dunjko and Jacob M. Taylor and H. Briegel},\n booktitle = {Physical Review Letters},\n journal = {Physical review letters},\n pages = {\n          130501\n        },\n title = {Quantum-enhanced machine learning},\n volume = {117 13},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:10b9283146b71ee5d38238eb3b30732a3d2bdbb2",
            "@type": "ScholarlyArticle",
            "paperId": "10b9283146b71ee5d38238eb3b30732a3d2bdbb2",
            "corpusId": 5539001,
            "url": "https://www.semanticscholar.org/paper/10b9283146b71ee5d38238eb3b30732a3d2bdbb2",
            "title": "Learning deep control policies for autonomous aerial vehicles with MPC-guided policy search",
            "venue": "IEEE International Conference on Robotics and Automation",
            "publicationVenue": {
                "id": "urn:research:3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                "name": "IEEE International Conference on Robotics and Automation",
                "alternate_names": [
                    "International Conference on Robotics and Automation",
                    "Int Conf Robot Autom",
                    "ICRA",
                    "IEEE Int Conf Robot Autom"
                ],
                "issn": "2152-4092",
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1509.06791",
                "DBLP": "journals/corr/ZhangKLA15",
                "MAG": "2952366022",
                "DOI": "10.1109/ICRA.2016.7487175",
                "CorpusId": 5539001
            },
            "abstract": "Model predictive control (MPC) is an effective method for controlling robotic systems, particularly autonomous aerial vehicles such as quadcopters. However, application of MPC can be computationally demanding, and typically requires estimating the state of the system, which can be challenging in complex, unstructured environments. Reinforcement learning can in principle forego the need for explicit state estimation and acquire a policy that directly maps sensor readings to actions, but is difficult to apply to unstable systems that are liable to fail catastrophically during training before an effective policy has been found. We propose to combine MPC with reinforcement learning in the framework of guided policy search, where MPC is used to generate data at training time, under full state observations provided by an instrumented training environment. This data is used to train a deep neural network policy, which is allowed to access only the raw observations from the vehicle's onboard sensors. After training, the neural network policy can successfully control the robot without knowledge of the full state, and at a fraction of the computational cost of MPC. We evaluate our method by learning obstacle avoidance policies for a simulated quadrotor, using simulated onboard sensors and no explicit state estimation at test time.",
            "referenceCount": 44,
            "citationCount": 397,
            "influentialCitationCount": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1509.06791",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-09-22",
            "journal": {
                "name": "2016 IEEE International Conference on Robotics and Automation (ICRA)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2015LearningDC,\n author = {Tianhao Zhang and G. Kahn and S. Levine and P. Abbeel},\n booktitle = {IEEE International Conference on Robotics and Automation},\n journal = {2016 IEEE International Conference on Robotics and Automation (ICRA)},\n pages = {528-535},\n title = {Learning deep control policies for autonomous aerial vehicles with MPC-guided policy search},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1f869232f148ec52066fab06a49855937f84098b",
            "@type": "ScholarlyArticle",
            "paperId": "1f869232f148ec52066fab06a49855937f84098b",
            "corpusId": 11551208,
            "url": "https://www.semanticscholar.org/paper/1f869232f148ec52066fab06a49855937f84098b",
            "title": "Reinforcement learning by reward-weighted regression for operational space control",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "conf/icml/PetersS07",
                "MAG": "2109169869",
                "DOI": "10.1145/1273496.1273590",
                "CorpusId": 11551208
            },
            "abstract": "Many robot control problems of practical importance, including operational space control, can be reformulated as immediate reward reinforcement learning problems. However, few of the known optimization or reinforcement learning algorithms can be used in online learning control for robots, as they are either prohibitively slow, do not scale to interesting domains of complex robots, or require trying out policies generated by random search, which are infeasible for a physical system. Using a generalization of the EM-base reinforcement learning framework suggested by Dayan & Hinton, we reduce the problem of learning with immediate rewards to a reward-weighted regression problem with an adaptive, integrated reward transformation for faster convergence. The resulting algorithm is efficient, learns smoothly without dangerous jumps in solution space, and works well in applications of complex high degree-of-freedom robots.",
            "referenceCount": 14,
            "citationCount": 233,
            "influentialCitationCount": 31,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://pure.mpg.de/pubman/item/item_1790444_3/component/file_3075658/ICML-2007-Peters.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-06-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Peters2007ReinforcementLB,\n author = {Jan Peters and S. Schaal},\n booktitle = {International Conference on Machine Learning},\n pages = {745-750},\n title = {Reinforcement learning by reward-weighted regression for operational space control},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9db77cb43da46de6b0c5350348d74c949112e1e1",
            "@type": "ScholarlyArticle",
            "paperId": "9db77cb43da46de6b0c5350348d74c949112e1e1",
            "corpusId": 6447995,
            "url": "https://www.semanticscholar.org/paper/9db77cb43da46de6b0c5350348d74c949112e1e1",
            "title": "Cross-domain transfer for reinforcement learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "conf/icml/TaylorS07",
                "MAG": "2164114810",
                "DOI": "10.1145/1273496.1273607",
                "CorpusId": 6447995
            },
            "abstract": "A typical goal for transfer learning algorithms is to utilize knowledge gained in a source task to learn a target task faster. Recently introduced transfer methods in reinforcement learning settings have shown considerable promise, but they typically transfer between pairs of very similar tasks. This work introduces Rule Transfer, a transfer algorithm that first learns rules to summarize a source task policy and then leverages those rules to learn faster in a target task. This paper demonstrates that Rule Transfer can effectively speed up learning in Keepaway, a benchmark RL problem in the robot soccer domain, based on experience from source tasks in the gridworld domain. We empirically show, through the use of three distinct transfer metrics, that Rule Transfer is effective across these domains.",
            "referenceCount": 11,
            "citationCount": 210,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.utexas.edu/users/pstone/Papers/bib2html-links/ICML07-taylor.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-06-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Taylor2007CrossdomainTF,\n author = {Matthew E. Taylor and P. Stone},\n booktitle = {International Conference on Machine Learning},\n pages = {879-886},\n title = {Cross-domain transfer for reinforcement learning},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7617db33f278d185f8455bedcf0b4899fb76105a",
            "@type": "ScholarlyArticle",
            "paperId": "7617db33f278d185f8455bedcf0b4899fb76105a",
            "corpusId": 14109474,
            "url": "https://www.semanticscholar.org/paper/7617db33f278d185f8455bedcf0b4899fb76105a",
            "title": "Reinforcement Learning in Continuous Action Spaces",
            "venue": "2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2101539915",
                "DOI": "10.1109/ADPRL.2007.368199",
                "CorpusId": 14109474
            },
            "abstract": "Quite some research has been done on reinforcement learning in continuous environments, but the research on problems where the actions can also be chosen from a continuous space is much more limited. We present a new class of algorithms named continuous actor critic learning automaton (CACLA) that can handle continuous states and actions. The resulting algorithm is straightforward to implement. An experimental comparison is made between this algorithm and other algorithms that can handle continuous action spaces. These experiments show that CACLA performs much better than the other algorithms, especially when it is combined with a Gaussian exploration method",
            "referenceCount": 10,
            "citationCount": 233,
            "influentialCitationCount": 23,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dspace.library.uu.nl/bitstream/handle/1874/25514/wiering_07_reinforcementlearning.pdf?sequence=1&isAllowed=y",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2007-04-01",
            "journal": {
                "name": "2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hasselt2007ReinforcementLI,\n author = {Hado Philip van Hasselt and M. Wiering},\n booktitle = {2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning},\n journal = {2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning},\n pages = {272-279},\n title = {Reinforcement Learning in Continuous Action Spaces},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:16050a256dd6add1e9187e8c4f5c30c85f342fd8",
            "@type": "ScholarlyArticle",
            "paperId": "16050a256dd6add1e9187e8c4f5c30c85f342fd8",
            "corpusId": 9681429,
            "url": "https://www.semanticscholar.org/paper/16050a256dd6add1e9187e8c4f5c30c85f342fd8",
            "title": "Building Portable Options: Skill Transfer in Reinforcement Learning",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "1492014007",
                "DBLP": "conf/ijcai/KonidarisB07",
                "CorpusId": 9681429
            },
            "abstract": "The options framework provides methods for reinforcement learning agents to build new high-level skills. However, since options are usually learned in the same state space as the problem the agent is solving, they cannot be used in other tasks that are similar but have different state spaces. We introduce the notion of learning options in agentspace, the space generated by a feature set that is present and retains the same semantics across successive problem instances, rather than in problemspace. Agent-space options can be reused in later tasks that share the same agent-space but have different problem-spaces. We present experimental results demonstrating the use of agent-space options in building transferrable skills, and show that they perform best when used in conjunction with problem-space options.",
            "referenceCount": 30,
            "citationCount": 280,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2007-01-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Konidaris2007BuildingPO,\n author = {G. Konidaris and A. Barto},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {895-900},\n title = {Building Portable Options: Skill Transfer in Reinforcement Learning},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:413b32dc8855f9db4c95657a3de5ca6c1d793da0",
            "@type": "ScholarlyArticle",
            "paperId": "413b32dc8855f9db4c95657a3de5ca6c1d793da0",
            "corpusId": 7013049,
            "url": "https://www.semanticscholar.org/paper/413b32dc8855f9db4c95657a3de5ca6c1d793da0",
            "title": "Policy gradient reinforcement learning for fast quadrupedal locomotion",
            "venue": "IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "conf/icra/KohlS04",
                "MAG": "2139053308",
                "DOI": "10.1109/ROBOT.2004.1307456",
                "CorpusId": 7013049
            },
            "abstract": "This paper presents a machine learning approach to optimizing a quadrupedal trot gait for forward speed. Given a parameterized walk designed for a specific robot, we propose using a form of policy gradient reinforcement learning to automatically search the set of possible parameters with the goal of finding the fastest possible walk. We implement and test our approach on a commercially available quadrupedal robot platform, namely the Sony Aibo robot. After about three hours of learning, all on the physical robots and with no human intervention other than to change the batteries, the robots achieved a gait faster than any previously known gait known for the Aibo, significantly outperforming a variety of existing hand-coded and learned solutions.",
            "referenceCount": 22,
            "citationCount": 624,
            "influentialCitationCount": 31,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-07-06",
            "journal": {
                "name": "IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Kohl2004PolicyGR,\n author = {Nate Kohl and P. Stone},\n booktitle = {IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004},\n journal = {IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004},\n pages = {2619-2624 Vol.3},\n title = {Policy gradient reinforcement learning for fast quadrupedal locomotion},\n volume = {3},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ee40e503717169171b2591a71e7dc6abdbe4b571",
            "@type": "ScholarlyArticle",
            "paperId": "ee40e503717169171b2591a71e7dc6abdbe4b571",
            "corpusId": 1563147,
            "url": "https://www.semanticscholar.org/paper/ee40e503717169171b2591a71e7dc6abdbe4b571",
            "title": "A Hybrid Reinforcement Learning Approach to Autonomic Resource Allocation",
            "venue": "International Conference on Automation and Computing",
            "publicationVenue": {
                "id": "urn:research:28a83bf1-b6e5-416f-8c5a-fdc53e54f3a1",
                "name": "International Conference on Automation and Computing",
                "alternate_names": [
                    "IEEE International Conference on Autonomic Computing",
                    "IEEE Int Conf Auton Comput",
                    "Int Conf Autom Comput",
                    "ICAC",
                    "International Conference on Autonomic Computing",
                    "Int Conf Auton Comput"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=1269"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "conf/icac/TesauroJDB06",
                "MAG": "2102558581",
                "DOI": "10.1109/ICAC.2006.1662383",
                "CorpusId": 1563147
            },
            "abstract": "Reinforcement Learning (RL) provides a promising new approach to systems performance management that differs radically from standard queuing-theoretic approaches making use of explicit system performance models. In principle, RL can automatically learn high-quality management policies without an explicit performance model or traffic model and with little or no built-in system specific knowledge. In our original work [1], [2], [3] we showed the feasibility of using online RL to learn resource valuation estimates (in lookup table form) which can be used to make high-quality server allocation decisions in a multi-application prototype Data Center scenario. The present work shows how to combine the strengths of both RL and queuing models in a hybrid approach in which RL trains offline on data collected while a queuing model policy controls the system. By training offline we avoid suffering potentially poor performance in live online training. We also now use RL to train nonlinear function approximators (e.g. multi-layer perceptrons) instead of lookup tables; this enables scaling to substantially larger state spaces. Our results now show that in both open-loop and closed-loop traffic, hybrid RL training can achieve significant performance improvements over a variety of initial model-based policies. We also find that, as expected, RL can deal effectively with both transients and switching delays, which lie outside the scope of traditional steady-state queuing theory.",
            "referenceCount": 26,
            "citationCount": 368,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2006-06-12",
            "journal": {
                "name": "2006 IEEE International Conference on Autonomic Computing",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tesauro2006AHR,\n author = {G. Tesauro and Nicholas K. Jong and Rajarshi Das and M. Bennani},\n booktitle = {International Conference on Automation and Computing},\n journal = {2006 IEEE International Conference on Autonomic Computing},\n pages = {65-73},\n title = {A Hybrid Reinforcement Learning Approach to Autonomic Resource Allocation},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3db0e4597c64576178d51410b1d7615a11e6f0dc",
            "@type": "ScholarlyArticle",
            "paperId": "3db0e4597c64576178d51410b1d7615a11e6f0dc",
            "corpusId": 2411167,
            "url": "https://www.semanticscholar.org/paper/3db0e4597c64576178d51410b1d7615a11e6f0dc",
            "title": "Kernel-Based Reinforcement Learning",
            "venue": "International Conference on Intelligent Computing",
            "publicationVenue": {
                "id": "urn:research:47e4f61e-9b0e-431f-bd17-e534b53c655b",
                "name": "International Conference on Intelligent Computing",
                "alternate_names": [
                    "ICIC",
                    "Int Conf Ind Instrum Control",
                    "Int Conf Intell Comput",
                    "International Conference on Industrial Instrumentation and Control"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=1382"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "conf/icic/HuQX06",
                "MAG": "1489912246",
                "DOI": "10.1007/11816157_92",
                "CorpusId": 2411167
            },
            "abstract": null,
            "referenceCount": 12,
            "citationCount": 379,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-08-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hu2006KernelBasedRL,\n author = {Guanghua Hu and Yuqin Qiu and Liming Xiang},\n booktitle = {International Conference on Intelligent Computing},\n pages = {757-766},\n title = {Kernel-Based Reinforcement Learning},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1d0b5a4af8d74e5474fca65790eb2eb9e1e0fb44",
            "@type": "ScholarlyArticle",
            "paperId": "1d0b5a4af8d74e5474fca65790eb2eb9e1e0fb44",
            "corpusId": 18619671,
            "url": "https://www.semanticscholar.org/paper/1d0b5a4af8d74e5474fca65790eb2eb9e1e0fb44",
            "title": "Constrained reinforcement learning from intrinsic and extrinsic rewards",
            "venue": "2007 IEEE 6th International Conference on Development and Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2155511972",
                "DOI": "10.1109/DEVLRN.2007.4354030",
                "CorpusId": 18619671
            },
            "abstract": "The main objective of a standard reinforcement learner is usually defined as maximization of a scalar reward function given externally from the environment. On the other hand, an intrinsically motivated reinforcement learner creates an intrinsic reward function from its own criteria such as curiosity, prediction error, and learning progress. This paper proposes a novel approach to deal with both intrinsic and extrinsic rewards for reinforcement learning from a viewpoint of constrained optimization problem. The extrinsic rewards construct inequality constraints to the stochastic policy while the intrinsic reward determines the current objective function for the learning agent. By integrating policy gradient reinforcement learning algorithms and techniques used in nonlinear programming, our proposed method, named the constrained policy gradient reinforcement learning (CPGRL), maximizes the long-term average intrinsic reward under the inequality constraints induced by the extrinsic rewards. The CPGRL is successfully applied to a simple MDP problem and a control task of a robot arm.",
            "referenceCount": 22,
            "citationCount": 53,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://openresearchlibrary.org/ext/api/media/530ed526-f64e-4165-8f32-4da6b07e8100/assets/external_content.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference"
            ],
            "publicationDate": "2007-07-11",
            "journal": {
                "name": "2007 IEEE 6th International Conference on Development and Learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Conference{Uchibe2007ConstrainedRL,\n author = {E. Uchibe and K. Doya},\n booktitle = {2007 IEEE 6th International Conference on Development and Learning},\n journal = {2007 IEEE 6th International Conference on Development and Learning},\n pages = {163-168},\n title = {Constrained reinforcement learning from intrinsic and extrinsic rewards},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dfba5cd86e37b46470ba64e4e4933b8ad247f419",
            "@type": "ScholarlyArticle",
            "paperId": "dfba5cd86e37b46470ba64e4e4933b8ad247f419",
            "corpusId": 52799874,
            "url": "https://www.semanticscholar.org/paper/dfba5cd86e37b46470ba64e4e4933b8ad247f419",
            "title": "Reinforcement learning: Computational theory and biological mechanisms",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2091789872",
                "DOI": "10.2976/1.2732246/10.2976/1",
                "CorpusId": 52799874
            },
            "abstract": "Reinforcement learning is a computational framework for an active agent to learn behaviors on the basis of a scalar reward signal. The agent can be an animal, a human, or an artificial system such as a robot or a computer program. The reward can be food, water, money, or whatever measure of the performance of the agent. The theory of reinforcement learning, which was developed in an artificial intelligence community with intuitions from animal learning theory, is now giving a coherent account on the function of the basal ganglia. It now serves as the \u201ccommon language\u201d in which biologists, engineers, and social scientists can exchange their problems and findings. This article reviews the basic theoretical framework of reinforcement learning and discusses its recent and future contributions toward the understanding of animal behaviors and human decision making.",
            "referenceCount": 72,
            "citationCount": 134,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.tandfonline.com/doi/pdf/10.2976/1.2732246/10.2976/1?needAccess=true",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2007-05-01",
            "journal": {
                "name": "HFSP Journal",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Doya2007ReinforcementLC,\n author = {K. Doya},\n journal = {HFSP Journal},\n pages = {30 - 40},\n title = {Reinforcement learning: Computational theory and biological mechanisms},\n volume = {1},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:587b8014cae8d8ec7dce42a111768d449d8d69ce",
            "@type": "ScholarlyArticle",
            "paperId": "587b8014cae8d8ec7dce42a111768d449d8d69ce",
            "corpusId": 11500621,
            "url": "https://www.semanticscholar.org/paper/587b8014cae8d8ec7dce42a111768d449d8d69ce",
            "title": "Evolving internal reinforcers for an intrinsically motivated reinforcement-learning robot",
            "venue": "2007 IEEE 6th International Conference on Development and Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2167815629",
                "DOI": "10.1109/DEVLRN.2007.4354052",
                "CorpusId": 11500621
            },
            "abstract": "Intrinsically motivated reinforcement learning (IMRL) has been proposed as a framework within which agents exploit \"internal reinforcement\" to acquire general-purpose building-block behaviors (\"skills\") which can be later combined for solving several specific tasks. The architectures so far proposed within this framework are limited in that: (1) they use hardwired \"salient events\" to form and train skills, and this limits agents' autonomy; (2) they are applicable only to problems with abstract states and actions, as grid-world problems. This paper proposes solutions to these problems in the form of a hierarchical reinforcement-learning architecture that: (1) exploits evolutionary robotics techniques so to allow the system to autonomously discover \"salient events\"; (2) uses neural networks so to allow the system to cope with continuous states and noisy environments. The viability of the proposed approach is demonstrated with a simulated robotic scenario.",
            "referenceCount": 22,
            "citationCount": 97,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://laral.istc.cnr.it/mirolli/papers/SchembriMirolliBaldassarre2007EvolvingInternalReinforcersIMRLRobot.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference"
            ],
            "publicationDate": "2007-07-11",
            "journal": {
                "name": "2007 IEEE 6th International Conference on Development and Learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Conference{Schembri2007EvolvingIR,\n author = {Massimiliano Schembri and M. Mirolli and G. Baldassarre},\n booktitle = {2007 IEEE 6th International Conference on Development and Learning},\n journal = {2007 IEEE 6th International Conference on Development and Learning},\n pages = {282-287},\n title = {Evolving internal reinforcers for an intrinsically motivated reinforcement-learning robot},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8ba4952e652e04649cddddb3e1eba5ad4362036f",
            "@type": "ScholarlyArticle",
            "paperId": "8ba4952e652e04649cddddb3e1eba5ad4362036f",
            "corpusId": 14492380,
            "url": "https://www.semanticscholar.org/paper/8ba4952e652e04649cddddb3e1eba5ad4362036f",
            "title": "Collaborative Multiagent Reinforcement Learning by Payoff Propagation",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2110906765",
                "DBLP": "journals/jmlr/KokV06",
                "CorpusId": 14492380
            },
            "abstract": "In this article we describe a set of scalable techniques for learning the behavior of a group of agents in a collaborative multiagent setting. As a basis we use the framework of coordination graphs of Guestrin, Koller, and Parr (2002a) which exploits the dependencies between agents to decompose the global payoff function into a sum of local terms. First, we deal with the single-state case and describe a payoff propagation algorithm that computes the individual actions that approximately maximize the global payoff function. The method can be viewed as the decision-making analogue of belief propagation in Bayesian networks. Second, we focus on learning the behavior of the agents in sequential decision-making tasks. We introduce different model-free reinforcement-learning techniques, unitedly called Sparse Cooperative Q-learning, which approximate the global action-value function based on the topology of a coordination graph, and perform updates using the contribution of the individual agents to the maximal global action value. The combined use of an edge-based decomposition of the action-value function and the payoff propagation algorithm for efficient action selection, result in an approach that scales only linearly in the problem size. We provide experimental evidence that our method outperforms related multiagent reinforcement-learning methods based on temporal differences.",
            "referenceCount": 58,
            "citationCount": 330,
            "influentialCitationCount": 41,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Kok2006CollaborativeMR,\n author = {J. R. Kok and N. Vlassis},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1789-1828},\n title = {Collaborative Multiagent Reinforcement Learning by Payoff Propagation},\n volume = {7},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:94e9d4c63268abdebcff529c452a30b272221b5c",
            "@type": "ScholarlyArticle",
            "paperId": "94e9d4c63268abdebcff529c452a30b272221b5c",
            "corpusId": 15005997,
            "url": "https://www.semanticscholar.org/paper/94e9d4c63268abdebcff529c452a30b272221b5c",
            "title": "Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2097931172",
                "DBLP": "conf/nips/AuerO06",
                "DOI": "10.7551/mitpress/7503.003.0011",
                "CorpusId": 15005997
            },
            "abstract": "We present a learning algorithm for undiscounted reinforcement learning. Our interest lies in bounds for the algorithm's online performance after some finite number of steps. In the spirit of similar methods already successfully applied for the exploration-exploitation tradeoff in multi-armed bandit problems, we use upper confidence bounds to show that our UCRL algorithm achieves logarithmic online regret in the number of steps taken with respect to an optimal policy.",
            "referenceCount": 17,
            "citationCount": 231,
            "influentialCitationCount": 48,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://papers.nips.cc/paper/3052-logarithmic-online-regret-bounds-for-undiscounted-reinforcement-learning.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2006-12-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Auer2006LogarithmicOR,\n author = {P. Auer and R. Ortner},\n booktitle = {Neural Information Processing Systems},\n pages = {49-56},\n title = {Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:19951eb192fb5ba73c9958568777b57b9044f2ce",
            "@type": "ScholarlyArticle",
            "paperId": "19951eb192fb5ba73c9958568777b57b9044f2ce",
            "corpusId": 18928198,
            "url": "https://www.semanticscholar.org/paper/19951eb192fb5ba73c9958568777b57b9044f2ce",
            "title": "On adaptation, maximization, and reinforcement learning among cognitive strategies.",
            "venue": "Psychology Review",
            "publicationVenue": {
                "id": "urn:research:dfb7f114-609c-4c34-9ee7-8cb1fc3e4a6b",
                "name": "Psychology Review",
                "alternate_names": [
                    "Psychol rev",
                    "Psychological review",
                    "Psychol Rev",
                    "Psychological Review"
                ],
                "issn": "1354-1129",
                "url": "http://www.apa.org/journals/rev/"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2090148394",
                "DOI": "10.1037/0033-295X.112.4.912",
                "CorpusId": 18928198,
                "PubMed": "16262473"
            },
            "abstract": "Analysis of binary choice behavior in iterated tasks with immediate feedback reveals robust deviations from maximization that can be described as indications of 3 effects: (a) a payoff variability effect, in which high payoff variability seems to move choice behavior toward random choice; (b) underweighting of rare events, in which alternatives that yield the best payoffs most of the time are attractive even when they are associated with a lower expected return; and (c) loss aversion, in which alternatives that minimize the probability of losses can be more attractive than those that maximize expected payoffs. The results are closer to probability matching than to maximization. Best approximation is provided with a model of reinforcement learning among cognitive strategies (RELACS). This model captures the 3 deviations, the learning curves, and the effect of information on uncertainty avoidance. It outperforms other models in fitting the data and in predicting behavior in other experiments.",
            "referenceCount": 97,
            "citationCount": 457,
            "influentialCitationCount": 39,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2005-10-01",
            "journal": {
                "name": "Psychological review",
                "volume": "112 4"
            },
            "citationStyles": {
                "bibtex": "@Article{Erev2005OnAM,\n author = {Ido Erev and Greg Barron},\n booktitle = {Psychology Review},\n journal = {Psychological review},\n pages = {\n          912-931\n        },\n title = {On adaptation, maximization, and reinforcement learning among cognitive strategies.},\n volume = {112 4},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:40e4d0eaff2ce7538ecff773bda326388a87b515",
            "@type": "ScholarlyArticle",
            "paperId": "40e4d0eaff2ce7538ecff773bda326388a87b515",
            "corpusId": 1713408,
            "url": "https://www.semanticscholar.org/paper/40e4d0eaff2ce7538ecff773bda326388a87b515",
            "title": "An analytic solution to discrete Bayesian reinforcement learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2116459397",
                "DBLP": "conf/icml/PoupartVHR06",
                "DOI": "10.1145/1143844.1143932",
                "CorpusId": 1713408
            },
            "abstract": "Reinforcement learning (RL) was originally proposed as a framework to allow agents to learn in an online fashion as they interact with their environment. Existing RL algorithms come short of achieving this goal because the amount of exploration required is often too costly and/or too time consuming for online learning. As a result, RL is mostly used for offline learning in simulated environments. We propose a new algorithm, called BEETLE, for effective online learning that is computationally efficient while minimizing the amount of exploration. We take a Bayesian model-based approach, framing RL as a partially observable Markov decision process. Our two main contributions are the analytical derivation that the optimal value function is the upper envelope of a set of multivariate polynomials, and an efficient point-based value iteration algorithm that exploits this simple parameterization.",
            "referenceCount": 16,
            "citationCount": 318,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2006-06-25",
            "journal": {
                "name": "Proceedings of the 23rd international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Poupart2006AnAS,\n author = {P. Poupart and N. Vlassis and J. Hoey and K. Regan},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the 23rd international conference on Machine learning},\n title = {An analytic solution to discrete Bayesian reinforcement learning},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9a0cb6ea07d772f26bdfea3515f1f1e38f32ee5f",
            "@type": "ScholarlyArticle",
            "paperId": "9a0cb6ea07d772f26bdfea3515f1f1e38f32ee5f",
            "corpusId": 632032,
            "url": "https://www.semanticscholar.org/paper/9a0cb6ea07d772f26bdfea3515f1f1e38f32ee5f",
            "title": "Probabilistic policy reuse in a reinforcement learning agent",
            "venue": "Adaptive Agents and Multi-Agent Systems",
            "publicationVenue": {
                "id": "urn:research:6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                "name": "Adaptive Agents and Multi-Agent Systems",
                "alternate_names": [
                    "Adapt Agent Multi-agent Syst",
                    "International Joint Conference on Autonomous Agents & Multiagent Systems",
                    "Adapt Agent Multi-agents Syst",
                    "AAMAS",
                    "Adaptive Agents and Multi-Agents Systems",
                    "Int Jt Conf Auton Agent  Multiagent Syst"
                ],
                "issn": null,
                "url": "http://www.ifaamas.org/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2031727428",
                "DBLP": "conf/atal/FernandezV06",
                "DOI": "10.1145/1160633.1160762",
                "CorpusId": 632032
            },
            "abstract": "We contribute Policy Reuse as a technique to improve a reinforcement learning agent with guidance from past learned similar policies. Our method relies on using the past policies as a probabilistic bias where the learning agent faces three choices: the exploitation of the ongoing learned policy, the exploration of random unexplored actions, and the exploitation of past policies. We introduce the algorithm and its major components: an exploration strategy to include the new reuse bias, and a similarity function to estimate the similarity of past policies with respect to a new one. We provide empirical results demonstrating that Policy Reuse improves the learning performance over different strategies that learn without reuse. Interestingly and almost as a side effect, Policy Reuse also identifies classes of similar policies revealing a basis of core policies of the domain. We demonstrate that such a basis can be built incrementally, contributing the learning of the structure of a domain.",
            "referenceCount": 18,
            "citationCount": 310,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.cmu.edu/afs/cs/user/mmv/www/papers/06aamas-policy-reuse.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-05-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Fern\u00e1ndez2006ProbabilisticPR,\n author = {F. Fern\u00e1ndez and M. Veloso},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {720-727},\n title = {Probabilistic policy reuse in a reinforcement learning agent},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:712ec1bd9287ac210a7630ce03ca2b0930ebd351",
            "@type": "ScholarlyArticle",
            "paperId": "712ec1bd9287ac210a7630ce03ca2b0930ebd351",
            "corpusId": 1212267,
            "url": "https://www.semanticscholar.org/paper/712ec1bd9287ac210a7630ce03ca2b0930ebd351",
            "title": "Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2150339816",
                "DBLP": "journals/ml/SinghJLS00",
                "DOI": "10.1023/A:1007678930559",
                "CorpusId": 1212267
            },
            "abstract": null,
            "referenceCount": 47,
            "citationCount": 660,
            "influentialCitationCount": 41,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1007678930559.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2000-03-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "38"
            },
            "citationStyles": {
                "bibtex": "@Article{Singh2000ConvergenceRF,\n author = {Satinder Singh and T. Jaakkola and M. Littman and Csaba Szepesvari},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {287-308},\n title = {Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms},\n volume = {38},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c2e54466d3f680b47524d2ed5430fbc18626da3d",
            "@type": "ScholarlyArticle",
            "paperId": "c2e54466d3f680b47524d2ed5430fbc18626da3d",
            "corpusId": 15093602,
            "url": "https://www.semanticscholar.org/paper/c2e54466d3f680b47524d2ed5430fbc18626da3d",
            "title": "Reinforcement Learning and Savings Behavior",
            "venue": "Journal of Finance",
            "publicationVenue": {
                "id": "urn:research:430be0b0-fbe2-4c0d-98a5-137e3e454c79",
                "name": "Journal of Finance",
                "alternate_names": [
                    "J Finance"
                ],
                "issn": "0022-1082",
                "url": "http://www.blackwell-synergy.com/rd.asp?code=jofi&goto=journal"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2163673686",
                "DOI": "10.1111/J.1540-6261.2009.01509.X",
                "CorpusId": 15093602,
                "PubMed": "20352013"
            },
            "abstract": "We show that individual investors over-extrapolate from their personal experience when making savings decisions. Investors who experience particularly rewarding outcomes from saving in their 401(k)-a high average and/or low variance return-increase their 401(k) savings rate more than investors who have less rewarding experiences with saving. This finding is not driven by aggregate time-series shocks, income effects, rational learning about investing skill, investor fixed effects, or time-varying investor-level heterogeneity that is correlated with portfolio allocations to stock, bond, and cash asset classes. We discuss implications for the equity premium puzzle and interventions aimed at improving household financial outcomes.",
            "referenceCount": 38,
            "citationCount": 313,
            "influentialCitationCount": 22,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc2845178?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Economics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Economics",
                    "source": "external"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-09-14",
            "journal": {
                "name": "Microeconomic Theory eJournal",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Choi2007ReinforcementLA,\n author = {James J. Choi and David Laibson and B. Madrian and Andrew Metrick},\n booktitle = {Journal of Finance},\n journal = {Microeconomic Theory eJournal},\n title = {Reinforcement Learning and Savings Behavior},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:235a9b0733accac54785ec272fcba0c07f4e8b0f",
            "@type": "ScholarlyArticle",
            "paperId": "235a9b0733accac54785ec272fcba0c07f4e8b0f",
            "corpusId": 566650,
            "url": "https://www.semanticscholar.org/paper/235a9b0733accac54785ec272fcba0c07f4e8b0f",
            "title": "Reinforcement learning for optimized trade execution",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2160519132",
                "DBLP": "conf/icml/NevmyvakaFK06",
                "DOI": "10.1145/1143844.1143929",
                "CorpusId": 566650
            },
            "abstract": "We present the first large-scale empirical application of reinforcement learning to the important problem of optimized trade execution in modern financial markets. Our experiments are based on 1.5 years of millisecond time-scale limit order data from NASDAQ, and demonstrate the promise of reinforcement learning methods to market microstructure problems. Our learning algorithm introduces and exploits a natural \"low-impact\" factorization of the state space.",
            "referenceCount": 11,
            "citationCount": 238,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Business",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2006-06-25",
            "journal": {
                "name": "Proceedings of the 23rd international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Nevmyvaka2006ReinforcementLF,\n author = {Yuriy Nevmyvaka and Yi Feng and Michael Kearns},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the 23rd international conference on Machine learning},\n title = {Reinforcement learning for optimized trade execution},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3cc71552a5b942b099786d683577a3b09a6e735a",
            "@type": "ScholarlyArticle",
            "paperId": "3cc71552a5b942b099786d683577a3b09a6e735a",
            "corpusId": 7483505,
            "url": "https://www.semanticscholar.org/paper/3cc71552a5b942b099786d683577a3b09a6e735a",
            "title": "Autonomous shaping: knowledge transfer in reinforcement learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2079247031",
                "DBLP": "conf/icml/KonidarisB06",
                "DOI": "10.1145/1143844.1143906",
                "CorpusId": 7483505
            },
            "abstract": "We introduce the use of learned shaping rewards in reinforcement learning tasks, where an agent uses prior experience on a sequence of tasks to learn a portable predictor that estimates intermediate rewards, resulting in accelerated learning in later tasks that are related but distinct. Such agents can be trained on a sequence of relatively easy tasks in order to develop a more informative measure of reward that can be transferred to improve performance on more difficult tasks without requiring a hand coded shaping function. We use a rod positioning task to show that this significantly improves performance even after a very brief training period.",
            "referenceCount": 40,
            "citationCount": 223,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2006-06-25",
            "journal": {
                "name": "Proceedings of the 23rd international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Konidaris2006AutonomousSK,\n author = {G. Konidaris and A. Barto},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the 23rd international conference on Machine learning},\n title = {Autonomous shaping: knowledge transfer in reinforcement learning},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0fcd047a1a6e391c8a6c140967355ec2a0ad8c9e",
            "@type": "ScholarlyArticle",
            "paperId": "0fcd047a1a6e391c8a6c140967355ec2a0ad8c9e",
            "corpusId": 12192538,
            "url": "https://www.semanticscholar.org/paper/0fcd047a1a6e391c8a6c140967355ec2a0ad8c9e",
            "title": "Reinforcement Learning with Human Teachers: Evidence of Feedback and Guidance with Implications for Learning Performance",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "conf/aaai/ThomazB06",
                "MAG": "121023703",
                "CorpusId": 12192538
            },
            "abstract": "As robots become a mass consumer product, they will need to learn new skills by interacting with typical human users. Past approaches have adapted reinforcement learning (RL) to accept a human reward signal; however, we question the implicit assumption that people shall only want to give the learner feedback on its past actions. We present findings from a human user study showing that people use the reward signal not only to provide feedback about past actions, but also to provide future directed rewards to guide subsequent actions. Given this, we made specific modifications to the simulated RL robot to incorporate guidance. We then analyze and evaluate its learning performance in a second user study, and we report significant improvements on several measures. This work demonstrates the importance of understanding the human-teacher/robot-learner system as a whole in order to design algorithms that support how people want to teach while simultaneously improving the robot's learning performance.",
            "referenceCount": 26,
            "citationCount": 281,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2006-07-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Thomaz2006ReinforcementLW,\n author = {A. Thomaz and C. Breazeal},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {1000-1006},\n title = {Reinforcement Learning with Human Teachers: Evidence of Feedback and Guidance with Implications for Learning Performance},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:30d8e493ae35a64b2bebbe6ec90dc190488f82fa",
            "@type": "ScholarlyArticle",
            "paperId": "30d8e493ae35a64b2bebbe6ec90dc190488f82fa",
            "corpusId": 13904296,
            "url": "https://www.semanticscholar.org/paper/30d8e493ae35a64b2bebbe6ec90dc190488f82fa",
            "title": "Using inaccurate models in reinforcement learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "conf/icml/AbbeelQN06",
                "MAG": "2132602063",
                "DOI": "10.1145/1143844.1143845",
                "CorpusId": 13904296
            },
            "abstract": "In the model-based policy search approach to reinforcement learning (RL), policies are found using a model (or \"simulator\") of the Markov decision process. However, for high-dimensional continuous-state tasks, it can be extremely difficult to build an accurate model, and thus often the algorithm returns a policy that works in simulation but not in real-life. The other extreme, model-free RL, tends to require infeasibly large numbers of real-life trials. In this paper, we present a hybrid algorithm that requires only an approximate model, and only a small number of real-life trials. The key idea is to successively \"ground\" the policy evaluations using real-life trials, but to rely on the approximate model to suggest local changes. Our theoretical results show that this algorithm achieves near-optimal performance in the real system, even when the model is only approximate. Empirical results also demonstrate that---when given only a crude model and a small number of real-life trials---our algorithm can obtain near-optimal performance in the real system.",
            "referenceCount": 23,
            "citationCount": 250,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.robotics.stanford.edu/~ang/papers/icml06-usinginaccuratemodelsinrl.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2006-06-25",
            "journal": {
                "name": "Proceedings of the 23rd international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Abbeel2006UsingIM,\n author = {P. Abbeel and M. Quigley and A. Ng},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the 23rd international conference on Machine learning},\n title = {Using inaccurate models in reinforcement learning},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1927133a377d011fbc06c5b07399fc6d23a5b2f2",
            "@type": "ScholarlyArticle",
            "paperId": "1927133a377d011fbc06c5b07399fc6d23a5b2f2",
            "corpusId": 564520,
            "url": "https://www.semanticscholar.org/paper/1927133a377d011fbc06c5b07399fc6d23a5b2f2",
            "title": "Evolutionary Function Approximation for Reinforcement Learning",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "journals/jmlr/WhitesonS06",
                "MAG": "2116339921",
                "CorpusId": 564520
            },
            "abstract": "Temporal difference methods are theoretically grounded and empirically effective methods for addressing reinforcement learning problems. In most real-world reinforcement learning tasks, TD methods require a function approximator to represent the value function. However, using function approximators requires manually making crucial representational decisions. This paper investigates evolutionary function approximation, a novel approach to automatically selecting function approximator representations that enable efficient individual learning. This method evolves individuals that are better able to learn. We present a fully implemented instantiation of evolutionary function approximation which combines NEAT, a neuroevolutionary optimization technique, with Q-learning, a popular TD method. The resulting NEAT+Q algorithm automatically discovers effective representations for neural network function approximators. This paper also presents on-line evolutionary computation, which improves the on-line performance of evolutionary computation by borrowing selection mechanisms used in TD methods to choose individual actions and using them in evolutionary computation to select policies for evaluation. We evaluate these contributions with extended empirical studies in two domains: 1) the mountain car task, a standard reinforcement learning benchmark on which neural network function approximators have previously performed poorly and 2) server job scheduling, a large probabilistic domain drawn from the field of autonomic computing. The results demonstrate that evolutionary function approximation can significantly improve the performance of TD methods and on-line evolutionary computation can significantly improve evolutionary methods. This paper also presents additional tests that offer insight into what factors can make neural network function approximation difficult in practice.",
            "referenceCount": 81,
            "citationCount": 280,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Whiteson2006EvolutionaryFA,\n author = {Shimon Whiteson and P. Stone},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {877-917},\n title = {Evolutionary Function Approximation for Reinforcement Learning},\n volume = {7},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:421e6c7247f41c419a46212477d7b29540cbf7b1",
            "@type": "ScholarlyArticle",
            "paperId": "421e6c7247f41c419a46212477d7b29540cbf7b1",
            "corpusId": 2035627,
            "url": "https://www.semanticscholar.org/paper/421e6c7247f41c419a46212477d7b29540cbf7b1",
            "title": "High speed obstacle avoidance using monocular vision and reinforcement learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "conf/icml/MichelsSN05",
                "MAG": "2132400125",
                "DOI": "10.1145/1102351.1102426",
                "CorpusId": 2035627
            },
            "abstract": "We consider the task of driving a remote control car at high speeds through unstructured outdoor environments. We present an approach in which supervised learning is first used to estimate depths from single monocular images. The learning algorithm can be trained either on real camera images labeled with ground-truth distances to the closest obstacles, or on a training set consisting of synthetic graphics images. The resulting algorithm is able to learn monocular vision cues that accurately estimate the relative depths of obstacles in a scene. Reinforcement learning/policy search is then applied within a simulator that renders synthetic scenes. This learns a control policy that selects a steering direction as a function of the vision system's output. We present results evaluating the predictive ability of the algorithm both on held out test data, and in actual autonomous driving experiments.",
            "referenceCount": 25,
            "citationCount": 448,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2005-08-07",
            "journal": {
                "name": "Proceedings of the 22nd international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Michels2005HighSO,\n author = {J. Michels and Ashutosh Saxena and A. Ng},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the 22nd international conference on Machine learning},\n title = {High speed obstacle avoidance using monocular vision and reinforcement learning},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:03dbb37d2373500115735ed69871e94c7f6b2c5e",
            "@type": "ScholarlyArticle",
            "paperId": "03dbb37d2373500115735ed69871e94c7f6b2c5e",
            "corpusId": 1223826,
            "url": "https://www.semanticscholar.org/paper/03dbb37d2373500115735ed69871e94c7f6b2c5e",
            "title": "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2143435603",
                "DBLP": "conf/icml/McGovernB01",
                "CorpusId": 1223826
            },
            "abstract": "This paper presents a method by which a reinforcement learning agent can automatically discover certain types of subgoals online. By creating useful new subgoals while learning, the agent is able to accelerate learning on the current task and to transfer its expertise to other, related tasks through the reuse of its ability to attain subgoals. The agent discovers subgoals based on commonalities across multiple paths to a solution. We cast the task of finding these commonalities as a multiple-instance learning problem and use the concept of diverse density to find solutions. We illustrate this approach using several gridworld tasks.",
            "referenceCount": 19,
            "citationCount": 539,
            "influentialCitationCount": 36,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2001-06-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{McGovern2001AutomaticDO,\n author = {A. McGovern and A. Barto},\n booktitle = {International Conference on Machine Learning},\n pages = {361-368},\n title = {Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:03f6ee322583d360acabf33b43d3a7db1d5d8d56",
            "@type": "ScholarlyArticle",
            "paperId": "03f6ee322583d360acabf33b43d3a7db1d5d8d56",
            "corpusId": 31310192,
            "url": "https://www.semanticscholar.org/paper/03f6ee322583d360acabf33b43d3a7db1d5d8d56",
            "title": "Opposition-Based Reinforcement Learning",
            "venue": "Journal of Advanced Computational Intelligence and Intelligent Informatics",
            "publicationVenue": {
                "id": "urn:research:bf876fe0-7ad0-4242-9322-8b48c0d40b64",
                "name": "Journal of Advanced Computational Intelligence and Intelligent Informatics",
                "alternate_names": [
                    "J Adv Comput Intell Intell Informatics"
                ],
                "issn": "1883-8014",
                "url": "http://www.fujipress.jp/JACIII/index.html"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "journals/jaciii/Tizhoosh06",
                "MAG": "190427941",
                "DOI": "10.20965/jaciii.2006.p0578",
                "CorpusId": 31310192
            },
            "abstract": "Reinforcement learning is a machine intelligence scheme for learning in highly dynamic, probabilistic environments. By interaction with the environment, reinforcement agents learn optimal control policies, especially in the absence of a priori knowledge and/or a sufficiently large amount of training data. Despite its advantages, however, reinforcement learning suffers from a major drawback \u2013 high calculation cost because convergence to an optimal solution usually requires that all states be visited frequently to ensure that policy is reliable. This is not always possible, however, due to the complex, high-dimensional state space in many applications. This paper introduces opposition-based reinforcement learning, inspired by opposition-based learning, to speed up convergence. Considering opposite actions simultaneously enables individual states to be updated more than once shortening exploration and expediting convergence. Three versions of Q-learning algorithm will be given as examples. Experimental results for the grid world problem of different sizes demonstrate the superior performance of the proposed approach.",
            "referenceCount": 16,
            "citationCount": 199,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-07-20",
            "journal": {
                "name": "J. Adv. Comput. Intell. Intell. Informatics",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Tizhoosh2006OppositionBasedRL,\n author = {H. Tizhoosh},\n booktitle = {Journal of Advanced Computational Intelligence and Intelligent Informatics},\n journal = {J. Adv. Comput. Intell. Intell. Informatics},\n pages = {578-585},\n title = {Opposition-Based Reinforcement Learning},\n volume = {10},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:644a079073969a92674f69483c4a85679d066545",
            "@type": "ScholarlyArticle",
            "paperId": "644a079073969a92674f69483c4a85679d066545",
            "corpusId": 5155799,
            "url": "https://www.semanticscholar.org/paper/644a079073969a92674f69483c4a85679d066545",
            "title": "Double Q-learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2115211925",
                "DBLP": "conf/nips/Hasselt10",
                "CorpusId": 5155799
            },
            "abstract": "In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation.",
            "referenceCount": 24,
            "citationCount": 1231,
            "influentialCitationCount": 192,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-12-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hasselt2010DoubleQ,\n author = {H. V. Hasselt},\n booktitle = {Neural Information Processing Systems},\n pages = {2613-2621},\n title = {Double Q-learning},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a28c4a55514bf6a8ddd536e2aeaa4fc6a31018c8",
            "@type": "ScholarlyArticle",
            "paperId": "a28c4a55514bf6a8ddd536e2aeaa4fc6a31018c8",
            "corpusId": 39988136,
            "url": "https://www.semanticscholar.org/paper/a28c4a55514bf6a8ddd536e2aeaa4fc6a31018c8",
            "title": "Recent Advances in Hierarchical Reinforcement Learning",
            "venue": "Discrete event dynamic systems",
            "publicationVenue": {
                "id": "urn:research:20d20002-fb29-4a0e-84bb-89f05649d0ca",
                "name": "Discrete event dynamic systems",
                "alternate_names": [
                    "Discret Event Dyn Syst",
                    "Discret event dyn syst",
                    "Discrete Event Dynamic Systems"
                ],
                "issn": "0924-6703",
                "url": "https://link.springer.com/journal/10626"
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "journals/deds/BartoM03a",
                "DOI": "10.1023/A:1025696116075",
                "CorpusId": 39988136
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 508,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Discrete Event Dynamic Systems",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Barto2003RecentAI,\n author = {A. Barto and S. Mahadevan},\n booktitle = {Discrete event dynamic systems},\n journal = {Discrete Event Dynamic Systems},\n pages = {341-379},\n title = {Recent Advances in Hierarchical Reinforcement Learning},\n volume = {13},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7ba47c292d108fc56024a1f82e45d97381342550",
            "@type": "ScholarlyArticle",
            "paperId": "7ba47c292d108fc56024a1f82e45d97381342550",
            "corpusId": 61056011,
            "url": "https://www.semanticscholar.org/paper/7ba47c292d108fc56024a1f82e45d97381342550",
            "title": "Simulation-Based Optimization: Parametric Optimization Techniques and Reinforcement Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2505159842",
                "DOI": "10.1007/978-1-4757-3766-0",
                "CorpusId": 61056011
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 522,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2003-06-30",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Gosavi2003SimulationBasedOP,\n author = {A. Gosavi},\n title = {Simulation-Based Optimization: Parametric Optimization Techniques and Reinforcement Learning},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3bbfbf8311a8cf127d0975d9f94654c4811958b8",
            "@type": "ScholarlyArticle",
            "paperId": "3bbfbf8311a8cf127d0975d9f94654c4811958b8",
            "corpusId": 875526,
            "url": "https://www.semanticscholar.org/paper/3bbfbf8311a8cf127d0975d9f94654c4811958b8",
            "title": "From recurrent choice to skill learning: a reinforcement-learning model.",
            "venue": "Journal of experimental psychology. General",
            "publicationVenue": {
                "id": "urn:research:ba388b36-981e-4c1b-8048-464cdaa9c9fc",
                "name": "Journal of experimental psychology. General",
                "alternate_names": [
                    "J Exp Psychol Gen",
                    "J exp psychol Gen",
                    "Journal of Experimental Psychology: General"
                ],
                "issn": "0096-3445",
                "url": "http://www.apa.org/journals/xge.html"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "1964571600",
                "DOI": "10.1037/0096-3445.135.2.184",
                "CorpusId": 875526,
                "PubMed": "16719650"
            },
            "abstract": "The authors propose a reinforcement-learning mechanism as a model for recurrent choice and extend it to account for skill learning. The model was inspired by recent research in neurophysiological studies of the basal ganglia and provides an integrated explanation of recurrent choice behavior and skill learning. The behavior includes effects of differential probabilities, magnitudes, variabilities, and delay of reinforcement. The model can also produce the violation of independence, preference reversals, and the goal gradient of reinforcement in maze learning. An experiment was conducted to study learning of action sequences in a multistep task. The fit of the model to the data demonstrated its ability to account for complex skill learning. The advantages of incorporating the mechanism into a larger cognitive architecture are discussed.",
            "referenceCount": 107,
            "citationCount": 165,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2006-05-01",
            "journal": {
                "name": "Journal of experimental psychology. General",
                "volume": "135 2"
            },
            "citationStyles": {
                "bibtex": "@Article{Fu2006FromRC,\n author = {W. Fu and John R. Anderson},\n booktitle = {Journal of experimental psychology. General},\n journal = {Journal of experimental psychology. General},\n pages = {\n          184-206\n        },\n title = {From recurrent choice to skill learning: a reinforcement-learning model.},\n volume = {135 2},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b6cc21b30912bdaecd9f178d700a4c545b1d0838",
            "@type": "ScholarlyArticle",
            "paperId": "b6cc21b30912bdaecd9f178d700a4c545b1d0838",
            "corpusId": 2187487,
            "url": "https://www.semanticscholar.org/paper/b6cc21b30912bdaecd9f178d700a4c545b1d0838",
            "title": "Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2151210636",
                "DBLP": "conf/nips/GuoSLLW14",
                "CorpusId": 2187487
            },
            "abstract": "The combination of modern Reinforcement Learning and Deep Learning approaches holds the promise of making significant progress on challenging applications requiring both rich perception and policy-selection. The Arcade Learning Environment (ALE) provides a set of Atari games that represent a useful benchmark set of such applications. A recent breakthrough in combining model-free reinforcement learning with deep learning, called DQN, achieves the best real-time agents thus far. Planning-based approaches achieve far higher scores than the best model-free approaches, but they exploit information that is not available to human players, and they are orders of magnitude slower than needed for real-time play. Our main goal in this work is to build a better real-time Atari game playing agent than DQN. The central idea is to use the slow planning-based agents to provide training data for a deep-learning architecture capable of real-time play. We proposed new agents based on this idea and show that they outperform DQN.",
            "referenceCount": 24,
            "citationCount": 343,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-12-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Guo2014DeepLF,\n author = {Xiaoxiao Guo and Satinder Singh and Honglak Lee and Richard L. Lewis and Xiaoshi Wang},\n booktitle = {Neural Information Processing Systems},\n pages = {3338-3346},\n title = {Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8f4ebd515d171d3ef9a65411818b639da931a4c1",
            "@type": "ScholarlyArticle",
            "paperId": "8f4ebd515d171d3ef9a65411818b639da931a4c1",
            "corpusId": 13160988,
            "url": "https://www.semanticscholar.org/paper/8f4ebd515d171d3ef9a65411818b639da931a4c1",
            "title": "Reinforcement Learning for MDPs with Constraints",
            "venue": "European Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:f9a81beb-9bcb-43bc-96a7-67cf23dba2d6",
                "name": "European Conference on Machine Learning",
                "alternate_names": [
                    "Eur Conf Mach Learn",
                    "European conference on Machine Learning",
                    "Eur conf Mach Learn",
                    "ECML"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/ecml"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "conf/ecml/Geibel06",
                "MAG": "2743472117",
                "DOI": "10.1007/11871842_63",
                "CorpusId": 13160988
            },
            "abstract": null,
            "referenceCount": 9,
            "citationCount": 98,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/11871842_63.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2006-09-18",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Geibel2006ReinforcementLF,\n author = {Peter Geibel},\n booktitle = {European Conference on Machine Learning},\n pages = {646-653},\n title = {Reinforcement Learning for MDPs with Constraints},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3380537060e84f804d467305d39c1b28daf01fca",
            "@type": "ScholarlyArticle",
            "paperId": "3380537060e84f804d467305d39c1b28daf01fca",
            "corpusId": 2304982,
            "url": "https://www.semanticscholar.org/paper/3380537060e84f804d467305d39c1b28daf01fca",
            "title": "Graph kernels and Gaussian processes for relational reinforcement learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2109722166",
                "DBLP": "journals/ml/DriessensRG06",
                "DOI": "10.1007/s10994-006-8258-y",
                "CorpusId": 2304982
            },
            "abstract": null,
            "referenceCount": 51,
            "citationCount": 110,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10994-006-8258-y.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-09-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "64"
            },
            "citationStyles": {
                "bibtex": "@Article{G\u00e4rtner2006GraphKA,\n author = {Thomas G\u00e4rtner and K. Driessens and J. Ramon},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {91-119},\n title = {Graph kernels and Gaussian processes for relational reinforcement learning},\n volume = {64},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4dfa8530ca5c8152af5de6cc091052acb24d8b28",
            "@type": "ScholarlyArticle",
            "paperId": "4dfa8530ca5c8152af5de6cc091052acb24d8b28",
            "corpusId": 9792692,
            "url": "https://www.semanticscholar.org/paper/4dfa8530ca5c8152af5de6cc091052acb24d8b28",
            "title": "Learning the structure of Factored Markov Decision Processes in reinforcement learning problems",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2131660762",
                "DBLP": "conf/icml/DegrisSW06",
                "DOI": "10.1145/1143844.1143877",
                "CorpusId": 9792692
            },
            "abstract": "Recent decision-theoric planning algorithms are able to find optimal solutions in large problems, using Factored Markov Decision Processes (FMDPs). However, these algorithms need a perfect knowledge of the structure of the problem. In this paper, we propose SDYNA, a general framework for addressing large reinforcement learning problems by trial-and-error and with no initial knowledge of their structure. SDYNA integrates incremental planning algorithms based on FMDPs with supervised learning techniques building structured representations of the problem. We describe SPITI, an instantiation of SDYNA, that uses incremental decision tree induction to learn the structure of a problem combined with an incremental version of the Structured Value Iteration algorithm. We show that SPITI can build a factored representation of a reinforcement learning problem and may improve the policy faster than tabular reinforcement learning algorithms by exploiting the generalization property of decision tree induction algorithms.",
            "referenceCount": 17,
            "citationCount": 115,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://people.bordeaux.inria.fr/degris/papers/DegrisICML06.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2006-06-25",
            "journal": {
                "name": "Proceedings of the 23rd international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Degris2006LearningTS,\n author = {T. Degris and Olivier Sigaud and Pierre-Henri Wuillemin},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the 23rd international conference on Machine learning},\n title = {Learning the structure of Factored Markov Decision Processes in reinforcement learning problems},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0a8f4c3f0d317fd1f0bfe0b2e3e51aac893b8144",
            "@type": "ScholarlyArticle",
            "paperId": "0a8f4c3f0d317fd1f0bfe0b2e3e51aac893b8144",
            "corpusId": 7799595,
            "url": "https://www.semanticscholar.org/paper/0a8f4c3f0d317fd1f0bfe0b2e3e51aac893b8144",
            "title": "Generalization in Reinforcement Learning: Safely Approximating the Value Function",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1994,
            "externalIds": {
                "DBLP": "conf/nips/BoyanM94",
                "MAG": "2125074935",
                "CorpusId": 7799595
            },
            "abstract": "A straightforward approach to the curse of dimensionality in reinforcement learning and dynamic programming is to replace the lookup table with a generalizing function approximator such as a neural net. Although this has been successful in the domain of backgammon, there is no guarantee of convergence. In this paper, we show that the combination of dynamic programming and function approximation is not robust, and in even very benign cases, may produce an entirely wrong policy. We then introduce Grow-Support, a new algorithm which is safe from divergence yet can still reap the benefits of successful generalization.",
            "referenceCount": 14,
            "citationCount": 682,
            "influentialCitationCount": 47,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Boyan1994GeneralizationIR,\n author = {J. Boyan and A. Moore},\n booktitle = {Neural Information Processing Systems},\n pages = {369-376},\n title = {Generalization in Reinforcement Learning: Safely Approximating the Value Function},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2c0a703f036ef2d0ea4d97a123ec010cd73646b0",
            "@type": "ScholarlyArticle",
            "paperId": "2c0a703f036ef2d0ea4d97a123ec010cd73646b0",
            "corpusId": 155100049,
            "url": "https://www.semanticscholar.org/paper/2c0a703f036ef2d0ea4d97a123ec010cd73646b0",
            "title": "Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2950102663",
                "DBLP": "journals/corr/abs-1905-06750",
                "ArXiv": "1905.06750",
                "CorpusId": 155100049
            },
            "abstract": "We consider the problem of imitation learning from a finite set of expert trajectories, without access to reinforcement signals. The classical approach of extracting the expert's reward function via inverse reinforcement learning, followed by reinforcement learning is indirect and may be computationally expensive. Recent generative adversarial methods based on matching the policy distribution between the expert and the agent could be unstable during training. We propose a new framework for imitation learning by estimating the support of the expert policy to compute a fixed reward function, which allows us to re-frame imitation learning within the standard reinforcement learning setting. We demonstrate the efficacy of our reward function on both discrete and continuous domains, achieving comparable or better performance than the state of the art under different reinforcement learning algorithms.",
            "referenceCount": 30,
            "citationCount": 59,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-05-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2019RandomED,\n author = {Ruohan Wang and C. Ciliberto and P. Amadori and Y. Demiris},\n booktitle = {International Conference on Machine Learning},\n pages = {6536-6544},\n title = {Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:48cce5ee49facf75eeb12832c387452424b645dd",
            "@type": "ScholarlyArticle",
            "paperId": "48cce5ee49facf75eeb12832c387452424b645dd",
            "corpusId": 983224,
            "url": "https://www.semanticscholar.org/paper/48cce5ee49facf75eeb12832c387452424b645dd",
            "title": "A Bayesian Framework for Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "1582436621",
                "DBLP": "conf/icml/Strens00",
                "CorpusId": 983224
            },
            "abstract": "The reinforcement learning problem can be decomposed into two parallel types of inference: (i) estimating the parameters of a model for the underlying process; (ii) determining behavior which maximizes return under the estimated model. Following Dearden, Friedman and Andre (1999), it is proposed that the learning process estimates online the full posterior distribution over models. To determine behavior, a hypothesis is sampled from this distribution and the greedy policy with respect to the hypothesis is obtained by dynamic programming. By using a different hypothesis for each trial appropriate exploratory and exploitative behavior is obtained. This Bayesian method always converges to the optimal policy for a stationary process with discrete states.",
            "referenceCount": 16,
            "citationCount": 478,
            "influentialCitationCount": 59,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2000-06-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Strens2000ABF,\n author = {M. Strens},\n booktitle = {International Conference on Machine Learning},\n pages = {943-950},\n title = {A Bayesian Framework for Reinforcement Learning},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a7e5a400e63e74f44d07be8b4742472c981ca5b8",
            "@type": "ScholarlyArticle",
            "paperId": "a7e5a400e63e74f44d07be8b4742472c981ca5b8",
            "corpusId": 14551518,
            "url": "https://www.semanticscholar.org/paper/a7e5a400e63e74f44d07be8b4742472c981ca5b8",
            "title": "Improving Elevator Performance Using Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1995,
            "externalIds": {
                "DBLP": "conf/nips/CritesB95",
                "MAG": "2117341272",
                "CorpusId": 14551518
            },
            "abstract": "This paper describes the application of reinforcement learning (RL) to the difficult real world problem of elevator dispatching. The elevator domain poses a combination of challenges not seen in most RL research to date. Elevator systems operate in continuous state spaces and in continuous time as discrete event dynamic systems. Their states are not fully observable and they are nonstationary due to changing passenger arrival rates. In addition, we use a team of RL agents, each of which is responsible for controlling one elevator car. The team receives a global reinforcement signal which appears noisy to each agent due to the effects of the actions of the other agents, the random nature of the arrivals and the incomplete observation of the state. In spite of these complications, we show results that in simulation surpass the best of the heuristic elevator control algorithms of which we are aware. These results demonstrate the power of RL on a very large scale stochastic dynamic optimization problem of practical utility.",
            "referenceCount": 8,
            "citationCount": 651,
            "influentialCitationCount": 28,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1995-11-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Crites1995ImprovingEP,\n author = {Robert H. Crites and A. Barto},\n booktitle = {Neural Information Processing Systems},\n pages = {1017-1023},\n title = {Improving Elevator Performance Using Reinforcement Learning},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b66d590193b5662b5c68400818ce7fa87fcf47fc",
            "@type": "ScholarlyArticle",
            "paperId": "b66d590193b5662b5c68400818ce7fa87fcf47fc",
            "corpusId": 6197270,
            "url": "https://www.semanticscholar.org/paper/b66d590193b5662b5c68400818ce7fa87fcf47fc",
            "title": "Ant-Q: A Reinforcement Learning Approach to the Traveling Salesman Problem",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1995,
            "externalIds": {
                "MAG": "1566652554",
                "DBLP": "conf/icml/GambardellaD95",
                "DOI": "10.1016/b978-1-55860-377-6.50039-6",
                "CorpusId": 6197270
            },
            "abstract": null,
            "referenceCount": 18,
            "citationCount": 710,
            "influentialCitationCount": 55,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1995-07-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Gambardella1995AntQAR,\n author = {L. Gambardella and M. Dorigo},\n booktitle = {International Conference on Machine Learning},\n pages = {252-260},\n title = {Ant-Q: A Reinforcement Learning Approach to the Traveling Salesman Problem},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ba4bcd8c4ebb1427d335c23d50249d370778ae49",
            "@type": "ScholarlyArticle",
            "paperId": "ba4bcd8c4ebb1427d335c23d50249d370778ae49",
            "corpusId": 363516,
            "url": "https://www.semanticscholar.org/paper/ba4bcd8c4ebb1427d335c23d50249d370778ae49",
            "title": "Reinforcement learning with Gaussian processes",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2156974606",
                "DBLP": "conf/icml/EngelMM05",
                "DOI": "10.1145/1102351.1102377",
                "CorpusId": 363516
            },
            "abstract": "Gaussian Process Temporal Difference (GPTD) learning offers a Bayesian solution to the policy evaluation problem of reinforcement learning. In this paper we extend the GPTD framework by addressing two pressing issues, which were not adequately treated in the original GPTD paper (Engel et al., 2003). The first is the issue of stochasticity in the state transitions, and the second is concerned with action selection and policy improvement. We present a new generative model for the value function, deduced from its relation with the discounted return. We derive a corresponding on-line algorithm for learning the posterior moments of the value Gaussian process. We also present a SARSA based extension of GPTD, termed GPSARSA, that allows the selection of actions and the gradual improvement of policies without requiring a world-model.",
            "referenceCount": 11,
            "citationCount": 400,
            "influentialCitationCount": 39,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2005-08-07",
            "journal": {
                "name": "Proceedings of the 22nd international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Engel2005ReinforcementLW,\n author = {Y. Engel and Shie Mannor and R. Meir},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the 22nd international conference on Machine learning},\n title = {Reinforcement learning with Gaussian processes},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4c21accc28f1ab02404948ca4c315ef4a8596ed1",
            "@type": "ScholarlyArticle",
            "paperId": "4c21accc28f1ab02404948ca4c315ef4a8596ed1",
            "corpusId": 5860317,
            "url": "https://www.semanticscholar.org/paper/4c21accc28f1ab02404948ca4c315ef4a8596ed1",
            "title": "Reinforcement Learning with Replacing Eligibility Traces",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "journals/ml/SinghS96",
                "DOI": "10.1023/A:1018012322525",
                "CorpusId": 5860317
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 393,
            "influentialCitationCount": 32,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023%2FA%3A1018012322525.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Singh2005ReinforcementLW,\n author = {Satinder Singh and R. Sutton},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {123-158},\n title = {Reinforcement Learning with Replacing Eligibility Traces},\n volume = {22},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6c1309a5cb147856f785253d99424685cf5c12fc",
            "@type": "ScholarlyArticle",
            "paperId": "6c1309a5cb147856f785253d99424685cf5c12fc",
            "corpusId": 8543294,
            "url": "https://www.semanticscholar.org/paper/6c1309a5cb147856f785253d99424685cf5c12fc",
            "title": "Reinforcement learning for true adaptive traffic signal control",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2124657875",
                "DOI": "10.1061/(ASCE)0733-947X(2003)129:3(278)",
                "CorpusId": 8543294
            },
            "abstract": "The ability to exert real-time, adaptive control of transportation processes is the core of many intelligent transportation systems decision support tools. Reinforcement learning, an artificial intelligence approach undergoing development in the machine- learning community, offers key advantages in this regard. The ability of a control agent to learn relationships between control actions and their effect on the environment while pursuing a goal is a distinct improvement over prespecified models of the environment. Prespecified models are a prerequisite of conventional control methods and their accuracy limits the performance of control agents. This paper contains an introduction to Q-learning, a simple yet powerful reinforcement learning algorithm, and presents a case study involving application to traffic signal control. Encouraging results of the application to an isolated traffic signal, particularly under variable traffic conditions, are presented. A broader research effort is outlined, including extension to linear and networked signal systems and integration with dynamic route guidance. The research objective involves optimal control of heavily congested traffic across a two-dimensional road network\u2014a challenging task for conventional traffic signal control methodologies.",
            "referenceCount": 23,
            "citationCount": 449,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2003-05-01",
            "journal": {
                "name": "Journal of Transportation Engineering-asce",
                "volume": "129"
            },
            "citationStyles": {
                "bibtex": "@Article{Abdulhai2003ReinforcementLF,\n author = {B. Abdulhai and R. Pringle and G. J. Karakoulas},\n journal = {Journal of Transportation Engineering-asce},\n pages = {278-285},\n title = {Reinforcement learning for true adaptive traffic signal control},\n volume = {129},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2dfc4e6847aef5e87a2c78eee4c6a58694c03b24",
            "@type": "ScholarlyArticle",
            "paperId": "2dfc4e6847aef5e87a2c78eee4c6a58694c03b24",
            "corpusId": 14458943,
            "url": "https://www.semanticscholar.org/paper/2dfc4e6847aef5e87a2c78eee4c6a58694c03b24",
            "title": "Multiple Model-Based Reinforcement Learning",
            "venue": "Neural Computation",
            "publicationVenue": {
                "id": "urn:research:69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                "name": "Neural Computation",
                "alternate_names": [
                    "Neural Comput"
                ],
                "issn": "0899-7667",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667"
            },
            "year": 2002,
            "externalIds": {
                "DBLP": "journals/neco/DoyaSKK02",
                "MAG": "2168342951",
                "DOI": "10.1162/089976602753712972",
                "CorpusId": 14458943,
                "PubMed": "12020450"
            },
            "abstract": "We propose a modular reinforcement learning architecture for nonlinear, nonstationary control tasks, which we call multiple model-based reinforcement learning (MMRL). The basic idea is to decompose a complex task into multiple domains in space and time based on the predictability of the environmental dynamics. The system is composed of multiple modules, each of which consists of a state prediction model and a reinforcement learning controller. The responsibility signal, which is given by the softmax function of the prediction errors, is used to weight the outputs of multiple modules, as well as to gate the learning of the prediction models and the reinforcement learning controllers. We formulate MMRL for both discrete-time, finite-state case and continuous-time, continuous-state case. The performance of MMRL was demonstrated for discrete case in a nonstationary hunting task in a grid world and for continuous case in a nonlinear, nonstationary control task of swinging up a pendulum with variable physical parameters.",
            "referenceCount": 34,
            "citationCount": 480,
            "influentialCitationCount": 30,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2002-06-01",
            "journal": {
                "name": "Neural Computation",
                "volume": "14"
            },
            "citationStyles": {
                "bibtex": "@Article{Doya2002MultipleMR,\n author = {K. Doya and K. Samejima and Ken-ichi Katagiri and M. Kawato},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {1347-1369},\n title = {Multiple Model-Based Reinforcement Learning},\n volume = {14},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:eb6412a2fa74513609ab64c09970ae8fde747a1c",
            "@type": "ScholarlyArticle",
            "paperId": "eb6412a2fa74513609ab64c09970ae8fde747a1c",
            "corpusId": 14610547,
            "url": "https://www.semanticscholar.org/paper/eb6412a2fa74513609ab64c09970ae8fde747a1c",
            "title": "Reinforcement Learning in the Multi-Robot Domain",
            "venue": "Auton. Robots",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1997,
            "externalIds": {
                "DBLP": "journals/arobots/Matari97",
                "MAG": "1507591516",
                "DOI": "10.1023/A:1008819414322",
                "CorpusId": 14610547
            },
            "abstract": null,
            "referenceCount": 34,
            "citationCount": 533,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1997-03-27",
            "journal": {
                "name": "Autonomous Robots",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Matari\u01071997ReinforcementLI,\n author = {M. Matari\u0107},\n booktitle = {Auton. Robots},\n journal = {Autonomous Robots},\n pages = {73-83},\n title = {Reinforcement Learning in the Multi-Robot Domain},\n volume = {4},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d00595481313734057f8c5205cee3e79a11ebe10",
            "@type": "ScholarlyArticle",
            "paperId": "d00595481313734057f8c5205cee3e79a11ebe10",
            "corpusId": 9053126,
            "url": "https://www.semanticscholar.org/paper/d00595481313734057f8c5205cee3e79a11ebe10",
            "title": "Effective reinforcement learning for mobile robots",
            "venue": "Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2002,
            "externalIds": {
                "DBLP": "conf/icra/SmartK02",
                "MAG": "2103285838",
                "DOI": "10.1109/ROBOT.2002.1014237",
                "CorpusId": 9053126
            },
            "abstract": "Programming mobile robots can be a long, time-consuming process. Specifying the low-level mapping from sensors to actuators is prone to programmer misconceptions, and debugging such a mapping can be tedious. The idea of having a robot learn how to accomplish a task, rather than being told explicitly, is an appealing one. It seems easier and much more intuitive for the programmer to specify what the robot should be doing, and to let it learn the fine details of how to do it. In this paper, we introduce a framework for reinforcement learning on mobile robots and describe our experiments using it to learn simple tasks.",
            "referenceCount": 12,
            "citationCount": 427,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2002-08-07",
            "journal": {
                "name": "Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292)",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Smart2002EffectiveRL,\n author = {W. Smart and L. Kaelbling},\n booktitle = {Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292)},\n journal = {Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292)},\n pages = {3404-3410 vol.4},\n title = {Effective reinforcement learning for mobile robots},\n volume = {4},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1187a77f857ad029168863ba0005ddf6d2b957c8",
            "@type": "ScholarlyArticle",
            "paperId": "1187a77f857ad029168863ba0005ddf6d2b957c8",
            "corpusId": 5259564,
            "url": "https://www.semanticscholar.org/paper/1187a77f857ad029168863ba0005ddf6d2b957c8",
            "title": "Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2108682071",
                "DBLP": "journals/jmlr/GreensmithBB04",
                "CorpusId": 5259564
            },
            "abstract": "Policy gradient methods for reinforcement learning avoid some of the undesirable properties of the value function approaches, such as policy degradation (Baxter and Bartlett, 2001). However, the variance of the performance gradient estimates obtained from the simulation is sometimes excessive. In this paper, we consider variance reduction methods that were developed for Monte Carlo estimates of integrals. We study two commonly used policy gradient techniques, the baseline and actor-critic methods, from this perspective. Both can be interpreted as additive control variate variance reduction methods. We consider the expected average reward performance measure, and we focus on the GPOMDP algorithm for estimating performance gradients in partially observable Markov decision processes controlled by stochastic reactive policies. We give bounds for the estimation error of the gradient estimates for both baseline and actor-critic algorithms, in terms of the sample size and mixing properties of the controlled system. For the baseline technique, we compute the optimal baseline, and show that the popular approach of using the average reward to define the baseline can be suboptimal. For actor-critic algorithms, we show that using the true value function as the critic can be suboptimal. We also discuss algorithms for estimating the optimal baseline and approximate value function.",
            "referenceCount": 36,
            "citationCount": 436,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-01-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Greensmith2001VarianceRT,\n author = {Evan Greensmith and P. Bartlett and Jonathan Baxter},\n booktitle = {Journal of machine learning research},\n pages = {1507-1514},\n title = {Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5ee38bf9494a91ca8665f9fbe59830464c223b82",
            "@type": "ScholarlyArticle",
            "paperId": "5ee38bf9494a91ca8665f9fbe59830464c223b82",
            "corpusId": 60716402,
            "url": "https://www.semanticscholar.org/paper/5ee38bf9494a91ca8665f9fbe59830464c223b82",
            "title": "Reinforcement learning with selective perception and hidden state",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1996,
            "externalIds": {
                "MAG": "1583380718",
                "CorpusId": 60716402
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 574,
            "influentialCitationCount": 72,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{McCallum1996ReinforcementLW,\n author = {A. McCallum and D. Ballard},\n title = {Reinforcement learning with selective perception and hidden state},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9813c4933a6a3f5d0c9a95cdff43258522979fef",
            "@type": "ScholarlyArticle",
            "paperId": "9813c4933a6a3f5d0c9a95cdff43258522979fef",
            "corpusId": 16795817,
            "url": "https://www.semanticscholar.org/paper/9813c4933a6a3f5d0c9a95cdff43258522979fef",
            "title": "The O.D.E. Method for Convergence of Stochastic Approximation and Reinforcement Learning",
            "venue": "SIAM Journal of Control and Optimization",
            "publicationVenue": {
                "id": "urn:research:d64970b5-c111-42a8-aef4-66705a3b73dd",
                "name": "SIAM Journal of Control and Optimization",
                "alternate_names": [
                    "SIAM J Control Optim",
                    "Siam Journal on Control and Optimization",
                    "Siam J Control Optim"
                ],
                "issn": "0363-0129",
                "url": "https://epubs.siam.org/journal/sjcodc"
            },
            "year": 2000,
            "externalIds": {
                "DBLP": "journals/siamco/BorkarM00",
                "MAG": "2071983464",
                "DOI": "10.1137/S0363012997331639",
                "CorpusId": 16795817
            },
            "abstract": "It is shown here that stability of the stochastic approximation algorithm is implied by the asymptotic stability of the origin for an associated ODE. This in turn implies convergence of the algorithm. Several specific classes of algorithms are considered as applications. It is found that the results provide (i) a simpler derivation of known results for reinforcement learning algorithms; (ii) a proof for the first time that a class of asynchronous stochastic approximation algorithms are convergent without using any a priori assumption of stability; (iii) a proof for the first time that asynchronous adaptive critic and Q-learning algorithms are convergent for the average cost optimal control problem.",
            "referenceCount": 22,
            "citationCount": 400,
            "influentialCitationCount": 55,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://eprints.iisc.ac.in/1737/1/ODE.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "SIAM J. Control. Optim.",
                "volume": "38"
            },
            "citationStyles": {
                "bibtex": "@Article{Borkar2000TheOM,\n author = {V. Borkar and Sean P. Meyn},\n booktitle = {SIAM Journal of Control and Optimization},\n journal = {SIAM J. Control. Optim.},\n pages = {447-469},\n title = {The O.D.E. Method for Convergence of Stochastic Approximation and Reinforcement Learning},\n volume = {38},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e0da42f0d1e2fd6fb74fb3402cabf8ccfb8bd2f0",
            "@type": "ScholarlyArticle",
            "paperId": "e0da42f0d1e2fd6fb74fb3402cabf8ccfb8bd2f0",
            "corpusId": 1394727,
            "url": "https://www.semanticscholar.org/paper/e0da42f0d1e2fd6fb74fb3402cabf8ccfb8bd2f0",
            "title": "Coordinated Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2002,
            "externalIds": {
                "DBLP": "conf/icml/GuestrinLP02",
                "MAG": "2118318536",
                "CorpusId": 1394727
            },
            "abstract": "We present several new algorithms for multiagent reinforcement learning. A common feature of these algorithms is a parameterized, structured representation of a policy or value function. This structure is leveraged in an approach we call coordinated reinforcement learning, by which agents coordinate both their action selection activities and their parameter updates. Within the limits of our parametric representations, the agents will determine a jointly optimal action without explicitly considering every possible action in their exponentially large joint action space. Our methods differ from many previous reinforcement learning approaches to multiagent coordination in that structured communication and coordination between agents appears at the core of both the learning algorithm and the execution architecture.",
            "referenceCount": 16,
            "citationCount": 402,
            "influentialCitationCount": 36,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2002-07-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Guestrin2002CoordinatedRL,\n author = {Carlos Guestrin and M. Lagoudakis and Ronald E. Parr},\n booktitle = {International Conference on Machine Learning},\n pages = {227-234},\n title = {Coordinated Reinforcement Learning},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cb01aa49fdd9b59127b8651b36fa187095097799",
            "@type": "ScholarlyArticle",
            "paperId": "cb01aa49fdd9b59127b8651b36fa187095097799",
            "corpusId": 608769,
            "url": "https://www.semanticscholar.org/paper/cb01aa49fdd9b59127b8651b36fa187095097799",
            "title": "Efficient reinforcement learning through symbiotic evolution",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "journals/ml/MoriartyM96",
                "MAG": "2162813238",
                "DOI": "10.1007/BF00114722",
                "CorpusId": 608769
            },
            "abstract": null,
            "referenceCount": 47,
            "citationCount": 331,
            "influentialCitationCount": 43,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/BF00114722.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Moriarty2004EfficientRL,\n author = {David E. Moriarty and Risto Miikkulainen},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {11-32},\n title = {Efficient reinforcement learning through symbiotic evolution},\n volume = {22},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:96a25df486c7dfa475a93a0ca31d0418f79a8771",
            "@type": "ScholarlyArticle",
            "paperId": "96a25df486c7dfa475a93a0ca31d0418f79a8771",
            "corpusId": 17149277,
            "url": "https://www.semanticscholar.org/paper/96a25df486c7dfa475a93a0ca31d0418f79a8771",
            "title": "Reinforcement Learning Methods for Continuous-Time Markov Decision Problems",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1994,
            "externalIds": {
                "MAG": "2153947321",
                "DBLP": "conf/nips/BradtkeD94",
                "CorpusId": 17149277
            },
            "abstract": "Semi-Markov Decision Problems are continuous time generalizations of discrete time Markov Decision Problems. A number of reinforcement learning algorithms have been developed recently for the solution of Markov Decision Problems, based on the ideas of asynchronous dynamic programming and stochastic approximation. Among these are TD(\u03bb), Q-learning, and Real-time Dynamic Programming. After reviewing semi-Markov Decision Problems and Bellman's optimality equation in that context, we propose algorithms similar to those named above, adapted to the solution of semi-Markov Decision Problems. We demonstrate these algorithms by applying them to the problem of determining the optimal control for a simple queueing system. We conclude with a discussion of circumstances under which these algorithms may be usefully applied.",
            "referenceCount": 13,
            "citationCount": 361,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bradtke1994ReinforcementLM,\n author = {Steven J. Bradtke and M. Duff},\n booktitle = {Neural Information Processing Systems},\n pages = {393-400},\n title = {Reinforcement Learning Methods for Continuous-Time Markov Decision Problems},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9aa1d909544fd9ffe061b84a90eb344ac303e6d9",
            "@type": "ScholarlyArticle",
            "paperId": "9aa1d909544fd9ffe061b84a90eb344ac303e6d9",
            "corpusId": 10568560,
            "url": "https://www.semanticscholar.org/paper/9aa1d909544fd9ffe061b84a90eb344ac303e6d9",
            "title": "The MAXQ Method for Hierarchical Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1488730473",
                "DBLP": "conf/icml/Dietterich98",
                "CorpusId": 10568560
            },
            "abstract": "This paper presents a new approach to hierarchical reinforcement learning based on the MAXQ decomposition of the value function. The MAXQ decomposition has both a procedural semantics\u2014as a subroutine hierarchy\u2014and a declarative semantics\u2014as a representation of the value function of a hierarchical policy. MAXQ unifies and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. Conditions under which the MAXQ decomposition can represent the optimal value function are derived. The paper defines a hierarchical Q learning algorithm, proves its convergence, and shows experimentally that it can learn much faster than ordinary \u201cflat\u201d Q learning. Finally, the paper discusses some interesting issues that arise in hierarchical reinforcement learning including the hierarchical credit assignment problem and non-hierarchical execution of the MAXQ hierarchy.",
            "referenceCount": 8,
            "citationCount": 367,
            "influentialCitationCount": 43,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1998-07-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dietterich1998TheMM,\n author = {Thomas G. Dietterich},\n booktitle = {International Conference on Machine Learning},\n pages = {118-126},\n title = {The MAXQ Method for Hierarchical Reinforcement Learning},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:26b8747eb4d7fb4d4fc45707606d5e969b9afb0c",
            "@type": "ScholarlyArticle",
            "paperId": "26b8747eb4d7fb4d4fc45707606d5e969b9afb0c",
            "corpusId": 1115058,
            "url": "https://www.semanticscholar.org/paper/26b8747eb4d7fb4d4fc45707606d5e969b9afb0c",
            "title": "Issues in Using Function Approximation for Reinforcement Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "51508254",
                "CorpusId": 1115058
            },
            "abstract": "Reinforcement learning techniques address the problem of learning to select actions in unknown, dynamic environments. It is widely acknowledged that to be of use in complex domains, reinforcement learning techniques must be combined with generalizing function approximation methods such as artificial neural networks. Little, however, is understood about the theoretical properties of such combinations, and many researchers have encountered failures in practice. In this paper we identify a prime source of such failures\u2014namely, a systematic overestimation of utility values. Using Watkins\u2019 Q-Learning [18] as an example, we give a theoretical account of the phenomenon, deriving conditions under which one may expected it to cause learning to fail. Employing some of the most popular function approximators, we present experimental results which support the theoretical findings.",
            "referenceCount": 15,
            "citationCount": 406,
            "influentialCitationCount": 41,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Thrun1999IssuesIU,\n author = {S. Thrun and Anton Schwartz},\n title = {Issues in Using Function Approximation for Reinforcement Learning},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e21956fdbc06204db7984aacea09db7eda6355ad",
            "@type": "ScholarlyArticle",
            "paperId": "e21956fdbc06204db7984aacea09db7eda6355ad",
            "corpusId": 38901226,
            "url": "https://www.semanticscholar.org/paper/e21956fdbc06204db7984aacea09db7eda6355ad",
            "title": "Prioritized Sweeping: Reinforcement Learning with Less Data and Less Time",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1993,
            "externalIds": {
                "MAG": "2048226872",
                "DOI": "10.1023/A:1022635613229",
                "CorpusId": 38901226
            },
            "abstract": null,
            "referenceCount": 33,
            "citationCount": 379,
            "influentialCitationCount": 37,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1022635613229.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1993-10-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Moore1993PrioritizedSR,\n author = {A. Moore and C. Atkeson},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {103-130},\n title = {Prioritized Sweeping: Reinforcement Learning with Less Data and Less Time},\n volume = {13},\n year = {1993}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7069f79e14f79ff58c003c90ddb0b4366dab9b38",
            "@type": "ScholarlyArticle",
            "paperId": "7069f79e14f79ff58c003c90ddb0b4366dab9b38",
            "corpusId": 14463034,
            "url": "https://www.semanticscholar.org/paper/7069f79e14f79ff58c003c90ddb0b4366dab9b38",
            "title": "Attention-Gated Reinforcement Learning of Internal Representations for Classification",
            "venue": "Neural Computation",
            "publicationVenue": {
                "id": "urn:research:69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                "name": "Neural Computation",
                "alternate_names": [
                    "Neural Comput"
                ],
                "issn": "0899-7667",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2120632836",
                "DBLP": "journals/neco/RoelfsemaO05",
                "DOI": "10.1162/0899766054615699",
                "CorpusId": 14463034,
                "PubMed": "16105222"
            },
            "abstract": "Animal learning is associated with changes in the efficacy of connections between neurons. The rules that govern this plasticity can be tested in neural networks. Rules that train neural networks to map stimuli onto outputs are given by supervised learning and reinforcement learning theories. Supervised learning is efficient but biologically implausible. In contrast, reinforcement learning is biologically plausible but comparatively inefficient. It lacks a mechanism that can identify units at early processing levels that play a decisive role in the stimulus-response mapping. Here we show that this so-called credit assignment problem can be solved by a new role for attention in learning. There are two factors in our new learning scheme that determine synaptic plasticity: (1) a reinforcement signal that is homogeneous across the network and depends on the amount of reward obtained after a trial, and (2) an attentional feedback signal from the output layer that limits plasticity to those units at earlier processing levels that are crucial for the stimulus-response mapping. The new scheme is called attention-gated reinforcement learning (AGREL). We show that it is as efficient as supervised learning in classification tasks. AGREL is biologically realistic and integrates the role of feedback connections, attention effects, synaptic plasticity, and reinforcement learning signals into a coherent framework.",
            "referenceCount": 105,
            "citationCount": 267,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Study"
            ],
            "publicationDate": "2005-10-01",
            "journal": {
                "name": "Neural Computation",
                "volume": "17"
            },
            "citationStyles": {
                "bibtex": "@Article{Roelfsema2005AttentionGatedRL,\n author = {P. Roelfsema and A. V. Ooyen},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {2176-2214},\n title = {Attention-Gated Reinforcement Learning of Internal Representations for Classification},\n volume = {17},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3b76c51beceb5057b1285bd7d709817cda17adc0",
            "@type": "ScholarlyArticle",
            "paperId": "3b76c51beceb5057b1285bd7d709817cda17adc0",
            "corpusId": 7355869,
            "url": "https://www.semanticscholar.org/paper/3b76c51beceb5057b1285bd7d709817cda17adc0",
            "title": "Exploration and apprenticeship learning in reinforcement learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "1996625075",
                "DBLP": "conf/icml/AbbeelN05",
                "DOI": "10.1145/1102351.1102352",
                "CorpusId": 7355869
            },
            "abstract": "We consider reinforcement learning in systems with unknown dynamics. Algorithms such as E3 (Kearns and Singh, 2002) learn near-optimal policies by using \"exploration policies\" to drive the system towards poorly modeled states, so as to encourage exploration. But this makes these algorithms impractical for many systems; for example, on an autonomous helicopter, overly aggressive exploration may well result in a crash. In this paper, we consider the apprenticeship learning setting in which a teacher demonstration of the task is available. We show that, given the initial demonstration, no explicit exploration is necessary, and we can attain near-optimal performance (compared to the teacher) simply by repeatedly executing \"exploitation policies\" that try to maximize rewards. In finite-state MDPs, our algorithm scales polynomially in the number of states; in continuous-state linear dynamical systems, it scales polynomially in the dimension of the state. These results are proved using a martingale construction over relative losses.",
            "referenceCount": 21,
            "citationCount": 266,
            "influentialCitationCount": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://ai.stanford.edu/~pabbeel//pubs/AbbeelNg_eaalirl_ICML2005long.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2005-08-07",
            "journal": {
                "name": "Proceedings of the 22nd international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Abbeel2005ExplorationAA,\n author = {P. Abbeel and A. Ng},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the 22nd international conference on Machine learning},\n title = {Exploration and apprenticeship learning in reinforcement learning},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1c59bfa0e8654ebea94277064f82062875cae8b6",
            "@type": "ScholarlyArticle",
            "paperId": "1c59bfa0e8654ebea94277064f82062875cae8b6",
            "corpusId": 5857707,
            "url": "https://www.semanticscholar.org/paper/1c59bfa0e8654ebea94277064f82062875cae8b6",
            "title": "Identifying useful subgoals in reinforcement learning by local graph partitioning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2090170171",
                "DBLP": "conf/icml/SimsekWB05",
                "DOI": "10.1145/1102351.1102454",
                "CorpusId": 5857707
            },
            "abstract": "We present a new subgoal-based method for automatically creating useful skills in reinforcement learning. Our method identifies subgoals by partitioning local state transition graphs---those that are constructed using only the most recent experiences of the agent. The local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek---states that lie between two densely-connected regions of the state space while producing an algorithm with low computational cost.",
            "referenceCount": 22,
            "citationCount": 271,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1006&context=cs_faculty_pubs",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2005-08-07",
            "journal": {
                "name": "Proceedings of the 22nd international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Simsek2005IdentifyingUS,\n author = {\u00d6zg\u00fcr Simsek and Alicia P. Wolfe and A. Barto},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the 22nd international conference on Machine learning},\n title = {Identifying useful subgoals in reinforcement learning by local graph partitioning},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9d94165d22ae17b7d933dafffabb16ad2b8e147a",
            "@type": "ScholarlyArticle",
            "paperId": "9d94165d22ae17b7d933dafffabb16ad2b8e147a",
            "corpusId": 28535139,
            "url": "https://www.semanticscholar.org/paper/9d94165d22ae17b7d933dafffabb16ad2b8e147a",
            "title": "An Algorithm for Distributed Reinforcement Learning in Cooperative Multi-Agent Systems",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "1560074431",
                "DBLP": "conf/icml/LauerR00",
                "CorpusId": 28535139
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 484,
            "influentialCitationCount": 57,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2000-06-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lauer2000AnAF,\n author = {M. Lauer and Martin A. Riedmiller},\n booktitle = {International Conference on Machine Learning},\n pages = {535-542},\n title = {An Algorithm for Distributed Reinforcement Learning in Cooperative Multi-Agent Systems},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:45b2ff4c2dc8d944b05e9b55d55f2a9245bef767",
            "@type": "ScholarlyArticle",
            "paperId": "45b2ff4c2dc8d944b05e9b55d55f2a9245bef767",
            "corpusId": 6820915,
            "url": "https://www.semanticscholar.org/paper/45b2ff4c2dc8d944b05e9b55d55f2a9245bef767",
            "title": "Reinforcement Learning for Humanoid Robotics",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2003,
            "externalIds": {
                "MAG": "1594783240",
                "CorpusId": 6820915
            },
            "abstract": "Reinforcement learning offers one of the most general framework to take traditional robotics towards true autonomy and versatility. However, applying reinforcement learning to high dimensional movement systems like humanoid robots remains an unsolved problem. In this paper, we discuss different approaches of reinforcement learning in terms of their applicability in humanoid robotics. Methods can be coarsely classified into three different categories, i.e., greedy methods, \u2018vanilla\u2019 policy gradient methods, and natural gradient methods. We discuss that greedy methods are not likely to scale into the domain humanoid robotics as they are problematic when used with function approximation. \u2018Vanilla\u2019 policy gradient methods on the other hand have been successfully applied on real-world robots including at least one humanoid robot [3]. We demonstrate that these methods can be significantly improved using the natural policy gradient instead of the regular policy gradient. A derivation of the natural policy gradient is provided, proving that the average policy gradient of Kakade [10] is indeed the true natural gradient. A general algorithm for estimating the natural gradient, the Natural Actor-Critic algorithm, is introduced. This algorithm converges to the nearest local minimum of the cost function with respect to the Fisher information metric under suitable conditions. The algorithm outperforms non-natural policy gradients by far in a cart-pole balancing evaluation, and for learning nonlinear dynamic motor primitives for humanoid robot control. It offers a promising route for the development of reinforcement learning for truly high-dimensionally continuous state-action systems.",
            "referenceCount": 30,
            "citationCount": 382,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2003-09-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Peters2003ReinforcementLF,\n author = {Jan Peters and S. Vijayakumar and S. Schaal},\n pages = {1-20},\n title = {Reinforcement Learning for Humanoid Robotics},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a62d15d6b73c3323e69270ab995aafa6a692a59c",
            "@type": "ScholarlyArticle",
            "paperId": "a62d15d6b73c3323e69270ab995aafa6a692a59c",
            "corpusId": 3291174,
            "url": "https://www.semanticscholar.org/paper/a62d15d6b73c3323e69270ab995aafa6a692a59c",
            "title": "Average reward reinforcement learning: Foundations, algorithms, and empirical results",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2101915445",
                "DOI": "10.1007/BF00114727",
                "CorpusId": 3291174
            },
            "abstract": null,
            "referenceCount": 42,
            "citationCount": 357,
            "influentialCitationCount": 34,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/BF00114727.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Mahadevan2004AverageRR,\n author = {S. Mahadevan},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {159-195},\n title = {Average reward reinforcement learning: Foundations, algorithms, and empirical results},\n volume = {22},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:13e2d5c4d39bc58b42f9004e5b03905f847dfa0f",
            "@type": "ScholarlyArticle",
            "paperId": "13e2d5c4d39bc58b42f9004e5b03905f847dfa0f",
            "corpusId": 7142905,
            "url": "https://www.semanticscholar.org/paper/13e2d5c4d39bc58b42f9004e5b03905f847dfa0f",
            "title": "Autonomous Helicopter Flight via Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2108734173",
                "DBLP": "conf/nips/NgKJS03",
                "CorpusId": 7142905
            },
            "abstract": "Autonomous helicopter flight represents a challenging control problem, with complex, noisy, dynamics. In this paper, we describe a successful application of reinforcement learning to autonomous helicopter flight. We first fit a stochastic, nonlinear model of the helicopter dynamics. We then use the model to learn to hover in place, and to fly a number of maneuvers taken from an RC helicopter competition.",
            "referenceCount": 13,
            "citationCount": 357,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2003-12-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ng2003AutonomousHF,\n author = {A. Ng and H. Kim and Michael I. Jordan and S. Sastry},\n booktitle = {Neural Information Processing Systems},\n pages = {799-806},\n title = {Autonomous Helicopter Flight via Reinforcement Learning},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5d0a7ebd3bc2d25deee869e8ef3dd80f9278607d",
            "@type": "ScholarlyArticle",
            "paperId": "5d0a7ebd3bc2d25deee869e8ef3dd80f9278607d",
            "corpusId": 7123834,
            "url": "https://www.semanticscholar.org/paper/5d0a7ebd3bc2d25deee869e8ef3dd80f9278607d",
            "title": "Reinforcement learning with replacing eligibility traces",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2113913482",
                "DOI": "10.1007/BF00114726",
                "CorpusId": 7123834
            },
            "abstract": null,
            "referenceCount": 31,
            "citationCount": 271,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/BF00114726.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Singh2004ReinforcementLW,\n author = {Satinder Singh and R. Sutton},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {123-158},\n title = {Reinforcement learning with replacing eligibility traces},\n volume = {22},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9d612473d6bba6fb7b611b88d0b5f7fff6c17fdd",
            "@type": "ScholarlyArticle",
            "paperId": "9d612473d6bba6fb7b611b88d0b5f7fff6c17fdd",
            "corpusId": 114918,
            "url": "https://www.semanticscholar.org/paper/9d612473d6bba6fb7b611b88d0b5f7fff6c17fdd",
            "title": "Automatic Programming of Behavior-Based Robots Using Reinforcement Learning",
            "venue": "Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:96018464-22dc-4b5c-a172-c2f4a30ce131",
                "name": "Artificial Intelligence",
                "alternate_names": [
                    "Artif Intell"
                ],
                "issn": "0004-3702",
                "url": "http://www.elsevier.com/locate/artint"
            },
            "year": 1991,
            "externalIds": {
                "MAG": "1979071892",
                "DBLP": "journals/ai/MahadevanC92",
                "DOI": "10.1016/0004-3702(92)90058-6",
                "CorpusId": 114918
            },
            "abstract": null,
            "referenceCount": 25,
            "citationCount": 730,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1991-07-14",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Mahadevan1991AutomaticPO,\n author = {S. Mahadevan and J. Connell},\n booktitle = {Artificial Intelligence},\n pages = {768-773},\n title = {Automatic Programming of Behavior-Based Robots Using Reinforcement Learning},\n year = {1991}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:856695874f6c482f9609db902326f15d8f6d7c83",
            "@type": "ScholarlyArticle",
            "paperId": "856695874f6c482f9609db902326f15d8f6d7c83",
            "corpusId": 1403530,
            "url": "https://www.semanticscholar.org/paper/856695874f6c482f9609db902326f15d8f6d7c83",
            "title": "Evolutionary Algorithms for Reinforcement Learning",
            "venue": "Journal of Artificial Intelligence Research",
            "publicationVenue": {
                "id": "urn:research:aef12dca-60a0-4ca3-819b-cad26d309d4e",
                "name": "Journal of Artificial Intelligence Research",
                "alternate_names": [
                    "JAIR",
                    "J Artif Intell Res",
                    "The Journal of Artificial Intelligence Research"
                ],
                "issn": "1076-9757",
                "url": "http://www.jair.org/"
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1914583973",
                "DBLP": "journals/jair/MoriartySG99",
                "ArXiv": "1106.0221",
                "DOI": "10.1613/jair.613",
                "CorpusId": 1403530
            },
            "abstract": "There are two distinct approaches to solving reinforcement learning problems, namely, searching in value function space and searching in policy space. Temporal difference methods and evolutionary algorithms are well-known examples of these approaches. Kaelbling, Littman and Moore recently provided an informative survey of temporal difference methods. This article focuses on the application of evolutionary algorithms to the reinforcement learning problem, emphasizing alternative policy representations, credit assignment methods, and problem-specific genetic operators. Strengths and weaknesses of the evolutionary approach to reinforcement learning are presented, along with a survey of representative applications.",
            "referenceCount": 60,
            "citationCount": 395,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jair.org/index.php/jair/article/download/10240/24373",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1999-07-01",
            "journal": {
                "name": "J. Artif. Intell. Res.",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Moriarty1999EvolutionaryAF,\n author = {David E. Moriarty and A. Schultz and J. Grefenstette},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {J. Artif. Intell. Res.},\n pages = {241-276},\n title = {Evolutionary Algorithms for Reinforcement Learning},\n volume = {11},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a1b055577a86141df13f13a3203c76a32bffdc3a",
            "@type": "ScholarlyArticle",
            "paperId": "a1b055577a86141df13f13a3203c76a32bffdc3a",
            "corpusId": 1963904,
            "url": "https://www.semanticscholar.org/paper/a1b055577a86141df13f13a3203c76a32bffdc3a",
            "title": "Reinforcement Learning with Perceptual Aliasing: The Perceptual Distinctions Approach",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 1992,
            "externalIds": {
                "DBLP": "conf/aaai/Chrisman92",
                "MAG": "1657674574",
                "CorpusId": 1963904
            },
            "abstract": "It is known that Perceptual Aliasing may significantly diminish the effectiveness of reinforcement learning algorithms [Whitehead and Ballard, 1991]. Perceptual aliasing occurs when multiple situations that are indistinguishable from immediate perceptual input require different responses from the system. For example, if a robot can only see forward, yet the presence of a battery charger behind it determines whether or not it should backup, immediate perception alone is insufficient for determining the most appropriate action. It is problematic since reinforcement algorithms typically learn a control policy from immediate perceptual input to the optimal choice of action. \n \nThis paper introduces the predictive distinctions approach to compensate for perceptual aliasing caused from incomplete perception of the world. An additional component, a predictive model, is utilized to track aspects of the world that may not be visible at all times. In addition to the control policy, the model must also be learned, and to allow for stochastic actions and noisy perception, a probabilistic model is learned from experience. In the process, the system must discover, on its own, the important distinctions in the world. Experimental results are given for a simple simulated domain, and additional issues are discussed.",
            "referenceCount": 19,
            "citationCount": 406,
            "influentialCitationCount": 35,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1992-07-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chrisman1992ReinforcementLW,\n author = {L. Chrisman},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {183-188},\n title = {Reinforcement Learning with Perceptual Aliasing: The Perceptual Distinctions Approach},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9a208167b153e6dfc3327415068ae4a7a6dcd006",
            "@type": "ScholarlyArticle",
            "paperId": "9a208167b153e6dfc3327415068ae4a7a6dcd006",
            "corpusId": 6578281,
            "url": "https://www.semanticscholar.org/paper/9a208167b153e6dfc3327415068ae4a7a6dcd006",
            "title": "Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1994,
            "externalIds": {
                "DBLP": "conf/nips/JaakkolaSJ94",
                "MAG": "2160067530",
                "CorpusId": 6578281
            },
            "abstract": "Increasing attention has been paid to reinforcement learning algorithms in recent years, partly due to successes in the theoretical analysis of their behavior in Markov environments. If the Markov assumption is removed, however, neither generally the algorithms nor the analyses continue to be usable. We propose and analyze a new learning algorithm to solve a certain class of non-Markov decision problems. Our algorithm applies to problems in which the environment is Markov, but the learner has restricted access to state information. The algorithm involves a Monte-Carlo policy evaluation combined with a policy improvement method that is similar to that of Markov decision problems and is guaranteed to converge to a local maximum. The algorithm operates in the space of stochastic policies, a space which can yield a policy that performs considerably better than any deterministic policy. Although the space of stochastic policies is continuous--even for a discrete action space--our algorithm is computationally tractable.",
            "referenceCount": 9,
            "citationCount": 438,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Jaakkola1994ReinforcementLA,\n author = {T. Jaakkola and Satinder Singh and Michael I. Jordan},\n booktitle = {Neural Information Processing Systems},\n pages = {345-352},\n title = {Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9d8f6219fbd2da14d8d55562dcedf43fe671d0e3",
            "@type": "ScholarlyArticle",
            "paperId": "9d8f6219fbd2da14d8d55562dcedf43fe671d0e3",
            "corpusId": 28257125,
            "url": "https://www.semanticscholar.org/paper/9d8f6219fbd2da14d8d55562dcedf43fe671d0e3",
            "title": "Learning to Drive a Bicycle Using Reinforcement Learning and Shaping",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1998,
            "externalIds": {
                "DBLP": "conf/icml/RandlovA98",
                "MAG": "1499408472",
                "CorpusId": 28257125
            },
            "abstract": "We present and solve a real-world problem of learning to drive a bicycle. We solve the problem by online reinforcement learning using the Sarsa(A)-algorithm. Then we solve the composite problem of learning to balance a bicycle and then drive to 'It goal. In our approach the reinforcement function is independent of the task the agent tries to learn to solve.",
            "referenceCount": 21,
            "citationCount": 384,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1998-07-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Randl\u00f8v1998LearningTD,\n author = {J. Randl\u00f8v and P. Alstr\u00f8m},\n booktitle = {International Conference on Machine Learning},\n pages = {463-471},\n title = {Learning to Drive a Bicycle Using Reinforcement Learning and Shaping},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea899c8a3806a02a225061a35f802b00d90a0a20",
            "@type": "ScholarlyArticle",
            "paperId": "ea899c8a3806a02a225061a35f802b00d90a0a20",
            "corpusId": 10082747,
            "url": "https://www.semanticscholar.org/paper/ea899c8a3806a02a225061a35f802b00d90a0a20",
            "title": "Reinforcement Learning with Soft State Aggregation",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1994,
            "externalIds": {
                "MAG": "2125510930",
                "DBLP": "conf/nips/SinghJJ94",
                "CorpusId": 10082747
            },
            "abstract": "It is widely accepted that the use of more compact representations than lookup tables is crucial to scaling reinforcement learning (RL) algorithms to real-world problems. Unfortunately almost all of the theory of reinforcement learning assumes lookup table representations. In this paper we address the pressing issue of combining function approximation and RL, and present 1) a function approximator based on a simple extension to state aggregation (a commonly used form of compact representation), namely soft state aggregation, 2) a theory of convergence for RL with arbitrary, but fixed, soft state aggregation, 3) a novel intuitive understanding of the effect of state aggregation on online RL, and 4) a new heuristic adaptive state aggregation algorithm that finds improved compact representations by exploiting the non-discrete nature of soft state aggregation. Preliminary empirical results are also presented.",
            "referenceCount": 10,
            "citationCount": 374,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Singh1994ReinforcementLW,\n author = {Satinder Singh and T. Jaakkola and Michael I. Jordan},\n booktitle = {Neural Information Processing Systems},\n pages = {361-368},\n title = {Reinforcement Learning with Soft State Aggregation},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b550e3e05701cbf6c76a8c71e91beb95f950b080",
            "@type": "ScholarlyArticle",
            "paperId": "b550e3e05701cbf6c76a8c71e91beb95f950b080",
            "corpusId": 6822470,
            "url": "https://www.semanticscholar.org/paper/b550e3e05701cbf6c76a8c71e91beb95f950b080",
            "title": "A Reinforcement Learning Approach to job-shop Scheduling",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 1995,
            "externalIds": {
                "MAG": "1585546214",
                "DBLP": "conf/ijcai/ZhangD95",
                "CorpusId": 6822470
            },
            "abstract": "We apply reinforcement learning methods to learn domain-specific heuristics for job shop scheduling. A repair-based scheduler starts with a critical-path schedule and incrementally repairs constraint violations with the goal of finding a short conflict-free schedule. The temporal difference algorithm TD(\u03bb) is applied to tram a neural network to learn a heuristic evaluation function over states. This evaluation function is used by a one-step lookahead search procedure to find good solutions to new scheduling problems. We evaluate this approach on synthetic problems and on problems from a NASA space shuttle pay load processing task. The evaluation function is trained on problems involving a small number of jobs and then tested on larger problems. The TD scheduler performs better than the best known existing algorithm for this task--Zwehen's iterative repair method based on simulated annealing. The results suggest that reinforcement learning can provide a new method for constructing high-performance scheduling systems.",
            "referenceCount": 8,
            "citationCount": 453,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1995-08-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang1995ARL,\n author = {Wei Zhang and Thomas G. Dietterich},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {1114-1120},\n title = {A Reinforcement Learning Approach to job-shop Scheduling},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a69e33ce29c451655d917bf9387ed57e115fcfc7",
            "@type": "ScholarlyArticle",
            "paperId": "a69e33ce29c451655d917bf9387ed57e115fcfc7",
            "corpusId": 260427653,
            "url": "https://www.semanticscholar.org/paper/a69e33ce29c451655d917bf9387ed57e115fcfc7",
            "title": "Multi-Agent Reinforcement Learning:a critical survey",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2103561211",
                "CorpusId": 260427653
            },
            "abstract": "We survey the recent work in AI on multi-agent reinforcement learning (that is, learning in stochastic games). We then argue that, while exciting, this work is flawed. The fundamental flaw is unclarity about the problem or problems being addressed. After tracing a representative sample of the recent literature, we identify four well-defined problems in multi-agent reinforcement learning, single out the problem that in our view is most suitable for AI, and make some remarks about how we believe progress is to be made on this problem.",
            "referenceCount": 37,
            "citationCount": 311,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Shoham2003MultiAgentRL,\n author = {Y. Shoham and Rob Powers and Trond Grenager},\n title = {Multi-Agent Reinforcement Learning:a critical survey},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3cc0095b47615c2f47beaca9bd8f675811fb118a",
            "@type": "ScholarlyArticle",
            "paperId": "3cc0095b47615c2f47beaca9bd8f675811fb118a",
            "corpusId": 13417437,
            "url": "https://www.semanticscholar.org/paper/3cc0095b47615c2f47beaca9bd8f675811fb118a",
            "title": "Gaussian Processes in Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2151268438",
                "DBLP": "conf/nips/RasmussenK03",
                "CorpusId": 13417437
            },
            "abstract": "We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.",
            "referenceCount": 8,
            "citationCount": 276,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2003-12-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rasmussen2003GaussianPI,\n author = {C. Rasmussen and M. Kuss},\n booktitle = {Neural Information Processing Systems},\n pages = {751-758},\n title = {Gaussian Processes in Reinforcement Learning},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fd4de76fadddd1cc9a91cea954200c8d656e1dbb",
            "@type": "ScholarlyArticle",
            "paperId": "fd4de76fadddd1cc9a91cea954200c8d656e1dbb",
            "corpusId": 18328568,
            "url": "https://www.semanticscholar.org/paper/fd4de76fadddd1cc9a91cea954200c8d656e1dbb",
            "title": "Using relative novelty to identify useful temporal abstractions in reinforcement learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "1968768508",
                "DBLP": "conf/icml/SimsekB04",
                "DOI": "10.1145/1015330.1015353",
                "CorpusId": 18328568
            },
            "abstract": "We present a new method for automatically creating useful temporal abstractions in reinforcement learning. We argue that states that allow the agent to transition to a different region of the state space are useful subgoals, and propose a method for identifying them using the concept of relative novelty. When such a state is identified, a temporally-extended activity (e.g., an option) is generated that takes the agent efficiently to this state. We illustrate the utility of the method in a number of tasks.",
            "referenceCount": 20,
            "citationCount": 231,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1020&context=cs_faculty_pubs",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2004-07-04",
            "journal": {
                "name": "Proceedings of the twenty-first international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Simsek2004UsingRN,\n author = {\u00d6zg\u00fcr Simsek and A. Barto},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the twenty-first international conference on Machine learning},\n title = {Using relative novelty to identify useful temporal abstractions in reinforcement learning},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4f3ac36b49b5d8e628a6d048f2eeaaf318508532",
            "@type": "ScholarlyArticle",
            "paperId": "4f3ac36b49b5d8e628a6d048f2eeaaf318508532",
            "corpusId": 14523421,
            "url": "https://www.semanticscholar.org/paper/4f3ac36b49b5d8e628a6d048f2eeaaf318508532",
            "title": "Power systems stability control: reinforcement learning framework",
            "venue": "IEEE Transactions on Power Systems",
            "publicationVenue": {
                "id": "urn:research:dbbda9ef-0504-4875-b893-5c964f6b8f0e",
                "name": "IEEE Transactions on Power Systems",
                "alternate_names": [
                    "IEEE Trans Power Syst"
                ],
                "issn": "0885-8950",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=59"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2098172344",
                "DOI": "10.1109/TPWRS.2003.821457",
                "CorpusId": 14523421
            },
            "abstract": "In this paper, we explore how a computational approach to learning from interactions, called reinforcement learning (RL), can be applied to control power systems. We describe some challenges in power system control and discuss how some of those challenges could be met by using these RL methods. The difficulties associated with their application to control power systems are described and discussed as well as strategies that can be adopted to overcome them. Two reinforcement learning modes are considered: the online mode in which the interaction occurs with the real power system and the offline mode in which the interaction occurs with a simulation model of the real power system. We present two case studies made on a four-machine power system model. The first one concerns the design by means of RL algorithms used in offline mode of a dynamic brake controller. The second concerns RL methods used in online mode when applied to control a thyristor controlled series capacitor (TCSC) aimed to damp power system oscillations.",
            "referenceCount": 27,
            "citationCount": 211,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2004/EGW04/IEEEtrans-2003.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2004-02-19",
            "journal": {
                "name": "IEEE Transactions on Power Systems",
                "volume": "19"
            },
            "citationStyles": {
                "bibtex": "@Article{Ernst2004PowerSS,\n author = {D. Ernst and M. Glavic and L. Wehenkel},\n booktitle = {IEEE Transactions on Power Systems},\n journal = {IEEE Transactions on Power Systems},\n pages = {427-435},\n title = {Power systems stability control: reinforcement learning framework},\n volume = {19},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3b90b73fa0f904a2dc84bca4b3f80cbb51d7025f",
            "@type": "ScholarlyArticle",
            "paperId": "3b90b73fa0f904a2dc84bca4b3f80cbb51d7025f",
            "corpusId": 6627108,
            "url": "https://www.semanticscholar.org/paper/3b90b73fa0f904a2dc84bca4b3f80cbb51d7025f",
            "title": "Reinforcement Learning with Long Short-Term Memory",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2001,
            "externalIds": {
                "DBLP": "conf/nips/Bakker01",
                "MAG": "2096533821",
                "CorpusId": 6627108
            },
            "abstract": "This paper presents reinforcement learning with a Long Short-Term Memory recurrent neural network: RL-LSTM. Model-free RL-LSTM using Advantage (\u03bb) learning and directed exploration can solve non-Markovian tasks with long-term dependencies between relevant events. This is demonstrated in a T-maze task, as well as in a difficult variation of the pole balancing task.",
            "referenceCount": 13,
            "citationCount": 280,
            "influentialCitationCount": 33,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2001-01-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bakker2001ReinforcementLW,\n author = {B. Bakker},\n booktitle = {Neural Information Processing Systems},\n pages = {1475-1482},\n title = {Reinforcement Learning with Long Short-Term Memory},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:48bf148ca96f928d762c5be9231f1cdff8090cc7",
            "@type": "ScholarlyArticle",
            "paperId": "48bf148ca96f928d762c5be9231f1cdff8090cc7",
            "corpusId": 16398811,
            "url": "https://www.semanticscholar.org/paper/48bf148ca96f928d762c5be9231f1cdff8090cc7",
            "title": "Learning Options in Reinforcement Learning",
            "venue": "Symposium on Abstraction, Reformulation and Approximation",
            "publicationVenue": {
                "id": "urn:research:7deb8ac2-57cb-42a2-a356-d28c146c535c",
                "name": "Symposium on Abstraction, Reformulation and Approximation",
                "alternate_names": [
                    "SARA",
                    "Symp Abstr Reformul Approx"
                ],
                "issn": null,
                "url": null
            },
            "year": 2002,
            "externalIds": {
                "DBLP": "conf/sara/StolleP02",
                "MAG": "1556824961",
                "DOI": "10.1007/3-540-45622-8_16",
                "CorpusId": 16398811
            },
            "abstract": null,
            "referenceCount": 29,
            "citationCount": 329,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2002-08-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Stolle2002LearningOI,\n author = {M. Stolle and Doina Precup},\n booktitle = {Symposium on Abstraction, Reformulation and Approximation},\n pages = {212-223},\n title = {Learning Options in Reinforcement Learning},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:985f2c1baba284e9b7b604b7169a2e2778540fe6",
            "@type": "ScholarlyArticle",
            "paperId": "985f2c1baba284e9b7b604b7169a2e2778540fe6",
            "corpusId": 28998047,
            "url": "https://www.semanticscholar.org/paper/985f2c1baba284e9b7b604b7169a2e2778540fe6",
            "title": "Temporal abstraction in reinforcement learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "1585861384",
                "CorpusId": 28998047
            },
            "abstract": "Decision making usually involves choosing among different courses of action over a broad range of time scales. For instance, a person planning a trip to a distant location makes high-level decisions regarding what means of transportation to use, but also chooses low-level actions, such as the movements for getting into a car. The problem of picking an appropriate time scale for reasoning and learning has been explored in artificial intelligence, control theory and robotics. In this dissertation we develop a framework that allows novel solutions to this problem, in the context of Markov Decision Processes (MDPs) and reinforcement learning. \nIn this dissertation, we present a general framework for prediction, control and learning at multiple temporal scales. In this framework, temporally extended actions are represented by a way of behaving (a policy) together with a termination condition. An action represented in this way is called an option. Options can be easily incorporated in MDPs, allowing an agent to use existing controllers, heuristics for picking actions, or learned courses of action. \nThe effects of behaving according to an option can be predicted using multi-time models, learned by interacting with the environment. In this dissertation we develop multi-time models, and we illustrate the way in which they can be used to produce plans of behavior very quickly, using classical dynamic programming or reinforcement learning techniques. \nThe most interesting feature of our framework is that it allows an agent to work simultaneously with high-level and low-level temporal representations. The interplay of these levels can be exploited in order to learn and plan more efficiently and more accurately. We develop new algorithms that take advantage of this structure to improve the quality of plans, and to learn in parallel about the effects of many different options.",
            "referenceCount": 57,
            "citationCount": 328,
            "influentialCitationCount": 31,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Precup2000TemporalAI,\n author = {Doina Precup and R. Sutton},\n booktitle = {International Conference on Machine Learning},\n title = {Temporal abstraction in reinforcement learning},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:156e32df7f0c149de468f7d5ed4d7c5e17eba7d2",
            "@type": "ScholarlyArticle",
            "paperId": "156e32df7f0c149de468f7d5ed4d7c5e17eba7d2",
            "corpusId": 1479889,
            "url": "https://www.semanticscholar.org/paper/156e32df7f0c149de468f7d5ed4d7c5e17eba7d2",
            "title": "State abstraction for programmable reinforcement learning agents",
            "venue": "AAAI/IAAI",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2002,
            "externalIds": {
                "DBLP": "conf/aaai/AndreR02",
                "MAG": "2089561656",
                "CorpusId": 1479889
            },
            "abstract": "Safe state abstraction in reinforcement learning allows an agent to ignore aspects of its current state that are irrelevant to its current decision, and therefore speeds up dynamic programming and learning. This paper explores safe state abstraction in hierarchical reinforcement learning, where learned behaviors must conform to a given partial, hierarchical program. Unlike previous approaches to this problem, our methods yield significant state abstraction while maintaining <i>hierarchical optimality</i>, i.e., optimality among all policies consistent with the partial program. We show how to achieve this for a partial programming language that is essentially Lisp augmented with nondeterministic constructs. We demonstrate our methods on two variants of Dietterich's taxi domain, showing how state abstraction and hierarchical optimality result in faster learning of better policies and enable the transfer of learned skills from one problem to another.",
            "referenceCount": 15,
            "citationCount": 292,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2002-07-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Andre2002StateAF,\n author = {D. Andre and Stuart J. Russell},\n booktitle = {AAAI/IAAI},\n pages = {119-125},\n title = {State abstraction for programmable reinforcement learning agents},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d26b6180c52013f7eabd06c0c576604bb3e60fb4",
            "@type": "ScholarlyArticle",
            "paperId": "d26b6180c52013f7eabd06c0c576604bb3e60fb4",
            "corpusId": 6423530,
            "url": "https://www.semanticscholar.org/paper/d26b6180c52013f7eabd06c0c576604bb3e60fb4",
            "title": "Using Reinforcement Learning to Spider the Web Efficiently",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1485536830",
                "DBLP": "conf/icml/RennieM99",
                "CorpusId": 6423530
            },
            "abstract": "Consider the task of exploring the Web in order to find pages of a particular kind or on a particular topic. This task arises in the construction of search engines and Web knowledge bases. This paper argues that the creation of efficient web spiders is best framed and solved by reinforcement learning, a branch of machine learning that concerns itself with optimal sequential decision making. One strength of reinforcement learning is that it provides a formalism for measuring the utility of actions that give benefit only in the future. We present an algorithm for learning a value function that maps hyperlinks to future discounted reward using a naive Bayes text classifier. Experiments on two real-world spidering tasks show a threefold improvement in spidering efficiency over traditional breadth-first search, and up to a two-fold improvement over reinforcement learning with immediate reward only.",
            "referenceCount": 18,
            "citationCount": 295,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1999-06-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rennie1999UsingRL,\n author = {Jason D. M. Rennie and A. McCallum},\n booktitle = {International Conference on Machine Learning},\n pages = {335-343},\n title = {Using Reinforcement Learning to Spider the Web Efficiently},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:70b5f973be9b0283546bb33948b3de1f25708cf5",
            "@type": "ScholarlyArticle",
            "paperId": "70b5f973be9b0283546bb33948b3de1f25708cf5",
            "corpusId": 5848466,
            "url": "https://www.semanticscholar.org/paper/70b5f973be9b0283546bb33948b3de1f25708cf5",
            "title": "Experiments with Reinforcement Learning in Problems with Continuous State and Action Spaces",
            "venue": "Adaptive Behavior",
            "publicationVenue": {
                "id": "urn:research:f4f9e4d0-b477-46a6-bd60-07475fc3a0bd",
                "name": "Adaptive Behavior",
                "alternate_names": [
                    "Adapt Behav"
                ],
                "issn": "1059-7123",
                "url": "http://www.sagepub.com/journals/Journal201570/title"
            },
            "year": 1997,
            "externalIds": {
                "DBLP": "journals/adb/SantamariaSR97",
                "MAG": "2154549708",
                "DOI": "10.1177/105971239700600201",
                "CorpusId": 5848466
            },
            "abstract": "A key element in the solution of reinforcement learning problems is the value function. The purpose of this function is to measure the long-term utility or value of any given state. The function is important because an agent can use this measure to decide what to do next. A common problem in reinforcement learning when applied to systems having continuous states and action spaces is that the value function must operate with a domain consisting of real-valued variables, which means that it should be able to represent the value of infinitely many state and action pairs. For this reason, function approximators are used to represent the value function when a close-form solution of the optimal policy is not available. In this article, we extend a previously proposed reinforcement learning algorithm so that it can be used with function approximators that generalize the value of individual experiences across both state and action spaces. In particular, we discuss the benefits of using sparse coarse-coded function approximators to represent value functions and describe in detail three implementations: cerebellar model articulation controllers, instance-based, and case-based. Additionally, we discuss how function approximators having different degrees of resolution in different regions of the state and action spaces may influence the performance and learning efficiency of the agent. We propose a simple and modular technique that can be used to implement function approximators with nonuniform degrees of resolution so that the value function can be represented with higher accuracy in important regions of the state and action spaces. We performed extensive experiments in the double-integrator and pendulum swing-up systems to demonstrate the proposed ideas. '",
            "referenceCount": 27,
            "citationCount": 328,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://smartech.gatech.edu/bitstream/1853/21747/1/tech-report.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1997-09-01",
            "journal": {
                "name": "Adaptive Behavior",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Santamar\u00eda1997ExperimentsWR,\n author = {J. Santamar\u00eda and R. Sutton and A. Ram},\n booktitle = {Adaptive Behavior},\n journal = {Adaptive Behavior},\n pages = {163 - 217},\n title = {Experiments with Reinforcement Learning in Problems with Continuous State and Action Spaces},\n volume = {6},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:97efafdb4a3942ab3efba53ded7413199f79c054",
            "@type": "ScholarlyArticle",
            "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
            "corpusId": 265672495,
            "url": "https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054",
            "title": "Reinforcement Learning: An Introduction",
            "venue": "IEEE Trans. Neural Networks",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "DBLP": "journals/tnn/SuttonB98",
                "MAG": "2121863487",
                "DOI": "10.1109/TNN.1998.712192",
                "CorpusId": 265672495
            },
            "abstract": "Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.",
            "referenceCount": 453,
            "citationCount": 313,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "IEEE Trans. Neural Networks",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Sutton1998ReinforcementLA,\n author = {R. S. Sutton and A. Barto},\n booktitle = {IEEE Trans. Neural Networks},\n journal = {IEEE Trans. Neural Networks},\n pages = {1054-1054},\n title = {Reinforcement Learning: An Introduction},\n volume = {9},\n year = {1998}\n}\n"
            }
        }
    }
]