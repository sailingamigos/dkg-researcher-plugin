[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:936a67aad36a9d9a7799237f0499d2f588d6e8ba",
            "@type": "ScholarlyArticle",
            "paperId": "936a67aad36a9d9a7799237f0499d2f588d6e8ba",
            "corpusId": 15949950,
            "url": "https://www.semanticscholar.org/paper/936a67aad36a9d9a7799237f0499d2f588d6e8ba",
            "title": "A comparison of direct and model-based reinforcement learning",
            "venue": "Proceedings of International Conference on Robotics and Automation",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1997,
            "externalIds": {
                "MAG": "2117629901",
                "DBLP": "conf/icra/AtkesonS97a",
                "DOI": "10.1109/ROBOT.1997.606886",
                "CorpusId": 15949950
            },
            "abstract": "This paper compares direct reinforcement learning (no explicit model) and model-based reinforcement learning on a simple task: pendulum swing up. We find that in this task model-based approaches support reinforcement learning from smaller amounts of training data and efficient handling of changing goals.",
            "referenceCount": 35,
            "citationCount": 270,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1997-04-20",
            "journal": {
                "name": "Proceedings of International Conference on Robotics and Automation",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Atkeson1997ACO,\n author = {C. Atkeson and J. Santamar\u00eda},\n booktitle = {Proceedings of International Conference on Robotics and Automation},\n journal = {Proceedings of International Conference on Robotics and Automation},\n pages = {3557-3564 vol.4},\n title = {A comparison of direct and model-based reinforcement learning},\n volume = {4},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:29b756ebab9a00e65dcc8e0cc77544dbec5d4531",
            "@type": "ScholarlyArticle",
            "paperId": "29b756ebab9a00e65dcc8e0cc77544dbec5d4531",
            "corpusId": 765279,
            "url": "https://www.semanticscholar.org/paper/29b756ebab9a00e65dcc8e0cc77544dbec5d4531",
            "title": "Reinforcement learning of coordination in cooperative multi-agent systems",
            "venue": "AAAI/IAAI",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2085366587",
                "DBLP": "conf/aaai/KapetanakisK02",
                "CorpusId": 765279
            },
            "abstract": "We report on an investigation of reinforcement learning techniques for the learning of coordination in cooperative multi-agent systems. Specifically, we focus on a novel action selection strategy for Q-learning (Watkins 1989). The new technique is applicable to scenarios where mutual observation of actions is not possible.To date, reinforcement learning approaches for such <i>independent </i> agents did not guarantee convergence to the optimal joint action in scenarios with high miscoordination costs. We improve on previous results (Claus & Boutilier 1998) by demonstrating empirically that our extension causes the agents to converge almost always to the optimal joint action even in these difficult cases.",
            "referenceCount": 11,
            "citationCount": 229,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2002-07-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kapetanakis2002ReinforcementLO,\n author = {S. Kapetanakis and D. Kudenko},\n booktitle = {AAAI/IAAI},\n pages = {326-331},\n title = {Reinforcement learning of coordination in cooperative multi-agent systems},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6bc8db0c7444d9c07aad440393b2fd300fb3595c",
            "@type": "ScholarlyArticle",
            "paperId": "6bc8db0c7444d9c07aad440393b2fd300fb3595c",
            "corpusId": 18972907,
            "url": "https://www.semanticscholar.org/paper/6bc8db0c7444d9c07aad440393b2fd300fb3595c",
            "title": "Function Optimization using Connectionist Reinforcement Learning Algorithms",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1991,
            "externalIds": {
                "MAG": "1993411524",
                "DOI": "10.1080/09540099108946587",
                "CorpusId": 18972907
            },
            "abstract": "Any non-associative reinforcement learning algorithm can be viewed as a method for performing function optimization through (possibly noise-corrupted) sampling of function values. We describe the results of simulations in which the optima of several deterministic functions studied by Ackley were sought using variants of REINFORCE algorithms. Some of the algorithms used here incorporated additional heuristic features resembling certain aspects of some of the algorithms used in Ackley's studies. Differing levels of performance were achieved by the various algorithms investigated, but a number of them performed at a level comparable to the best found in Ackley's studies on a number of the tasks, in spite of their simplicity. One of these variants, called REINFORCE/MENT, represents a novel but principled approach to reinforcement learning in nontrivial networks which incorporates an entropy maximization strategy. This was found to perform especially well on more hierarchically organized tasks.",
            "referenceCount": 24,
            "citationCount": 336,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Connection Science",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Williams1991FunctionOU,\n author = {Ronald J. Williams and Jing Peng},\n journal = {Connection Science},\n pages = {241-268},\n title = {Function Optimization using Connectionist Reinforcement Learning Algorithms},\n volume = {3},\n year = {1991}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a7cabf8bbf69510fa15595064fe21fe4b2e69b16",
            "@type": "ScholarlyArticle",
            "paperId": "a7cabf8bbf69510fa15595064fe21fe4b2e69b16",
            "corpusId": 3068608,
            "url": "https://www.semanticscholar.org/paper/a7cabf8bbf69510fa15595064fe21fe4b2e69b16",
            "title": "Practical Reinforcement Learning in Continuous Spaces",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "1585546346",
                "DBLP": "conf/icml/SmartK00",
                "CorpusId": 3068608
            },
            "abstract": "Dynamic control tasks are good candidates for the application of reinforcement learning techniques. However, many of these tasks inherently have continuous state or action variables. This can cause problems for traditional reinforcement learning algorithms which assume discrete states and actions. In this paper, we introduce an algorithm that safely approximates the value function for continuous state control tasks, and that learns quickly from a small amount of data. We give experimental results using this algorithm to learn policies for both a simulated task and also for a real robot, operating in an unaltered environment. The algorithm works well in a traditional learning setting, and demonstrates extremely good learning when bootstrapped with a small amount of human-provided data.",
            "referenceCount": 9,
            "citationCount": 263,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2000-06-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Smart2000PracticalRL,\n author = {W. Smart and L. Kaelbling},\n booktitle = {International Conference on Machine Learning},\n pages = {903-910},\n title = {Practical Reinforcement Learning in Continuous Spaces},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4b7dcca3de306591b54b6cba36cd4a4982860630",
            "@type": "ScholarlyArticle",
            "paperId": "4b7dcca3de306591b54b6cba36cd4a4982860630",
            "corpusId": 2597131,
            "url": "https://www.semanticscholar.org/paper/4b7dcca3de306591b54b6cba36cd4a4982860630",
            "title": "An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email",
            "venue": "Journal of Artificial Intelligence Research",
            "publicationVenue": {
                "id": "urn:research:aef12dca-60a0-4ca3-819b-cad26d309d4e",
                "name": "Journal of Artificial Intelligence Research",
                "alternate_names": [
                    "JAIR",
                    "J Artif Intell Res",
                    "The Journal of Artificial Intelligence Research"
                ],
                "issn": "1076-9757",
                "url": "http://www.jair.org/"
            },
            "year": 2000,
            "externalIds": {
                "DBLP": "journals/jair/Walker00",
                "ArXiv": "1106.0241",
                "MAG": "2062244797",
                "DOI": "10.1613/jair.713",
                "CorpusId": 2597131
            },
            "abstract": "This paper describes a novel method by which a spoken dialogue system can learn to choose an optimal dialogue strategy from its experience interacting with human users. The method is based on a combination of reinforcement learning and performance modeling of spoken dialogue systems. The reinforcement learning component applies Q-learning (Watkins, 1989), while the performance modeling component applies the PARADISE evaluation framework (Walker et al., 1997) to learn the performance function (reward) used in reinforcement learning. We illustrate the method with a spoken dialogue system named elvis (EmaiL Voice Interactive System), that supports access to email over the phone. We conduct a set of experiments for training an optimal dialogue strategy on a corpus of 219 dialogues in which human users interact with elvis over the phone. We then test that strategy on a corpus of 18 dialogues. We show that elvis can learn to optimize its strategy selection for agent initiative, for reading messages, and for summarizing email folders.",
            "referenceCount": 67,
            "citationCount": 245,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1106.0241",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2000-02-01",
            "journal": {
                "name": "J. Artif. Intell. Res.",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Walker2000AnAO,\n author = {M. Walker},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {J. Artif. Intell. Res.},\n pages = {387-416},\n title = {An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email},\n volume = {12},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d186a1a0f03d1da56ef89d4e53127b7efd324f22",
            "@type": "ScholarlyArticle",
            "paperId": "d186a1a0f03d1da56ef89d4e53127b7efd324f22",
            "corpusId": 2384747,
            "url": "https://www.semanticscholar.org/paper/d186a1a0f03d1da56ef89d4e53127b7efd324f22",
            "title": "Hierarchical multi-agent reinforcement learning",
            "venue": "International Conference on Autonomous Agents",
            "publicationVenue": {
                "id": "urn:research:5ac7efbe-1e13-4240-a305-41ba875cf38e",
                "name": "International Conference on Autonomous Agents",
                "alternate_names": [
                    "Agents",
                    "Int Conf Auton Agent"
                ],
                "issn": null,
                "url": "http://www.acm.org/pubs/contents/proceedings/series/agents/"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2989068617",
                "DBLP": "conf/agents/MakarMG01",
                "DOI": "10.1007/s10458-006-7035-4",
                "CorpusId": 2384747
            },
            "abstract": null,
            "referenceCount": 50,
            "citationCount": 245,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www-anw.cs.umass.edu/pubs/2004/ghavamzadeh_m_TECH04.pdf",
                "status": "CLOSED"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-05-28",
            "journal": {
                "name": "Autonomous Agents and Multi-Agent Systems",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Ghavamzadeh2001HierarchicalMR,\n author = {M. Ghavamzadeh and S. Mahadevan and Rajbala Makar},\n booktitle = {International Conference on Autonomous Agents},\n journal = {Autonomous Agents and Multi-Agent Systems},\n pages = {197-229},\n title = {Hierarchical multi-agent reinforcement learning},\n volume = {13},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:db3dd93699cb1c27edfaa430a93c7ab1d8a14275",
            "@type": "ScholarlyArticle",
            "paperId": "db3dd93699cb1c27edfaa430a93c7ab1d8a14275",
            "corpusId": 2306465,
            "url": "https://www.semanticscholar.org/paper/db3dd93699cb1c27edfaa430a93c7ab1d8a14275",
            "title": "Elevator Group Control Using Multiple Reinforcement Learning Agents",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1574700590",
                "DBLP": "journals/ml/CritesB98",
                "DOI": "10.1023/A:1007518724497",
                "CorpusId": 2306465
            },
            "abstract": null,
            "referenceCount": 51,
            "citationCount": 291,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1007518724497.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1998-12-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "33"
            },
            "citationStyles": {
                "bibtex": "@Article{Crites1998ElevatorGC,\n author = {Robert H. Crites and A. Barto},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {235-262},\n title = {Elevator Group Control Using Multiple Reinforcement Learning Agents},\n volume = {33},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:327950ac8d59054c4b4a37f901ac9bdbb8168087",
            "@type": "ScholarlyArticle",
            "paperId": "327950ac8d59054c4b4a37f901ac9bdbb8168087",
            "corpusId": 5956371,
            "url": "https://www.semanticscholar.org/paper/327950ac8d59054c4b4a37f901ac9bdbb8168087",
            "title": "A Generalized Reinforcement-Learning Model: Convergence and Applications",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1996,
            "externalIds": {
                "DBLP": "conf/icml/LittmanS96",
                "MAG": "1626977535",
                "CorpusId": 5956371
            },
            "abstract": "Reinforcement learning is the process by which an autonomous agent uses its experience interacting with an environment to improve its behavior. The Markov decision process (MDP) model is a popular way of formalizing the reinforcement-learning problem, but it is by no means the only way. In this paper, we show how many of the important theoretical results concerning reinforcement learning in MDPs extend to a generalized MDP model that includes MDPs, two-player games and MDPs under a worst-case optimality criterion as special cases. The basis of this extension is a stochastic-approximation theorem that reduces asynchronous convergence to synchronous convergence. Keywords: Reinforcement learning, Q-learning convergence, Markov games",
            "referenceCount": 33,
            "citationCount": 274,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1996-02-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Littman1996AGR,\n author = {M. Littman and Csaba Szepesvari},\n booktitle = {International Conference on Machine Learning},\n pages = {310-318},\n title = {A Generalized Reinforcement-Learning Model: Convergence and Applications},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:19059f39effd43aa1501c9b84fcbddfa1c925de4",
            "@type": "ScholarlyArticle",
            "paperId": "19059f39effd43aa1501c9b84fcbddfa1c925de4",
            "corpusId": 5588386,
            "url": "https://www.semanticscholar.org/paper/19059f39effd43aa1501c9b84fcbddfa1c925de4",
            "title": "Discovering Hierarchy in Reinforcement Learning with HEXQ",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2002,
            "externalIds": {
                "DBLP": "conf/icml/Hengst02",
                "MAG": "1598052524",
                "CorpusId": 5588386
            },
            "abstract": "An open problem in reinforcement learning is discovering hierarchical structure. HEXQ, an algorithm which automatically attempts to decompose and solve a model-free factored MDP hierarchically is described. By searching for aliased Markov sub-space regions based on the state variables the algorithm uses temporal and state abstraction to construct a hierarchy of interlinked smaller MDPs.",
            "referenceCount": 14,
            "citationCount": 235,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2002-07-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hengst2002DiscoveringHI,\n author = {B. Hengst},\n booktitle = {International Conference on Machine Learning},\n pages = {243-250},\n title = {Discovering Hierarchy in Reinforcement Learning with HEXQ},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4d92df4a844c94fbb31b95157488e4b562b4f681",
            "@type": "ScholarlyArticle",
            "paperId": "4d92df4a844c94fbb31b95157488e4b562b4f681",
            "corpusId": 7317294,
            "url": "https://www.semanticscholar.org/paper/4d92df4a844c94fbb31b95157488e4b562b4f681",
            "title": "The Optimal Reward Baseline for Gradient-Based Reinforcement Learning",
            "venue": "Conference on Uncertainty in Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:f9af8000-42f8-410d-a622-e8811e41660a",
                "name": "Conference on Uncertainty in Artificial Intelligence",
                "alternate_names": [
                    "Uncertainty in Artificial Intelligence",
                    "UAI",
                    "Conf Uncertain Artif Intell",
                    "Uncertain Artif Intell"
                ],
                "issn": null,
                "url": "http://www.auai.org/"
            },
            "year": 2001,
            "externalIds": {
                "DBLP": "conf/uai/WeaverT01",
                "MAG": "2156718681",
                "ArXiv": "1301.2315",
                "CorpusId": 7317294
            },
            "abstract": "There exist a number of reinforcement learning algorithms which learn by climbing the gradient of expected reward. Their long-run convergence has been proved, even in partially observable environments with non-deterministic actions, and without the need for a system model. However, the variance of the gradient estimator has been found to be a significant practical problem. Recent approaches have discounted future rewards, introducing a bias-variance trade-off into the gradient estimate. We incorporate a reward baseline into the learning system, and show that it affects variance without introducing further bias. In particular, as we approach the zerobias, high-variance parametedzation, the optimal (or variance minimizing) constant reward baseline is equal to the long-term average expected reward. Modified policy-gradient algorithms are presented, and a number of experiments demonstrate their improvement over previous work.",
            "referenceCount": 25,
            "citationCount": 230,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2001-08-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Weaver2001TheOR,\n author = {Lex Weaver and Nigel Tao},\n booktitle = {Conference on Uncertainty in Artificial Intelligence},\n pages = {538-545},\n title = {The Optimal Reward Baseline for Gradient-Based Reinforcement Learning},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9bc780c0c9e8e4dba0533f10a0068f8476cf9b24",
            "@type": "ScholarlyArticle",
            "paperId": "9bc780c0c9e8e4dba0533f10a0068f8476cf9b24",
            "corpusId": 15370837,
            "url": "https://www.semanticscholar.org/paper/9bc780c0c9e8e4dba0533f10a0068f8476cf9b24",
            "title": "Dynamic preferences in multi-criteria reinforcement learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "conf/icml/NatarajanT05",
                "MAG": "1998649829",
                "DOI": "10.1145/1102351.1102427",
                "CorpusId": 15370837
            },
            "abstract": "The current framework of reinforcement learning is based on maximizing the expected returns based on scalar rewards. But in many real world situations, tradeoffs must be made among multiple objectives. Moreover, the agent's preferences between different objectives may vary with time. In this paper, we consider the problem of learning in the presence of time-varying preferences among multiple objectives, using numeric weights to represent their importance. We propose a method that allows us to store a finite number of policies, choose an appropriate policy for any weight vector and improve upon it. The idea is that although there are infinitely many weight vectors, they may be well-covered by a small number of optimal policies. We show this empirically in two domains: a version of the Buridan's ass problem and network routing.",
            "referenceCount": 23,
            "citationCount": 143,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2005-08-07",
            "journal": {
                "name": "Proceedings of the 22nd international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Natarajan2005DynamicPI,\n author = {Sriraam Natarajan and Prasad Tadepalli},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the 22nd international conference on Machine learning},\n title = {Dynamic preferences in multi-criteria reinforcement learning},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a2fe9dbd8858e4e0dcb8ece01563d87ceebe5cbc",
            "@type": "ScholarlyArticle",
            "paperId": "a2fe9dbd8858e4e0dcb8ece01563d87ceebe5cbc",
            "corpusId": 20156726,
            "url": "https://www.semanticscholar.org/paper/a2fe9dbd8858e4e0dcb8ece01563d87ceebe5cbc",
            "title": "Algorithms and representations for reinforcement learning (\u05e2\u05dd \u05ea\u05e7\u05e6\u05d9\u05e8 \u05d1\u05e2\u05d1\u05e8\u05d9\u05ea, \u05ea\u05db\u05df \u05d5\u05e9\u05e2\u05e8 \u05e0\u05d5\u05e1\u05e3: \u05d0\u05dc\u05d2\u05d5\u05e8\u05d9\u05ea\u05de\u05d9\u05dd \u05d5\u05d9\u05d9\u05e6\u05d5\u05d2\u05d9\u05dd \u05dc\u05dc\u05de\u05d9\u05d3\u05d4 \u05de\u05d7\u05d9\u05d6\u05d5\u05e7\u05d9\u05dd.; \u05d0\u05dc\u05d2\u05d5\u05e8\u05d9\u05ea\u05de\u05d9\u05dd \u05d5\u05d9\u05d9\u05e6\u05d5\u05d2\u05d9\u05dd \u05dc\u05dc\u05de\u05d9\u05d3\u05d4 \u05de\u05d7\u05d9\u05d6\u05d5\u05e7\u05d9\u05dd.)",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2133733518",
                "DBLP": "phd/il/Engel05",
                "CorpusId": 20156726
            },
            "abstract": "Machine Learning is a field of research aimed at constructing intelligent machines that gain and improve their skills by learning and adaptation. As such, Machine Learning research addresses several classes of learning problems, including for instance, supervised and unsupervised learning. Arguably, the most ubiquitous and realistic class of learning problems, faced by both living creatures and artificial agents, is known as Reinforcement Learning. Reinforcement Learning problems are characterized by a long-term interaction between the learning agent and a dynamic, unfamiliar, uncertain, possibly even hostile environment. Mathematically, this interaction is modeled as a Markov Decision Process (MDP). Probably the most significant contribution of this thesis is in the introduction of a new class of Reinforcement Learning algorithms, which leverage the power of a statistical set of tools known as Gaussian Processes. This new approach to Reinforcement Learning offers viable solutions to some of the major limitations of current Reinforcement Learning methods, such as the lack of confidence intervals for performance predictions, and the difficulty of appropriately reconciling exploration with exploitation. Analysis of these algorithms and their relationship with existing methods also provides us with new insights into the assumptions underlying some of the most popular Reinforcement Learning algorithms to date.",
            "referenceCount": 97,
            "citationCount": 98,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Engel2005AlgorithmsAR,\n author = {Y. Engel},\n title = {Algorithms and representations for reinforcement learning (\u05e2\u05dd \u05ea\u05e7\u05e6\u05d9\u05e8 \u05d1\u05e2\u05d1\u05e8\u05d9\u05ea, \u05ea\u05db\u05df \u05d5\u05e9\u05e2\u05e8 \u05e0\u05d5\u05e1\u05e3: \u05d0\u05dc\u05d2\u05d5\u05e8\u05d9\u05ea\u05de\u05d9\u05dd \u05d5\u05d9\u05d9\u05e6\u05d5\u05d2\u05d9\u05dd \u05dc\u05dc\u05de\u05d9\u05d3\u05d4 \u05de\u05d7\u05d9\u05d6\u05d5\u05e7\u05d9\u05dd.; \u05d0\u05dc\u05d2\u05d5\u05e8\u05d9\u05ea\u05de\u05d9\u05dd \u05d5\u05d9\u05d9\u05e6\u05d5\u05d2\u05d9\u05dd \u05dc\u05dc\u05de\u05d9\u05d3\u05d4 \u05de\u05d7\u05d9\u05d6\u05d5\u05e7\u05d9\u05dd.)},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c11ec1d4e7c7f8bace39182b695e2baa6e8aa340",
            "@type": "ScholarlyArticle",
            "paperId": "c11ec1d4e7c7f8bace39182b695e2baa6e8aa340",
            "corpusId": 7295794,
            "url": "https://www.semanticscholar.org/paper/c11ec1d4e7c7f8bace39182b695e2baa6e8aa340",
            "title": "Proto-value functions: developmental reinforcement learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2143958939",
                "DBLP": "conf/icml/Mahadevan05",
                "DOI": "10.1145/1102351.1102421",
                "CorpusId": 7295794
            },
            "abstract": "This paper presents a novel framework called proto-reinforcement learning (PRL), based on a mathematical model of a proto-value function: these are task-independent basis functions that form the building blocks of all value functions on a given state space manifold. Proto-value functions are learned not from rewards, but instead from analyzing the topology of the state space. Formally, proto-value functions are Fourier eigenfunctions of the Laplace-Beltrami diffusion operator on the state space manifold. Proto-value functions facilitate structural decomposition of large state spaces, and form geodesically smooth orthonormal basis functions for approximating any value function. The theoretical basis for proto-value functions combines insights from spectral graph theory, harmonic analysis, and Riemannian manifolds. Proto-value functions enable a novel generation of algorithms called representation policy iteration, unifying the learning of representation and behavior.",
            "referenceCount": 12,
            "citationCount": 135,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2005-08-07",
            "journal": {
                "name": "Proceedings of the 22nd international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Mahadevan2005ProtovalueFD,\n author = {S. Mahadevan},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the 22nd international conference on Machine learning},\n title = {Proto-value functions: developmental reinforcement learning},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bb9fbee22eae2b47bbf304804a6ac07def1aecdb",
            "@type": "ScholarlyArticle",
            "paperId": "bb9fbee22eae2b47bbf304804a6ac07def1aecdb",
            "corpusId": 16264735,
            "url": "https://www.semanticscholar.org/paper/bb9fbee22eae2b47bbf304804a6ac07def1aecdb",
            "title": "Evolutionary game theory and multi-agent reinforcement learning",
            "venue": "Knowledge engineering review (Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "journals/ker/TuylsN05",
                "MAG": "2045817941",
                "DOI": "10.1017/S026988890500041X",
                "CorpusId": 16264735
            },
            "abstract": "In this paper we survey the basics of reinforcement learning and (evolutionary) game theory, applied to the field of multi-agent systems. This paper contains three parts. We start with an overview on the fundamentals of reinforcement learning. Next we summarize the most important aspects of evolutionary game theory. Finally, we discuss the state-of-the-art of multi-agent reinforcement learning and the mathematical connection with evolutionary game theory.",
            "referenceCount": 69,
            "citationCount": 111,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2005-03-01",
            "journal": {
                "name": "The Knowledge Engineering Review",
                "volume": "20"
            },
            "citationStyles": {
                "bibtex": "@Article{Tuyls2005EvolutionaryGT,\n author = {K. Tuyls and A. Now\u00e9},\n booktitle = {Knowledge engineering review (Print)},\n journal = {The Knowledge Engineering Review},\n pages = {63 - 90},\n title = {Evolutionary game theory and multi-agent reinforcement learning},\n volume = {20},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:165c7cc7c36112bb4215631c0270b5a8a5241dcd",
            "@type": "ScholarlyArticle",
            "paperId": "165c7cc7c36112bb4215631c0270b5a8a5241dcd",
            "corpusId": 962364,
            "url": "https://www.semanticscholar.org/paper/165c7cc7c36112bb4215631c0270b5a8a5241dcd",
            "title": "Concurrent Hierarchical Reinforcement Learning",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "conf/aaai/Marthi05",
                "MAG": "2463485124",
                "CorpusId": 962364
            },
            "abstract": "We consider applying hierarchical reinforcement learning techniques to problems in which an agent has several effectors to control simultaneously. We argue that the kind of prior knowledge one typically has about such problems is best expressed using a multithreaded partial program, and present concurrent ALisp, a language for specifying such partial programs. We describe algorithms for learning and acting with concurrent ALisp that can be efficient even when there are exponentially many joint choices at each decision point. Finally, we show results of applying these methods to a complex computer game domain.",
            "referenceCount": 80,
            "citationCount": 107,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2005-07-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Marthi2005ConcurrentHR,\n author = {B. Marthi},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {779-785},\n title = {Concurrent Hierarchical Reinforcement Learning},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b218063e7dd82f4350ec1f543e9a8234f54bb3b4",
            "@type": "ScholarlyArticle",
            "paperId": "b218063e7dd82f4350ec1f543e9a8234f54bb3b4",
            "corpusId": 4869621,
            "url": "https://www.semanticscholar.org/paper/b218063e7dd82f4350ec1f543e9a8234f54bb3b4",
            "title": "Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "journals/ml/AsadaNTH96",
                "DOI": "10.1023/A:1018237008823",
                "CorpusId": 4869621
            },
            "abstract": null,
            "referenceCount": 20,
            "citationCount": 159,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1018237008823.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "23"
            },
            "citationStyles": {
                "bibtex": "@Article{Asada2005PurposiveBA,\n author = {M. Asada and S. Noda and Sukoya Tawaratsumida and K. Hosoda},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {279-303},\n title = {Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning},\n volume = {23},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b603452fd76e12ebe27de698e2d91e58b4da45f9",
            "@type": "ScholarlyArticle",
            "paperId": "b603452fd76e12ebe27de698e2d91e58b4da45f9",
            "corpusId": 15612854,
            "url": "https://www.semanticscholar.org/paper/b603452fd76e12ebe27de698e2d91e58b4da45f9",
            "title": "Reinforcement Learning Neural Network to the Problem of Autonomous Mobile Robot Obstacle Avoidance",
            "venue": "International Conference on Machine Learning and Computing",
            "publicationVenue": {
                "id": "urn:research:d79c0d2a-37de-4287-87e7-1e57576dcae7",
                "name": "International Conference on Machine Learning and Computing",
                "alternate_names": [
                    "Int Conf Mach Learn Cybern",
                    "International Conference on Machine Learning and Cybernetics",
                    "International Conference Machine Learning and Computing",
                    "Int Conf Mach Learn Comput",
                    "ICMLC"
                ],
                "issn": null,
                "url": "http://www.icmlc.com/"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2151908411",
                "DOI": "10.1109/ICMLC.2005.1526924",
                "CorpusId": 15612854
            },
            "abstract": "An approach to the problem of autonomous mobile robot obstacle avoidance using reinforcement learning neural network is proposed in this paper. Q-learning is one kind of reinforcement learning method that is similar to dynamic programming and the neural network has a powerful ability to store the values. We integrate these two methods with the aim to ensure autonomous robot behavior in complicated unpredictable environment. The simulation results show that the simulated robot using the reinforcement learning neural network can enhance its learning ability obviously and can finish the given task in a complex environment.",
            "referenceCount": 12,
            "citationCount": 112,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference"
            ],
            "publicationDate": "2005-11-07",
            "journal": {
                "name": "2005 International Conference on Machine Learning and Cybernetics",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Conference{Huang2005ReinforcementLN,\n author = {Bingqiang Huang and G. Cao and Min Guo},\n booktitle = {International Conference on Machine Learning and Computing},\n journal = {2005 International Conference on Machine Learning and Cybernetics},\n pages = {85-89},\n title = {Reinforcement Learning Neural Network to the Problem of Autonomous Mobile Robot Obstacle Avoidance},\n volume = {1},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:183013b85235f2f84abd606e8527160a1d915190",
            "@type": "ScholarlyArticle",
            "paperId": "183013b85235f2f84abd606e8527160a1d915190",
            "corpusId": 403512,
            "url": "https://www.semanticscholar.org/paper/183013b85235f2f84abd606e8527160a1d915190",
            "title": "Multiagent Reinforcement Learning for Multi-Robot Systems: A Survey",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "MAG": "56117469",
                "CorpusId": 403512
            },
            "abstract": "Multiagent reinforcement learning for multirobot systems is a challenging issue in both robotics and artificial intelligence. With the ever increasing interests in theoretical researches and practical applications, currently there have been a lot of efforts towards providing some solutions to this challenge. However, there are still many difficulties in scaling up the multiagent reinforcement learning to multi-robot systems. The main objective of this paper is to provide a survey, though not completely on the multiagent reinforcement learning in multi-robot systems. After reviewing important advances in this field, some challenging problems and promising research directions are analyzed. A concluding remark is made from the perspectives of the authors.",
            "referenceCount": 72,
            "citationCount": 182,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Yang2004MultiagentRL,\n author = {Erfu Yang and Dongbing Gu},\n title = {Multiagent Reinforcement Learning for Multi-Robot Systems: A Survey},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3cce92bc77e86fcf6f31d8b0b9d57502eb92934a",
            "@type": "ScholarlyArticle",
            "paperId": "3cce92bc77e86fcf6f31d8b0b9d57502eb92934a",
            "corpusId": 10511015,
            "url": "https://www.semanticscholar.org/paper/3cce92bc77e86fcf6f31d8b0b9d57502eb92934a",
            "title": "Accelerating Reinforcement Learning through Implicit Imitation",
            "venue": "Journal of Artificial Intelligence Research",
            "publicationVenue": {
                "id": "urn:research:aef12dca-60a0-4ca3-819b-cad26d309d4e",
                "name": "Journal of Artificial Intelligence Research",
                "alternate_names": [
                    "JAIR",
                    "J Artif Intell Res",
                    "The Journal of Artificial Intelligence Research"
                ],
                "issn": "1076-9757",
                "url": "http://www.jair.org/"
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2097113539",
                "DBLP": "journals/corr/abs-1106-0681",
                "ArXiv": "1106.0681",
                "DOI": "10.1613/jair.898",
                "CorpusId": 10511015
            },
            "abstract": "Imitation can be viewed as a means of enhancing learning in multiagent environments. It augments an agent's ability to learn useful behaviors by making intelligent use of the knowledge implicit in behaviors demonstrated by cooperative teachers or other more experienced agents. We propose and study a formal model of implicit imitation that can accelerate reinforcement learning dramatically in certain cases. Roughly, by observing a mentor, a reinforcement-learning agent can extract information about its own capabilities in, and the relative value of, unvisited parts of the state space. We study two specific instantiations of this model, one in which the learning agent and the mentor have identical abilities, and one designed to deal with agents and mentors with difierent action sets. We illustrate the benefits of implicit imitation by integrating it with prioritized sweeping, and demonstrating improved performance and convergence through observation of single and multiple mentors. Though we make some stringent assumptions regarding observability and possible interactions, we briefly comment on extensions of the model that relax these restricitions.",
            "referenceCount": 74,
            "citationCount": 194,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jair.org/index.php/jair/article/download/10348/24745",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2003-07-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1106.0681"
            },
            "citationStyles": {
                "bibtex": "@Article{Boutilier2003AcceleratingRL,\n author = {Craig Boutilier and Bob Price},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {ArXiv},\n title = {Accelerating Reinforcement Learning through Implicit Imitation},\n volume = {abs/1106.0681},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2f650fc2daf01cf343c36537e155e9d040b5df13",
            "@type": "ScholarlyArticle",
            "paperId": "2f650fc2daf01cf343c36537e155e9d040b5df13",
            "corpusId": 59728138,
            "url": "https://www.semanticscholar.org/paper/2f650fc2daf01cf343c36537e155e9d040b5df13",
            "title": "A Menu of Designs for Reinforcement Learning Over Time",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1995,
            "externalIds": {
                "MAG": "92805021",
                "DOI": "10.7551/mitpress/4939.003.0007",
                "CorpusId": 59728138
            },
            "abstract": "This chapter contains sections titled: Introduction and Overview, A Simple Two-Component Adaptive Critic Design, HDP and Dynamic Programming, Alternative Ways to Figure 3.2 in Adapting the Action Network, Alternatives to HDP in Adapting the Critic Network, Some Topics for Further Research, Equations and Code For Implementation, References",
            "referenceCount": 0,
            "citationCount": 582,
            "influentialCitationCount": 30,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Miller1995AMO,\n author = {W. Miller and R. Sutton and P. Werbos},\n pages = {67-95},\n title = {A Menu of Designs for Reinforcement Learning Over Time},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:170cc2f8373322ab91036bbb66fc52b5c5c37e83",
            "@type": "ScholarlyArticle",
            "paperId": "170cc2f8373322ab91036bbb66fc52b5c5c37e83",
            "corpusId": 15986335,
            "url": "https://www.semanticscholar.org/paper/170cc2f8373322ab91036bbb66fc52b5c5c37e83",
            "title": "Performance functions and reinforcement learning for trading systems and portfolios",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1977051850",
                "DOI": "10.1002/(SICI)1099-131X(1998090)17:5/6<441::AID-FOR707>3.0.CO;2-#",
                "CorpusId": 15986335
            },
            "abstract": "We propose to train trading systems and portfolios by optimizing objective functions that directly measure trading and investment performance. Rather than basing a trading system on forecasts or training via a supervised learning algorithm using labelled trading data, we train our systems using recurrent reinforcement learning (RRL) algorithms. The performance functions that we consider for reinforcement learning are profit or wealth, economic utility, the Sharpe ratio and our proposed differential Sharpe ratio. The trading and portfolio management systems require prior decisions as input in order to properly take into account the effects of transactions costs, market impact, and taxes. This temporal dependence on system state requires the use of reinforcement versions of standard recurrent learning algorithms. We present empirical results in controlled experiments that demonstrate the efficacy of some of our methods for optimizing trading systems and portfolios. For a long/short trader, we find that maximizing the differential Sharpe ratio yields more consistent results than maximizing profits, and that both methods outperform a trading system based on forecasts that minimize MSE. We find that portfolio traders trained to maximize the differential Sharpe ratio achieve better risk-adjusted returns than those trained to maximize profit. Finally, we provide simulation results for an S&P 500/TBill asset allocation system that demonstrate the presence of out-of-sample predictability in the monthly S&P 500 stock index for the 25 year period 1970 through 1994. Copyright \u00a9 1998 John Wiley & Sons, Ltd.",
            "referenceCount": 27,
            "citationCount": 232,
            "influentialCitationCount": 33,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Economics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Economics",
                    "source": "external"
                },
                {
                    "category": "Business",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1998-09-01",
            "journal": {
                "name": "Journal of Forecasting",
                "volume": "17"
            },
            "citationStyles": {
                "bibtex": "@Article{Moody1998PerformanceFA,\n author = {J. Moody and Lizhong Wu and Yuansong Liao and M. Saffell},\n journal = {Journal of Forecasting},\n pages = {441-470},\n title = {Performance functions and reinforcement learning for trading systems and portfolios},\n volume = {17},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f9f5e6ebe5e4195db2e795335bff5e176fdc1ad4",
            "@type": "ScholarlyArticle",
            "paperId": "f9f5e6ebe5e4195db2e795335bff5e176fdc1ad4",
            "corpusId": 5894831,
            "url": "https://www.semanticscholar.org/paper/f9f5e6ebe5e4195db2e795335bff5e176fdc1ad4",
            "title": "Scaling Reinforcement Learning toward RoboCup Soccer",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2155791599",
                "DBLP": "conf/icml/StoneS01",
                "CorpusId": 5894831
            },
            "abstract": "RoboCup simulated soccer presents many challenges to reinforcement learning methods, including a large state space, hidden and uncertain state, multiple agents, and long and variable delays in the effects of actions. We describe our application of episodic SMDP Sarsa(\u03bb) with linear tile-coding function approximation and variable \u03bb to learning higher-level decisions in a keepaway subtask of RoboCup soccer. In keepaway, one team, \u201cthe keepers,\u201d tries to keep control of the ball for as long as possible despite the efforts of \u201cthe takers.\u201d The keepers learn individually when to hold the ball and when to pass to a teammate, while the takers learn when to charge the ball-holder and when to cover possible passing lanes. Our agents learned policies that significantly out-performed a range of benchmark policies. We demonstrate the generality of our approach by applying it to a number of task variations including different field sizes and different numbers of players on each team.",
            "referenceCount": 29,
            "citationCount": 218,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2001-06-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Stone2001ScalingRL,\n author = {P. Stone and R. Sutton},\n booktitle = {International Conference on Machine Learning},\n pages = {537-544},\n title = {Scaling Reinforcement Learning toward RoboCup Soccer},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:839f1b61803e65f20f067b361d0ebf6db337172c",
            "@type": "ScholarlyArticle",
            "paperId": "839f1b61803e65f20f067b361d0ebf6db337172c",
            "corpusId": 6543198,
            "url": "https://www.semanticscholar.org/paper/839f1b61803e65f20f067b361d0ebf6db337172c",
            "title": "Reinforcement Learning for Spoken Dialogue Systems",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1999,
            "externalIds": {
                "MAG": "2163068732",
                "DBLP": "conf/nips/SinghKLW99",
                "CorpusId": 6543198
            },
            "abstract": "Recently, a number of authors have proposed treating dialogue systems as Markov decision processes (MDPs). However, the practical application of MDP algorithms to dialogue systems faces a number of severe technical challenges. We have built a general software tool (RLDS, for Reinforcement Learning for Dialogue Systems) based on the MDP framework, and have applied it to dialogue corpora gathered from two dialogue systems built at AT&T Labs. Our experiments demonstrate that RLDS holds promise as a tool for \"browsing\" and understanding correlations in complex, temporally dependent dialogue corpora.",
            "referenceCount": 9,
            "citationCount": 230,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1999-11-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Singh1999ReinforcementLF,\n author = {Satinder Singh and Michael Kearns and D. Litman and M. Walker},\n booktitle = {Neural Information Processing Systems},\n pages = {956-962},\n title = {Reinforcement Learning for Spoken Dialogue Systems},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:45d2bdf5e7072c1d5a91a38efa365715def2f45d",
            "@type": "ScholarlyArticle",
            "paperId": "45d2bdf5e7072c1d5a91a38efa365715def2f45d",
            "corpusId": 7213327,
            "url": "https://www.semanticscholar.org/paper/45d2bdf5e7072c1d5a91a38efa365715def2f45d",
            "title": "Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 1991,
            "externalIds": {
                "DBLP": "conf/ijcai/ChapmanK91",
                "MAG": "1640646391",
                "CorpusId": 7213327
            },
            "abstract": "Delayed reinforcement learning is an attractive framework for the unsupervised learning of action policies for autonomous agents. Some existing delayed reinforcement learning techniques have shown promise in simple domains. However, a number of hurdles must be passed before they are applicable to realistic problems. This paper describes one such difficulty, the input generalization problem (whereby the system must generalize to produce similar actions in similar situations) and an implemented solution, the G algorithm. This algorithm is based on recursive splitting of the state space based on statistical measures of differences in reinforcements received. Connectionist backpropagation has previously been used for input generalization in reinforcement learning. We compare the two techniques analytically and empirically. The G algorithm's sound statistical basis makes it easy to predict when it should and should not work, whereas the behavior of back-propagation is unpredictable. We found that a previous successful use of backpropagation can be explained by the linearity of the application domain. We found that in another domain, G reliably found the optimal policy, whereas none of a set of runs of backpropagation with many combinations of parameters did.",
            "referenceCount": 22,
            "citationCount": 289,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1991-08-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chapman1991InputGI,\n author = {David Chapman and L. Kaelbling},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {726-731},\n title = {Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons},\n year = {1991}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e3678df4a8c183fc50d59e6277284078785600e6",
            "@type": "ScholarlyArticle",
            "paperId": "e3678df4a8c183fc50d59e6277284078785600e6",
            "corpusId": 7620621,
            "url": "https://www.semanticscholar.org/paper/e3678df4a8c183fc50d59e6277284078785600e6",
            "title": "Efficient Reinforcement Learning in Factored MDPs",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1554015367",
                "DBLP": "conf/ijcai/KearnsK99",
                "CorpusId": 7620621
            },
            "abstract": "We present a provably efficient and near-optimal algorithm for reinforcement learning in Markov decision processes (MDPs) whose transition model can be factored as a dynamic Bayesian network (DBN). Our algorithm generalizes the recent E3 algorithm of Kearns and Singh, and assumes that we are given both an algorithm for approximate planning, and the graphical structure (but not the parameters) of the DBN. Unlike the original E algorithm, our new algorithm exploits the DBN structure to achieve a running time that scales polynomially in the number of parameters of the DBN, which may be exponentially smaller than the number of global states.",
            "referenceCount": 12,
            "citationCount": 220,
            "influentialCitationCount": 28,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1999-07-31",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kearns1999EfficientRL,\n author = {M. Kearns and D. Koller},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {740-747},\n title = {Efficient Reinforcement Learning in Factored MDPs},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0a29eb3e524809f869f99481549ebbb25f487494",
            "@type": "ScholarlyArticle",
            "paperId": "0a29eb3e524809f869f99481549ebbb25f487494",
            "corpusId": 47405459,
            "url": "https://www.semanticscholar.org/paper/0a29eb3e524809f869f99481549ebbb25f487494",
            "title": "Reinforcement learning is direct adaptive optimal control",
            "venue": "IEEE Control Systems",
            "publicationVenue": {
                "id": "urn:research:c7be0e82-620f-44c5-b6de-46310d59c7b8",
                "name": "IEEE Control Systems",
                "alternate_names": [
                    "IEEE Control Syst",
                    "IEEE Control Systems Magazine",
                    "IEEE Control Syst Mag"
                ],
                "issn": "1066-033X",
                "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5488303"
            },
            "year": 1991,
            "externalIds": {
                "MAG": "1599610710",
                "DOI": "10.23919/ACC.1991.4791776",
                "CorpusId": 47405459
            },
            "abstract": "Neural network reinforcement learning methods are described and considered as a direct approach to adaptive optimal control of nonlinear systems. These methods have their roots in studies of animal learning and in early learning control work. An emerging deeper understanding of these methods is summarized that is obtained by viewing them as a synthesis of dynamic programming and stochastic approximation methods. The focus is on Q-learning systems, which maintain estimates of utilities for all state-action pairs and make use of these estimates to select actions. The use of hybrid direct/indirect methods is briefly discussed.<<ETX>>",
            "referenceCount": 15,
            "citationCount": 300,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1991-06-26",
            "journal": {
                "name": "IEEE Control Systems",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Sutton1991ReinforcementLI,\n author = {R. Sutton and A. Barto and Ronald J. Williams},\n booktitle = {IEEE Control Systems},\n journal = {IEEE Control Systems},\n pages = {19-22},\n title = {Reinforcement learning is direct adaptive optimal control},\n volume = {12},\n year = {1991}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e391fc78cafaac5d31d556cf5ef8070be5107022",
            "@type": "ScholarlyArticle",
            "paperId": "e391fc78cafaac5d31d556cf5ef8070be5107022",
            "corpusId": 1990879,
            "url": "https://www.semanticscholar.org/paper/e391fc78cafaac5d31d556cf5ef8070be5107022",
            "title": "A Geometric Approach to Multi-Criterion Reinforcement Learning",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2117626647",
                "DBLP": "journals/jmlr/MannorS04",
                "CorpusId": 1990879
            },
            "abstract": "We consider the problem of reinforcement learning in a controlled Markov environment with multiple objective functions of the long-term average reward type. The environment is initially unknown, and furthermore may be affected by the actions of other agents, actions that are observed but cannot be predicted beforehand. We capture this situation using a stochastic game model, where the learning agent is facing an adversary whose policy is arbitrary and unknown, and where the reward function is vector-valued. State recurrence conditions are imposed throughout. In our basic problem formulation, a desired target set is specified in the vector reward space, and the objective of the learning agent is to approach the target set, in the sense that the long-term average reward vector will belong to this set. We devise appropriate learning algorithms, that essentially use multiple reinforcement learning algorithms for the standard scalar reward problem, which are combined using the geometric insight from the theory of approachability for vector-valued stochastic games. We then address the more general and optimization-related problem, where a nested class of possible target sets is prescribed, and the goal of the learning agent is to approach the smallest possible target set (which will generally depend on the unknown system parameters). A particular case which falls into this framework is that of stochastic games with average reward constraints, and further specialization provides a reinforcement learning algorithm for constrained Markov decision processes. Some basic examples are provided to illustrate these results.",
            "referenceCount": 48,
            "citationCount": 113,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2004-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Mannor2004AGA,\n author = {Shie Mannor and N. Shimkin},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {325-360},\n title = {A Geometric Approach to Multi-Criterion Reinforcement Learning},\n volume = {5},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a58956f47901848c34db8eda7c40590acebc4a36",
            "@type": "ScholarlyArticle",
            "paperId": "a58956f47901848c34db8eda7c40590acebc4a36",
            "corpusId": 7885581,
            "url": "https://www.semanticscholar.org/paper/a58956f47901848c34db8eda7c40590acebc4a36",
            "title": "Integrating Guidance into Relational Reinforcement Learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2133632477",
                "DBLP": "journals/ml/DriessensD04",
                "DOI": "10.1023/B:MACH.0000039779.47329.3a",
                "CorpusId": 7885581
            },
            "abstract": null,
            "referenceCount": 47,
            "citationCount": 100,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://lirias.kuleuven.be/bitstream/123456789/125299/1/2004_mlj_driessens.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2004-12-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "57"
            },
            "citationStyles": {
                "bibtex": "@Article{Driessens2004IntegratingGI,\n author = {K. Driessens and S. D\u017eeroski},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {271-304},\n title = {Integrating Guidance into Relational Reinforcement Learning},\n volume = {57},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ecfa791e76929fa6a8ea61f66df49c70f3b1a600",
            "@type": "ScholarlyArticle",
            "paperId": "ecfa791e76929fa6a8ea61f66df49c70f3b1a600",
            "corpusId": 3330642,
            "url": "https://www.semanticscholar.org/paper/ecfa791e76929fa6a8ea61f66df49c70f3b1a600",
            "title": "Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2004,
            "externalIds": {
                "DOI": "10.1023/A:1022628806385",
                "CorpusId": 3330642
            },
            "abstract": null,
            "referenceCount": 28,
            "citationCount": 157,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1022628806385.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Lin2004SelfImprovingRA,\n author = {Longxin Lin},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {293-321},\n title = {Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching},\n volume = {8},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:18d700bf151eb6f13341b03c81ac3764d74196ea",
            "@type": "ScholarlyArticle",
            "paperId": "18d700bf151eb6f13341b03c81ac3764d74196ea",
            "corpusId": 8938271,
            "url": "https://www.semanticscholar.org/paper/18d700bf151eb6f13341b03c81ac3764d74196ea",
            "title": "Kernel-Based Reinforcement Learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "1550698229",
                "DBLP": "journals/ml/OrmoneitS02",
                "DOI": "10.1023/A:1017928328829",
                "CorpusId": 8938271
            },
            "abstract": null,
            "referenceCount": 33,
            "citationCount": 153,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1017928328829.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "49"
            },
            "citationStyles": {
                "bibtex": "@Article{Ormoneit2004KernelBasedRL,\n author = {Dirk Ormoneit and \u015a. Sen},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {161-178},\n title = {Kernel-Based Reinforcement Learning},\n volume = {49},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1fe7e65d0682e5a54b92c0ff61f461fd93381f37",
            "@type": "ScholarlyArticle",
            "paperId": "1fe7e65d0682e5a54b92c0ff61f461fd93381f37",
            "corpusId": 9828797,
            "url": "https://www.semanticscholar.org/paper/1fe7e65d0682e5a54b92c0ff61f461fd93381f37",
            "title": "Genetic Reinforcement Learning for Neurocontrol Problems",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2004,
            "externalIds": {
                "DOI": "10.1023/A:1022674030396",
                "CorpusId": 9828797
            },
            "abstract": null,
            "referenceCount": 42,
            "citationCount": 109,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1022674030396.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Whitley2004GeneticRL,\n author = {L. D. Whitley and Stephen Dominic and Rajarshi Das and C. Anderson},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {259-284},\n title = {Genetic Reinforcement Learning for Neurocontrol Problems},\n volume = {13},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f872ae07c74a2cc274c897d01a47f09912a68d8f",
            "@type": "ScholarlyArticle",
            "paperId": "f872ae07c74a2cc274c897d01a47f09912a68d8f",
            "corpusId": 1335417,
            "url": "https://www.semanticscholar.org/paper/f872ae07c74a2cc274c897d01a47f09912a68d8f",
            "title": "Resource allocation in the grid using reinforcement learning",
            "venue": "Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "conf/atal/GalstyanCL04",
                "MAG": "2129095758",
                "DOI": "10.1109/AAMAS.2004.232",
                "CorpusId": 1335417
            },
            "abstract": "In this paper we study a minimalist decentralized algorithm for resource allocation in a simplified Grid-like environment. We consider a system consisting of large number of heterogenous reinforcement learning agents that share common resources for their computational needs. There is no communication between the agents: the only information that agents receive is the (expected) completion time of a job it submitted to a particular resource and which serves as a reinforcement signal for the agent. The results of our experiments suggest that reinforcement learning can be used to improve the quality of resource allocation in large scale heterogenous system.",
            "referenceCount": 29,
            "citationCount": 132,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-07-19",
            "journal": {
                "name": "Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004.",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Galstyan2004ResourceAI,\n author = {A. Galstyan and K. Czajkowski and Kristina Lerman},\n booktitle = {Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004.},\n journal = {Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004.},\n pages = {1314-1315},\n title = {Resource allocation in the grid using reinforcement learning},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3bd75be0641828fbf8364cee4ef791e7d8609708",
            "@type": "ScholarlyArticle",
            "paperId": "3bd75be0641828fbf8364cee4ef791e7d8609708",
            "corpusId": 64232782,
            "url": "https://www.semanticscholar.org/paper/3bd75be0641828fbf8364cee4ef791e7d8609708",
            "title": "Research on Reinforcement Learning Technology: A Review",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2380961559",
                "CorpusId": 64232782
            },
            "abstract": "Reinforcement learning gets optimal policy through trial-and-error and interaction with dynamic environment. Its properties of self-improving and online learning make reinforcement learning become one of most important machine learning methods. In this paper, we firstly survey the foundation, structure and algorithms of reinforcement learning. We also discuss the exploration oriented algorithms and the exploitation oriented algorithms in Markov and non-Markov surroundings. Then we deeply discuss some key concepts of reinforcement learning, including partially observable environment, function approximation, multi-agent reinforcement learning and rule extraction from reinforcement learning. Finally, we briefly introduce some applications of reinforcement leaning and point out some directions of reinforcement learning.",
            "referenceCount": 0,
            "citationCount": 67,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Acta Automatica Sinica",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Shi2004ResearchOR,\n author = {C. Shi},\n journal = {Acta Automatica Sinica},\n title = {Research on Reinforcement Learning Technology: A Review},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e514e7e57a6c7b912a062e0a8756dc3c060bbe0c",
            "@type": "ScholarlyArticle",
            "paperId": "e514e7e57a6c7b912a062e0a8756dc3c060bbe0c",
            "corpusId": 5303949,
            "url": "https://www.semanticscholar.org/paper/e514e7e57a6c7b912a062e0a8756dc3c060bbe0c",
            "title": "Relational Reinforcement Learning: An Overview",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2521075165",
                "CorpusId": 5303949
            },
            "abstract": "Relational reinforcement learning RRL is both a young and an old eld In this pa per we trace the history of the eld to re lated disciplines outline some current work and promising new directions and survey the research issues and opportunities that lie ahead",
            "referenceCount": 35,
            "citationCount": 123,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Tadepalli2004RelationalRL,\n author = {Prasad Tadepalli and R. Givan and K. Driessens},\n booktitle = {International Conference on Machine Learning},\n pages = {1-9},\n title = {Relational Reinforcement Learning: An Overview},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e1f9cb45bd8f0300a013a3da3e15546bfeeb60d0",
            "@type": "ScholarlyArticle",
            "paperId": "e1f9cb45bd8f0300a013a3da3e15546bfeeb60d0",
            "corpusId": 10282145,
            "url": "https://www.semanticscholar.org/paper/e1f9cb45bd8f0300a013a3da3e15546bfeeb60d0",
            "title": "Reinforcement Learning as Classification: Leveraging Modern Classifiers",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "conf/icml/LagoudakisP03",
                "MAG": "2134289401",
                "CorpusId": 10282145
            },
            "abstract": "The basic tools of machine learning appear in the inner loop of most reinforcement learning algorithms, typically in the form of Monte Carlo methods or function approximation techniques. To a large extent, however, current reinforcement learning algorithms draw upon machine learning techniques that are at least ten years old and, with a few exceptions, very little has been done to exploit recent advances in classification learning for the purposes of reinforcement learning. We use a variant of approximate policy iteration based on rollouts that allows us to use a pure classification learner, such as a support vector machine (SVM), in the inner loop of the algorithm. We argue that the use of SVMs, particularly in combination with the kernel trick, can make it easier to apply reinforcement learning as an \"out-of-the-box\" technique, without extensive feature engineering. Our approach opens the door to modern classification methods, but does not preclude the use of classical methods. We present experimental results in the pendulum balancing and bicycle riding domains using both SVMs and neural networks for classifiers.",
            "referenceCount": 19,
            "citationCount": 174,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2003-08-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lagoudakis2003ReinforcementLA,\n author = {M. Lagoudakis and Ronald E. Parr},\n booktitle = {International Conference on Machine Learning},\n pages = {424-431},\n title = {Reinforcement Learning as Classification: Leveraging Modern Classifiers},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b8b6b33f750b93b3fb0b90ef219f7e0d0acc66aa",
            "@type": "ScholarlyArticle",
            "paperId": "b8b6b33f750b93b3fb0b90ef219f7e0d0acc66aa",
            "corpusId": 6581656,
            "url": "https://www.semanticscholar.org/paper/b8b6b33f750b93b3fb0b90ef219f7e0d0acc66aa",
            "title": "Principled Methods for Advising Reinforcement Learning Agents",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "conf/icml/WiewioraCE03",
                "MAG": "2164419340",
                "CorpusId": 6581656
            },
            "abstract": "An important issue in reinforcement learning is how to incorporate expert knowledge in a principled manner, especially as we scale up to real-world tasks. In this paper, we present a method for incorporating arbitrary advice into the reward structure of a reinforcement learning agent without altering the optimal policy. This method extends the potential-based shaping method proposed by Ng et al. (1999) to the case of shaping functions based on both states and actions. This allows for much more specific information to guide the agent - which action to choose - without requiring the agent to discover this from the rewards on states alone. We develop two qualitatively different methods for converting a potential function into advice for the agent. We also provide theoretical and experimental justifications for choosing between these advice-giving algorithms based on the properties of the potential function.",
            "referenceCount": 13,
            "citationCount": 178,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2003-08-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wiewiora2003PrincipledMF,\n author = {Eric Wiewiora and G. Cottrell and C. Elkan},\n booktitle = {International Conference on Machine Learning},\n pages = {792-799},\n title = {Principled Methods for Advising Reinforcement Learning Agents},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1c0f7087367315e4e8cd1d8654ab33db12663c2b",
            "@type": "ScholarlyArticle",
            "paperId": "1c0f7087367315e4e8cd1d8654ab33db12663c2b",
            "corpusId": 10243780,
            "url": "https://www.semanticscholar.org/paper/1c0f7087367315e4e8cd1d8654ab33db12663c2b",
            "title": "Lyapunov Design for Safe Reinforcement Learning",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2164479831",
                "DBLP": "journals/jmlr/PerkinsB02",
                "CorpusId": 10243780
            },
            "abstract": "Lyapunov design methods are used widely in control engineering to design controllers that achieve qualitative objectives, such as stabilizing a system or maintaining a system's state in a desired operating range. We propose a method for constructing safe, reliable reinforcement learning agents based on Lyapunov design principles. In our approach, an agent learns to control a system by switching among a number of given, base-level controllers. These controllers are designed using Lyapunov domain knowledge so that any switching policy is safe and enjoys basic performance guarantees. Our approach thus ensures qualitatively satisfactory agent behavior for virtually any reinforcement learning algorithm and at all times, including while the agent is learning and taking exploratory actions. We demonstrate the process of designing safe agents for four different control problems. In simulation experiments, we find that our theoretically motivated designs also enjoy a number of practical benefits, including reasonable performance initially and throughout learning, and accelerated learning.",
            "referenceCount": 41,
            "citationCount": 177,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2003-03-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Perkins2003LyapunovDF,\n author = {T. Perkins and A. Barto},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {803-832},\n title = {Lyapunov Design for Safe Reinforcement Learning},\n volume = {3},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fa2c122933a1473692c584d9896fc8e1f7b671a0",
            "@type": "ScholarlyArticle",
            "paperId": "fa2c122933a1473692c584d9896fc8e1f7b671a0",
            "corpusId": 5376984,
            "url": "https://www.semanticscholar.org/paper/fa2c122933a1473692c584d9896fc8e1f7b671a0",
            "title": "Q-Decomposition for Reinforcement Learning Agents",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "conf/icml/RussellZ03",
                "MAG": "2136202932",
                "CorpusId": 5376984
            },
            "abstract": "The paper explores a very simple agent design method called Q-decomposition, wherein a complex agent is built from simpler subagents. Each subagent has its own reward function and runs its own reinforcement learning process. It supplies to a central arbitrator the Q-values (according to its own reward function) for each possible action. The arbitrator selects an action maximizing the sum of Q-values from all the subagents. This approach has advantages over designs in which subagents recommend actions. It also has the property that if each subagent runs the Sarsa reinforcement learning algorithm to learn its local Q-function, then a globally optimal policy is achieved. (On the other hand, local Q-learning leads to globally suboptimal behavior.) In some cases, this form of agent decomposition allows the local Q-functions to be expressed by much-reduced state and action spaces. These results are illustrated in two domains that require effective coordination of behaviors.",
            "referenceCount": 25,
            "citationCount": 164,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2003-08-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Russell2003QDecompositionFR,\n author = {Stuart J. Russell and A. Zimdars},\n booktitle = {International Conference on Machine Learning},\n pages = {656-663},\n title = {Q-Decomposition for Reinforcement Learning Agents},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b1d8f8625cabf4275561e7d430919c1a94c56efa",
            "@type": "ScholarlyArticle",
            "paperId": "b1d8f8625cabf4275561e7d430919c1a94c56efa",
            "corpusId": 5328902,
            "url": "https://www.semanticscholar.org/paper/b1d8f8625cabf4275561e7d430919c1a94c56efa",
            "title": "Active Perception and Reinforcement Learning",
            "venue": "Neural Computation",
            "publicationVenue": {
                "id": "urn:research:69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                "name": "Neural Computation",
                "alternate_names": [
                    "Neural Comput"
                ],
                "issn": "0899-7667",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667"
            },
            "year": 1990,
            "externalIds": {
                "DBLP": "conf/icml/WhiteheadB90",
                "MAG": "1500024457",
                "DOI": "10.1162/neco.1990.2.4.409",
                "CorpusId": 5328902
            },
            "abstract": "This paper considers adaptive control architectures that integrate active sensorimotor systems with decision systems based on reinforcement learning. One unavoidable consequence of active perception is that the agent's internal representation often confounds external world states. We call this phenomenon perceptual aliasing and show that it destabilizes existing reinforcement learning algorithms with respect to the optimal decision policy. A new decision system that overcomes these difficulties is described. The system incorporates a perceptual subcycle within the overall decision cycle and uses a modified learning algorithm to suppress the effects of perceptual aliasing. The result is a control architecture that learns not only how to solve a task but also where to focus its attention in order to collect necessary sensory information.",
            "referenceCount": 27,
            "citationCount": 217,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1990-06-01",
            "journal": {
                "name": "Neural Computation",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Whitehead1990ActivePA,\n author = {S. Whitehead and D. Ballard},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {409-419},\n title = {Active Perception and Reinforcement Learning},\n volume = {2},\n year = {1990}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:165f4a4bc8080093c89b8e9916b779aee8b37b81",
            "@type": "ScholarlyArticle",
            "paperId": "165f4a4bc8080093c89b8e9916b779aee8b37b81",
            "corpusId": 29789746,
            "url": "https://www.semanticscholar.org/paper/165f4a4bc8080093c89b8e9916b779aee8b37b81",
            "title": "Risk-Sensitive Reinforcement Learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1585575029",
                "DBLP": "journals/ml/MihatschN02",
                "DOI": "10.1023/A:1017940631555",
                "CorpusId": 29789746
            },
            "abstract": null,
            "referenceCount": 23,
            "citationCount": 134,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1017940631555.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1998-12-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "49"
            },
            "citationStyles": {
                "bibtex": "@Article{Mihatsch1998RiskSensitiveRL,\n author = {O. Mihatsch and R. Neuneier},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {267-290},\n title = {Risk-Sensitive Reinforcement Learning},\n volume = {49},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a05eaa8ebed3ad39d99ee4d90910afc196e05d9f",
            "@type": "ScholarlyArticle",
            "paperId": "a05eaa8ebed3ad39d99ee4d90910afc196e05d9f",
            "corpusId": 1588520,
            "url": "https://www.semanticscholar.org/paper/a05eaa8ebed3ad39d99ee4d90910afc196e05d9f",
            "title": "Implicit Imitation in Multiagent Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1999,
            "externalIds": {
                "DBLP": "conf/icml/PriceB99",
                "MAG": "1594602740",
                "CorpusId": 1588520
            },
            "abstract": "Imitation is actively being studied as an effective means of learning in multi-agent environments. It allows an agent to learn how to act well (perhaps optimally) by passively observing the actions of cooperative teachers or other more experienced agents its environment. We propose a straightforward imitation mechanism called model extraction that can be integrated easily into standard model-based reinforcement learning algorithms. Roughly, by observing a mentor with similar capabilities, an agent can extract information about its own capabilities in unvisited parts of state space. The extracted information can accelerate learning dramatically. We illustrate the benefits of model extraction by integrating it with prioritized sweeping, and demonstrating improved performance and convergence through observation of single and multiple mentors. Though we make some stringent assumptions regarding observability, possible interactions and common abilities, we briefly comment on extensions of the model that relax these.",
            "referenceCount": 21,
            "citationCount": 79,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1999-06-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Price1999ImplicitII,\n author = {Bob Price and Craig Boutilier},\n booktitle = {International Conference on Machine Learning},\n pages = {325-334},\n title = {Implicit Imitation in Multiagent Reinforcement Learning},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:11463e2a6ed218e87e22cba2c2f24fb5992d0293",
            "@type": "ScholarlyArticle",
            "paperId": "11463e2a6ed218e87e22cba2c2f24fb5992d0293",
            "corpusId": 18649966,
            "url": "https://www.semanticscholar.org/paper/11463e2a6ed218e87e22cba2c2f24fb5992d0293",
            "title": "Learning and problem-solving with multilayer connectionist systems (adaptive, strategy learning, neural networks, reinforcement learning)",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1986,
            "externalIds": {
                "MAG": "1686514609",
                "CorpusId": 18649966
            },
            "abstract": "THE DIFFICULTIES OF LEARNING IN MULTILAYERED NETWORKS OF COMPUTATIONAL UNITS HAS LIMITED THE USE OF CONNECTIONIST SYSTEMS IN COMPLEX DOMAINS. THIS DISSERTATION ELUCIDATES THE ISSUES OF LEARNING IN A NETWORK''S HIDDEN UNITS, AND REVIEWS METHODS FOR ADDRESSING THESE ISSUES THAT HAVE BEEN DEVELOPED THROUGH THE YEARS. ISSUES OF LEARNING IN HIDDEN UNITS ARE SHOWN TO BE ANALOGOUS TO LEARNING ISSUES FOR MULTILAYER SYSTEMS EMPLOYING SYMBOLIC REPRSENTATIONS. COMPARISONS OF A NUMBER OF ALGORITHMS FOR LEARNING IN HIDDEN UNITS ARE MADE BY APPLYING THEM IN A CONSISTENT MANNER TO SEVERAL TASKS. RECENTLY DEVELOPED ALGORITHMS, INCLUDING RUMELHART, ET AL''S, ERROR BACK-PROPOGATIONS ALGORITHM AND BARTO, ET AL''S, REINFORCEMENT-LEARNING ALGORITHMS, LEARN THE SOLUTIONS TO THE TASKS MUCH MORE SUCCESSFULLY THAN METHODS OF THE PAST. A NOVEL ALGORITHM IS EXAMINED THAT COMBINES ASPECTS OF REINFORCEMENT LEARNING AND A DATA-DIRECTED SEARCH FOR USEFUL WEIGHTS, AND IS SHOWN TO OUT PERFORM REINFORMCEMENT-LEARNING ALGORITHMS. A CONNECTIONIST FRAMEWORK FOR THE LEARNING OF STRATEGIES IS DESCRIBED WHICH COMBINES THE ERROR BACK-PROPOGATION ALGORITHM FOR LEARNING IN HIDDEN UNITS WITH SUTTON''S AHC ALGORITHM TO LEARN EVALUATION FUNCTIONS AND WITH A REINFORCEMENT-LEARNING ALGORITHM TO LEARN SEARCH HEURISTICS. THE GENERAL- ITY OF THIS HYBRID SYSTEM IS DEMONSTRATED THROUGH SUCCESSFUL APPLICATIONS",
            "referenceCount": 85,
            "citationCount": 140,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "1986-12-31",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Anderson1986LearningAP,\n author = {C. Anderson},\n title = {Learning and problem-solving with multilayer connectionist systems (adaptive, strategy learning, neural networks, reinforcement learning)},\n year = {1986}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a30685e07329b43fc8feecf1bc60e5e2323dbe56",
            "@type": "ScholarlyArticle",
            "paperId": "a30685e07329b43fc8feecf1bc60e5e2323dbe56",
            "corpusId": 8205815,
            "url": "https://www.semanticscholar.org/paper/a30685e07329b43fc8feecf1bc60e5e2323dbe56",
            "title": "Multi-criteria Reinforcement Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "CorpusId": 8205815
            },
            "abstract": "We consider multi-criteria sequential decision making problems where the vector-valued evaluations are compared by a given, xed total ordering. Conditions for the optimality of stationary policies and the Bellman optimality equation are given. The analysis requires special care as the topology introduced by pointwise convergence and the order-topology introduced by the preference order are in general incompatible. Reinforcement learning algorithms are proposed and analyzed. Preliminary computer experiments connrm the validity of the derived algorithms. It is observed that in the medium-term multi-criteria RL often converges to better solutions (measured by the rst criterion) than their single-criterion counterparts. These type of multi-criteria problems are most useful when there are several optimal solutions to a problem and one wants to choose the one among these which is optimal according to another xed criterion. Example applications include alternating games, when in addition to maximizing the probability of win, the decision maker also minimizes the expected number of steps to win in states when it is possible to win, while in states when it is not possible to win it tries to play for time. Another application comes from specifying desired behaviors for robots in terms of a list of ordered, parallel sub-goals, e.g., a football playing robot's primary goal could be to shoot goals, while it's subordinate, parallel goal could be to keep clear of opponents as much as possible.",
            "referenceCount": 22,
            "citationCount": 171,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Thege1998MulticriteriaRL,\n author = {Konkoly Thege},\n title = {Multi-criteria Reinforcement Learning},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e41cda7b81cc49640210173fd45eb06cdbd6e824",
            "@type": "ScholarlyArticle",
            "paperId": "e41cda7b81cc49640210173fd45eb06cdbd6e824",
            "corpusId": 9569834,
            "url": "https://www.semanticscholar.org/paper/e41cda7b81cc49640210173fd45eb06cdbd6e824",
            "title": "Finding Structure in Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1994,
            "externalIds": {
                "MAG": "2114451917",
                "DBLP": "conf/nips/ThrunS94",
                "CorpusId": 9569834
            },
            "abstract": "Reinforcement learning addresses the problem of learning to select actions in order to maximize one's performance in unknown environments. To scale reinforcement learning to complex real-world tasks, such as typically studied in AI, one must ultimately be able to discover the structure in the world, in order to abstract away the myriad of details and to operate in more tractable problem spaces. \n \nThis paper presents the SKILLS algorithm. SKILLS discovers skills, which are partially defined action policies that arise in the context of multiple, related tasks. Skills collapse whole action sequences into single operators. They are learned by minimizing the compactness of action policies, using a description length argument on their representation. Empirical results in simple grid navigation tasks illustrate the successful discovery of structure in reinforcement learning.",
            "referenceCount": 12,
            "citationCount": 245,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Thrun1994FindingSI,\n author = {S. Thrun and Anton Schwartz},\n booktitle = {Neural Information Processing Systems},\n pages = {385-392},\n title = {Finding Structure in Reinforcement Learning},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c9404262274a05987366a5b9085d14a66266f4f4",
            "@type": "ScholarlyArticle",
            "paperId": "c9404262274a05987366a5b9085d14a66266f4f4",
            "corpusId": 10156886,
            "url": "https://www.semanticscholar.org/paper/c9404262274a05987366a5b9085d14a66266f4f4",
            "title": "Introduction: The challenge of reinforcement learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1992,
            "externalIds": {
                "MAG": "2031745992",
                "DOI": "10.1007/BF00992695",
                "CorpusId": 10156886
            },
            "abstract": null,
            "referenceCount": 21,
            "citationCount": 140,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/BF00992695.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1992-05-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Sutton1992IntroductionTC,\n author = {R. Sutton},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {225-227},\n title = {Introduction: The challenge of reinforcement learning},\n volume = {8},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dcbef60688ca014a63e72e844fb997e738f6fde7",
            "@type": "ScholarlyArticle",
            "paperId": "dcbef60688ca014a63e72e844fb997e738f6fde7",
            "corpusId": 106845402,
            "url": "https://www.semanticscholar.org/paper/dcbef60688ca014a63e72e844fb997e738f6fde7",
            "title": "Efficient Exploration In Reinforcement Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1992,
            "externalIds": {
                "MAG": "1504212531",
                "CorpusId": 106845402
            },
            "abstract": "Exploration plays a fundamental role in any active learning system. This study evaluates the role of exploration in active learning and describes several local techniques for exploration in finite, discrete domains, embedded in a reinforcement learning framework (delayed reinforcement). This paper distinguishes between two families of exploration schemes: undirected and directed exploration. While the former family is closely related to random walk exploration, directed exploration techniques memorize exploration-specific knowledge which is used for guiding the exploration search. In many finite deterministic domains, any learning technique based on undirected exploration is inefficient in terms of learning time, i.e., learning time is expected to scale exponentially with the size of the state space. We prove that for all these domains, reinforcement learning using a directed technique can always be performed in polynomial time, demonstrating the important role of exploration in reinforcement learning. (The proof is given for one specific directed exploration technique named counter-based exploration.) Subsequently, several exploration techniques found in recent reinforcement learning and connectionist adaptive control literature are described. In order to trade off efficiently between exploration and exploitation --- a trade-off which characterizes many real-world active learning tasks --- combination methods are described which explore and avoid costs simultaneously. This includes a selective attention mechanism, which allows smooth switching between exploration and exploitation. All techniques are evaluated and compared on a discrete reinforcement learning task (robot navigation). The empirical evaluation is followed by an extensive discussion of benefits and limitations of this work.",
            "referenceCount": 0,
            "citationCount": 151,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Thrun1992EfficientEI,\n author = {S. Thrun},\n title = {Efficient Exploration In Reinforcement Learning},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:26bdf4aad6013b8854f798a59124857b8e1e033d",
            "@type": "ScholarlyArticle",
            "paperId": "26bdf4aad6013b8854f798a59124857b8e1e033d",
            "corpusId": 590390,
            "url": "https://www.semanticscholar.org/paper/26bdf4aad6013b8854f798a59124857b8e1e033d",
            "title": "GA-based fuzzy reinforcement learning for control of a magnetic bearing system",
            "venue": "IEEE Trans. Syst. Man Cybern. Part B",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2139884824",
                "DBLP": "journals/tsmc/LinJ00",
                "DOI": "10.1109/3477.836376",
                "CorpusId": 590390,
                "PubMed": "18244754"
            },
            "abstract": "This paper proposes a TD (temporal difference) and GA (genetic algorithm)-based reinforcement (TDGAR) learning method and applies it to the control of a real magnetic bearing system. The TDGAR learning scheme is a new hybrid GA, which integrates the TD prediction method and the GA to perform the reinforcement learning task. The TDGAR learning system is composed of two integrated feedforward networks. One neural network acts as a critic network to guide the learning of the other network (the action network) which determines the outputs (actions) of the TDGAR learning system. The action network can be a normal neural network or a neural fuzzy network. Using the TD prediction method, the critic network can predict the external reinforcement signal and provide a more informative internal reinforcement signal to the action network. The action network uses the GA to adapt itself according to the internal reinforcement signal. The key concept of the TDGAR learning scheme is to formulate the internal reinforcement signal as the fitness function for the GA such that the GA can evaluate the candidate solutions (chromosomes) regularly, even during periods without external feedback from the environment. This enables the GA to proceed to new generations regularly without waiting for the arrival of the external reinforcement signal. This can usually accelerate the GA learning since a reinforcement signal may only be available at a time long after a sequence of actions has occurred in the reinforcement learning problem. The proposed TDGAR learning system has been used to control an active magnetic bearing (AMB) system in practice. A systematic design procedure is developed to achieve successful integration of all the subsystems including magnetic suspension, mechanical structure, and controller training. The results show that the TDGAR learning scheme can successfully find a neural controller or a neural fuzzy controller for a self-designed magnetic bearing system.",
            "referenceCount": 56,
            "citationCount": 101,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2000-04-01",
            "journal": {
                "name": "IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society",
                "volume": "30 2"
            },
            "citationStyles": {
                "bibtex": "@Article{Lin2000GAbasedFR,\n author = {Chin-Teng Lin and Chong-Ping Jou},\n booktitle = {IEEE Trans. Syst. Man Cybern. Part B},\n journal = {IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society},\n pages = {\n          276-89\n        },\n title = {GA-based fuzzy reinforcement learning for control of a magnetic bearing system},\n volume = {30 2},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5312b96a62d4f942e3896521bdf6d8c8cc8b50ad",
            "@type": "ScholarlyArticle",
            "paperId": "5312b96a62d4f942e3896521bdf6d8c8cc8b50ad",
            "corpusId": 60653738,
            "url": "https://www.semanticscholar.org/paper/5312b96a62d4f942e3896521bdf6d8c8cc8b50ad",
            "title": "Reinforcement Learning: A Tutorial.",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1997,
            "externalIds": {
                "MAG": "1497347107",
                "DOI": "10.21236/ada323194",
                "CorpusId": 60653738
            },
            "abstract": "Abstract : The purpose of this tutorial is to provide an introduction to reinforcement learning (RL) at a level easily understood by students and researchers in a wide range of disciplines. The intent is not to present a rigorous mathematical discussion that requires a great deal of effort on the part of the reader, but rather to present a conceptual framework that might serve as an introduction to a more rigorous study of RL. The fundamental principles and techniques used to solve RL problems are presented. The most popular RL algorithms are presented. Section (1) presents an overview of RL and provides a simple example to develop intuition of the underlying dynamic programming mechanism. In Section (2) the parts of a reinforcement learning problem are discussed. These include the environment, reinforcement function, and value function. Section (3) gives a description of the most widely used reinforcement learning algorithms. These include TD(lambda) and both the residual and direct forms of value iteration, Q-learning, and advantage learning. In Section (4) some of the ancillary issues of RL are briefly discussed, such as choosing an exploration strategy and a discount factor. The conclusion is given in Section (5). Finally, Section (6) is a glossary of commonly used terms followed by references and bibliography.",
            "referenceCount": 18,
            "citationCount": 181,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.dtic.mil/dtic/tr/fulltext/u2/a323194.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Harmon1997ReinforcementLA,\n author = {M. Harmon and Stephanie S. Harmon},\n title = {Reinforcement Learning: A Tutorial.},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bb09f2cd352865f8ee082d39c1e5583a5d208445",
            "@type": "ScholarlyArticle",
            "paperId": "bb09f2cd352865f8ee082d39c1e5583a5d208445",
            "corpusId": 15139976,
            "url": "https://www.semanticscholar.org/paper/bb09f2cd352865f8ee082d39c1e5583a5d208445",
            "title": "Relational Reinforcement Learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2521833123",
                "DBLP": "reference/ml/Driessens10",
                "DOI": "10.1023/A:1007694015589",
                "CorpusId": 15139976
            },
            "abstract": null,
            "referenceCount": 41,
            "citationCount": 136,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1007694015589.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1998-07-24",
            "journal": {
                "name": "Machine Learning",
                "volume": "43"
            },
            "citationStyles": {
                "bibtex": "@Article{Driessens1998RelationalRL,\n author = {K. Driessens},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {7-52},\n title = {Relational Reinforcement Learning},\n volume = {43},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b5cc2b2829a1fcf0260a1405f5f931efb21ffd5a",
            "@type": "ScholarlyArticle",
            "paperId": "b5cc2b2829a1fcf0260a1405f5f931efb21ffd5a",
            "corpusId": 5972929,
            "url": "https://www.semanticscholar.org/paper/b5cc2b2829a1fcf0260a1405f5f931efb21ffd5a",
            "title": "Reinforcement Learning with a Hierarchy of Abstract Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 1992,
            "externalIds": {
                "MAG": "16046748",
                "DBLP": "conf/aaai/Singh92",
                "CorpusId": 5972929
            },
            "abstract": "Reinforcement learning (RL) algorithms have traditionally been thought of as trial and error learning methods that use actual control experience to incrementally improve a control policy. Sutton's DYNA architecture demonstrated that RL algorithms can work as well using simulated experience from an environment model, and that the resulting computation was similar to doing one-step lookahead planning. Inspired by the literature on hierarchical planning, I propose learning a hierarchy of models of the environment that abstract temporal detail as a means of improving the scalability of RL algorithms. I present H-DYNA (Hierarchical DYNA), an extension to Sutton's DYNA architecture that is able to learn such a hierarchy of abstract models. H-DYNA differs from hierarchical planners in two ways: first, the abstract models are learned using experience gained while learning to solve other tasks in the same environment, and second, the abstract models can be used to solve stochastic control tasks. Simulations on a set of compositionally-structured navigation tasks show that H-DYNA can learn to solve them faster than conventional RL algorithms. The abstract models also serve as mechanisms for achieving transfer of learning across multiple tasks.",
            "referenceCount": 15,
            "citationCount": 143,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1992-07-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Singh1992ReinforcementLW,\n author = {Satinder Singh},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {202-207},\n title = {Reinforcement Learning with a Hierarchy of Abstract Models},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7888be55fb513c20919f94f0274bd9b78bda44b6",
            "@type": "ScholarlyArticle",
            "paperId": "7888be55fb513c20919f94f0274bd9b78bda44b6",
            "corpusId": 10757799,
            "url": "https://www.semanticscholar.org/paper/7888be55fb513c20919f94f0274bd9b78bda44b6",
            "title": "A Unified Analysis of Value-Function-Based Reinforcement-Learning Algorithms",
            "venue": "Neural Computation",
            "publicationVenue": {
                "id": "urn:research:69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                "name": "Neural Computation",
                "alternate_names": [
                    "Neural Comput"
                ],
                "issn": "0899-7667",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667"
            },
            "year": 1999,
            "externalIds": {
                "DBLP": "journals/neco/SzepesvariL99",
                "MAG": "2089415692",
                "DOI": "10.1162/089976699300016070",
                "CorpusId": 10757799,
                "PubMed": "10578043"
            },
            "abstract": "Reinforcement learning is the problem of generating optimal behavior in a sequential decision-making environment given the opportunity of interacting with it. Many algorithms for solving reinforcement-learning problems work by computing improved estimates of the optimal value function. We extend prior analyses of reinforcement-learning algorithms and present a powerful new theorem that can provide a unified analysis of such value-function-based reinforcement-learning algorithms. The usefulness of the theorem lies in how it allows the convergence of a complex asynchronous reinforcement-learning algorithm to be proved by verifying that a simpler synchronous algorithm converges. We illustrate the application of the theorem by analyzing the convergence of Q-learning, model-based reinforcement learning, Q-learning with multistate updates, Q-learning for Markov games, and risk-sensitive reinforcement learning.",
            "referenceCount": 53,
            "citationCount": 183,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.ualberta.ca/%7Eszepesva/papers/nc-97-gmdp.ps.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1999-11-01",
            "journal": {
                "name": "Neural Computation",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Szepesvari1999AUA,\n author = {Csaba Szepesvari and M. Littman},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {2017-2060},\n title = {A Unified Analysis of Value-Function-Based Reinforcement-Learning Algorithms},\n volume = {11},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f3d4ce37a4016807ebb18903003f007b61894f5a",
            "@type": "ScholarlyArticle",
            "paperId": "f3d4ce37a4016807ebb18903003f007b61894f5a",
            "corpusId": 14043536,
            "url": "https://www.semanticscholar.org/paper/f3d4ce37a4016807ebb18903003f007b61894f5a",
            "title": "Stock price prediction using reinforcement learning",
            "venue": "ISIE 2001. 2001 IEEE International Symposium on Industrial Electronics Proceedings (Cat. No.01TH8570)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "MAG": "1491302875",
                "DOI": "10.1109/ISIE.2001.931880",
                "CorpusId": 14043536
            },
            "abstract": "Recently, numerous investigations for stock price prediction and portfolio management using machine learning have been trying to develop efficient mechanical trading systems. But these systems have a limitation in that they are mainly based on the supervised learning which is not so adequate for learning problems with long-term goals and delayed rewards. This paper proposes a method of applying reinforcement learning, suitable for modeling and learning various kinds of interactions in real situations, to the problem of stock price prediction. The stock price prediction problem is considered as Markov process which can be optimized by reinforcement learning based algorithm. TD(0), a reinforcement learning algorithm which learns only from experiences, is adopted and function approximation by an artificial neural network is performed to learn the values of states each of which corresponds to a stock price trend at a given time. An experimental result based on the Korean stock market is presented to evaluate the performance of the proposed method.",
            "referenceCount": 10,
            "citationCount": 97,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2001-06-12",
            "journal": {
                "name": "ISIE 2001. 2001 IEEE International Symposium on Industrial Electronics Proceedings (Cat. No.01TH8570)",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Lee2001StockPP,\n author = {Jae Won Lee},\n booktitle = {ISIE 2001. 2001 IEEE International Symposium on Industrial Electronics Proceedings (Cat. No.01TH8570)},\n journal = {ISIE 2001. 2001 IEEE International Symposium on Industrial Electronics Proceedings (Cat. No.01TH8570)},\n pages = {690-695 vol.1},\n title = {Stock price prediction using reinforcement learning},\n volume = {1},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f90bb9b59b64ded0c98b909deb949acc1d9c71df",
            "@type": "ScholarlyArticle",
            "paperId": "f90bb9b59b64ded0c98b909deb949acc1d9c71df",
            "corpusId": 13287928,
            "url": "https://www.semanticscholar.org/paper/f90bb9b59b64ded0c98b909deb949acc1d9c71df",
            "title": "Complexity Analysis of Real-Time Reinforcement Learning",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 1992,
            "externalIds": {
                "MAG": "1512774646",
                "DBLP": "conf/aaai/KoenigS93",
                "CorpusId": 13287928
            },
            "abstract": "This paper analyzes the complexity of on-line reinforcement learning algorithms, namely asynchronous realtime versions of Q-learning and value-iteration, applied to the problem of reaching a goal state in deterministic domains. Previous work had concluded that, in many cases, tabula rasa reinforcement learning was exponential for such problems, or was tractable only if the learning algorithm was augmented. We show that, to the contrary, the algorithms are tractable with only a simple change in the task representation or initialization. We provide tight bounds on the worst-case complexity, and show how the complexity is even smaller if the reinforcement learning algorithms have initial knowledge of the topology of the state space or the domain has certain special properties. We also present a novel bidirectional Q-learning algorithm to find optimal paths from all states to a goal state and show that it is no more complex than the other algorithms.",
            "referenceCount": 44,
            "citationCount": 199,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1992-12-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Koenig1992ComplexityAO,\n author = {Sven Koenig and R. Simmons},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {99-107},\n title = {Complexity Analysis of Real-Time Reinforcement Learning},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1655cde00456867e6f12de9952fe3a78170fe7bb",
            "@type": "ScholarlyArticle",
            "paperId": "1655cde00456867e6f12de9952fe3a78170fe7bb",
            "corpusId": 13257886,
            "url": "https://www.semanticscholar.org/paper/1655cde00456867e6f12de9952fe3a78170fe7bb",
            "title": "Decision Tree Function Approximation in Reinforcement Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1783866091",
                "CorpusId": 13257886
            },
            "abstract": "The goal in reinforcement learning is to learn the value of taking each action from each possible state in order to maximize the total reward. In scaling reinforcement learning to problems with large numbers of states and/or actions, the representation of the value function becomes critical. We present a decision tree based approach to function approximation in reinforcement learning. We compare our approach with table lookup and a neural network function approximator on three problems: the well known mountain car and pole balance problems as well as a simulated automobile race car. We find that the decision tree can provide better learning performance than the neural network function approximation and can solve large problems that are infeasible using table lookup.",
            "referenceCount": 16,
            "citationCount": 115,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Pyeatt1999DecisionTF,\n author = {Larry D. Pyeatt and A. Howe},\n title = {Decision Tree Function Approximation in Reinforcement Learning},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:de71874b6724506a3370c256da1e9ad8c801051d",
            "@type": "ScholarlyArticle",
            "paperId": "de71874b6724506a3370c256da1e9ad8c801051d",
            "corpusId": 5842616,
            "url": "https://www.semanticscholar.org/paper/de71874b6724506a3370c256da1e9ad8c801051d",
            "title": "Evolution of Reinforcement Learning in Uncertain Environments: A Simple Explanation for Complex Foraging Behaviors",
            "venue": "Adaptive Behavior",
            "publicationVenue": {
                "id": "urn:research:f4f9e4d0-b477-46a6-bd60-07475fc3a0bd",
                "name": "Adaptive Behavior",
                "alternate_names": [
                    "Adapt Behav"
                ],
                "issn": "1059-7123",
                "url": "http://www.sagepub.com/journals/Journal201570/title"
            },
            "year": 2002,
            "externalIds": {
                "DBLP": "journals/adb/NivJMR02",
                "MAG": "2044724603",
                "DOI": "10.1177/1059-712302-010001-01",
                "CorpusId": 5842616
            },
            "abstract": "Reinforcement learning is a fundamental process by which organisms learn to achieve goals from their interactions with the environment. Using evolutionary computation techniques we evolve (near-)optimal neuronal learning rules in a simple neural network model of reinforcement learning in bumblebees foraging for nectar. The resulting neural networks exhibit efficient reinforcement learning, allowing the bees to respond rapidly to changes in reward contingencies. The evolved synaptic plasticity dynamics give rise to varying exploration/exploitation levels and to the well-documented choice strategies of risk aversion and probability matching. Additionally, risk aversion is shown to emerge even when bees are evolved in a completely risk-less environment. In contrast to existing theories in economics and game theory, risk-averse behavior is shown to be a direct consequence of (near-)optimal reinforcement learning, without requiring additional assumptions such as the existence of a nonlinear subjective utility function for rewards. Our results are corroborated by a rigorous mathematical analysis, and their robustness in real-world situations is supported by experiments in a mobile robot. Thus we provide a biologically founded, parsimonious, and novel explanation for risk aversion and probability matching.",
            "referenceCount": 56,
            "citationCount": 183,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2002-01-01",
            "journal": {
                "name": null,
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Niv2002EvolutionOR,\n author = {Y. Niv and D. Joel and I. Meilijson and E. Ruppin},\n booktitle = {Adaptive Behavior},\n pages = {24 - 5},\n title = {Evolution of Reinforcement Learning in Uncertain Environments: A Simple Explanation for Complex Foraging Behaviors},\n volume = {10},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1355de09f6cc7ca8ec59f6f5e2f65acc3996b516",
            "@type": "ScholarlyArticle",
            "paperId": "1355de09f6cc7ca8ec59f6f5e2f65acc3996b516",
            "corpusId": 6054391,
            "url": "https://www.semanticscholar.org/paper/1355de09f6cc7ca8ec59f6f5e2f65acc3996b516",
            "title": "An Analysis of Stochastic Game Theory for Multiagent Reinforcement Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2000,
            "externalIds": {
                "MAG": "1835254890",
                "CorpusId": 6054391
            },
            "abstract": "Abstract : Learning behaviors in a multiagent environment are crucial for developing and adapting multiagent systems. Reinforcement learning techniques have addressed this problem for a single agent acting in a stationary environment, which is modeled as a Markov decision process (MDP). But, multiagent environments are inherently non-stationary since the other agents are free to change their behavior as they also learn and adapt. Stochastic games, first studied in the game theory community, are a natural extension of MDPs to include multiple agents. In this paper we contribute a comprehensive presentation of the relevant techniques for solving stochastic games from both the game theory community and reinforcement learning communities. We examine the assumptions and limitations of these algorithms, and identify similarities between these algorithms, single agent reinforcement learners, and basic game theory techniques.",
            "referenceCount": 26,
            "citationCount": 154,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2000-10-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Bowling2000AnAO,\n author = {Michael Bowling and M. Veloso},\n title = {An Analysis of Stochastic Game Theory for Multiagent Reinforcement Learning},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fae8f070c3444918ef625dbaae0d243ce036a123",
            "@type": "ScholarlyArticle",
            "paperId": "fae8f070c3444918ef625dbaae0d243ce036a123",
            "corpusId": 11164962,
            "url": "https://www.semanticscholar.org/paper/fae8f070c3444918ef625dbaae0d243ce036a123",
            "title": "Using Abstract Models of Behaviours to Automatically Generate Reinforcement Learning Hierarchies",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "1855398616",
                "DBLP": "conf/icml/Ryan02",
                "CorpusId": 11164962
            },
            "abstract": "In this paper we present a hybrid system combining techniques from symbolic planning and reinforcement learning. Planning is used to automatically construct task hierarchies for hierarchical reinforcement learning based on abstract models of the behaviours\u2019 purpose, and to perform intelligent termination improvement when an executing behaviour is no longer appropriate. Reinforcement learning is used to produce concrete implementations of abstractly defined behaviours and to learn the best possible choice of behaviour when plans are ambiguous. Two new hierarchical reinforcement learning algorithms are presented: Planned Hierarchical Semi-Markov Q-Learning (P-HSMQ), a variant of the HSMQ algorithm (Dietterich, 2000b) which uses plan-built task hierarchies, and TeleoReactive Q-Learning (TRQ) a more complex algorithm which implements hierarchical reinforcement learning with teleo-reactive execution semantics (Nilsson, 1994). Each algorithm is demonstrated in a simple grid-world domain.",
            "referenceCount": 10,
            "citationCount": 57,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2002-07-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ryan2002UsingAM,\n author = {Malcolm R. K. Ryan},\n booktitle = {International Conference on Machine Learning},\n pages = {522-529},\n title = {Using Abstract Models of Behaviours to Automatically Generate Reinforcement Learning Hierarchies},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:79936bfab7ab464afba9e653f1921d147a883a2c",
            "@type": "ScholarlyArticle",
            "paperId": "79936bfab7ab464afba9e653f1921d147a883a2c",
            "corpusId": 12610697,
            "url": "https://www.semanticscholar.org/paper/79936bfab7ab464afba9e653f1921d147a883a2c",
            "title": "Algorithm Selection using Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2000,
            "externalIds": {
                "DBLP": "conf/icml/LagoudakisL00",
                "MAG": "1492936035",
                "CorpusId": 12610697
            },
            "abstract": "Many computational problems can be solved by multiple algorithms, with different algorithms fastest for different problem sizes, input distributions, and hardware characteristics. We consider the problem of algorithm selection: dynamically choose an algorithm to attack an instance of a problem with the goal of minimizing the overall execution time. We formulate the problem as a kind of Markov decision process (MDP), and use ideas from reinforcement learning to solve it. This paper introduces a kind of MDP that models the algorithm selection problem by allowing multiple state transitions. The well known Q-learning algorithm is adapted for this case in a way that combines both Monte-Carlo and Temporal Difference methods. Also, this work uses, and extends in a way to control problems, the Least-Squares Temporal Difference algorithm (LSTD(0)) of Boyan. The experimental study focuses on the classic problems of order statistic selection and sorting. The encouraging results reveal the potential of applying learning methods to traditional computational problems.",
            "referenceCount": 8,
            "citationCount": 160,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2000-06-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lagoudakis2000AlgorithmSU,\n author = {M. Lagoudakis and M. Littman},\n booktitle = {International Conference on Machine Learning},\n pages = {511-518},\n title = {Algorithm Selection using Reinforcement Learning},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fe83d9daeeceac28b5107c65a3d8bd3d19c4a60c",
            "@type": "ScholarlyArticle",
            "paperId": "fe83d9daeeceac28b5107c65a3d8bd3d19c4a60c",
            "corpusId": 260907322,
            "url": "https://www.semanticscholar.org/paper/fe83d9daeeceac28b5107c65a3d8bd3d19c4a60c",
            "title": "Reinforcement learning and its application to control",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1992,
            "externalIds": {
                "MAG": "185726777",
                "CorpusId": 260907322
            },
            "abstract": "Learning control involves modifying a controller's behavior to improve its performance as measured by some predefined index of performance (IP). If control actions that improve performance as measured by the IP are known, supervised learning methods, or methods for learning from examples, can be used to train the controller. But when such control actions are not known a priori, appropriate control behavior has to be inferred from observations of the IP. One can distinguish between two classes of methods for training controllers under such circumstances. Indirect methods involve constructing a model of the problem's IP and using the model to obtain training information for the controller. On the other hand, direct, or model-free, methods obtain the requisite training information by observing the effects of perturbing the controlled process on the IP. Despite its reputation for inefficiency, we argue that for certain types of problems the latter approach, of which reinforcement learning is an example, can yield faster, more reliable learning. Using several control problems as examples, we illustrate how the complexity of model construction can often exceed that of solving the original control problem using direct reinforcement learning methods, making indirect methods relatively inefficient. These results indicate the importance of considering direct reinforcement learning methods as tools for learning to solve control problems. We also present several techniques for augmenting the power of reinforcement learning methods. These include (1) the use of local models to guide assigning credit to the components of a reinforcement learning system, (2) implementing a procedure from experimental psychology called \"shaping\" to improve the efficiency of learning, thereby making more complex problems amenable to solution, and (3) implementing a multi-level learning architecture designed for exploiting task decomposability by using previously-learned behaviors as primitives for learning more complex tasks.",
            "referenceCount": 0,
            "citationCount": 105,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Gullapalli1992ReinforcementLA,\n author = {V. Gullapalli},\n title = {Reinforcement learning and its application to control},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:15664d5c2233dc92f394a80f3b425bece20d8289",
            "@type": "ScholarlyArticle",
            "paperId": "15664d5c2233dc92f394a80f3b425bece20d8289",
            "corpusId": 56661047,
            "url": "https://www.semanticscholar.org/paper/15664d5c2233dc92f394a80f3b425bece20d8289",
            "title": "Reinforcement learning in continuous time: advantage updating",
            "venue": "International Conference on Neural Networks",
            "publicationVenue": {
                "id": "urn:research:4bac083d-70ae-4de5-a6d0-66883eba600e",
                "name": "International Conference on Neural Networks",
                "alternate_names": [
                    "Int Conf Nanosci Nanotechnol",
                    "ICNN",
                    "International Conference on Nanoscience and Nanotechnology",
                    "Int Conf Neural Netw"
                ],
                "issn": null,
                "url": null
            },
            "year": 1994,
            "externalIds": {
                "MAG": "1520048352",
                "DOI": "10.1109/ICNN.1994.374604",
                "CorpusId": 56661047
            },
            "abstract": "A new algorithm for reinforcement learning, advantage updating, is described. Advantage updating is a direct learning technique; it does not require a model to be given or learned. It is incremental, requiring only a constant amount of calculation per time step, independent of the number of possible actions, possible outcomes from a given action, or number of states. Analysis and simulation indicate that advantage updating is applicable to reinforcement learning systems working in continuous time (or discrete time with small time steps) for which standard algorithms such as Q-learning are not applicable. Simulation results are presented indicating that for a simple linear quadratic regulator (LQR) problem, advantage updating learns more quickly than Q-learning by a factor of 100,000 when the time step is small. Even for large time steps, advantage updating is never slower than Q-learning, and advantage updating is more resistant to noise than is Q-learning. Convergence properties are discussed. It is proved that the learning rule for advantage updating converges to the optimal policy with probability one.<<ETX>>",
            "referenceCount": 12,
            "citationCount": 182,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference"
            ],
            "publicationDate": "1994-06-27",
            "journal": {
                "name": "Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Conference{Iii1994ReinforcementLI,\n author = {Leemon C Baird Iii},\n booktitle = {International Conference on Neural Networks},\n journal = {Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)},\n pages = {2448-2453 vol.4},\n title = {Reinforcement learning in continuous time: advantage updating},\n volume = {4},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:555780312ce3552ad54c96929912787660d92245",
            "@type": "ScholarlyArticle",
            "paperId": "555780312ce3552ad54c96929912787660d92245",
            "corpusId": 34643962,
            "url": "https://www.semanticscholar.org/paper/555780312ce3552ad54c96929912787660d92245",
            "title": "Speeding Up Relational Reinforcement Learning through the Use of an Incremental First Order Decision Tree Learner",
            "venue": "European Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:f9a81beb-9bcb-43bc-96a7-67cf23dba2d6",
                "name": "European Conference on Machine Learning",
                "alternate_names": [
                    "Eur Conf Mach Learn",
                    "European conference on Machine Learning",
                    "Eur conf Mach Learn",
                    "ECML"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/ecml"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2154441654",
                "DBLP": "conf/ecml/DriessensRB01",
                "DOI": "10.1007/3-540-44795-4_9",
                "CorpusId": 34643962
            },
            "abstract": null,
            "referenceCount": 10,
            "citationCount": 94,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/3-540-44795-4_9.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2001-09-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Driessens2001SpeedingUR,\n author = {K. Driessens and J. Ramon and H. Blockeel},\n booktitle = {European Conference on Machine Learning},\n pages = {97-108},\n title = {Speeding Up Relational Reinforcement Learning through the Use of an Incremental First Order Decision Tree Learner},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3ff964e3764db5d0e12bbbd9eaa0b29e488f4b30",
            "@type": "ScholarlyArticle",
            "paperId": "3ff964e3764db5d0e12bbbd9eaa0b29e488f4b30",
            "corpusId": 18188449,
            "url": "https://www.semanticscholar.org/paper/3ff964e3764db5d0e12bbbd9eaa0b29e488f4b30",
            "title": "Reinforcement Learning with Self-Modifying Policies",
            "venue": "Learning to Learn",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "DBLP": "books/sp/98/SchmidhuberZS98",
                "MAG": "1533058732",
                "DOI": "10.1007/978-1-4615-5529-2_12",
                "CorpusId": 18188449
            },
            "abstract": null,
            "referenceCount": 42,
            "citationCount": 92,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1998-05-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Schmidhuber1998ReinforcementLW,\n author = {J. Schmidhuber and Jieyu Zhao and N. Schraudolph},\n booktitle = {Learning to Learn},\n pages = {293-309},\n title = {Reinforcement Learning with Self-Modifying Policies},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:13193f151ecabb2e950b18dc402eca77642465e6",
            "@type": "ScholarlyArticle",
            "paperId": "13193f151ecabb2e950b18dc402eca77642465e6",
            "corpusId": 60440589,
            "url": "https://www.semanticscholar.org/paper/13193f151ecabb2e950b18dc402eca77642465e6",
            "title": "Genetic reinforcement learning for neurocontrol problems",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1993,
            "externalIds": {
                "DBLP": "journals/ml/Whitleya93",
                "MAG": "2049287437",
                "DOI": "10.1007/BF00993045",
                "CorpusId": 60440589
            },
            "abstract": null,
            "referenceCount": 31,
            "citationCount": 94,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2FBF00993045.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1993-11-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Whitley1993GeneticRL,\n author = {L. D. Whitley},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {259-284},\n title = {Genetic reinforcement learning for neurocontrol problems},\n volume = {13},\n year = {1993}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9c51af4f811f7dc196d8d64e44f68ab7a8a0cf89",
            "@type": "ScholarlyArticle",
            "paperId": "9c51af4f811f7dc196d8d64e44f68ab7a8a0cf89",
            "corpusId": 5073845,
            "url": "https://www.semanticscholar.org/paper/9c51af4f811f7dc196d8d64e44f68ab7a8a0cf89",
            "title": "Reinforcement learning for adaptive routing",
            "venue": "Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2949715023",
                "ArXiv": "cs/0703138",
                "DBLP": "journals/corr/abs-cs-0703138",
                "DOI": "10.1109/IJCNN.2002.1007796",
                "CorpusId": 5073845
            },
            "abstract": "Reinforcement learning means learning a policy-a mapping of observations into actions-based on feedback from the environment. The learning can be viewed as browsing a set of policies while evaluating them by trial through interaction with the environment. We present an application of a gradient ascent algorithm for reinforcement learning to a complex domain of packet routing in network communication and compare the performance of this algorithm to other routing methods on a benchmark problem.",
            "referenceCount": 21,
            "citationCount": 118,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/cs/0703138",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2002-08-07",
            "journal": {
                "name": "Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290)",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Peshkin2002ReinforcementLF,\n author = {L. Peshkin and Virginia Savova},\n booktitle = {Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290)},\n journal = {Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290)},\n pages = {1825-1830 vol.2},\n title = {Reinforcement learning for adaptive routing},\n volume = {2},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f67cd79df4f62742d6f3d9f1e8cd5c0cc9194d38",
            "@type": "ScholarlyArticle",
            "paperId": "f67cd79df4f62742d6f3d9f1e8cd5c0cc9194d38",
            "corpusId": 8783611,
            "url": "https://www.semanticscholar.org/paper/f67cd79df4f62742d6f3d9f1e8cd5c0cc9194d38",
            "title": "Efficient reinforcement learning",
            "venue": "Annual Conference Computational Learning Theory",
            "publicationVenue": {
                "id": "urn:research:24b0721b-0592-414a-ac79-7271515aaab0",
                "name": "Annual Conference Computational Learning Theory",
                "alternate_names": [
                    "Conf Learn Theory",
                    "COLT",
                    "Conference on Learning Theory",
                    "Annu Conf Comput Learn Theory"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=536"
            },
            "year": 1994,
            "externalIds": {
                "MAG": "2083143894",
                "DBLP": "conf/colt/Fiechter94",
                "DOI": "10.1145/180139.181019",
                "CorpusId": 8783611
            },
            "abstract": "In this paper we propose a new formal model for studying reinforcement learning, based on Valiant's PAC framework.\nIn our model the learner does not have direct access to every state of the environment. Instead, every sequence of experiments starts in a fixed initial state and the learner is provided with a \u201creset\u201d operation that interrupts the current sequence of experiments and starts a new one (from the initial state).\nWe do not require the agent to learn the optimal policy but only a good approximation of it with high probability. More precisely, we require the learner to produce a policy whose expected value from the initial state is \u03b5-close to that of the optimal policy, with probability no less than 1\u2212\u03b4.\nFor this model, we describe an algorithm that produces such an (\u03b5,\u03b4)-optimal policy for any environment, in time polynomial in <italic>N,K</italic>,1/\u03b5,1/\u03b4,1/(1\u2212\u03b2) and <italic>r<subscrpt>max</subscrpt></italic>, where <italic>N</italic> is the number of states of the environment, <italic>K</italic> is the maximum number of actions in a state, \u03b2 is the discount factor and <italic>r<subscrpt>max</subscrpt></italic> is the maximum reward on any transition.",
            "referenceCount": 14,
            "citationCount": 111,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1994-07-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Fiechter1994EfficientRL,\n author = {C. Fiechter},\n booktitle = {Annual Conference Computational Learning Theory},\n pages = {88-97},\n title = {Efficient reinforcement learning},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b930f68e73fb06bd70d72e05781cf00d2781bd95",
            "@type": "ScholarlyArticle",
            "paperId": "b930f68e73fb06bd70d72e05781cf00d2781bd95",
            "corpusId": 1115657,
            "url": "https://www.semanticscholar.org/paper/b930f68e73fb06bd70d72e05781cf00d2781bd95",
            "title": "Programmable Reinforcement Learning Agents",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2145739724",
                "DBLP": "conf/nips/AndreR00",
                "CorpusId": 1115657
            },
            "abstract": "This dissertation examines the use of partial programming as a means of designing agents for large Markov Decision Problems. In this approach, a programmer specifies only that which they know to be correct and the system then learns the rest from experience using reinforcement learning. \nIn contrast to previous low-level languages for partial programming, this dissertation presents ALisp, a Lisp-based high-level partial programming language. ALisp allows the programmer to constrain the policies considered by a learning process and to express his or her prior knowledge in a concise manner. Optimally completing a partial ALisp program is shown to be equivalent to solving a Semi-Markov Decision Problem (SMDP). Under a finite memory-use condition, online learning algorithms for ALisp are proved to converge to an optimal solution of the SMDP and thus to an optimal completion of the partial program. \nThis dissertation then presents methods for exploiting the modularity allows an agent to ignore aspects of its current state that are irrelevant to its current decision, and therefore speeds up reinforcement learning. By decomposing representations of the value of actions along subroutine boundaries, optimality, i.e., optimality among all policies consistent with the partial program. These methods are demonstrated on two simulated taxi tasks. \nFunction approximation, a method for representing the value of actions, allows reinforcement learning to be applied to problems where exact methods are intractable. Soft shaping is a method for guiding an agent toward a solution without constraining the search space. Both can be integrated with ALisp. ALisp with function approximation and reward shaping is successfully applied on a difficult continuous variant of the simulated taxi task. \nTogether, the methods presented in this work comprise a system for agent design that allows the programmer to specify what they know, hint at what they suspect using soft shaping, and leave unspecified that which they don't know; the system then optimally completes the program through experience and takes advantage of the hierarchical structure of the specified program to speed learning.",
            "referenceCount": 13,
            "citationCount": 138,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Andre2000ProgrammableRL,\n author = {D. Andre and Stuart J. Russell},\n booktitle = {Neural Information Processing Systems},\n pages = {1019-1025},\n title = {Programmable Reinforcement Learning Agents},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0f13891132dff5d564f30c58c6c4a4bfdc632784",
            "@type": "ScholarlyArticle",
            "paperId": "0f13891132dff5d564f30c58c6c4a4bfdc632784",
            "corpusId": 1906622,
            "url": "https://www.semanticscholar.org/paper/0f13891132dff5d564f30c58c6c4a4bfdc632784",
            "title": "Explanation-Based Learning and Reinforcement Learning: A Unified View",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1995,
            "externalIds": {
                "DBLP": "journals/ml/DietterichF97",
                "MAG": "2094438588",
                "DOI": "10.1023/A:1007355226281",
                "CorpusId": 1906622
            },
            "abstract": null,
            "referenceCount": 55,
            "citationCount": 69,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1007355226281.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1995-07-09",
            "journal": {
                "name": "Machine Learning",
                "volume": "28"
            },
            "citationStyles": {
                "bibtex": "@Article{Dietterich1995ExplanationBasedLA,\n author = {Thomas G. Dietterich and N. Flann},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {169-210},\n title = {Explanation-Based Learning and Reinforcement Learning: A Unified View},\n volume = {28},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:99e245ea74c84c2ce9423ee5dfa9246ab3f4ebdf",
            "@type": "ScholarlyArticle",
            "paperId": "99e245ea74c84c2ce9423ee5dfa9246ab3f4ebdf",
            "corpusId": 3520014,
            "url": "https://www.semanticscholar.org/paper/99e245ea74c84c2ce9423ee5dfa9246ab3f4ebdf",
            "title": "Team-partitioned, opaque-transition reinforcement learning",
            "venue": "International Conference on Autonomous Agents",
            "publicationVenue": {
                "id": "urn:research:5ac7efbe-1e13-4240-a305-41ba875cf38e",
                "name": "International Conference on Autonomous Agents",
                "alternate_names": [
                    "Agents",
                    "Int Conf Auton Agent"
                ],
                "issn": null,
                "url": "http://www.acm.org/pubs/contents/proceedings/series/agents/"
            },
            "year": 1999,
            "externalIds": {
                "DBLP": "conf/agents/StoneV99",
                "MAG": "2130985368",
                "DOI": "10.1145/301136.301195",
                "CorpusId": 3520014
            },
            "abstract": "We present a novel multi-agent learning paradigm called team-partitioned, opaque-transition reinforcement learning (TPOT-RL). TPOT-RL introduces the use of action-dependent features to generalize the state space. In our work, we use a learned action-dependent feature space to aid higher-level reinforcement learning. TPOT-RL is an effective technique to allow a team of agents to learn to cooperate towards the achievement of a specific goal. It is an adaptation of traditional RL methods that is applicable in complex, non-Markovian, multi-agent domains with large state spaces and limited training opportunities. TPOT-RL is fully implemented and has been tested in the robotic soccer domain, a complex, multi-agent framework. This paper presents the algorithmic details of TPOT-RL as well as empirical results demonstrating the effectiveness of the developed multi-agent learning approach with learned features.",
            "referenceCount": 21,
            "citationCount": 154,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1999-04-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Stone1999TeampartitionedOR,\n author = {P. Stone and M. Veloso},\n booktitle = {International Conference on Autonomous Agents},\n pages = {206-212},\n title = {Team-partitioned, opaque-transition reinforcement learning},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:56981125cd3e714e748adbac264d7bc8b22d8394",
            "@type": "ScholarlyArticle",
            "paperId": "56981125cd3e714e748adbac264d7bc8b22d8394",
            "corpusId": 62703909,
            "url": "https://www.semanticscholar.org/paper/56981125cd3e714e748adbac264d7bc8b22d8394",
            "title": "Reinforcement Learning is Direct Adaptive Optimal Control",
            "venue": "American Control Conference",
            "publicationVenue": {
                "id": "urn:research:fe4d09f8-d278-4bfb-b73a-1a6a0e22f6a3",
                "name": "American Control Conference",
                "alternate_names": [
                    "Int Conf Adv Comput Control",
                    "ACC",
                    "Advances in Computing and Communications",
                    "Adv Comput Commun",
                    "Am Control Conf",
                    "Advances in Computer and Communication",
                    "International Conference on Advanced Computer Control"
                ],
                "issn": "2767-2875",
                "url": "http://a2c2.org/conferences/american-control-conferences"
            },
            "year": 1992,
            "externalIds": {
                "MAG": "1990005421",
                "DOI": "10.1109/37.126844",
                "CorpusId": 62703909
            },
            "abstract": "Control problems can be divided into two classes: 1) regulation and tracking problems, in which the objective is to follow a reference trajectory, and 2) optimal control problems, in which the objective is to extremize a functional of the controlled system's behavior that is not necessarily defined in terms of a reference trajectory. Adaptive methods for problems of the first kind are well known, and include self-tuning regulators and model-reference methods, whereas adaptive methods for optimal-control problems have received relatively little attention. Moreover, the adaptive optimal-control methods that have been studied are almost all indirect methods, in which controls are recomputed from an estimated system model at each step. This computation is inherently complex, making adaptive methods in which the optimal controls are estimated directly more attractive. Here we present reinforcement learning methods as a computationally simple, direct approach to the adaptive optimal control of nonlinear systems.",
            "referenceCount": 24,
            "citationCount": 268,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference"
            ],
            "publicationDate": "1992-04-01",
            "journal": {
                "name": "1991 American Control Conference",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Conference{Sutton1992ReinforcementLI,\n author = {R. Sutton and A. Barto and Ronald J. Williams},\n booktitle = {American Control Conference},\n journal = {1991 American Control Conference},\n pages = {2143-2146},\n title = {Reinforcement Learning is Direct Adaptive Optimal Control},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9fbfcf4080992723481fbb447bdfe76ad2f46d5e",
            "@type": "ScholarlyArticle",
            "paperId": "9fbfcf4080992723481fbb447bdfe76ad2f46d5e",
            "corpusId": 7330860,
            "url": "https://www.semanticscholar.org/paper/9fbfcf4080992723481fbb447bdfe76ad2f46d5e",
            "title": "Naive Reinforcement Learning With Endogenous Aspirations",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2166692080",
                "DOI": "10.1111/1468-2354.00090",
                "CorpusId": 7330860
            },
            "abstract": "This article considers a simple model of reinforcement learning. All behavior change derives from the reinforcing or deterring effect of instantaneous payoff experiences. Payoff experiences are reinforcing or deterring depending on whether the payoff exceeds an aspiration level or falls short of it. Over time, the aspiration level is adjusted toward the actually experienced payoffs. This article shows that aspiration level adjustments may improve the decision maker\u2019s long-run performance by preventing him or her from feeling dissatisfied with even the best available strategies. However, such movements also lead to persistent deviations from expected payoff maximization by creating \u201cprobability matching\u201d effects.",
            "referenceCount": 30,
            "citationCount": 180,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.dklevine.com/archive/refs4381.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Economics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Economics",
                    "source": "external"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2000-11-01",
            "journal": {
                "name": "International Economic Review",
                "volume": "41"
            },
            "citationStyles": {
                "bibtex": "@Article{B\u00f6rgers2000NaiveRL,\n author = {T. B\u00f6rgers and R. Sarin},\n journal = {International Economic Review},\n pages = {921-950},\n title = {Naive Reinforcement Learning With Endogenous Aspirations},\n volume = {41},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9bac29e1cc7420375666ca85e6ef2db47f1fa1f2",
            "@type": "ScholarlyArticle",
            "paperId": "9bac29e1cc7420375666ca85e6ef2db47f1fa1f2",
            "corpusId": 265672501,
            "url": "https://www.semanticscholar.org/paper/9bac29e1cc7420375666ca85e6ef2db47f1fa1f2",
            "title": "Reinforcement Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1992,
            "externalIds": {
                "MAG": "2911283634",
                "CorpusId": 265672501
            },
            "abstract": "Reinforcement learning is an approach to artificial intelligence that emphasizes learning by the individual from its interaction with its environment. This contrasts with classical approaches to artificial intelligence and machine learning, which have downplayed learning from interaction, focusing instead on learning from a knowledgeable teacher, or on reasoning from a complete model of the environment. Modern reinforcement learning research is highly interdisciplinary; it includes researchers specializing in operations research, genetic algorithms, neural networks, psychology, and control engineering. Reinforcement learning is learning what to do\u2014how to map situations to actions\u2014so as to maximize a scalar reward signal. The learner is not told which action to take, as in most forms of machine learning, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward, but also the next situation, and through that all subsequent rewards. These two characteristics\u2014trial-and-error search and delayed reward\u2014are the two most important distinguishing features of reinforcement learning. One of the challenges that arises in reinforcement learning and not in other kinds of learning is the tradeoff between exploration and exploitation. To obtain a lot of reward, a reinforcement learning agent must prefer actions that it has tried in the past and found to be effective in producing reward. But to discover which actions these are it has to select actions that it has not tried before. The agent has to exploit what it already knows in order to obtain reward, but it also has to explore in order to make better action selections in the future. The dilemma is that neither exploitation nor exploration can be pursued exclusively without failing at the task. Modern reinforcement learning research uses the formal framework of",
            "referenceCount": 2,
            "citationCount": 79,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1992-06-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Sutton1992ReinforcementL,\n author = {R. S. Sutton},\n title = {Reinforcement Learning},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:628b80ac7952a67155d62e10dc2854ac8c04a6e4",
            "@type": "ScholarlyArticle",
            "paperId": "628b80ac7952a67155d62e10dc2854ac8c04a6e4",
            "corpusId": 17939820,
            "url": "https://www.semanticscholar.org/paper/628b80ac7952a67155d62e10dc2854ac8c04a6e4",
            "title": "Using Expectation-Maximization for Reinforcement Learning",
            "venue": "Neural Computation",
            "publicationVenue": {
                "id": "urn:research:69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                "name": "Neural Computation",
                "alternate_names": [
                    "Neural Comput"
                ],
                "issn": "0899-7667",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667"
            },
            "year": 1997,
            "externalIds": {
                "MAG": "2080039641",
                "DBLP": "journals/neco/DayanH97",
                "DOI": "10.1162/neco.1997.9.2.271",
                "CorpusId": 17939820
            },
            "abstract": "We discuss Hinton's (1989) relative payoff procedure (RPP), a static reinforcement learning algorithm whose foundation is not stochastic gradient ascent. We show circumstances under which applying the RPP is guaranteed to increase the mean return, even though it can make large changes in the values of the parameters. The proof is based on a mapping between the RPP and a form of the expectation-maximization procedure of Dempster, Laird, and Rubin (1977).",
            "referenceCount": 10,
            "citationCount": 207,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1997-02-15",
            "journal": {
                "name": "Neural Computation",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Dayan1997UsingEF,\n author = {P. Dayan and Geoffrey E. Hinton},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {271-278},\n title = {Using Expectation-Maximization for Reinforcement Learning},\n volume = {9},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:24c3c87a7314bc335295f10fd5790cfea26cddf8",
            "@type": "ScholarlyArticle",
            "paperId": "24c3c87a7314bc335295f10fd5790cfea26cddf8",
            "corpusId": 15578598,
            "url": "https://www.semanticscholar.org/paper/24c3c87a7314bc335295f10fd5790cfea26cddf8",
            "title": "Multi-agent Reinforcement Learning: A Modular Approach",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1996,
            "externalIds": {
                "MAG": "2147667317",
                "CorpusId": 15578598
            },
            "abstract": "To investigate tile potentials and limitations of multi-agent reinforcement learning, several attempts have been made to let multiple monolithic reinforcement-learning agents synthesize coordinated decision policies needed to accomplish their common goals effectively. Most of these straightforward reinforcement-learning approaches, howe~-er, scale poorly to more complex multi-agent learning problems because the state space for each learning agent grows exponenti~dly in the number of its partner agents engaged in the joint task. In this paper, we consider a modified w~rsion of the Pursuit Problem as a multi-agent learning problem which is computationally intractable by those straightforward approaches. We show how successfully a collection of nmdular Q-learning hunter agents synthesize coordinated decision policies needed to capture a randomlyfleeing prey agent, by specializing their individual fimctionality and synthesizing herding behavior.",
            "referenceCount": 14,
            "citationCount": 95,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Fukumoto1996MultiagentRL,\n author = {Kenji Fukumoto},\n title = {Multi-agent Reinforcement Learning: A Modular Approach},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f0b79becda09a9a85ee5900481a061c5e6974497",
            "@type": "ScholarlyArticle",
            "paperId": "f0b79becda09a9a85ee5900481a061c5e6974497",
            "corpusId": 16944713,
            "url": "https://www.semanticscholar.org/paper/f0b79becda09a9a85ee5900481a061c5e6974497",
            "title": "An on-line algorithm for dynamic reinforcement learning and planning in reactive environments",
            "venue": "1990 IJCNN International Joint Conference on Neural Networks",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1990,
            "externalIds": {
                "DBLP": "conf/ijcnn/Schmidhuber90",
                "MAG": "2084920657",
                "DOI": "10.1109/IJCNN.1990.137723",
                "CorpusId": 16944713
            },
            "abstract": "An online learning algorithm for reinforcement learning with continually running recurrent networks in nonstationary reactive environments is described. Various kinds of reinforcement are considered as special types of input to an agent living in the environment. The agent's only goal is to maximize the amount of reinforcement received over time. Supervised learning techniques for recurrent networks serve to construct a differentiable model of the environmental dynamics which includes a model of future reinforcement. This model is used for learning goal-directed behavior in an online fashion. The possibility of using the system for planning future action sequences is investigated and this approach is compared to approaches based on temporal difference methods. A connection to metalearning (learning how to learn) is noted",
            "referenceCount": 14,
            "citationCount": 82,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1990-06-17",
            "journal": {
                "name": "1990 IJCNN International Joint Conference on Neural Networks",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Schmidhuber1990AnOA,\n author = {J. Schmidhuber},\n booktitle = {1990 IJCNN International Joint Conference on Neural Networks},\n journal = {1990 IJCNN International Joint Conference on Neural Networks},\n pages = {253-258 vol.2},\n title = {An on-line algorithm for dynamic reinforcement learning and planning in reactive environments},\n year = {1990}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:109186b81a0575936297ae3a0ff41491124a4bf2",
            "@type": "ScholarlyArticle",
            "paperId": "109186b81a0575936297ae3a0ff41491124a4bf2",
            "corpusId": 462880,
            "url": "https://www.semanticscholar.org/paper/109186b81a0575936297ae3a0ff41491124a4bf2",
            "title": "A social reinforcement learning agent",
            "venue": "International Conference on Autonomous Agents",
            "publicationVenue": {
                "id": "urn:research:5ac7efbe-1e13-4240-a305-41ba875cf38e",
                "name": "International Conference on Autonomous Agents",
                "alternate_names": [
                    "Agents",
                    "Int Conf Auton Agent"
                ],
                "issn": null,
                "url": "http://www.acm.org/pubs/contents/proceedings/series/agents/"
            },
            "year": 2001,
            "externalIds": {
                "DBLP": "conf/agents/IsbellSKSS01",
                "MAG": "2118756286",
                "DOI": "10.1145/375735.376334",
                "CorpusId": 462880
            },
            "abstract": "We report on our reinforcement learning work on Cobot, a software agent that resides in the well-known online chat community LambdaMOO. Our initial work on Cobot~\\cite{cobotaaai} provided him with the ability to collect {\\em social statistics\\/} and report them to users in a reactive manner. Here we describe our application of reinforcement learning to allow Cobot to proactively take actions in this complex social environment, and adapt his behavior from multiple sources of human reward. After 5 months of training, Cobot received 3171 reward and punishment events from 254 different Lambda\\-MOO users, and learned nontrivial preferences for a number of users. Cobot modifies his behavior based on his current state in an attempt to maximize reward. Here we describe LambdaMOO and the state and action spaces of Cobot, and report the statistical results of the learning experiment.",
            "referenceCount": 9,
            "citationCount": 154,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-05-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Isbell2001ASR,\n author = {C. Isbell and C. Shelton and Michael Kearns and Satinder Singh and P. Stone},\n booktitle = {International Conference on Autonomous Agents},\n pages = {377-384},\n title = {A social reinforcement learning agent},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d319eb0a9399c4e7280b2c945f4d10e8138a9304",
            "@type": "ScholarlyArticle",
            "paperId": "d319eb0a9399c4e7280b2c945f4d10e8138a9304",
            "corpusId": 11768398,
            "url": "https://www.semanticscholar.org/paper/d319eb0a9399c4e7280b2c945f4d10e8138a9304",
            "title": "Acquiring robot skills via reinforcement learning",
            "venue": "IEEE Control Systems",
            "publicationVenue": {
                "id": "urn:research:c7be0e82-620f-44c5-b6de-46310d59c7b8",
                "name": "IEEE Control Systems",
                "alternate_names": [
                    "IEEE Control Syst",
                    "IEEE Control Systems Magazine",
                    "IEEE Control Syst Mag"
                ],
                "issn": "1066-033X",
                "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5488303"
            },
            "year": 1994,
            "externalIds": {
                "MAG": "1988071341",
                "DOI": "10.1109/37.257890",
                "CorpusId": 11768398
            },
            "abstract": "Skill acquisition is a difficult , yet important problem in robot performance. The authors focus on two skills, namely robotic assembly and balancing and on two classic tasks to develop these skills via learning: the peg-in hole insertion task, and the ball balancing task. A stochastic real-valued (SRV) reinforcement learning algorithm is described and used for learning control and the authors show how it can be used with nonlinear multilayer ANNs. In the peg-in-hole insertion task the SRV network successfully learns to insert to insert a peg into a hole with extremely low clearance, in spite of high sensor noise. In the ball balancing task the SRV network successfully learns to balance the ball with minimal feedback.<<ETX>>",
            "referenceCount": 27,
            "citationCount": 222,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1994-02-01",
            "journal": {
                "name": "IEEE Control Systems",
                "volume": "14"
            },
            "citationStyles": {
                "bibtex": "@Article{Gullapalli1994AcquiringRS,\n author = {V. Gullapalli and J. Franklin and H. Benbrahim},\n booktitle = {IEEE Control Systems},\n journal = {IEEE Control Systems},\n pages = {13-24},\n title = {Acquiring robot skills via reinforcement learning},\n volume = {14},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:787fb0aad7fd83e8bbb002d60dcb7a83c589b1fa",
            "@type": "ScholarlyArticle",
            "paperId": "787fb0aad7fd83e8bbb002d60dcb7a83c589b1fa",
            "corpusId": 28481517,
            "url": "https://www.semanticscholar.org/paper/787fb0aad7fd83e8bbb002d60dcb7a83c589b1fa",
            "title": "A Complexity Analysis of Cooperative Mechanisms in Reinforcement Learning",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 1991,
            "externalIds": {
                "DBLP": "conf/aaai/Whitehead91",
                "MAG": "145683767",
                "CorpusId": 28481517
            },
            "abstract": "Reinforcement learning algorithms, when used to solve multi-stage decision problems, perform a kind of online (incremental) search to find an optimal decision policy. The time complexity of this search strongly depends upon the size and structure of the state space and upon a priori knowledge encoded in the learners initial parameter values. When a priori knowledge is not available, search is unbiased and can be excessive. \n \nCooperative mechanisms help reduce search by providing the learner with shorter latency feedback and auxiliary sources of experience. These mechanisms are based on the observation that in nature, intelligent agents exist in a cooperative social environment that helps structure and guide learning. Within this context, learning involves information transfer as much as it does discovery by trial-and-error. \n \nTwo cooperative mechanisms are described: Learning with an External Critic (or LEC) and Learning By Watching (or LBW). The search time complexity of these algorithms, along with unbiased Q-learning, are analyzed for problem solving tasks on a restricted class of state spaces. The results indicate that while unbiased search can be expected to require time moderately exponential in the size of the state space, the LEC and LBW algorithms require at most time linear in the size of the state space and under appropriate conditions, are independent of the state space size altogether; requiring time proportional to the length of the optimal solution path. While these analytic results apply only to a restricted class of tasks, they shed light on the complexity of search in reinforcement learning in general and the utility of cooperative mechanisms for reducing search.",
            "referenceCount": 8,
            "citationCount": 209,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1991-07-14",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Whitehead1991ACA,\n author = {S. Whitehead},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {607-613},\n title = {A Complexity Analysis of Cooperative Mechanisms in Reinforcement Learning},\n year = {1991}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:07b6e294c47ef0d72b3229ca6b891dd772adb47d",
            "@type": "ScholarlyArticle",
            "paperId": "07b6e294c47ef0d72b3229ca6b891dd772adb47d",
            "corpusId": 886415,
            "url": "https://www.semanticscholar.org/paper/07b6e294c47ef0d72b3229ca6b891dd772adb47d",
            "title": "Theoretical Results on Reinforcement Learning with Temporally Abstract Options",
            "venue": "European Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:f9a81beb-9bcb-43bc-96a7-67cf23dba2d6",
                "name": "European Conference on Machine Learning",
                "alternate_names": [
                    "Eur Conf Mach Learn",
                    "European conference on Machine Learning",
                    "Eur conf Mach Learn",
                    "ECML"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/ecml"
            },
            "year": 1998,
            "externalIds": {
                "DBLP": "conf/ecml/PrecupSS98",
                "MAG": "1600813180",
                "DOI": "10.1007/BFb0026709",
                "CorpusId": 886415
            },
            "abstract": null,
            "referenceCount": 25,
            "citationCount": 138,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2FBFb0026709.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1998-04-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Precup1998TheoreticalRO,\n author = {Doina Precup and R. Sutton and Satinder Singh},\n booktitle = {European Conference on Machine Learning},\n pages = {382-393},\n title = {Theoretical Results on Reinforcement Learning with Temporally Abstract Options},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:017e5041aeece3cb5c3ef7c8c3ee7d6526bbb524",
            "@type": "ScholarlyArticle",
            "paperId": "017e5041aeece3cb5c3ef7c8c3ee7d6526bbb524",
            "corpusId": 16180068,
            "url": "https://www.semanticscholar.org/paper/017e5041aeece3cb5c3ef7c8c3ee7d6526bbb524",
            "title": "Learning to fly by combining reinforcement learning with behavioural cloning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "conf/icml/MoralesS04",
                "MAG": "2062122188",
                "DOI": "10.1145/1015330.1015384",
                "CorpusId": 16180068
            },
            "abstract": "Reinforcement learning deals with learning optimal or near optimal policies while interacting with the environment. Application domains with many continuous variables are difficult to solve with existing reinforcement learning methods due to the large search space. In this paper, we use a relational representation to define powerful abstractions that allow us to incorporate domain knowledge and re-use previously learned policies in other similar problems. We also describe how to learn useful actions from human traces using a behavioural cloning approach combined with an exploration phase. Since several conflicting actions may be induced for the same abstract state, reinforcement learning is used to learn an optimal policy over this reduced space. It is shown experimentally how a combination of behavioural cloning and reinforcement learning using a relational representation is powerful enough to learn how to fly an aircraft through different points in space and different turbulence conditions.",
            "referenceCount": 12,
            "citationCount": 45,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2004-07-04",
            "journal": {
                "name": "Proceedings of the twenty-first international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Morales2004LearningTF,\n author = {E. Morales and C. Sammut},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the twenty-first international conference on Machine learning},\n title = {Learning to fly by combining reinforcement learning with behavioural cloning},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1df6a82fd3b325e507fe8d391ee3ada9d9575e74",
            "@type": "ScholarlyArticle",
            "paperId": "1df6a82fd3b325e507fe8d391ee3ada9d9575e74",
            "corpusId": 15075814,
            "url": "https://www.semanticscholar.org/paper/1df6a82fd3b325e507fe8d391ee3ada9d9575e74",
            "title": "On the convergence of reinforcement learning",
            "venue": "Journal of Economics Theory",
            "publicationVenue": {
                "id": "urn:research:f1fea8c4-2198-43fc-84e0-264508a44af4",
                "name": "Journal of Economics Theory",
                "alternate_names": [
                    "Journal of Economic Theory",
                    "J Econ Theory"
                ],
                "issn": "1994-8212",
                "url": "http://www.medwelljournals.com/archive.php?jid=1994-8212"
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "journals/jet/Beggs05",
                "MAG": "2167425257",
                "DOI": "10.1016/j.jet.2004.03.008",
                "CorpusId": 15075814
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 174,
            "influentialCitationCount": 32,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ora.ox.ac.uk/objects/uuid:97339c58-0d4c-40ca-a289-ce0e96fa04d6/files/m94be334f386ec89f1d261d80565e5755",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Economics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Economics",
                    "source": "external"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2005-05-01",
            "journal": {
                "name": "J. Econ. Theory",
                "volume": "122"
            },
            "citationStyles": {
                "bibtex": "@Article{None,\n booktitle = {Journal of Economics Theory},\n journal = {J. Econ. Theory},\n pages = {1-36},\n title = {On the convergence of reinforcement learning},\n volume = {122},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8c63689611fd1bf40d6a5ca14ba730ca7e62d283",
            "@type": "ScholarlyArticle",
            "paperId": "8c63689611fd1bf40d6a5ca14ba730ca7e62d283",
            "corpusId": 195937764,
            "url": "https://www.semanticscholar.org/paper/8c63689611fd1bf40d6a5ca14ba730ca7e62d283",
            "title": "Reinforcement Learning: : An Introduction",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2312609093",
                "DOI": "10.1108/K.1998.27.9.1093.3",
                "CorpusId": 195937764
            },
            "abstract": null,
            "referenceCount": 1,
            "citationCount": 374,
            "influentialCitationCount": 38,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1998-12-01",
            "journal": {
                "name": "Kybernetes",
                "volume": "27"
            },
            "citationStyles": {
                "bibtex": "@Article{Andrew1998ReinforcementL,\n author = {A. Andrew},\n journal = {Kybernetes},\n pages = {1093-1096},\n title = {Reinforcement Learning: : An Introduction},\n volume = {27},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:42af0ed020c2caecafb7dbe826064d7f9ba2022b",
            "@type": "ScholarlyArticle",
            "paperId": "42af0ed020c2caecafb7dbe826064d7f9ba2022b",
            "corpusId": 11077257,
            "url": "https://www.semanticscholar.org/paper/42af0ed020c2caecafb7dbe826064d7f9ba2022b",
            "title": "Dynamic abstraction in reinforcement learning via clustering",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2160808139",
                "DBLP": "conf/icml/MannorMHK04",
                "DOI": "10.1145/1015330.1015355",
                "CorpusId": 11077257
            },
            "abstract": "We consider a graph theoretic approach for automatic construction of options in a dynamic environment. A map of the environment is generated on-line by the learning agent, representing the topological structure of the state transitions. A clustering algorithm is then used to partition the state space to different regions. Policies for reaching the different parts of the space are separately learned and added to the model in a form of options (macro-actions). The options are used for accelerating the Q-Learning algorithm. We extend the basic algorithm and consider building a map that includes preliminary indication of the location of \"interesting\" regions of the state space, where the value gradient is significant and additional exploration might be beneficial. Experiments indicate significant speedups, especially in the initial learning phase.",
            "referenceCount": 20,
            "citationCount": 238,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-07-04",
            "journal": {
                "name": "Proceedings of the twenty-first international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Mannor2004DynamicAI,\n author = {Shie Mannor and Ishai Menache and Amit Hoze and Uri Klein},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the twenty-first international conference on Machine learning},\n title = {Dynamic abstraction in reinforcement learning via clustering},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ba34fd2fd9628d92d2f4dcff73277d15c35bd6cb",
            "@type": "ScholarlyArticle",
            "paperId": "ba34fd2fd9628d92d2f4dcff73277d15c35bd6cb",
            "corpusId": 2943343,
            "url": "https://www.semanticscholar.org/paper/ba34fd2fd9628d92d2f4dcff73277d15c35bd6cb",
            "title": "Value-function reinforcement learning in Markov games",
            "venue": "Cognitive Systems Research",
            "publicationVenue": {
                "id": "urn:research:6051439f-768c-4aae-a75c-3c82be5fa675",
                "name": "Cognitive Systems Research",
                "alternate_names": [
                    "Cogn Syst Res"
                ],
                "issn": "1389-0417",
                "url": "http://www.elsevier.com/locate/cogsys"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "1973039793",
                "DBLP": "journals/cogsr/Littman01",
                "DOI": "10.1016/S1389-0417(01)00015-8",
                "CorpusId": 2943343
            },
            "abstract": null,
            "referenceCount": 39,
            "citationCount": 406,
            "influentialCitationCount": 35,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cogsci.rpi.edu/~rsun/si-mal/article3.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-04-01",
            "journal": {
                "name": "Cognitive Systems Research",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Littman2001ValuefunctionRL,\n author = {M. Littman},\n booktitle = {Cognitive Systems Research},\n journal = {Cognitive Systems Research},\n pages = {55-66},\n title = {Value-function reinforcement learning in Markov games},\n volume = {2},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:99b2fd28dcab3657c5f1271a05223f4740e4b65c",
            "@type": "ScholarlyArticle",
            "paperId": "99b2fd28dcab3657c5f1271a05223f4740e4b65c",
            "corpusId": 10564390,
            "url": "https://www.semanticscholar.org/paper/99b2fd28dcab3657c5f1271a05223f4740e4b65c",
            "title": "A Reinforcement Learning Method for Maximizing Undiscounted Rewards",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1993,
            "externalIds": {
                "DBLP": "conf/icml/Schwartz93",
                "MAG": "1549353711",
                "DOI": "10.1016/b978-1-55860-307-3.50045-9",
                "CorpusId": 10564390
            },
            "abstract": null,
            "referenceCount": 21,
            "citationCount": 347,
            "influentialCitationCount": 48,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1993-06-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Schwartz1993ARL,\n author = {Anton Schwartz},\n booktitle = {International Conference on Machine Learning},\n pages = {298-305},\n title = {A Reinforcement Learning Method for Maximizing Undiscounted Rewards},\n year = {1993}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dca9444e1c69eee36c0be04703d71114a762c84a",
            "@type": "ScholarlyArticle",
            "paperId": "dca9444e1c69eee36c0be04703d71114a762c84a",
            "corpusId": 7830103,
            "url": "https://www.semanticscholar.org/paper/dca9444e1c69eee36c0be04703d71114a762c84a",
            "title": "Q-Cut - Dynamic Discovery of Sub-goals in Reinforcement Learning",
            "venue": "European Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:f9a81beb-9bcb-43bc-96a7-67cf23dba2d6",
                "name": "European Conference on Machine Learning",
                "alternate_names": [
                    "Eur Conf Mach Learn",
                    "European conference on Machine Learning",
                    "Eur conf Mach Learn",
                    "ECML"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/ecml"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "1536990779",
                "DBLP": "conf/ecml/MenacheMS02",
                "DOI": "10.1007/3-540-36755-1_25",
                "CorpusId": 7830103
            },
            "abstract": null,
            "referenceCount": 19,
            "citationCount": 275,
            "influentialCitationCount": 22,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2F3-540-36755-1_25.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2002-08-19",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Menache2002QCutD,\n author = {Ishai Menache and Shie Mannor and N. Shimkin},\n booktitle = {European Conference on Machine Learning},\n pages = {295-306},\n title = {Q-Cut - Dynamic Discovery of Sub-goals in Reinforcement Learning},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3e06f9b4e4679d23f99fabab6b94105afa4705a7",
            "@type": "ScholarlyArticle",
            "paperId": "3e06f9b4e4679d23f99fabab6b94105afa4705a7",
            "corpusId": 911871,
            "url": "https://www.semanticscholar.org/paper/3e06f9b4e4679d23f99fabab6b94105afa4705a7",
            "title": "Multiagent reinforcement learning in the Iterated Prisoner's Dilemma.",
            "venue": "Bio Systems",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1996,
            "externalIds": {
                "MAG": "2053616263",
                "DOI": "10.1016/0303-2647(95)01551-5",
                "CorpusId": 911871,
                "PubMed": "8924633"
            },
            "abstract": null,
            "referenceCount": 42,
            "citationCount": 359,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Bio Systems",
                "volume": "37 1-2"
            },
            "citationStyles": {
                "bibtex": "@Article{Sandholm1996MultiagentRL,\n author = {T. Sandholm and Robert H. Crites},\n booktitle = {Bio Systems},\n journal = {Bio Systems},\n pages = {\n          147-66\n        },\n title = {Multiagent reinforcement learning in the Iterated Prisoner's Dilemma.},\n volume = {37 1-2},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6b3f41d409d7e2031ce55b2a7e85a9a621ae39fa",
            "@type": "ScholarlyArticle",
            "paperId": "6b3f41d409d7e2031ce55b2a7e85a9a621ae39fa",
            "corpusId": 206088928,
            "url": "https://www.semanticscholar.org/paper/6b3f41d409d7e2031ce55b2a7e85a9a621ae39fa",
            "title": "Meta-learning in Reinforcement Learning",
            "venue": "Neural Networks",
            "publicationVenue": {
                "id": "urn:research:a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                "name": "Neural Networks",
                "alternate_names": [
                    "Neural Netw"
                ],
                "issn": "0893-6080",
                "url": "http://www.elsevier.com/locate/neunet"
            },
            "year": 2003,
            "externalIds": {
                "MAG": "1996579288",
                "DBLP": "journals/nn/SchweighoferD03",
                "DOI": "10.1016/S0893-6080(02)00228-9",
                "CorpusId": 206088928,
                "PubMed": "12576101"
            },
            "abstract": null,
            "referenceCount": 17,
            "citationCount": 237,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Neural networks : the official journal of the International Neural Network Society",
                "volume": "16 1"
            },
            "citationStyles": {
                "bibtex": "@Article{Schweighofer2003MetalearningIR,\n author = {N. Schweighofer and K. Doya},\n booktitle = {Neural Networks},\n journal = {Neural networks : the official journal of the International Neural Network Society},\n pages = {\n          5-9\n        },\n title = {Meta-learning in Reinforcement Learning},\n volume = {16 1},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a48afa674b3dac1d941f62f4f40e25ce7bd5da2c",
            "@type": "ScholarlyArticle",
            "paperId": "a48afa674b3dac1d941f62f4f40e25ce7bd5da2c",
            "corpusId": 14988400,
            "url": "https://www.semanticscholar.org/paper/a48afa674b3dac1d941f62f4f40e25ce7bd5da2c",
            "title": "Reinforcement learning and neural reinforcement learning",
            "venue": "The European Symposium on Artificial Neural Networks",
            "publicationVenue": {
                "id": "urn:research:93d6c444-c90a-48ee-a3ad-ef1f015bc28a",
                "name": "The European Symposium on Artificial Neural Networks",
                "alternate_names": [
                    "Eur Symp Artif Neural Netw",
                    "ESANN"
                ],
                "issn": null,
                "url": "https://www.esann.org/"
            },
            "year": 1994,
            "externalIds": {
                "DBLP": "conf/esann/SehadT94",
                "MAG": "2407471301",
                "CorpusId": 14988400
            },
            "abstract": "In this paper, we address an under-represented class of learning algorithms in the study of connectionism: reinforcement learning. We first introduce these classic methods in a new formalism which highlights the particularities of implementations such as Q-Learning, QLearning with Hamming distance, Q-Learning with statistical clustering and Dyna-Q. We then present in this formalism a neural implementation of reinforcement which clearly points out the advantages and the disadvantages of each approach.",
            "referenceCount": 17,
            "citationCount": 9,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Sehad1994ReinforcementLA,\n author = {S. Sehad and C. Touzet},\n booktitle = {The European Symposium on Artificial Neural Networks},\n title = {Reinforcement learning and neural reinforcement learning},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e9dc8d71572719cec58ec815bbd331fbd07fa15",
            "@type": "ScholarlyArticle",
            "paperId": "5e9dc8d71572719cec58ec815bbd331fbd07fa15",
            "corpusId": 21486916,
            "url": "https://www.semanticscholar.org/paper/5e9dc8d71572719cec58ec815bbd331fbd07fa15",
            "title": "A stochastic reinforcement learning algorithm for learning real-valued functions",
            "venue": "Neural Networks",
            "publicationVenue": {
                "id": "urn:research:a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                "name": "Neural Networks",
                "alternate_names": [
                    "Neural Netw"
                ],
                "issn": "0893-6080",
                "url": "http://www.elsevier.com/locate/neunet"
            },
            "year": 1990,
            "externalIds": {
                "MAG": "2080759927",
                "DBLP": "journals/nn/Gullapalli90",
                "DOI": "10.1016/0893-6080(90)90056-Q",
                "CorpusId": 21486916
            },
            "abstract": null,
            "referenceCount": 19,
            "citationCount": 320,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Neural Networks",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Gullapalli1990ASR,\n author = {V. Gullapalli},\n booktitle = {Neural Networks},\n journal = {Neural Networks},\n pages = {671-692},\n title = {A stochastic reinforcement learning algorithm for learning real-valued functions},\n volume = {3},\n year = {1990}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:35c599bd30ea3ac0291c06e81cbc5c73c7816c32",
            "@type": "ScholarlyArticle",
            "paperId": "35c599bd30ea3ac0291c06e81cbc5c73c7816c32",
            "corpusId": 11469828,
            "url": "https://www.semanticscholar.org/paper/35c599bd30ea3ac0291c06e81cbc5c73c7816c32",
            "title": "Multi-criteria Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2117428849",
                "DBLP": "conf/icml/GaborKS98",
                "CorpusId": 11469828
            },
            "abstract": "Ve cOllf:iider multi- criteria f:iequent,ial decision making problems where the vcctor-\"valucd evaluations arc compared by a given, fixed total ordering. Condit.ions for the opt.irnality of statiOIl<-l,r}' p()lichs ;-weI the Bellman opti\u00ad malit,y equatio n a re given for a. speci al, but. important class of problems ''v hell the eval\u00ad uation of policies can be computed for the criteria, independently of each other. The anal)'sis requires special cafC as t.he t.opol\u00ad ag,Y int.roduced by polnL\\visc convergence a.ncl the or<1cr- topology introduced by the prefer\u00ad ence order arc in general incompatible. Re\u00ad inf orcement. learning algorithms are proposed and analY7,ed.",
            "referenceCount": 47,
            "citationCount": 78,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1998-07-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{G\u00e1bor1998MulticriteriaRL,\n author = {Zolt\u00e1n G\u00e1bor and Z. Kalm\u00e1r and Csaba Szepesvari},\n booktitle = {International Conference on Machine Learning},\n pages = {197-205},\n title = {Multi-criteria Reinforcement Learning},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:60922d2ca51acbebff794f4c43f6daadf4b8d103",
            "@type": "ScholarlyArticle",
            "paperId": "60922d2ca51acbebff794f4c43f6daadf4b8d103",
            "corpusId": 17045473,
            "url": "https://www.semanticscholar.org/paper/60922d2ca51acbebff794f4c43f6daadf4b8d103",
            "title": "Reinforcement Learning with Factored States and Actions",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2145805610",
                "DBLP": "journals/jmlr/SallansH04",
                "CorpusId": 17045473
            },
            "abstract": "A novel approximation method is presented for approximating the value function and selecting good actions for Markov decision processes with large state and action spaces. The method approximates state-action values as negative free energies in an undirected graphical model called a product of experts. The model parameters can be learned efficiently because values and derivatives can be efficiently computed for a product of experts. Actions can be found even in large factored action spaces by the use of Markov chain Monte Carlo sampling. Simulation results show that the product of experts approximation can be used to solve large problems. In one simulation it is used to find actions in action spaces of size 240.",
            "referenceCount": 59,
            "citationCount": 168,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2004-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Sallans2004ReinforcementLW,\n author = {B. Sallans and Geoffrey E. Hinton},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1063-1088},\n title = {Reinforcement Learning with Factored States and Actions},\n volume = {5},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d3da3e6a6c76868e0178adf149ef96d4e99a620d",
            "@type": "ScholarlyArticle",
            "paperId": "d3da3e6a6c76868e0178adf149ef96d4e99a620d",
            "corpusId": 43766241,
            "url": "https://www.semanticscholar.org/paper/d3da3e6a6c76868e0178adf149ef96d4e99a620d",
            "title": "A Teaching Method for Reinforcement Learning",
            "venue": "ML Workshop",
            "publicationVenue": {
                "id": "urn:research:9870ecbf-4ccb-4444-b6d2-518744221ef1",
                "name": "ML Workshop",
                "alternate_names": [
                    "ML",
                    "ML Workshop"
                ],
                "issn": null,
                "url": null
            },
            "year": 1992,
            "externalIds": {
                "MAG": "1497976081",
                "DBLP": "conf/icml/ClouseU92",
                "DOI": "10.1016/b978-1-55860-247-2.50017-6",
                "CorpusId": 43766241
            },
            "abstract": null,
            "referenceCount": 10,
            "citationCount": 148,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1992-07-07",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Clouse1992ATM,\n author = {J. Clouse and P. Utgoff},\n booktitle = {ML Workshop},\n pages = {92-110},\n title = {A Teaching Method for Reinforcement Learning},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fb30e650971451f372029d26800b06bf1783572a",
            "@type": "ScholarlyArticle",
            "paperId": "fb30e650971451f372029d26800b06bf1783572a",
            "corpusId": 63015837,
            "url": "https://www.semanticscholar.org/paper/fb30e650971451f372029d26800b06bf1783572a",
            "title": "Dimensions of Reinforcement Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2545988881",
                "CorpusId": 63015837
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 121,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Sutton1998DimensionsOR,\n author = {R. Sutton and A. Barto},\n pages = {255-260},\n title = {Dimensions of Reinforcement Learning},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:98a0a8f24b21f37869402635f7becfb08f46c803",
            "@type": "ScholarlyArticle",
            "paperId": "98a0a8f24b21f37869402635f7becfb08f46c803",
            "corpusId": 53917211,
            "url": "https://www.semanticscholar.org/paper/98a0a8f24b21f37869402635f7becfb08f46c803",
            "title": "Reinforcement learning architectures for animats",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1991,
            "externalIds": {
                "MAG": "1499697567",
                "DOI": "10.7551/mitpress/3115.003.0040",
                "CorpusId": 53917211
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 209,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Art",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1991-02-14",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Sutton1991ReinforcementLA,\n author = {R. Sutton},\n pages = {288-296},\n title = {Reinforcement learning architectures for animats},\n year = {1991}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8f7cd02988a7178c8d9e2be70ef9af29bafe0e18",
            "@type": "ScholarlyArticle",
            "paperId": "8f7cd02988a7178c8d9e2be70ef9af29bafe0e18",
            "corpusId": 141049233,
            "url": "https://www.semanticscholar.org/paper/8f7cd02988a7178c8d9e2be70ef9af29bafe0e18",
            "title": "Reinforcement learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2487464021",
                "DOI": "10.1016/s1474-6670(17)38315-5",
                "CorpusId": 141049233
            },
            "abstract": null,
            "referenceCount": 35,
            "citationCount": 213,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1998-10-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Barto1998ReinforcementL,\n author = {A. Barto},\n pages = {804-809},\n title = {Reinforcement learning},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f5d5a699770808228a2de1b5f99e76ce4bd2b9ee",
            "@type": "ScholarlyArticle",
            "paperId": "f5d5a699770808228a2de1b5f99e76ce4bd2b9ee",
            "corpusId": 115821126,
            "url": "https://www.semanticscholar.org/paper/f5d5a699770808228a2de1b5f99e76ce4bd2b9ee",
            "title": "Consideration of Risk in Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1994,
            "externalIds": {
                "DBLP": "conf/icml/Heger94",
                "MAG": "134786152",
                "DOI": "10.1016/b978-1-55860-335-6.50021-0",
                "CorpusId": 115821126
            },
            "abstract": null,
            "referenceCount": 11,
            "citationCount": 208,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Heger1994ConsiderationOR,\n author = {M. Heger},\n booktitle = {International Conference on Machine Learning},\n pages = {105-111},\n title = {Consideration of Risk in Reinforcement Learning},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:171b9d5f0f29388c1e638de55aed3d19c3524df5",
            "@type": "ScholarlyArticle",
            "paperId": "171b9d5f0f29388c1e638de55aed3d19c3524df5",
            "corpusId": 60745539,
            "url": "https://www.semanticscholar.org/paper/171b9d5f0f29388c1e638de55aed3d19c3524df5",
            "title": "Reinforcement learning with hidden states",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1993,
            "externalIds": {
                "MAG": "1499371387",
                "DOI": "10.7551/mitpress/3116.003.0038",
                "CorpusId": 60745539
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 132,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1993-08-09",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Lin1993ReinforcementLW,\n author = {Longxin Lin and Tom Michael Mitchell},\n pages = {271-280},\n title = {Reinforcement learning with hidden states},\n year = {1993}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c46d80f83813fba0e8363a0ab36a19fba062540e",
            "@type": "ScholarlyArticle",
            "paperId": "c46d80f83813fba0e8363a0ab36a19fba062540e",
            "corpusId": 53776053,
            "url": "https://www.semanticscholar.org/paper/c46d80f83813fba0e8363a0ab36a19fba062540e",
            "title": "Learning Actionable Representations with Goal-Conditioned Policies",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963321092",
                "DBLP": "journals/corr/abs-1811-07819",
                "ArXiv": "1811.07819",
                "CorpusId": 53776053
            },
            "abstract": "Representation learning is a central challenge across a range of machine learning areas. In reinforcement learning, effective and functional representations have the potential to tremendously accelerate learning progress and solve more challenging problems. Most prior work on representation learning has focused on generative approaches, learning representations that capture all underlying factors of variation in the observation space in a more disentangled or well-ordered manner. In this paper, we instead aim to learn functionally salient representations: representations that are not necessarily complete in terms of capturing all factors of variation in the observation space, but rather aim to capture those factors of variation that are important for decision making -- that are \"actionable.\" These representations are aware of the dynamics of the environment, and capture only the elements of the observation that are necessary for decision making rather than all factors of variation, without explicit reconstruction of the observation. We show how these representations can be useful to improve exploration for sparse reward problems, to enable long horizon hierarchical reinforcement learning, and as a state representation for learning policies for downstream tasks. We evaluate our method on a number of simulated environments, and compare it to prior methods for representation learning, exploration, and hierarchical reinforcement learning.",
            "referenceCount": 44,
            "citationCount": 95,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1811.07819"
            },
            "citationStyles": {
                "bibtex": "@Article{Ghosh2018LearningAR,\n author = {Dibya Ghosh and Abhishek Gupta and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning Actionable Representations with Goal-Conditioned Policies},\n volume = {abs/1811.07819},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a74d52915f3c9f45d6c66a29f6d17d27d2c3d745",
            "@type": "ScholarlyArticle",
            "paperId": "a74d52915f3c9f45d6c66a29f6d17d27d2c3d745",
            "corpusId": 10541508,
            "url": "https://www.semanticscholar.org/paper/a74d52915f3c9f45d6c66a29f6d17d27d2c3d745",
            "title": "Learning to Learn: Meta-Critic Networks for Sample Efficient Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2726717203",
                "DBLP": "journals/corr/SungZXHY17",
                "ArXiv": "1706.09529",
                "CorpusId": 10541508
            },
            "abstract": "We propose a novel and flexible approach to meta-learning for learning-to-learn from only a few examples. Our framework is motivated by actor-critic reinforcement learning, but can be applied to both reinforcement and supervised learning. The key idea is to learn a meta-critic: an action-value function neural network that learns to criticise any actor trying to solve any specified task. For supervised learning, this corresponds to the novel idea of a trainable task-parametrised loss generator. This meta-critic approach provides a route to knowledge transfer that can flexibly deal with few-shot and semi-supervised conditions for both reinforcement and supervised learning. Promising results are shown on both reinforcement and supervised learning problems.",
            "referenceCount": 41,
            "citationCount": 118,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-06-29",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1706.09529"
            },
            "citationStyles": {
                "bibtex": "@Article{Sung2017LearningTL,\n author = {Flood Sung and Li Zhang and T. Xiang and Timothy M. Hospedales and Yongxin Yang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Learning to Learn: Meta-Critic Networks for Sample Efficient Learning},\n volume = {abs/1706.09529},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5d1ec3b42a4ab0def4ea522c38c7640263355f32",
            "@type": "ScholarlyArticle",
            "paperId": "5d1ec3b42a4ab0def4ea522c38c7640263355f32",
            "corpusId": 11669331,
            "url": "https://www.semanticscholar.org/paper/5d1ec3b42a4ab0def4ea522c38c7640263355f32",
            "title": "Online learning control by association and reinforcement",
            "venue": "Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2969049402",
                "DBLP": "journals/tnn/SiW01",
                "DOI": "10.1109/IJCNN.2000.861307",
                "CorpusId": 11669331,
                "PubMed": "18244383"
            },
            "abstract": "This paper focuses on a systematic treatment for developing a generic online learning control system based on the fundamental principle of reinforcement learning or more specifically neuro-dynamic programming. This real time learning system improves its performance over time in two aspects: it learns from its own mistakes through the reinforcement signal from the external environment and try to reinforce its action to improve future performance; and system's state associated with the positive reinforcement is memorized through a network learning process where in the future, similar states will be more positively associated with a control action leading to a positive reinforcement. Two successful candidates of online learning control designs are introduced. Real time learning algorithms can be derived for individual components in the learning system. Some analytical insights are provided to give some guidelines on the entire online learning control system.",
            "referenceCount": 26,
            "citationCount": 738,
            "influentialCitationCount": 62,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2001-03-01",
            "journal": {
                "name": "Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Si2001OnlineLC,\n author = {J. Si and Yu-Tsung Wang},\n booktitle = {Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium},\n journal = {Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium},\n pages = {221-226 vol.3},\n title = {Online learning control by association and reinforcement},\n volume = {3},\n year = {2001}\n}\n"
            }
        }
    }
]