[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:82a262a2034b349abaa720c7f8229a0ef19e87cd",
            "@type": "ScholarlyArticle",
            "paperId": "82a262a2034b349abaa720c7f8229a0ef19e87cd",
            "corpusId": 49546141,
            "url": "https://www.semanticscholar.org/paper/82a262a2034b349abaa720c7f8229a0ef19e87cd",
            "title": "RLlib: Abstractions for Distributed Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2949532984",
                "DBLP": "conf/icml/LiangLNMFGGJS18",
                "CorpusId": 49546141
            },
            "abstract": "Reinforcement learning (RL) algorithms involve the deep nesting of highly irregular computation patterns, each of which typically exhibits opportunities for distributed computation. We argue for distributing RL components in a composable way by adapting algorithms for top-down hierarchical control, thereby encapsulating parallelism and resource requirements within short-running compute tasks. We demonstrate the benefits of this principle through RLlib: a library that provides scalable software primitives for RL. These primitives enable a broad range of algorithms to be implemented with high performance, scalability, and substantial code reuse. RLlib is available at this https URL.",
            "referenceCount": 42,
            "citationCount": 632,
            "influentialCitationCount": 89,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-12-26",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Liang2017RLlibAF,\n author = {Eric Liang and Richard Liaw and Robert Nishihara and Philipp Moritz and Roy Fox and Ken Goldberg and Joseph E. Gonzalez and Michael I. Jordan and I. Stoica},\n booktitle = {International Conference on Machine Learning},\n pages = {3059-3068},\n title = {RLlib: Abstractions for Distributed Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dee85f0ed3571d7b591e23000848c584242186ef",
            "@type": "ScholarlyArticle",
            "paperId": "dee85f0ed3571d7b591e23000848c584242186ef",
            "corpusId": 3952962,
            "url": "https://www.semanticscholar.org/paper/dee85f0ed3571d7b591e23000848c584242186ef",
            "title": "Composable Deep Reinforcement Learning for Robotic Manipulation",
            "venue": "IEEE International Conference on Robotics and Automation",
            "publicationVenue": {
                "id": "urn:research:3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                "name": "IEEE International Conference on Robotics and Automation",
                "alternate_names": [
                    "International Conference on Robotics and Automation",
                    "Int Conf Robot Autom",
                    "ICRA",
                    "IEEE Int Conf Robot Autom"
                ],
                "issn": "2152-4092",
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963403593",
                "ArXiv": "1803.06773",
                "DBLP": "conf/icra/HaarnojaPZDAL18",
                "DOI": "10.1109/ICRA.2018.8460756",
                "CorpusId": 3952962
            },
            "abstract": "Model-free deep reinforcement learning has been shown to exhibit good performance in domains ranging from video games to simulated robotic manipulation and locomotion. However, model-free methods are known to perform poorly when the interaction time with the environment is limited, as is the case for most real-world robotic tasks. In this paper, we study how maximum entropy policies trained using soft Q-learning can be applied to real-world robotic manipulation. The application of this method to real-world manipulation is facilitated by two important features of soft Q-learning. First, soft Q-learning can learn multimodal exploration strategies by learning policies represented by expressive energy-based models. Second, we show that policies learned with soft Q-learning can be composed to create new policies, and that the optimality of the resulting policy can be bounded in terms of the divergence between the composed policies. This compositionality provides an especially valuable tool for real-world manipulation, where constructing new policies by composing existing skills can provide a large gain in efficiency over training from scratch. Our experimental evaluation demonstrates that soft Q-learning is substantially more sample efficient than prior model-free deep reinforcement learning methods, and that compositionality can be performed for both simulated and real-world tasks.",
            "referenceCount": 37,
            "citationCount": 194,
            "influentialCitationCount": 26,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1803.06773",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-03-19",
            "journal": {
                "name": "2018 IEEE International Conference on Robotics and Automation (ICRA)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Haarnoja2018ComposableDR,\n author = {Tuomas Haarnoja and Vitchyr H. Pong and Aurick Zhou and Murtaza Dalal and P. Abbeel and S. Levine},\n booktitle = {IEEE International Conference on Robotics and Automation},\n journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},\n pages = {6244-6251},\n title = {Composable Deep Reinforcement Learning for Robotic Manipulation},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9172cd6c253edf7c3a1568e03577db20648ad0c4",
            "@type": "ScholarlyArticle",
            "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
            "corpusId": 11227891,
            "url": "https://www.semanticscholar.org/paper/9172cd6c253edf7c3a1568e03577db20648ad0c4",
            "title": "Reinforcement Learning with Deep Energy-Based Policies",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2594103415",
                "ArXiv": "1702.08165",
                "DBLP": "conf/icml/HaarnojaTAL17",
                "CorpusId": 11227891
            },
            "abstract": "We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.",
            "referenceCount": 52,
            "citationCount": 1031,
            "influentialCitationCount": 164,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-02-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Haarnoja2017ReinforcementLW,\n author = {Tuomas Haarnoja and Haoran Tang and P. Abbeel and S. Levine},\n booktitle = {International Conference on Machine Learning},\n pages = {1352-1361},\n title = {Reinforcement Learning with Deep Energy-Based Policies},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6127b8dc39497a2388a0fce2512595e0dcb7121b",
            "@type": "ScholarlyArticle",
            "paperId": "6127b8dc39497a2388a0fce2512595e0dcb7121b",
            "corpusId": 28808621,
            "url": "https://www.semanticscholar.org/paper/6127b8dc39497a2388a0fce2512595e0dcb7121b",
            "title": "StarCraft II: A New Challenge for Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2749807327",
                "ArXiv": "1708.04782",
                "DBLP": "journals/corr/abs-1708-04782",
                "CorpusId": 28808621
            },
            "abstract": "This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.",
            "referenceCount": 39,
            "citationCount": 726,
            "influentialCitationCount": 83,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-08-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1708.04782"
            },
            "citationStyles": {
                "bibtex": "@Article{Vinyals2017StarCraftIA,\n author = {Oriol Vinyals and Timo Ewalds and Sergey Bartunov and Petko Georgiev and A. Vezhnevets and Michelle Yeo and Alireza Makhzani and Heinrich K\u00fcttler and J. Agapiou and Julian Schrittwieser and John Quan and Stephen Gaffney and Stig Petersen and K. Simonyan and T. Schaul and H. V. Hasselt and David Silver and T. Lillicrap and Kevin Calderone and Paul Keet and Anthony Brunasso and David Lawrence and Anders Ekermo and J. Repp and Rodney Tsing},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {StarCraft II: A New Challenge for Reinforcement Learning},\n volume = {abs/1708.04782},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0732925ffaf7ee525a57786f7c9791491ef084db",
            "@type": "ScholarlyArticle",
            "paperId": "0732925ffaf7ee525a57786f7c9791491ef084db",
            "corpusId": 59604361,
            "url": "https://www.semanticscholar.org/paper/0732925ffaf7ee525a57786f7c9791491ef084db",
            "title": "Learning to Schedule Communication in Multi-agent Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/iclr/KimMHKLSY19",
                "MAG": "2907606902",
                "ArXiv": "1902.01554",
                "CorpusId": 59604361
            },
            "abstract": "Many real-world reinforcement learning tasks require multiple agents to make sequential decisions under the agents' interaction, where well-coordinated actions among the agents are crucial to achieve the target goal better at these tasks. One way to accelerate the coordination effect is to enable multiple agents to communicate with each other in a distributed manner and behave as a group. In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication medium so that only a restricted number of agents are able to simultaneously use the medium, as in the state-of-the-art wireless networking standards. This calls for a certain form of communication scheduling. In that regard, we propose a multi-agent deep reinforcement learning framework, called SchedNet, in which agents learn how to schedule themselves, how to encode the messages, and how to select actions based on received messages. SchedNet is capable of deciding which agents should be entitled to broadcasting their (encoded) messages, by learning the importance of each agent's partially observed information. We evaluate SchedNet against multiple baselines under two different applications, namely, cooperative communication and navigation, and predator-prey. Our experiments show a non-negligible performance gap between SchedNet and other mechanisms such as the ones without communication and with vanilla scheduling methods, e.g., round robin, ranging from 32% to 43%.",
            "referenceCount": 44,
            "citationCount": 147,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-02-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1902.01554"
            },
            "citationStyles": {
                "bibtex": "@Article{Kim2019LearningTS,\n author = {Daewoo Kim and Sang-chul Moon and D. Hostallero and Wan Ju Kang and Taeyoung Lee and Kyunghwan Son and Yung Yi},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning to Schedule Communication in Multi-agent Reinforcement Learning},\n volume = {abs/1902.01554},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:47b6493e8ca3ea6a26f0180dd2f639b5aeaa5088",
            "@type": "ScholarlyArticle",
            "paperId": "47b6493e8ca3ea6a26f0180dd2f639b5aeaa5088",
            "corpusId": 88499156,
            "url": "https://www.semanticscholar.org/paper/47b6493e8ca3ea6a26f0180dd2f639b5aeaa5088",
            "title": "Reinforcement learning with analogue memristor arrays",
            "venue": "Nature Electronics",
            "publicationVenue": {
                "id": "urn:research:72753051-64d8-439b-a76f-c70555ac654d",
                "name": "Nature Electronics",
                "alternate_names": [
                    "Nat Electron"
                ],
                "issn": "2520-1131",
                "url": "https://www.nature.com/natelectron/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2922168646",
                "DOI": "10.1038/S41928-019-0221-6",
                "CorpusId": 88499156
            },
            "abstract": null,
            "referenceCount": 49,
            "citationCount": 223,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2019-03-01",
            "journal": {
                "name": "Nature Electronics",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2019ReinforcementLW,\n author = {Zhongrui Wang and Can Li and Wenhao Song and Mingyi Rao and Daniel Belkin and Yunning Li and Peng Yan and Hao Jiang and P. Lin and Miao Hu and J. Strachan and Ning Ge and Mark D. Barnell and Qing Wu and A. Barto and Qinru Qiu and R. S. Williams and Q. Xia and J. Yang},\n booktitle = {Nature Electronics},\n journal = {Nature Electronics},\n pages = {115-124},\n title = {Reinforcement learning with analogue memristor arrays},\n volume = {2},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:951af7222535d934ca2b401ca0cd2181b28284f9",
            "@type": "ScholarlyArticle",
            "paperId": "951af7222535d934ca2b401ca0cd2181b28284f9",
            "corpusId": 189473795,
            "url": "https://www.semanticscholar.org/paper/951af7222535d934ca2b401ca0cd2181b28284f9",
            "title": "Reinforcement learning in artificial and biological systems",
            "venue": "Nature Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:6457124b-39bf-4d02-bff4-73752ff21562",
                "name": "Nature Machine Intelligence",
                "alternate_names": [
                    "Nat Mach Intell"
                ],
                "issn": "2522-5839",
                "url": "https://www.nature.com/natmachintell/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2919912236",
                "DOI": "10.1038/S42256-019-0025-4",
                "CorpusId": 189473795
            },
            "abstract": null,
            "referenceCount": 127,
            "citationCount": 168,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s42256-019-0025-4.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2019-03-01",
            "journal": {
                "name": "Nature Machine Intelligence",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Neftci2019ReinforcementLI,\n author = {E. Neftci and B. Averbeck},\n booktitle = {Nature Machine Intelligence},\n journal = {Nature Machine Intelligence},\n pages = {133-143},\n title = {Reinforcement learning in artificial and biological systems},\n volume = {1},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3606c313dccd21f784bedf5759152a3677f5719d",
            "@type": "ScholarlyArticle",
            "paperId": "3606c313dccd21f784bedf5759152a3677f5719d",
            "corpusId": 199543783,
            "url": "https://www.semanticscholar.org/paper/3606c313dccd21f784bedf5759152a3677f5719d",
            "title": "Behaviour Suite for Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1908-03568",
                "MAG": "2967210407",
                "ArXiv": "1908.03568",
                "CorpusId": 199543783
            },
            "abstract": "This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks. To complement this effort, we open source this http URL, which automates evaluation and analysis of any agent on bsuite. This library facilitates reproducible and accessible research on the core issues in RL, and ultimately the design of superior learning algorithms. Our code is Python, and easy to use within existing projects. We include examples with OpenAI Baselines, Dopamine as well as new reference implementations. Going forward, we hope to incorporate more excellent experiments from the research community, and commit to a periodic review of bsuite from a committee of prominent researchers.",
            "referenceCount": 70,
            "citationCount": 146,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-08-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1908.03568"
            },
            "citationStyles": {
                "bibtex": "@Article{Osband2019BehaviourSF,\n author = {Ian Osband and Yotam Doron and Matteo Hessel and J. Aslanides and Eren Sezener and Andre Saraiva and Katrina McKinney and Tor Lattimore and Csaba Szepesvari and Satinder Singh and Benjamin Van Roy and R. Sutton and David Silver and H. V. Hasselt},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Behaviour Suite for Reinforcement Learning},\n volume = {abs/1908.03568},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d37620e6f8fe678a43e12930743281cd8cca6a66",
            "@type": "ScholarlyArticle",
            "paperId": "d37620e6f8fe678a43e12930743281cd8cca6a66",
            "corpusId": 4669377,
            "url": "https://www.semanticscholar.org/paper/d37620e6f8fe678a43e12930743281cd8cca6a66",
            "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2950799402",
                "ArXiv": "1604.06057",
                "DBLP": "conf/nips/KulkarniNST16",
                "CorpusId": 4669377
            },
            "abstract": "Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.",
            "referenceCount": 56,
            "citationCount": 980,
            "influentialCitationCount": 87,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-04-20",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1604.06057"
            },
            "citationStyles": {
                "bibtex": "@Article{Kulkarni2016HierarchicalDR,\n author = {Tejas D. Kulkarni and Karthik Narasimhan and A. Saeedi and J. Tenenbaum},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation},\n volume = {abs/1604.06057},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1ab7aa767e1779c87d822325859e47fe2986e6b2",
            "@type": "ScholarlyArticle",
            "paperId": "1ab7aa767e1779c87d822325859e47fe2986e6b2",
            "corpusId": 207244918,
            "url": "https://www.semanticscholar.org/paper/1ab7aa767e1779c87d822325859e47fe2986e6b2",
            "title": "Resource Management with Deep Reinforcement Learning",
            "venue": "ACM Workshop on Hot Topics in Networks",
            "publicationVenue": {
                "id": "urn:research:89d61e17-5d06-4d45-a0d0-c7b62b0a35b4",
                "name": "ACM Workshop on Hot Topics in Networks",
                "alternate_names": [
                    "HotNets",
                    "ACM Workshop Hot Top Netw",
                    "Hot Top Netw",
                    "Hot Topics in Networks"
                ],
                "issn": null,
                "url": "http://www.sigcomm.org/events/hotnets-workshop"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/hotnets/MaoAMK16",
                "MAG": "2546571074",
                "DOI": "10.1145/3005745.3005750",
                "CorpusId": 207244918
            },
            "abstract": "Resource management problems in systems and networking often manifest as difficult online decision making tasks where appropriate solutions depend on understanding the workload and environment. Inspired by recent advances in deep reinforcement learning for AI problems, we consider building systems that learn to manage resources directly from experience. We present DeepRM, an example solution that translates the problem of packing tasks with multiple resource demands into a learning problem. Our initial results show that DeepRM performs comparably to state-of-the-art heuristics, adapts to different conditions, converges quickly, and learns strategies that are sensible in hindsight.",
            "referenceCount": 42,
            "citationCount": 909,
            "influentialCitationCount": 91,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle"
            ],
            "publicationDate": "2016-11-09",
            "journal": {
                "name": "Proceedings of the 15th ACM Workshop on Hot Topics in Networks",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Mao2016ResourceMW,\n author = {Hongzi Mao and Mohammad Alizadeh and Ishai Menache and Srikanth Kandula},\n booktitle = {ACM Workshop on Hot Topics in Networks},\n journal = {Proceedings of the 15th ACM Workshop on Hot Topics in Networks},\n title = {Resource Management with Deep Reinforcement Learning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d7bd6e3addd8bc8e2e154048300eea15f030ed33",
            "@type": "ScholarlyArticle",
            "paperId": "d7bd6e3addd8bc8e2e154048300eea15f030ed33",
            "corpusId": 14717992,
            "url": "https://www.semanticscholar.org/paper/d7bd6e3addd8bc8e2e154048300eea15f030ed33",
            "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1611.05397",
                "DBLP": "conf/iclr/JaderbergMCSLSK17",
                "MAG": "2950872548",
                "CorpusId": 14717992
            },
            "abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880\\% expert human performance, and a challenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks leading to a mean speedup in learning of 10$\\times$ and averaging 87\\% expert human performance on Labyrinth.",
            "referenceCount": 34,
            "citationCount": 1123,
            "influentialCitationCount": 88,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.05397"
            },
            "citationStyles": {
                "bibtex": "@Article{Jaderberg2016ReinforcementLW,\n author = {Max Jaderberg and Volodymyr Mnih and Wojciech M. Czarnecki and T. Schaul and Joel Z. Leibo and David Silver and K. Kavukcuoglu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Reinforcement Learning with Unsupervised Auxiliary Tasks},\n volume = {abs/1611.05397},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4b27b8b28b0959989e144ac7273aacfe05267cf8",
            "@type": "ScholarlyArticle",
            "paperId": "4b27b8b28b0959989e144ac7273aacfe05267cf8",
            "corpusId": 208248040,
            "url": "https://www.semanticscholar.org/paper/4b27b8b28b0959989e144ac7273aacfe05267cf8",
            "title": "Deep Reinforcement Learning for Trading",
            "venue": "The Journal of Financial Data Science",
            "publicationVenue": {
                "id": "urn:research:416e012e-addf-4119-9545-20281520b489",
                "name": "The Journal of Financial Data Science",
                "alternate_names": [
                    "J Financial Data Sci"
                ],
                "issn": "2640-3943",
                "url": "https://jfds.pm-research.com/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1911.10107",
                "MAG": "2991549572",
                "DBLP": "journals/corr/abs-1911-10107",
                "DOI": "10.3905/jfds.2020.1.030",
                "CorpusId": 208248040
            },
            "abstract": "In this article, the authors adopt deep reinforcement learning algorithms to design trading strategies for continuous futures contracts. Both discrete and continuous action spaces are considered, and volatility scaling is incorporated to create reward functions that scale trade positions based on market volatility. They test their algorithms on 50 very liquid futures contracts from 2011 to 2019 and investigate how performance varies across different asset classes, including commodities, equity indexes, fixed income, and foreign exchange markets. They compare their algorithms against classical time-series momentum strategies and show that their method outperforms such baseline models, delivering positive profits despite heavy transaction costs. The experiments show that the proposed algorithms can follow large market trends without changing positions and can also scale down, or hold, through consolidation periods. TOPICS: Futures and forward contracts, exchanges/markets/clearinghouses, statistical methods, simulations Key Findings \u2022 In this article, the authors introduce reinforcement learning algorithms to design trading strategies for futures contracts. They investigate both discrete and continuous action spaces and improve reward functions by using volatility scaling to scale trade positions based on market volatility. \u2022 The authors discuss the connection between modern portfolio theory and the reinforcement learning reward hypothesis and show that they are equivalent if a linear utility function is used. \u2022 The authors back test their methods on 50 very liquid futures contracts from 2011 to 2019, and their algorithms deliver positive profits despite heavy transaction costs.",
            "referenceCount": 65,
            "citationCount": 145,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jfds.pm-research.com/content/iijjfds/2/2/25.full.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Economics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Economics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Business",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-11-22",
            "journal": {
                "name": null,
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2019DeepRL,\n author = {Zihao Zhang and S. Zohren and Stephen J. Roberts},\n booktitle = {The Journal of Financial Data Science},\n pages = {25 - 40},\n title = {Deep Reinforcement Learning for Trading},\n volume = {2},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e010ba3ff5744604cdbfe44a733e2a98649ee907",
            "@type": "ScholarlyArticle",
            "paperId": "e010ba3ff5744604cdbfe44a733e2a98649ee907",
            "corpusId": 4780901,
            "url": "https://www.semanticscholar.org/paper/e010ba3ff5744604cdbfe44a733e2a98649ee907",
            "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations",
            "venue": "Robotics: Science and Systems",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2963411833",
                "DBLP": "journals/corr/abs-1709-10087",
                "ArXiv": "1709.10087",
                "DOI": "10.15607/RSS.2018.XIV.049",
                "CorpusId": 4780901
            },
            "abstract": "Dexterous multi-fingered hands are extremely versatile and provide a generic way to perform multiple tasks in human-centric environments. However, effectively controlling them remains challenging due to their high dimensionality and large number of potential contacts. Deep reinforcement learning (DRL) provides a model-agnostic approach to control complex dynamical systems, but has not been shown to scale to high-dimensional dexterous manipulation. Furthermore, deployment of DRL on physical systems remains challenging due to sample inefficiency. Thus, the success of DRL in robotics has thus far been limited to simpler manipulators and tasks. In this work, we show that model-free DRL with natural policy gradients can effectively scale up to complex manipulation tasks with a high-dimensional 24-DoF hand, and solve them from scratch in simulated experiments. Furthermore, with the use of a small number of human demonstrations, the sample complexity can be significantly reduced, and enable learning within the equivalent of a few hours of robot experience. We demonstrate successful policies for multiple complex tasks: object relocation, in-hand manipulation, tool use, and door opening.",
            "referenceCount": 62,
            "citationCount": 790,
            "influentialCitationCount": 81,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.15607/rss.2018.xiv.049",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-09-28",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1709.10087"
            },
            "citationStyles": {
                "bibtex": "@Article{Rajeswaran2017LearningCD,\n author = {A. Rajeswaran and Vikash Kumar and Abhishek Gupta and J. Schulman and E. Todorov and S. Levine},\n booktitle = {Robotics: Science and Systems},\n journal = {ArXiv},\n title = {Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations},\n volume = {abs/1709.10087},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9c4082bfbd46b781e70657f14895306c57c842e3",
            "@type": "ScholarlyArticle",
            "paperId": "9c4082bfbd46b781e70657f14895306c57c842e3",
            "corpusId": 15313471,
            "url": "https://www.semanticscholar.org/paper/9c4082bfbd46b781e70657f14895306c57c842e3",
            "title": "Robust Adversarial Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2602963933",
                "DBLP": "journals/corr/PintoDSG17",
                "ArXiv": "1703.02702",
                "CorpusId": 15313471
            },
            "abstract": "Deep neural networks coupled with fast simulation and improved computation have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H\u221e control methods, we note that both modeling errors and differences in training and test scenarios can be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced - that is, it learns an optimal destabilization policy. We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper, Walker2d and Ant) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary.",
            "referenceCount": 43,
            "citationCount": 657,
            "influentialCitationCount": 73,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Pinto2017RobustAR,\n author = {Lerrel Pinto and James Davidson and R. Sukthankar and A. Gupta},\n booktitle = {International Conference on Machine Learning},\n pages = {2817-2826},\n title = {Robust Adversarial Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
            "@type": "ScholarlyArticle",
            "paperId": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
            "corpusId": 3543784,
            "url": "https://www.semanticscholar.org/paper/c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
            "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations",
            "venue": "IEEE International Conference on Robotics and Automation",
            "publicationVenue": {
                "id": "urn:research:3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                "name": "IEEE International Conference on Robotics and Automation",
                "alternate_names": [
                    "International Conference on Robotics and Automation",
                    "Int Conf Robot Autom",
                    "ICRA",
                    "IEEE Int Conf Robot Autom"
                ],
                "issn": "2152-4092",
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1709.10089",
                "MAG": "2756826236",
                "DBLP": "journals/corr/abs-1709-10089",
                "DOI": "10.1109/ICRA.2018.8463162",
                "CorpusId": 3543784
            },
            "abstract": "Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.",
            "referenceCount": 41,
            "citationCount": 633,
            "influentialCitationCount": 75,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1709.10089",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-09-28",
            "journal": {
                "name": "2018 IEEE International Conference on Robotics and Automation (ICRA)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nair2017OvercomingEI,\n author = {Ashvin Nair and Bob McGrew and Marcin Andrychowicz and Wojciech Zaremba and P. Abbeel},\n booktitle = {IEEE International Conference on Robotics and Automation},\n journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},\n pages = {6292-6299},\n title = {Overcoming Exploration in Reinforcement Learning with Demonstrations},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:25a7b8c2e110a1bcd5c42ab5de55a0c08b0b8846",
            "@type": "ScholarlyArticle",
            "paperId": "25a7b8c2e110a1bcd5c42ab5de55a0c08b0b8846",
            "corpusId": 201314465,
            "url": "https://www.semanticscholar.org/paper/25a7b8c2e110a1bcd5c42ab5de55a0c08b0b8846",
            "title": "Double Reinforcement Learning for Efficient Off-Policy Evaluation in Markov Decision Processes",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/jmlr/KallusU20",
                "MAG": "2969337892",
                "ArXiv": "1908.08526",
                "CorpusId": 201314465
            },
            "abstract": "Off-policy evaluation (OPE) in reinforcement learning allows one to evaluate novel decision policies without needing to conduct exploration, which is often costly or otherwise infeasible. We consider for the first time the semiparametric efficiency limits of OPE in Markov decision processes (MDPs), where actions, rewards, and states are memoryless. We show existing OPE estimators may fail to be efficient in this setting. We develop a new estimator based on cross-fold estimation of $q$-functions and marginalized density ratios, which we term double reinforcement learning (DRL). We show that DRL is efficient when both components are estimated at fourth-root rates and is also doubly robust when only one component is consistent. We investigate these properties empirically and demonstrate the performance benefits due to harnessing memorylessness.",
            "referenceCount": 80,
            "citationCount": 138,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-08-22",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "21"
            },
            "citationStyles": {
                "bibtex": "@Article{Kallus2019DoubleRL,\n author = {Nathan Kallus and Masatoshi Uehara},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {167:1-167:63},\n title = {Double Reinforcement Learning for Efficient Off-Policy Evaluation in Markov Decision Processes},\n volume = {21},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0d9595193c6b87494d168b1bcc03c4ad10642c72",
            "@type": "ScholarlyArticle",
            "paperId": "0d9595193c6b87494d168b1bcc03c4ad10642c72",
            "corpusId": 240596253,
            "url": "https://www.semanticscholar.org/paper/0d9595193c6b87494d168b1bcc03c4ad10642c72",
            "title": "Reinforcement Learning and Deep Reinforcement Learning",
            "venue": "Deep Learning in Science",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "MAG": "3201629542",
                "DOI": "10.1017/9781108955652.016",
                "CorpusId": 240596253
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2021-07-01",
            "journal": {
                "name": "Deep Learning in Science",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Baldi2021ReinforcementLA,\n author = {P. Baldi},\n booktitle = {Deep Learning in Science},\n journal = {Deep Learning in Science},\n title = {Reinforcement Learning and Deep Reinforcement Learning},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9d01ca964043a52b265534e94abc37210a50cc5a",
            "@type": "ScholarlyArticle",
            "paperId": "9d01ca964043a52b265534e94abc37210a50cc5a",
            "corpusId": 208006292,
            "url": "https://www.semanticscholar.org/paper/9d01ca964043a52b265534e94abc37210a50cc5a",
            "title": "Kinematic State Abstraction and Provably Efficient Rich-Observation Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "3035642820",
                "ArXiv": "1911.05815",
                "DBLP": "conf/icml/MisraHK020",
                "CorpusId": 208006292
            },
            "abstract": "We present an algorithm, HOMER, for exploration and reinforcement learning in rich observation environments that are summarizable by an unknown latent state space. The algorithm interleaves representation learning to identify a new notion of kinematic state abstraction with strategic exploration to reach new states using the learned abstraction. The algorithm provably explores the environment with sample complexity scaling polynomially in the number of latent states and the time horizon, and, crucially, with no dependence on the size of the observation space, which could be infinitely large. This exploration guarantee further enables sample-efficient global policy optimization for any reward function. On the computational side, we show that the algorithm can be implemented efficiently whenever certain supervised learning problems are tractable. Empirically, we evaluate HOMER on a challenging exploration problem, where we show that the algorithm is exponentially more sample efficient than standard reinforcement learning baselines.",
            "referenceCount": 56,
            "citationCount": 128,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-11-13",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1911.05815"
            },
            "citationStyles": {
                "bibtex": "@Article{Misra2019KinematicSA,\n author = {Dipendra Kumar Misra and Mikael Henaff and A. Krishnamurthy and J. Langford},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Kinematic State Abstraction and Provably Efficient Rich-Observation Reinforcement Learning},\n volume = {abs/1911.05815},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:049c6e5736313374c6e594c34b9be89a3a09dced",
            "@type": "ScholarlyArticle",
            "paperId": "049c6e5736313374c6e594c34b9be89a3a09dced",
            "corpusId": 6656096,
            "url": "https://www.semanticscholar.org/paper/049c6e5736313374c6e594c34b9be89a3a09dced",
            "title": "FeUdal Networks for Hierarchical Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/VezhnevetsOSHJS17",
                "MAG": "2949267040",
                "ArXiv": "1703.01161",
                "CorpusId": 6656096
            },
            "abstract": "We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.",
            "referenceCount": 35,
            "citationCount": 751,
            "influentialCitationCount": 62,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-03",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1703.01161"
            },
            "citationStyles": {
                "bibtex": "@Article{Vezhnevets2017FeUdalNF,\n author = {A. Vezhnevets and Simon Osindero and T. Schaul and N. Heess and Max Jaderberg and David Silver and K. Kavukcuoglu},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {FeUdal Networks for Hierarchical Reinforcement Learning},\n volume = {abs/1703.01161},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:47875832106e1ab8e0fdde96773fd043c9662ac8",
            "@type": "ScholarlyArticle",
            "paperId": "47875832106e1ab8e0fdde96773fd043c9662ac8",
            "corpusId": 9421360,
            "url": "https://www.semanticscholar.org/paper/47875832106e1ab8e0fdde96773fd043c9662ac8",
            "title": "Cooperative Multi-agent Control Using Deep Reinforcement Learning",
            "venue": "AAMAS Workshops",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/atal/GuptaEK17",
                "MAG": "2768629321",
                "DOI": "10.1007/978-3-319-71682-4_5",
                "CorpusId": 9421360
            },
            "abstract": null,
            "referenceCount": 45,
            "citationCount": 676,
            "influentialCitationCount": 53,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-05-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Gupta2017CooperativeMC,\n author = {Jayesh K. Gupta and M. Egorov and Mykel J. Kochenderfer},\n booktitle = {AAMAS Workshops},\n pages = {66-83},\n title = {Cooperative Multi-agent Control Using Deep Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4c54efd3b9ce84f4268b165f5896c0692a50bcd1",
            "@type": "ScholarlyArticle",
            "paperId": "4c54efd3b9ce84f4268b165f5896c0692a50bcd1",
            "corpusId": 59553460,
            "url": "https://www.semanticscholar.org/paper/4c54efd3b9ce84f4268b165f5896c0692a50bcd1",
            "title": "Learning Action Representations for Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1902.00183",
                "DBLP": "journals/corr/abs-1902-00183",
                "MAG": "2964158321",
                "CorpusId": 59553460
            },
            "abstract": "Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provided a priori. We show how a policy can be decomposed into a component that acts in a low-dimensional space of action representations and a component that transforms these representations into actual actions. These representations improve generalization over large, finite action sets by allowing the agent to infer the outcomes of actions similar to actions already taken. We provide an algorithm to both learn and use action representations and provide conditions for its convergence. The efficacy of the proposed method is demonstrated on large-scale real-world problems.",
            "referenceCount": 49,
            "citationCount": 128,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-02-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chandak2019LearningAR,\n author = {Yash Chandak and Georgios Theocharous and James E. Kostas and Scott M. Jordan and P. Thomas},\n booktitle = {International Conference on Machine Learning},\n pages = {941-950},\n title = {Learning Action Representations for Reinforcement Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:741ebf91225327bbd11e0e4fa08e329d45637b13",
            "@type": "ScholarlyArticle",
            "paperId": "741ebf91225327bbd11e0e4fa08e329d45637b13",
            "corpusId": 208910151,
            "url": "https://www.semanticscholar.org/paper/741ebf91225327bbd11e0e4fa08e329d45637b13",
            "title": "Optimism in Reinforcement Learning with Generalized Linear Function Approximation",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1912-04136",
                "MAG": "2991929641",
                "ArXiv": "1912.04136",
                "CorpusId": 208910151
            },
            "abstract": "We design a new provably efficient algorithm for episodic reinforcement learning with generalized linear function approximation. We analyze the algorithm under a new expressivity assumption that we call \"optimistic closure,\" which is strictly weaker than assumptions from prior analyses for the linear setting. With optimistic closure, we prove that our algorithm enjoys a regret bound of $\\tilde{O}(\\sqrt{d^3 T})$ where $d$ is the dimensionality of the state-action features and $T$ is the number of episodes. This is the first statistically and computationally efficient algorithm for reinforcement learning with generalized linear functions.",
            "referenceCount": 30,
            "citationCount": 114,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-12-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1912.04136"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2019OptimismIR,\n author = {Yining Wang and Ruosong Wang and S. Du and A. Krishnamurthy},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Optimism in Reinforcement Learning with Generalized Linear Function Approximation},\n volume = {abs/1912.04136},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b0d8165eecf2aa04a85e701d0c6bb4edd4b3811b",
            "@type": "ScholarlyArticle",
            "paperId": "b0d8165eecf2aa04a85e701d0c6bb4edd4b3811b",
            "corpusId": 9072400,
            "url": "https://www.semanticscholar.org/paper/b0d8165eecf2aa04a85e701d0c6bb4edd4b3811b",
            "title": "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2963937357",
                "ArXiv": "1711.00832",
                "DBLP": "conf/nips/LanctotZGLTPSG17",
                "CorpusId": 9072400
            },
            "abstract": "To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and fictitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.",
            "referenceCount": 112,
            "citationCount": 523,
            "influentialCitationCount": 60,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-11-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lanctot2017AUG,\n author = {Marc Lanctot and V. Zambaldi and A. Gruslys and Angeliki Lazaridou and K. Tuyls and J. P\u00e9rolat and David Silver and T. Graepel},\n booktitle = {Neural Information Processing Systems},\n pages = {4190-4203},\n title = {A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:869f4fc59d74a96098ed46935cb6fd1a537c38ce",
            "@type": "ScholarlyArticle",
            "paperId": "869f4fc59d74a96098ed46935cb6fd1a537c38ce",
            "corpusId": 22079070,
            "url": "https://www.semanticscholar.org/paper/869f4fc59d74a96098ed46935cb6fd1a537c38ce",
            "title": "Information theoretic MPC for model-based reinforcement learning",
            "venue": "IEEE International Conference on Robotics and Automation",
            "publicationVenue": {
                "id": "urn:research:3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                "name": "IEEE International Conference on Robotics and Automation",
                "alternate_names": [
                    "International Conference on Robotics and Automation",
                    "Int Conf Robot Autom",
                    "ICRA",
                    "IEEE Int Conf Robot Autom"
                ],
                "issn": "2152-4092",
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/icra/WilliamsWGDRBT17",
                "MAG": "2738778707",
                "DOI": "10.1109/ICRA.2017.7989202",
                "CorpusId": 22079070
            },
            "abstract": "We introduce an information theoretic model predictive control (MPC) algorithm capable of handling complex cost criteria and general nonlinear dynamics. The generality of the approach makes it possible to use multi-layer neural networks as dynamics models, which we incorporate into our MPC algorithm in order to solve model-based reinforcement learning tasks. We test the algorithm in simulation on a cart-pole swing up and quadrotor navigation task, as well as on actual hardware in an aggressive driving task. Empirical results demonstrate that the algorithm is capable of achieving a high level of performance and does so only utilizing data collected from the system.",
            "referenceCount": 25,
            "citationCount": 408,
            "influentialCitationCount": 59,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-05-01",
            "journal": {
                "name": "2017 IEEE International Conference on Robotics and Automation (ICRA)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Williams2017InformationTM,\n author = {Grady Williams and Nolan Wagener and Brian Goldfain and P. Drews and James M. Rehg and Byron Boots and Evangelos A. Theodorou},\n booktitle = {IEEE International Conference on Robotics and Automation},\n journal = {2017 IEEE International Conference on Robotics and Automation (ICRA)},\n pages = {1714-1721},\n title = {Information theoretic MPC for model-based reinforcement learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:96dc41d3b004fd4c7f96b71b4e174beb3088b2bb",
            "@type": "ScholarlyArticle",
            "paperId": "96dc41d3b004fd4c7f96b71b4e174beb3088b2bb",
            "corpusId": 19219023,
            "url": "https://www.semanticscholar.org/paper/96dc41d3b004fd4c7f96b71b4e174beb3088b2bb",
            "title": "Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2738318237",
                "DBLP": "conf/cvpr/YunCYYC17",
                "DOI": "10.1109/CVPR.2017.148",
                "CorpusId": 19219023
            },
            "abstract": "This paper proposes a novel tracker which is controlled by sequentially pursuing actions learned by deep reinforcement learning. In contrast to the existing trackers using deep networks, the proposed tracker is designed to achieve a light computation as well as satisfactory tracking accuracy in both location and scale. The deep network to control actions is pre-trained using various training sequences and fine-tuned during tracking for online adaptation to target and background changes. The pre-training is done by utilizing deep reinforcement learning as well as supervised learning. The use of reinforcement learning enables even partially labeled data to be successfully utilized for semi-supervised learning. Through evaluation of the OTB dataset, the proposed tracker is validated to achieve a competitive performance that is three times faster than state-of-the-art, deep network&#x2013;based trackers. The fast version of the proposed method, which operates in real-time on GPU, outperforms the state-of-the-art real-time trackers.",
            "referenceCount": 42,
            "citationCount": 456,
            "influentialCitationCount": 60,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-07-01",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yun2017ActionDecisionNF,\n author = {Sangdoo Yun and Jongwon Choi and Y. Yoo and Kimin Yun and J. Choi},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {1349-1358},\n title = {Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:615e443f15778e9fdde27fecebd5c6d028816e27",
            "@type": "ScholarlyArticle",
            "paperId": "615e443f15778e9fdde27fecebd5c6d028816e27",
            "corpusId": 186206900,
            "url": "https://www.semanticscholar.org/paper/615e443f15778e9fdde27fecebd5c6d028816e27",
            "title": "Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1906-04737",
                "ArXiv": "1906.04737",
                "MAG": "2949464762",
                "CorpusId": 186206900
            },
            "abstract": "Recent developments in deep reinforcement learning are concerned with creating decision-making agents which can perform well in various complex domains. A particular approach which has received increasing attention is multi-agent reinforcement learning, in which multiple agents learn concurrently to coordinate their actions. In such multi-agent environments, additional learning problems arise due to the continually changing decision-making policies of agents. This paper surveys recent works that address the non-stationarity problem in multi-agent deep reinforcement learning. The surveyed methods range from modifications in the training procedure, such as centralized training, to learning representations of the opponent's policy, meta-learning, communication, and decentralized learning. The survey concludes with a list of open problems and possible lines of future research.",
            "referenceCount": 42,
            "citationCount": 137,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-06-11",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1906.04737"
            },
            "citationStyles": {
                "bibtex": "@Article{Papoudakis2019DealingWN,\n author = {Georgios Papoudakis and Filippos Christianos and Arrasy Rahman and Stefano V. Albrecht},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning},\n volume = {abs/1906.04737},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef",
            "@type": "ScholarlyArticle",
            "paperId": "ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef",
            "corpusId": 4718601,
            "url": "https://www.semanticscholar.org/paper/ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef",
            "title": "Latent Space Policies for Hierarchical Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963722050",
                "ArXiv": "1804.02808",
                "DBLP": "conf/icml/HaarnojaHAL18",
                "CorpusId": 4718601
            },
            "abstract": "We address the problem of learning hierarchical deep neural network policies for reinforcement learning. In contrast to methods that explicitly restrict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entropy reinforcement learning objective. Each layer is also augmented with latent random variables, which are sampled from a prior distribution during the training of that layer. The maximum entropy objective causes these latent variables to be incorporated into the layer's policy, and the higher level layer can directly control the behavior of the lower layer through this latent space. Furthermore, by constraining the mapping from latent variables to actions to be invertible, higher layers retain full expressivity: neither the higher layers nor the lower layers are constrained in their behavior. Our experimental evaluation demonstrates that we can improve on the performance of single-layer policies on standard benchmark tasks simply by adding additional layers, and that our method can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives.",
            "referenceCount": 40,
            "citationCount": 163,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-04-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1804.02808"
            },
            "citationStyles": {
                "bibtex": "@Article{Haarnoja2018LatentSP,\n author = {Tuomas Haarnoja and Kristian Hartikainen and P. Abbeel and S. Levine},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Latent Space Policies for Hierarchical Reinforcement Learning},\n volume = {abs/1804.02808},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cce22bf6405042a965a86557684c46a441f2a736",
            "@type": "ScholarlyArticle",
            "paperId": "cce22bf6405042a965a86557684c46a441f2a736",
            "corpusId": 206853161,
            "url": "https://www.semanticscholar.org/paper/cce22bf6405042a965a86557684c46a441f2a736",
            "title": "Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning",
            "venue": "IEEE International Conference on Robotics and Automation",
            "publicationVenue": {
                "id": "urn:research:3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                "name": "IEEE International Conference on Robotics and Automation",
                "alternate_names": [
                    "International Conference on Robotics and Automation",
                    "Int Conf Robot Autom",
                    "ICRA",
                    "IEEE Int Conf Robot Autom"
                ],
                "issn": "2152-4092",
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/icra/NagabandiKFL18",
                "MAG": "2962872206",
                "ArXiv": "1708.02596",
                "DOI": "10.1109/ICRA.2018.8463189",
                "CorpusId": 206853161
            },
            "abstract": "Model-free deep reinforcement learning algorithms have been shown to be capable of learning a wide range of robotic skills, but typically require a very large number of samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that neural network dynamics models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits that accomplish various complex locomotion tasks. We further propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent sample efficiency, and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of $3-5\\times$ on swimmer, cheetah, hopper, and ant agents. Videos can be found at https://sites.google.com/view/mbmf",
            "referenceCount": 46,
            "citationCount": 807,
            "influentialCitationCount": 50,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1708.02596",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-08-08",
            "journal": {
                "name": "2018 IEEE International Conference on Robotics and Automation (ICRA)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nagabandi2017NeuralND,\n author = {Anusha Nagabandi and G. Kahn and R. Fearing and S. Levine},\n booktitle = {IEEE International Conference on Robotics and Automation},\n journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},\n pages = {7559-7566},\n title = {Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c18df1edc0a45891806d44896a8f666944e93d01",
            "@type": "ScholarlyArticle",
            "paperId": "c18df1edc0a45891806d44896a8f666944e93d01",
            "corpusId": 1448723,
            "url": "https://www.semanticscholar.org/paper/c18df1edc0a45891806d44896a8f666944e93d01",
            "title": "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/iccv/DasKMLB17",
                "ArXiv": "1703.06585",
                "MAG": "2603266952",
                "DOI": "10.1109/ICCV.2017.321",
                "CorpusId": 1448723
            },
            "abstract": "We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative \u2018image guessing\u2019 game between two agents \u2013 Q-BOT and A-BOT\u2013 who communicate in natural language dialog so that Q-BOT can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end \u2013 from pixels to multi-agent multi-round dialog to game reward.,,We demonstrate two experimental results.,,First, as a \u2018sanity check\u2019 demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabularies, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/style). Thus, we demonstrate the emergence of grounded language and communication among \u2018visual\u2019 dialog agents with no human supervision.,,Second, we conduct large-scale real-image experiments on the VisDial dataset [5], where we pretrain on dialog data with supervised learning (SL) and show that the RL finetuned agents significantly outperform supervised pretraining. Interestingly, the RL Q-BOT learns to ask questions that A-BOT is good at, ultimately resulting in more informative dialog and a better team.",
            "referenceCount": 42,
            "citationCount": 390,
            "influentialCitationCount": 51,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1703.06585",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-20",
            "journal": {
                "name": "2017 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Das2017LearningCV,\n author = {Abhishek Das and Satwik Kottur and J. Moura and Stefan Lee and Dhruv Batra},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\n pages = {2970-2979},\n title = {Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:68c108795deef06fa929d1f6e96b75dbf7ce8531",
            "@type": "ScholarlyArticle",
            "paperId": "68c108795deef06fa929d1f6e96b75dbf7ce8531",
            "corpusId": 3418899,
            "url": "https://www.semanticscholar.org/paper/68c108795deef06fa929d1f6e96b75dbf7ce8531",
            "title": "Meta-Reinforcement Learning of Structured Exploration Strategies",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/nips/GuptaMLAL18",
                "MAG": "2963176272",
                "ArXiv": "1802.07245",
                "CorpusId": 3418899
            },
            "abstract": "Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm -- model agnostic exploration with structured noise (MAESN) -- to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.",
            "referenceCount": 47,
            "citationCount": 285,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Gupta2018MetaReinforcementLO,\n author = {Abhishek Gupta and Russell Mendonca and Yuxuan Liu and P. Abbeel and S. Levine},\n booktitle = {Neural Information Processing Systems},\n pages = {5307-5316},\n title = {Meta-Reinforcement Learning of Structured Exploration Strategies},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b223b7d1dc76be5591bc261e9550ae4d168b6222",
            "@type": "ScholarlyArticle",
            "paperId": "b223b7d1dc76be5591bc261e9550ae4d168b6222",
            "corpusId": 4790080,
            "url": "https://www.semanticscholar.org/paper/b223b7d1dc76be5591bc261e9550ae4d168b6222",
            "title": "Reinforcement Learning for UAV Attitude Control",
            "venue": "ACM Trans. Cyber Phys. Syst.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2950954298",
                "ArXiv": "1804.04154",
                "DBLP": "journals/tcps/KochMWB19",
                "DOI": "10.1145/3301273",
                "CorpusId": 4790080
            },
            "abstract": "Autopilot systems are typically composed of an \u201cinner loop\u201d providing stability and control, whereas an \u201couter loop\u201d is responsible for mission-level objectives, such as way-point navigation. Autopilot systems for unmanned aerial vehicles are predominately implemented using Proportional-Integral-Derivative (PID) control systems, which have demonstrated exceptional performance in stable environments. However, more sophisticated control is required to operate in unpredictable and harsh environments. Intelligent flight control systems is an active area of research addressing limitations of PID control most recently through the use of reinforcement learning (RL), which has had success in other applications, such as robotics. Yet previous work has focused primarily on using RL at the mission-level controller. In this work, we investigate the performance and accuracy of the inner control loop providing attitude control when using intelligent flight control systems trained with state-of-the-art RL algorithms\u2014Deep Deterministic Policy Gradient, Trust Region Policy Optimization, and Proximal Policy Optimization. To investigate these unknowns, we first developed an open source high-fidelity simulation environment to train a flight controller attitude control of a quadrotor through RL. We then used our environment to compare their performance to that of a PID controller to identify if using RL is appropriate in high-precision, time-critical flight control.",
            "referenceCount": 41,
            "citationCount": 282,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3301273",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-04-11",
            "journal": {
                "name": "ACM Transactions on Cyber-Physical Systems",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Koch2018ReinforcementLF,\n author = {W. Koch and R. Mancuso and R. West and Azer Bestavros},\n booktitle = {ACM Trans. Cyber Phys. Syst.},\n journal = {ACM Transactions on Cyber-Physical Systems},\n pages = {1 - 21},\n title = {Reinforcement Learning for UAV Attitude Control},\n volume = {3},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e4b4066e46fd160cb84e246b0895a4cd674d8ce4",
            "@type": "ScholarlyArticle",
            "paperId": "e4b4066e46fd160cb84e246b0895a4cd674d8ce4",
            "corpusId": 3132647,
            "url": "https://www.semanticscholar.org/paper/e4b4066e46fd160cb84e246b0895a4cd674d8ce4",
            "title": "Safe Reinforcement Learning via Shielding",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2963575966",
                "DBLP": "conf/aaai/AlshiekhBEKNT18",
                "ArXiv": "1708.08611",
                "DOI": "10.1609/aaai.v32i1.11797",
                "CorpusId": 3132647
            },
            "abstract": "\n \n Reinforcement learning algorithms discover policies that maximize reward, but do not necessarily guarantee safety during learning or execution phases. We introduce a new approach to learn optimal policies while enforcing properties expressed in temporal logic. To this end, given the temporal logic specification that is to be obeyed by the learning system, we propose to synthesize a reactive system called a shield. The shield monitors the actions from the learner and corrects them only if the chosen action causes a violation of the specification. We discuss which requirements a shield must meet to preserve the convergence guarantees of the learner. Finally, we demonstrate the versatility of our approach on several challenging reinforcement learning scenarios.\n \n",
            "referenceCount": 26,
            "citationCount": 483,
            "influentialCitationCount": 46,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11797/11656",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-08-29",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1708.08611"
            },
            "citationStyles": {
                "bibtex": "@Article{Alshiekh2017SafeRL,\n author = {Mohammed Alshiekh and R. Bloem and R\u00fcdiger Ehlers and Bettina K\u00f6nighofer and S. Niekum and U. Topcu},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Safe Reinforcement Learning via Shielding},\n volume = {abs/1708.08611},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ae4d32f05cf40e4cc01c69d7787149a258c95eda",
            "@type": "ScholarlyArticle",
            "paperId": "ae4d32f05cf40e4cc01c69d7787149a258c95eda",
            "corpusId": 54464184,
            "url": "https://www.semanticscholar.org/paper/ae4d32f05cf40e4cc01c69d7787149a258c95eda",
            "title": "Residual Reinforcement Learning for Robot Control",
            "venue": "IEEE International Conference on Robotics and Automation",
            "publicationVenue": {
                "id": "urn:research:3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                "name": "IEEE International Conference on Robotics and Automation",
                "alternate_names": [
                    "International Conference on Robotics and Automation",
                    "Int Conf Robot Autom",
                    "ICRA",
                    "IEEE Int Conf Robot Autom"
                ],
                "issn": "2152-4092",
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1812.03201",
                "MAG": "2904746163",
                "DBLP": "conf/icra/JohanninkBNLKLO19",
                "DOI": "10.1109/ICRA.2019.8794127",
                "CorpusId": 54464184
            },
            "abstract": "Conventional feedback control methods can solve various types of robot control problems very efficiently by capturing the structure with explicit models, such as rigid body equations of motion. However, many control problems in modern manufacturing deal with contacts and friction, which are difficult to capture with first-order physical modeling. Hence, applying control design methodologies to these kinds of problems often results in brittle and inaccurate controllers, which have to be manually tuned for deployment. Reinforcement learning (RL) methods have been demonstrated to be capable of learning continuous robot controllers from interactions with the environment, even for problems that include friction and contacts. In this paper, we study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. The final control policy is a superposition of both control signals. We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects.",
            "referenceCount": 35,
            "citationCount": 275,
            "influentialCitationCount": 20,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1812.03201",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-12-07",
            "journal": {
                "name": "2019 International Conference on Robotics and Automation (ICRA)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Johannink2018ResidualRL,\n author = {T. Johannink and Shikhar Bahl and Ashvin Nair and Jianlan Luo and Avinash Kumar and M. Loskyll and J. A. Ojea and Eugen Solowjow and S. Levine},\n booktitle = {IEEE International Conference on Robotics and Automation},\n journal = {2019 International Conference on Robotics and Automation (ICRA)},\n pages = {6023-6029},\n title = {Residual Reinforcement Learning for Robot Control},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:471f9742b4e32d8ee68f9ee493768ff0466a231d",
            "@type": "ScholarlyArticle",
            "paperId": "471f9742b4e32d8ee68f9ee493768ff0466a231d",
            "corpusId": 22729745,
            "url": "https://www.semanticscholar.org/paper/471f9742b4e32d8ee68f9ee493768ff0466a231d",
            "title": "Automatic Goal Generation for Reinforcement Learning Agents",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1705.06366",
                "MAG": "2616430965",
                "DBLP": "conf/icml/FlorensaHGA18",
                "CorpusId": 22729745
            },
            "abstract": "Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.",
            "referenceCount": 51,
            "citationCount": 398,
            "influentialCitationCount": 42,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-05-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Held2017AutomaticGG,\n author = {David Held and Xinyang Geng and Carlos Florensa and P. Abbeel},\n booktitle = {International Conference on Machine Learning},\n pages = {1514-1523},\n title = {Automatic Goal Generation for Reinforcement Learning Agents},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:17b6829678802a20e51558ec28c5369414defe42",
            "@type": "ScholarlyArticle",
            "paperId": "17b6829678802a20e51558ec28c5369414defe42",
            "corpusId": 202889229,
            "url": "https://www.semanticscholar.org/paper/17b6829678802a20e51558ec28c5369414defe42",
            "title": "Data Valuation using Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "3035748261",
                "DBLP": "journals/corr/abs-1909-11671",
                "ArXiv": "1909.11671",
                "CorpusId": 202889229
            },
            "abstract": "Quantifying the value of data is a fundamental problem in machine learning. Data valuation has multiple important use cases: (1) building insights about the learning task, (2) domain adaptation, (3) corrupted sample discovery, and (4) robust learning. To adaptively learn data values jointly with the target task predictor model, we propose a meta learning framework which we name Data Valuation using Reinforcement Learning (DVRL). We employ a data value estimator (modeled by a deep neural network) to learn how likely each datum is used in training of the predictor model. We train the data value estimator using a reinforcement signal of the reward obtained on a small validation set that reflects performance on the target task. We demonstrate that DVRL yields superior data value estimates compared to alternative methods across different types of datasets and in a diverse set of application scenarios. The corrupted sample discovery performance of DVRL is close to optimal in many regimes (i.e. as if the noisy samples were known apriori), and for domain adaptation and robust learning DVRL significantly outperforms state-of-the-art by 14.6% and 10.8%, respectively.",
            "referenceCount": 36,
            "citationCount": 105,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-09-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1909.11671"
            },
            "citationStyles": {
                "bibtex": "@Article{Yoon2019DataVU,\n author = {Jinsung Yoon and Sercan \u00d6. Arik and Tomas Pfister},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Data Valuation using Reinforcement Learning},\n volume = {abs/1909.11671},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f8ad22941c633b62573aaee9cee651a9bc895fe5",
            "@type": "ScholarlyArticle",
            "paperId": "f8ad22941c633b62573aaee9cee651a9bc895fe5",
            "corpusId": 15294098,
            "url": "https://www.semanticscholar.org/paper/f8ad22941c633b62573aaee9cee651a9bc895fe5",
            "title": "Deep Reinforcement Learning",
            "venue": "International Conference on Computing Communication and Networking Technologies",
            "publicationVenue": {
                "id": "urn:research:47c47088-7640-441b-85ba-375fe8f5c3a4",
                "name": "International Conference on Computing Communication and Networking Technologies",
                "alternate_names": [
                    "Int Conf Comput Commun Netw Technol",
                    "International Conference on Computing, Communication and Networking Technologies",
                    "ICCCNT"
                ],
                "issn": null,
                "url": null
            },
            "year": 2023,
            "externalIds": {
                "DBLP": "conf/icccnt/Krichen23a",
                "DOI": "10.1109/ICCCNT56998.2023.10306453",
                "CorpusId": 15294098
            },
            "abstract": "Deep Reinforcement Learning (DRL) is a powerful technique for learning policies for complex decision-making tasks. In this paper, we provide an overview of DRL, including its basic components, key algorithms and techniques, and applications in areas s.a. robotics, game playing, and autonomous driving. We also discuss some of the challenges and limitations of DRL, s.a. sample inefficiency and safety concerns, and we identify some of the promising directions for future research in DRL, s.a. meta-learning, hierarchical reinforcement learning, and combining DRL with formal techniques. In the second part of the paper, we discuss several important applications of DRL, including transfer learning, multi-agent reinforcement learning, and explainable reinforcement learning. We also explore the combination of DRL with formal techniques, a promising area of research for ensuring the safety and reliability of DRL applications. Finally, we identify some of the limitations and open issues in DRL, including sample efficiency, safety, and scalability concerns. To help practitioners effectively apply DRL in their work, we provide recommendations for starting with simple problems, choosing appropriate algorithms and architectures, paying attention to safety and ethics, collaborating with experts, and staying up to date with the latest research in the field. We conclude by highlighting the potential impact of DRL in a wide range of applications and emphasizing the need for careful consideration of the ethical and societal implications of DRL.",
            "referenceCount": 59,
            "citationCount": 135,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2023-07-06",
            "journal": {
                "name": "2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sharma2023DeepRL,\n author = {Sahil Sharma and A. Srinivas and Balaraman Ravindran},\n booktitle = {International Conference on Computing Communication and Networking Technologies},\n journal = {2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT)},\n pages = {1-7},\n title = {Deep Reinforcement Learning},\n year = {2023}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c0f2c4104ef6e36bb67022001179887e6600d24d",
            "@type": "ScholarlyArticle",
            "paperId": "c0f2c4104ef6e36bb67022001179887e6600d24d",
            "corpusId": 2497153,
            "url": "https://www.semanticscholar.org/paper/c0f2c4104ef6e36bb67022001179887e6600d24d",
            "title": "A comprehensive survey on safe reinforcement learning",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1845972764",
                "DBLP": "journals/jmlr/GarciaF15",
                "DOI": "10.5555/2789272.2886795",
                "CorpusId": 2497153
            },
            "abstract": "Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. We categorize and analyze two approaches of Safe Reinforcement Learning. The first is based on the modification of the optimality criterion, the classic discounted finite/infinite horizon, with a safety factor. The second is based on the modification of the exploration process through the incorporation of external knowledge or the guidance of a risk metric. We use the proposed classification to survey the existing literature, as well as suggesting future directions for Safe Reinforcement Learning.",
            "referenceCount": 123,
            "citationCount": 1222,
            "influentialCitationCount": 52,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "16"
            },
            "citationStyles": {
                "bibtex": "@Article{Garc\u00eda2015ACS,\n author = {Javier Garc\u00eda and F. Fern\u00e1ndez},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1437-1480},\n title = {A comprehensive survey on safe reinforcement learning},\n volume = {16},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:675770f77cd913abc7b7f0466f281cb6c4b383f7",
            "@type": "ScholarlyArticle",
            "paperId": "675770f77cd913abc7b7f0466f281cb6c4b383f7",
            "corpusId": 208857488,
            "url": "https://www.semanticscholar.org/paper/675770f77cd913abc7b7f0466f281cb6c4b383f7",
            "title": "Observational Overfitting in Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/iclr/SongJTDN20",
                "ArXiv": "1912.02975",
                "MAG": "2996283175",
                "CorpusId": 208857488
            },
            "abstract": "A major component of overfitting in model-free reinforcement learning (RL) involves the case where the agent may mistakenly correlate reward with certain spurious features from the observations generated by the Markov Decision Process (MDP). We provide a general framework for analyzing this scenario, which we use to design multiple synthetic benchmarks from only modifying the observation space of an MDP. When an agent overfits to different observation spaces even if the underlying MDP dynamics is fixed, we term this observational overfitting. Our experiments expose intriguing properties especially with regards to implicit regularization, and also corroborate results from previous works in RL generalization and supervised learning (SL).",
            "referenceCount": 69,
            "citationCount": 104,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-12-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1912.02975"
            },
            "citationStyles": {
                "bibtex": "@Article{Song2019ObservationalOI,\n author = {Xingyou Song and Yiding Jiang and Stephen Tu and Yilun Du and Behnam Neyshabur},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Observational Overfitting in Reinforcement Learning},\n volume = {abs/1912.02975},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4660c2338ef2bf9aa3ae2c29cad97240c87a3d17",
            "@type": "ScholarlyArticle",
            "paperId": "4660c2338ef2bf9aa3ae2c29cad97240c87a3d17",
            "corpusId": 186206599,
            "url": "https://www.semanticscholar.org/paper/4660c2338ef2bf9aa3ae2c29cad97240c87a3d17",
            "title": "Reinforcement Learning for Integer Programming: Learning to Cut",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/icml/Tang0F20",
                "ArXiv": "1906.04859",
                "MAG": "2950058920",
                "CorpusId": 186206599
            },
            "abstract": "Integer programming (IP) is a general optimization framework widely applicable to a variety of unstructured and structured problems arising in, e.g., scheduling, production planning, and graph optimization. As IP models many provably hard to solve problems, modern IP solvers rely on many heuristics. These heuristics are usually human-designed, and naturally prone to suboptimality. The goal of this work is to show that the performance of those solvers can be greatly enhanced using reinforcement learning (RL). In particular, we investigate a specific methodology for solving IPs, known as the Cutting Plane Method. This method is employed as a subroutine by all modern IP solvers. We present a deep RL formulation, network architecture, and algorithms for intelligent adaptive selection of cutting planes (aka cuts). Across a wide range of IP tasks, we show that the trained RL agent significantly outperforms human-designed heuristics, and effectively generalizes to 10X larger instances and across IP problem classes. The trained agent is also demonstrated to benefit the popular downstream application of cutting plane methods in Branch-and-Cut algorithm, which is the backbone of state-of-the-art commercial IP solvers.",
            "referenceCount": 32,
            "citationCount": 105,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-06-11",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1906.04859"
            },
            "citationStyles": {
                "bibtex": "@Article{Tang2019ReinforcementLF,\n author = {Yunhao Tang and Shipra Agrawal and Yuri Faenza},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Reinforcement Learning for Integer Programming: Learning to Cut},\n volume = {abs/1906.04859},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c43bba87b4237a93d96b2a3e91da25d91fd0bb91",
            "@type": "ScholarlyArticle",
            "paperId": "c43bba87b4237a93d96b2a3e91da25d91fd0bb91",
            "corpusId": 4712980,
            "url": "https://www.semanticscholar.org/paper/c43bba87b4237a93d96b2a3e91da25d91fd0bb91",
            "title": "Programmatically Interpretable Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2796284132",
                "DBLP": "journals/corr/abs-1804-02477",
                "ArXiv": "1804.02477",
                "CorpusId": 4712980
            },
            "abstract": "We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural \"oracle\". We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.",
            "referenceCount": 41,
            "citationCount": 264,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-04-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1804.02477"
            },
            "citationStyles": {
                "bibtex": "@Article{Verma2018ProgrammaticallyIR,\n author = {Abhinav Verma and V. Murali and Rishabh Singh and Pushmeet Kohli and Swarat Chaudhuri},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Programmatically Interpretable Reinforcement Learning},\n volume = {abs/1804.02477},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:65769b53e71ea7c52b3a07ad32bd4fdade6a0173",
            "@type": "ScholarlyArticle",
            "paperId": "65769b53e71ea7c52b3a07ad32bd4fdade6a0173",
            "corpusId": 52194333,
            "url": "https://www.semanticscholar.org/paper/65769b53e71ea7c52b3a07ad32bd4fdade6a0173",
            "title": "Multi-task Deep Reinforcement Learning with PopArt",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/aaai/HesselSE0SH19",
                "MAG": "2951032747",
                "ArXiv": "1809.04474",
                "DOI": "10.1609/AAAI.V33I01.33013796",
                "CorpusId": 52194333
            },
            "abstract": "The reinforcement learning (RL) community has made great strides in designing algorithms capable of exceeding human performance on specific tasks. These algorithms are mostly trained one task at the time, each new task requiring to train a brand new agent instance. This means the learning algorithm is general, but each solution is not; each agent can only solve the one task it was trained on. In this work, we study the problem of learning to master not one but multiple sequentialdecision tasks at once. A general issue in multi-task learning is that a balance must be found between the needs of multiple tasks competing for the limited resources of a single learning system. Many learning algorithms can get distracted by certain tasks in the set of tasks to solve. Such tasks appear more salient to the learning process, for instance because of the density or magnitude of the in-task rewards. This causes the algorithm to focus on those salient tasks at the expense of generality. We propose to automatically adapt the contribution of each task to the agent\u2019s updates, so that all tasks have a similar impact on the learning dynamics. This resulted in state of the art performance on learning to play all games in a set of 57 diverse Atari games. Excitingly, our method learned a single trained policy - with a single set of weights - that exceeds median human performance. To our knowledge, this was the first time a single agent surpassed human-level performance on this multi-task domain. The same approach also demonstrated state of the art performance on a set of 30 tasks in the 3D reinforcement learning platform DeepMind Lab.",
            "referenceCount": 67,
            "citationCount": 264,
            "influentialCitationCount": 24,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4266/4144",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-09-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hessel2018MultitaskDR,\n author = {Matteo Hessel and Hubert Soyer and Lasse Espeholt and Wojciech M. Czarnecki and Simon Schmitt and H. V. Hasselt},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {3796-3803},\n title = {Multi-task Deep Reinforcement Learning with PopArt},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:96a067e188f1c89db9faea1fea2314a15ae51bbc",
            "@type": "ScholarlyArticle",
            "paperId": "96a067e188f1c89db9faea1fea2314a15ae51bbc",
            "corpusId": 3615386,
            "url": "https://www.semanticscholar.org/paper/96a067e188f1c89db9faea1fea2314a15ae51bbc",
            "title": "Bridging the Gap Between Value and Policy Based Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/NachumNXS17",
                "MAG": "2951507724",
                "ArXiv": "1702.08892",
                "CorpusId": 3615386
            },
            "abstract": "We establish a new connection between value and policy based reinforcement learning (RL) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization. Specifically, we show that softmax consistent action values correspond to optimal entropy regularized policy probabilities along any action sequence, regardless of provenance. From this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes a notion of soft consistency error along multi-step action sequences extracted from both on- and off-policy traces. We examine the behavior of PCL in different scenarios and show that PCL can be interpreted as generalizing both actor-critic and Q-learning algorithms. We subsequently deepen the relationship by showing how a single model can be used to represent both a policy and the corresponding softmax state values, eliminating the need for a separate critic. The experimental evaluation demonstrates that PCL significantly outperforms strong actor-critic and Q-learning baselines across several benchmarks.",
            "referenceCount": 68,
            "citationCount": 381,
            "influentialCitationCount": 42,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-02-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nachum2017BridgingTG,\n author = {Ofir Nachum and Mohammad Norouzi and Kelvin Xu and Dale Schuurmans},\n booktitle = {Neural Information Processing Systems},\n pages = {2775-2785},\n title = {Bridging the Gap Between Value and Policy Based Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e5afacab42153f24b8e4ca543bd0ecae4e810f10",
            "@type": "ScholarlyArticle",
            "paperId": "e5afacab42153f24b8e4ca543bd0ecae4e810f10",
            "corpusId": 174800824,
            "url": "https://www.semanticscholar.org/paper/e5afacab42153f24b8e4ca543bd0ecae4e810f10",
            "title": "Multi-Agent Adversarial Inverse Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1907.13220",
                "DBLP": "journals/corr/abs-1907-13220",
                "MAG": "2946233467",
                "CorpusId": 174800824
            },
            "abstract": "Reinforcement learning agents are prone to undesired behaviors due to reward mis-specification. Finding a set of reward functions to properly guide agent behaviors is particularly challenging in multi-agent scenarios. Inverse reinforcement learning provides a framework to automatically acquire suitable reward functions from expert demonstrations. Its extension to multi-agent settings, however, is difficult due to the more complex notions of rational behaviors. In this paper, we propose MA-AIRL, a new framework for multi-agent inverse reinforcement learning, which is effective and scalable for Markov games with high-dimensional state-action space and unknown dynamics. We derive our algorithm based on a new solution concept and maximum pseudolikelihood estimation within an adversarial reward learning framework. In the experiments, we demonstrate that MA-AIRL can recover reward functions that are highly correlated with ground truth ones, and significantly outperforms prior methods in terms of policy imitation.",
            "referenceCount": 59,
            "citationCount": 88,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-05-24",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1907.13220"
            },
            "citationStyles": {
                "bibtex": "@Article{Yu2019MultiAgentAI,\n author = {Lantao Yu and Jiaming Song and Stefano Ermon},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Multi-Agent Adversarial Inverse Reinforcement Learning},\n volume = {abs/1907.13220},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:88880d88073a99107bbc009c9f4a4197562e1e44",
            "@type": "ScholarlyArticle",
            "paperId": "88880d88073a99107bbc009c9f4a4197562e1e44",
            "corpusId": 636855,
            "url": "https://www.semanticscholar.org/paper/88880d88073a99107bbc009c9f4a4197562e1e44",
            "title": "Safe Model-based Reinforcement Learning with Stability Guarantees",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/nips/BerkenkampTS017",
                "MAG": "2618318883",
                "ArXiv": "1705.08551",
                "CorpusId": 636855
            },
            "abstract": "Reinforcement learning is a powerful paradigm for learning optimal policies from experimental data. However, to find optimal policies, most reinforcement learning algorithms explore all possible actions, which may be harmful for real-world systems. As a consequence, learning algorithms are rarely applied on safety-critical systems in the real world. In this paper, we present a learning algorithm that explicitly considers safety, defined in terms of stability guarantees. Specifically, we extend control-theoretic results on Lyapunov stability verification and show how to use statistical models of the dynamics to obtain high-performance control policies with provable stability certificates. Moreover, under additional regularity assumptions in terms of a Gaussian process prior, we prove that one can effectively and safely collect data in order to learn about the dynamics and thus both improve control performance and expand the safe region of the state space. In our experiments, we show how the resulting algorithm can safely optimize a neural network policy on a simulated inverted pendulum, without the pendulum ever falling down.",
            "referenceCount": 44,
            "citationCount": 706,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-05-23",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Berkenkamp2017SafeMR,\n author = {Felix Berkenkamp and M. Turchetta and Angela P. Schoellig and Andreas Krause},\n booktitle = {Neural Information Processing Systems},\n pages = {908-918},\n title = {Safe Model-based Reinforcement Learning with Stability Guarantees},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8a191a0ccec5f4d060db3ddf5e9ea852d53d3f34",
            "@type": "ScholarlyArticle",
            "paperId": "8a191a0ccec5f4d060db3ddf5e9ea852d53d3f34",
            "corpusId": 208921034,
            "url": "https://www.semanticscholar.org/paper/8a191a0ccec5f4d060db3ddf5e9ea852d53d3f34",
            "title": "ChainerRL: A Deep Reinforcement Learning Library",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1912-03905",
                "ArXiv": "1912.03905",
                "MAG": "2994240296",
                "CorpusId": 208921034
            },
            "abstract": "In this paper, we introduce ChainerRL, an open-source Deep Reinforcement Learning (DRL) library built using Python and the Chainer deep learning framework. ChainerRL implements a comprehensive set of DRL algorithms and techniques drawn from the state-of-the-art research in the field. To foster reproducible research, and for instructional purposes, ChainerRL provides scripts that closely replicate the original papers' experimental settings and reproduce published benchmark results for several algorithms. Lastly, ChainerRL offers a visualization tool that enables the qualitative inspection of trained agents. The ChainerRL source code can be found on GitHub: this https URL .",
            "referenceCount": 41,
            "citationCount": 88,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-12-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1912.03905"
            },
            "citationStyles": {
                "bibtex": "@Article{Fujita2019ChainerRLAD,\n author = {Yasuhiro Fujita and Toshiki Kataoka and P. Nagarajan and T. Ishikawa},\n booktitle = {Journal of machine learning research},\n journal = {ArXiv},\n title = {ChainerRL: A Deep Reinforcement Learning Library},\n volume = {abs/1912.03905},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ba7bf6e857da202314176895ec2bc47d330256e2",
            "@type": "ScholarlyArticle",
            "paperId": "ba7bf6e857da202314176895ec2bc47d330256e2",
            "corpusId": 51952349,
            "url": "https://www.semanticscholar.org/paper/ba7bf6e857da202314176895ec2bc47d330256e2",
            "title": "Learning to Optimize Join Queries With Deep Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1808-03196",
                "ArXiv": "1808.03196",
                "MAG": "2885801653",
                "CorpusId": 51952349
            },
            "abstract": "Exhaustive enumeration of all possible join orders is often avoided, and most optimizers leverage heuristics to prune the search space. The design and implementation of heuristics are well-understood when the cost model is roughly linear, and we find that these heuristics can be significantly suboptimal when there are non-linearities in cost. Ideally, instead of a fixed heuristic, we would want a strategy to guide the search space in a more data-driven way---tailoring the search to a specific dataset and query workload. Recognizing the link between classical Dynamic Programming enumeration methods and recent results in Reinforcement Learning (RL), we propose a new method for learning optimized join search strategies. We present our RL-based DQ optimizer, which currently optimizes select-project-join blocks. We implement three versions of DQ to illustrate the ease of integration into existing DBMSes: (1) A version built on top of Apache Calcite, (2) a version integrated into PostgreSQL, and (3) a version integrated into SparkSQL. Our extensive evaluation shows that DQ achieves plans with optimization costs and query execution times competitive with the native query optimizer in each system, but can execute significantly faster after learning (often by orders of magnitude).",
            "referenceCount": 49,
            "citationCount": 192,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-08-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1808.03196"
            },
            "citationStyles": {
                "bibtex": "@Article{Krishnan2018LearningTO,\n author = {S. Krishnan and Zongheng Yang and Ken Goldberg and J. Hellerstein and I. Stoica},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Learning to Optimize Join Queries With Deep Reinforcement Learning},\n volume = {abs/1808.03196},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:caea502325b6a82b1b437c62585992609b5aa542",
            "@type": "ScholarlyArticle",
            "paperId": "caea502325b6a82b1b437c62585992609b5aa542",
            "corpusId": 53104210,
            "url": "https://www.semanticscholar.org/paper/caea502325b6a82b1b437c62585992609b5aa542",
            "title": "Assessing Generalization in Deep Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2898436992",
                "DBLP": "journals/corr/abs-1810-12282",
                "ArXiv": "1810.12282",
                "CorpusId": 53104210
            },
            "abstract": "Deep reinforcement learning (RL) has achieved breakthrough results on many tasks, but agents often fail to generalize beyond the environment they were trained in. As a result, deep RL algorithms that promote generalization are receiving increasing attention. However, works in this area use a wide variety of tasks and experimental setups for evaluation. The literature lacks a controlled assessment of the merits of different generalization schemes. Our aim is to catalyze community-wide progress on generalization in deep RL. To this end, we present a benchmark and experimental protocol, and conduct a systematic empirical study. Our framework contains a diverse set of environments, our methodology covers both in-distribution and out-of-distribution generalization, and our evaluation includes deep RL algorithms that specifically tackle generalization. Our key finding is that `vanilla' deep RL algorithms generalize better than specialized schemes that were proposed specifically to tackle generalization.",
            "referenceCount": 53,
            "citationCount": 186,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1810.12282"
            },
            "citationStyles": {
                "bibtex": "@Article{Packer2018AssessingGI,\n author = {Charles Packer and Katelyn Gao and Jernej Kos and Philipp Kr\u00e4henb\u00fchl and V. Koltun and D. Song},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Assessing Generalization in Deep Reinforcement Learning},\n volume = {abs/1810.12282},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a473f545318325ba23b7a6b477485d29777ba873",
            "@type": "ScholarlyArticle",
            "paperId": "a473f545318325ba23b7a6b477485d29777ba873",
            "corpusId": 430714,
            "url": "https://www.semanticscholar.org/paper/a473f545318325ba23b7a6b477485d29777ba873",
            "title": "ViZDoom: A Doom-based AI research platform for visual reinforcement learning",
            "venue": "IEEE Conference on Computational Intelligence and Games",
            "publicationVenue": {
                "id": "urn:research:6fc6b436-90f0-4b58-a339-85aed71b82f7",
                "name": "IEEE Conference on Computational Intelligence and Games",
                "alternate_names": [
                    "CIG",
                    "IEEE Conf Comput Intell Game",
                    "Comput Intell Game",
                    "Computational Intelligence and Games"
                ],
                "issn": null,
                "url": "http://www.ieee-cig.org/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1605.02097",
                "MAG": "2362143032",
                "DBLP": "journals/corr/KempkaWRTJ16",
                "DOI": "10.1109/CIG.2016.7860433",
                "CorpusId": 430714
            },
            "abstract": "The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.",
            "referenceCount": 32,
            "citationCount": 626,
            "influentialCitationCount": 99,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1605.02097",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-05-06",
            "journal": {
                "name": "2016 IEEE Conference on Computational Intelligence and Games (CIG)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kempka2016ViZDoomAD,\n author = {Michal Kempka and Marek Wydmuch and Grzegorz Runc and Jakub Toczek and Wojciech Ja\u015bkowski},\n booktitle = {IEEE Conference on Computational Intelligence and Games},\n journal = {2016 IEEE Conference on Computational Intelligence and Games (CIG)},\n pages = {1-8},\n title = {ViZDoom: A Doom-based AI research platform for visual reinforcement learning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3ac0fea1e5395cfb0dc1f0ee2b921fe22b23fed0",
            "@type": "ScholarlyArticle",
            "paperId": "3ac0fea1e5395cfb0dc1f0ee2b921fe22b23fed0",
            "corpusId": 14631101,
            "url": "https://www.semanticscholar.org/paper/3ac0fea1e5395cfb0dc1f0ee2b921fe22b23fed0",
            "title": "Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/icml/FoersterNFATKW17",
                "MAG": "2949201811",
                "ArXiv": "1702.08887",
                "CorpusId": 14631101
            },
            "abstract": "Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on single-agent RL to the multi-agent setting. A major stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep Q-learning relies. This paper proposes two methods that address this problem: 1) using a multi-agent variant of importance sampling to naturally decay obsolete data and 2) conditioning each agent's value function on a fingerprint that disambiguates the age of the data sampled from the replay memory. Results on a challenging decentralised variant of StarCraft unit micro-management confirm that these methods enable the successful combination of experience replay with multi-agent RL.",
            "referenceCount": 38,
            "citationCount": 516,
            "influentialCitationCount": 41,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-02-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Foerster2017StabilisingER,\n author = {Jakob N. Foerster and Nantas Nardelli and Gregory Farquhar and Triantafyllos Afouras and Philip H. S. Torr and Pushmeet Kohli and Shimon Whiteson},\n booktitle = {International Conference on Machine Learning},\n pages = {1146-1155},\n title = {Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fe2ef22089712fcff33a77761860a10b7834da47",
            "@type": "ScholarlyArticle",
            "paperId": "fe2ef22089712fcff33a77761860a10b7834da47",
            "corpusId": 14149803,
            "url": "https://www.semanticscholar.org/paper/fe2ef22089712fcff33a77761860a10b7834da47",
            "title": "Socially aware motion planning with deep reinforcement learning",
            "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
            "publicationVenue": {
                "id": "urn:research:37275deb-3fcf-4d16-ae77-95db9899b1f3",
                "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                "alternate_names": [
                    "IROS",
                    "Intelligent Robots and Systems",
                    "Intell Robot Syst",
                    "IEEE/RJS Int Conf Intell Robot Syst"
                ],
                "issn": null,
                "url": "http://www.iros.org/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/ChenELH17",
                "MAG": "2604216058",
                "ArXiv": "1703.08862",
                "DOI": "10.1109/IROS.2017.8202312",
                "CorpusId": 14149803
            },
            "abstract": "For robotic vehicles to navigate safely and efficiently in pedestrian-rich environments, it is important to model subtle human behaviors and navigation rules (e.g., passing on the right). However, while instinctive to humans, socially compliant navigation is still difficult to quantify due to the stochasticity in people's behaviors. Existing works are mostly focused on using feature-matching techniques to describe and imitate human paths, but often do not generalize well since the feature values can vary from person to person, and even run to run. This work notes that while it is challenging to directly specify the details of what to do (precise mechanisms of human navigation), it is straightforward to specify what not to do (violations of social norms). Specifically, using deep reinforcement learning, this work develops a time-efficient navigation policy that respects common social norms. The proposed method is shown to enable fully autonomous navigation of a robotic vehicle moving at human walking speed in an environment with many pedestrians.",
            "referenceCount": 29,
            "citationCount": 550,
            "influentialCitationCount": 41,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dspace.mit.edu/bitstream/1721.1/114480/1/1703.08862.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-26",
            "journal": {
                "name": "2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2017SociallyAM,\n author = {Yu Fan Chen and Michael Everett and Miao Liu and J. How},\n booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},\n journal = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\n pages = {1343-1350},\n title = {Socially aware motion planning with deep reinforcement learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a2141a5ec0c65ea0a9861ae562f4c9fb8020d197",
            "@type": "ScholarlyArticle",
            "paperId": "a2141a5ec0c65ea0a9861ae562f4c9fb8020d197",
            "corpusId": 9129880,
            "url": "https://www.semanticscholar.org/paper/a2141a5ec0c65ea0a9861ae562f4c9fb8020d197",
            "title": "DARLA: Improving Zero-Shot Transfer in Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1707.08475",
                "DBLP": "journals/corr/HigginsPRMBPBBL17",
                "MAG": "2739083961",
                "CorpusId": 9129880
            },
            "abstract": "Domain adaptation is an important open problem in deep reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA's vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts - even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).",
            "referenceCount": 72,
            "citationCount": 366,
            "influentialCitationCount": 43,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-07-26",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Higgins2017DARLAIZ,\n author = {I. Higgins and Arka Pal and Andrei A. Rusu and L. Matthey and Christopher P. Burgess and A. Pritzel and M. Botvinick and C. Blundell and Alexander Lerchner},\n booktitle = {International Conference on Machine Learning},\n pages = {1480-1490},\n title = {DARLA: Improving Zero-Shot Transfer in Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cf90552b5d2e992e93ab838fd615e1c36618e31c",
            "@type": "ScholarlyArticle",
            "paperId": "cf90552b5d2e992e93ab838fd615e1c36618e31c",
            "corpusId": 31009408,
            "url": "https://www.semanticscholar.org/paper/cf90552b5d2e992e93ab838fd615e1c36618e31c",
            "title": "Distral: Robust multitask reinforcement learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2951545807",
                "DBLP": "conf/nips/TehBCQKHHP17",
                "ArXiv": "1707.04175",
                "CorpusId": 31009408
            },
            "abstract": "Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (Distill & transfer learning). Instead of sharing parameters between the different workers, we propose to share a \"distilled\" policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning.",
            "referenceCount": 35,
            "citationCount": 470,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-07-13",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Teh2017DistralRM,\n author = {Y. Teh and V. Bapst and Wojciech M. Czarnecki and John Quan and J. Kirkpatrick and R. Hadsell and N. Heess and Razvan Pascanu},\n booktitle = {Neural Information Processing Systems},\n pages = {4496-4506},\n title = {Distral: Robust multitask reinforcement learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:98c08d28a2319ce79b62b108dfed3acc0dd80983",
            "@type": "ScholarlyArticle",
            "paperId": "98c08d28a2319ce79b62b108dfed3acc0dd80983",
            "corpusId": 145049081,
            "url": "https://www.semanticscholar.org/paper/98c08d28a2319ce79b62b108dfed3acc0dd80983",
            "title": "Collaborative Evolutionary Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1905.00976",
                "MAG": "2943810245",
                "DBLP": "journals/corr/abs-1905-00976",
                "CorpusId": 145049081
            },
            "abstract": "Deep reinforcement learning algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically struggle with achieving effective exploration and are extremely sensitive to the choice of hyperparameters. One reason is that most approaches use a noisy version of their operating policy to explore - thereby limiting the range of exploration. In this paper, we introduce Collaborative Evolutionary Reinforcement Learning (CERL), a scalable framework that comprises a portfolio of policies that simultaneously explore and exploit diverse regions of the solution space. A collection of learners - typically proven algorithms like TD3 - optimize over varying time-horizons leading to this diverse portfolio. All learners contribute to and use a shared replay buffer to achieve greater sample efficiency. Computational resources are dynamically distributed to favor the best learners as a form of online algorithm selection. Neuroevolution binds this entire process to generate a single emergent learner that exceeds the capabilities of any individual learner. Experiments in a range of continuous control benchmarks demonstrate that the emergent learner significantly outperforms its composite learners while remaining overall more sample-efficient - notably solving the Mujoco Humanoid benchmark where all of its composite learners (TD3) fail entirely in isolation.",
            "referenceCount": 49,
            "citationCount": 78,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-05-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Khadka2019CollaborativeER,\n author = {Shauharda Khadka and Somdeb Majumdar and Tarek Nassar and Zach Dwiel and E. Tumer and Santiago Miret and Yinyin Liu and Kagan Tumer},\n booktitle = {International Conference on Machine Learning},\n pages = {3341-3350},\n title = {Collaborative Evolutionary Reinforcement Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fe14961357ef37735ed95be0ffb30767a468e14f",
            "@type": "ScholarlyArticle",
            "paperId": "fe14961357ef37735ed95be0ffb30767a468e14f",
            "corpusId": 49567896,
            "url": "https://www.semanticscholar.org/paper/fe14961357ef37735ed95be0ffb30767a468e14f",
            "title": "Supervised Reinforcement Learning with Recurrent Neural Network for Dynamic Treatment Recommendation",
            "venue": "Knowledge Discovery and Data Mining",
            "publicationVenue": {
                "id": "urn:research:a0edb93b-1e95-4128-a295-6b1659149cef",
                "name": "Knowledge Discovery and Data Mining",
                "alternate_names": [
                    "KDD",
                    "Knowl Discov Data Min"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigkdd/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2952380488",
                "DBLP": "conf/kdd/WangZHZ18",
                "ArXiv": "1807.01473",
                "DOI": "10.1145/3219819.3219961",
                "CorpusId": 49567896
            },
            "abstract": "Dynamic treatment recommendation systems based on large-scale electronic health records (EHRs) become a key to successfully improve practical clinical outcomes. Prior relevant studies recommend treatments either use supervised learning (e.g. matching the indicator signal which denotes doctor prescriptions), or reinforcement learning (e.g. maximizing evaluation signal which indicates cumulative reward from survival rates). However, none of these studies have considered to combine the benefits of supervised learning and reinforcement learning. In this paper, we propose Supervised Reinforcement Learning with Recurrent Neural Network (SRL-RNN), which fuses them into a synergistic learning framework. Specifically, SRL-RNN applies an off-policy actor-critic framework to handle complex relations among multiple medications, diseases and individual characteristics. The \"actor'' in the framework is adjusted by both the indicator signal and evaluation signal to ensure effective prescription and low mortality. RNN is further utilized to solve the Partially-Observed Markov Decision Process (POMDP) problem due to lack of fully observed states in real world applications. Experiments on the publicly real-world dataset, i.e., MIMIC-3, illustrate that our model can reduce the estimated mortality, while providing promising accuracy in matching doctors' prescriptions.",
            "referenceCount": 48,
            "citationCount": 200,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1807.01473",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2018-07-04",
            "journal": {
                "name": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2018SupervisedRL,\n author = {Lu Wang and Wei Zhang and Xiaofeng He and H. Zha},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},\n title = {Supervised Reinforcement Learning with Recurrent Neural Network for Dynamic Treatment Recommendation},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c85b3bd562039471b54b03eebbae2ae5c6a574cc",
            "@type": "ScholarlyArticle",
            "paperId": "c85b3bd562039471b54b03eebbae2ae5c6a574cc",
            "corpusId": 5046995,
            "url": "https://www.semanticscholar.org/paper/c85b3bd562039471b54b03eebbae2ae5c6a574cc",
            "title": "A Reinforcement Learning Based Approach for Automated Lane Change Maneuvers",
            "venue": "2018 IEEE Intelligent Vehicles Symposium (IV)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1804-07871",
                "MAG": "2963322416",
                "ArXiv": "1804.07871",
                "DOI": "10.1109/IVS.2018.8500556",
                "CorpusId": 5046995
            },
            "abstract": "Lane change is a crucial vehicle maneuver which needs coordination with surrounding vehicles. Automated lane changing functions built on rule-based models may perform well under pre-defined operating conditions, but they may be prone to failure when unexpected situations are encountered. In our study, we proposed a Reinforcement Learning based approach to train the vehicle agent to learn an automated lane change behavior such that it can intelligently make a lane change under diverse and even unforeseen scenarios. Particularly, we treated both state space and action space as continuous, and designed a Q-function approximator that has a closed-form greedy policy, which contributes to the computation efficiency of our deep Q-learning algorithm. Extensive simulations are conducted for training the algorithm, and the results illustrate that the Reinforcement Learning based vehicle agent is capable of learning a smooth and efficient driving policy for lane change maneuvers.",
            "referenceCount": 21,
            "citationCount": 200,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1804.07871",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-04-21",
            "journal": {
                "name": "2018 IEEE Intelligent Vehicles Symposium (IV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2018ARL,\n author = {Pin Wang and Ching-yao Chan and A. D. L. Fortelle},\n booktitle = {2018 IEEE Intelligent Vehicles Symposium (IV)},\n journal = {2018 IEEE Intelligent Vehicles Symposium (IV)},\n pages = {1379-1384},\n title = {A Reinforcement Learning Based Approach for Automated Lane Change Maneuvers},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e49c80f8b12a100c5f4518897c4cbf72710c252",
            "@type": "ScholarlyArticle",
            "paperId": "5e49c80f8b12a100c5f4518897c4cbf72710c252",
            "corpusId": 46939951,
            "url": "https://www.semanticscholar.org/paper/5e49c80f8b12a100c5f4518897c4cbf72710c252",
            "title": "Relational Deep Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2807340089",
                "DBLP": "journals/corr/abs-1806-01830",
                "ArXiv": "1806.01830",
                "CorpusId": 46939951
            },
            "abstract": "We introduce an approach for deep reinforcement learning (RL) that improves upon the efficiency, generalization capacity, and interpretability of conventional approaches through structured perception and relational reasoning. It uses self-attention to iteratively reason about the relations between entities in a scene and to guide a model-free policy. Our results show that in a novel navigation and planning task called Box-World, our agent finds interpretable solutions that improve upon baselines in terms of sample complexity, ability to generalize to more complex scenes than experienced during training, and overall performance. In the StarCraft II Learning Environment, our agent achieves state-of-the-art performance on six mini-games -- surpassing human grandmaster performance on four. By considering architectural inductive biases, our work opens new directions for overcoming important, but stubborn, challenges in deep RL.",
            "referenceCount": 39,
            "citationCount": 196,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.01830"
            },
            "citationStyles": {
                "bibtex": "@Article{Zambaldi2018RelationalDR,\n author = {V. Zambaldi and David Raposo and Adam Santoro and V. Bapst and Yujia Li and Igor Babuschkin and K. Tuyls and David P. Reichert and T. Lillicrap and Edward Lockhart and M. Shanahan and Victoria Langston and Razvan Pascanu and M. Botvinick and Oriol Vinyals and P. Battaglia},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Relational Deep Reinforcement Learning},\n volume = {abs/1806.01830},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9d4d8509f6da094a7c31e063f307e0e8592db27f",
            "@type": "ScholarlyArticle",
            "paperId": "9d4d8509f6da094a7c31e063f307e0e8592db27f",
            "corpusId": 49312150,
            "url": "https://www.semanticscholar.org/paper/9d4d8509f6da094a7c31e063f307e0e8592db27f",
            "title": "A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress",
            "venue": "Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:96018464-22dc-4b5c-a172-c2f4a30ce131",
                "name": "Artificial Intelligence",
                "alternate_names": [
                    "Artif Intell"
                ],
                "issn": "0004-3702",
                "url": "http://www.elsevier.com/locate/artint"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1806-06877",
                "ArXiv": "1806.06877",
                "MAG": "3138984732",
                "DOI": "10.1016/J.ARTINT.2021.103500",
                "CorpusId": 49312150
            },
            "abstract": null,
            "referenceCount": 117,
            "citationCount": 333,
            "influentialCitationCount": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://manuscript.elsevier.com/S0004370221000515/pdf/S0004370221000515.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-06-18",
            "journal": {
                "name": "Artif. Intell.",
                "volume": "297"
            },
            "citationStyles": {
                "bibtex": "@Article{Arora2018ASO,\n author = {Saurabh Arora and Prashant Doshi},\n booktitle = {Artificial Intelligence},\n journal = {Artif. Intell.},\n pages = {103500},\n title = {A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress},\n volume = {297},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8f06c78435b9712aa77094796075b8e6b3f50c05",
            "@type": "ScholarlyArticle",
            "paperId": "8f06c78435b9712aa77094796075b8e6b3f50c05",
            "corpusId": 59222794,
            "url": "https://www.semanticscholar.org/paper/8f06c78435b9712aa77094796075b8e6b3f50c05",
            "title": "Causal Reasoning from Meta-reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1901.08162",
                "DBLP": "journals/corr/abs-1901-08162",
                "MAG": "2909534157",
                "CorpusId": 59222794
            },
            "abstract": "Discovering and exploiting the causal structure in the environment is a crucial challenge for intelligent agents. Here we explore whether causal reasoning can emerge via meta-reinforcement learning. We train a recurrent network with model-free reinforcement learning to solve a range of problems that each contain causal structure. We find that the trained agent can perform causal reasoning in novel situations in order to obtain rewards. The agent can select informative interventions, draw causal inferences from observational data, and make counterfactual predictions. Although established formal causal reasoning algorithms also exist, in this paper we show that such reasoning can arise from model-free reinforcement learning, and suggest that causal reasoning in complex settings may benefit from the more end-to-end learning-based approaches presented here. This work also offers new strategies for structured exploration in reinforcement learning, by providing agents with the ability to perform -- and interpret -- experiments.",
            "referenceCount": 42,
            "citationCount": 98,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-01-23",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1901.08162"
            },
            "citationStyles": {
                "bibtex": "@Article{Dasgupta2019CausalRF,\n author = {Ishita Dasgupta and Jane X. Wang and S. Chiappa and Jovana Mitrovic and Pedro A. Ortega and David Raposo and Edward Hughes and P. Battaglia and M. Botvinick and Z. Kurth-Nelson},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Causal Reasoning from Meta-reinforcement Learning},\n volume = {abs/1901.08162},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ee9893ff2aa325ff3c9920f247436c514fd8b512",
            "@type": "ScholarlyArticle",
            "paperId": "ee9893ff2aa325ff3c9920f247436c514fd8b512",
            "corpusId": 153312660,
            "url": "https://www.semanticscholar.org/paper/ee9893ff2aa325ff3c9920f247436c514fd8b512",
            "title": "SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2951553872",
                "DBLP": "conf/icml/ZhangVSA0L19",
                "CorpusId": 153312660
            },
            "abstract": "Model-based reinforcement learning (RL) has proven to be a data efficient approach for learning control tasks but is difficult to utilize in domains with complex observations such as images. In this paper, we present a method for learning representations that are suitable for iterative model-based policy improvement, even when the underlying dynamical system has complex dynamics and image observations, in that these representations are optimized for inferring simple dynamics and cost models given data from the current policy. This enables a model-based RL method based on the linear-quadratic regulator (LQR) to be used for systems with image observations. We evaluate our approach on a range of robotics tasks, including manipulation with a real-world robotic arm directly from images. We find that our method produces substantially better final performance than other model-based RL methods while being significantly more efficient than model-free RL.",
            "referenceCount": 58,
            "citationCount": 193,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-08-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2018SOLARDS,\n author = {Marvin Zhang and S. Vikram and Laura M. Smith and P. Abbeel and Matthew J. Johnson and S. Levine},\n booktitle = {International Conference on Machine Learning},\n pages = {7444-7453},\n title = {SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6bc692616db7b1a7ef2ea7c270c893adfb57ed0e",
            "@type": "ScholarlyArticle",
            "paperId": "6bc692616db7b1a7ef2ea7c270c893adfb57ed0e",
            "corpusId": 54446702,
            "url": "https://www.semanticscholar.org/paper/6bc692616db7b1a7ef2ea7c270c893adfb57ed0e",
            "title": "Deep Reinforcement Learning and the Deadly Triad",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2902098903",
                "DBLP": "journals/corr/abs-1812-02648",
                "ArXiv": "1812.02648",
                "CorpusId": 54446702
            },
            "abstract": "We know from reinforcement learning theory that temporal difference learning can fail in certain cases. Sutton and Barto (2018) identify a deadly triad of function approximation, bootstrapping, and off-policy learning. When these three properties are combined, learning can diverge with the value estimates becoming unbounded. However, several algorithms successfully combine these three properties, which indicates that there is at least a partial gap in our understanding. In this work, we investigate the impact of the deadly triad in practice, in the context of a family of popular deep reinforcement learning models - deep Q-networks trained with experience replay - analysing how the components of this system play a role in the emergence of the deadly triad, and in the agent's performance",
            "referenceCount": 34,
            "citationCount": 182,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-12-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1812.02648"
            },
            "citationStyles": {
                "bibtex": "@Article{Hasselt2018DeepRL,\n author = {H. V. Hasselt and Yotam Doron and Florian Strub and Matteo Hessel and Nicolas Sonnerat and Joseph Modayil},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Deep Reinforcement Learning and the Deadly Triad},\n volume = {abs/1812.02648},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d8a545a02aa0e0cbe42607212433b2b4f446e5ab",
            "@type": "ScholarlyArticle",
            "paperId": "d8a545a02aa0e0cbe42607212433b2b4f446e5ab",
            "corpusId": 201133778,
            "url": "https://www.semanticscholar.org/paper/d8a545a02aa0e0cbe42607212433b2b4f446e5ab",
            "title": "Reinforcement Learning in Financial Markets",
            "venue": "International Conference on Data Technologies and Applications",
            "publicationVenue": {
                "id": "urn:research:dea8d7a8-c89e-4de9-bde3-0787594a055f",
                "name": "International Conference on Data Technologies and Applications",
                "alternate_names": [
                    "DATA",
                    "International Conference on Data Science, E-Learning and Information Systems",
                    "Int Conf Data Sci E-learning Inf Syst",
                    "Data",
                    "Int Conf Data Technol Appl"
                ],
                "issn": "2306-5729",
                "url": "http://www.dataconference.org/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/data/MengK19",
                "MAG": "2965771985",
                "DOI": "10.3390/DATA4030110",
                "CorpusId": 201133778
            },
            "abstract": "Recently there has been an exponential increase in the use of artificial intelligence for trading in financial markets such as stock and forex. Reinforcement learning has become of particular interest to financial traders ever since the program AlphaGo defeated the strongest human contemporary Go board game player Lee Sedol in 2016. We systematically reviewed all recent stock/forex prediction or trading articles that used reinforcement learning as their primary machine learning method. All reviewed articles had some unrealistic assumptions such as no transaction costs, no liquidity issues and no bid or ask spread issues. Transaction costs had significant impacts on the profitability of the reinforcement learning algorithms compared with the baseline algorithms tested. Despite showing statistically significant profitability when reinforcement learning was used in comparison with baseline models in many studies, some showed no meaningful level of profitability, in particular with large changes in the price pattern between the system training and testing data. Furthermore, few performance comparisons between reinforcement learning and other sophisticated machine/deep learning models were provided. The impact of transaction costs, including the bid/ask spread on profitability has also been assessed. In conclusion, reinforcement learning in stock/forex trading is still in its early development and further research is needed to make it a reliable method in this domain.",
            "referenceCount": 30,
            "citationCount": 89,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2306-5729/4/3/110/pdf?version=1567155534",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Business",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Business",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-07-01",
            "journal": {
                "name": "Data",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Meng2019ReinforcementLI,\n author = {Terry Lingze Meng and Matloob Khushi},\n booktitle = {International Conference on Data Technologies and Applications},\n journal = {Data},\n pages = {110},\n title = {Reinforcement Learning in Financial Markets},\n volume = {4},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c37e0d93d19efbd8bb50d1a4d92979793f4341d2",
            "@type": "ScholarlyArticle",
            "paperId": "c37e0d93d19efbd8bb50d1a4d92979793f4341d2",
            "corpusId": 3307812,
            "url": "https://www.semanticscholar.org/paper/c37e0d93d19efbd8bb50d1a4d92979793f4341d2",
            "title": "Reinforcement Learning from Imperfect Demonstrations",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2785379783",
                "DBLP": "journals/corr/abs-1802-05313",
                "ArXiv": "1802.05313",
                "CorpusId": 3307812
            },
            "abstract": "Robust real-world learning should benefit from both demonstrations and interactions with the environment. Current approaches to learning from demonstration and reward perform supervised learning on expert demonstration data and use reinforcement learning to further improve performance based on the reward received from the environment. These tasks have divergent losses which are difficult to jointly optimize and such methods can be very sensitive to noisy demonstrations. We propose a unified reinforcement learning algorithm, Normalized Actor-Critic (NAC), that effectively normalizes the Q-function, reducing the Q-values of actions unseen in the demonstration data. NAC learns an initial policy network from demonstrations and refines the policy in the environment, surpassing the demonstrator's performance. Crucially, both learning from demonstration and interactive refinement use the same objective, unlike prior approaches that combine distinct supervised and reinforcement losses. This makes NAC robust to suboptimal demonstration data since the method is not forced to mimic all of the examples in the dataset. We show that our unified reinforcement learning algorithm can learn robustly and outperform existing baselines when evaluated on several realistic driving games.",
            "referenceCount": 41,
            "citationCount": 181,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.05313"
            },
            "citationStyles": {
                "bibtex": "@Article{Gao2018ReinforcementLF,\n author = {Yang Gao and Huazhe Xu and Ji Lin and F. Yu and S. Levine and Trevor Darrell},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Reinforcement Learning from Imperfect Demonstrations},\n volume = {abs/1802.05313},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e4a89a978f747d0b548f5887b2380c5f618061f0",
            "@type": "ScholarlyArticle",
            "paperId": "e4a89a978f747d0b548f5887b2380c5f618061f0",
            "corpusId": 52909341,
            "url": "https://www.semanticscholar.org/paper/e4a89a978f747d0b548f5887b2380c5f618061f0",
            "title": "Near-Optimal Representation Learning for Hierarchical Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2894605519",
                "ArXiv": "1810.01257",
                "DBLP": "journals/corr/abs-1810-01257",
                "CorpusId": 52909341
            },
            "abstract": "We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation -- the mapping of observation space to goal space -- is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods (see videos at this https URL).",
            "referenceCount": 35,
            "citationCount": 170,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-10-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1810.01257"
            },
            "citationStyles": {
                "bibtex": "@Article{Nachum2018NearOptimalRL,\n author = {Ofir Nachum and S. Gu and Honglak Lee and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Near-Optimal Representation Learning for Hierarchical Reinforcement Learning},\n volume = {abs/1810.01257},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3ada65624811cfc46bc113509f3120100bcf4f52",
            "@type": "ScholarlyArticle",
            "paperId": "3ada65624811cfc46bc113509f3120100bcf4f52",
            "corpusId": 86443261,
            "url": "https://www.semanticscholar.org/paper/3ada65624811cfc46bc113509f3120100bcf4f52",
            "title": "Universal quantum control through deep reinforcement learning",
            "venue": "npj Quantum Information",
            "publicationVenue": {
                "id": "urn:research:f8411b17-d726-4af8-a6ae-1ee0b6a5877f",
                "name": "npj Quantum Information",
                "alternate_names": [
                    "npj Quantum Inf"
                ],
                "issn": "2056-6387",
                "url": "http://www.nature.com/npjqi/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2792427217",
                "ArXiv": "1803.01857",
                "DOI": "10.1038/s41534-019-0141-3",
                "CorpusId": 86443261
            },
            "abstract": null,
            "referenceCount": 65,
            "citationCount": 240,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41534-019-0141-3.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Physics",
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2018-03-05",
            "journal": {
                "name": "npj Quantum Information",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Niu2018UniversalQC,\n author = {M. Niu and S. Boixo and V. Smelyanskiy and H. Neven},\n booktitle = {npj Quantum Information},\n journal = {npj Quantum Information},\n title = {Universal quantum control through deep reinforcement learning},\n volume = {5},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d9b1d6313615e81ce862273373e48a72293b3223",
            "@type": "ScholarlyArticle",
            "paperId": "d9b1d6313615e81ce862273373e48a72293b3223",
            "corpusId": 54085699,
            "url": "https://www.semanticscholar.org/paper/d9b1d6313615e81ce862273373e48a72293b3223",
            "title": "Reinforcement Learning with Neural Networks for Quantum Feedback",
            "venue": "Physical Review X",
            "publicationVenue": {
                "id": "urn:research:98eedf55-1e67-4c3d-a25d-79861b87ae04",
                "name": "Physical Review X",
                "alternate_names": [
                    "Phys Rev X"
                ],
                "issn": "2160-3308",
                "url": "https://journals.aps.org/prx/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2786295636",
                "ArXiv": "1802.05267",
                "DOI": "10.1103/PhysRevX.8.031084",
                "CorpusId": 54085699
            },
            "abstract": "Machine learning with artificial neural networks is revolutionizing science. The most advanced challenges require discovering answers autonomously. This is the domain of reinforcement learning, where control strategies are improved according to a reward function. The power of neural-network-based reinforcement learning has been highlighted by spectacular recent successes, such as playing Go, but its benefits for physics are yet to be demonstrated. Here, we show how a network-based \"agent\" can discover complete quantum-error-correction strategies, protecting a collection of qubits against noise. These strategies require feedback adapted to measurement outcomes. Finding them from scratch, without human guidance, tailored to different hardware resources, is a formidable challenge due to the combinatorially large search space. To solve this, we develop two ideas: two-stage learning with teacher/student networks and a reward quantifying the capability to recover the quantum information stored in a multi-qubit system. Beyond its immediate impact on quantum computation, our work more generally demonstrates the promise of neural-network-based reinforcement learning in physics.",
            "referenceCount": 79,
            "citationCount": 194,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://link.aps.org/pdf/10.1103/PhysRevX.8.031084",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Physics",
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2018-02-14",
            "journal": {
                "name": "Physical Review X",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Fosel2018ReinforcementLW,\n author = {T. Fosel and P. Tighineanu and Talitha Weiss and F. Marquardt},\n booktitle = {Physical Review X},\n journal = {Physical Review X},\n title = {Reinforcement Learning with Neural Networks for Quantum Feedback},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c8221c054459e37edbf313668523d667fe5c1536",
            "@type": "ScholarlyArticle",
            "paperId": "c8221c054459e37edbf313668523d667fe5c1536",
            "corpusId": 336219,
            "url": "https://www.semanticscholar.org/paper/c8221c054459e37edbf313668523d667fe5c1536",
            "title": "Maximum Entropy Inverse Reinforcement Learning",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/aaai/ZiebartMBD08",
                "MAG": "2098774185",
                "CorpusId": 336219
            },
            "abstract": "Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. \n \nWe develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.",
            "referenceCount": 10,
            "citationCount": 2473,
            "influentialCitationCount": 451,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2008-07-13",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ziebart2008MaximumEI,\n author = {Brian D. Ziebart and Andrew L. Maas and J. Bagnell and A. Dey},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {1433-1438},\n title = {Maximum Entropy Inverse Reinforcement Learning},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1bead9000a719cb258bac7320228055aee650d2c",
            "@type": "ScholarlyArticle",
            "paperId": "1bead9000a719cb258bac7320228055aee650d2c",
            "corpusId": 23192910,
            "url": "https://www.semanticscholar.org/paper/1bead9000a719cb258bac7320228055aee650d2c",
            "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2741122588",
                "DBLP": "journals/corr/VecerikHSWPPHRL17",
                "ArXiv": "1707.08817",
                "CorpusId": 23192910
            },
            "abstract": "We propose a general and model-free approach for Reinforcement Learning (RL) on real robotics with sparse rewards. We build upon the Deep Deterministic Policy Gradient (DDPG) algorithm to use demonstrations. Both demonstrations and actual interactions are used to fill a replay buffer and the sampling ratio between demonstrations and transitions is automatically tuned via a prioritized replay mechanism. Typically, carefully engineered shaping rewards are required to enable the agents to efficiently explore on high dimensional control problems such as robotics. They are also required for model-based acceleration methods relying on local solvers such as iLQG (e.g. Guided Policy Search and Normalized Advantage Function). The demonstrations replace the need for carefully engineered rewards, and reduce the exploration problem encountered by classical RL approaches in these domains. Demonstrations are collected by a robot kinesthetically force-controlled by a human demonstrator. Results on four simulated insertion tasks show that DDPG from demonstrations out-performs DDPG, and does not require engineered rewards. Finally, we demonstrate the method on a real robotics task consisting of inserting a clip (flexible object) into a rigid object.",
            "referenceCount": 22,
            "citationCount": 530,
            "influentialCitationCount": 34,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-07-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1707.08817"
            },
            "citationStyles": {
                "bibtex": "@Article{Vecer\u00edk2017LeveragingDF,\n author = {Matej Vecer\u00edk and Todd Hester and Jonathan Scholz and Fumin Wang and O. Pietquin and Bilal Piot and N. Heess and Thomas Roth\u00f6rl and Thomas Lampe and Martin A. Riedmiller},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards},\n volume = {abs/1707.08817},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f96478d0694f18384934fc19a2655170f32e2d8c",
            "@type": "ScholarlyArticle",
            "paperId": "f96478d0694f18384934fc19a2655170f32e2d8c",
            "corpusId": 9398383,
            "url": "https://www.semanticscholar.org/paper/f96478d0694f18384934fc19a2655170f32e2d8c",
            "title": "Deep Direct Reinforcement Learning for Financial Signal Representation and Trading",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/tnn/DengBKRD17",
                "MAG": "2344786740",
                "DOI": "10.1109/TNNLS.2016.2522401",
                "CorpusId": 9398383,
                "PubMed": "26890927"
            },
            "abstract": "Can we train the computer to beat experienced traders for financial assert trading? In this paper, we try to address this challenge by introducing a recurrent deep neural network (NN) for real-time financial signal representation and trading. Our model is inspired by two biological-related learning concepts of deep learning (DL) and reinforcement learning (RL). In the framework, the DL part automatically senses the dynamic market condition for informative feature learning. Then, the RL module interacts with deep representations and makes trading decisions to accumulate the ultimate rewards in an unknown environment. The learning system is implemented in a complex NN that exhibits both the deep and recurrent structures. Hence, we propose a task-aware backpropagation through time method to cope with the gradient vanishing issue in deep training. The robustness of the neural system is verified on both the stock and the commodity future markets under broad testing conditions.",
            "referenceCount": 46,
            "citationCount": 532,
            "influentialCitationCount": 34,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-03-01",
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "28"
            },
            "citationStyles": {
                "bibtex": "@Article{Deng2017DeepDR,\n author = {Yue Deng and Feng Bao and Youyong Kong and Zhiquan Ren and Qionghai Dai},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {653-664},\n title = {Deep Direct Reinforcement Learning for Financial Signal Representation and Trading},\n volume = {28},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:600bfe5f0597ebd84898f0c4270ddfb3750594f5",
            "@type": "ScholarlyArticle",
            "paperId": "600bfe5f0597ebd84898f0c4270ddfb3750594f5",
            "corpusId": 13301124,
            "url": "https://www.semanticscholar.org/paper/600bfe5f0597ebd84898f0c4270ddfb3750594f5",
            "title": "Imagination-Augmented Agents for Deep Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2964295739",
                "ArXiv": "1707.06203",
                "DBLP": "conf/nips/RacaniereWRBGRB17",
                "CorpusId": 13301124
            },
            "abstract": "We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.",
            "referenceCount": 60,
            "citationCount": 492,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-07-19",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1707.06203"
            },
            "citationStyles": {
                "bibtex": "@Article{Racani\u00e8re2017ImaginationAugmentedAF,\n author = {S. Racani\u00e8re and T. Weber and David P. Reichert and Lars Buesing and A. Guez and Danilo Jimenez Rezende and Adri\u00e0 Puigdom\u00e8nech Badia and Oriol Vinyals and N. Heess and Yujia Li and Razvan Pascanu and P. Battaglia and D. Hassabis and David Silver and Daan Wierstra},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Imagination-Augmented Agents for Deep Reinforcement Learning},\n volume = {abs/1707.06203},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fb9693183bc74568c72188431c18cb2b07c87213",
            "@type": "ScholarlyArticle",
            "paperId": "fb9693183bc74568c72188431c18cb2b07c87213",
            "corpusId": 3694591,
            "url": "https://www.semanticscholar.org/paper/fb9693183bc74568c72188431c18cb2b07c87213",
            "title": "Hierarchical Imitation and Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2789740610",
                "ArXiv": "1803.00590",
                "DBLP": "conf/icml/0001JADYD18",
                "CorpusId": 3694591
            },
            "abstract": "We study how to effectively leverage expert feedback to learn sequential decision-making policies. We focus on problems with sparse rewards and long time horizons, which typically pose significant challenges in reinforcement learning. We propose an algorithmic framework, called hierarchical guidance, that leverages the hierarchical structure of the underlying problem to integrate different modes of expert interaction. Our framework can incorporate different combinations of imitation learning (IL) and reinforcement learning (RL) at different levels, leading to dramatic reductions in both expert effort and cost of exploration. Using long-horizon benchmarks, including Montezuma's Revenge, we demonstrate that our approach can learn significantly faster than hierarchical RL, and be significantly more label-efficient than standard IL. We also theoretically analyze labeling cost for certain instantiations of our framework.",
            "referenceCount": 38,
            "citationCount": 159,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-03-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1803.00590"
            },
            "citationStyles": {
                "bibtex": "@Article{Le2018HierarchicalIA,\n author = {Hoang Minh Le and Nan Jiang and Alekh Agarwal and Miroslav Dud\u00edk and Yisong Yue and Hal Daum\u00e9},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Hierarchical Imitation and Reinforcement Learning},\n volume = {abs/1803.00590},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4edd98e3947d8406ec95518c294721757afffb5d",
            "@type": "ScholarlyArticle",
            "paperId": "4edd98e3947d8406ec95518c294721757afffb5d",
            "corpusId": 38125055,
            "url": "https://www.semanticscholar.org/paper/4edd98e3947d8406ec95518c294721757afffb5d",
            "title": "Deep reinforcement learning for de novo drug design",
            "venue": "Science Advances",
            "publicationVenue": {
                "id": "urn:research:cb30f0c9-2980-4b7d-bbcb-68fc5472b97c",
                "name": "Science Advances",
                "alternate_names": [
                    "Sci Adv"
                ],
                "issn": "2375-2548",
                "url": "http://www.scienceadvances.org/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "3100751385",
                "DBLP": "journals/corr/abs-1711-10907",
                "PubMedCentral": "6059760",
                "ArXiv": "1711.10907",
                "DOI": "10.1126/sciadv.aap7885",
                "CorpusId": 38125055,
                "PubMed": "30050984"
            },
            "abstract": "We introduce an artificial intelligence approach to de novo design of molecules with desired physical or biological properties. We have devised and implemented a novel computational strategy for de novo design of molecules with desired properties termed ReLeaSE (Reinforcement Learning for Structural Evolution). On the basis of deep and reinforcement learning (RL) approaches, ReLeaSE integrates two deep neural networks\u2014generative and predictive\u2014that are trained separately but are used jointly to generate novel targeted chemical libraries. ReLeaSE uses simple representation of molecules by their simplified molecular-input line-entry system (SMILES) strings only. Generative models are trained with a stack-augmented memory network to produce chemically feasible SMILES strings, and predictive models are derived to forecast the desired properties of the de novo\u2013generated compounds. In the first phase of the method, generative and predictive models are trained separately with a supervised learning algorithm. In the second phase, both models are trained jointly with the RL approach to bias the generation of new chemical structures toward those with the desired physical and/or biological properties. In the proof-of-concept study, we have used the ReLeaSE method to design chemical libraries with a bias toward structural complexity or toward compounds with maximal, minimal, or specific range of physical properties, such as melting point or hydrophobicity, or toward compounds with inhibitory activity against Janus protein kinase 2. The approach proposed herein can find a general use for generating targeted chemical libraries of novel compounds optimized for either a single desired property or multiple properties.",
            "referenceCount": 83,
            "citationCount": 781,
            "influentialCitationCount": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://advances.sciencemag.org/content/advances/4/7/eaap7885.full.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-11-29",
            "journal": {
                "name": "Science Advances",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Popova2017DeepRL,\n author = {Mariya Popova and O. Isayev and A. Tropsha},\n booktitle = {Science Advances},\n journal = {Science Advances},\n title = {Deep reinforcement learning for de novo drug design},\n volume = {4},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8db9df2eadea654f128c1887722c677c708e8a47",
            "@type": "ScholarlyArticle",
            "paperId": "8db9df2eadea654f128c1887722c677c708e8a47",
            "corpusId": 12064877,
            "url": "https://www.semanticscholar.org/paper/8db9df2eadea654f128c1887722c677c708e8a47",
            "title": "Deep Reinforcement Learning framework for Autonomous Driving",
            "venue": "Autonomous Vehicles and Machines",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1704.02532",
                "MAG": "3100944043",
                "DBLP": "conf/ei-avm/SallabAPY17",
                "DOI": "10.2352/ISSN.2470-1173.2017.19.AVM-023",
                "CorpusId": 12064877
            },
            "abstract": "Reinforcement learning is considered to be a strong AI paradigm which can be used to teach machines through interaction with the environment and learning from their mistakes. Despite its perceived utility, it has not yet been successfully applied in automotive applications. Motivated by the successful demonstrations of learning of Atari games and Go by Google DeepMind, we propose a framework for autonomous driving using deep reinforcement learning. This is of particular relevance as it is difficult to pose autonomous driving as a supervised learning problem due to strong interactions with the environment including other vehicles, pedestrians and roadworks. As it is a relatively new area of research for autonomous driving, we provide a short overview of deep reinforcement learning and then describe our proposed framework. It incorporates Recurrent Neural Networks for information integration, enabling the car to handle partially observable scenarios. It also integrates the recent work on attention models to focus on relevant information, thereby reducing the computational complexity for deployment on embedded hardware. The framework was tested in an open source 3D car racing simulator called TORCS. Our simulation results demonstrate learning of autonomous maneuvering in a scenario of complex road curvatures and simple interaction of other vehicles.",
            "referenceCount": 24,
            "citationCount": 810,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1704.02532",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-04-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sallab2017DeepRL,\n author = {Ahmad El Sallab and Mohammed Abdou and E. Perot and S. Yogamani},\n booktitle = {Autonomous Vehicles and Machines},\n pages = {70-76},\n title = {Deep Reinforcement Learning framework for Autonomous Driving},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:050a89a91b3e828c841972a81f17807f82c79713",
            "@type": "ScholarlyArticle",
            "paperId": "050a89a91b3e828c841972a81f17807f82c79713",
            "corpusId": 53108700,
            "url": "https://www.semanticscholar.org/paper/050a89a91b3e828c841972a81f17807f82c79713",
            "title": "SURREAL: Open-Source Reinforcement Learning Framework and Robot Manipulation Benchmark",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/corl/FanZZLZGCSF18",
                "MAG": "2899059606",
                "CorpusId": 53108700
            },
            "abstract": "Reproducibility has been a significant challenge in deep reinforcement learning and robotics research. Open-source frameworks and standardized benchmarks can serve an integral role in rigorous evaluation and reproducible research. We introduce SURREAL, an open-source scalable framework that supports stateof-the-art distributed reinforcement learning algorithms. We design a principled distributed learning formulation that accommodates both on-policy and off-policy learning. We demonstrate that SURREAL algorithms outperform existing opensource implementations in both agent performance and learning efficiency. We also introduce SURREAL Robotics Suite, an accessible set of benchmarking tasks in physical simulation for reproducible robot manipulation research. We provide extensive evaluations of SURREAL algorithms and establish strong baseline results.",
            "referenceCount": 33,
            "citationCount": 137,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-10-23",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Fan2018SURREALOR,\n author = {Linxi (Jim) Fan and Yuke Zhu and Jiren Zhu and Zihua Liu and Orien Zeng and Anchit Gupta and Joan Creus-Costa and S. Savarese and Li Fei-Fei},\n booktitle = {Conference on Robot Learning},\n pages = {767-782},\n title = {SURREAL: Open-Source Reinforcement Learning Framework and Robot Manipulation Benchmark},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:16481307a862ddae514045b13b944ca043c36a4b",
            "@type": "ScholarlyArticle",
            "paperId": "16481307a862ddae514045b13b944ca043c36a4b",
            "corpusId": 67856031,
            "url": "https://www.semanticscholar.org/paper/16481307a862ddae514045b13b944ca043c36a4b",
            "title": "Generative Adversarial User Model for Reinforcement Learning Based Recommendation System",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/icml/Chen0LJQS19",
                "ArXiv": "1812.10613",
                "MAG": "2953017267",
                "CorpusId": 67856031
            },
            "abstract": "There are great interests as well as many challenges in applying reinforcement learning (RL) to recommendation systems. In this setting, an online user is the environment; neither the reward function nor the environment dynamics are clearly defined, making the application of RL challenging. In this paper, we propose a novel model-based reinforcement learning framework for recommendation systems, where we develop a generative adversarial network to imitate user behavior dynamics and learn her reward function. Using this user model as the simulation environment, we develop a novel Cascading DQN algorithm to obtain a combinatorial recommendation policy which can handle a large number of candidate items efficiently. In our experiments with real data, we show this generative adversarial user model can better explain user behavior than alternatives, and the RL policy based on this model can lead to a better long-term reward for the user and higher click rate for the system.",
            "referenceCount": 30,
            "citationCount": 151,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-12-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2018GenerativeAU,\n author = {Xinshi Chen and Shuang Li and Hui Li and Shaohua Jiang and Yuan Qi and Le Song},\n booktitle = {International Conference on Machine Learning},\n pages = {1052-1061},\n title = {Generative Adversarial User Model for Reinforcement Learning Based Recommendation System},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:26b90ccf7541fd2cd4235118493e1e49d358c351",
            "@type": "ScholarlyArticle",
            "paperId": "26b90ccf7541fd2cd4235118493e1e49d358c351",
            "corpusId": 4559747,
            "url": "https://www.semanticscholar.org/paper/26b90ccf7541fd2cd4235118493e1e49d358c351",
            "title": "StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning",
            "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
            "publicationVenue": {
                "id": "urn:research:544cddb9-1149-469a-8377-d8c34f08d8b1",
                "name": "IEEE Transactions on Emerging Topics in Computational Intelligence",
                "alternate_names": [
                    "IEEE Trans Emerg Top Comput Intell"
                ],
                "issn": "2471-285X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=7433297"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2795717084",
                "DBLP": "journals/tetci/ShaoZZ19",
                "ArXiv": "1804.00810",
                "DOI": "10.1109/TETCI.2018.2823329",
                "CorpusId": 4559747
            },
            "abstract": "Real-time strategy games have been an important field of game artificial intelligence in recent years. This paper presents a reinforcement learning and curriculum transfer learning method to control multiple units in StarCraft micromanagement. We define an efficient state representation, which breaks down the complexity caused by the large state space in the game environment. Then, a parameter sharing multi-agent gradient-descent Sarsa($\\lambda$) algorithm is proposed to train the units. The learning policy is shared among our units to encourage cooperative behaviors. We use a neural network as a function approximator to estimate the action\u2013value function, and propose a reward function to help units balance their move and attack. In addition, a transfer learning method is used to extend our model to more difficult scenarios, which accelerates the training process and improves the learning performance. In small-scale scenarios, our units successfully learn to combat and defeat the built-in AI with 100% win rates. In large-scale scenarios, the curriculum transfer learning method is used to progressively train a group of units, and it shows superior performance over some baseline methods in target scenarios. With reinforcement learning and curriculum transfer learning, our units are able to learn appropriate strategies in StarCraft micromanagement scenarios.",
            "referenceCount": 65,
            "citationCount": 137,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1804.00810",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-04-03",
            "journal": {
                "name": "IEEE Transactions on Emerging Topics in Computational Intelligence",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Shao2018StarCraftMW,\n author = {Kun Shao and Yuanheng Zhu and Dongbin Zhao},\n booktitle = {IEEE Transactions on Emerging Topics in Computational Intelligence},\n journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},\n pages = {73-84},\n title = {StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning},\n volume = {3},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9917363277c783a01bff32af1c27fc9b373ad55d",
            "@type": "ScholarlyArticle",
            "paperId": "9917363277c783a01bff32af1c27fc9b373ad55d",
            "corpusId": 3332126,
            "url": "https://www.semanticscholar.org/paper/9917363277c783a01bff32af1c27fc9b373ad55d",
            "title": "DeepLoco: dynamic locomotion skills using hierarchical deep reinforcement learning",
            "venue": "ACM Transactions on Graphics",
            "publicationVenue": {
                "id": "urn:research:aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                "name": "ACM Transactions on Graphics",
                "alternate_names": [
                    "ACM Trans Graph"
                ],
                "issn": "0730-0301",
                "url": "http://www.acm.org/tog/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2739330054",
                "DBLP": "journals/tog/PengBYP17",
                "DOI": "10.1145/3072959.3073602",
                "CorpusId": 3332126
            },
            "abstract": "Learning physics-based locomotion skills is a difficult problem, leading to solutions that typically exploit prior knowledge of various forms. In this paper we aim to learn a variety of environment-aware locomotion skills with a limited amount of prior knowledge. We adopt a two-level hierarchical control framework. First, low-level controllers are learned that operate at a fine timescale and which achieve robust walking gaits that satisfy stepping-target and style objectives. Second, high-level controllers are then learned which plan at the timescale of steps by invoking desired step targets for the low-level controller. The high-level controller makes decisions directly based on high-dimensional inputs, including terrain maps or other suitable representations of the surroundings. Both levels of the control policy are trained using deep reinforcement learning. Results are demonstrated on a simulated 3D biped. Low-level controllers are learned for a variety of motion styles and demonstrate robustness with respect to force-based disturbances, terrain variations, and style interpolation. High-level controllers are demonstrated that are capable of following trails through terrains, dribbling a soccer ball towards a target location, and navigating through static or dynamic obstacles.",
            "referenceCount": 62,
            "citationCount": 513,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-07-20",
            "journal": {
                "name": "ACM Trans. Graph.",
                "volume": "36"
            },
            "citationStyles": {
                "bibtex": "@Article{Peng2017DeepLocoDL,\n author = {X. B. Peng and G. Berseth and KangKang Yin and M. V. D. Panne},\n booktitle = {ACM Transactions on Graphics},\n journal = {ACM Trans. Graph.},\n pages = {41:1-41:13},\n title = {DeepLoco: dynamic locomotion skills using hierarchical deep reinforcement learning},\n volume = {36},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:31f5864ada5fb08b69da74b6d5ad99e385dcc737",
            "@type": "ScholarlyArticle",
            "paperId": "31f5864ada5fb08b69da74b6d5ad99e385dcc737",
            "corpusId": 7473831,
            "url": "https://www.semanticscholar.org/paper/31f5864ada5fb08b69da74b6d5ad99e385dcc737",
            "title": "Sentence Simplification with Deep Reinforcement Learning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2605243085",
                "DBLP": "journals/corr/ZhangL17d",
                "ACL": "D17-1062",
                "ArXiv": "1703.10931",
                "DOI": "10.18653/v1/D17-1062",
                "CorpusId": 7473831
            },
            "abstract": "Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call DRESS (as shorthand for Deep REinforcement Sentence Simplification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.",
            "referenceCount": 51,
            "citationCount": 347,
            "influentialCitationCount": 70,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D17-1062.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-31",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2017SentenceSW,\n author = {Xingxing Zhang and Mirella Lapata},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {584-594},\n title = {Sentence Simplification with Deep Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dc3e905bfb27d21675ee1720413e007b014b37d3",
            "@type": "ScholarlyArticle",
            "paperId": "dc3e905bfb27d21675ee1720413e007b014b37d3",
            "corpusId": 492962,
            "url": "https://www.semanticscholar.org/paper/dc3e905bfb27d21675ee1720413e007b014b37d3",
            "title": "Safe and Efficient Off-Policy Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2411690432",
                "ArXiv": "1606.02647",
                "DBLP": "conf/nips/MunosSHB16",
                "CorpusId": 492962
            },
            "abstract": "In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace($\\lambda$), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of \"off-policyness\"; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies. We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. We believe this is the first return-based off-policy control algorithm converging a.s. to $Q^*$ without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q($\\lambda$), which was an open problem since 1989. We illustrate the benefits of Retrace($\\lambda$) on a standard suite of Atari 2600 games.",
            "referenceCount": 22,
            "citationCount": 535,
            "influentialCitationCount": 79,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Munos2016SafeAE,\n author = {R. Munos and T. Stepleton and A. Harutyunyan and Marc G. Bellemare},\n booktitle = {Neural Information Processing Systems},\n pages = {1046-1054},\n title = {Safe and Efficient Off-Policy Reinforcement Learning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b68673a166f9c620e13152f63d358fb8fce7850d",
            "@type": "ScholarlyArticle",
            "paperId": "b68673a166f9c620e13152f63d358fb8fce7850d",
            "corpusId": 8894704,
            "url": "https://www.semanticscholar.org/paper/b68673a166f9c620e13152f63d358fb8fce7850d",
            "title": "Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/OmidshafieiPAHV17",
                "MAG": "2951896791",
                "ArXiv": "1703.06182",
                "CorpusId": 8894704
            },
            "abstract": "Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrently-exploring teammates. Approaches that learn specialized policies for individual tasks face problems when applied to the real world: not only do agents have to learn and store distinct policies for each task, but in practice identities of tasks are often non-observable, making these approaches inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.",
            "referenceCount": 51,
            "citationCount": 435,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1703.06182"
            },
            "citationStyles": {
                "bibtex": "@Article{Omidshafiei2017DeepDM,\n author = {Shayegan Omidshafiei and Jason Pazis and Chris Amato and J. How and J. Vian},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability},\n volume = {abs/1703.06182},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8b2a6808ce5cec406b41a8e77e67570246105bd0",
            "@type": "ScholarlyArticle",
            "paperId": "8b2a6808ce5cec406b41a8e77e67570246105bd0",
            "corpusId": 49867653,
            "url": "https://www.semanticscholar.org/paper/8b2a6808ce5cec406b41a8e77e67570246105bd0",
            "title": "Deep Reinforcement Learning for Swarm Systems",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1807-06613",
                "ArXiv": "1807.06613",
                "MAG": "2883532348",
                "CorpusId": 49867653
            },
            "abstract": "Recently, deep reinforcement learning (RL) methods have been applied successfully to multi-agent scenarios. Typically, the observation vector for decentralized decision making is represented by a concatenation of the (local) information an agent gathers about other agents. However, concatenation scales poorly to swarm systems with a large number of homogeneous agents as it does not exploit the fundamental properties inherent to these systems: (i) the agents in the swarm are interchangeable and (ii) the exact number of agents in the swarm is irrelevant. Therefore, we propose a new state representation for deep multi-agent RL based on mean embeddings of distributions, where we treat the agents as samples and use the empirical mean embedding as input for a decentralized policy. We define different feature spaces of the mean embedding using histograms, radial basis functions and neural networks trained end-to-end. We evaluate the representation on two well-known problems from the swarm literature in a globally and locally observable setup. For the local setup we furthermore introduce simple communication protocols. Of all approaches, the mean embedding representation using neural network features enables the richest information exchange between neighboring agents, facilitating the development of complex collective strategies.",
            "referenceCount": 46,
            "citationCount": 155,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-07-17",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "20"
            },
            "citationStyles": {
                "bibtex": "@Article{H\u00fcttenrauch2018DeepRL,\n author = {Maximilian H\u00fcttenrauch and Adrian \u0160o\u0161i\u0107 and G. Neumann},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {54:1-54:31},\n title = {Deep Reinforcement Learning for Swarm Systems},\n volume = {20},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
            "@type": "ScholarlyArticle",
            "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
            "corpusId": 14357699,
            "url": "https://www.semanticscholar.org/paper/0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
            "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1611.04717",
                "MAG": "2949475445",
                "DBLP": "journals/corr/TangHFSCDSTA16",
                "CorpusId": 14357699
            },
            "abstract": "Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.",
            "referenceCount": 51,
            "citationCount": 636,
            "influentialCitationCount": 52,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-11-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.04717"
            },
            "citationStyles": {
                "bibtex": "@Article{Tang2016ExplorationAS,\n author = {Haoran Tang and Rein Houthooft and Davis Foote and Adam Stooke and Xi Chen and Yan Duan and J. Schulman and F. Turck and P. Abbeel},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning},\n volume = {abs/1611.04717},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d8686b657b61a37da351af2952aabd8b281de408",
            "@type": "ScholarlyArticle",
            "paperId": "d8686b657b61a37da351af2952aabd8b281de408",
            "corpusId": 4650427,
            "url": "https://www.semanticscholar.org/paper/d8686b657b61a37da351af2952aabd8b281de408",
            "title": "Successor Features for Transfer in Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/BarretoMSS16",
                "ArXiv": "1606.05312",
                "MAG": "2950426624",
                "CorpusId": 4650427
            },
            "abstract": "Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. We propose a transfer framework for the scenario where the reward function changes between tasks but the environment's dynamics remain the same. Our approach rests on two key ideas: \"successor features\", a value function representation that decouples the dynamics of the environment from the rewards, and \"generalized policy improvement\", a generalization of dynamic programming's policy improvement operation that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows the free exchange of information across tasks. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice, significantly outperforming alternative methods in a sequence of navigation tasks and in the control of a simulated robotic arm.",
            "referenceCount": 40,
            "citationCount": 454,
            "influentialCitationCount": 71,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1606.05312"
            },
            "citationStyles": {
                "bibtex": "@Article{Barreto2016SuccessorFF,\n author = {Andr\u00e9 Barreto and Will Dabney and R. Munos and Jonathan J. Hunt and T. Schaul and David Silver and H. V. Hasselt},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Successor Features for Transfer in Reinforcement Learning},\n volume = {abs/1606.05312},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:059a1297a61afc812de48edede631229608dc513",
            "@type": "ScholarlyArticle",
            "paperId": "059a1297a61afc812de48edede631229608dc513",
            "corpusId": 30994737,
            "url": "https://www.semanticscholar.org/paper/059a1297a61afc812de48edede631229608dc513",
            "title": "Survey of Model-Based Reinforcement Learning: Applications on Robotics",
            "venue": "J. Intell. Robotic Syst.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/jirs/PolydorosN17",
                "MAG": "2580909119",
                "DOI": "10.1007/s10846-017-0468-y",
                "CorpusId": 30994737
            },
            "abstract": null,
            "referenceCount": 96,
            "citationCount": 387,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-05-01",
            "journal": {
                "name": "Journal of Intelligent & Robotic Systems",
                "volume": "86"
            },
            "citationStyles": {
                "bibtex": "@Article{Polydoros2017SurveyOM,\n author = {Athanasios S. Polydoros and L. Nalpantidis},\n booktitle = {J. Intell. Robotic Syst.},\n journal = {Journal of Intelligent & Robotic Systems},\n pages = {153-173},\n title = {Survey of Model-Based Reinforcement Learning: Applications on Robotics},\n volume = {86},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ec8a2f6cfe72309f5f1608d22ec28778d3ee976a",
            "@type": "ScholarlyArticle",
            "paperId": "ec8a2f6cfe72309f5f1608d22ec28778d3ee976a",
            "corpusId": 9311215,
            "url": "https://www.semanticscholar.org/paper/ec8a2f6cfe72309f5f1608d22ec28778d3ee976a",
            "title": "Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/ThomasB16",
                "MAG": "2950404003",
                "ArXiv": "1604.00923",
                "CorpusId": 9311215
            },
            "abstract": "In this paper we present a new way of predicting the performance of a reinforcement learning policy given historical data that may have been generated by a different policy. The ability to evaluate a policy from historical data is important for applications where the deployment of a bad policy can be dangerous or costly. We show empirically that our algorithm produces estimates that often have orders of magnitude lower mean squared error than existing methods---it makes more efficient use of the available data. Our new estimator is based on two advances: an extension of the doubly robust estimator (Jiang and Li, 2015), and a new way to mix between model based estimates and importance sampling based estimates.",
            "referenceCount": 42,
            "citationCount": 486,
            "influentialCitationCount": 52,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-04-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1604.00923"
            },
            "citationStyles": {
                "bibtex": "@Article{Thomas2016DataEfficientOP,\n author = {P. Thomas and E. Brunskill},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning},\n volume = {abs/1604.00923},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9e09cff6cd4c0cfbe9675cd1f0ae67c9c75a8ac9",
            "@type": "ScholarlyArticle",
            "paperId": "9e09cff6cd4c0cfbe9675cd1f0ae67c9c75a8ac9",
            "corpusId": 24546802,
            "url": "https://www.semanticscholar.org/paper/9e09cff6cd4c0cfbe9675cd1f0ae67c9c75a8ac9",
            "title": "Control of a Quadrotor With Reinforcement Learning",
            "venue": "IEEE Robotics and Automation Letters",
            "publicationVenue": {
                "id": "urn:research:93c335b7-edf4-45f5-8ddc-7c5835154945",
                "name": "IEEE Robotics and Automation Letters",
                "alternate_names": [
                    "IEEE Robot Autom Lett"
                ],
                "issn": "2377-3766",
                "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2733312032",
                "ArXiv": "1707.05110",
                "DBLP": "journals/corr/HwangboSSH17",
                "DOI": "10.1109/LRA.2017.2720851",
                "CorpusId": 24546802
            },
            "abstract": "In this letter, we present a method to control a quadrotor with a neural network trained using reinforcement learning techniques. With reinforcement learning, a common network can be trained to directly map state to actuator command making any predefined control structure obsolete for training. Moreover, we present a new learning algorithm that differs from the existing ones in certain aspects. Our algorithm is conservative but stable for complicated tasks. We found that it is more applicable to controlling a quadrotor than existing algorithms. We demonstrate the performance of the trained policy both in simulation and with a real quadrotor. Experiments show that our policy network can react to step response relatively accurately. With the same policy, we also demonstrate that we can stabilize the quadrotor in the air even under very harsh initialization (manually throwing it upside-down in the air with an initial velocity of 5 m/s). Computation time of evaluating the policy is only 7  $\\mu$s per time step, which is two orders of magnitude less than common trajectory optimization algorithms with an approximated model.",
            "referenceCount": 22,
            "citationCount": 383,
            "influentialCitationCount": 23,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1707.05110",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-06-28",
            "journal": {
                "name": "IEEE Robotics and Automation Letters",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Hwangbo2017ControlOA,\n author = {Jemin Hwangbo and Inkyu Sa and R. Siegwart and M. Hutter},\n booktitle = {IEEE Robotics and Automation Letters},\n journal = {IEEE Robotics and Automation Letters},\n pages = {2096-2103},\n title = {Control of a Quadrotor With Reinforcement Learning},\n volume = {2},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2a38aeccad784c9b0b87019fefaa2accdc052d5b",
            "@type": "ScholarlyArticle",
            "paperId": "2a38aeccad784c9b0b87019fefaa2accdc052d5b",
            "corpusId": 4245127,
            "url": "https://www.semanticscholar.org/paper/2a38aeccad784c9b0b87019fefaa2accdc052d5b",
            "title": "Learning State Representations for Query Optimization with Deep Reinforcement Learning",
            "venue": "DEEM@SIGMOD",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1803-08604",
                "MAG": "2949705223",
                "ArXiv": "1803.08604",
                "DOI": "10.1145/3209889.3209890",
                "CorpusId": 4245127
            },
            "abstract": "We explore the idea of using deep reinforcement learning for query optimization. The approach is to build queries incrementally by encoding properties of subqueries using a learned representation. In this paper, we focus specifically on the state representation problem and the formation of the state transition function. We show preliminary results and discuss how we can use the state representation to improve query optimization using reinforcement learning.",
            "referenceCount": 20,
            "citationCount": 141,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3209889.3209890",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book"
            ],
            "publicationDate": "2018-03-22",
            "journal": {
                "name": "Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ortiz2018LearningSR,\n author = {Jennifer Ortiz and M. Balazinska and J. Gehrke and S. Keerthi},\n booktitle = {DEEM@SIGMOD},\n journal = {Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning},\n title = {Learning State Representations for Query Optimization with Deep Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:15a06d8601539b5eb6df5baf6bc4c3bdefb34855",
            "@type": "ScholarlyArticle",
            "paperId": "15a06d8601539b5eb6df5baf6bc4c3bdefb34855",
            "corpusId": 44063077,
            "url": "https://www.semanticscholar.org/paper/15a06d8601539b5eb6df5baf6bc4c3bdefb34855",
            "title": "Deep Reinforcement Learning for Sequence-to-Sequence Models",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/tnn/KeneshlooSRR20",
                "ArXiv": "1805.09461",
                "MAG": "2968831808",
                "DOI": "10.1109/TNNLS.2019.2929141",
                "CorpusId": 44063077,
                "PubMed": "31425057"
            },
            "abstract": "In recent times, sequence-to-sequence (seq2seq) models have gained a lot of popularity and provide state-of-the-art performance in a wide variety of tasks, such as machine translation, headline generation, text summarization, speech-to-text conversion, and image caption generation. The underlying framework for all these models is usually a deep neural network comprising an encoder and a decoder. Although simple encoder\u2013decoder models produce competitive results, many researchers have proposed additional improvements over these seq2seq models, e.g., using an attention-based model over the input, pointer-generation models, and self-attention models. However, such seq2seq models suffer from two common problems: 1) exposure bias and 2) inconsistency between train/test measurement. Recently, a completely novel point of view has emerged in addressing these two problems in seq2seq models, leveraging methods from reinforcement learning (RL). In this survey, we consider seq2seq problems from the RL point of view and provide a formulation combining the power of RL methods in decision-making with seq2seq models that enable remembering long-term memories. We present some of the most recent frameworks that combine the concepts from RL and deep neural networks. Our work aims to provide insights into some of the problems that inherently arise with current approaches and how we can address them with better RL models. We also provide the source code for implementing most of the RL models discussed in this paper to support the complex task of abstractive text summarization and provide some targeted experiments for these RL models, both in terms of performance and training time.",
            "referenceCount": 212,
            "citationCount": 155,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1805.09461",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-05-24",
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "31"
            },
            "citationStyles": {
                "bibtex": "@Article{Keneshloo2018DeepRL,\n author = {Yaser Keneshloo and Tian Shi and Naren Ramakrishnan and C. Reddy},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {2469-2489},\n title = {Deep Reinforcement Learning for Sequence-to-Sequence Models},\n volume = {31},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:30741e7e30976f7b482eef3d00295ae4740ec21b",
            "@type": "ScholarlyArticle",
            "paperId": "30741e7e30976f7b482eef3d00295ae4740ec21b",
            "corpusId": 327425,
            "url": "https://www.semanticscholar.org/paper/30741e7e30976f7b482eef3d00295ae4740ec21b",
            "title": "On-Line Building Energy Optimization Using Deep Reinforcement Learning",
            "venue": "IEEE Transactions on Smart Grid",
            "publicationVenue": {
                "id": "urn:research:1c2f3998-b5ca-48ca-9991-94b71c71ecb7",
                "name": "IEEE Transactions on Smart Grid",
                "alternate_names": [
                    "IEEE Trans Smart Grid"
                ],
                "issn": "1949-3053",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5165411"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2963317745",
                "ArXiv": "1707.05878",
                "DBLP": "journals/corr/MocanuMNLWGS17",
                "DOI": "10.1109/TSG.2018.2834219",
                "CorpusId": 327425
            },
            "abstract": "Unprecedented high volumes of data are becoming available with the growth of the advanced metering infrastructure. These are expected to benefit planning and operation of the future power systems and to help customers transition from a passive to an active role. In this paper, we explore for the first time in the smart grid context the benefits of using deep reinforcement learning, a hybrid type of methods that combines reinforcement learning with deep learning, to perform on-line optimization of schedules for building energy management systems. The learning procedure was explored using two methods, Deep Q-learning and deep policy gradient, both of which have been extended to perform multiple actions simultaneously. The proposed approach was validated on the large-scale Pecan Street Inc. database. This highly dimensional database includes information about photovoltaic power generation, electric vehicles and buildings appliances. Moreover, these on-line energy scheduling strategies could be used to provide real-time feedback to consumers to encourage more efficient use of electricity.",
            "referenceCount": 37,
            "citationCount": 369,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1707.05878",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-07-18",
            "journal": {
                "name": "IEEE Transactions on Smart Grid",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Mocanu2017OnLineBE,\n author = {Elena Mocanu and D. Mocanu and Phuong H. Nguyen and A. Liotta and M. Webber and M. Gibescu and J. Slootweg},\n booktitle = {IEEE Transactions on Smart Grid},\n journal = {IEEE Transactions on Smart Grid},\n pages = {3698-3708},\n title = {On-Line Building Energy Optimization Using Deep Reinforcement Learning},\n volume = {10},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:39f1cbef12f64dcdb3a7683f9e70f436a7742328",
            "@type": "ScholarlyArticle",
            "paperId": "39f1cbef12f64dcdb3a7683f9e70f436a7742328",
            "corpusId": 9823884,
            "url": "https://www.semanticscholar.org/paper/39f1cbef12f64dcdb3a7683f9e70f436a7742328",
            "title": "Applications of Deep Learning and Reinforcement Learning to Biological Data",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2768956845",
                "DBLP": "journals/tnn/MahmudKHV18",
                "ArXiv": "1711.03985",
                "DOI": "10.1109/TNNLS.2018.2790388",
                "CorpusId": 9823884,
                "PubMed": "29771663"
            },
            "abstract": "Rapid advances in hardware-based technologies during the past decades have opened up new possibilities for life scientists to gather multimodal data in various application domains, such as omics, bioimaging, medical imaging, and (brain/body)\u2013machine interfaces. These have generated novel opportunities for development of dedicated data-intensive machine learning techniques. In particular, recent research in deep learning (DL), reinforcement learning (RL), and their combination (deep RL) promise to revolutionize the future of artificial intelligence. The growth in computational power accompanied by faster and increased data storage, and declining computing costs have already allowed scientists in various fields to apply these techniques on data sets that were previously intractable owing to their size and complexity. This paper provides a comprehensive survey on the application of DL, RL, and deep RL techniques in mining biological data. In addition, we compare the performances of DL techniques when applied to different data sets across various application domains. Finally, we outline open issues in this challenging research area and discuss future development perspectives.",
            "referenceCount": 213,
            "citationCount": 565,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://dspace.stir.ac.uk/bitstream/1893/26814/1/1711.03985.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-11-10",
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "29"
            },
            "citationStyles": {
                "bibtex": "@Article{Mahmud2017ApplicationsOD,\n author = {M. Mahmud and M. S. Kaiser and A. Hussain and S. Vassanelli},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {2063-2079},\n title = {Applications of Deep Learning and Reinforcement Learning to Biological Data},\n volume = {29},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cb1092c1bb7d021fab6b4976ea3a51cc6d61f59b",
            "@type": "ScholarlyArticle",
            "paperId": "cb1092c1bb7d021fab6b4976ea3a51cc6d61f59b",
            "corpusId": 21691003,
            "url": "https://www.semanticscholar.org/paper/cb1092c1bb7d021fab6b4976ea3a51cc6d61f59b",
            "title": "Distributed Economic Dispatch in Microgrids Based on Cooperative Reinforcement Learning",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/tnn/LiuZLPH18",
                "MAG": "2790306973",
                "DOI": "10.1109/TNNLS.2018.2801880",
                "CorpusId": 21691003,
                "PubMed": "29771671"
            },
            "abstract": "Microgrids incorporated with distributed generation (DG) units and energy storage (ES) devices are expected to play more and more important roles in the future power systems. Yet, achieving efficient distributed economic dispatch in microgrids is a challenging issue due to the randomness and nonlinear characteristics of DG units and loads. This paper proposes a cooperative reinforcement learning algorithm for distributed economic dispatch in microgrids. Utilizing the learning algorithm can avoid the difficulty of stochastic modeling and high computational complexity. In the cooperative reinforcement learning algorithm, the function approximation is leveraged to deal with the large and continuous state spaces. And a diffusion strategy is incorporated to coordinate the actions of DG units and ES devices. Based on the proposed algorithm, each node in microgrids only needs to communicate with its local neighbors, without relying on any centralized controllers. Algorithm convergence is analyzed, and simulations based on real-world meteorological and load data are conducted to validate the performance of the proposed algorithm.",
            "referenceCount": 0,
            "citationCount": 148,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-03-02",
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "29"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2018DistributedED,\n author = {Weirong Liu and Zhu Peng and Hao Liang and Jun Peng and Zhiwu Huang},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {2192-2203},\n title = {Distributed Economic Dispatch in Microgrids Based on Cooperative Reinforcement Learning},\n volume = {29},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cd4ee7825ea974f0fbb69445d403c9dde54f6a04",
            "@type": "ScholarlyArticle",
            "paperId": "cd4ee7825ea974f0fbb69445d403c9dde54f6a04",
            "corpusId": 53226603,
            "url": "https://www.semanticscholar.org/paper/cd4ee7825ea974f0fbb69445d403c9dde54f6a04",
            "title": "Policy Certificates: Towards Accountable Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1811-03056",
                "MAG": "2963490519",
                "ArXiv": "1811.03056",
                "CorpusId": 53226603
            },
            "abstract": "The performance of a reinforcement learning algorithm can vary drastically during learning because of exploration. Existing algorithms provide little information about the quality of their current policy before executing it, and thus have limited use in high-stakes applications like healthcare. We address this lack of accountability by proposing that algorithms output policy certificates. These certificates bound the sub-optimality and return of the policy in the next episode, allowing humans to intervene when the certified quality is not satisfactory. We further introduce two new algorithms with certificates and present a new framework for theoretical analysis that guarantees the quality of their policies and certificates. For tabular MDPs, we show that computing certificates can even improve the sample-efficiency of optimism-based exploration. As a result, one of our algorithms is the first to achieve minimax-optimal PAC bounds up to lower-order terms, and this algorithm also matches (and in some settings slightly improves upon) existing minimax regret bounds.",
            "referenceCount": 51,
            "citationCount": 127,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-11-07",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dann2018PolicyCT,\n author = {Christoph Dann and Lihong Li and Wei Wei and E. Brunskill},\n booktitle = {International Conference on Machine Learning},\n pages = {1507-1516},\n title = {Policy Certificates: Towards Accountable Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0f710daa7bbba3350169f0bbb5d24f8db3e5199e",
            "@type": "ScholarlyArticle",
            "paperId": "0f710daa7bbba3350169f0bbb5d24f8db3e5199e",
            "corpusId": 46977753,
            "url": "https://www.semanticscholar.org/paper/0f710daa7bbba3350169f0bbb5d24f8db3e5199e",
            "title": "Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2949144938",
                "DBLP": "journals/corr/abs-1806-02813",
                "ArXiv": "1806.02813",
                "CorpusId": 46977753
            },
            "abstract": "In this work, we take a representation learning perspective on hierarchical reinforcement learning, where the problem of learning lower layers in a hierarchy is transformed into the problem of learning trajectory-level generative models. We show that we can learn continuous latent representations of trajectories, which are effective in solving temporally extended and multi-stage problems. Our proposed model, SeCTAR, draws inspiration from variational autoencoders, and learns latent representations of trajectories. A key component of this method is to learn both a latent-conditioned policy and a latent-conditioned model which are consistent with each other. Given the same latent, the policy generates a trajectory which should match the trajectory predicted by the model. This model provides a built-in prediction mechanism, by predicting the outcome of closed loop policy behavior. We propose a novel algorithm for performing hierarchical RL with this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. We show that our model is effective at reasoning over long horizons with sparse rewards for several simulated tasks, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.",
            "referenceCount": 34,
            "citationCount": 128,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-06-07",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Co-Reyes2018SelfConsistentTA,\n author = {John D. Co-Reyes and Yuxuan Liu and Abhishek Gupta and Benjamin Eysenbach and P. Abbeel and S. Levine},\n booktitle = {International Conference on Machine Learning},\n pages = {1008-1017},\n title = {Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:412fbb7ffa29aa6b2e172c1f545223baf18f1b6e",
            "@type": "ScholarlyArticle",
            "paperId": "412fbb7ffa29aa6b2e172c1f545223baf18f1b6e",
            "corpusId": 53714296,
            "url": "https://www.semanticscholar.org/paper/412fbb7ffa29aa6b2e172c1f545223baf18f1b6e",
            "title": "Practical Deep Reinforcement Learning Approach for Stock Trading",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1811.07522",
                "DBLP": "journals/corr/abs-1811-07522",
                "MAG": "2901174038",
                "CorpusId": 53714296
            },
            "abstract": "Stock trading strategy plays a crucial role in investment companies. However, it is challenging to obtain optimal strategy in the complex and dynamic stock market. We explore the potential of deep reinforcement learning to optimize stock trading strategy and thus maximize investment return. 30 stocks are selected as our trading stocks and their daily prices are used as the training and trading market environment. We train a deep reinforcement learning agent and obtain an adaptive trading strategy. The agent's performance is evaluated and compared with Dow Jones Industrial Average and the traditional min-variance portfolio allocation strategy. The proposed deep reinforcement learning approach is shown to outperform the two baselines in terms of both the Sharpe ratio and cumulative returns.",
            "referenceCount": 23,
            "citationCount": 120,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Economics",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Economics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Business",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-11-19",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1811.07522"
            },
            "citationStyles": {
                "bibtex": "@Article{Xiong2018PracticalDR,\n author = {Zhuoran Xiong and Xiao-Yang Liu and Shanli Zhong and Hongyang Yang and A. Elwalid},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Practical Deep Reinforcement Learning Approach for Stock Trading},\n volume = {abs/1811.07522},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:86d5f469f0f541f0b2223d2d3e80f22fb1d66af7",
            "@type": "ScholarlyArticle",
            "paperId": "86d5f469f0f541f0b2223d2d3e80f22fb1d66af7",
            "corpusId": 59413995,
            "url": "https://www.semanticscholar.org/paper/86d5f469f0f541f0b2223d2d3e80f22fb1d66af7",
            "title": "A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on a Moving Platform",
            "venue": "Journal of Intelligent and Robotic Systems",
            "publicationVenue": {
                "id": "urn:research:38655176-1177-4a65-a6f5-e546f718158a",
                "name": "Journal of Intelligent and Robotic Systems",
                "alternate_names": [
                    "J Intell Robot Syst"
                ],
                "issn": "0921-0296",
                "url": "https://www.springer.com/journal/10846"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/jirs/Rodriguez-Ramos19",
                "MAG": "2810217655",
                "DOI": "10.1007/s10846-018-0891-8",
                "CorpusId": 59413995
            },
            "abstract": null,
            "referenceCount": 52,
            "citationCount": 135,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://oa.upm.es/67140/1/INVE_MEM_2019_317723.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-07-03",
            "journal": {
                "name": "Journal of Intelligent & Robotic Systems",
                "volume": "93"
            },
            "citationStyles": {
                "bibtex": "@Article{Rodriguez-Ramos2018ADR,\n author = {Alejandro Rodriguez-Ramos and Carlos Sampedro and Hriday Bavle and P. de la Puente and P. Campoy},\n booktitle = {Journal of Intelligent and Robotic Systems},\n journal = {Journal of Intelligent & Robotic Systems},\n pages = {351 - 366},\n title = {A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on a Moving Platform},\n volume = {93},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6865bc5ff0d28da7d2857f5821afc645c1e80807",
            "@type": "ScholarlyArticle",
            "paperId": "6865bc5ff0d28da7d2857f5821afc645c1e80807",
            "corpusId": 53108607,
            "url": "https://www.semanticscholar.org/paper/6865bc5ff0d28da7d2857f5821afc645c1e80807",
            "title": "Benchmarks for reinforcement learning in mixed-autonomy traffic",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2899076365",
                "DBLP": "conf/corl/VinitskyKFKJWLL18",
                "CorpusId": 53108607
            },
            "abstract": "We release new benchmarks in the use of deep reinforcement learning (RL) to create controllers for mixed-autonomy traffic, where connected and autonomous vehicles (CAVs) interact with human drivers and infrastructure. Benchmarks, such as Mujoco or the Arcade Learning Environment, have spurred new research by enabling researchers to effectively compare their results so that they can focus on algorithmic improvements and control techniques rather than system design. To promote similar advances in traffic control via RL, we propose four benchmarks, based on three new traffic scenarios, illustrating distinct reinforcement learning problems with applications to mixed-autonomy traffic. We provide an introduction to each control problem, an overview of their MDP structures, and preliminary performance results from commonly used RL algorithms. For the purpose of reproducibility, the benchmarks, reference implementations, and tutorials are available at https://github.com/flow-project/flow.",
            "referenceCount": 35,
            "citationCount": 116,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-10-23",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Vinitsky2018BenchmarksFR,\n author = {Eugene Vinitsky and Aboudy Kreidieh and Luc Le Flem and Nishant Kheterpal and Kathy Jang and Cathy Wu and Fangyu Wu and Richard Liaw and Eric Liang and A. Bayen},\n booktitle = {Conference on Robot Learning},\n pages = {399-409},\n title = {Benchmarks for reinforcement learning in mixed-autonomy traffic},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
            "@type": "ScholarlyArticle",
            "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
            "corpusId": 6719686,
            "url": "https://www.semanticscholar.org/paper/c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
            "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2604763608",
                "DBLP": "journals/corr/FinnAL17",
                "ArXiv": "1703.03400",
                "CorpusId": 6719686
            },
            "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.",
            "referenceCount": 52,
            "citationCount": 8840,
            "influentialCitationCount": 2249,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Finn2017ModelAgnosticMF,\n author = {Chelsea Finn and P. Abbeel and S. Levine},\n booktitle = {International Conference on Machine Learning},\n pages = {1126-1135},\n title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c4f529934b6f22aa38e014e295a9737daa6e7db5",
            "@type": "ScholarlyArticle",
            "paperId": "c4f529934b6f22aa38e014e295a9737daa6e7db5",
            "corpusId": 4949295,
            "url": "https://www.semanticscholar.org/paper/c4f529934b6f22aa38e014e295a9737daa6e7db5",
            "title": "Lipschitz Continuity in Model-based Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/icml/AsadiML18",
                "MAG": "2797811993",
                "ArXiv": "1804.07193",
                "CorpusId": 4949295
            },
            "abstract": "Model-based reinforcement-learning methods learn transition and reward models and use them to guide behavior. We analyze the impact of learning models that are Lipschitz continuous---the distance between function values for two inputs is bounded by a linear function of the distance between the inputs. Our first result shows a tight bound on model errors for multi-step predictions with Lipschitz continuous models. We go on to prove an error bound for the value-function estimate arising from such models and show that the estimated value function is itself Lipschitz continuous. We conclude with empirical results that demonstrate significant benefits to enforcing Lipschitz continuity of neural net models during reinforcement learning.",
            "referenceCount": 49,
            "citationCount": 105,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-04-19",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Asadi2018LipschitzCI,\n author = {Kavosh Asadi and Dipendra Kumar Misra and M. Littman},\n booktitle = {International Conference on Machine Learning},\n pages = {264-273},\n title = {Lipschitz Continuity in Model-based Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cf4d2e27da4ff7ee6dbd0e7a844db833444437d1",
            "@type": "ScholarlyArticle",
            "paperId": "cf4d2e27da4ff7ee6dbd0e7a844db833444437d1",
            "corpusId": 5068169,
            "url": "https://www.semanticscholar.org/paper/cf4d2e27da4ff7ee6dbd0e7a844db833444437d1",
            "title": "PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963709735",
                "DBLP": "conf/ijcai/YangLLG18",
                "ArXiv": "1804.07779",
                "DOI": "10.24963/ijcai.2018/675",
                "CorpusId": 5068169
            },
            "abstract": "Reinforcement learning and symbolic planning have both been used to build intelligent autonomous agents. Reinforcement learning relies on learning from interactions with real world, which often requires an unfeasibly large amount of experience. Symbolic planning relies on manually crafted symbolic knowledge, which may not be robust to domain uncertainties and changes. In this paper we present a unified framework PEORL that integrates symbolic planning with hierarchical reinforcement learning (HRL) to cope with decision-making in dynamic environment with uncertainties. Symbolic plans are used to guide the agent's task execution and learning, and the learned experience is fed back to symbolic knowledge to improve planning. This method leads to rapid policy search and robust symbolic plans in complex domains. The framework is tested on benchmark domains of HRL.",
            "referenceCount": 38,
            "citationCount": 108,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ijcai.org/proceedings/2018/0675.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-04-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2018PEORLIS,\n author = {Fangkai Yang and Daoming Lyu and Bo Liu and Steven M. Gustafson},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {4860-4866},\n title = {PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b80991d12b41a5a68dc14dd87b692c0f903ceb9c",
            "@type": "ScholarlyArticle",
            "paperId": "b80991d12b41a5a68dc14dd87b692c0f903ceb9c",
            "corpusId": 3707461,
            "url": "https://www.semanticscholar.org/paper/b80991d12b41a5a68dc14dd87b692c0f903ceb9c",
            "title": "Some Considerations on Learning to Explore via Meta-Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1803.01118",
                "DBLP": "journals/corr/abs-1803-01118",
                "MAG": "2962849330",
                "CorpusId": 3707461
            },
            "abstract": "We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and E-$\\text{RL}^2$. Results are presented on a novel environment we call `Krazy World' and a set of maze environments. We show E-MAML and E-$\\text{RL}^2$ deliver better performance on tasks where exploration is important.",
            "referenceCount": 51,
            "citationCount": 100,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1803.01118"
            },
            "citationStyles": {
                "bibtex": "@Article{Stadie2018SomeCO,\n author = {Bradly C. Stadie and Ge Yang and Rein Houthooft and Xi Chen and Yan Duan and Yuhuai Wu and P. Abbeel and Ilya Sutskever},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Some Considerations on Learning to Explore via Meta-Reinforcement Learning},\n volume = {abs/1803.01118},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:920febb03475b068286a855c10ea09b968fe7ee3",
            "@type": "ScholarlyArticle",
            "paperId": "920febb03475b068286a855c10ea09b968fe7ee3",
            "corpusId": 29150021,
            "url": "https://www.semanticscholar.org/paper/920febb03475b068286a855c10ea09b968fe7ee3",
            "title": "Reinforcement Learning of Theorem Proving",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2951549805",
                "ArXiv": "1805.07563",
                "DBLP": "conf/nips/KaliszykUMO18",
                "CorpusId": 29150021
            },
            "abstract": "We introduce a theorem proving algorithm that uses practically no domain heuristics for guiding its connection-style proof search. Instead, it runs many Monte-Carlo simulations guided by reinforcement learning from previous proof attempts. We produce several versions of the prover, parameterized by different learning and guiding algorithms. The strongest version of the system is trained on a large corpus of mathematical problems and evaluated on previously unseen problems. The trained system solves within the same number of inferences over 40% more problems than a baseline prover, which is an unusually high improvement in this hard AI domain. To our knowledge this is the first time reinforcement learning has been convincingly applied to solving general mathematical problems on a large scale.",
            "referenceCount": 53,
            "citationCount": 117,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-05-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1805.07563"
            },
            "citationStyles": {
                "bibtex": "@Article{Kaliszyk2018ReinforcementLO,\n author = {C. Kaliszyk and J. Urban and H. Michalewski and M. Ols\u00e1k},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Reinforcement Learning of Theorem Proving},\n volume = {abs/1805.07563},\n year = {2018}\n}\n"
            }
        }
    }
]