[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b80991d12b41a5a68dc14dd87b692c0f903ceb9c",
            "@type": "ScholarlyArticle",
            "paperId": "b80991d12b41a5a68dc14dd87b692c0f903ceb9c",
            "corpusId": 3707461,
            "url": "https://www.semanticscholar.org/paper/b80991d12b41a5a68dc14dd87b692c0f903ceb9c",
            "title": "Some Considerations on Learning to Explore via Meta-Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1803.01118",
                "DBLP": "journals/corr/abs-1803-01118",
                "MAG": "2962849330",
                "CorpusId": 3707461
            },
            "abstract": "We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and E-$\\text{RL}^2$. Results are presented on a novel environment we call `Krazy World' and a set of maze environments. We show E-MAML and E-$\\text{RL}^2$ deliver better performance on tasks where exploration is important.",
            "referenceCount": 51,
            "citationCount": 100,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1803.01118"
            },
            "citationStyles": {
                "bibtex": "@Article{Stadie2018SomeCO,\n author = {Bradly C. Stadie and Ge Yang and Rein Houthooft and Xi Chen and Yan Duan and Yuhuai Wu and P. Abbeel and Ilya Sutskever},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Some Considerations on Learning to Explore via Meta-Reinforcement Learning},\n volume = {abs/1803.01118},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:920febb03475b068286a855c10ea09b968fe7ee3",
            "@type": "ScholarlyArticle",
            "paperId": "920febb03475b068286a855c10ea09b968fe7ee3",
            "corpusId": 29150021,
            "url": "https://www.semanticscholar.org/paper/920febb03475b068286a855c10ea09b968fe7ee3",
            "title": "Reinforcement Learning of Theorem Proving",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2951549805",
                "ArXiv": "1805.07563",
                "DBLP": "conf/nips/KaliszykUMO18",
                "CorpusId": 29150021
            },
            "abstract": "We introduce a theorem proving algorithm that uses practically no domain heuristics for guiding its connection-style proof search. Instead, it runs many Monte-Carlo simulations guided by reinforcement learning from previous proof attempts. We produce several versions of the prover, parameterized by different learning and guiding algorithms. The strongest version of the system is trained on a large corpus of mathematical problems and evaluated on previously unseen problems. The trained system solves within the same number of inferences over 40% more problems than a baseline prover, which is an unusually high improvement in this hard AI domain. To our knowledge this is the first time reinforcement learning has been convincingly applied to solving general mathematical problems on a large scale.",
            "referenceCount": 53,
            "citationCount": 117,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-05-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1805.07563"
            },
            "citationStyles": {
                "bibtex": "@Article{Kaliszyk2018ReinforcementLO,\n author = {C. Kaliszyk and J. Urban and H. Michalewski and M. Ols\u00e1k},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Reinforcement Learning of Theorem Proving},\n volume = {abs/1805.07563},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cd5a26b89f0799db1cbc1dff5607cb6815739fe7",
            "@type": "ScholarlyArticle",
            "paperId": "cd5a26b89f0799db1cbc1dff5607cb6815739fe7",
            "corpusId": 1640103,
            "url": "https://www.semanticscholar.org/paper/cd5a26b89f0799db1cbc1dff5607cb6815739fe7",
            "title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2950338507",
                "DBLP": "journals/corr/abs-1012-2599",
                "ArXiv": "1012.2599",
                "CorpusId": 1640103
            },
            "abstract": "We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.",
            "referenceCount": 107,
            "citationCount": 2136,
            "influentialCitationCount": 226,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2010-12-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1012.2599"
            },
            "citationStyles": {
                "bibtex": "@Article{Brochu2010ATO,\n author = {E. Brochu and Vlad M. Cora and Nando de Freitas},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning},\n volume = {abs/1012.2599},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3f4c70c93c30352ed2b0259307ac51b5b6ffd478",
            "@type": "ScholarlyArticle",
            "paperId": "3f4c70c93c30352ed2b0259307ac51b5b6ffd478",
            "corpusId": 46995658,
            "url": "https://www.semanticscholar.org/paper/3f4c70c93c30352ed2b0259307ac51b5b6ffd478",
            "title": "State Abstractions for Lifelong Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/icml/AbelALL18",
                "MAG": "2803178532",
                "CorpusId": 46995658
            },
            "abstract": "In lifelong reinforcement learning, agents must effectively transfer knowledge across tasks while simultaneously addressing exploration, credit assignment, and generalization. State abstraction can help overcome these hurdles by compressing the representation used by an agent, thereby reducing the computational and statistical burdens of learning. To this end, we here develop theory to compute and use state abstractions in lifelong reinforcement learning. We introduce two new classes of abstractions: (1) transitive state abstractions, whose optimal form can be computed ef\ufb01-ciently, and (2) PAC state abstractions, which are guaranteed to hold with respect to a distribution of tasks. We show that the joint family of transitive PAC abstractions can be acquired ef\ufb01ciently, preserve near optimal-behavior, and experimentally reduce sample complexity in simple domains, thereby yielding a family of desirable abstractions for use in lifelong reinforcement learning. Along with these positive results, we show that there are pathological cases where state abstractions can negatively impact performance.",
            "referenceCount": 43,
            "citationCount": 96,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-07-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Abel2018StateAF,\n author = {David Abel and Dilip Arumugam and Lucas Lehnert and M. Littman},\n booktitle = {International Conference on Machine Learning},\n pages = {10-19},\n title = {State Abstractions for Lifelong Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:43c0f5ebd97225369e88418b4e957c3d592c0389",
            "@type": "ScholarlyArticle",
            "paperId": "43c0f5ebd97225369e88418b4e957c3d592c0389",
            "corpusId": 21697434,
            "url": "https://www.semanticscholar.org/paper/43c0f5ebd97225369e88418b4e957c3d592c0389",
            "title": "Self-Paced Prioritized Curriculum Learning With Coverage Penalty in Deep Reinforcement Learning",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/tnn/RenDLC18",
                "MAG": "2793798239",
                "DOI": "10.1109/TNNLS.2018.2790981",
                "CorpusId": 21697434,
                "PubMed": "29771673"
            },
            "abstract": "In this paper, a new training paradigm is proposed for deep reinforcement learning using self-paced prioritized curriculum learning with coverage penalty. The proposed deep curriculum reinforcement learning (DCRL) takes the most advantage of experience replay by adaptively selecting appropriate transitions from replay memory based on the complexity of each transition. The criteria of complexity in DCRL consist of self-paced priority as well as coverage penalty. The self-paced priority reflects the relationship between the temporal-difference error and the difficulty of the current curriculum for sample efficiency. The coverage penalty is taken into account for sample diversity. With comparison to deep Q network (DQN) and prioritized experience replay (PER) methods, the DCRL algorithm is evaluated on Atari 2600 games, and the experimental results show that DCRL outperforms DQN and PER on most of these games. More results further show that the proposed curriculum training paradigm of DCRL is also applicable and effective for other memory-based deep reinforcement learning approaches, such as double DQN and dueling network. All the experimental results demonstrate that DCRL can achieve improved training efficiency and robustness for deep reinforcement learning.",
            "referenceCount": 0,
            "citationCount": 89,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-01",
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "29"
            },
            "citationStyles": {
                "bibtex": "@Article{Ren2018SelfPacedPC,\n author = {Zhipeng Ren and D. Dong and Huaxiong Li and Chunlin Chen},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {2216-2226},\n title = {Self-Paced Prioritized Curriculum Learning With Coverage Penalty in Deep Reinforcement Learning},\n volume = {29},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:40cc7cdffa0a861cb557410518246d97d1678642",
            "@type": "ScholarlyArticle",
            "paperId": "40cc7cdffa0a861cb557410518246d97d1678642",
            "corpusId": 52306614,
            "url": "https://www.semanticscholar.org/paper/40cc7cdffa0a861cb557410518246d97d1678642",
            "title": "Benchmarking Reinforcement Learning Algorithms on Real-World Robots",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2950268955",
                "ArXiv": "1809.07731",
                "DBLP": "journals/corr/abs-1809-07731",
                "CorpusId": 52306614
            },
            "abstract": "Through many recent successes in simulation, model-free reinforcement learning has emerged as a promising approach to solving continuous control robotic tasks. The research community is now able to reproduce, analyze and build quickly on these results due to open source implementations of learning algorithms and simulated benchmark tasks. To carry forward these successes to real-world applications, it is crucial to withhold utilizing the unique advantages of simulations that do not transfer to the real world and experiment directly with physical robots. However, reinforcement learning research with physical robots faces substantial resistance due to the lack of benchmark tasks and supporting source code. In this work, we introduce several reinforcement learning tasks with multiple commercially available robots that present varying levels of learning difficulty, setup, and repeatability. On these tasks, we test the learning performance of off-the-shelf implementations of four reinforcement learning algorithms and analyze sensitivity to their hyper-parameters to determine their readiness for applications in various real-world tasks. Our results show that with a careful setup of the task interface and computations, some of these implementations can be readily applicable to physical robots. We find that state-of-the-art learning algorithms are highly sensitive to their hyper-parameters and their relative ordering does not transfer across tasks, indicating the necessity of re-tuning them for each task for best performance. On the other hand, the best hyper-parameter configuration from one task may often result in effective learning on held-out tasks even with different robots, providing a reasonable default. We make the benchmark tasks publicly available to enhance reproducibility in real-world reinforcement learning.",
            "referenceCount": 27,
            "citationCount": 119,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-20",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1809.07731"
            },
            "citationStyles": {
                "bibtex": "@Article{Mahmood2018BenchmarkingRL,\n author = {A. Mahmood and D. Korenkevych and G. Vasan and W. Ma and J. Bergstra},\n booktitle = {Conference on Robot Learning},\n journal = {ArXiv},\n title = {Benchmarking Reinforcement Learning Algorithms on Real-World Robots},\n volume = {abs/1809.07731},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:84bb62e3f40434a1e367d24783bd81432a5396d6",
            "@type": "ScholarlyArticle",
            "paperId": "84bb62e3f40434a1e367d24783bd81432a5396d6",
            "corpusId": 3549024,
            "url": "https://www.semanticscholar.org/paper/84bb62e3f40434a1e367d24783bd81432a5396d6",
            "title": "Integrating State Representation Learning Into Deep Reinforcement Learning",
            "venue": "IEEE Robotics and Automation Letters",
            "publicationVenue": {
                "id": "urn:research:93c335b7-edf4-45f5-8ddc-7c5835154945",
                "name": "IEEE Robotics and Automation Letters",
                "alternate_names": [
                    "IEEE Robot Autom Lett"
                ],
                "issn": "2377-3766",
                "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/ral/BruinKTB18",
                "MAG": "2790924949",
                "DOI": "10.1109/LRA.2018.2800101",
                "CorpusId": 3549024
            },
            "abstract": "Most deep reinforcement learning techniques are unsuitable for robotics, as they require too much interaction time to learn useful, general control policies. This problem can be largely attributed to the fact that a state representation needs to be learned as a part of learning control policies, which can only be done through fitting expected returns based on observed rewards. While the reward function provides information on the desirability of the state of the world, it does not necessarily provide information on how to distill a good, general representation of that state from the sensory observations. State representation learning objectives can be used to help learn such a representation. While many of these objectives have been proposed, they are typically not directly combined with reinforcement learning algorithms. We investigate several methods for integrating state representation learning into reinforcement learning. In these methods, the state representation learning objectives help regularize the state representation during the reinforcement learning, and the reinforcement learning itself is viewed as a crucial state representation learning objective and allowed to help shape the representation. Using autonomous racing tests in the TORCS simulator, we show how the integrated methods quickly learn policies that generalize to new environments much better than deep reinforcement learning without state representation learning.",
            "referenceCount": 32,
            "citationCount": 96,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://repository.tudelft.nl/islandora/object/uuid%3Aadbec839-4b4b-4877-9143-09346b560249/datastream/OBJ/download",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-01-31",
            "journal": {
                "name": "IEEE Robotics and Automation Letters",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Bruin2018IntegratingSR,\n author = {Tim de Bruin and J. Kober and K. Tuyls and Robert Babu\u0161ka},\n booktitle = {IEEE Robotics and Automation Letters},\n journal = {IEEE Robotics and Automation Letters},\n pages = {1394-1401},\n title = {Integrating State Representation Learning Into Deep Reinforcement Learning},\n volume = {3},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b93317f61c6ed99542da9d1d691ded9732c16c1c",
            "@type": "ScholarlyArticle",
            "paperId": "b93317f61c6ed99542da9d1d691ded9732c16c1c",
            "corpusId": 48364213,
            "url": "https://www.semanticscholar.org/paper/b93317f61c6ed99542da9d1d691ded9732c16c1c",
            "title": "Unsupervised Meta-Learning for Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1806-04640",
                "MAG": "2808682055",
                "ArXiv": "1806.04640",
                "CorpusId": 48364213
            },
            "abstract": "Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta-learning methods generalize best to tasks from the same distribution as the meta-training tasks. In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design. If we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automated. In this work, we take a step in this direction, proposing a family of unsupervised meta-learning algorithms for reinforcement learning. We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approach. Our conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can be used to train optimal meta-learners. Our experimental results indicate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design and these procedures exceed the performance of learning from scratch.",
            "referenceCount": 69,
            "citationCount": 93,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.04640"
            },
            "citationStyles": {
                "bibtex": "@Article{Gupta2018UnsupervisedMF,\n author = {Abhishek Gupta and Benjamin Eysenbach and Chelsea Finn and S. Levine},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Unsupervised Meta-Learning for Reinforcement Learning},\n volume = {abs/1806.04640},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1cb6edbedc4a1ac5c32f61a435a23264e42a9071",
            "@type": "ScholarlyArticle",
            "paperId": "1cb6edbedc4a1ac5c32f61a435a23264e42a9071",
            "corpusId": 51609598,
            "url": "https://www.semanticscholar.org/paper/1cb6edbedc4a1ac5c32f61a435a23264e42a9071",
            "title": "Towards Sample Efficient Reinforcement Learning",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/ijcai/Yu18",
                "MAG": "2877093712",
                "DOI": "10.24963/ijcai.2018/820",
                "CorpusId": 51609598
            },
            "abstract": "Reinforcement learning is a major tool to realize intelligent agents that can be autonomously adaptive to the environment. With deep models, reinforcement learning has shown great potential in complex tasks such as playing games from pixels. However, current reinforcement learning techniques are still suffer from requiring a huge amount of interaction data, which could result in unbearable cost in real-world applications. In this article, we share our understanding of the problem, and discuss possible ways to alleviate the sample cost of reinforcement learning, from the aspects of exploration, optimization, environment modeling, experience transfer, and abstraction. We also discuss some challenges in real-world applications, with the hope of inspiring future researches.",
            "referenceCount": 41,
            "citationCount": 120,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ijcai.org/proceedings/2018/0820.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-07-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yu2018TowardsSE,\n author = {Yang Yu},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {5739-5743},\n title = {Towards Sample Efficient Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d4e137eeec6ca4883df9f9cf40cc49f62e8388be",
            "@type": "ScholarlyArticle",
            "paperId": "d4e137eeec6ca4883df9f9cf40cc49f62e8388be",
            "corpusId": 580203,
            "url": "https://www.semanticscholar.org/paper/d4e137eeec6ca4883df9f9cf40cc49f62e8388be",
            "title": "Multi-agent Reinforcement Learning in Sequential Social Dilemmas",
            "venue": "Adaptive Agents and Multi-Agent Systems",
            "publicationVenue": {
                "id": "urn:research:6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                "name": "Adaptive Agents and Multi-Agent Systems",
                "alternate_names": [
                    "Adapt Agent Multi-agent Syst",
                    "International Joint Conference on Autonomous Agents & Multiagent Systems",
                    "Adapt Agent Multi-agents Syst",
                    "AAMAS",
                    "Adaptive Agents and Multi-Agents Systems",
                    "Int Jt Conf Auton Agent  Multiagent Syst"
                ],
                "issn": null,
                "url": "http://www.ifaamas.org/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2964345382",
                "ArXiv": "1702.03037",
                "DBLP": "journals/corr/LeiboZLMG17",
                "CorpusId": 580203
            },
            "abstract": "Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.",
            "referenceCount": 52,
            "citationCount": 528,
            "influentialCitationCount": 45,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-02-10",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Leibo2017MultiagentRL,\n author = {Joel Z. Leibo and V. Zambaldi and Marc Lanctot and J. Marecki and T. Graepel},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {464-473},\n title = {Multi-agent Reinforcement Learning in Sequential Social Dilemmas},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:168b7d0ab57a331a228ce21ffd1becbb93066f79",
            "@type": "ScholarlyArticle",
            "paperId": "168b7d0ab57a331a228ce21ffd1becbb93066f79",
            "corpusId": 29535948,
            "url": "https://www.semanticscholar.org/paper/168b7d0ab57a331a228ce21ffd1becbb93066f79",
            "title": "Neural Optimizer Search with Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/icml/BelloZVL17",
                "ArXiv": "1709.07417",
                "MAG": "2962716258",
                "CorpusId": 29535948
            },
            "abstract": "We present an approach to automate the process of discovering optimization methods, with a focus on deep learning architectures. We train a Recurrent Neural Network controller to generate a string in a domain specific language that describes a mathematical update equation based on a list of primitive functions, such as the gradient, running average of the gradient, etc. The controller is trained with Reinforcement Learning to maximize the performance of a model after a few epochs. On CIFAR-10, our method discovers several update rules that are better than many commonly used optimizers, such as Adam, RMSProp, or SGD with and without Momentum on a ConvNet model. We introduce two new optimizers, named PowerSign and AddSign, which we show transfer well and improve training on a variety of different tasks and architectures, including ImageNet classification and Google's neural machine translation system.",
            "referenceCount": 49,
            "citationCount": 347,
            "influentialCitationCount": 33,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-08-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1709.07417"
            },
            "citationStyles": {
                "bibtex": "@Article{Bello2017NeuralOS,\n author = {Irwan Bello and Barret Zoph and Vijay Vasudevan and Quoc V. Le},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Neural Optimizer Search with Reinforcement Learning},\n volume = {abs/1709.07417},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e0b65d3839e3bf703d156b524d7db7a5e10a2623",
            "@type": "ScholarlyArticle",
            "paperId": "e0b65d3839e3bf703d156b524d7db7a5e10a2623",
            "corpusId": 13973139,
            "url": "https://www.semanticscholar.org/paper/e0b65d3839e3bf703d156b524d7db7a5e10a2623",
            "title": "Playing FPS Games with Deep Reinforcement Learning",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1609.05521",
                "MAG": "2522489477",
                "DBLP": "journals/corr/LampleC16",
                "DOI": "10.1609/aaai.v31i1.10827",
                "CorpusId": 13973139
            },
            "abstract": "\n \n Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in AI agents of the game as well as average humans in deathmatch scenarios.\n \n",
            "referenceCount": 20,
            "citationCount": 505,
            "influentialCitationCount": 30,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10827/10686",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-09-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1609.05521"
            },
            "citationStyles": {
                "bibtex": "@Article{Lample2016PlayingFG,\n author = {Guillaume Lample and Devendra Singh Chaplot},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Playing FPS Games with Deep Reinforcement Learning},\n volume = {abs/1609.05521},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1e6abd43fcb157fde4d4ddc3ac8787ae45dbf777",
            "@type": "ScholarlyArticle",
            "paperId": "1e6abd43fcb157fde4d4ddc3ac8787ae45dbf777",
            "corpusId": 6208061,
            "url": "https://www.semanticscholar.org/paper/1e6abd43fcb157fde4d4ddc3ac8787ae45dbf777",
            "title": "Cooperative Inverse Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2410842990",
                "ArXiv": "1606.03137",
                "DBLP": "journals/corr/Hadfield-Menell16",
                "CorpusId": 6208061
            },
            "abstract": "For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.",
            "referenceCount": 34,
            "citationCount": 518,
            "influentialCitationCount": 33,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hadfield-Menell2016CooperativeIR,\n author = {Dylan Hadfield-Menell and Stuart J. Russell and P. Abbeel and A. Dragan},\n booktitle = {Neural Information Processing Systems},\n pages = {3909-3917},\n title = {Cooperative Inverse Reinforcement Learning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:64a2b359144f3eaea6f24101053fed8c24717c70",
            "@type": "ScholarlyArticle",
            "paperId": "64a2b359144f3eaea6f24101053fed8c24717c70",
            "corpusId": 56517178,
            "url": "https://www.semanticscholar.org/paper/64a2b359144f3eaea6f24101053fed8c24717c70",
            "title": "Optimizing Quantum Error Correction Codes with Reinforcement Learning",
            "venue": "Quantum",
            "publicationVenue": {
                "id": "urn:research:15e7785d-fff7-4214-bf1d-bb4ded7ed461",
                "name": "Quantum",
                "alternate_names": null,
                "issn": "2521-327X",
                "url": "https://quantum-journal.org/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2905355477",
                "ArXiv": "1812.08451",
                "DBLP": "journals/corr/abs-1812-08451",
                "DOI": "10.22331/q-2019-12-16-215",
                "CorpusId": 56517178
            },
            "abstract": "Quantum error correction is widely thought to be the key to fault-tolerant quantum computation. However, determining the most suited encoding for unknown error channels or specific laboratory setups is highly challenging. Here, we present a reinforcement learning framework for optimizing and fault-tolerantly adapting quantum error correction codes. We consider a reinforcement learning agent tasked with modifying a family of surface code quantum memories until a desired logical error rate is reached. Using efficient simulations with about 70 data qubits with arbitrary connectivity, we demonstrate that such a reinforcement learning agent can determine near-optimal solutions, in terms of the number of data qubits, for various error models of interest. Moreover, we show that agents trained on one setting are able to successfully transfer their experience to different settings. This ability for transfer learning showcases the inherent strengths of reinforcement learning and the applicability of our approach for optimization from off-line simulations to on-line laboratory settings.",
            "referenceCount": 110,
            "citationCount": 115,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://quantum-journal.org/papers/q-2019-12-16-215/pdf/",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Physics",
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-12-20",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1812.08451"
            },
            "citationStyles": {
                "bibtex": "@Article{Nautrup2018OptimizingQE,\n author = {Hendrik Poulsen Nautrup and Nicolas Delfosse and V. Dunjko and H. Briegel and N. Friis},\n booktitle = {Quantum},\n journal = {ArXiv},\n title = {Optimizing Quantum Error Correction Codes with Reinforcement Learning},\n volume = {abs/1812.08451},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9862caed8ee93321c78b0196e0b7eef516b545ba",
            "@type": "ScholarlyArticle",
            "paperId": "9862caed8ee93321c78b0196e0b7eef516b545ba",
            "corpusId": 19181872,
            "url": "https://www.semanticscholar.org/paper/9862caed8ee93321c78b0196e0b7eef516b545ba",
            "title": "Reverse Curriculum Generation for Reinforcement Learning",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/FlorensaHWA17",
                "ArXiv": "1707.05300",
                "MAG": "2963311874",
                "CorpusId": 19181872
            },
            "abstract": "Many relevant tasks require an agent to reach a certain state, or to manipulate objects into a desired configuration. For example, we might want a robot to align and assemble a gear onto an axle or insert and turn a key in a lock. These goal-oriented tasks present a considerable challenge for reinforcement learning, since their natural reward function is sparse and prohibitive amounts of exploration are required to reach the goal and receive some learning signal. Past approaches tackle these problems by exploiting expert demonstrations or by manually designing a task-specific reward shaping function to guide the learning agent. Instead, we propose a method to learn these tasks without requiring any prior knowledge other than obtaining a single state in which the task is achieved. The robot is trained in reverse, gradually learning to reach the goal from a set of start states increasingly far from the goal. Our method automatically generates a curriculum of start states that adapts to the agent's performance, leading to efficient training on goal-oriented tasks. We demonstrate our approach on difficult simulated navigation and fine-grained manipulation problems, not solvable by state-of-the-art reinforcement learning methods.",
            "referenceCount": 42,
            "citationCount": 358,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-07-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1707.05300"
            },
            "citationStyles": {
                "bibtex": "@Article{Florensa2017ReverseCG,\n author = {Carlos Florensa and David Held and Markus Wulfmeier and Michael Zhang and P. Abbeel},\n booktitle = {Conference on Robot Learning},\n journal = {ArXiv},\n title = {Reverse Curriculum Generation for Reinforcement Learning},\n volume = {abs/1707.05300},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:75a760c6bd5ae15e0fc489a074bc42bc1fc4e697",
            "@type": "ScholarlyArticle",
            "paperId": "75a760c6bd5ae15e0fc489a074bc42bc1fc4e697",
            "corpusId": 5136932,
            "url": "https://www.semanticscholar.org/paper/75a760c6bd5ae15e0fc489a074bc42bc1fc4e697",
            "title": "Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/Shalev-ShwartzS16a",
                "MAG": "2530849036",
                "ArXiv": "1610.03295",
                "CorpusId": 5136932
            },
            "abstract": "Autonomous driving is a multi-agent setting where the host vehicle must apply sophisticated negotiation skills with other road users when overtaking, giving way, merging, taking left and right turns and while pushing ahead in unstructured urban roadways. Since there are many possible scenarios, manually tackling all possible cases will likely yield a too simplistic policy. Moreover, one must balance between unexpected behavior of other drivers/pedestrians and at the same time not to be too defensive so that normal traffic flow is maintained. \nIn this paper we apply deep reinforcement learning to the problem of forming long term driving strategies. We note that there are two major challenges that make autonomous driving different from other robotic tasks. First, is the necessity for ensuring functional safety - something that machine learning has difficulty with given that performance is optimized at the level of an expectation over many instances. Second, the Markov Decision Process model often used in robotics is problematic in our case because of unpredictable behavior of other agents in this multi-agent scenario. We make three contributions in our work. First, we show how policy gradient iterations can be used without Markovian assumptions. Second, we decompose the problem into a composition of a Policy for Desires (which is to be learned) and trajectory planning with hard constraints (which is not learned). The goal of Desires is to enable comfort of driving, while hard constraints guarantees the safety of driving. Third, we introduce a hierarchical temporal abstraction we call an \"Option Graph\" with a gating mechanism that significantly reduces the effective horizon and thereby reducing the variance of the gradient estimation even further.",
            "referenceCount": 45,
            "citationCount": 652,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-10-11",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1610.03295"
            },
            "citationStyles": {
                "bibtex": "@Article{Shalev-Shwartz2016SafeMR,\n author = {S. Shalev-Shwartz and Shaked Shammah and A. Shashua},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving},\n volume = {abs/1610.03295},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:236b40f3144b95cd84779484c8269092122920aa",
            "@type": "ScholarlyArticle",
            "paperId": "236b40f3144b95cd84779484c8269092122920aa",
            "corpusId": 4476190,
            "url": "https://www.semanticscholar.org/paper/236b40f3144b95cd84779484c8269092122920aa",
            "title": "Tactics of Adversarial Attack on Deep Reinforcement Learning Agents",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2952603690",
                "DBLP": "conf/iclr/LinHLS0S17",
                "ArXiv": "1703.06748",
                "DOI": "10.24963/ijcai.2017/525",
                "CorpusId": 4476190
            },
            "abstract": "We introduce two tactics, namely the strategically-timed attack and the enchanting attack, to attack reinforcement learning agents trained by deep reinforcement learning algorithms using adversarial examples. In the strategically-timed attack, the adversary aims at minimizing the agent's reward by only attacking the agent at a small subset of time steps in an episode. Limiting the attack activity to this subset helps prevent detection of the attack by the agent. We propose a novel method to determine when an adversarial example should be crafted and applied. In the enchanting attack, the adversary aims at luring the agent to a designated target state. This is achieved by combining a generative model and a planning algorithm: while the generative model predicts the future states, the planning algorithm generates a preferred sequence of actions for luring the agent. A sequence of adversarial examples is then crafted to lure the agent to take the preferred sequence of actions. We apply the proposed tactics to the agents trained by the state-of-the-art deep reinforcement learning algorithm including DQN and A3C. In 5 Atari games, our strategically-timed attack reduces as much reward as the uniform attack (i.e., attacking at every time step) does by attacking the agent 4 times less often. Our enchanting attack lures the agent toward designated target states with a more than 70% success rate. Example videos are available at http://yclin.me/adversarial_attack_RL/.",
            "referenceCount": 21,
            "citationCount": 340,
            "influentialCitationCount": 32,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ijcai.org/proceedings/2017/0525.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lin2017TacticsOA,\n author = {Yen-Chen Lin and Zhang-Wei Hong and Yuan-Hong Liao and Meng-Li Shih and Ming-Yu Liu and Min Sun},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {3756-3762},\n title = {Tactics of Adversarial Attack on Deep Reinforcement Learning Agents},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d77bc27d16a362e5e1b727904c3789355dda6062",
            "@type": "ScholarlyArticle",
            "paperId": "d77bc27d16a362e5e1b727904c3789355dda6062",
            "corpusId": 2978311,
            "url": "https://www.semanticscholar.org/paper/d77bc27d16a362e5e1b727904c3789355dda6062",
            "title": "Molecular de-novo design through deep reinforcement learning",
            "venue": "Journal of Cheminformatics",
            "publicationVenue": {
                "id": "urn:research:fd4675fe-4136-446c-aefd-3658aae698ac",
                "name": "Journal of Cheminformatics",
                "alternate_names": [
                    "J Cheminformatics"
                ],
                "issn": "1758-2946",
                "url": "https://jcheminf.biomedcentral.com/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2949437520",
                "DBLP": "journals/corr/OlivecronaBEC17",
                "PubMedCentral": "5583141",
                "ArXiv": "1704.07555",
                "DOI": "10.1186/s13321-017-0235-x",
                "CorpusId": 2978311,
                "PubMed": "29086083"
            },
            "abstract": null,
            "referenceCount": 48,
            "citationCount": 754,
            "influentialCitationCount": 53,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jcheminf.biomedcentral.com/track/pdf/10.1186/s13321-017-0235-x",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-04-25",
            "journal": {
                "name": "Journal of Cheminformatics",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Olivecrona2017MolecularDD,\n author = {Marcus Olivecrona and T. Blaschke and O. Engkvist and Hongming Chen},\n booktitle = {Journal of Cheminformatics},\n journal = {Journal of Cheminformatics},\n title = {Molecular de-novo design through deep reinforcement learning},\n volume = {9},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8037db97f3cadb842ffa4bee83d59878fe7974d0",
            "@type": "ScholarlyArticle",
            "paperId": "8037db97f3cadb842ffa4bee83d59878fe7974d0",
            "corpusId": 49321700,
            "url": "https://www.semanticscholar.org/paper/8037db97f3cadb842ffa4bee83d59878fe7974d0",
            "title": "Sim-to-Real Reinforcement Learning for Deformable Object Manipulation",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2808844346",
                "ArXiv": "1806.07851",
                "DBLP": "journals/corr/abs-1806-07851",
                "CorpusId": 49321700
            },
            "abstract": "We have seen much recent progress in rigid object manipulation, but interaction with deformable objects has notably lagged behind. Due to the large configuration space of deformable objects, solutions using traditional modelling approaches require significant engineering work. Perhaps then, bypassing the need for explicit modelling and instead learning the control in an end-to-end manner serves as a better approach? Despite the growing interest in the use of end-to-end robot learning approaches, only a small amount of work has focused on their applicability to deformable object manipulation. Moreover, due to the large amount of data needed to learn these end-to-end solutions, an emerging trend is to learn control policies in simulation and then transfer them over to the real world. To-date, no work has explored whether it is possible to learn and transfer deformable object policies. We believe that if sim-to-real methods are to be employed further, then it should be possible to learn to interact with a wide variety of objects, and not only rigid objects. In this work, we use a combination of state-of-the-art deep reinforcement learning algorithms to solve the problem of manipulating deformable objects (specifically cloth). We evaluate our approach on three tasks --- folding a towel up to a mark, folding a face towel diagonally, and draping a piece of cloth over a hanger. Our agents are fully trained in simulation with domain randomisation, and then successfully deployed in the real world without having seen any real deformable objects.",
            "referenceCount": 37,
            "citationCount": 277,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-20",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.07851"
            },
            "citationStyles": {
                "bibtex": "@Article{Matas2018SimtoRealRL,\n author = {J. Matas and Stephen James and A. Davison},\n booktitle = {Conference on Robot Learning},\n journal = {ArXiv},\n title = {Sim-to-Real Reinforcement Learning for Deformable Object Manipulation},\n volume = {abs/1806.07851},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b587ee7c802a5bd222a69090f59285e0dfdb29f1",
            "@type": "ScholarlyArticle",
            "paperId": "b587ee7c802a5bd222a69090f59285e0dfdb29f1",
            "corpusId": 6940861,
            "url": "https://www.semanticscholar.org/paper/b587ee7c802a5bd222a69090f59285e0dfdb29f1",
            "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
            "venue": "Neural Networks",
            "publicationVenue": {
                "id": "urn:research:a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                "name": "Neural Networks",
                "alternate_names": [
                    "Neural Netw"
                ],
                "issn": "0893-6080",
                "url": "http://www.elsevier.com/locate/neunet"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2594171637",
                "ArXiv": "1702.03118",
                "DBLP": "journals/corr/ElfwingUD17",
                "DOI": "10.1016/j.neunet.2017.12.012",
                "CorpusId": 6940861,
                "PubMed": "29395652"
            },
            "abstract": null,
            "referenceCount": 33,
            "citationCount": 792,
            "influentialCitationCount": 49,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-02-10",
            "journal": {
                "name": "Neural networks : the official journal of the International Neural Network Society",
                "volume": "107"
            },
            "citationStyles": {
                "bibtex": "@Article{Elfwing2017SigmoidWeightedLU,\n author = {Stefan Elfwing and E. Uchibe and K. Doya},\n booktitle = {Neural Networks},\n journal = {Neural networks : the official journal of the International Neural Network Society},\n pages = {\n          3-11\n        },\n title = {Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning},\n volume = {107},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea6956bbb4fda636a582f69c918fb8c3b3c8e9cc",
            "@type": "ScholarlyArticle",
            "paperId": "ea6956bbb4fda636a582f69c918fb8c3b3c8e9cc",
            "corpusId": 226011965,
            "url": "https://www.semanticscholar.org/paper/ea6956bbb4fda636a582f69c918fb8c3b3c8e9cc",
            "title": "Reinforcement Learning in Finanza = Reinforcement Learning in Finance",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3045788062",
                "CorpusId": 226011965
            },
            "abstract": "Reinforcement Learning in Finanza In questa tesi si analizza come gli algoritmi di Reinforcement Learning si comportano nel caso di problemi applicati alla finanza; in particolare problemi di trading ad alta frequenza sui mercati finanziari . Sono analizzati algoritmi di reinforcement learnig model free in cui non e necessario conoscere come sia modelizzato l'ambiente circostante per ottenere la politica ottimale, ma lo spazio degli stati e lo spazio delle azioni, sono sufficienti per raggiungere quest'ultima. Sono sviluppati due differenti algoritmi, la value iteration e il Q-Learning, con in aggiunta la sua variante epsilon-QLearning. Il primo e possibile applicarlo sia nel caso di spazio degli stati discreto sia nel caso di spazio degli stati continuo; per quanto riguarda il secondo si analizza solo il caso di spazio degli stati discreto. Anche lo spazio delle azioni e considerato, nel corso delle analisi, discreto. Tutte le analisi sono svolte nel caso di orizzonte temporale discreto ed infinito. Sono sviluppati tre differenti casi di studio per verificare l'efficacia degli algoritmi descritti. Il primo caso e puramente accademico e considera l'analisi di un asset che ha un movimento del prezzo mean reverting. Gli algoritmi devono essere in grado di cambiare la posizione assunta sull'asset in modo da incrementare la profit & loss cumulata. Si analizza il caso di studio attraverso l'utilizzo della value-iteration e del Q-Learning, attraverso un primo processo di validazione dei parametri e un successivo test. Lo stesso meccanismo e utilizzato per gli altri casi di studio. I due casi di studio rimanenti sono invece situazioni reali. Il secondo ricerca dei pattern storici su strumenti volatili, quali gli ETF, ETC, ETN. Questa ricerca e fatta con l'ausilio di alcuni indicatori di analisi tecnica che inoltre permettono di definire lo spazio degli stati. In questo caso gli algoritmi devono decidere quale momento sia opportuno per aprire una posizione, lunga o corta, sullo strumento in analisi. La profit & Loss si misura attraverso il rendimento; la chiusura della posizione e vincolata al tempo oppure al raggiungimento di un criterio di stop. In questo caso di studio si utilizzano tutti e tre gli algoritmi analizzati e si confrontano i risultati ottenuti al fine di ottenere l'algoritmo migliore. Infine nell'ultimo caso di studio si analizza una nuova soluzione per un problema importante per gli istituti finanziari che svolgono l'attivita di market making; ovvero fornire liquidita al mercato. Si analizza il problema dell'hedging, ovvero una copertura di un investimento. E' analizzato l'hedging sui titoli di stato attraverso uno strumento derivato molto liquido chiamato futures. In particolare si analizza il caso di un hedging breve, nel quale la posizione sullo strumento potrebbe durare pochi minuti. L'obbiettivo degli algortmi e trovare la quantita di futures da acquistare al fine di essere hedgati. In questo caso di studio si utilizza il Q-Learning e la sua variante. Si confrontano i risultati ottenuti anche con la politica di benchmark attualmente utilizzata.",
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Art"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Art",
                    "source": "external"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2020-03-19",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Bernucchio2020ReinforcementLI,\n author = {Alessandro Ruo Bernucchio},\n title = {Reinforcement Learning in Finanza = Reinforcement Learning in Finance},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7",
            "@type": "ScholarlyArticle",
            "paperId": "3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7",
            "corpusId": 14711954,
            "url": "https://www.semanticscholar.org/paper/3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7",
            "title": "Modular Multitask Reinforcement Learning with Policy Sketches",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2553882142",
                "ArXiv": "1611.01796",
                "DBLP": "journals/corr/AndreasKL16",
                "CorpusId": 14711954
            },
            "abstract": "We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them\u2014specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor-critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level sub-goals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.",
            "referenceCount": 39,
            "citationCount": 397,
            "influentialCitationCount": 48,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-11-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.01796"
            },
            "citationStyles": {
                "bibtex": "@Article{Andreas2016ModularMR,\n author = {Jacob Andreas and D. Klein and S. Levine},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Modular Multitask Reinforcement Learning with Policy Sketches},\n volume = {abs/1611.01796},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d09223da6cf1ae3b7692095a05fe45f5e6a95a13",
            "@type": "ScholarlyArticle",
            "paperId": "d09223da6cf1ae3b7692095a05fe45f5e6a95a13",
            "corpusId": 44019896,
            "url": "https://www.semanticscholar.org/paper/d09223da6cf1ae3b7692095a05fe45f5e6a95a13",
            "title": "Traffic signal timing via deep reinforcement learning",
            "venue": "IEEE/CAA Journal of Automatica Sinica",
            "publicationVenue": {
                "id": "urn:research:ef1356d5-69c7-484e-a110-3efae1e93ecc",
                "name": "IEEE/CAA Journal of Automatica Sinica",
                "alternate_names": [
                    "IEEE/CAA J Autom Sin"
                ],
                "issn": "2329-9266",
                "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=6570654"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2498017881",
                "DOI": "10.1109/JAS.2016.7508798",
                "CorpusId": 44019896
            },
            "abstract": "In this paper, we propose a set of algorithms to design signal timing plans via deep reinforcement learning. The core idea of this approach is to set up a deep neural network (DNN) to learn the Q-function of reinforcement learning from the sampled traffic state/control inputs and the corresponding traffic system performance output. Based on the obtained DNN, we can find the appropriate signal timing policies by implicitly modeling the control actions and the change of system states. We explain the possible benefits and implementation tricks of this new approach. The relationships between this new approach and some existing approaches are also carefully discussed.",
            "referenceCount": 28,
            "citationCount": 404,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2016-07-10",
            "journal": {
                "name": "IEEE/CAA Journal of Automatica Sinica",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2016TrafficST,\n author = {Li Li and Yisheng Lv and Feiyue Wang},\n booktitle = {IEEE/CAA Journal of Automatica Sinica},\n journal = {IEEE/CAA Journal of Automatica Sinica},\n pages = {247-254},\n title = {Traffic signal timing via deep reinforcement learning},\n volume = {3},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ecc297a7439b7a11677c4fc4b24e847d98517f5a",
            "@type": "ScholarlyArticle",
            "paperId": "ecc297a7439b7a11677c4fc4b24e847d98517f5a",
            "corpusId": 1459416,
            "url": "https://www.semanticscholar.org/paper/ecc297a7439b7a11677c4fc4b24e847d98517f5a",
            "title": "Reinforcement Learning And Approximate Dynamic Programming For Feedback Control",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "CorpusId": 1459416
            },
            "abstract": "feedback control of dynamic systems 6th solution PDF feedback control of dynamic systems 6th solutions PDF feedback control of dynamic systems 5th edition pdf PDF feedback control of dynamic systems solution PDF feedback control of dynamic systems 7th edition PDF feedback control of dynamic systems 6th edition PDF feedback control of dynamic systems solutions PDF feedback control of dynamic systems solution manual PDF feedback control of dynamic systems solutions manual PDF feedback control dynamic systems 5th edition solutions PDF solutions manual feedback control of dynamic systems PDF feedback control of dynamic systems solution manual 6th PDF feedback control of dynamic systems solutions manual 5th PDF feedback control of dynamic systems franklin solutions PDF feedback control of dynamic systems solutions 6th edition PDF feedback control of dynamic systems 6th edition solutions PDF solutions feedback control dynamic systems 6th edition PDF feedback control of dynamic systems 6th solutions manual PDF feedback control of dynamic systems 6th edition solution manual PDF feedback control of dynamic systems franklin 5th edition solution PDF dynamic programming and optimal control PDF dynamic programming & optimal control vol i PDF dynamic programming and optimal control solution manual PDF learning microsoft windows server 2012 dynamic access control PDF data-variant kernel analysis adaptive and cognitive dynamic systems signal processing learning communications and control PDF",
            "referenceCount": 1,
            "citationCount": 491,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{None,\n title = {Reinforcement Learning And Approximate Dynamic Programming For Feedback Control},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:163a35665374fcd5283da47a7ee611af7df2d876",
            "@type": "ScholarlyArticle",
            "paperId": "163a35665374fcd5283da47a7ee611af7df2d876",
            "corpusId": 1012400,
            "url": "https://www.semanticscholar.org/paper/163a35665374fcd5283da47a7ee611af7df2d876",
            "title": "Deep reinforcement learning for building HVAC control",
            "venue": "Design Automation Conference",
            "publicationVenue": {
                "id": "urn:research:021b37d3-cef1-4c12-a442-257f7900c23d",
                "name": "Design Automation Conference",
                "alternate_names": [
                    "Des Autom Conf",
                    "DAC"
                ],
                "issn": null,
                "url": "http://www.dac.com/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2625874945",
                "DBLP": "conf/dac/WeiWZ17",
                "DOI": "10.1145/3061639.3062224",
                "CorpusId": 1012400
            },
            "abstract": "Buildings account for nearly 40% of the total energy consumption in the United States, about half of which is used by the HVAC (heating, ventilation, and air conditioning) system. Intelligent scheduling of building HVAC systems has the potential to significantly reduce the energy cost. However, the traditional rule-based and model-based strategies are often inefficient in practice, due to the complexity in building thermal dynamics and heterogeneous environment disturbances. In this work, we develop a data-driven approach that leverages the deep reinforcement learning (DRL) technique, to intelligently learn the effective strategy for operating the building HVAC systems. We evaluate the performance of our DRL algorithm through simulations using the widely-adopted EnergyPlus tool. Experiments demonstrate that our DRL-based algorithm is more effective in energy cost reduction compared with the traditional rule-based approach, while maintaining the room temperature within desired range.",
            "referenceCount": 25,
            "citationCount": 310,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-06-18",
            "journal": {
                "name": "2017 54th ACM/EDAC/IEEE Design Automation Conference (DAC)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Wei2017DeepRL,\n author = {Tianshu Wei and Yanzhi Wang and Qi Zhu},\n booktitle = {Design Automation Conference},\n journal = {2017 54th ACM/EDAC/IEEE Design Automation Conference (DAC)},\n pages = {1-6},\n title = {Deep reinforcement learning for building HVAC control},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7541c956a13f50d8ffbef853539b466f6a77a80d",
            "@type": "ScholarlyArticle",
            "paperId": "7541c956a13f50d8ffbef853539b466f6a77a80d",
            "corpusId": 13117738,
            "url": "https://www.semanticscholar.org/paper/7541c956a13f50d8ffbef853539b466f6a77a80d",
            "title": "Virtual to Real Reinforcement Learning for Autonomous Driving",
            "venue": "British Machine Vision Conference",
            "publicationVenue": {
                "id": "urn:research:78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                "name": "British Machine Vision Conference",
                "alternate_names": [
                    "Br Mach Vis Conf",
                    "BMVC"
                ],
                "issn": null,
                "url": "http://www.bmva.org/bmvc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2962977206",
                "DBLP": "journals/corr/YouPWL17",
                "ArXiv": "1704.03952",
                "DOI": "10.5244/C.31.11",
                "CorpusId": 13117738
            },
            "abstract": "Reinforcement learning is considered as a promising direction for driving policy learning. However, training autonomous driving vehicle with reinforcement learning in real environment involves non-affordable trial-and-error. It is more desirable to first train in a virtual environment and then transfer to the real environment. In this paper, we propose a novel realistic translation network to make model trained in virtual environment be workable in real world. The proposed network can convert non-realistic virtual image input into a realistic one with similar scene structure. Given realistic frames as input, driving policy trained by reinforcement learning can nicely adapt to real world driving. Experiments show that our proposed virtual to real (VR) reinforcement learning (RL) works pretty well. To our knowledge, this is the first successful case of driving policy trained by reinforcement learning that can adapt to real world driving data.",
            "referenceCount": 36,
            "citationCount": 294,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.bmva.org/bmvc/2017/papers/paper011/paper011.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-04-13",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1704.03952"
            },
            "citationStyles": {
                "bibtex": "@Article{You2017VirtualTR,\n author = {Yurong You and Xinlei Pan and Ziyan Wang and Cewu Lu},\n booktitle = {British Machine Vision Conference},\n journal = {ArXiv},\n title = {Virtual to Real Reinforcement Learning for Autonomous Driving},\n volume = {abs/1704.03952},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:953c6f0c1ae5311e1bcc523fbba2de57d751c248",
            "@type": "ScholarlyArticle",
            "paperId": "953c6f0c1ae5311e1bcc523fbba2de57d751c248",
            "corpusId": 8683793,
            "url": "https://www.semanticscholar.org/paper/953c6f0c1ae5311e1bcc523fbba2de57d751c248",
            "title": "Deep-Reinforcement Learning Multiple Access for Heterogeneous Wireless Networks",
            "venue": "2018 IEEE International Conference on Communications (ICC)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "3034883985",
                "ArXiv": "1712.00162",
                "DBLP": "journals/corr/abs-1712-00162",
                "DOI": "10.1109/ICC.2018.8422168",
                "CorpusId": 8683793
            },
            "abstract": "This paper investigates the use of deep reinforcement learning (DRL) in the design of a \"universal\" MAC protocol referred to as Deep-reinforcement Learning Multiple Access (DLMA). The design framework is partially inspired by the vision of DARPA SC2, a 3-year competition whereby competitors are to come up with a clean-slate design that \"best share spectrum with any network(s), in any environment, without prior knowledge, leveraging on machine-learning technique\". While the scope of DARPA SC2 is broad and involves the redesign of PHY, MAC, and Network layers, this paper's focus is narrower and only involves the MAC design. In particular, we consider the problem of sharing time slots among a multiple of time-slotted networks that adopt different MAC protocols. One of the MAC protocols is DLMA. The other two are TDMA and ALOHA. The DRL agents of DLMA do not know that the other two MAC protocols are TDMA and ALOHA. Yet, by a series of observations of the environment, its own actions, and the rewards - in accordance with the DRL algorithmic framework - a DRL agent can learn the optimal MAC strategy for harmonious co-existence with TDMA and ALOHA nodes. In particular, the use of neural networks in DRL (as opposed to traditional reinforcement learning) allows for fast convergence to optimal solutions and robustness against perturbation in hyper- parameter settings, two essential properties for practical deployment of DLMA in real wireless networks.",
            "referenceCount": 21,
            "citationCount": 255,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1712.00162",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-12-01",
            "journal": {
                "name": "2018 IEEE International Conference on Communications (ICC)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yu2017DeepReinforcementLM,\n author = {Yiding Yu and Taotao Wang and S. Liew},\n booktitle = {2018 IEEE International Conference on Communications (ICC)},\n journal = {2018 IEEE International Conference on Communications (ICC)},\n pages = {1-7},\n title = {Deep-Reinforcement Learning Multiple Access for Heterogeneous Wireless Networks},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3b6c891fbccaa564ea4fd8914a5e3952fcf42ee3",
            "@type": "ScholarlyArticle",
            "paperId": "3b6c891fbccaa564ea4fd8914a5e3952fcf42ee3",
            "corpusId": 34383906,
            "url": "https://www.semanticscholar.org/paper/3b6c891fbccaa564ea4fd8914a5e3952fcf42ee3",
            "title": "Robust Deep Reinforcement Learning with Adversarial Attacks",
            "venue": "Adaptive Agents and Multi-Agent Systems",
            "publicationVenue": {
                "id": "urn:research:6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                "name": "Adaptive Agents and Multi-Agent Systems",
                "alternate_names": [
                    "Adapt Agent Multi-agent Syst",
                    "International Joint Conference on Autonomous Agents & Multiagent Systems",
                    "Adapt Agent Multi-agents Syst",
                    "AAMAS",
                    "Adaptive Agents and Multi-Agents Systems",
                    "Int Jt Conf Auton Agent  Multiagent Syst"
                ],
                "issn": null,
                "url": "http://www.ifaamas.org/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2773525213",
                "DBLP": "conf/atal/PattanaikTLBC18",
                "ArXiv": "1712.03632",
                "CorpusId": 34383906
            },
            "abstract": "This paper proposes adversarial attacks for Reinforcement Learning (RL) and then improves the robustness of Deep Reinforcement Learning algorithms (DRL) to parameter uncertainties with the help of these attacks. We show that even a naively engineered attack successfully degrades the performance of DRL algorithm. We further improve the attack using gradient information of an engineered loss function which leads to further degradation in performance. These attacks are then leveraged during training to improve the robustness of RL within robust control framework. We show that this adversarial training of DRL algorithms like Deep Double Q learning and Deep Deterministic Policy Gradients leads to significant increase in robustness to parameter variations for RL benchmarks such as Cart-pole, Mountain Car, Hopper and Half Cheetah environment.",
            "referenceCount": 25,
            "citationCount": 239,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-12-11",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Pattanaik2017RobustDR,\n author = {Anay Pattanaik and Zhenyi Tang and Shuijing Liu and Gautham Bommannan and Girish V. Chowdhary},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {2040-2042},\n title = {Robust Deep Reinforcement Learning with Adversarial Attacks},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d0edf834c1d86aac1835e23c11b23b8723b65136",
            "@type": "ScholarlyArticle",
            "paperId": "d0edf834c1d86aac1835e23c11b23b8723b65136",
            "corpusId": 22272492,
            "url": "https://www.semanticscholar.org/paper/d0edf834c1d86aac1835e23c11b23b8723b65136",
            "title": "Learning how to Active Learn: A Deep Reinforcement Learning Approach",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1708.02383",
                "ACL": "D17-1063",
                "MAG": "2952741193",
                "DBLP": "journals/corr/abs-1708-02383",
                "DOI": "10.18653/v1/D17-1063",
                "CorpusId": 22272492
            },
            "abstract": "Active learning aims to select a small subset of data for annotation such that a classifier learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic. Importantly, our method allows the selection policy learned using simulation to one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning algorithms.",
            "referenceCount": 36,
            "citationCount": 230,
            "influentialCitationCount": 24,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D17-1063.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-08-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1708.02383"
            },
            "citationStyles": {
                "bibtex": "@Article{Fang2017LearningHT,\n author = {Meng Fang and Yuan Li and Trevor Cohn},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Learning how to Active Learn: A Deep Reinforcement Learning Approach},\n volume = {abs/1708.02383},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8423cc50c18d68f797adaa4f571f5e4efbe325a5",
            "@type": "ScholarlyArticle",
            "paperId": "8423cc50c18d68f797adaa4f571f5e4efbe325a5",
            "corpusId": 7795421,
            "url": "https://www.semanticscholar.org/paper/8423cc50c18d68f797adaa4f571f5e4efbe325a5",
            "title": "A Laplacian Framework for Option Discovery in Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1703.00956",
                "DBLP": "journals/corr/MachadoBB17",
                "MAG": "2950040888",
                "CorpusId": 7795421
            },
            "abstract": "Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs. In this paper we address the option discovery problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are discovered without taking the environment's rewards into consideration. Moreover, different options act at different time scales, making them helpful for exploration. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.",
            "referenceCount": 43,
            "citationCount": 232,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Machado2017ALF,\n author = {Marlos C. Machado and Marc G. Bellemare and Michael Bowling},\n booktitle = {International Conference on Machine Learning},\n pages = {2295-2304},\n title = {A Laplacian Framework for Option Discovery in Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1bd66197d692581f4ac507107184ca9110b61d7d",
            "@type": "ScholarlyArticle",
            "paperId": "1bd66197d692581f4ac507107184ca9110b61d7d",
            "corpusId": 44179095,
            "url": "https://www.semanticscholar.org/paper/1bd66197d692581f4ac507107184ca9110b61d7d",
            "title": "SBEED: Convergent Reinforcement Learning with Nonlinear Function Approximation",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2806985155",
                "DBLP": "conf/icml/DaiS0XHLCS18",
                "CorpusId": 44179095
            },
            "abstract": "When function approximation is used, solving the Bellman optimality equation with stability guarantees has remained a major open problem in reinforcement learning for decades. The fundamental difficulty is that the Bellman operator may become an expansion in general, resulting in oscillating and even divergent behavior of popular algorithms like Q-learning. In this paper, we revisit the Bellman equation, and reformulate it into a novel primal-dual optimization problem using Nesterov\u2019s smoothing technique and the Legendre-Fenchel transformation. We then develop a new algorithm, called Smoothed Bellman Error Embedding, to solve this optimization problem where any differentiable function class may be used. We provide what we believe to be the first convergence guarantee for general nonlinear function approximation, and analyze the algorithm\u2019s sample complexity. Empirically, our algorithm compares favorably to state-of-the-art baselines in several benchmark control problems.",
            "referenceCount": 64,
            "citationCount": 240,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-12-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dai2017SBEEDCR,\n author = {Bo Dai and Albert Eaton Shaw and Lihong Li and Lin Xiao and Niao He and Z. Liu and Jianshu Chen and Le Song},\n booktitle = {International Conference on Machine Learning},\n pages = {1133-1142},\n title = {SBEED: Convergent Reinforcement Learning with Nonlinear Function Approximation},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:84082634110fcedaaa32632f6cc16a034eedb2a0",
            "@type": "ScholarlyArticle",
            "paperId": "84082634110fcedaaa32632f6cc16a034eedb2a0",
            "corpusId": 703818,
            "url": "https://www.semanticscholar.org/paper/84082634110fcedaaa32632f6cc16a034eedb2a0",
            "title": "A Survey of Preference-Based Reinforcement Learning Methods",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2785324569",
                "DBLP": "journals/jmlr/WirthANF17",
                "CorpusId": 703818
            },
            "abstract": "Reinforcement learning (RL) techniques optimize the accumulated long-term reward of a suitably chosen reward function. However, designing such a reward function often requires a lot of task-specific prior knowledge. The designer needs to consider different objectives that do not only influence the learned behavior but also the learning progress. To alleviate these issues, preference-based reinforcement learning algorithms (PbRL) have been proposed that can directly learn from an expert's preferences instead of a hand-designed numeric reward. PbRL has gained traction in recent years due to its ability to resolve the reward shaping problem, its ability to learn from non numeric rewards and the possibility to reduce the dependence on expert knowledge. We provide a unified framework for PbRL that describes the task formally and points out the different design principles that affect the evaluation task for the human as well as the computational complexity. The design principles include the type of feedback that is assumed, the representation that is learned to capture the preferences, the optimization problem that has to be solved as well as how the exploration/exploitation problem is tackled. Furthermore, we point out shortcomings of current algorithms, propose open research questions and briefly survey practical tasks that have been solved using PbRL.",
            "referenceCount": 110,
            "citationCount": 229,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "18"
            },
            "citationStyles": {
                "bibtex": "@Article{Wirth2017ASO,\n author = {Christian Wirth and R. Akrour and G. Neumann and Johannes F\u00fcrnkranz},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {136:1-136:46},\n title = {A Survey of Preference-Based Reinforcement Learning Methods},\n volume = {18},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:30834ae1497c35d362eea14857d93c28d2d12b57",
            "@type": "ScholarlyArticle",
            "paperId": "30834ae1497c35d362eea14857d93c28d2d12b57",
            "corpusId": 11974467,
            "url": "https://www.semanticscholar.org/paper/30834ae1497c35d362eea14857d93c28d2d12b57",
            "title": "Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2620290674",
                "ArXiv": "1706.05064",
                "DBLP": "conf/icml/OhSLK17",
                "CorpusId": 11974467
            },
            "abstract": "As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.",
            "referenceCount": 49,
            "citationCount": 240,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-06-15",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Oh2017ZeroShotTG,\n author = {Junhyuk Oh and Satinder Singh and Honglak Lee and Pushmeet Kohli},\n booktitle = {International Conference on Machine Learning},\n pages = {2661-2670},\n title = {Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1d65848c563b2c3a7f0153551c1b39e0e5c2d776",
            "@type": "ScholarlyArticle",
            "paperId": "1d65848c563b2c3a7f0153551c1b39e0e5c2d776",
            "corpusId": 1562290,
            "url": "https://www.semanticscholar.org/paper/1d65848c563b2c3a7f0153551c1b39e0e5c2d776",
            "title": "Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks",
            "venue": "IAPR International Conference on Machine Learning and Data Mining in Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:c22cbb06-d23e-491c-886e-7f7d402229ea",
                "name": "IAPR International Conference on Machine Learning and Data Mining in Pattern Recognition",
                "alternate_names": [
                    "IAPR Int Conf Mach Learn Data Min Pattern Recognit",
                    "Machine Learning and Data Mining in Pattern Recognition",
                    "Mach Learn Data Min Pattern Recognit",
                    "MLDM"
                ],
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1701.04143",
                "MAG": "2572659264",
                "DBLP": "conf/mldm/BehzadanM17",
                "DOI": "10.1007/978-3-319-62416-7_19",
                "CorpusId": 1562290
            },
            "abstract": null,
            "referenceCount": 23,
            "citationCount": 236,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1701.04143",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-01-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Behzadan2017VulnerabilityOD,\n author = {Vahid Behzadan and Arslan Munir},\n booktitle = {IAPR International Conference on Machine Learning and Data Mining in Pattern Recognition},\n pages = {262-275},\n title = {Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f2c20cb6ebd2ad704c5bcae4eb8b942d3c62f8e0",
            "@type": "ScholarlyArticle",
            "paperId": "f2c20cb6ebd2ad704c5bcae4eb8b942d3c62f8e0",
            "corpusId": 5349381,
            "url": "https://www.semanticscholar.org/paper/f2c20cb6ebd2ad704c5bcae4eb8b942d3c62f8e0",
            "title": "Uncertainty-Aware Reinforcement Learning for Collision Avoidance",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/KahnVPAL17",
                "MAG": "2586067474",
                "ArXiv": "1702.01182",
                "CorpusId": 5349381
            },
            "abstract": "Reinforcement learning can enable complex, adaptive behavior to be learned automatically for autonomous robotic platforms. However, practical deployment of reinforcement learning methods must contend with the fact that the training process itself can be unsafe for the robot. In this paper, we consider the specific case of a mobile robot learning to navigate an a priori unknown environment while avoiding collisions. In order to learn collision avoidance, the robot must experience collisions at training time. However, high-speed collisions, even at training time, could damage the robot. A successful learning method must therefore proceed cautiously, experiencing only low-speed collisions until it gains confidence. To this end, we present an uncertainty-aware model-based learning algorithm that estimates the probability of collision together with a statistical estimate of uncertainty. By formulating an uncertainty-dependent cost function, we show that the algorithm naturally chooses to proceed cautiously in unfamiliar environments, and increases the velocity of the robot in settings where it has high confidence. Our predictive model is based on bootstrapped neural networks using dropout, allowing it to process raw sensory inputs from high-bandwidth sensors such as cameras. Our experimental evaluation demonstrates that our method effectively minimizes dangerous collisions at training time in an obstacle avoidance task for a simulated and real-world quadrotor, and a real-world RC car. Videos of the experiments can be found at this https URL",
            "referenceCount": 38,
            "citationCount": 274,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-02-03",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1702.01182"
            },
            "citationStyles": {
                "bibtex": "@Article{Kahn2017UncertaintyAwareRL,\n author = {G. Kahn and A. Villaflor and Vitchyr H. Pong and P. Abbeel and S. Levine},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Uncertainty-Aware Reinforcement Learning for Collision Avoidance},\n volume = {abs/1702.01182},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ef45dc196af747363b610e7e16f9f555ad7915af",
            "@type": "ScholarlyArticle",
            "paperId": "ef45dc196af747363b610e7e16f9f555ad7915af",
            "corpusId": 55212377,
            "url": "https://www.semanticscholar.org/paper/ef45dc196af747363b610e7e16f9f555ad7915af",
            "title": "Reinforcement Learning in Different Phases of Quantum Control",
            "venue": "Physical Review X",
            "publicationVenue": {
                "id": "urn:research:98eedf55-1e67-4c3d-a25d-79861b87ae04",
                "name": "Physical Review X",
                "alternate_names": [
                    "Phys Rev X"
                ],
                "issn": "2160-3308",
                "url": "https://journals.aps.org/prx/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "3101119258",
                "ArXiv": "1705.00565",
                "DOI": "10.1103/PhysRevX.8.031086",
                "CorpusId": 55212377
            },
            "abstract": "The ability to prepare a physical system in a desired quantum state is central to many areas of physics such as nuclear magnetic resonance, cold atoms, and quantum computing. Yet, preparing states quickly and with high fidelity remains a formidable challenge. In this work we implement cutting-edge Reinforcement Learning (RL) techniques and show that their performance is comparable to optimal control methods in the task of finding short, high-fidelity driving protocol from an initial to a target state in non-integrable many-body quantum systems of interacting qubits. RL methods learn about the underlying physical system solely through a single scalar reward (the fidelity of the resulting state) calculated from numerical simulations of the physical system. We further show that quantum state manipulation, viewed as an optimization problem, exhibits a spin-glass-like phase transition in the space of protocols as a function of the protocol duration. Our RL-aided approach helps identify variational protocols with nearly optimal fidelity, even in the glassy phase, where optimal state manipulation is exponentially hard. This study highlights the potential usefulness of RL for applications in out-of-equilibrium quantum physics.",
            "referenceCount": 96,
            "citationCount": 259,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://link.aps.org/pdf/10.1103/PhysRevX.8.031086",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2017-05-01",
            "journal": {
                "name": "Physical Review X",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bukov2017ReinforcementLI,\n author = {Marin Bukov and A. G. Day and Dries Sels and P. Weinberg and A. Polkovnikov and P. Mehta},\n booktitle = {Physical Review X},\n journal = {Physical Review X},\n title = {Reinforcement Learning in Different Phases of Quantum Control},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9ea92ebeb7462f2db346cfa3281ad7497b1063d6",
            "@type": "ScholarlyArticle",
            "paperId": "9ea92ebeb7462f2db346cfa3281ad7497b1063d6",
            "corpusId": 59233950,
            "url": "https://www.semanticscholar.org/paper/9ea92ebeb7462f2db346cfa3281ad7497b1063d6",
            "title": "Deep reinforcement learning with relational inductive biases",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/iclr/ZambaldiRSBLBTR19",
                "MAG": "2907502844",
                "CorpusId": 59233950
            },
            "abstract": ",",
            "referenceCount": 48,
            "citationCount": 168,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-27",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Zambaldi2018DeepRL,\n author = {V. Zambaldi and David Raposo and Adam Santoro and V. Bapst and Yujia Li and Igor Babuschkin and K. Tuyls and David P. Reichert and T. Lillicrap and Edward Lockhart and M. Shanahan and Victoria Langston and Razvan Pascanu and M. Botvinick and Oriol Vinyals and P. Battaglia},\n booktitle = {International Conference on Learning Representations},\n title = {Deep reinforcement learning with relational inductive biases},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3ee01ec27e4e66e089b72a9989724be611c2ad90",
            "@type": "ScholarlyArticle",
            "paperId": "3ee01ec27e4e66e089b72a9989724be611c2ad90",
            "corpusId": 534043,
            "url": "https://www.semanticscholar.org/paper/3ee01ec27e4e66e089b72a9989724be611c2ad90",
            "title": "Neural Map: Structured Memory for Deep Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2594903727",
                "DBLP": "conf/iclr/ParisottoS18",
                "ArXiv": "1702.08360",
                "CorpusId": 534043
            },
            "abstract": "A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training.",
            "referenceCount": 23,
            "citationCount": 233,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-02-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1702.08360"
            },
            "citationStyles": {
                "bibtex": "@Article{Parisotto2017NeuralMS,\n author = {Emilio Parisotto and R. Salakhutdinov},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Neural Map: Structured Memory for Deep Reinforcement Learning},\n volume = {abs/1702.08360},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3c63f8b8263cd6cc4c8c7429d46bb656accddc49",
            "@type": "ScholarlyArticle",
            "paperId": "3c63f8b8263cd6cc4c8c7429d46bb656accddc49",
            "corpusId": 2294751,
            "url": "https://www.semanticscholar.org/paper/3c63f8b8263cd6cc4c8c7429d46bb656accddc49",
            "title": "Hybrid Reward Architecture for Reinforcement Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2624731731",
                "DBLP": "journals/corr/SeijenFRLBT17",
                "ArXiv": "1706.04208",
                "CorpusId": 2294751
            },
            "abstract": "One of the main challenges in reinforcement learning (RL) is generalisation. In typical deep RL methods this is achieved by approximating the optimal value function with a low-dimensional representation using a deep network. While this approach works well in many domains, in domains where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable. This paper contributes towards tackling such challenging domains, by proposing a new method, called Hybrid Reward Architecture (HRA). HRA takes as input a decomposed reward function and learns a separate value function for each component reward function. Because each component typically only depends on a subset of all features, the corresponding value function can be approximated more easily by a low-dimensional representation, enabling more effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human performance.",
            "referenceCount": 32,
            "citationCount": 215,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-06-13",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1706.04208"
            },
            "citationStyles": {
                "bibtex": "@Article{Seijen2017HybridRA,\n author = {H. V. Seijen and Mehdi Fatemi and R. Laroche and Joshua Romoff and Tavian Barnes and Jeffrey Tsang},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Hybrid Reward Architecture for Reinforcement Learning},\n volume = {abs/1706.04208},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7f710932a0aa99f2c2ddbec6e7765f10c48d3fc2",
            "@type": "ScholarlyArticle",
            "paperId": "7f710932a0aa99f2c2ddbec6e7765f10c48d3fc2",
            "corpusId": 6587686,
            "url": "https://www.semanticscholar.org/paper/7f710932a0aa99f2c2ddbec6e7765f10c48d3fc2",
            "title": "Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1703.02949",
                "MAG": "2605368761",
                "DBLP": "journals/corr/GuptaDLAL17",
                "CorpusId": 6587686
            },
            "abstract": "People can learn a wide range of tasks from their own experience, but can also learn from observing other creatures. This can accelerate acquisition of new skills even when the observed agent differs substantially from the learning agent in terms of morphology. In this paper, we examine how reinforcement learning algorithms can transfer knowledge between morphologically different agents (e.g., different robots). We introduce a problem formulation where two agents are tasked with learning multiple skills by sharing information. Our method uses the skills that were learned by both agents to train invariant feature spaces that can then be used to transfer other skills from one agent to another. The process of learning these invariant feature spaces can be viewed as a kind of \"analogy making\", or implicit learning of partial correspondences between two distinct domains. We evaluate our transfer learning algorithm in two simulated robotic manipulation skills, and illustrate that we can transfer knowledge between simulated robotic arms with different numbers of links, as well as simulated arms with different actuation mechanisms, where one robot is torque-driven while the other is tendon-driven.",
            "referenceCount": 37,
            "citationCount": 217,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-03-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1703.02949"
            },
            "citationStyles": {
                "bibtex": "@Article{Gupta2017LearningIF,\n author = {Abhishek Gupta and Coline Devin and Yuxuan Liu and P. Abbeel and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning},\n volume = {abs/1703.02949},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:62c16289e72ffecfa68e8b3d8f42f2e1eb9be25b",
            "@type": "ScholarlyArticle",
            "paperId": "62c16289e72ffecfa68e8b3d8f42f2e1eb9be25b",
            "corpusId": 25163644,
            "url": "https://www.semanticscholar.org/paper/62c16289e72ffecfa68e8b3d8f42f2e1eb9be25b",
            "title": "A Deep Reinforcement Learning Chatbot",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1709.02349",
                "MAG": "2751124354",
                "DBLP": "journals/corr/abs-1709-02349",
                "CorpusId": 25163644
            },
            "abstract": "We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than many competing systems. Due to its machine learning architecture, the system is likely to improve with additional data.",
            "referenceCount": 86,
            "citationCount": 216,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-09-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1709.02349"
            },
            "citationStyles": {
                "bibtex": "@Article{Serban2017ADR,\n author = {Iulian Serban and Chinnadhurai Sankar and M. Germain and Saizheng Zhang and Zhouhan Lin and Sandeep Subramanian and Taesup Kim and Michael Pieper and A. Chandar and Nan Rosemary Ke and Sai Mudumba and A. D. Br\u00e9bisson and Jose M. R. Sotelo and Dendi Suhubdy and Vincent Michalski and A. Nguyen and Joelle Pineau and Yoshua Bengio},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Deep Reinforcement Learning Chatbot},\n volume = {abs/1709.02349},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9da4df00780136ea1b9d0885b85f1fd8dfe874fc",
            "@type": "ScholarlyArticle",
            "paperId": "9da4df00780136ea1b9d0885b85f1fd8dfe874fc",
            "corpusId": 3482957,
            "url": "https://www.semanticscholar.org/paper/9da4df00780136ea1b9d0885b85f1fd8dfe874fc",
            "title": "Deep-Reinforcement-Learning-Based Optimization for Cache-Enabled Opportunistic Interference Alignment Wireless Networks",
            "venue": "IEEE Transactions on Vehicular Technology",
            "publicationVenue": {
                "id": "urn:research:983b0731-eddf-4f05-9c9b-81059a9f9c51",
                "name": "IEEE Transactions on Vehicular Technology",
                "alternate_names": [
                    "IEEE Trans Veh Technol"
                ],
                "issn": "0018-9545",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=25"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2756144809",
                "DBLP": "journals/tvt/HeZYZYLZ17",
                "DOI": "10.1109/TVT.2017.2751641",
                "CorpusId": 3482957
            },
            "abstract": "Both caching and interference alignment (IA) are promising techniques for next-generation wireless networks. Nevertheless, most of the existing works on cache-enabled IA wireless networks assume that the channel is invariant, which is unrealistic considering the time-varying nature of practical wireless environments. In this paper, we consider realistic time-varying channels. Specifically, the channel is formulated as a finite-state Markov channel (FSMC). The complexity of the system is very high when we consider realistic FSMC models. Therefore, in this paper, we propose a novel deep reinforcement learning approach, which is an advanced reinforcement learning algorithm that uses a deep <inline-formula><tex-math notation=\"LaTeX\">$Q$</tex-math></inline-formula> network to approximate the <inline-formula><tex-math notation=\"LaTeX\">$Q$</tex-math></inline-formula> value-action function. We use Google TensorFlow to implement deep reinforcement learning in this paper to obtain the optimal IA user selection policy in cache-enabled opportunistic IA wireless networks. Simulation results are presented to show that the performance of cache-enabled opportunistic IA networks in terms of the network's sum rate and energy efficiency can be significantly improved by using the proposed approach.",
            "referenceCount": 38,
            "citationCount": 226,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-09-13",
            "journal": {
                "name": "IEEE Transactions on Vehicular Technology",
                "volume": "66"
            },
            "citationStyles": {
                "bibtex": "@Article{He2017DeepReinforcementLearningBasedOF,\n author = {Ying He and Zheng Zhang and F. Yu and Nan Zhao and Hongxi Yin and Victor C. M. Leung and Yanhua Zhang},\n booktitle = {IEEE Transactions on Vehicular Technology},\n journal = {IEEE Transactions on Vehicular Technology},\n pages = {10433-10445},\n title = {Deep-Reinforcement-Learning-Based Optimization for Cache-Enabled Opportunistic Interference Alignment Wireless Networks},\n volume = {66},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bc9f3c466c6f6b386f4ef1195853d498cf3c182e",
            "@type": "ScholarlyArticle",
            "paperId": "bc9f3c466c6f6b386f4ef1195853d498cf3c182e",
            "corpusId": 10458880,
            "url": "https://www.semanticscholar.org/paper/bc9f3c466c6f6b386f4ef1195853d498cf3c182e",
            "title": "Mapping Instructions and Visual Observations to Actions with Reinforcement Learning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/MisraLA17",
                "ArXiv": "1704.08795",
                "ACL": "D17-1106",
                "MAG": "2952331309",
                "DOI": "10.18653/v1/D17-1106",
                "CorpusId": 10458880
            },
            "abstract": "We propose to directly map raw visual observations and text input to actions for instruction execution. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single model to jointly reason about linguistic and visual input. We use reinforcement learning in a contextual bandit setting to train a neural network agent. To guide the agent\u2019s exploration, we use reward shaping with different forms of supervision. Our approach does not require intermediate representations, planning procedures, or training different models. We evaluate in a simulated environment, and show significant improvements over supervised learning and common reinforcement learning variants.",
            "referenceCount": 65,
            "citationCount": 215,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D17-1106.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-04-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Misra2017MappingIA,\n author = {Dipendra Kumar Misra and J. Langford and Yoav Artzi},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {1004-1015},\n title = {Mapping Instructions and Visual Observations to Actions with Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bfbd10ebffc9494423770a5bd30ebd0f9cbce66d",
            "@type": "ScholarlyArticle",
            "paperId": "bfbd10ebffc9494423770a5bd30ebd0f9cbce66d",
            "corpusId": 3635282,
            "url": "https://www.semanticscholar.org/paper/bfbd10ebffc9494423770a5bd30ebd0f9cbce66d",
            "title": "Device Placement Optimization with Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2617411258",
                "DBLP": "journals/corr/MirhoseiniPLSLZ17",
                "ArXiv": "1706.04972",
                "CorpusId": 3635282
            },
            "abstract": "The past few years have witnessed a growth in size and computational requirements for training and inference with neural networks. Currently, a common approach to address these requirements is to use a heterogeneous distributed environment with a mixture of hardware devices such as CPUs and GPUs. Importantly, the decision of placing parts of the neural models on devices is often made by human experts based on simple heuristics and intuitions. In this paper, we propose a method which learns to optimize device placement for TensorFlow computational graphs. Key to our method is the use of a sequence-to-sequence model to predict which subsets of operations in a TensorFlow graph should run on which of the available devices. The execution time of the predicted placements is then used as the reward signal to optimize the parameters of the sequence-to-sequence model. Our main result is that on Inception-V3 for ImageNet classification, and on RNN LSTM, for language modeling and neural machine translation, our model finds non-trivial device placements that outperform hand-crafted heuristics and traditional algorithmic methods.",
            "referenceCount": 50,
            "citationCount": 380,
            "influentialCitationCount": 32,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-06-13",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Mirhoseini2017DevicePO,\n author = {Azalia Mirhoseini and Hieu Pham and Quoc V. Le and Benoit Steiner and Rasmus Larsen and Yuefeng Zhou and Naveen Kumar and Mohammad Norouzi and Samy Bengio and J. Dean},\n booktitle = {International Conference on Machine Learning},\n pages = {2430-2439},\n title = {Device Placement Optimization with Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:017b07fe36e8d43965b2125f6170a97c9d747fca",
            "@type": "ScholarlyArticle",
            "paperId": "017b07fe36e8d43965b2125f6170a97c9d747fca",
            "corpusId": 13268930,
            "url": "https://www.semanticscholar.org/paper/017b07fe36e8d43965b2125f6170a97c9d747fca",
            "title": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2610395436",
                "DBLP": "journals/corr/PopovHLHBVLTER17",
                "ArXiv": "1704.03073",
                "CorpusId": 13268930
            },
            "abstract": "Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.",
            "referenceCount": 42,
            "citationCount": 232,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-04-10",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1704.03073"
            },
            "citationStyles": {
                "bibtex": "@Article{Popov2017DataefficientDR,\n author = {I. Popov and N. Heess and T. Lillicrap and Roland Hafner and Gabriel Barth-Maron and Matej Vecer\u00edk and Thomas Lampe and Yuval Tassa and Tom Erez and Martin A. Riedmiller},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Data-efficient Deep Reinforcement Learning for Dexterous Manipulation},\n volume = {abs/1704.03073},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b2961394ecb6051ffe1847fcc9e5eb7685d3ec6d",
            "@type": "ScholarlyArticle",
            "paperId": "b2961394ecb6051ffe1847fcc9e5eb7685d3ec6d",
            "corpusId": 257767360,
            "url": "https://www.semanticscholar.org/paper/b2961394ecb6051ffe1847fcc9e5eb7685d3ec6d",
            "title": "Inverse Reinforcement Learning without Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2023,
            "externalIds": {
                "DBLP": "conf/icml/SwamyWCBW23",
                "ArXiv": "2303.14623",
                "DOI": "10.48550/arXiv.2303.14623",
                "CorpusId": 257767360
            },
            "abstract": "Inverse Reinforcement Learning (IRL) is a powerful set of techniques for imitation learning that aims to learn a reward function that rationalizes expert demonstrations. Unfortunately, traditional IRL methods suffer from a computational weakness: they require repeatedly solving a hard reinforcement learning (RL) problem as a subroutine. This is counter-intuitive from the viewpoint of reductions: we have reduced the easier problem of imitation learning to repeatedly solving the harder problem of RL. Another thread of work has proved that access to the side-information of the distribution of states where a strong policy spends time can dramatically reduce the sample and computational complexities of solving an RL problem. In this work, we demonstrate for the first time a more informed imitation learning reduction where we utilize the state distribution of the expert to alleviate the global exploration component of the RL subroutine, providing an exponential speedup in theory. In practice, we find that we are able to significantly speed up the prior art on continuous control tasks.",
            "referenceCount": 55,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2303.14623",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2023-03-26",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2303.14623"
            },
            "citationStyles": {
                "bibtex": "@Article{Swamy2023InverseRL,\n author = {Gokul Swamy and David J. Wu and Sanjiban Choudhury and J. Bagnell and Zhiwei Steven Wu},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Inverse Reinforcement Learning without Reinforcement Learning},\n volume = {abs/2303.14623},\n year = {2023}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a1d2a7ef81960846b9cec00bce8eefa06ccc8796",
            "@type": "ScholarlyArticle",
            "paperId": "a1d2a7ef81960846b9cec00bce8eefa06ccc8796",
            "corpusId": 12936702,
            "url": "https://www.semanticscholar.org/paper/a1d2a7ef81960846b9cec00bce8eefa06ccc8796",
            "title": "Deep Reinforcement Learning from Self-Play in Imperfect-Information Games",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2964164283",
                "DBLP": "journals/corr/HeinrichS16",
                "ArXiv": "1603.01121",
                "CorpusId": 12936702
            },
            "abstract": "Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge. Our method combines fictitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a strategy that approached the performance of state-of-the-art, superhuman algorithms based on significant domain expertise.",
            "referenceCount": 120,
            "citationCount": 317,
            "influentialCitationCount": 44,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-03-03",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1603.01121"
            },
            "citationStyles": {
                "bibtex": "@Article{Heinrich2016DeepRL,\n author = {Johannes Heinrich and David Silver},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Deep Reinforcement Learning from Self-Play in Imperfect-Information Games},\n volume = {abs/1603.01121},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4ab08b2f1193d770c241b41f5d9f1c841a3663d3",
            "@type": "ScholarlyArticle",
            "paperId": "4ab08b2f1193d770c241b41f5d9f1c841a3663d3",
            "corpusId": 1341709,
            "url": "https://www.semanticscholar.org/paper/4ab08b2f1193d770c241b41f5d9f1c841a3663d3",
            "title": "Optimizing Chemical Reactions with Deep Reinforcement Learning",
            "venue": "ACS Central Science",
            "publicationVenue": {
                "id": "urn:research:df882f0f-d88c-4139-8462-219dcb05d97c",
                "name": "ACS Central Science",
                "alternate_names": [
                    "AC Central Sci",
                    "ACS central science",
                    "AC central sci"
                ],
                "issn": "2374-7943",
                "url": "https://pubs.acs.org/journal/acscii"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2774977638",
                "PubMedCentral": "5746857",
                "DOI": "10.1021/acscentsci.7b00492",
                "CorpusId": 1341709,
                "PubMed": "29296675"
            },
            "abstract": "Deep reinforcement learning was employed to optimize chemical reactions. Our model iteratively records the results of a chemical reaction and chooses new experimental conditions to improve the reaction outcome. This model outperformed a state-of-the-art blackbox optimization algorithm by using 71% fewer steps on both simulations and real reactions. Furthermore, we introduced an efficient exploration strategy by drawing the reaction conditions from certain probability distributions, which resulted in an improvement on regret from 0.062 to 0.039 compared with a deterministic policy. Combining the efficient exploration policy with accelerated microdroplet reactions, optimal reaction conditions were determined in 30 min for the four reactions considered, and a better understanding of the factors that control microdroplet reactions was reached. Moreover, our model showed a better performance after training on reactions with similar or even dissimilar underlying mechanisms, which demonstrates its learning ability.",
            "referenceCount": 42,
            "citationCount": 303,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://pubs.acs.org/doi/pdf/10.1021/acscentsci.7b00492",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-12-15",
            "journal": {
                "name": "ACS Central Science",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhou2017OptimizingCR,\n author = {Zhenpeng Zhou and Xiaocheng Li and R. Zare},\n booktitle = {ACS Central Science},\n journal = {ACS Central Science},\n pages = {1337 - 1344},\n title = {Optimizing Chemical Reactions with Deep Reinforcement Learning},\n volume = {3},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b227f3e4c0dc96e5ac5426b85485a70f2175a205",
            "@type": "ScholarlyArticle",
            "paperId": "b227f3e4c0dc96e5ac5426b85485a70f2175a205",
            "corpusId": 49670925,
            "url": "https://www.semanticscholar.org/paper/b227f3e4c0dc96e5ac5426b85485a70f2175a205",
            "title": "Representation Learning with Contrastive Predictive Coding",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1807-03748",
                "ArXiv": "1807.03748",
                "MAG": "2842511635",
                "CorpusId": 49670925
            },
            "abstract": "While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
            "referenceCount": 57,
            "citationCount": 6619,
            "influentialCitationCount": 1116,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-07-10",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1807.03748"
            },
            "citationStyles": {
                "bibtex": "@Article{Oord2018RepresentationLW,\n author = {A\u00e4ron van den Oord and Yazhe Li and Oriol Vinyals},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Representation Learning with Contrastive Predictive Coding},\n volume = {abs/1807.03748},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d6757aedcb53142bc439ec64bfd0b056d99b1881",
            "@type": "ScholarlyArticle",
            "paperId": "d6757aedcb53142bc439ec64bfd0b056d99b1881",
            "corpusId": 7782433,
            "url": "https://www.semanticscholar.org/paper/d6757aedcb53142bc439ec64bfd0b056d99b1881",
            "title": "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2593766708",
                "ArXiv": "1703.01732",
                "DBLP": "journals/corr/AchiamS17",
                "CorpusId": 7782433
            },
            "abstract": "Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as $\\epsilon$-greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent's surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the $k$-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.",
            "referenceCount": 30,
            "citationCount": 209,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-03-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1703.01732"
            },
            "citationStyles": {
                "bibtex": "@Article{Achiam2017SurpriseBasedIM,\n author = {Joshua Achiam and S. Sastry},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning},\n volume = {abs/1703.01732},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2fdb536da39a014c598ea67b0db88431fcd852a8",
            "@type": "ScholarlyArticle",
            "paperId": "2fdb536da39a014c598ea67b0db88431fcd852a8",
            "corpusId": 5806691,
            "url": "https://www.semanticscholar.org/paper/2fdb536da39a014c598ea67b0db88431fcd852a8",
            "title": "Doubly Robust Off-policy Value Evaluation for Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2952690945",
                "DBLP": "conf/icml/JiangL16",
                "ArXiv": "1511.03722",
                "CorpusId": 5806691
            },
            "abstract": "We study the problem of off-policy value evaluation in reinforcement learning (RL), where one aims to estimate the value of a new policy based on data collected by a different policy. This problem is often a critical step when applying RL in real-world problems. Despite its importance, existing general methods either have uncontrolled bias or suffer high variance. In this work, we extend the doubly robust estimator for bandits to sequential decision-making problems, which gets the best of both worlds: it is guaranteed to be unbiased and can have a much lower variance than the popular importance sampling estimators. We demonstrate the estimator's accuracy in several benchmark problems, and illustrate its use as a subroutine in safe policy improvement. We also provide theoretical results on the hardness of the problem, and show that our estimator can match the lower bound in certain scenarios.",
            "referenceCount": 44,
            "citationCount": 512,
            "influentialCitationCount": 66,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-11-11",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Jiang2015DoublyRO,\n author = {Nan Jiang and Lihong Li},\n booktitle = {International Conference on Machine Learning},\n pages = {652-661},\n title = {Doubly Robust Off-policy Value Evaluation for Reinforcement Learning},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fd35994fe82682594864d4b317b202ff1b7d7641",
            "@type": "ScholarlyArticle",
            "paperId": "fd35994fe82682594864d4b317b202ff1b7d7641",
            "corpusId": 3560433,
            "url": "https://www.semanticscholar.org/paper/fd35994fe82682594864d4b317b202ff1b7d7641",
            "title": "Navigating Intersections with Autonomous Vehicles using Deep Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/IseleCSF17",
                "MAG": "2611629860",
                "ArXiv": "1705.01196",
                "CorpusId": 3560433
            },
            "abstract": "Providing an efficient strategy to navigate safely through unsignaled intersections is a difficult task that requires determining the intent of other drivers. We explore the effectiveness of Deep Reinforcement Learning to handle intersection problems. Using recent advances in Deep RL, we are able to learn policies that surpass the performance of a commonly-used heuristic approach in several metrics including task completion time and goal success rate and have limited ability to generalize. We then explore a system's ability to learn active sensing behaviors to enable navigating safely in the case of occlusions. Our analysis, provides insight into the intersection handling problem, the solutions learned by the network point out several shortcomings of current rule-based methods, and the failures of our current deep reinforcement learning system point to future research directions.",
            "referenceCount": 42,
            "citationCount": 235,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-05-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1705.01196"
            },
            "citationStyles": {
                "bibtex": "@Article{Isele2017NavigatingIW,\n author = {David Isele and Akansel Cosgun and K. Subramanian and K. Fujimura},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Navigating Intersections with Autonomous Vehicles using Deep Reinforcement Learning},\n volume = {abs/1705.01196},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a3be639d7e915b5f4e1499e52e1fcfd0940a31e5",
            "@type": "ScholarlyArticle",
            "paperId": "a3be639d7e915b5f4e1499e52e1fcfd0940a31e5",
            "corpusId": 21646317,
            "url": "https://www.semanticscholar.org/paper/a3be639d7e915b5f4e1499e52e1fcfd0940a31e5",
            "title": "Paraphrase Generation with Deep Reinforcement Learning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1711.00279",
                "MAG": "2949197847",
                "ACL": "D18-1421",
                "DBLP": "conf/emnlp/LiJSL18",
                "DOI": "10.18653/v1/D18-1421",
                "CorpusId": 21646317
            },
            "abstract": "Automatic generation of paraphrases from a given sentence is an important yet challenging task in natural language processing (NLP). In this paper, we present a deep reinforcement learning approach to paraphrase generation. Specifically, we propose a new framework for the task, which consists of a generator and an evaluator, both of which are learned from data. The generator, built as a sequence-to-sequence learning model, can produce paraphrases given a sentence. The evaluator, constructed as a deep matching model, can judge whether two sentences are paraphrases of each other. The generator is first trained by deep learning and then further fine-tuned by reinforcement learning in which the reward is given by the evaluator. For the learning of the evaluator, we propose two methods based on supervised learning and inverse reinforcement learning respectively, depending on the type of available training data. Experimental results on two datasets demonstrate the proposed models (the generators) can produce more accurate paraphrases and outperform the state-of-the-art methods in paraphrase generation in both automatic evaluation and human evaluation.",
            "referenceCount": 74,
            "citationCount": 191,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D18-1421.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-11-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1711.00279"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2017ParaphraseGW,\n author = {Zichao Li and Xin Jiang and Lifeng Shang and Hang Li},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Paraphrase Generation with Deep Reinforcement Learning},\n volume = {abs/1711.00279},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:314ee6b03f8c6c3533d2107fe05f9909b9202cbd",
            "@type": "ScholarlyArticle",
            "paperId": "314ee6b03f8c6c3533d2107fe05f9909b9202cbd",
            "corpusId": 2012188,
            "url": "https://www.semanticscholar.org/paper/314ee6b03f8c6c3533d2107fe05f9909b9202cbd",
            "title": "Deep Reinforcement Learning for Mention-Ranking Coreference Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1609.08667",
                "ACL": "D16-1245",
                "DBLP": "journals/corr/ClarkM16a",
                "MAG": "2526182867",
                "DOI": "10.18653/v1/D16-1245",
                "CorpusId": 2012188
            },
            "abstract": "Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. We experiment with two approaches: the REINFORCE policy gradient algorithm and a reward-rescaled max-margin objective. We find the latter to be more effective, resulting in significant improvements over the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task.",
            "referenceCount": 29,
            "citationCount": 330,
            "influentialCitationCount": 41,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D16-1245.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-09-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1609.08667"
            },
            "citationStyles": {
                "bibtex": "@Article{Clark2016DeepRL,\n author = {Kevin Clark and Christopher D. Manning},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Deep Reinforcement Learning for Mention-Ranking Coreference Models},\n volume = {abs/1609.08667},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1def5d3711ebd1d86787b1ed57c91832c5ddc90b",
            "@type": "ScholarlyArticle",
            "paperId": "1def5d3711ebd1d86787b1ed57c91832c5ddc90b",
            "corpusId": 8241258,
            "url": "https://www.semanticscholar.org/paper/1def5d3711ebd1d86787b1ed57c91832c5ddc90b",
            "title": "Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2174786457",
                "ArXiv": "1511.06342",
                "DBLP": "journals/corr/ParisottoBS15",
                "CorpusId": 8241258
            },
            "abstract": "The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed \"Actor-Mimic\", exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.",
            "referenceCount": 22,
            "citationCount": 537,
            "influentialCitationCount": 44,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-19",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1511.06342"
            },
            "citationStyles": {
                "bibtex": "@Article{Parisotto2015ActorMimicDM,\n author = {Emilio Parisotto and Jimmy Ba and R. Salakhutdinov},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning},\n volume = {abs/1511.06342},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:753d1f31ef1bdee51e01d33e33e2b153a65ed978",
            "@type": "ScholarlyArticle",
            "paperId": "753d1f31ef1bdee51e01d33e33e2b153a65ed978",
            "corpusId": 9315738,
            "url": "https://www.semanticscholar.org/paper/753d1f31ef1bdee51e01d33e33e2b153a65ed978",
            "title": "The drift diffusion model as the choice rule in reinforcement learning",
            "venue": "Psychonomic Bulletin & Review",
            "publicationVenue": {
                "id": "urn:research:b5fb729c-41d1-4db6-b4f3-4c23f8ba5294",
                "name": "Psychonomic Bulletin & Review",
                "alternate_names": [
                    "Psychon Bull  Rev"
                ],
                "issn": "1069-9384",
                "url": "https://www.springer.com/psychology/cognitive+psychology/journal/13423"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2561424158",
                "DOI": "10.3758/s13423-016-1199-y",
                "CorpusId": 9315738,
                "PubMed": "27966103"
            },
            "abstract": null,
            "referenceCount": 102,
            "citationCount": 163,
            "influentialCitationCount": 20,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.3758/s13423-016-1199-y.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-08-01",
            "journal": {
                "name": "Psychonomic Bulletin & Review",
                "volume": "24"
            },
            "citationStyles": {
                "bibtex": "@Article{Pedersen2017TheDD,\n author = {Mads L. Pedersen and M. Frank and G. Biele},\n booktitle = {Psychonomic Bulletin & Review},\n journal = {Psychonomic Bulletin & Review},\n pages = {1234-1251},\n title = {The drift diffusion model as the choice rule in reinforcement learning},\n volume = {24},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3fefb5c8f618cfa8badea08b81e7d334fcf9c26f",
            "@type": "ScholarlyArticle",
            "paperId": "3fefb5c8f618cfa8badea08b81e7d334fcf9c26f",
            "corpusId": 68014005,
            "url": "https://www.semanticscholar.org/paper/3fefb5c8f618cfa8badea08b81e7d334fcf9c26f",
            "title": "Reinforcement Learning and Deep Reinforcement Learning",
            "venue": "Deep Reinforcement Learning for Wireless Networks",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2909958171",
                "DOI": "10.1007/978-3-030-10546-4_2",
                "CorpusId": 68014005
            },
            "abstract": null,
            "referenceCount": 9,
            "citationCount": 8,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Deep Reinforcement Learning for Wireless Networks",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yu2019ReinforcementLA,\n author = {F. Yu and Ying He},\n booktitle = {Deep Reinforcement Learning for Wireless Networks},\n journal = {Deep Reinforcement Learning for Wireless Networks},\n title = {Reinforcement Learning and Deep Reinforcement Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6ecb8a743f92db6c6b8691ab8e8aebbb06fb1b48",
            "@type": "ScholarlyArticle",
            "paperId": "6ecb8a743f92db6c6b8691ab8e8aebbb06fb1b48",
            "corpusId": 2215254,
            "url": "https://www.semanticscholar.org/paper/6ecb8a743f92db6c6b8691ab8e8aebbb06fb1b48",
            "title": "Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1611.01929",
                "DBLP": "conf/icml/AnschelBS17",
                "MAG": "2953364219",
                "CorpusId": 2215254
            },
            "abstract": "Instability and variability of Deep Reinforcement Learning (DRL) algorithms tend to adversely affect their performance. Averaged-DQN is a simple extension to the DQN algorithm, based on averaging previously learned Q-values estimates, which leads to a more stable training procedure and improved performance by reducing approximation error variance in the target values. To understand the effect of the algorithm, we examine the source of value function estimation errors and provide an analytical comparison within a simplified model. We further present experiments on the Arcade Learning Environment benchmark that demonstrate significantly improved stability and performance due to the proposed extension.",
            "referenceCount": 34,
            "citationCount": 250,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-11-07",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Anschel2016AveragedDQNVR,\n author = {Oron Anschel and Nir Baram and N. Shimkin},\n booktitle = {International Conference on Machine Learning},\n pages = {176-185},\n title = {Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:92d621a603cda8c32214d70953e180fe5a442f3e",
            "@type": "ScholarlyArticle",
            "paperId": "92d621a603cda8c32214d70953e180fe5a442f3e",
            "corpusId": 13352766,
            "url": "https://www.semanticscholar.org/paper/92d621a603cda8c32214d70953e180fe5a442f3e",
            "title": "N2N Learning: Network to Network Compression via Policy Gradient Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/iclr/AshokRBK18",
                "MAG": "2963387524",
                "ArXiv": "1709.06030",
                "CorpusId": 13352766
            },
            "abstract": "While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints. Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics. Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task. In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning. Our approach takes a larger `teacher' network as input and outputs a compressed `student' network derived from the `teacher' network. In the first stage of our method, a recurrent policy network aggressively removes layers from the large `teacher' model. In the second stage, another recurrent policy network carefully reduces the size of each remaining layer. The resulting network is then evaluated to obtain a reward -- a score based on the accuracy and compression of the network. Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network. Our experiments show that we can achieve compression rates of more than 10x for models such as ResNet-34 while maintaining similar performance to the input `teacher' network. We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller `teacher' networks can be used to rapidly speed up training on larger `teacher' networks.",
            "referenceCount": 42,
            "citationCount": 152,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-09-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1709.06030"
            },
            "citationStyles": {
                "bibtex": "@Article{Ashok2017N2NLN,\n author = {A. Ashok and Nicholas Rhinehart and Fares N. Beainy and Kris M. Kitani},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {N2N Learning: Network to Network Compression via Policy Gradient Reinforcement Learning},\n volume = {abs/1709.06030},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3deecaee4ec1a37de3cb10420eaabff067669e17",
            "@type": "ScholarlyArticle",
            "paperId": "3deecaee4ec1a37de3cb10420eaabff067669e17",
            "corpusId": 7774489,
            "url": "https://www.semanticscholar.org/paper/3deecaee4ec1a37de3cb10420eaabff067669e17",
            "title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/FlorensaDA17",
                "MAG": "2606433045",
                "ArXiv": "1704.03012",
                "CorpusId": 7774489
            },
            "abstract": "Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.",
            "referenceCount": 53,
            "citationCount": 326,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1704.03012"
            },
            "citationStyles": {
                "bibtex": "@Article{Florensa2016StochasticNN,\n author = {Carlos Florensa and Yan Duan and P. Abbeel},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Stochastic Neural Networks for Hierarchical Reinforcement Learning},\n volume = {abs/1704.03012},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a7fb199f85943b3fb6b5f7e9f1680b2e2a445cce",
            "@type": "ScholarlyArticle",
            "paperId": "a7fb199f85943b3fb6b5f7e9f1680b2e2a445cce",
            "corpusId": 15254659,
            "url": "https://www.semanticscholar.org/paper/a7fb199f85943b3fb6b5f7e9f1680b2e2a445cce",
            "title": "Learning from Demonstrations for Real World Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/HesterVPLSPSDOA17",
                "ArXiv": "1704.03732",
                "MAG": "2607198029",
                "CorpusId": 15254659
            },
            "abstract": "Deep reinforcement learning (RL) has achieved several high profile successes in difficult decision-making problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages this data to massively accelerate the learning process even from relatively small amounts of demonstration data and is able to automatically assess the necessary ratio of demonstration data while learning thanks to a prioritized replay mechanism. DQfD works by combining temporal difference updates with supervised classification of the demonstrator's actions. We show that DQfD has better initial performance than Prioritized Dueling Double Deep Q-Networks (PDD DQN) as it starts with better scores on the first million steps on 41 of 42 games and on average it takes PDD DQN 82 million steps to catch up to DQfD's performance. DQfD learns to out-perform the best demonstration given in 14 of 42 games. In addition, DQfD leverages human demonstrations to achieve state-of-the-art results for 17 games. Finally, we show that DQfD performs better than three related algorithms for incorporating demonstration data into DQN.",
            "referenceCount": 43,
            "citationCount": 146,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-04-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1704.03732"
            },
            "citationStyles": {
                "bibtex": "@Article{Hester2017LearningFD,\n author = {Todd Hester and Matej Vecer\u00edk and O. Pietquin and Marc Lanctot and T. Schaul and Bilal Piot and A. Sendonaris and Gabriel Dulac-Arnold and Ian Osband and J. Agapiou and Joel Z. Leibo and A. Gruslys},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Learning from Demonstrations for Real World Reinforcement Learning},\n volume = {abs/1704.03732},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:aa8e4263ef59d095dc0f87fb0dae19b441bfa6c5",
            "@type": "ScholarlyArticle",
            "paperId": "aa8e4263ef59d095dc0f87fb0dae19b441bfa6c5",
            "corpusId": 3393747,
            "url": "https://www.semanticscholar.org/paper/aa8e4263ef59d095dc0f87fb0dae19b441bfa6c5",
            "title": "Opponent Modeling in Deep Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2951360189",
                "DBLP": "journals/corr/HeBKD16",
                "ArXiv": "1609.05559",
                "CorpusId": 3393747
            },
            "abstract": "Opponent modeling is necessary in multi-agent settings where secondary agents with competing goals also adapt their strategies, yet it remains challenging because strategies interact with each other and change. Most previous work focuses on developing probabilistic models or parameterized strategies for specific applications. Inspired by the recent success of deep reinforcement learning, we present neural-based models that jointly learn a policy and the behavior of opponents. Instead of explicitly predicting the opponent's action, we encode observation of the opponents into a deep Q-Network (DQN); however, we retain explicit modeling (if desired) using multitasking. By using a Mixture-of-Experts architecture, our model automatically discovers different strategy patterns of opponents without extra supervision. We evaluate our models on a simulated soccer game and a popular trivia game, showing superior performance over DQN and its variants.",
            "referenceCount": 25,
            "citationCount": 267,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-19",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1609.05559"
            },
            "citationStyles": {
                "bibtex": "@Article{He2016OpponentMI,\n author = {He He and Jordan L. Boyd-Graber},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Opponent Modeling in Deep Reinforcement Learning},\n volume = {abs/1609.05559},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:31c8082ac852693431b53afcdc3ea97ed7974e9a",
            "@type": "ScholarlyArticle",
            "paperId": "31c8082ac852693431b53afcdc3ea97ed7974e9a",
            "corpusId": 27254961,
            "url": "https://www.semanticscholar.org/paper/31c8082ac852693431b53afcdc3ea97ed7974e9a",
            "title": "Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2769541360",
                "DBLP": "journals/corr/abs-1711-06782",
                "ArXiv": "1711.06782",
                "CorpusId": 27254961
            },
            "abstract": "Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a large amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires extensive human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and reset policy, with the reset policy resetting the environment for a subsequent attempt. By learning a value function for the reset policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the reset policy can greatly reduce the number of manual resets required to learn a task, can reduce the number of unsafe actions that lead to non-reversible states, and can automatically induce a curriculum.",
            "referenceCount": 25,
            "citationCount": 119,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-11-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1711.06782"
            },
            "citationStyles": {
                "bibtex": "@Article{Eysenbach2017LeaveNT,\n author = {Benjamin Eysenbach and S. Gu and Julian Ibarz and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning},\n volume = {abs/1711.06782},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b542d5f3973970902eab247154f74cc5abb5cbb4",
            "@type": "ScholarlyArticle",
            "paperId": "b542d5f3973970902eab247154f74cc5abb5cbb4",
            "corpusId": 4005032,
            "url": "https://www.semanticscholar.org/paper/b542d5f3973970902eab247154f74cc5abb5cbb4",
            "title": "Time Limits in Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2963193690",
                "ArXiv": "1712.00378",
                "DBLP": "conf/icml/PardoTLK18",
                "CorpusId": 4005032
            },
            "abstract": "In reinforcement learning, it is common to let an agent interact for a fixed amount of time with its environment before resetting it and repeating the process in a series of episodes. The task that the agent has to learn can either be to maximize its performance over (i) that fixed period, or (ii) an indefinite period where time limits are only used during training to diversify experience. In this paper, we provide a formal account for how time limits could effectively be handled in each of the two cases and explain why not doing so can cause state-aliasing and invalidation of experience replay, leading to suboptimal policies and training instability. In case (i), we argue that the terminations due to time limits are in fact part of the environment, and thus a notion of the remaining time should be included as part of the agent's input to avoid violation of the Markov property. In case (ii), the time limits are not part of the environment and are only used to facilitate learning. We argue that this insight should be incorporated by bootstrapping from the value of the state at the end of each partial episode. For both cases, we illustrate empirically the significance of our considerations in improving the performance and stability of existing reinforcement learning algorithms, showing state-of-the-art results on several control tasks.",
            "referenceCount": 33,
            "citationCount": 110,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-12-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Pardo2017TimeLI,\n author = {Fabio Pardo and Arash Tavakoli and Vitaly Levdik and Petar Kormushev},\n booktitle = {International Conference on Machine Learning},\n pages = {4042-4051},\n title = {Time Limits in Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d5ed07113ddcd038062525a5a54550c012ac9a74",
            "@type": "ScholarlyArticle",
            "paperId": "d5ed07113ddcd038062525a5a54550c012ac9a74",
            "corpusId": 8577212,
            "url": "https://www.semanticscholar.org/paper/d5ed07113ddcd038062525a5a54550c012ac9a74",
            "title": "Massively Parallel Methods for Deep Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1658008008",
                "DBLP": "journals/corr/NairSBAFMPSBPLM15",
                "ArXiv": "1507.04296",
                "CorpusId": 8577212
            },
            "abstract": "We present the first massively distributed architecture for deep reinforcement learning. This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience. We used our architecture to implement the Deep Q-Network algorithm (DQN). Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters. Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.",
            "referenceCount": 20,
            "citationCount": 465,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-07-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1507.04296"
            },
            "citationStyles": {
                "bibtex": "@Article{Nair2015MassivelyPM,\n author = {Arun Nair and Praveen Srinivasan and Sam Blackwell and Cagdas Alcicek and R. Fearon and A. D. Maria and Vedavyas Panneershelvam and Mustafa Suleyman and Charlie Beattie and Stig Petersen and S. Legg and Volodymyr Mnih and K. Kavukcuoglu and David Silver},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Massively Parallel Methods for Deep Reinforcement Learning},\n volume = {abs/1507.04296},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:da981e00c5f4a2aa931d02ae2a3211b0a0ac1b63",
            "@type": "ScholarlyArticle",
            "paperId": "da981e00c5f4a2aa931d02ae2a3211b0a0ac1b63",
            "corpusId": 3347463,
            "url": "https://www.semanticscholar.org/paper/da981e00c5f4a2aa931d02ae2a3211b0a0ac1b63",
            "title": "Emotion in reinforcement learning agents and robots: a survey",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/ml/MoerlandBJ18",
                "MAG": "3098163860",
                "ArXiv": "1705.05172",
                "DOI": "10.1007/s10994-017-5666-0",
                "CorpusId": 3347463
            },
            "abstract": null,
            "referenceCount": 145,
            "citationCount": 141,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10994-017-5666-0.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-05-15",
            "journal": {
                "name": "Machine Learning",
                "volume": "107"
            },
            "citationStyles": {
                "bibtex": "@Article{Moerland2017EmotionIR,\n author = {T. Moerland and J. Broekens and C. Jonker},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {443 - 480},\n title = {Emotion in reinforcement learning agents and robots: a survey},\n volume = {107},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:904307cb58795241b22cfaa34f560e610997f5c1",
            "@type": "ScholarlyArticle",
            "paperId": "904307cb58795241b22cfaa34f560e610997f5c1",
            "corpusId": 997870,
            "url": "https://www.semanticscholar.org/paper/904307cb58795241b22cfaa34f560e610997f5c1",
            "title": "Divide-and-Conquer Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2963133245",
                "ArXiv": "1711.09874",
                "DBLP": "journals/corr/abs-1711-09874",
                "CorpusId": 997870
            },
            "abstract": "Standard model-free deep reinforcement learning (RL) algorithms sample a new initial state for each trial, allowing them to optimize policies that can perform well even in highly stochastic environments. However, problems that exhibit considerable initial state variation typically produce high-variance gradient estimates for model-free RL, making direct policy or value function optimization challenging. In this paper, we develop a novel algorithm that instead partitions the initial state space into \"slices\", and optimizes an ensemble of policies, each on a different slice. The ensemble is gradually unified into a single policy that can succeed on the whole state space. This approach, which we term divide-and-conquer RL, is able to solve complex tasks where conventional deep RL methods are ineffective. Our results show that divide-and-conquer RL greatly outperforms conventional policy gradient methods on challenging grasping, manipulation, and locomotion tasks, and exceeds the performance of a variety of prior methods. Videos of policies learned by our algorithm can be viewed at this http URL",
            "referenceCount": 24,
            "citationCount": 101,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-11-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1711.09874"
            },
            "citationStyles": {
                "bibtex": "@Article{Ghosh2017DivideandConquerRL,\n author = {Dibya Ghosh and Avi Singh and A. Rajeswaran and Vikash Kumar and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Divide-and-Conquer Reinforcement Learning},\n volume = {abs/1711.09874},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ae0e5ca59ae70df8ddb722a9c6bec6e4e6e2b743",
            "@type": "ScholarlyArticle",
            "paperId": "ae0e5ca59ae70df8ddb722a9c6bec6e4e6e2b743",
            "corpusId": 2093019,
            "url": "https://www.semanticscholar.org/paper/ae0e5ca59ae70df8ddb722a9c6bec6e4e6e2b743",
            "title": "Reward Shaping in Episodic Reinforcement Learning",
            "venue": "Adaptive Agents and Multi-Agent Systems",
            "publicationVenue": {
                "id": "urn:research:6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                "name": "Adaptive Agents and Multi-Agent Systems",
                "alternate_names": [
                    "Adapt Agent Multi-agent Syst",
                    "International Joint Conference on Autonomous Agents & Multiagent Systems",
                    "Adapt Agent Multi-agents Syst",
                    "AAMAS",
                    "Adaptive Agents and Multi-Agents Systems",
                    "Int Jt Conf Auton Agent  Multiagent Syst"
                ],
                "issn": null,
                "url": "http://www.ifaamas.org/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2620974420",
                "DBLP": "conf/atal/Grzes17",
                "CorpusId": 2093019
            },
            "abstract": "Recent advancements in reinforcement learning confirm that reinforcement learning techniques can solve large scale problems leading to high quality autonomous decision making. It is a matter of time until we will see large scale applications of reinforcement learning in various sectors, such as healthcare and cyber-security, among others. However, reinforcement learning can be time-consuming because the learning algorithms have to determine the long term consequences of their actions using delayed feedback or rewards. Reward shaping is a method of incorporating domain knowledge into reinforcement learning so that the algorithms are guided faster towards more promising solutions. Under an overarching theme of episodic reinforcement learning, this paper shows a unifying analysis of potential-based reward shaping which leads to new theoretical insights into reward shaping in both model-free and model-based algorithms, as well as in multi-agent reinforcement learning.",
            "referenceCount": 28,
            "citationCount": 99,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-05-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Grzes2017RewardSI,\n author = {M. Grzes},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {565-573},\n title = {Reward Shaping in Episodic Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3b2aff88ee03e82993c066c3e698d51da62d5496",
            "@type": "ScholarlyArticle",
            "paperId": "3b2aff88ee03e82993c066c3e698d51da62d5496",
            "corpusId": 13512886,
            "url": "https://www.semanticscholar.org/paper/3b2aff88ee03e82993c066c3e698d51da62d5496",
            "title": "Deep Reinforcement Learning in Large Discrete Action Spaces",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2215378786",
                "ArXiv": "1512.07679",
                "CorpusId": 13512886
            },
            "abstract": "Being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. Recommender systems, industrial plants and language models are only some of the many real-world tasks involving large numbers of discrete actions for which current methods are difficult or even often impossible to apply. An ability to generalize over the set of actions as well as sub-linear complexity relative to the size of the set are both necessary to handle such tasks. Current approaches are not able to provide both of these, which motivates the work in this paper. Our proposed approach leverages prior information about the actions to embed them in a continuous space upon which it can generalize. Additionally, approximate nearest-neighbor methods allow for logarithmic-time lookup complexity relative to the number of actions, which is necessary for time-wise tractable training. This combined approach allows reinforcement learning methods to be applied to large-scale learning problems previously intractable with current methods. We demonstrate our algorithm\u2019s abilities on a series of tasks having up to one million actions.",
            "referenceCount": 21,
            "citationCount": 452,
            "influentialCitationCount": 59,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2015-12-24",
            "journal": {
                "name": "arXiv: Artificial Intelligence",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Dulac-Arnold2015DeepRL,\n author = {Gabriel Dulac-Arnold and Richard Evans and H. V. Hasselt and P. Sunehag and T. Lillicrap and Jonathan J. Hunt and Timothy A. Mann and T. Weber and T. Degris and Ben Coppin},\n journal = {arXiv: Artificial Intelligence},\n title = {Deep Reinforcement Learning in Large Discrete Action Spaces},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:10a4992ece5baea79326a8878a6244eeacbc6af5",
            "@type": "ScholarlyArticle",
            "paperId": "10a4992ece5baea79326a8878a6244eeacbc6af5",
            "corpusId": 11965834,
            "url": "https://www.semanticscholar.org/paper/10a4992ece5baea79326a8878a6244eeacbc6af5",
            "title": "Deep Successor Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/KulkarniSGG16",
                "ArXiv": "1606.02396",
                "MAG": "2417089653",
                "CorpusId": 11965834
            },
            "abstract": "Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms. There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map. The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards. The value function of a state can be computed as the inner product between the successor map and the reward weights. In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework. DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy. We show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains (MazeBase) and the Doom game engine.",
            "referenceCount": 42,
            "citationCount": 188,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-06-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1606.02396"
            },
            "citationStyles": {
                "bibtex": "@Article{Kulkarni2016DeepSR,\n author = {Tejas D. Kulkarni and A. Saeedi and Simanta Gautam and S. Gershman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Deep Successor Reinforcement Learning},\n volume = {abs/1606.02396},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:88909a57da9a43ceb52aae8424b1f348dba99cab",
            "@type": "ScholarlyArticle",
            "paperId": "88909a57da9a43ceb52aae8424b1f348dba99cab",
            "corpusId": 1763828,
            "url": "https://www.semanticscholar.org/paper/88909a57da9a43ceb52aae8424b1f348dba99cab",
            "title": "Why is Posterior Sampling Better than Optimism for Reinforcement Learning?",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2963158178",
                "DBLP": "conf/icml/OsbandR17",
                "ArXiv": "1607.00215",
                "CorpusId": 1763828
            },
            "abstract": "Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an $\\tilde{O}(H\\sqrt{SAT})$ Bayesian expected regret bound for PSRL in finite-horizon episodic Markov decision processes, where $H$ is the horizon, $S$ is the number of states, $A$ is the number of actions and $T$ is the time elapsed. This improves upon the best previous bound of $\\tilde{O}(H S \\sqrt{AT})$ for any reinforcement learning algorithm.",
            "referenceCount": 41,
            "citationCount": 219,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-07-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1607.00215"
            },
            "citationStyles": {
                "bibtex": "@Article{Osband2016WhyIP,\n author = {Ian Osband and Benjamin Van Roy},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Why is Posterior Sampling Better than Optimism for Reinforcement Learning?},\n volume = {abs/1607.00215},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bc69708aeaae562ab1406ca7dd0e50c1ec247635",
            "@type": "ScholarlyArticle",
            "paperId": "bc69708aeaae562ab1406ca7dd0e50c1ec247635",
            "corpusId": 5492891,
            "url": "https://www.semanticscholar.org/paper/bc69708aeaae562ab1406ca7dd0e50c1ec247635",
            "title": "Terrain-adaptive locomotion skills using deep reinforcement learning",
            "venue": "ACM Transactions on Graphics",
            "publicationVenue": {
                "id": "urn:research:aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                "name": "ACM Transactions on Graphics",
                "alternate_names": [
                    "ACM Trans Graph"
                ],
                "issn": "0730-0301",
                "url": "http://www.acm.org/tog/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/tog/Panne16",
                "MAG": "2460299708",
                "DOI": "10.1145/2897824.2925881",
                "CorpusId": 5492891
            },
            "abstract": "Reinforcement learning offers a promising methodology for developing skills for simulated characters, but typically requires working with sparse hand-crafted features. Building on recent progress in deep reinforcement learning (DeepRL), we introduce a mixture of actor-critic experts (MACE) approach that learns terrain-adaptive dynamic locomotion skills using high-dimensional state and terrain descriptions as input, and parameterized leaps or steps as output actions. MACE learns more quickly than a single actor-critic approach and results in actor-critic experts that exhibit specialization. Additional elements of our solution that contribute towards efficient learning include Boltzmann exploration and the use of initial actor biases to encourage specialization. Results are demonstrated for multiple planar characters and terrain classes.",
            "referenceCount": 65,
            "citationCount": 233,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-07-11",
            "journal": {
                "name": "ACM Transactions on Graphics (TOG)",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{Peng2016TerrainadaptiveLS,\n author = {X. B. Peng and G. Berseth and M. V. D. Panne},\n booktitle = {ACM Transactions on Graphics},\n journal = {ACM Transactions on Graphics (TOG)},\n pages = {1 - 12},\n title = {Terrain-adaptive locomotion skills using deep reinforcement learning},\n volume = {35},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:376f23cce537235122fdce5524d084e3a869c403",
            "@type": "ScholarlyArticle",
            "paperId": "376f23cce537235122fdce5524d084e3a869c403",
            "corpusId": 10335455,
            "url": "https://www.semanticscholar.org/paper/376f23cce537235122fdce5524d084e3a869c403",
            "title": "Towards Deep Symbolic Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2521274174",
                "ArXiv": "1609.05518",
                "DBLP": "journals/corr/GarneloAS16",
                "CorpusId": 10335455
            },
            "abstract": "Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system -- though just a prototype -- learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game.",
            "referenceCount": 41,
            "citationCount": 206,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-09-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1609.05518"
            },
            "citationStyles": {
                "bibtex": "@Article{Garnelo2016TowardsDS,\n author = {M. Garnelo and Kai Arulkumaran and M. Shanahan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Towards Deep Symbolic Reinforcement Learning},\n volume = {abs/1609.05518},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:83bf91012997019f432179aad798e6d3fbb95c36",
            "@type": "ScholarlyArticle",
            "paperId": "83bf91012997019f432179aad798e6d3fbb95c36",
            "corpusId": 12046082,
            "url": "https://www.semanticscholar.org/paper/83bf91012997019f432179aad798e6d3fbb95c36",
            "title": "Multiagent cooperation and competition with deep reinforcement learning",
            "venue": "PLoS ONE",
            "publicationVenue": {
                "id": "urn:research:0aed7a40-85f3-4c66-9e1b-c1556c57001b",
                "name": "PLoS ONE",
                "alternate_names": [
                    "Plo ONE",
                    "PLOS ONE",
                    "PLO ONE"
                ],
                "issn": "1932-6203",
                "url": "https://journals.plos.org/plosone/"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1511.08779",
                "MAG": "2951120231",
                "DBLP": "journals/corr/TampuuMKKKAAV15",
                "PubMedCentral": "5381785",
                "DOI": "10.1371/journal.pone.0172395",
                "CorpusId": 12046082,
                "PubMed": "28380078"
            },
            "abstract": "Evolution of cooperation and competition can appear when multiple adaptive agents share a biological, social, or technological niche. In the present work we study how cooperation and competition emerge between autonomous agents that learn by reinforcement while using only their raw visual input as the state representation. In particular, we extend the Deep Q-Learning framework to multiagent environments to investigate the interaction between two learning agents in the well-known video game Pong. By manipulating the classical rewarding scheme of Pong we show how competitive and collaborative behaviors emerge. We also describe the progression from competitive to collaborative behavior when the incentive to cooperate is increased. Finally we show how learning by playing against another adaptive agent, instead of against a hard-wired algorithm, results in more robust strategies. The present work shows that Deep Q-Networks can become a useful tool for studying decentralized learning of multiagent systems coping with high-dimensional environments.",
            "referenceCount": 39,
            "citationCount": 657,
            "influentialCitationCount": 59,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0172395&type=printable",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Biology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-27",
            "journal": {
                "name": "PLoS ONE",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Tampuu2015MultiagentCA,\n author = {Ardi Tampuu and Tambet Matiisen and Dorian Kodelja and Ilya Kuzovkin and Kristjan Korjus and Juhan Aru and Jaan Aru and Raul Vicente},\n booktitle = {PLoS ONE},\n journal = {PLoS ONE},\n title = {Multiagent cooperation and competition with deep reinforcement learning},\n volume = {12},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:eeb2cff641e2929d9d11f48b6bc86faa10117f27",
            "@type": "ScholarlyArticle",
            "paperId": "eeb2cff641e2929d9d11f48b6bc86faa10117f27",
            "corpusId": 204085274,
            "url": "https://www.semanticscholar.org/paper/eeb2cff641e2929d9d11f48b6bc86faa10117f27",
            "title": "Reinforcement Learning",
            "venue": "Handbook of Machine Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "DOI": "10.1142/9789813271234_0015",
                "CorpusId": 204085274
            },
            "abstract": "This coursework is concerned with learning an optimal policy for any instance of the \u201dRoad Fighter\u201d problem, as defined in https://github.com/cortu01/ rl_roadFighter. It builds on the material covered in the lectures on Markov Decision Processes (MDPs), Monte Carlo and Temporal-Difference Learning solutions to MDPs, and Generalisation and Function Approximation. The aim of the coursework is to better familiarise you with function approximation and sampling in the context of Reinforcement Learning (RL).",
            "referenceCount": 5,
            "citationCount": 10,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2018-10-18",
            "journal": {
                "name": "Handbook of Machine Learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nandy2018ReinforcementL,\n author = {Abhishek Nandy and Manisha Biswas},\n booktitle = {Handbook of Machine Learning},\n journal = {Handbook of Machine Learning},\n title = {Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7fbf55baccbc5fdc7ded1ba18330605909aef5e5",
            "@type": "ScholarlyArticle",
            "paperId": "7fbf55baccbc5fdc7ded1ba18330605909aef5e5",
            "corpusId": 8108362,
            "url": "https://www.semanticscholar.org/paper/7fbf55baccbc5fdc7ded1ba18330605909aef5e5",
            "title": "Markov Games as a Framework for Multi-Agent Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1994,
            "externalIds": {
                "DBLP": "conf/icml/Littman94",
                "MAG": "1542941925",
                "DOI": "10.1016/b978-1-55860-335-6.50027-1",
                "CorpusId": 8108362
            },
            "abstract": null,
            "referenceCount": 20,
            "citationCount": 2646,
            "influentialCitationCount": 241,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.ee.duke.edu/~lcarin/emag/seminar_presentations/Markov_Games_Littman.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1994-07-10",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Littman1994MarkovGA,\n author = {M. Littman},\n booktitle = {International Conference on Machine Learning},\n pages = {157-163},\n title = {Markov Games as a Framework for Multi-Agent Reinforcement Learning},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cdfb8f75c8f6459961359b483f1c017dbeec8282",
            "@type": "ScholarlyArticle",
            "paperId": "cdfb8f75c8f6459961359b483f1c017dbeec8282",
            "corpusId": 16561904,
            "url": "https://www.semanticscholar.org/paper/cdfb8f75c8f6459961359b483f1c017dbeec8282",
            "title": "Loss is its own Reward: Self-Supervision for Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2563830277",
                "ArXiv": "1612.07307",
                "DBLP": "conf/iclr/ShelhamerMAD17",
                "CorpusId": 16561904
            },
            "abstract": "Reinforcement learning optimizes policies for expected cumulative reward. Need the supervision be so narrow? Reward is delayed and sparse for many tasks, making it a difficult and impoverished signal for end-to-end optimization. To augment reward, we consider a range of self-supervised tasks that incorporate states, actions, and successors to provide auxiliary losses. These losses offer ubiquitous and instantaneous supervision for representation learning even in the absence of reward. While current results show that learning from reward alone is feasible, pure reinforcement learning methods are constrained by computational and data efficiency issues that can be remedied by auxiliary losses. Self-supervised pre-training and joint optimization improve the data efficiency and policy returns of end-to-end reinforcement learning.",
            "referenceCount": 34,
            "citationCount": 165,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-12-21",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1612.07307"
            },
            "citationStyles": {
                "bibtex": "@Article{Shelhamer2016LossII,\n author = {Evan Shelhamer and Parsa Mahmoudieh and Max Argus and Trevor Darrell},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Loss is its own Reward: Self-Supervision for Reinforcement Learning},\n volume = {abs/1612.07307},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:599f7863721d542dcef2da49b41d82b21e4f80b3",
            "@type": "ScholarlyArticle",
            "paperId": "599f7863721d542dcef2da49b41d82b21e4f80b3",
            "corpusId": 423406,
            "url": "https://www.semanticscholar.org/paper/599f7863721d542dcef2da49b41d82b21e4f80b3",
            "title": "Learning to Compose Words into Sentences with Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/iclr/YogatamaBDGL17",
                "MAG": "2963451457",
                "ArXiv": "1611.09100",
                "CorpusId": 423406
            },
            "abstract": "We use reinforcement learning to learn tree-structured neural networks for computing representations of natural language sentences. In contrast with prior work on tree-structured models in which the trees are either provided as input or predicted using supervision from explicit treebank annotations, the tree structures in this work are optimized to improve performance on a downstream task. Experiments demonstrate the benefit of learning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations. We analyze the induced trees and show that while they discover some linguistically intuitive structures (e.g., noun phrases, simple verb phrases), they are different than conventional English syntactic structures.",
            "referenceCount": 36,
            "citationCount": 152,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.09100"
            },
            "citationStyles": {
                "bibtex": "@Article{Yogatama2016LearningTC,\n author = {Dani Yogatama and Phil Blunsom and Chris Dyer and Edward Grefenstette and Wang Ling},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning to Compose Words into Sentences with Reinforcement Learning},\n volume = {abs/1611.09100},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:abb7670d0770d67e74e34b8622ad62a18cbdbdc1",
            "@type": "ScholarlyArticle",
            "paperId": "abb7670d0770d67e74e34b8622ad62a18cbdbdc1",
            "corpusId": 7665390,
            "url": "https://www.semanticscholar.org/paper/abb7670d0770d67e74e34b8622ad62a18cbdbdc1",
            "title": "Reinforcement learning with temporal logic rewards",
            "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
            "publicationVenue": {
                "id": "urn:research:37275deb-3fcf-4d16-ae77-95db9899b1f3",
                "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                "alternate_names": [
                    "IROS",
                    "Intelligent Robots and Systems",
                    "Intell Robot Syst",
                    "IEEE/RJS Int Conf Intell Robot Syst"
                ],
                "issn": null,
                "url": "http://www.iros.org/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1612.03471",
                "DBLP": "conf/iros/LiVB17",
                "MAG": "2567705466",
                "DOI": "10.1109/IROS.2017.8206234",
                "CorpusId": 7665390
            },
            "abstract": "Reinforcement learning (RL) depends critically on the choice of reward functions used to capture the desired behavior and constraints of a robot. Usually, these are handcrafted by a expert designer and represent heuristics for relatively simple tasks. Real world applications typically involve more complex tasks with rich temporal and logical structure. In this paper we take advantage of the expressive power of temporal logic (TL) to specify complex rules the robot should follow, and incorporate domain knowledge into learning. We propose Truncated Linear Temporal Logic (TLTL) as a specification language, We propose Truncated Linear Temporal Logic (TLTL) as a specification language, that is arguably well suited for the robotics applications, We show in simulated trials that learning is faster and policies obtained using the proposed approach outperform the ones learned using heuristic rewards in terms of the robustness degree, i.e., how well the tasks are satisfied. Furthermore, we demonstrate the proposed RL approach in a toast-placing task learned by a Baxter robot.",
            "referenceCount": 31,
            "citationCount": 152,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1612.03471",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-12-11",
            "journal": {
                "name": "2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Li2016ReinforcementLW,\n author = {Xiao Li and C. Vasile and C. Belta},\n booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},\n journal = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\n pages = {3834-3839},\n title = {Reinforcement learning with temporal logic rewards},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ae778db334e4e3a6f1d3ffceac70ca40bf164482",
            "@type": "ScholarlyArticle",
            "paperId": "ae778db334e4e3a6f1d3ffceac70ca40bf164482",
            "corpusId": 15556472,
            "url": "https://www.semanticscholar.org/paper/ae778db334e4e3a6f1d3ffceac70ca40bf164482",
            "title": "PAC Reinforcement Learning with Rich Observations",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2950443122",
                "DBLP": "conf/nips/KrishnamurthyAL16",
                "CorpusId": 15556472
            },
            "abstract": "We propose and study a new model for reinforcement learning with rich observations, generalizing contextual bandits to sequential decision making. These models require an agent to take actions based on observations (features) with the goal of achieving long-term performance competitive with a large set of policies. To avoid barriers to sample-efficient learning associated with large observation spaces and general POMDPs, we focus on problems that can be summarized by a small number of hidden states and have long-term rewards that are predictable by a reactive function class. In this setting, we design and analyze a new reinforcement learning algorithm, Least Squares Value Elimination by Exploration. We prove that the algorithm learns near optimal behavior after a number of episodes that is polynomial in all relevant parameters, logarithmic in the number of policies, and independent of the size of the observation space. Our result provides theoretical justification for reinforcement learning with function approximation.",
            "referenceCount": 29,
            "citationCount": 137,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-02-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Krishnamurthy2016PACRL,\n author = {A. Krishnamurthy and Alekh Agarwal and J. Langford},\n booktitle = {Neural Information Processing Systems},\n pages = {1840-1848},\n title = {PAC Reinforcement Learning with Rich Observations},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2470fcf0f89082de874ac9133ccb3a8667dd89a8",
            "@type": "ScholarlyArticle",
            "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
            "corpusId": 10296902,
            "url": "https://www.semanticscholar.org/paper/2470fcf0f89082de874ac9133ccb3a8667dd89a8",
            "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/corr/StadieLA15",
                "ArXiv": "1507.00814",
                "MAG": "779494576",
                "CorpusId": 10296902
            },
            "abstract": "Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.",
            "referenceCount": 30,
            "citationCount": 447,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-07-03",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1507.00814"
            },
            "citationStyles": {
                "bibtex": "@Article{Stadie2015IncentivizingEI,\n author = {Bradly C. Stadie and S. Levine and P. Abbeel},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models},\n volume = {abs/1507.00814},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:35b05886694ffaa0d5431b0510d2daa4560f37af",
            "@type": "ScholarlyArticle",
            "paperId": "35b05886694ffaa0d5431b0510d2daa4560f37af",
            "corpusId": 1632316,
            "url": "https://www.semanticscholar.org/paper/35b05886694ffaa0d5431b0510d2daa4560f37af",
            "title": "Reinforcement Learning of POMDPs using Spectral Methods",
            "venue": "Annual Conference Computational Learning Theory",
            "publicationVenue": {
                "id": "urn:research:24b0721b-0592-414a-ac79-7271515aaab0",
                "name": "Annual Conference Computational Learning Theory",
                "alternate_names": [
                    "Conf Learn Theory",
                    "COLT",
                    "Conference on Learning Theory",
                    "Annu Conf Comput Learn Theory"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=536"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1602.07764",
                "MAG": "2963254349",
                "DBLP": "conf/colt/Azizzadenesheli16",
                "CorpusId": 1632316
            },
            "abstract": "Author(s): Azizzadenesheli, Kamyar; Lazaric, Alessandro; Anandkumar, Animashree | Abstract: We propose a new reinforcement learning algorithm for partially observable Markov decision processes (POMDP) based on spectral decomposition methods. While spectral methods have been previously employed for consistent learning of (passive) latent variable models such as hidden Markov models, POMDPs are more challenging since the learner interacts with the environment and possibly changes the future observations in the process. We devise a learning algorithm running through episodes, in each episode we employ spectral techniques to learn the POMDP parameters from a trajectory generated by a fixed policy. At the end of the episode, an optimization oracle returns the optimal memoryless planning policy which maximizes the expected reward based on the estimated POMDP model. We prove an order-optimal regret bound with respect to the optimal memoryless policy and efficient scaling with respect to the dimensionality of observation and action spaces.",
            "referenceCount": 64,
            "citationCount": 115,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-02-25",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Azizzadenesheli2016ReinforcementLO,\n author = {K. Azizzadenesheli and A. Lazaric and Anima Anandkumar},\n booktitle = {Annual Conference Computational Learning Theory},\n pages = {193-256},\n title = {Reinforcement Learning of POMDPs using Spectral Methods},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:03fb702dc685215a78595c8a0aaef6046075aab8",
            "@type": "ScholarlyArticle",
            "paperId": "03fb702dc685215a78595c8a0aaef6046075aab8",
            "corpusId": 7137572,
            "url": "https://www.semanticscholar.org/paper/03fb702dc685215a78595c8a0aaef6046075aab8",
            "title": "Fairness in Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1611.03071",
                "DBLP": "conf/icml/JabbariJKMR17",
                "MAG": "2949116092",
                "CorpusId": 7137572
            },
            "abstract": "We initiate the study of fairness in reinforcement learning, where the actions of a learning algorithm may affect its environment and future rewards. Our fairness constraint requires that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take time exponential in the number of states to achieve non-trivial approximation to the optimal policy. We then provide a provably fair polynomial time algorithm under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness",
            "referenceCount": 41,
            "citationCount": 173,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-11-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Jabbari2016FairnessIR,\n author = {S. Jabbari and Matthew Joseph and Michael Kearns and Jamie Morgenstern and Aaron Roth},\n booktitle = {International Conference on Machine Learning},\n pages = {1617-1626},\n title = {Fairness in Reinforcement Learning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2735e6e760a5fce119d82dc1e449f329d072e172",
            "@type": "ScholarlyArticle",
            "paperId": "2735e6e760a5fce119d82dc1e449f329d072e172",
            "corpusId": 8506181,
            "url": "https://www.semanticscholar.org/paper/2735e6e760a5fce119d82dc1e449f329d072e172",
            "title": "Active Object Localization with Deep Reinforcement Learning",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/iccv/CaicedoL15",
                "ArXiv": "1511.06015",
                "MAG": "2951675993",
                "DOI": "10.1109/ICCV.2015.286",
                "CorpusId": 8506181
            },
            "abstract": "We present an active detection model for localizing objects in scenes. The model is class-specific and allows an agent to focus attention on candidate regions for identifying the correct location of a target object. This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most specific location of target objects following top-down reasoning. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.",
            "referenceCount": 37,
            "citationCount": 415,
            "influentialCitationCount": 44,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1511.06015",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-11-18",
            "journal": {
                "name": "2015 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Caicedo2015ActiveOL,\n author = {Juan C. Caicedo and S. Lazebnik},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2015 IEEE International Conference on Computer Vision (ICCV)},\n pages = {2488-2496},\n title = {Active Object Localization with Deep Reinforcement Learning},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:079021d93a88c6a4721af75397d14c2125af1f26",
            "@type": "ScholarlyArticle",
            "paperId": "079021d93a88c6a4721af75397d14c2125af1f26",
            "corpusId": 12070666,
            "url": "https://www.semanticscholar.org/paper/079021d93a88c6a4721af75397d14c2125af1f26",
            "title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1611.01843",
                "MAG": "2951943130",
                "DBLP": "conf/iclr/DenilAKEBF17",
                "CorpusId": 12070666
            },
            "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.",
            "referenceCount": 47,
            "citationCount": 163,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science",
                "Physics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.01843"
            },
            "citationStyles": {
                "bibtex": "@Article{Denil2016LearningTP,\n author = {Misha Denil and Pulkit Agrawal and Tejas D. Kulkarni and Tom Erez and P. Battaglia and Nando de Freitas},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning to Perform Physics Experiments via Deep Reinforcement Learning},\n volume = {abs/1611.01843},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:33baa52f583fb049f473a53d8e0ca5ac8e7abbf2",
            "@type": "ScholarlyArticle",
            "paperId": "33baa52f583fb049f473a53d8e0ca5ac8e7abbf2",
            "corpusId": 8198191,
            "url": "https://www.semanticscholar.org/paper/33baa52f583fb049f473a53d8e0ca5ac8e7abbf2",
            "title": "Reinforcement Learning for Visual Object Detection",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2469312016",
                "DBLP": "conf/cvpr/MathePS16",
                "DOI": "10.1109/CVPR.2016.316",
                "CorpusId": 8198191
            },
            "abstract": "One of the most widely used strategies for visual object detection is based on exhaustive spatial hypothesis search. While methods like sliding windows have been successful and effective for many years, they are still brute-force, independent of the image content and the visual category being searched. In this paper we present principled sequential models that accumulate evidence collected at a small set of image locations in order to detect visual objects effectively. By formulating sequential search as reinforcement learning of the search policy (including the stopping condition), our fully trainable model can explicitly balance for each class, specifically, the conflicting goals of exploration - sampling more image regions for better accuracy -, and exploitation - stopping the search efficiently when sufficiently confident about the target's location. The methodology is general and applicable to any detector response function. We report encouraging results in the PASCAL VOC 2012 object detection test set showing that the proposed methodology achieves almost two orders of magnitude speed-up over sliding window methods.",
            "referenceCount": 42,
            "citationCount": 146,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-01",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Mathe2016ReinforcementLF,\n author = {Stefan Mathe and Aleksis Pirinen and C. Sminchisescu},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2894-2902},\n title = {Reinforcement Learning for Visual Object Detection},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:759bbd8dd50cb4790cad7a3bccbdfcbfee5e3e89",
            "@type": "ScholarlyArticle",
            "paperId": "759bbd8dd50cb4790cad7a3bccbdfcbfee5e3e89",
            "corpusId": 15159737,
            "url": "https://www.semanticscholar.org/paper/759bbd8dd50cb4790cad7a3bccbdfcbfee5e3e89",
            "title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2963082979",
                "ArXiv": "1512.01629",
                "DBLP": "journals/jmlr/ChowGJP17",
                "CorpusId": 15159737
            },
            "abstract": "In many sequential decision-making problems one is interested in minimizing an expected cumulative cost while taking into account \\emph{risk}, i.e., increased awareness of events of small probability and high consequences. Accordingly, the objective of this paper is to present efficient reinforcement learning algorithms for risk-constrained Markov decision processes (MDPs), where risk is represented via a chance constraint or a constraint on the conditional value-at-risk (CVaR) of the cumulative cost. We collectively refer to such problems as percentile risk-constrained MDPs. \nSpecifically, we first derive a formula for computing the gradient of the Lagrangian function for percentile risk-constrained MDPs. Then, we devise policy gradient and actor-critic algorithms that (1) estimate such gradient, (2) update the policy in the descent direction, and (3) update the Lagrange multiplier in the ascent direction. For these algorithms we prove convergence to locally optimal policies. Finally, we demonstrate the effectiveness of our algorithms in an optimal stopping problem and an online marketing application.",
            "referenceCount": 62,
            "citationCount": 364,
            "influentialCitationCount": 42,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-12-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1512.01629"
            },
            "citationStyles": {
                "bibtex": "@Article{Chow2015RiskConstrainedRL,\n author = {Yinlam Chow and M. Ghavamzadeh and Lucas Janson and M. Pavone},\n booktitle = {Journal of machine learning research},\n journal = {ArXiv},\n title = {Risk-Constrained Reinforcement Learning with Percentile Risk Criteria},\n volume = {abs/1512.01629},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e73336a361e853c145d99745ddebcf63038fcd09",
            "@type": "ScholarlyArticle",
            "paperId": "e73336a361e853c145d99745ddebcf63038fcd09",
            "corpusId": 9489986,
            "url": "https://www.semanticscholar.org/paper/e73336a361e853c145d99745ddebcf63038fcd09",
            "title": "The successor representation in human reinforcement learning",
            "venue": "Nature Human Behaviour",
            "publicationVenue": {
                "id": "urn:research:b7e5a3b3-2519-4ca2-8870-c2b45ef76ab9",
                "name": "Nature Human Behaviour",
                "alternate_names": [
                    "Nat Hum Behav"
                ],
                "issn": "2397-3374",
                "url": "https://www.nature.com/nathumbehav/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2953319434",
                "DOI": "10.1038/s41562-017-0180-8",
                "CorpusId": 9489986,
                "PubMed": "31024137"
            },
            "abstract": null,
            "referenceCount": 68,
            "citationCount": 282,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc6941356?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Psychology",
                "Biology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-10-27",
            "journal": {
                "name": "Nature Human Behaviour",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Momennejad2016TheSR,\n author = {I. Momennejad and E. Russek and J. H. Cheong and M. Botvinick and N. Daw and S. Gershman},\n booktitle = {Nature Human Behaviour},\n journal = {Nature Human Behaviour},\n pages = {680 - 692},\n title = {The successor representation in human reinforcement learning},\n volume = {1},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:85d8b1b3483c7f4db999e7cf6b3e6231954c43dc",
            "@type": "ScholarlyArticle",
            "paperId": "85d8b1b3483c7f4db999e7cf6b3e6231954c43dc",
            "corpusId": 10713737,
            "url": "https://www.semanticscholar.org/paper/85d8b1b3483c7f4db999e7cf6b3e6231954c43dc",
            "title": "Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/HeLSP16",
                "ArXiv": "1611.01606",
                "MAG": "2962948087",
                "CorpusId": 10713737
            },
            "abstract": "We propose a novel training algorithm for reinforcement learning which combines the strength of deep Q-learning with a constrained optimization approach to tighten optimality and encourage faster reward propagation. Our novel technique makes deep reinforcement learning more practical by drastically reducing the training time. We evaluate the performance of our approach on the 49 games of the challenging Arcade Learning Environment, and report significant improvements in both training time and accuracy.",
            "referenceCount": 30,
            "citationCount": 75,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.01606"
            },
            "citationStyles": {
                "bibtex": "@Article{He2016LearningTP,\n author = {Frank S. He and Yang Liu and A. Schwing and Jian Peng},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening},\n volume = {abs/1611.01606},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4a026fd65af4ba3575e64174de56fee093fa3330",
            "@type": "ScholarlyArticle",
            "paperId": "4a026fd65af4ba3575e64174de56fee093fa3330",
            "corpusId": 2374643,
            "url": "https://www.semanticscholar.org/paper/4a026fd65af4ba3575e64174de56fee093fa3330",
            "title": "Taming the Noise in Reinforcement Learning via Soft Updates",
            "venue": "Conference on Uncertainty in Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:f9af8000-42f8-410d-a622-e8811e41660a",
                "name": "Conference on Uncertainty in Artificial Intelligence",
                "alternate_names": [
                    "Uncertainty in Artificial Intelligence",
                    "UAI",
                    "Conf Uncertain Artif Intell",
                    "Uncertain Artif Intell"
                ],
                "issn": null,
                "url": "http://www.auai.org/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/uai/FoxPT16",
                "ArXiv": "1512.08562",
                "MAG": "2963267001",
                "CorpusId": 2374643
            },
            "abstract": "Model-free reinforcement learning algorithms, such as Q-learning, perform poorly in the early stages of learning in noisy environments, because much effort is spent unlearning biased estimates of the state-action value function. The bias results from selecting, among several noisy estimates, the apparent optimum, which may actually be suboptimal. We propose G-learning, a new off-policy learning algorithm that regularizes the value estimates by penalizing deterministic policies in the beginning of the learning process. We show that this method reduces the bias of the value-function estimation, leading to faster convergence to the optimal value and the optimal policy. Moreover, G-learning enables the natural incorporation of prior domain knowledge, when available. The stochastic nature of G-learning also makes it avoid some exploration costs, a property usually attributed only to on-policy algorithms. We illustrate these ideas in several examples, where G-learning results in significant improvements of the convergence rate and the cost of the learning process.",
            "referenceCount": 42,
            "citationCount": 273,
            "influentialCitationCount": 43,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-12-28",
            "journal": {
                "name": "arXiv: Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Fox2015TamingTN,\n author = {Roy Fox and Ari Pakman and Naftali Tishby},\n booktitle = {Conference on Uncertainty in Artificial Intelligence},\n journal = {arXiv: Learning},\n title = {Taming the Noise in Reinforcement Learning via Soft Updates},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0858fb6efb0e7ef549db94813b9d6f896073d60a",
            "@type": "ScholarlyArticle",
            "paperId": "0858fb6efb0e7ef549db94813b9d6f896073d60a",
            "corpusId": 207179119,
            "url": "https://www.semanticscholar.org/paper/0858fb6efb0e7ef549db94813b9d6f896073d60a",
            "title": "Bayesian Reinforcement Learning: A Survey",
            "venue": "Found. Trends Mach. Learn.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2951930555",
                "ArXiv": "1609.04436",
                "DBLP": "journals/ftml/GhavamzadehMPT15",
                "DOI": "10.1561/2200000049",
                "CorpusId": 207179119
            },
            "abstract": "Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.",
            "referenceCount": 181,
            "citationCount": 381,
            "influentialCitationCount": 35,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1609.04436",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2015-11-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1609.04436"
            },
            "citationStyles": {
                "bibtex": "@Article{Ghavamzadeh2015BayesianRL,\n author = {M. Ghavamzadeh and Shie Mannor and Joelle Pineau and Aviv Tamar},\n booktitle = {Found. Trends Mach. Learn.},\n journal = {ArXiv},\n title = {Bayesian Reinforcement Learning: A Survey},\n volume = {abs/1609.04436},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d00e7779c39dc7b06d7d43cf6de6d734c8edc4b8",
            "@type": "ScholarlyArticle",
            "paperId": "d00e7779c39dc7b06d7d43cf6de6d734c8edc4b8",
            "corpusId": 8395799,
            "url": "https://www.semanticscholar.org/paper/d00e7779c39dc7b06d7d43cf6de6d734c8edc4b8",
            "title": "Language Understanding for Text-based Games using Deep Reinforcement Learning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1934909785",
                "ArXiv": "1506.08941",
                "DBLP": "journals/corr/NarasimhanKB15",
                "ACL": "D15-1001",
                "DOI": "10.18653/v1/D15-1001",
                "CorpusId": 8395799
            },
            "abstract": "In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-ofwords and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations. 1",
            "referenceCount": 35,
            "citationCount": 318,
            "influentialCitationCount": 38,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D15-1001.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-06-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Narasimhan2015LanguageUF,\n author = {Karthik Narasimhan and Tejas D. Kulkarni and R. Barzilay},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {1-11},\n title = {Language Understanding for Text-based Games using Deep Reinforcement Learning},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b249fa75021a27d69e13337a2fa326fe90352a50",
            "@type": "ScholarlyArticle",
            "paperId": "b249fa75021a27d69e13337a2fa326fe90352a50",
            "corpusId": 6104961,
            "url": "https://www.semanticscholar.org/paper/b249fa75021a27d69e13337a2fa326fe90352a50",
            "title": "Reinforcement learning using quantum Boltzmann machines",
            "venue": "Quantum information & computation",
            "publicationVenue": {
                "id": "urn:research:b211891f-0b6b-43bb-8223-5371034cd946",
                "name": "Quantum information & computation",
                "alternate_names": [
                    "Quantum Inf  Comput",
                    "Quantum Information & Computation",
                    "Quantum inf  comput"
                ],
                "issn": "1533-7146",
                "url": "http://www.rintonpress.com/journals/qic/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/CrawfordLGOR16",
                "ArXiv": "1612.05695",
                "MAG": "2953088218",
                "DOI": "10.26421/QIC18.1-2-3",
                "CorpusId": 6104961
            },
            "abstract": "We investigate whether quantum annealers with select chip layouts can outperform classical computers in reinforcement learning tasks. We associate a transverse field Ising spin Hamiltonian with a layout of qubits similar to that of a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to numerically simulate quantum sampling from this system. We design a reinforcement learning algorithm in which the set of visible nodes representing the states and actions of an optimal policy are the first and last layers of the deep network. In absence of a transverse field, our simulations show that DBMs are trained more effectively than restricted Boltzmann machines (RBM) with the same number of nodes. We then develop a framework for training the network as a quantum Boltzmann machine (QBM) in the presence of a significant transverse field for reinforcement learning. This method also outperforms the reinforcement learning method that uses RBMs.",
            "referenceCount": 68,
            "citationCount": 91,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1612.05695",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Physics",
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-12-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1612.05695"
            },
            "citationStyles": {
                "bibtex": "@Article{Crawford2016ReinforcementLU,\n author = {Daniel Crawford and A. Levit and Navid Ghadermarzy and J. S. Oberoi and Pooya Ronagh},\n booktitle = {Quantum information & computation},\n journal = {ArXiv},\n title = {Reinforcement learning using quantum Boltzmann machines},\n volume = {abs/1612.05695},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9ba266a4a4644e877fc37a64be3beddce8904cf7",
            "@type": "ScholarlyArticle",
            "paperId": "9ba266a4a4644e877fc37a64be3beddce8904cf7",
            "corpusId": 3757688,
            "url": "https://www.semanticscholar.org/paper/9ba266a4a4644e877fc37a64be3beddce8904cf7",
            "title": "Maximum Entropy Deep Inverse Reinforcement Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1929981607",
                "ArXiv": "1507.04888",
                "CorpusId": 3757688
            },
            "abstract": "This paper presents a general framework for exploiting the representational capacity of neural networks to approximate complex, nonlinear reward functions in the context of solving the inverse reinforcement learning (IRL) problem. We show in this context that the Maximum Entropy paradigm for IRL lends itself naturally to the efficient training of deep architectures. At test time, the approach leads to a computational complexity independent of the number of demonstrations, which makes it especially well-suited for applications in life-long learning scenarios. Our approach achieves performance commensurate to the state-of-the-art on existing benchmarks while exceeding on an alternative benchmark based on highly varying reward structures. Finally, we extend the basic architecture - which is equivalent to a simplified subclass of Fully Convolutional Neural Networks (FCNNs) with width one - to include larger convolutions in order to eliminate dependency on precomputed spatial features and work on raw input representations.",
            "referenceCount": 33,
            "citationCount": 321,
            "influentialCitationCount": 35,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2015-07-17",
            "journal": {
                "name": "arXiv: Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Wulfmeier2015MaximumED,\n author = {Markus Wulfmeier and Peter Ondruska and I. Posner},\n journal = {arXiv: Learning},\n title = {Maximum Entropy Deep Inverse Reinforcement Learning},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:df2da95303bedf417c76fa8439844d671fb056da",
            "@type": "ScholarlyArticle",
            "paperId": "df2da95303bedf417c76fa8439844d671fb056da",
            "corpusId": 34388150,
            "url": "https://www.semanticscholar.org/paper/df2da95303bedf417c76fa8439844d671fb056da",
            "title": "Multiobjective Reinforcement Learning: A Comprehensive Overview",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/tsmc/LiuXH15",
                "MAG": "1987725948",
                "DOI": "10.1109/TSMC.2014.2358639",
                "CorpusId": 34388150
            },
            "abstract": "Reinforcement learning (RL) is a powerful paradigm for sequential decision-making under uncertainties, and most RL algorithms aim to maximize some numerical value which represents only one long-term objective. However, multiple long-term objectives are exhibited in many real-world decision and control systems, so recently there has been growing interest in solving multiobjective reinforcement learning (MORL) problems where there are multiple conflicting objectives. The aim of this paper is to present a comprehensive overview of MORL. The basic architecture, research topics, and nai\u0308ve solutions of MORL are introduced at first. Then, several representative MORL approaches and some important directions of recent research are comprehensively reviewed. The relationships between MORL and other related research are also discussed, which include multiobjective optimization, hierarchical RL, and multiagent RL. Moreover, research challenges and open problems of MORL techniques are suggested.",
            "referenceCount": 101,
            "citationCount": 289,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2015-03-01",
            "journal": {
                "name": "IEEE Transactions on Systems, Man, and Cybernetics: Systems",
                "volume": "45"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2015MultiobjectiveRL,\n author = {Chunming Liu and Xin Xu and D. Hu},\n booktitle = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},\n pages = {385-398},\n title = {Multiobjective Reinforcement Learning: A Comprehensive Overview},\n volume = {45},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a70d75021fc68ed7606502488be0baa080733192",
            "@type": "ScholarlyArticle",
            "paperId": "a70d75021fc68ed7606502488be0baa080733192",
            "corpusId": 18446484,
            "url": "https://www.semanticscholar.org/paper/a70d75021fc68ed7606502488be0baa080733192",
            "title": "Reinforcement Learning in Multidimensional Environments Relies on Attention Mechanisms",
            "venue": "Journal of Neuroscience",
            "publicationVenue": {
                "id": "urn:research:52edfc09-8d65-4876-8560-84530d7259b9",
                "name": "Journal of Neuroscience",
                "alternate_names": [
                    "The Journal of Neuroscience",
                    "J Neurosci"
                ],
                "issn": "0270-6474",
                "url": "https://www.jneurosci.org/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1581202840",
                "DOI": "10.1523/JNEUROSCI.2978-14.2015",
                "CorpusId": 18446484,
                "PubMed": "26019331"
            },
            "abstract": "In recent years, ideas from the computational field of reinforcement learning have revolutionized the study of learning in the brain, famously providing new, precise theories of how dopamine affects learning in the basal ganglia. However, reinforcement learning algorithms are notorious for not scaling well to multidimensional environments, as is required for real-world learning. We hypothesized that the brain naturally reduces the dimensionality of real-world problems to only those dimensions that are relevant to predicting reward, and conducted an experiment to assess by what algorithms and with what neural mechanisms this \u201crepresentation learning\u201d process is realized in humans. Our results suggest that a bilateral attentional control network comprising the intraparietal sulcus, precuneus, and dorsolateral prefrontal cortex is involved in selecting what dimensions are relevant to the task at hand, effectively updating the task representation through trial and error. In this way, cortical attention mechanisms interact with learning in the basal ganglia to solve the \u201ccurse of dimensionality\u201d in reinforcement learning.",
            "referenceCount": 82,
            "citationCount": 284,
            "influentialCitationCount": 25,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.jneurosci.org/content/jneuro/35/21/8145.full.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-05-27",
            "journal": {
                "name": "The Journal of Neuroscience",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{Niv2015ReinforcementLI,\n author = {Y. Niv and R. Daniel and A. Geana and S. Gershman and Yuan Chang Leong and Angela Radulescu and Robert C. Wilson},\n booktitle = {Journal of Neuroscience},\n journal = {The Journal of Neuroscience},\n pages = {8145 - 8157},\n title = {Reinforcement Learning in Multidimensional Environments Relies on Attention Mechanisms},\n volume = {35},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:282a380fb5ac26d99667224cef8c630f6882704f",
            "@type": "ScholarlyArticle",
            "paperId": "282a380fb5ac26d99667224cef8c630f6882704f",
            "corpusId": 13623631,
            "url": "https://www.semanticscholar.org/paper/282a380fb5ac26d99667224cef8c630f6882704f",
            "title": "Learning to reinforcement learn",
            "venue": "Annual Meeting of the Cognitive Science Society",
            "publicationVenue": {
                "id": "urn:research:9c06885c-ecb6-4a76-ba1e-fbad53521efd",
                "name": "Annual Meeting of the Cognitive Science Society",
                "alternate_names": [
                    "CogSci",
                    "Annu Meet Cogn Sci Soc"
                ],
                "issn": null,
                "url": "http://cognitivesciencesociety.org/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2550182557",
                "DBLP": "journals/corr/WangKTSLMBKB16",
                "ArXiv": "1611.05763",
                "CorpusId": 13623631
            },
            "abstract": "In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.",
            "referenceCount": 52,
            "citationCount": 847,
            "influentialCitationCount": 104,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Psychology",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.05763"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2016LearningTR,\n author = {Jane X. Wang and Z. Kurth-Nelson and Hubert Soyer and Joel Z. Leibo and Dhruva Tirumala and R. Munos and C. Blundell and D. Kumaran and M. Botvinick},\n booktitle = {Annual Meeting of the Cognitive Science Society},\n journal = {ArXiv},\n title = {Learning to reinforcement learn},\n volume = {abs/1611.05763},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:49da57cf2900cd50d64a6d63d45e1bccd454fcbb",
            "@type": "ScholarlyArticle",
            "paperId": "49da57cf2900cd50d64a6d63d45e1bccd454fcbb",
            "corpusId": 16553385,
            "url": "https://www.semanticscholar.org/paper/49da57cf2900cd50d64a6d63d45e1bccd454fcbb",
            "title": "Deep Reinforcement Learning in Parameterized Action Space",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/corr/HausknechtS15a",
                "MAG": "2950193671",
                "ArXiv": "1511.04143",
                "CorpusId": 16553385
            },
            "abstract": "Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces. However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces. To fill this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which features a small set of discrete action types, each of which is parameterized with continuous variables. The best learned agent can score goals more reliably than the 2012 RoboCup champion agent. As such, this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space MDPs.",
            "referenceCount": 25,
            "citationCount": 262,
            "influentialCitationCount": 29,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-13",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1511.04143"
            },
            "citationStyles": {
                "bibtex": "@Article{Hausknecht2015DeepRL,\n author = {Matthew J. Hausknecht and P. Stone},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Deep Reinforcement Learning in Parameterized Action Space},\n volume = {abs/1511.04143},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0ca51467d034828689a723b6faeccbbbcada5485",
            "@type": "ScholarlyArticle",
            "paperId": "0ca51467d034828689a723b6faeccbbbcada5485",
            "corpusId": 8104710,
            "url": "https://www.semanticscholar.org/paper/0ca51467d034828689a723b6faeccbbbcada5485",
            "title": "An Alternative Softmax Operator for Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2963169817",
                "DBLP": "conf/icml/AsadiL17",
                "CorpusId": 8104710
            },
            "abstract": "A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one's weight behind a single maximum utility decision. The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior. In this work, we study a differentiable softmax operator that, among other properties, is a non-expansion ensuring a convergent behavior in learning and planning. We introduce a variant of SARSA algorithm that, by utilizing the new operator, computes a Boltzmann policy with a state-dependent temperature parameter. We show that the algorithm is convergent and that it performs favorably in practice.",
            "referenceCount": 36,
            "citationCount": 159,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-12-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Asadi2016AnAS,\n author = {Kavosh Asadi and M. Littman},\n booktitle = {International Conference on Machine Learning},\n pages = {243-252},\n title = {An Alternative Softmax Operator for Reinforcement Learning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bf807c251962a694499b29938bdc54e716ca2dda",
            "@type": "ScholarlyArticle",
            "paperId": "bf807c251962a694499b29938bdc54e716ca2dda",
            "corpusId": 3138852,
            "url": "https://www.semanticscholar.org/paper/bf807c251962a694499b29938bdc54e716ca2dda",
            "title": "Reinforcement learning improves behaviour from evaluative feedback",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/nature/Littman15",
                "MAG": "1589747210",
                "DOI": "10.1038/nature14540",
                "CorpusId": 3138852,
                "PubMed": "26017443"
            },
            "abstract": null,
            "referenceCount": 65,
            "citationCount": 273,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2015-05-27",
            "journal": {
                "name": "Nature",
                "volume": "521"
            },
            "citationStyles": {
                "bibtex": "@Article{Littman2015ReinforcementLI,\n author = {M. Littman},\n booktitle = {Nature},\n journal = {Nature},\n pages = {445-451},\n title = {Reinforcement learning improves behaviour from evaluative feedback},\n volume = {521},\n year = {2015}\n}\n"
            }
        }
    }
]