[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2728ef33147b97ec9c38f5863c569f5dd207c115",
            "@type": "ScholarlyArticle",
            "paperId": "2728ef33147b97ec9c38f5863c569f5dd207c115",
            "corpusId": 15083675,
            "url": "https://www.semanticscholar.org/paper/2728ef33147b97ec9c38f5863c569f5dd207c115",
            "title": "From Pixels to Torques: Policy Learning with Deep Dynamical Models",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2950643525",
                "ArXiv": "1502.02251",
                "DBLP": "journals/corr/WahlstromSD15",
                "CorpusId": 15083675
            },
            "abstract": "Data-efficient learning in continuous state-action spaces using very high-dimensional observations remains a key challenge in developing fully autonomous systems. In this paper, we consider one instance of this challenge, the pixels-totorques problem, where an agent must learn a closed-loop control policy from pixel information only. We introduce a data-efficient, model-based reinforcement learning algorithm that learns such a closed-loop policy directly from pixel information. The key ingredient is a deep dynamical model that uses deep autoencoders to learn a low-dimensional embedding of images jointly with a prediction model in this low-dimensional feature space. This joint learning ensures that not only static properties of the data are accounted for, but also dynamic properties. This is crucial for long-term predictions, which lie at the core of the adaptive model predictive control strategy that we use for closedloop control. Compared to state-of-the-art reinforcement learning methods, our approach learns quickly, scales to high-dimensional state spaces and facilitates fully autonomous learning from pixels to torques.",
            "referenceCount": 40,
            "citationCount": 169,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-02-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1502.02251"
            },
            "citationStyles": {
                "bibtex": "@Article{Wahlstrom2015FromPT,\n author = {Niklas Wahlstrom and Thomas Bo Sch\u00f6n and M. Deisenroth},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {From Pixels to Torques: Policy Learning with Deep Dynamical Models},\n volume = {abs/1502.02251},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:518f370c246c74f897059c7f89d374603aa77087",
            "@type": "ScholarlyArticle",
            "paperId": "518f370c246c74f897059c7f89d374603aa77087",
            "corpusId": 144714471,
            "url": "https://www.semanticscholar.org/paper/518f370c246c74f897059c7f89d374603aa77087",
            "title": "Reinforcement, expectancy, and learning.",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1972,
            "externalIds": {
                "MAG": "2067238826",
                "DOI": "10.1037/H0033120",
                "CorpusId": 144714471
            },
            "abstract": null,
            "referenceCount": 29,
            "citationCount": 1050,
            "influentialCitationCount": 29,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1972-09-01",
            "journal": {
                "name": "Psychological Review",
                "volume": "79"
            },
            "citationStyles": {
                "bibtex": "@Article{Bolles1972ReinforcementEA,\n author = {R. Bolles},\n journal = {Psychological Review},\n pages = {394-409},\n title = {Reinforcement, expectancy, and learning.},\n volume = {79},\n year = {1972}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:10bc3b9bf71c4448ea0f4bec441403c0e50a0691",
            "@type": "ScholarlyArticle",
            "paperId": "10bc3b9bf71c4448ea0f4bec441403c0e50a0691",
            "corpusId": 47125649,
            "url": "https://www.semanticscholar.org/paper/10bc3b9bf71c4448ea0f4bec441403c0e50a0691",
            "title": "Machine Learning - An Algorithmic Perspective",
            "venue": "Chapman and Hall / CRC machine learning and pattern recognition series",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "1549998098",
                "DBLP": "books/daglib/0029547",
                "CorpusId": 47125649
            },
            "abstract": "Written in an easily accessible style, this book provides the ideal blend of theory and practical, applicable knowledge. It covers neural networks, graphical models, reinforcement learning, evolutionary algorithms, dimensionality reduction methods, and the important area of optimization. It treads the fine line between adequate academic rigor and overwhelming students with equations and mathematical concepts. The author includes examples based on widely available datasets and practical and theoretical problems to test understanding and application of the material. The book describes algorithms with code examples backed up by a website that provides working implementations in Python.",
            "referenceCount": 4,
            "citationCount": 1052,
            "influentialCitationCount": 125,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2009-04-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Marsland2009MachineL,\n author = {S. Marsland},\n booktitle = {Chapman and Hall / CRC machine learning and pattern recognition series},\n pages = {I-XVI, 1-390},\n title = {Machine Learning - An Algorithmic Perspective},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9852a252032c65e501a4e4e5ec226c85f0b3c2fb",
            "@type": "ScholarlyArticle",
            "paperId": "9852a252032c65e501a4e4e5ec226c85f0b3c2fb",
            "corpusId": 15290958,
            "url": "https://www.semanticscholar.org/paper/9852a252032c65e501a4e4e5ec226c85f0b3c2fb",
            "title": "Experience\u2010weighted Attraction Learning in Normal Form Games",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "2096348520",
                "DOI": "10.1111/1468-0262.00054",
                "CorpusId": 15290958
            },
            "abstract": "In \u2018experience-weighted attraction\u2019 (EWA) learning, strategies have attractions that reflect initial predispositions, are updated based on payoff experience, and determine choice probabilities according to some rule (e.g., logit). A key feature is a parameter \u03b4 that weights the strength of hypothetical reinforcement of strategies that were not chosen according to the payoff they would have yielded, relative to reinforcement of chosen strategies according to received payoffs. The other key features are two discount rates, \u03c6 and \u03c1, which separately discount previous attractions, and an experience weight. EWA includes reinforcement learning and weighted fictitious play (belief learning) as special cases, and hybridizes their key elements. When \u03b4= 0 and \u03c1= 0, cumulative choice reinforcement results. When \u03b4= 1 and \u03c1=\u03c6, levels of reinforcement of strategies are exactly the same as expected payoffs given weighted fictitious play beliefs. Using three sets of experimental data, parameter estimates of the model were calibrated on part of the data and used to predict a holdout sample. Estimates of \u03b4 are generally around .50, \u03c6 around .8 \u2212 1, and \u03c1 varies from 0 to \u03c6. Reinforcement and belief-learning special cases are generally rejected in favor of EWA, though belief models do better in some constant-sum games. EWA is able to combine the best features of previous approaches, allowing attractions to begin and grow flexibly as choice reinforcement does, but reinforcing unchosen strategies substantially as belief-based models implicitly do.",
            "referenceCount": 64,
            "citationCount": 1492,
            "influentialCitationCount": 105,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://authors.library.caltech.edu/22107/1/1468-0262.00054%5B1%5D.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1999-07-01",
            "journal": {
                "name": "Econometrica",
                "volume": "67"
            },
            "citationStyles": {
                "bibtex": "@Article{Camerer1999ExperienceweightedAL,\n author = {Colin Camerer and Teck-Hua Ho},\n journal = {Econometrica},\n pages = {827-874},\n title = {Experience\u2010weighted Attraction Learning in Normal Form Games},\n volume = {67},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2a1724427b9c473e73f6abff648e7416269d3a68",
            "@type": "ScholarlyArticle",
            "paperId": "2a1724427b9c473e73f6abff648e7416269d3a68",
            "corpusId": 19706,
            "url": "https://www.semanticscholar.org/paper/2a1724427b9c473e73f6abff648e7416269d3a68",
            "title": "Cooperative Multi-Agent Learning: The State of the Art",
            "venue": "Autonomous Agents and Multi-Agent Systems",
            "publicationVenue": {
                "id": "urn:research:86b22730-8744-40a9-ae4d-d21830dfb282",
                "name": "Autonomous Agents and Multi-Agent Systems",
                "alternate_names": [
                    "Auton Agent Multi-agent Syst"
                ],
                "issn": "1387-2532",
                "url": "https://www.springer.com/computer/ai/journal/10458"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2107544712",
                "DBLP": "journals/aamas/PanaitL05",
                "DOI": "10.1007/s10458-005-2631-2",
                "CorpusId": 19706
            },
            "abstract": null,
            "referenceCount": 349,
            "citationCount": 1316,
            "influentialCitationCount": 65,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.gmu.edu/~lpanait/papers/panait05cmasl.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2005-11-01",
            "journal": {
                "name": "Autonomous Agents and Multi-Agent Systems",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Panait2005CooperativeML,\n author = {Liviu Panait and S. Luke},\n booktitle = {Autonomous Agents and Multi-Agent Systems},\n journal = {Autonomous Agents and Multi-Agent Systems},\n pages = {387-434},\n title = {Cooperative Multi-Agent Learning: The State of the Art},\n volume = {11},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ddb8ff88fc687608c2ffba35ef0e051eccdd658c",
            "@type": "ScholarlyArticle",
            "paperId": "ddb8ff88fc687608c2ffba35ef0e051eccdd658c",
            "corpusId": 13933921,
            "url": "https://www.semanticscholar.org/paper/ddb8ff88fc687608c2ffba35ef0e051eccdd658c",
            "title": "Reinforcement and Reversal Learning in First-Episode Psychosis",
            "venue": "Schizophrenia bulletin",
            "publicationVenue": {
                "id": "urn:research:ac88e0cc-a51a-43d1-b09c-518195f3f8a2",
                "name": "Schizophrenia bulletin",
                "alternate_names": [
                    "Schizophrenia Bulletin",
                    "Schizophr bull",
                    "Schizophr Bull"
                ],
                "issn": "1787-9965",
                "url": "http://schizophreniabulletin.oxfordjournals.org/"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2106995489",
                "PubMedCentral": "2518639",
                "DOI": "10.1093/schbul/sbn078",
                "CorpusId": 13933921,
                "PubMed": "18628272"
            },
            "abstract": "Background: Abnormalities in reinforcement learning and reversal learning have been reported in psychosis, possibly secondary to subcortical dopamine abnormalities. Methods: We studied simple discrimination (SD) learning and reversal learning in a sample of 119 first-episode psychosis patients from the Cambridge early psychosis service (CAMEO) and 107 control participants. We used data on reinforcement learning and reversal learning extracted from the Cambridge Neuropsychological Test Automated Battery Intradimensional-Extradimensional shift task, which measures cognitive flexibility but also involves simple reinforcement learning (SD learning) and reversal learning stages. We also gathered diagnostic information to examine whether there were any differences between patients ultimately diagnosed with schizophrenia-spectrum disorders and those diagnosed with affective psychosis. Results: Psychosis patients demonstrated deficits in simple reinforcement learning (SD learning) and in reversal learning, with no differences between affective psychosis and schizophrenia-spectrum psychosis. There was a significant modest correlation between reversal errors and negative symptoms (Spearman \u03c1 = 0.3, P = .02). Conclusions: There are reinforcement learning abnormalities in first-episode psychosis, which correlate with negative symptoms, suggesting a possible role for orbitofrontal cortex and ventral striatal pathology in the pathogenesis of motivational deficits in psychosis.",
            "referenceCount": 63,
            "citationCount": 150,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://academic.oup.com/schizophreniabulletin/article-pdf/34/5/848/5325252/sbn078.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-07-15",
            "journal": {
                "name": "Schizophrenia Bulletin",
                "volume": "34"
            },
            "citationStyles": {
                "bibtex": "@Article{Murray2008ReinforcementAR,\n author = {G. Murray and F. Cheng and L. Clark and J. Barnett and A. Blackwell and P. C. Fletcher and T. Robbins and E. Bullmore and Peter B. Jones},\n booktitle = {Schizophrenia bulletin},\n journal = {Schizophrenia Bulletin},\n pages = {848 - 855},\n title = {Reinforcement and Reversal Learning in First-Episode Psychosis},\n volume = {34},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:47ae3eeb90d91b89be8e4e7d833502a9ccc86886",
            "@type": "ScholarlyArticle",
            "paperId": "47ae3eeb90d91b89be8e4e7d833502a9ccc86886",
            "corpusId": 5730628,
            "url": "https://www.semanticscholar.org/paper/47ae3eeb90d91b89be8e4e7d833502a9ccc86886",
            "title": "Model-based and model-free mechanisms of human motor learning.",
            "venue": "Advances in Experimental Medicine and Biology",
            "publicationVenue": {
                "id": "urn:research:a8009bad-664b-465f-9fc2-8656bc637bc4",
                "name": "Advances in Experimental Medicine and Biology",
                "alternate_names": [
                    "Adv Exp Med Biology"
                ],
                "issn": "0065-2598",
                "url": "https://www.springer.com/series/5584"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2181409883",
                "DOI": "10.1007/978-1-4614-5465-6_1",
                "CorpusId": 5730628,
                "PubMed": "23296478"
            },
            "abstract": null,
            "referenceCount": 91,
            "citationCount": 210,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc3570165?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Advances in experimental medicine and biology",
                "volume": "782"
            },
            "citationStyles": {
                "bibtex": "@Article{Haith2013ModelbasedAM,\n author = {A. Haith and J. Krakauer},\n booktitle = {Advances in Experimental Medicine and Biology},\n journal = {Advances in experimental medicine and biology},\n pages = {\n          1-21\n        },\n title = {Model-based and model-free mechanisms of human motor learning.},\n volume = {782},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d766bffc357127e0dc86dd69561d5aeb520d6f4c",
            "@type": "ScholarlyArticle",
            "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
            "corpusId": 246426909,
            "url": "https://www.semanticscholar.org/paper/d766bffc357127e0dc86dd69561d5aeb520d6f4c",
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "conf/nips/Ouyang0JAWMZASR22",
                "ArXiv": "2203.02155",
                "DOI": "10.48550/arXiv.2203.02155",
                "CorpusId": 246426909
            },
            "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
            "referenceCount": 81,
            "citationCount": 3731,
            "influentialCitationCount": 529,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2203.02155",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-03-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2203.02155"
            },
            "citationStyles": {
                "bibtex": "@Article{Ouyang2022TrainingLM,\n author = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and J. Schulman and Jacob Hilton and Fraser Kelton and Luke E. Miller and Maddie Simens and Amanda Askell and P. Welinder and P. Christiano and J. Leike and Ryan J. Lowe},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Training language models to follow instructions with human feedback},\n volume = {abs/2203.02155},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:66618e819c5fa4d5ee7ea81f8563eb57ade36c39",
            "@type": "ScholarlyArticle",
            "paperId": "66618e819c5fa4d5ee7ea81f8563eb57ade36c39",
            "corpusId": 16791350,
            "url": "https://www.semanticscholar.org/paper/66618e819c5fa4d5ee7ea81f8563eb57ade36c39",
            "title": "Learning Through Reinforcement and Replicator Dynamics",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1997,
            "externalIds": {
                "MAG": "1995622844",
                "DOI": "10.1006/JETH.1997.2319",
                "CorpusId": 16791350
            },
            "abstract": "This paper considers a version of Bush and Mosteller's stochastic learning theory in the context of games. We compare this model of learning to a model of biological evolution. The purpose is to investigate analogies between learning and evolution. We and that in the continuous time limit the biological model coincides with the deterministic, continuous time replicator process. We give conditions under which the same is true for the learning model. For the case that these conditions do not hold, we show that the replicator process continues to play an important role in characterising the continuous time limit of the learning model, but that a di\u00aeerent e\u00aeect (\\Probability Matching\") enters as well.(This abstract was borrowed from another version of this item.)",
            "referenceCount": 41,
            "citationCount": 725,
            "influentialCitationCount": 55,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.dklevine.com/archive/refs4380.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1997-11-01",
            "journal": {
                "name": "Journal of Economic Theory",
                "volume": "77"
            },
            "citationStyles": {
                "bibtex": "@Article{B\u00f6rgers1997LearningTR,\n author = {T. B\u00f6rgers and R. Sarin},\n journal = {Journal of Economic Theory},\n pages = {1-14},\n title = {Learning Through Reinforcement and Replicator Dynamics},\n volume = {77},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9c6933244bcf31ce8a05a1e4ee0ec6d015416616",
            "@type": "ScholarlyArticle",
            "paperId": "9c6933244bcf31ce8a05a1e4ee0ec6d015416616",
            "corpusId": 220758222,
            "url": "https://www.semanticscholar.org/paper/9c6933244bcf31ce8a05a1e4ee0ec6d015416616",
            "title": "Independent reinforcement learners in cooperative Markov games: a survey regarding coordination problems",
            "venue": "Knowledge engineering review (Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "journals/ker/MatignonLF12",
                "MAG": "2096145798",
                "DOI": "10.1017/S0269888912000057",
                "CorpusId": 220758222
            },
            "abstract": "Abstract In the framework of fully cooperative multi-agent systems, independent (non-communicative) agents that learn by reinforcement must overcome several difficulties to manage to coordinate. This paper identifies several challenges responsible for the non-coordination of independent agents: Pareto-selection, non-stationarity, stochasticity, alter-exploration and shadowed equilibria. A selection of multi-agent domains is classified according to those challenges: matrix games, Boutilier's coordination game, predators pursuit domains and a special multi-state game. Moreover, the performance of a range of algorithms for independent reinforcement learners is evaluated empirically. Those algorithms are Q-learning variants: decentralized Q-learning, distributed Q-learning, hysteretic Q-learning, recursive frequency maximum Q-value and win-or-learn fast policy hill climbing. An overview of the learning algorithms\u2019 strengths and weaknesses against each challenge concludes the paper and can serve as a basis for choosing the appropriate algorithm for a new domain. Furthermore, the distilled challenges may assist in the design of new learning algorithms that overcome these problems and achieve higher performance in multi-agent applications.",
            "referenceCount": 66,
            "citationCount": 356,
            "influentialCitationCount": 35,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://hal.archives-ouvertes.fr/hal-00720669/file/Matignon2012independent.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2012-02-01",
            "journal": {
                "name": "The Knowledge Engineering Review",
                "volume": "27"
            },
            "citationStyles": {
                "bibtex": "@Article{Matignon2012IndependentRL,\n author = {L. Matignon and G. Laurent and N. L. Fort-Piat},\n booktitle = {Knowledge engineering review (Print)},\n journal = {The Knowledge Engineering Review},\n pages = {1 - 31},\n title = {Independent reinforcement learners in cooperative Markov games: a survey regarding coordination problems},\n volume = {27},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1a4999c918c6206cd9804c48f7dce1bac6ec5b4a",
            "@type": "ScholarlyArticle",
            "paperId": "1a4999c918c6206cd9804c48f7dce1bac6ec5b4a",
            "corpusId": 3137380,
            "url": "https://www.semanticscholar.org/paper/1a4999c918c6206cd9804c48f7dce1bac6ec5b4a",
            "title": "Learning to trade via direct reinforcement",
            "venue": "IEEE Trans. Neural Networks",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2169015875",
                "DBLP": "journals/tnn/MoodyS01",
                "DOI": "10.1109/72.935097",
                "CorpusId": 3137380,
                "PubMed": "18249919"
            },
            "abstract": "We present methods for optimizing portfolios, asset allocations, and trading systems based on direct reinforcement (DR). In this approach, investment decision-making is viewed as a stochastic control problem, and strategies are discovered directly. We present an adaptive algorithm called recurrent reinforcement learning (RRL) for discovering investment policies. The need to build forecasting models is eliminated, and better trading performance is obtained. The direct reinforcement approach differs from dynamic programming and reinforcement algorithms such as TD-learning and Q-learning, which attempt to estimate a value function for the control problem. We find that the RRL direct reinforcement framework enables a simpler problem representation, avoids Bellman's curse of dimensionality and offers compelling advantages in efficiency. We demonstrate how direct reinforcement can be used to optimize risk-adjusted investment returns (including the differential Sharpe ratio), while accounting for the effects of transaction costs. In extensive simulation work using real financial data, we find that our approach based on RRL produces better trading strategies than systems utilizing Q-learning (a value function method). Real-world applications include an intra-daily currency trader and a monthly asset allocation system for the S&P 500 Stock Index and T-Bills.",
            "referenceCount": 52,
            "citationCount": 415,
            "influentialCitationCount": 35,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-07-01",
            "journal": {
                "name": "IEEE transactions on neural networks",
                "volume": "12 4"
            },
            "citationStyles": {
                "bibtex": "@Article{Moody2001LearningTT,\n author = {J. Moody and M. Saffell},\n booktitle = {IEEE Trans. Neural Networks},\n journal = {IEEE transactions on neural networks},\n pages = {\n          875-89\n        },\n title = {Learning to trade via direct reinforcement},\n volume = {12 4},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9a33f21c90b29a797138611e990240f7abcbdbb7",
            "@type": "ScholarlyArticle",
            "paperId": "9a33f21c90b29a797138611e990240f7abcbdbb7",
            "corpusId": 37248049,
            "url": "https://www.semanticscholar.org/paper/9a33f21c90b29a797138611e990240f7abcbdbb7",
            "title": "Reinforcement and learning",
            "venue": "Evolutionary Ecology",
            "publicationVenue": {
                "id": "urn:research:ad3cee6c-b49e-4951-9015-5c211cc902cf",
                "name": "Evolutionary Ecology",
                "alternate_names": [
                    "Evol Ecol"
                ],
                "issn": "0269-7653",
                "url": "http://www.springer.com/life+sci/journal/10682"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2046887854",
                "DOI": "10.1007/s10682-007-9188-2",
                "CorpusId": 37248049
            },
            "abstract": null,
            "referenceCount": 99,
            "citationCount": 100,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Biology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Evolutionary Ecology",
                "volume": "23"
            },
            "citationStyles": {
                "bibtex": "@Article{Servedio2007ReinforcementAL,\n author = {M. Servedio and S. S\u00e6ther and Glenn-Peter S\u00e6tre},\n booktitle = {Evolutionary Ecology},\n journal = {Evolutionary Ecology},\n pages = {109-123},\n title = {Reinforcement and learning},\n volume = {23},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:384613b85ddb6e990395fcac0870819cddcdf5e0",
            "@type": "ScholarlyArticle",
            "paperId": "384613b85ddb6e990395fcac0870819cddcdf5e0",
            "corpusId": 6323590,
            "url": "https://www.semanticscholar.org/paper/384613b85ddb6e990395fcac0870819cddcdf5e0",
            "title": "Fuzzy inference system learning by reinforcement methods",
            "venue": "IEEE Trans. Syst. Man Cybern. Part C",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2305205647",
                "DBLP": "journals/tsmc/Jouffe98",
                "DOI": "10.1109/5326.704563",
                "CorpusId": 6323590
            },
            "abstract": "Fuzzy Actor-Critic Learning (FACL) and Fuzzy Q-Learning (FQL) are reinforcement learning methods based on dynamic programming (DP) principles. In the paper, they are used to tune online the conclusion part of fuzzy inference systems (FIS). The only information available for learning is the system feedback, which describes in terms of reward and punishment the task the fuzzy agent has to realize. At each time step, the agent receives a reinforcement signal according to the last action it has performed in the previous state. The problem involves optimizing not only the direct reinforcement, but also the total amount of reinforcements the agent can receive in the future. To illustrate the use of these two learning methods, we first applied them to a problem that involves finding a fuzzy controller to drive a boat from one bank to another, across a river with a strong nonlinear current. Then, we used the well known Cart-Pole Balancing and Mountain-Car problems to be able to compare our methods to other reinforcement learning methods and focus on important characteristic aspects of FACL and FQL. We found that the genericity of our methods allows us to learn every kind of reinforcement learning problem (continuous states, discrete/continuous actions, various type of reinforcement functions). The experimental studies also show the superiority of these methods with respect to the other related methods we can find in the literature.",
            "referenceCount": 41,
            "citationCount": 391,
            "influentialCitationCount": 42,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1998-08-01",
            "journal": {
                "name": "IEEE Trans. Syst. Man Cybern. Part C",
                "volume": "28"
            },
            "citationStyles": {
                "bibtex": "@Article{Jouffe1998FuzzyIS,\n author = {L. Jouffe},\n booktitle = {IEEE Trans. Syst. Man Cybern. Part C},\n journal = {IEEE Trans. Syst. Man Cybern. Part C},\n pages = {338-355},\n title = {Fuzzy inference system learning by reinforcement methods},\n volume = {28},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cf593b3c8a69fad38f755e8783e768ac545319bc",
            "@type": "ScholarlyArticle",
            "paperId": "cf593b3c8a69fad38f755e8783e768ac545319bc",
            "corpusId": 264607488,
            "url": "https://www.semanticscholar.org/paper/cf593b3c8a69fad38f755e8783e768ac545319bc",
            "title": "Chapter 3: Hippocampal lesions facilitate instrumental learning with delayed reinforcement but induce impulsive choice in rats",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2006,
            "externalIds": {
                "CorpusId": 264607488
            },
            "abstract": "Background: Animals must frequently act to influence the world even when the reinforcing outcomes of their actions are delayed. Learning with action\u2013outcome delays is a complex problem, and little is known of the neural mechanisms that bridge such delays. When outcomes are delayed, they may be attributed to (or associated with) the action that caused them, or mistakenly attributed to other stimuli, such as the environmental context. Consequently, animals that are poor at forming context\u2013outcome associations might learn action\u2013outcome associations better with delayed reinforcement than normal animals. The hippocampus contributes to the representation of environmental context, being required for aspects of contextual conditioning. It was therefore hypothesized that animals with hippocampal lesions would be better than normal animals at learning to act on the basis of delayed reinforcement. These experiments tested the ability of H-lesioned rats to learn a free-operant instrumental response using delayed reinforcement, and what is potentially a related ability\u2014the ability to exhibit self-controlled choice, or to sacrifice an immediate, small reward in order to obtain a delayed but larger reward. Results: Rats with sham or excitotoxic hippocampal lesions acquired an instrumental response with different delays (0, 10, or 20 s) between the response and reinforcer delivery. These delays retarded learning in normal rats. Hlesioned rats responded slightly less than sham-operated controls in the absence of delays, but they became better at learning (relative to shams) as the delays increased; delays impaired learning less in H-lesioned rats than in shams. In contrast, lesioned rats exhibited impulsive choice, preferring an immediate, small reward to a delayed, larger reward, even though they preferred the large reward when it was not delayed. Conclusions: These results support the view that the hippocampus hinders action\u2013outcome learning with delayed outcomes, perhaps because it promotes the formation of context\u2013outcome associations instead. However, although lesioned rats were better at learning with delayed reinforcement, they were worse at choosing it, suggesting that selfcontrolled choice and learning with delayed reinforcement tax different psychological processes.",
            "referenceCount": 0,
            "citationCount": 163,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{None,\n title = {Chapter 3: Hippocampal lesions facilitate instrumental learning with delayed reinforcement but induce impulsive choice in rats},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3ac80af0463d3b1979e579b42d0d6c8df9d09ac8",
            "@type": "ScholarlyArticle",
            "paperId": "3ac80af0463d3b1979e579b42d0d6c8df9d09ac8",
            "corpusId": 60765384,
            "url": "https://www.semanticscholar.org/paper/3ac80af0463d3b1979e579b42d0d6c8df9d09ac8",
            "title": "Reinforcement structure/parameter learning for neural-network-based fuzzy logic control systems",
            "venue": "[Proceedings 1993] Second IEEE International Conference on Fuzzy Systems",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1993,
            "externalIds": {
                "MAG": "1769711052",
                "DBLP": "journals/tfs/LinL94",
                "DOI": "10.1109/FUZZY.1993.327458",
                "CorpusId": 60765384
            },
            "abstract": "The authors propose a reinforcement neural-network-based fuzzy logic control system (RNN-FLCS) for solving various reinforcement learning problems. RNN-FLCS is best applied to learning environments where obtaining exact training data is expensive. It is constructed by integrating two neural-network-based fuzzy logic controllers (NN-FLCs), each of which is a connectionist model with a feedforward multilayered network developed for the realization of a fuzzy logic controller. One NN-FLC functions as a fuzzy predictor and the other as a fuzzy controller. Using the temporal difference prediction method, the fuzzy predictor can predict the external reinforcement signal and provide a more informative internal reinforcement signal to the fuzzy controller. The fuzzy controller implements a stochastic exploratory algorithm to adapt itself according to the internal reinforcement signal. During the learning process, the RNN-FLCs can construct a fuzzy logic control system automatically and dynamically through a reward-penalty signal or through very simple fuzzy information feedback. Structure learning and parameter learning are performed simultaneously in the two NN-FLCs. Simulation results are presented.<<ETX>>",
            "referenceCount": 17,
            "citationCount": 350,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1993-03-28",
            "journal": {
                "name": "[Proceedings 1993] Second IEEE International Conference on Fuzzy Systems",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lin1993ReinforcementSL,\n author = {Chin-Teng Lin and C.S.G. Lee},\n booktitle = {[Proceedings 1993] Second IEEE International Conference on Fuzzy Systems},\n journal = {[Proceedings 1993] Second IEEE International Conference on Fuzzy Systems},\n pages = {88-93 vol.1},\n title = {Reinforcement structure/parameter learning for neural-network-based fuzzy logic control systems},\n year = {1993}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:082b1f5c791cadef18c4920ecc1396615a3fe7cb",
            "@type": "ScholarlyArticle",
            "paperId": "082b1f5c791cadef18c4920ecc1396615a3fe7cb",
            "corpusId": 27150180,
            "url": "https://www.semanticscholar.org/paper/082b1f5c791cadef18c4920ecc1396615a3fe7cb",
            "title": "Continual learning in reinforcement environments",
            "venue": "GMD-Bericht",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1995,
            "externalIds": {
                "DBLP": "phd/dnb/Ring95",
                "MAG": "1504638679",
                "CorpusId": 27150180
            },
            "abstract": "Continual learning is the constant development of complex behaviors with no nal end in mind. It is the process of learning ever more complicated skills by building on those skills already developed. In order for learning at one stage of development to serve as the foundation for later learning, a continual-learning agent should learn hierarchically. CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development is proposed, described, tested, and evaluated in this dissertation. CHILD accumulates useful behaviors in reinforcement environments by using the Temporal Transition Hierarchies learning algorithm, also derived in the dissertation. This constructive algorithm generates a hierarchical, higher-order neural network that can be used for predicting context-dependent temporal sequences and can learn sequential-task benchmarks more than two orders of magnitude faster than competing neural-network systems. Consequently, CHILD can quickly solve complicated non-Markovian reinforcement-learning tasks and can then transfer its skills to similar but even more complicated tasks, learning these faster still. This continual-learning approach is made possible by the unique properties of Temporal Transition Hierarchies, which allow existing skills to be amended and augmented in precisely the same way that they were constructed in the rst place. Table of",
            "referenceCount": 109,
            "citationCount": 249,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": "255"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Ring1995ContinualLI,\n author = {Mark B. Ring},\n booktitle = {GMD-Bericht},\n pages = {1-127},\n title = {Continual learning in reinforcement environments},\n volume = {255},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a77a460c08f0657373e6ac01c547affaefa1bd9c",
            "@type": "ScholarlyArticle",
            "paperId": "a77a460c08f0657373e6ac01c547affaefa1bd9c",
            "corpusId": 28737713,
            "url": "https://www.semanticscholar.org/paper/a77a460c08f0657373e6ac01c547affaefa1bd9c",
            "title": "VICARIOUS REINFORCEMENT AND IMITATIVE LEARNING.",
            "venue": "Journal of Abnormal Psychology",
            "publicationVenue": {
                "id": "urn:research:29a83d49-6b53-4fcf-b0aa-f59ba8b3e1a6",
                "name": "Journal of Abnormal Psychology",
                "alternate_names": [
                    "J Abnorm Psychol",
                    "The Journal of Abnormal Psychology"
                ],
                "issn": "0021-843X",
                "url": "http://www.apa.org/journals/abn/"
            },
            "year": 1963,
            "externalIds": {
                "MAG": "1973020713",
                "DOI": "10.1037/H0045550",
                "CorpusId": 28737713,
                "PubMed": "14084769"
            },
            "abstract": "VioLit summary: OBJECTIVE: The intent of this article by Bandura et al. was to explore the effects of vicarious reinforcement on conditioned emotional responses among nursery school children with a particular focus on modeling aggression. METHODOLOGY: The authors employed an experimental design by randomly assigning 40 boys and 40 girls from the Stanford University Nursery School into one of 2 experimental groups or one of 2 control groups. The mean age of the children was 51 months, with a range between 38 and 63 months. The children witnessed a 5 minute televised program which was either: 1) The Aggressive Model-Rewarded condition, in which a man's aggressive verbal and physical behavior in a play room were accompanied by a sense of victory and mastery over another man and his environment. 2) The Aggressive Model-Punished condition, in which a man's aggressive verbal and physical behavior in a play room (identical to the scenario in the previous model) were accompanied by severe punishment. 3) The Nonaggressive Model-Control group, in which the objects in the play environment were unchanged, but they were utilized by both characters in vigorous but nonaggressive play. 4) The Nonexposure Control group received no exposure to any of the models. The children were then tested for the incidence of postexposure imitative and nonimitative aggressive responses, while observed in an experimental play room. Several objects were available in the play room that could be used to reenact the aggressive behavior (baton, balls, Bobo dolls, hoola hoop, lasso, dart guns, cars, plastic farm animals) as well as several objects that tended to elicit predominantly nonaggressive responses (blackboard, doll house, cotton-stuffed dolls, building blocks). Each child spent 20 minutes in the play room, with only a noninteractive adult present (to ensure the child's comfort in a strange environment), and was rated by a hidden evaluator. Scores were obtained in predetermined categories of aggression which matched either those in the experimental aggressive models (kicking, lassoing or striking the Bobo doll with a ball or baton, shooting darts at cars or plastic farm animals, or repeating the verbal aggression which was displayed in the models) or those in predetermined categories of nonmatching aggression (punching or slapping the Bobo dolls, crashing the automobiles, acting out physical attacks toward the dolls or animals, etc.). A second rater witnessed 11 of the 80 children to provide an estimate of interrater reliability, and neither rater had knowledge of the treatment conditions to which the children had been exposed. The interscorer reliabilities were high, with product-moment coefficients in the .90s. After the rating each child was asked to discuss the behavior of the characters in the program and to identify the characters he or she preferred to emulate. FINDINGS/DISCUSSION: Many of the children, especially the girls, received low imitation scores and therefore nonparametric techniques were employed to evaluate the significance of the differences which were obtained. Results from the Kruskal-Wallis analysis of variance indicated that imitation was significantly influenced by response-consequences to the model (rewarded versus punished p AUTHORS' RECOMMENDATIONS: Additional research, argued the authors, should be implemented in which the difference between performance and learning can be better assessed, in order to identify the depth of impact from the vicarious transmission, rather than the just behavioral manifestation of the vicarious transmission. (CSPV Abstract - Copyright \u00a9 1992-2007 by the Center for the Study and Prevention of Violence, Institute of Behavioral Science, Regents of the University of Colorado) KW - California KW - Early Childhood KW - Child Aggression KW - Preschool Student KW - Reward KW - Punishment KW - Social Learning KW - Aggression Causes KW - Imitation Language: en",
            "referenceCount": 30,
            "citationCount": 666,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1963-12-01",
            "journal": {
                "name": "Journal of abnormal psychology",
                "volume": "67"
            },
            "citationStyles": {
                "bibtex": "@Article{Bandura1963VICARIOUSRA,\n author = {A. Bandura and D. Ross and S. Ross},\n booktitle = {Journal of Abnormal Psychology},\n journal = {Journal of abnormal psychology},\n pages = {\n          601-7\n        },\n title = {VICARIOUS REINFORCEMENT AND IMITATIVE LEARNING.},\n volume = {67},\n year = {1963}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:042816bfd2c87a303703db96fe32e9af5be951f1",
            "@type": "ScholarlyArticle",
            "paperId": "042816bfd2c87a303703db96fe32e9af5be951f1",
            "corpusId": 74872110,
            "url": "https://www.semanticscholar.org/paper/042816bfd2c87a303703db96fe32e9af5be951f1",
            "title": "Learning with prolonged delay of reinforcement",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1966,
            "externalIds": {
                "MAG": "2012492124",
                "DOI": "10.3758/BF03328311",
                "CorpusId": 74872110
            },
            "abstract": null,
            "referenceCount": 7,
            "citationCount": 727,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.3758/BF03328311.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1966-03-01",
            "journal": {
                "name": "Psychonomic Science",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Garcia1966LearningWP,\n author = {John Garcia and F. Ervin and R. A. Koelling},\n journal = {Psychonomic Science},\n pages = {121-122},\n title = {Learning with prolonged delay of reinforcement},\n volume = {5},\n year = {1966}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a34e7a4a0677dc785ca75f4d7a4dd9214d618999",
            "@type": "ScholarlyArticle",
            "paperId": "a34e7a4a0677dc785ca75f4d7a4dd9214d618999",
            "corpusId": 16629708,
            "url": "https://www.semanticscholar.org/paper/a34e7a4a0677dc785ca75f4d7a4dd9214d618999",
            "title": "Reward learning: Reinforcement, incentives, and expectations",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2000,
            "externalIds": {
                "MAG": "191146409",
                "DOI": "10.1016/S0079-7421(00)80022-5",
                "CorpusId": 16629708
            },
            "abstract": null,
            "referenceCount": 144,
            "citationCount": 335,
            "influentialCitationCount": 32,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Psychology of Learning and Motivation",
                "volume": "40"
            },
            "citationStyles": {
                "bibtex": "@Article{Berridge2000RewardLR,\n author = {K. Berridge},\n journal = {Psychology of Learning and Motivation},\n pages = {223-278},\n title = {Reward learning: Reinforcement, incentives, and expectations},\n volume = {40},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2fc73ec66c8e2428b185e60da5c49b842be5283c",
            "@type": "ScholarlyArticle",
            "paperId": "2fc73ec66c8e2428b185e60da5c49b842be5283c",
            "corpusId": 206924776,
            "url": "https://www.semanticscholar.org/paper/2fc73ec66c8e2428b185e60da5c49b842be5283c",
            "title": "Learning of sequential movements by neural network model with dopamine-like reinforcement signal",
            "venue": "Experimental Brain Research",
            "publicationVenue": {
                "id": "urn:research:8ef50053-d20c-47d4-aee4-50b4be666eac",
                "name": "Experimental Brain Research",
                "alternate_names": [
                    "Exp Brain Res"
                ],
                "issn": "0014-4819",
                "url": "https://www.springer.com/biomed/neuroscience/journal/221"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2076660877",
                "DOI": "10.1007/s002210050467",
                "CorpusId": 206924776,
                "PubMed": "9746140"
            },
            "abstract": null,
            "referenceCount": 29,
            "citationCount": 220,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1998-08-01",
            "journal": {
                "name": "Experimental Brain Research",
                "volume": "121"
            },
            "citationStyles": {
                "bibtex": "@Article{Suri1998LearningOS,\n author = {R. Suri and Wolfram Schultz},\n booktitle = {Experimental Brain Research},\n journal = {Experimental Brain Research},\n pages = {350-354},\n title = {Learning of sequential movements by neural network model with dopamine-like reinforcement signal},\n volume = {121},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c843f2d25f524c5c82d0514a294f0844acedea73",
            "@type": "ScholarlyArticle",
            "paperId": "c843f2d25f524c5c82d0514a294f0844acedea73",
            "corpusId": 13094712,
            "url": "https://www.semanticscholar.org/paper/c843f2d25f524c5c82d0514a294f0844acedea73",
            "title": "Errorless learning: reinforcement contingencies and stimulus control transfer in delayed prompting.",
            "venue": "Journal of Applied Behavior Analysis",
            "publicationVenue": {
                "id": "urn:research:9bd94862-09df-478a-8633-502e68f08f0f",
                "name": "Journal of Applied Behavior Analysis",
                "alternate_names": [
                    "J Appl Behav Anal"
                ],
                "issn": "0021-8855",
                "url": "http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1938-3703"
            },
            "year": 1984,
            "externalIds": {
                "MAG": "2085926075",
                "DOI": "10.1901/JABA.1984.17-175",
                "CorpusId": 13094712,
                "PubMed": "6735950"
            },
            "abstract": "Delayed prompting can produce errorless discrimination learning. There is inherent in the procedure a disparity in reinforcement density which favors unprompted over prompted responses. We used three schedules of reinforcement to investigate the impact of reinforcement probability on transfer of stimulus control. One schedule of reinforcement was equal prior to and following a prompt (CRF/CRF), the second favored unprompted responses (CRF/FR3), and the third favored responses following the prompt (FR3/CRF). Experimental questions concerned the probability of errors, the probability of transfer, and the rate of transfer in the context of delayed prompting. Transfer was accelerated when reinforcement probability favored anticipatory responding. The schedule that favored prompted responses did not prevent a shift to unprompted responding. Errors were infrequent across procedures. Reinforcement probability contributes to but does not entirely determine transfer of stimulus control from a delayed prompt.",
            "referenceCount": 3,
            "citationCount": 185,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc1307932?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1984-06-01",
            "journal": {
                "name": "Journal of applied behavior analysis",
                "volume": "17 2"
            },
            "citationStyles": {
                "bibtex": "@Article{Touchette1984ErrorlessLR,\n author = {P. Touchette and J. Howard},\n booktitle = {Journal of Applied Behavior Analysis},\n journal = {Journal of applied behavior analysis},\n pages = {\n          175-88\n        },\n title = {Errorless learning: reinforcement contingencies and stimulus control transfer in delayed prompting.},\n volume = {17 2},\n year = {1984}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
            "@type": "ScholarlyArticle",
            "paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
            "corpusId": 4704285,
            "url": "https://www.semanticscholar.org/paper/2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
            "title": "Overcoming catastrophic forgetting in neural networks",
            "venue": "Proceedings of the National Academy of Sciences of the United States of America",
            "publicationVenue": {
                "id": "urn:research:bb95bf2e-8383-4748-bf9d-d6906d091085",
                "name": "Proceedings of the National Academy of Sciences of the United States of America",
                "alternate_names": [
                    "PNAS",
                    "PNAS online",
                    "Proceedings of the National Academy of Sciences of the United States of America.",
                    "Proc National Acad Sci",
                    "Proceedings of the National Academy of Sciences",
                    "Proc National Acad Sci u s Am"
                ],
                "issn": "0027-8424",
                "url": "https://www.jstor.org/journal/procnatiacadscie"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1612.00796",
                "MAG": "2560647685",
                "DBLP": "journals/corr/KirkpatrickPRVD16",
                "DOI": "10.1073/pnas.1611835114",
                "CorpusId": 4704285,
                "PubMed": "28292907"
            },
            "abstract": "Significance Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially. The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.",
            "referenceCount": 48,
            "citationCount": 4745,
            "influentialCitationCount": 945,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.pnas.org/content/pnas/114/13/3521.full.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-12-02",
            "journal": {
                "name": "Proceedings of the National Academy of Sciences",
                "volume": "114"
            },
            "citationStyles": {
                "bibtex": "@Article{Kirkpatrick2016OvercomingCF,\n author = {J. Kirkpatrick and Razvan Pascanu and Neil C. Rabinowitz and J. Veness and Guillaume Desjardins and Andrei A. Rusu and Kieran Milan and John Quan and Tiago Ramalho and A. Grabska-Barwinska and D. Hassabis and C. Clopath and D. Kumaran and R. Hadsell},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {3521 - 3526},\n title = {Overcoming catastrophic forgetting in neural networks},\n volume = {114},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
            "@type": "ScholarlyArticle",
            "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
            "corpusId": 28695052,
            "url": "https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
            "title": "Proximal Policy Optimization Algorithms",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2736601468",
                "DBLP": "journals/corr/SchulmanWDRK17",
                "ArXiv": "1707.06347",
                "CorpusId": 28695052
            },
            "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",
            "referenceCount": 14,
            "citationCount": 11054,
            "influentialCitationCount": 3075,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-07-20",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1707.06347"
            },
            "citationStyles": {
                "bibtex": "@Article{Schulman2017ProximalPO,\n author = {J. Schulman and F. Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Proximal Policy Optimization Algorithms},\n volume = {abs/1707.06347},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c2a1a3211d50cfafd47b73c8fdea6ad401132587",
            "@type": "ScholarlyArticle",
            "paperId": "c2a1a3211d50cfafd47b73c8fdea6ad401132587",
            "corpusId": 10283818,
            "url": "https://www.semanticscholar.org/paper/c2a1a3211d50cfafd47b73c8fdea6ad401132587",
            "title": "A neural model of attention, reinforcement and discrimination learning.",
            "venue": "International review of neurobiology",
            "publicationVenue": {
                "id": "urn:research:7bd7067f-ac19-4eec-8a0c-31d89789afa0",
                "name": "International review of neurobiology",
                "alternate_names": [
                    "Int Rev Neurobiol",
                    "International Review of Neurobiology",
                    "Int rev neurobiol"
                ],
                "issn": "0074-7742",
                "url": "http://www.sciencedirect.com/science/bookseries/00747742"
            },
            "year": 1975,
            "externalIds": {
                "MAG": "2108939591",
                "DOI": "10.1007/978-94-009-7758-7_6",
                "CorpusId": 10283818,
                "PubMed": "1107246"
            },
            "abstract": null,
            "referenceCount": 49,
            "citationCount": 337,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "International review of neurobiology",
                "volume": "18"
            },
            "citationStyles": {
                "bibtex": "@Article{Grossberg1975ANM,\n author = {S. Grossberg},\n booktitle = {International review of neurobiology},\n journal = {International review of neurobiology},\n pages = {\n          263-327\n        },\n title = {A neural model of attention, reinforcement and discrimination learning.},\n volume = {18},\n year = {1975}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bbe29d36fb4066f2708400d519b66fa909cb0490",
            "@type": "ScholarlyArticle",
            "paperId": "bbe29d36fb4066f2708400d519b66fa909cb0490",
            "corpusId": 23950810,
            "url": "https://www.semanticscholar.org/paper/bbe29d36fb4066f2708400d519b66fa909cb0490",
            "title": "The Nature of Reinforcement in Cerebellar Learning",
            "venue": "Neurobiology of Learning and Memory",
            "publicationVenue": {
                "id": "urn:research:029ae4a0-5391-4160-8380-a1555528e9f4",
                "name": "Neurobiology of Learning and Memory",
                "alternate_names": [
                    "Neurobiol Learn Mem"
                ],
                "issn": "1074-7427",
                "url": "https://www.journals.elsevier.com/neurobiology-of-learning-and-memory"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2034424502",
                "DOI": "10.1006/nlme.1998.3845",
                "CorpusId": 23950810,
                "PubMed": "9753594"
            },
            "abstract": "In a now classic study, W. J. Brogden and W. H. Gantt (1942, American Journal of Physiology, 119, 277-278) demonstrated that movements (limbs, head, eyelid) elicited by direct electrical stimulation of certain regions of the cerebellum (particularly the ansiform lobe) could be trained to respond to neutral auditory or visual conditioned stimuli with appropriate pairing. In recent work we have replicated these results in detail and presented considerable evidence that the reinforcing or \"teaching\" pathway so activated for the learning of discrete movements is the inferior olive-climbing fiber projection system to the cerebellum. Very strong evidence now indicates that the memory traces for this \"skilled response\" learning are formed and stored in the cerebellum. The climbing fiber system and inhibitory pathway from the interpositus nucleus to the inferior olive appear to form a neural instantiation of the Resorla-Wagner formulation of classical conditioning and accounts for the \"cognitive\" phenomenon of blocking. It is concluded that reinforcement in this form of learning is not due simply to contiguity/contingency or to unconditioned stimulus aversiveness, per se, but rather to activation of a particular brain circuit, here the climbing fiber system, a circumstance that may apply to other forms of learning, with other reinforcement circuits, as well.",
            "referenceCount": 86,
            "citationCount": 96,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "1998-07-01",
            "journal": {
                "name": "Neurobiology of Learning and Memory",
                "volume": "70"
            },
            "citationStyles": {
                "bibtex": "@Article{Thompson1998TheNO,\n author = {Richard F. Thompson and Judith K. Thompson and J. J. Kim and D. Krupa and P. Shinkman},\n booktitle = {Neurobiology of Learning and Memory},\n journal = {Neurobiology of Learning and Memory},\n pages = {150-176},\n title = {The Nature of Reinforcement in Cerebellar Learning},\n volume = {70},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c1f457e31b611da727f9aef76c283a18157dfa83",
            "@type": "ScholarlyArticle",
            "paperId": "c1f457e31b611da727f9aef76c283a18157dfa83",
            "corpusId": 49411844,
            "url": "https://www.semanticscholar.org/paper/c1f457e31b611da727f9aef76c283a18157dfa83",
            "title": "DARTS: Differentiable Architecture Search",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2951104886",
                "DBLP": "journals/corr/abs-1806-09055",
                "ArXiv": "1806.09055",
                "CorpusId": 49411844
            },
            "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.",
            "referenceCount": 48,
            "citationCount": 3418,
            "influentialCitationCount": 1123,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-24",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.09055"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2018DARTSDA,\n author = {Hanxiao Liu and K. Simonyan and Yiming Yang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {DARTS: Differentiable Architecture Search},\n volume = {abs/1806.09055},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:846aedd869a00c09b40f1f1f35673cb22bc87490",
            "@type": "ScholarlyArticle",
            "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
            "corpusId": 515925,
            "url": "https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490",
            "title": "Mastering the game of Go with deep neural networks and tree search",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/nature/SilverHMGSDSAPL16",
                "MAG": "2257979135",
                "DOI": "10.1038/nature16961",
                "CorpusId": 515925,
                "PubMed": "26819042"
            },
            "abstract": null,
            "referenceCount": 72,
            "citationCount": 14437,
            "influentialCitationCount": 516,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/nature16961.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-01-27",
            "journal": {
                "name": "Nature",
                "volume": "529"
            },
            "citationStyles": {
                "bibtex": "@Article{Silver2016MasteringTG,\n author = {David Silver and Aja Huang and Chris J. Maddison and A. Guez and L. Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Vedavyas Panneershelvam and Marc Lanctot and S. Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and T. Lillicrap and M. Leach and K. Kavukcuoglu and T. Graepel and D. Hassabis},\n booktitle = {Nature},\n journal = {Nature},\n pages = {484-489},\n title = {Mastering the game of Go with deep neural networks and tree search},\n volume = {529},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
            "@type": "ScholarlyArticle",
            "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
            "corpusId": 16099293,
            "url": "https://www.semanticscholar.org/paper/2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
            "title": "OpenAI Gym",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1606.01540",
                "DBLP": "journals/corr/BrockmanCPSSTZ16",
                "CorpusId": 16099293
            },
            "abstract": "OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.",
            "referenceCount": 17,
            "citationCount": 4059,
            "influentialCitationCount": 491,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-06-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1606.01540"
            },
            "citationStyles": {
                "bibtex": "@Article{Brockman2016OpenAIG,\n author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and J. Schulman and Jie Tang and Wojciech Zaremba},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {OpenAI Gym},\n volume = {abs/1606.01540},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
            "@type": "ScholarlyArticle",
            "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
            "corpusId": 3544558,
            "url": "https://www.semanticscholar.org/paper/4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
            "title": "Addressing Function Approximation Error in Actor-Critic Methods",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963923407",
                "DBLP": "journals/corr/abs-1802-09477",
                "ArXiv": "1802.09477",
                "CorpusId": 3544558
            },
            "abstract": "In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.",
            "referenceCount": 49,
            "citationCount": 3254,
            "influentialCitationCount": 833,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-02-26",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Fujimoto2018AddressingFA,\n author = {Scott Fujimoto and H. V. Hoof and D. Meger},\n booktitle = {International Conference on Machine Learning},\n pages = {1582-1591},\n title = {Addressing Function Approximation Error in Actor-Critic Methods},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:697ff761c614dad5db023232d714163e92fe1202",
            "@type": "ScholarlyArticle",
            "paperId": "697ff761c614dad5db023232d714163e92fe1202",
            "corpusId": 56424856,
            "url": "https://www.semanticscholar.org/paper/697ff761c614dad5db023232d714163e92fe1202",
            "title": "The Empirical Status of Social Learning Theory: A Meta\u2010Analysis",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2143468612",
                "DOI": "10.1080/07418820903379610",
                "CorpusId": 56424856
            },
            "abstract": "Social learning theory has remained one of the core criminological paradigms over the last four decades. Although a large body of scholarship has emerged testing various propositions specified by the theory, the empirical status of the theory in its entirety is still unknown. Accordingly, in the present study, we subject this body of empirical literature to a meta\u2010analysis to assess its empirical status. Results reveal considerable variation in the magnitude and stability of effect sizes for variables specified by social learning theory across different methodological specifications. In particular, relationships of crime/deviance to measures of differential association and definitions (or antisocial attitudes) are quite strong, yet those for differential reinforcement and modeling/imitation are modest at best. Furthermore, effect sizes for differential association, definitions, and differential reinforcement all differed significantly according to variations in model specification and research designs across studies. The implications for the continued vitality of social learning in criminology are discussed.",
            "referenceCount": 237,
            "citationCount": 603,
            "influentialCitationCount": 48,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Sociology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2010-12-01",
            "journal": {
                "name": "Justice Quarterly",
                "volume": "27"
            },
            "citationStyles": {
                "bibtex": "@Article{Pratt2010TheES,\n author = {T. Pratt and F. Cullen and Christine S. Sellers and L. Thomas Winfree and Tamara D. Madensen and L. Daigle and Noelle E. Fearn and Jacinta M. Gau},\n journal = {Justice Quarterly},\n pages = {765 - 802},\n title = {The Empirical Status of Social Learning Theory: A Meta\u2010Analysis},\n volume = {27},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c27db32efa8137cbf654902f8f728f338e55cd1c",
            "@type": "ScholarlyArticle",
            "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
            "corpusId": 205261034,
            "url": "https://www.semanticscholar.org/paper/c27db32efa8137cbf654902f8f728f338e55cd1c",
            "title": "Mastering the game of Go without human knowledge",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2766447205",
                "DBLP": "journals/nature/SilverSSAHGHBLB17",
                "DOI": "10.1038/nature24270",
                "CorpusId": 205261034,
                "PubMed": "29052630"
            },
            "abstract": null,
            "referenceCount": 67,
            "citationCount": 7851,
            "influentialCitationCount": 363,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://discovery.ucl.ac.uk/10045895/1/agz_unformatted_nature.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-10-19",
            "journal": {
                "name": "Nature",
                "volume": "550"
            },
            "citationStyles": {
                "bibtex": "@Article{Silver2017MasteringTG,\n author = {David Silver and Julian Schrittwieser and K. Simonyan and Ioannis Antonoglou and Aja Huang and A. Guez and T. Hubert and Lucas baker and Matthew Lai and A. Bolton and Yutian Chen and T. Lillicrap and Fan Hui and L. Sifre and George van den Driessche and T. Graepel and D. Hassabis},\n booktitle = {Nature},\n journal = {Nature},\n pages = {354-359},\n title = {Mastering the game of Go without human knowledge},\n volume = {550},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:687d0e59d5c35f022ce4638b3e3a6142068efc94",
            "@type": "ScholarlyArticle",
            "paperId": "687d0e59d5c35f022ce4638b3e3a6142068efc94",
            "corpusId": 13928442,
            "url": "https://www.semanticscholar.org/paper/687d0e59d5c35f022ce4638b3e3a6142068efc94",
            "title": "Deterministic Policy Gradient Algorithms",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2165150801",
                "DBLP": "conf/icml/SilverLHDWR14",
                "CorpusId": 13928442
            },
            "abstract": "In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.",
            "referenceCount": 22,
            "citationCount": 3210,
            "influentialCitationCount": 475,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-06-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Silver2014DeterministicPG,\n author = {David Silver and Guy Lever and N. Heess and T. Degris and Daan Wierstra and Martin A. Riedmiller},\n booktitle = {International Conference on Machine Learning},\n pages = {387-395},\n title = {Deterministic Policy Gradient Algorithms},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ebf0615fc4d98cf1dbe527c79146ce1e50dce9af",
            "@type": "ScholarlyArticle",
            "paperId": "ebf0615fc4d98cf1dbe527c79146ce1e50dce9af",
            "corpusId": 5550767,
            "url": "https://www.semanticscholar.org/paper/ebf0615fc4d98cf1dbe527c79146ce1e50dce9af",
            "title": "CARLA: An Open Urban Driving Simulator",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2767621168",
                "DBLP": "conf/corl/DosovitskiyRCLK17",
                "ArXiv": "1711.03938",
                "CorpusId": 5550767
            },
            "abstract": "We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at this https URL",
            "referenceCount": 30,
            "citationCount": 3362,
            "influentialCitationCount": 711,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-10-18",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dosovitskiy2017CARLAAO,\n author = {A. Dosovitskiy and G. Ros and Felipe Codevilla and Antonio M. L\u00f3pez and V. Koltun},\n booktitle = {Conference on Robot Learning},\n pages = {1-16},\n title = {CARLA: An Open Urban Driving Simulator},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7f05d466841a04d1647c145482b99bb68bde545e",
            "@type": "ScholarlyArticle",
            "paperId": "7f05d466841a04d1647c145482b99bb68bde545e",
            "corpusId": 4405898,
            "url": "https://www.semanticscholar.org/paper/7f05d466841a04d1647c145482b99bb68bde545e",
            "title": "The primate amygdala represents the positive and negative value of visual stimuli during learning",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2144079442",
                "DOI": "10.1038/nature04490",
                "CorpusId": 4405898,
                "PubMed": "16482160"
            },
            "abstract": null,
            "referenceCount": 31,
            "citationCount": 876,
            "influentialCitationCount": 51,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc2396495?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Biology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-02-16",
            "journal": {
                "name": "Nature",
                "volume": "439"
            },
            "citationStyles": {
                "bibtex": "@Article{Paton2006ThePA,\n author = {Joseph J. Paton and Marina A. Belova and Sara E. Morrison and C. Salzman},\n booktitle = {Nature},\n journal = {Nature},\n pages = {865-870},\n title = {The primate amygdala represents the positive and negative value of visual stimuli during learning},\n volume = {439},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
            "@type": "ScholarlyArticle",
            "paperId": "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
            "corpusId": 207609497,
            "url": "https://www.semanticscholar.org/paper/1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
            "title": "Finite-time Analysis of the Multiarmed Bandit Problem",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2168405694",
                "DBLP": "journals/ml/AuerCF02",
                "DOI": "10.1023/A:1013689704352",
                "CorpusId": 207609497
            },
            "abstract": null,
            "referenceCount": 29,
            "citationCount": 6053,
            "influentialCitationCount": 1335,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1013689704352.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2002-05-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "47"
            },
            "citationStyles": {
                "bibtex": "@Article{Auer2002FinitetimeAO,\n author = {P. Auer and N. Cesa-Bianchi and P. Fischer},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {235-256},\n title = {Finite-time Analysis of the Multiarmed Bandit Problem},\n volume = {47},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1ffab78748e03ec8ca7e988590c4ff5782c96ea6",
            "@type": "ScholarlyArticle",
            "paperId": "1ffab78748e03ec8ca7e988590c4ff5782c96ea6",
            "corpusId": 232092445,
            "url": "https://www.semanticscholar.org/paper/1ffab78748e03ec8ca7e988590c4ff5782c96ea6",
            "title": "The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "conf/nips/YuVVGWBW22",
                "ArXiv": "2103.01955",
                "CorpusId": 232092445
            },
            "abstract": "Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that PPO is significantly less sample efficient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of PPO in cooperative multi-agent settings. We show that PPO-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the StarCraft multi-agent challenge, Google Research Football, and the Hanabi challenge, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. Importantly, compared to competitive off-policy methods, PPO often achieves competitive or superior results in both final returns and sample efficiency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to PPO's empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple PPO-based methods can be a strong baseline in cooperative multi-agent reinforcement learning. Source code is released at \\url{https://github.com/marlbenchmark/on-policy}.",
            "referenceCount": 53,
            "citationCount": 487,
            "influentialCitationCount": 136,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-03-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yu2021TheSE,\n author = {Chao Yu and Akash Velu and Eugene Vinitsky and Yu Wang and A. Bayen and Yi Wu},\n booktitle = {Neural Information Processing Systems},\n title = {The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
            "@type": "ScholarlyArticle",
            "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
            "corpusId": 3075448,
            "url": "https://www.semanticscholar.org/paper/d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
            "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1191599655",
                "DBLP": "journals/corr/SchulmanMLJA15",
                "ArXiv": "1506.02438",
                "CorpusId": 3075448
            },
            "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. \nOur approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.",
            "referenceCount": 31,
            "citationCount": 2481,
            "influentialCitationCount": 482,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-06-08",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1506.02438"
            },
            "citationStyles": {
                "bibtex": "@Article{Schulman2015HighDimensionalCC,\n author = {J. Schulman and Philipp Moritz and S. Levine and Michael I. Jordan and P. Abbeel},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},\n volume = {abs/1506.02438},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
            "@type": "ScholarlyArticle",
            "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
            "corpusId": 13022595,
            "url": "https://www.semanticscholar.org/paper/c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
            "title": "Prioritized Experience Replay",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2963477884",
                "ArXiv": "1511.05952",
                "DBLP": "journals/corr/SchaulQAS15",
                "CorpusId": 13022595
            },
            "abstract": "Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.",
            "referenceCount": 43,
            "citationCount": 3025,
            "influentialCitationCount": 419,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-18",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1511.05952"
            },
            "citationStyles": {
                "bibtex": "@Article{Schaul2015PrioritizedER,\n author = {T. Schaul and John Quan and Ioannis Antonoglou and David Silver},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Prioritized Experience Replay},\n volume = {abs/1511.05952},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8a756d4d25511d92a45d0f4545fa819de993851d",
            "@type": "ScholarlyArticle",
            "paperId": "8a756d4d25511d92a45d0f4545fa819de993851d",
            "corpusId": 17195923,
            "url": "https://www.semanticscholar.org/paper/8a756d4d25511d92a45d0f4545fa819de993851d",
            "title": "Recurrent Models of Visual Attention",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2014,
            "externalIds": {
                "ArXiv": "1406.6247",
                "DBLP": "conf/nips/MnihHGK14",
                "MAG": "2147527908",
                "CorpusId": 17195923
            },
            "abstract": "Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.",
            "referenceCount": 25,
            "citationCount": 3200,
            "influentialCitationCount": 193,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-06-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Mnih2014RecurrentMO,\n author = {Volodymyr Mnih and N. Heess and Alex Graves and K. Kavukcuoglu},\n booktitle = {Neural Information Processing Systems},\n pages = {2204-2212},\n title = {Recurrent Models of Visual Attention},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7ee42abcd6f8c3c16375aead3346d0c7f3a6f672",
            "@type": "ScholarlyArticle",
            "paperId": "7ee42abcd6f8c3c16375aead3346d0c7f3a6f672",
            "corpusId": 7989664,
            "url": "https://www.semanticscholar.org/paper/7ee42abcd6f8c3c16375aead3346d0c7f3a6f672",
            "title": "Neuro-Dynamic Programming",
            "venue": "Encyclopedia of Optimization",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2400458653",
                "DBLP": "reference/ml/X17pq",
                "DOI": "10.1007/978-0-387-74759-0_440",
                "CorpusId": 7989664
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 3839,
            "influentialCitationCount": 421,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Economics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Economics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Bertsekas2009NeuroDynamicP,\n author = {D. Bertsekas},\n booktitle = {Encyclopedia of Optimization},\n pages = {899},\n title = {Neuro-Dynamic Programming},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a82db864e472b5aa6313596ef9919f64e3363b1f",
            "@type": "ScholarlyArticle",
            "paperId": "a82db864e472b5aa6313596ef9919f64e3363b1f",
            "corpusId": 10566652,
            "url": "https://www.semanticscholar.org/paper/a82db864e472b5aa6313596ef9919f64e3363b1f",
            "title": "Dynamic programming and optimal control, 3rd Edition",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1995,
            "externalIds": {
                "MAG": "2098432798",
                "DBLP": "books/lib/Bertsekas05",
                "CorpusId": 10566652
            },
            "abstract": "The leading and most up-to-date textbook on the far-ranging algorithmic methododogy of Dynamic Programming, which can be used for optimal control, Markovian decision problems, planning and sequential decision making under uncertainty, and discrete/combinatorial optimization. The treatment focuses on basic unifying themes, and conceptual foundations. It illustrates the versatility, power, and generality of the method with many examples and applications from engineering, operations research, and other fields. It also addresses extensively the practical application of the methodology, possibly through the use of approximations, and provides an extensive treatment of the far-reaching methodology of Neuro-Dynamic Programming/Reinforcement Learning.",
            "referenceCount": 43,
            "citationCount": 9427,
            "influentialCitationCount": 854,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1995-05-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Bertsekas1995DynamicPA,\n author = {D. Bertsekas},\n pages = {I-XV, 1-543},\n title = {Dynamic programming and optimal control, 3rd Edition},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7c3ece1ba41c415d7e81cfa5ca33a8de66efd434",
            "@type": "ScholarlyArticle",
            "paperId": "7c3ece1ba41c415d7e81cfa5ca33a8de66efd434",
            "corpusId": 26419660,
            "url": "https://www.semanticscholar.org/paper/7c3ece1ba41c415d7e81cfa5ca33a8de66efd434",
            "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2950398823",
                "DBLP": "conf/nips/LoweWTHAM17",
                "ArXiv": "1706.02275",
                "CorpusId": 26419660
            },
            "abstract": "We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.",
            "referenceCount": 40,
            "citationCount": 3038,
            "influentialCitationCount": 710,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-06-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1706.02275"
            },
            "citationStyles": {
                "bibtex": "@Article{Lowe2017MultiAgentAF,\n author = {Ryan Lowe and Yi Wu and Aviv Tamar and J. Harb and P. Abbeel and Igor Mordatch},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},\n volume = {abs/1706.02275},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d03c916d49268d48fde3b76a68e64af7761835e7",
            "@type": "ScholarlyArticle",
            "paperId": "d03c916d49268d48fde3b76a68e64af7761835e7",
            "corpusId": 498161,
            "url": "https://www.semanticscholar.org/paper/d03c916d49268d48fde3b76a68e64af7761835e7",
            "title": "Evolving Neural Networks through Augmenting Topologies",
            "venue": "Evolutionary Computation",
            "publicationVenue": {
                "id": "urn:research:41b9fb21-e0f4-4304-8584-f1541bdd7efc",
                "name": "Evolutionary Computation",
                "alternate_names": [
                    "Evol Comput"
                ],
                "issn": "1063-6560",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=10636560"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2148872333",
                "DBLP": "journals/ec/StanleyM02",
                "DOI": "10.1162/106365602320169811",
                "CorpusId": 498161,
                "PubMed": "12180173"
            },
            "abstract": "An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is signicantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.",
            "referenceCount": 56,
            "citationCount": 3279,
            "influentialCitationCount": 474,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2002-06-01",
            "journal": {
                "name": "Evolutionary Computation",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Stanley2002EvolvingNN,\n author = {Kenneth O. Stanley and R. Miikkulainen},\n booktitle = {Evolutionary Computation},\n journal = {Evolutionary Computation},\n pages = {99-127},\n title = {Evolving Neural Networks through Augmenting Topologies},\n volume = {10},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b6b8a1b80891c96c28cc6340267b58186157e536",
            "@type": "ScholarlyArticle",
            "paperId": "b6b8a1b80891c96c28cc6340267b58186157e536",
            "corpusId": 7242892,
            "url": "https://www.semanticscholar.org/paper/b6b8a1b80891c96c28cc6340267b58186157e536",
            "title": "End-to-End Training of Deep Visuomotor Policies",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/corr/LevineFDA15",
                "MAG": "2155007355",
                "ArXiv": "1504.00702",
                "DOI": "10.5555/2946645.2946684",
                "CorpusId": 7242892
            },
            "abstract": "Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a partially observed guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.",
            "referenceCount": 99,
            "citationCount": 3011,
            "influentialCitationCount": 110,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-04-02",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "17"
            },
            "citationStyles": {
                "bibtex": "@Article{Levine2015EndtoEndTO,\n author = {S. Levine and Chelsea Finn and Trevor Darrell and P. Abbeel},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {39:1-39:40},\n title = {End-to-End Training of Deep Visuomotor Policies},\n volume = {17},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:60b7d47758a71978e74edff6dd8dea4d9c791d7a",
            "@type": "ScholarlyArticle",
            "paperId": "60b7d47758a71978e74edff6dd8dea4d9c791d7a",
            "corpusId": 14273320,
            "url": "https://www.semanticscholar.org/paper/60b7d47758a71978e74edff6dd8dea4d9c791d7a",
            "title": "PILCO: A Model-Based and Data-Efficient Approach to Policy Search",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "conf/icml/DeisenrothR11",
                "MAG": "2140135625",
                "CorpusId": 14273320
            },
            "abstract": "In this paper, we introduce PILCO, a practical, data-efficient model-based policy search method. PILCO reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, PILCO can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks.",
            "referenceCount": 32,
            "citationCount": 1454,
            "influentialCitationCount": 142,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-06-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Deisenroth2011PILCOAM,\n author = {M. Deisenroth and C. Rasmussen},\n booktitle = {International Conference on Machine Learning},\n pages = {465-472},\n title = {PILCO: A Model-Based and Data-Efficient Approach to Policy Search},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
            "@type": "ScholarlyArticle",
            "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
            "corpusId": 53115163,
            "url": "https://www.semanticscholar.org/paper/4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
            "title": "Exploration by Random Network Distillation",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1810-12894",
                "MAG": "2964067469",
                "ArXiv": "1810.12894",
                "CorpusId": 53115163
            },
            "abstract": "We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.",
            "referenceCount": 55,
            "citationCount": 969,
            "influentialCitationCount": 296,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1810.12894"
            },
            "citationStyles": {
                "bibtex": "@Article{Burda2018ExplorationBR,\n author = {Yuri Burda and Harrison Edwards and A. Storkey and Oleg Klimov},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Exploration by Random Network Distillation},\n volume = {abs/1810.12894},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6c8353697cdbb98dfba4f493875778c4286d3e3a",
            "@type": "ScholarlyArticle",
            "paperId": "6c8353697cdbb98dfba4f493875778c4286d3e3a",
            "corpusId": 206594923,
            "url": "https://www.semanticscholar.org/paper/6c8353697cdbb98dfba4f493875778c4286d3e3a",
            "title": "Self-Critical Sequence Training for Image Captioning",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/cvpr/RennieMMRG17",
                "MAG": "2963084599",
                "ArXiv": "1612.00563",
                "DOI": "10.1109/CVPR.2017.131",
                "CorpusId": 206594923
            },
            "abstract": "Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a baseline to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7.",
            "referenceCount": 26,
            "citationCount": 1556,
            "influentialCitationCount": 308,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1612.00563",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-12-02",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rennie2016SelfCriticalST,\n author = {Steven J. Rennie and E. Marcheret and Youssef Mroueh and Jerret Ross and V. Goel},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {1179-1195},\n title = {Self-Critical Sequence Training for Image Captioning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:da6057368920585bcf2443295b98418840f1fc80",
            "@type": "ScholarlyArticle",
            "paperId": "da6057368920585bcf2443295b98418840f1fc80",
            "corpusId": 1806222,
            "url": "https://www.semanticscholar.org/paper/da6057368920585bcf2443295b98418840f1fc80",
            "title": "Weight Uncertainty in Neural Networks",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2951266961",
                "DBLP": "journals/corr/BlundellCKW15",
                "ArXiv": "1505.05424",
                "CorpusId": 1806222
            },
            "abstract": "We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.",
            "referenceCount": 49,
            "citationCount": 1669,
            "influentialCitationCount": 305,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-05-20",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1505.05424"
            },
            "citationStyles": {
                "bibtex": "@Article{Blundell2015WeightUI,\n author = {C. Blundell and Julien Cornebise and K. Kavukcuoglu and Daan Wierstra},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Weight Uncertainty in Neural Networks},\n volume = {abs/1505.05424},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:80196cdfcd0c6ce2953bf65a7f019971e2026386",
            "@type": "ScholarlyArticle",
            "paperId": "80196cdfcd0c6ce2953bf65a7f019971e2026386",
            "corpusId": 3645060,
            "url": "https://www.semanticscholar.org/paper/80196cdfcd0c6ce2953bf65a7f019971e2026386",
            "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2950708659",
                "DBLP": "conf/icml/EspeholtSMSMWDF18",
                "ArXiv": "1802.01561",
                "CorpusId": 3645060
            },
            "abstract": "In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.",
            "referenceCount": 42,
            "citationCount": 1296,
            "influentialCitationCount": 250,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-02-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.01561"
            },
            "citationStyles": {
                "bibtex": "@Article{Espeholt2018IMPALASD,\n author = {Lasse Espeholt and Hubert Soyer and R. Munos and K. Simonyan and Volodymyr Mnih and Tom Ward and Yotam Doron and Vlad Firoiu and Tim Harley and Iain Dunning and S. Legg and K. Kavukcuoglu},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures},\n volume = {abs/1802.01561},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:429ed4c9845d0abd1f8204e1d7705919559bc2a2",
            "@type": "ScholarlyArticle",
            "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
            "corpusId": 3532908,
            "url": "https://www.semanticscholar.org/paper/429ed4c9845d0abd1f8204e1d7705919559bc2a2",
            "title": "Hindsight Experience Replay",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2964001908",
                "ArXiv": "1707.01495",
                "DBLP": "journals/corr/AndrychowiczWRS17",
                "CorpusId": 3532908
            },
            "abstract": "Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. \nWe demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.",
            "referenceCount": 46,
            "citationCount": 1807,
            "influentialCitationCount": 372,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-07-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1707.01495"
            },
            "citationStyles": {
                "bibtex": "@Article{Andrychowicz2017HindsightER,\n author = {Marcin Andrychowicz and Dwight Crow and Alex Ray and Jonas Schneider and Rachel Fong and P. Welinder and Bob McGrew and Joshua Tobin and P. Abbeel and Wojciech Zaremba},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Hindsight Experience Replay},\n volume = {abs/1707.01495},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:53c9443e4e667170acc60ca1b31a0ec7151fe753",
            "@type": "ScholarlyArticle",
            "paperId": "53c9443e4e667170acc60ca1b31a0ec7151fe753",
            "corpusId": 15350923,
            "url": "https://www.semanticscholar.org/paper/53c9443e4e667170acc60ca1b31a0ec7151fe753",
            "title": "Progressive Neural Networks",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1606.04671",
                "DBLP": "journals/corr/RusuRDSKKPH16",
                "CorpusId": 15350923
            },
            "abstract": "Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.",
            "referenceCount": 24,
            "citationCount": 1693,
            "influentialCitationCount": 164,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-06-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1606.04671"
            },
            "citationStyles": {
                "bibtex": "@Article{Rusu2016ProgressiveNN,\n author = {Andrei A. Rusu and Neil C. Rabinowitz and Guillaume Desjardins and Hubert Soyer and J. Kirkpatrick and K. Kavukcuoglu and Razvan Pascanu and R. Hadsell},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Progressive Neural Networks},\n volume = {abs/1606.04671},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:62adfea3cc1cd9eb6b53e0e8a40be5dfda2adf8d",
            "@type": "ScholarlyArticle",
            "paperId": "62adfea3cc1cd9eb6b53e0e8a40be5dfda2adf8d",
            "corpusId": 39895556,
            "url": "https://www.semanticscholar.org/paper/62adfea3cc1cd9eb6b53e0e8a40be5dfda2adf8d",
            "title": "Weight Uncertainty in Neural Network",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2164411961",
                "DBLP": "conf/icml/BlundellCKW15",
                "CorpusId": 39895556
            },
            "abstract": "We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.",
            "referenceCount": 39,
            "citationCount": 1415,
            "influentialCitationCount": 251,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-07-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Blundell2015WeightUI,\n author = {C. Blundell and Julien Cornebise and K. Kavukcuoglu and Daan Wierstra},\n booktitle = {International Conference on Machine Learning},\n pages = {1613-1622},\n title = {Weight Uncertainty in Neural Network},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:784ee73d5363c711118f784428d1ab89f019daa5",
            "@type": "ScholarlyArticle",
            "paperId": "784ee73d5363c711118f784428d1ab89f019daa5",
            "corpusId": 205251479,
            "url": "https://www.semanticscholar.org/paper/784ee73d5363c711118f784428d1ab89f019daa5",
            "title": "Hybrid computing using a neural network with dynamic external memory",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2530887700",
                "DBLP": "journals/nature/GravesWRHDGCGRA16",
                "DOI": "10.1038/nature20101",
                "CorpusId": 205251479,
                "PubMed": "27732574"
            },
            "abstract": null,
            "referenceCount": 54,
            "citationCount": 1429,
            "influentialCitationCount": 161,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ora.ox.ac.uk/objects/uuid:dd8473bd-2d70-424d-881b-86d9c9c66b51/download_file?safe_filename=outline.pdf&file_format=application%2Fpdf&type_of_work=Journal+article",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-10-12",
            "journal": {
                "name": "Nature",
                "volume": "538"
            },
            "citationStyles": {
                "bibtex": "@Article{Graves2016HybridCU,\n author = {Alex Graves and Greg Wayne and Malcolm Reynolds and Tim Harley and Ivo Danihelka and A. Grabska-Barwinska and Sergio Gomez Colmenarejo and Edward Grefenstette and Tiago Ramalho and J. Agapiou and Adri\u00e0 Puigdom\u00e8nech Badia and K. Hermann and Yori Zwols and Georg Ostrovski and Adam Cain and Helen King and C. Summerfield and Phil Blunsom and K. Kavukcuoglu and D. Hassabis},\n booktitle = {Nature},\n journal = {Nature},\n pages = {471-476},\n title = {Hybrid computing using a neural network with dynamic external memory},\n volume = {538},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
            "@type": "ScholarlyArticle",
            "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
            "corpusId": 5865729,
            "url": "https://www.semanticscholar.org/paper/4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
            "title": "Deep Exploration via Bootstrapped DQN",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2950112945",
                "ArXiv": "1602.04621",
                "DBLP": "journals/corr/OsbandBPR16",
                "CorpusId": 5865729
            },
            "abstract": "Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.",
            "referenceCount": 47,
            "citationCount": 1076,
            "influentialCitationCount": 182,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-02-15",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Osband2016DeepEV,\n author = {Ian Osband and C. Blundell and A. Pritzel and Benjamin Van Roy},\n booktitle = {Neural Information Processing Systems},\n pages = {4026-4034},\n title = {Deep Exploration via Bootstrapped DQN},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3d2c6941a9b4608ba52b328369a3352db2092ae0",
            "@type": "ScholarlyArticle",
            "paperId": "3d2c6941a9b4608ba52b328369a3352db2092ae0",
            "corpusId": 151231,
            "url": "https://www.semanticscholar.org/paper/3d2c6941a9b4608ba52b328369a3352db2092ae0",
            "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2284050935",
                "DBLP": "conf/nips/SalimansK16",
                "ArXiv": "1602.07868",
                "CorpusId": 151231
            },
            "abstract": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.",
            "referenceCount": 34,
            "citationCount": 1640,
            "influentialCitationCount": 120,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-02-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1602.07868"
            },
            "citationStyles": {
                "bibtex": "@Article{Salimans2016WeightNA,\n author = {Tim Salimans and Diederik P. Kingma},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks},\n volume = {abs/1602.07868},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:62c9ae261c14877d62156ede956db723ebff8a5a",
            "@type": "ScholarlyArticle",
            "paperId": "62c9ae261c14877d62156ede956db723ebff8a5a",
            "corpusId": 9590039,
            "url": "https://www.semanticscholar.org/paper/62c9ae261c14877d62156ede956db723ebff8a5a",
            "title": "Learning variable impedance control",
            "venue": "Int. J. Robotics Res.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "journals/ijrr/BuchliSTS11",
                "MAG": "2133932631",
                "DOI": "10.1177/0278364911402527",
                "CorpusId": 9590039
            },
            "abstract": "One of the hallmarks of the performance, versatility, and robustness of biological motor control is the ability to adapt the impedance of the overall biomechanical system to different task requirements and stochastic disturbances. A transfer of this principle to robotics is desirable, for instance to enable robots to work robustly and safely in everyday human environments. It is, however, not trivial to derive variable impedance controllers for practical high degree-of-freedom (DOF) robotic tasks. In this contribution, we accomplish such variable impedance control with the reinforcement learning (RL) algorithm PI 2 ( P olicy I mprovement with P ath I ntegrals). PI 2 is a model-free, sampling-based learning method derived from first principles of stochastic optimal control. The PI 2 algorithm requires no tuning of algorithmic parameters besides the exploration noise. The designer can thus fully focus on the cost function design to specify the task. From the viewpoint of robotics, a particular useful property of PI 2 is that it can scale to problems of many DOFs, so that reinforcement learning on real robotic systems becomes feasible. We sketch the PI 2 algorithm and its theoretical properties, and how it is applied to gain scheduling for variable impedance control. We evaluate our approach by presenting results on several simulated and real robots. We consider tasks involving accurate tracking through via points, and manipulation tasks requiring physical contact with the environment. In these tasks, the optimal strategy requires both tuning of a reference trajectory and the impedance of the end-effector. The results show that we can use path integral based reinforcement learning not only for planning but also to derive variable gain feedback controllers in realistic scenarios. Thus, the power of variable impedance control is made available to a wide variety of robotic systems and practical applications.",
            "referenceCount": 46,
            "citationCount": 323,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-06-01",
            "journal": {
                "name": "The International Journal of Robotics Research",
                "volume": "30"
            },
            "citationStyles": {
                "bibtex": "@Article{Buchli2011LearningVI,\n author = {J. Buchli and F. Stulp and Evangelos A. Theodorou and S. Schaal},\n booktitle = {Int. J. Robotics Res.},\n journal = {The International Journal of Robotics Research},\n pages = {820 - 833},\n title = {Learning variable impedance control},\n volume = {30},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5f79398057bf0bbda9ff50067bc1f2950c2a2266",
            "@type": "ScholarlyArticle",
            "paperId": "5f79398057bf0bbda9ff50067bc1f2950c2a2266",
            "corpusId": 40430109,
            "url": "https://www.semanticscholar.org/paper/5f79398057bf0bbda9ff50067bc1f2950c2a2266",
            "title": "Progressive Neural Architecture Search",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1712.00559",
                "MAG": "2949714964",
                "DBLP": "journals/corr/abs-1712-00559",
                "DOI": "10.1007/978-3-030-01246-5_2",
                "CorpusId": 40430109
            },
            "abstract": null,
            "referenceCount": 53,
            "citationCount": 1786,
            "influentialCitationCount": 270,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1712.00559",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-12-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2017ProgressiveNA,\n author = {Chenxi Liu and Barret Zoph and Jonathon Shlens and Wei Hua and Li-Jia Li and Li Fei-Fei and A. Yuille and Jonathan Huang and K. Murphy},\n booktitle = {European Conference on Computer Vision},\n pages = {19-35},\n title = {Progressive Neural Architecture Search},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:032274e57f7d8b456bd255fe76b909b2c1d7458e",
            "@type": "ScholarlyArticle",
            "paperId": "032274e57f7d8b456bd255fe76b909b2c1d7458e",
            "corpusId": 21850704,
            "url": "https://www.semanticscholar.org/paper/032274e57f7d8b456bd255fe76b909b2c1d7458e",
            "title": "A Deep Reinforced Model for Abstractive Summarization",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1705.04304",
                "DBLP": "journals/corr/PaulusXS17",
                "MAG": "2612675303",
                "CorpusId": 21850704
            },
            "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.",
            "referenceCount": 43,
            "citationCount": 1369,
            "influentialCitationCount": 193,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-05-11",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1705.04304"
            },
            "citationStyles": {
                "bibtex": "@Article{Paulus2017ADR,\n author = {Romain Paulus and Caiming Xiong and R. Socher},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {A Deep Reinforced Model for Abstractive Summarization},\n volume = {abs/1705.04304},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5dc2a215bd7cd5bdd3a0baa8c967575632696fac",
            "@type": "ScholarlyArticle",
            "paperId": "5dc2a215bd7cd5bdd3a0baa8c967575632696fac",
            "corpusId": 15897963,
            "url": "https://www.semanticscholar.org/paper/5dc2a215bd7cd5bdd3a0baa8c967575632696fac",
            "title": "Universal Value Function Approximators",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2787658090",
                "DBLP": "conf/icml/SchaulHGS15",
                "CorpusId": 15897963
            },
            "abstract": "Value functions are a core component of reinforcement learning systems. The main idea is to to construct a single function approximator V (s; \u03b8) that estimates the long-term reward from any state s, using parameters \u03b8. In this paper we introduce universal value function approximators (UVFAs) V (s, g; \u03b8) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.",
            "referenceCount": 26,
            "citationCount": 903,
            "influentialCitationCount": 115,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-07-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Schaul2015UniversalVF,\n author = {T. Schaul and Dan Horgan and Karol Gregor and David Silver},\n booktitle = {International Conference on Machine Learning},\n pages = {1312-1320},\n title = {Universal Value Function Approximators},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:767ed9b4c974411b46c445bd3e235515760ecc65",
            "@type": "ScholarlyArticle",
            "paperId": "767ed9b4c974411b46c445bd3e235515760ecc65",
            "corpusId": 16385268,
            "url": "https://www.semanticscholar.org/paper/767ed9b4c974411b46c445bd3e235515760ecc65",
            "title": "Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2167362547",
                "DOI": "10.1038/nn1560",
                "CorpusId": 16385268,
                "PubMed": "16286932"
            },
            "abstract": null,
            "referenceCount": 64,
            "citationCount": 2212,
            "influentialCitationCount": 170,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2005-12-01",
            "journal": {
                "name": "Nature Neuroscience",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Daw2005UncertaintybasedCB,\n author = {N. Daw and Y. Niv and P. Dayan},\n booktitle = {Nature Neuroscience},\n journal = {Nature Neuroscience},\n pages = {1704-1711},\n title = {Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control},\n volume = {8},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6ce1922802169f757bbafc6e087cc274a867c763",
            "@type": "ScholarlyArticle",
            "paperId": "6ce1922802169f757bbafc6e087cc274a867c763",
            "corpusId": 9545399,
            "url": "https://www.semanticscholar.org/paper/6ce1922802169f757bbafc6e087cc274a867c763",
            "title": "Regularizing Neural Networks by Penalizing Confident Output Distributions",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1701.06548",
                "DBLP": "conf/iclr/PereyraTCKH17",
                "MAG": "2581377246",
                "CorpusId": 9545399
            },
            "abstract": "We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers.",
            "referenceCount": 49,
            "citationCount": 953,
            "influentialCitationCount": 144,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-01-23",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1701.06548"
            },
            "citationStyles": {
                "bibtex": "@Article{Pereyra2017RegularizingNN,\n author = {Gabriel Pereyra and G. Tucker and J. Chorowski and Lukasz Kaiser and Geoffrey E. Hinton},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Regularizing Neural Networks by Penalizing Confident Output Distributions},\n volume = {abs/1701.06548},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
            "@type": "ScholarlyArticle",
            "paperId": "7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
            "corpusId": 10647707,
            "url": "https://www.semanticscholar.org/paper/7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
            "title": "Constrained Policy Optimization",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2952192457",
                "ArXiv": "1705.10528",
                "DBLP": "journals/corr/AchiamHTA17",
                "CorpusId": 10647707
            },
            "abstract": "For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. \nWe propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety.",
            "referenceCount": 32,
            "citationCount": 916,
            "influentialCitationCount": 207,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-05-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1705.10528"
            },
            "citationStyles": {
                "bibtex": "@Article{Achiam2017ConstrainedPO,\n author = {Joshua Achiam and David Held and Aviv Tamar and P. Abbeel},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Constrained Policy Optimization},\n volume = {abs/1705.10528},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3f0a2de309f21a957b4741dd68007eb08d9b12e3",
            "@type": "ScholarlyArticle",
            "paperId": "3f0a2de309f21a957b4741dd68007eb08d9b12e3",
            "corpusId": 56895416,
            "url": "https://www.semanticscholar.org/paper/3f0a2de309f21a957b4741dd68007eb08d9b12e3",
            "title": "SNAS: Stochastic Neural Architecture Search",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2905672847",
                "DBLP": "journals/corr/abs-1812-09926",
                "ArXiv": "1812.09926",
                "CorpusId": 56895416
            },
            "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets. We have released our implementation at this https URL.",
            "referenceCount": 38,
            "citationCount": 829,
            "influentialCitationCount": 154,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1812.09926"
            },
            "citationStyles": {
                "bibtex": "@Article{Xie2018SNASSN,\n author = {Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {SNAS: Stochastic Neural Architecture Search},\n volume = {abs/1812.09926},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b6bfae6efa1110a57a4d8362721d152d78aae358",
            "@type": "ScholarlyArticle",
            "paperId": "b6bfae6efa1110a57a4d8362721d152d78aae358",
            "corpusId": 17647665,
            "url": "https://www.semanticscholar.org/paper/b6bfae6efa1110a57a4d8362721d152d78aae358",
            "title": "A Survey on Policy Search for Robotics",
            "venue": "Found. Trends Robotics",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2012587148",
                "DBLP": "journals/ftrob/DeisenrothNP13",
                "DOI": "10.1561/2300000021",
                "CorpusId": 17647665
            },
            "abstract": "Policy search is a subfield in reinforcement learning which focuses on finding good parameters for a given policy parametrization. It is well suited for robotics as it can cope with high-dimensional state and action spaces, one of the main challenges in robot learning. We review recent successes of both model-free and model-based policy search in robot learning.Model-free policy search is a general approach to learn policies based on sampled trajectories. We classify model-free methods based on their policy evaluation strategy, policy update strategy, and exploration strategy and present a unified view on existing algorithms. Learning a policy is often easier than learning an accurate forward model, and, hence, model-free methods are more frequently used in practice. However, for each sampled trajectory, it is necessary to interact with the robot, which can be time consuming and challenging in practice. Model-based policy search addresses this problem by first learning a simulator of the robot's dynamics from data. Subsequently, the simulator generates trajectories that are used for policy learning. For both model-free and model-based policy search methods, we review their respective properties and their applicability to robotic systems.",
            "referenceCount": 101,
            "citationCount": 913,
            "influentialCitationCount": 69,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://eprints.lincoln.ac.uk/id/eprint/28029/1/PolicySearchReview.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2013-08-15",
            "journal": {
                "name": "Found. Trends Robotics",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Deisenroth2013ASO,\n author = {M. Deisenroth and G. Neumann and Jan Peters},\n booktitle = {Found. Trends Robotics},\n journal = {Found. Trends Robotics},\n pages = {1-142},\n title = {A Survey on Policy Search for Robotics},\n volume = {2},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b750a17921d32936425e05f8b00b96569e2fc5a6",
            "@type": "ScholarlyArticle",
            "paperId": "b750a17921d32936425e05f8b00b96569e2fc5a6",
            "corpusId": 3226593,
            "url": "https://www.semanticscholar.org/paper/b750a17921d32936425e05f8b00b96569e2fc5a6",
            "title": "Least-Squares Policy Iteration",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "journals/jmlr/LagoudakisP03",
                "MAG": "2130005627",
                "CorpusId": 3226593
            },
            "abstract": "We propose a new approach to reinforcement learning for control problems which combines value-function approximation with linear architectures and approximate policy iteration. This new approach is motivated by the least-squares temporal-difference learning algorithm (LSTD) for prediction problems, which is known for its efficient use of sample experiences compared to pure temporal-difference algorithms. Heretofore, LSTD has not had a straightforward application to control problems mainly because LSTD learns the state value function of a fixed policy which cannot be used for action selection and control without a model of the underlying process. Our new algorithm, least-squares policy iteration (LSPI), learns the state-action value function which allows for action selection without a model and for incremental policy improvement within a policy-iteration framework. LSPI is a model-free, off-policy method which can use efficiently (and reuse in each iteration) sample experiences collected in any manner. By separating the sample collection method, the choice of the linear approximation architecture, and the solution method, LSPI allows for focused attention on the distinct elements that contribute to practical reinforcement learning. LSPI is tested on the simple task of balancing an inverted pendulum and the harder task of balancing and riding a bicycle to a target location. In both cases, LSPI learns to control the pendulum or the bicycle by merely observing a relatively small number of trials where actions are selected randomly. LSPI is also compared against Q-learning (both with and without experience replay) using the same value function architecture. While LSPI achieves good performance fairly consistently on the difficult bicycle task, Q-learning variants were rarely able to balance for more than a small fraction of the time needed to reach the target location.",
            "referenceCount": 29,
            "citationCount": 1421,
            "influentialCitationCount": 276,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2003-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Lagoudakis2003LeastSquaresPI,\n author = {M. Lagoudakis and Ronald E. Parr},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1107-1149},\n title = {Least-Squares Policy Iteration},\n volume = {4},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:103f6fe35033f9327611ddafde74a2b544072980",
            "@type": "ScholarlyArticle",
            "paperId": "103f6fe35033f9327611ddafde74a2b544072980",
            "corpusId": 10485293,
            "url": "https://www.semanticscholar.org/paper/103f6fe35033f9327611ddafde74a2b544072980",
            "title": "Using Confidence Bounds for Exploitation-Exploration Trade-offs",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2108114251",
                "DBLP": "journals/jmlr/Auer02",
                "CorpusId": 10485293
            },
            "abstract": "We show how a standard tool from statistics --- namely confidence bounds --- can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-off. Our technique for designing and analyzing algorithms for such situations is general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process. We apply our technique to two models with such an exploitation-exploration trade-off. For the adversarial bandit problem with shifting our new algorithm suffers only O((ST)1/2) regret with high probability over T trials with S shifts. Such a regret bound was previously known only in expectation. The second model we consider is associative reinforcement learning with linear value functions. For this model our technique improves the regret from O(T3/4) to O(T1/2).",
            "referenceCount": 17,
            "citationCount": 1681,
            "influentialCitationCount": 254,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2003-03-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Auer2003UsingCB,\n author = {P. Auer},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {397-422},\n title = {Using Confidence Bounds for Exploitation-Exploration Trade-offs},\n volume = {3},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:94066dc12fe31e96af7557838159bde598cb4f10",
            "@type": "ScholarlyArticle",
            "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
            "corpusId": 5730166,
            "url": "https://www.semanticscholar.org/paper/94066dc12fe31e96af7557838159bde598cb4f10",
            "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1999,
            "externalIds": {
                "DBLP": "conf/icml/NgHR99",
                "MAG": "1777239053",
                "CorpusId": 5730166
            },
            "abstract": "This paper investigates conditions under which modi(cid:12)cations to the reward function of a Markov decision process preserve the optimal policy. It is shown that, besides the positive linear transformation familiar from utility theory, one can add a reward for transitions between states that is expressible as the di(cid:11)erence in value of an arbitrary potential function applied to those states. Further-more, this is shown to be a necessary condition for invariance, in the sense that any other transformation may yield suboptimal policies unless further assumptions are made about the underlying MDP. These results shed light on the practice of reward shaping, a method used in reinforcement learning whereby additional training rewards are used to guide the learning agent. In particular, some well-known \\bugs\" in reward shaping procedures are shown to arise from non-potential-based rewards, and methods are given for constructing shaping potentials corresponding to distance-based and subgoal-based heuristics. We show that such potentials can lead to substantial reductions in learning time.",
            "referenceCount": 14,
            "citationCount": 2012,
            "influentialCitationCount": 202,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1999-06-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ng1999PolicyIU,\n author = {A. Ng and D. Harada and Stuart J. Russell},\n booktitle = {International Conference on Machine Learning},\n pages = {278-287},\n title = {Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:256c3bd45ab7452bb51721eb25d3367bb654225e",
            "@type": "ScholarlyArticle",
            "paperId": "256c3bd45ab7452bb51721eb25d3367bb654225e",
            "corpusId": 2994241,
            "url": "https://www.semanticscholar.org/paper/256c3bd45ab7452bb51721eb25d3367bb654225e",
            "title": "Interactively shaping agents via human reinforcement: the TAMER framework",
            "venue": "International Conference on Knowledge Capture",
            "publicationVenue": {
                "id": "urn:research:8d2784da-c796-42e8-a25a-cbd60f1b6c92",
                "name": "International Conference on Knowledge Capture",
                "alternate_names": [
                    "Int Conf Knowl Capture",
                    "K-CAP"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=1900"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2156869222",
                "DBLP": "conf/kcap/KnoxS09",
                "DOI": "10.1145/1597735.1597738",
                "CorpusId": 2994241
            },
            "abstract": "As computational learning agents move into domains that incur real costs (e.g., autonomous driving or financial investment), it will be necessary to learn good policies without numerous high-cost learning trials. One promising approach to reducing sample complexity of learning a task is knowledge transfer from humans to agents. Ideally, methods of transfer should be accessible to anyone with task knowledge, regardless of that person's expertise in programming and AI. This paper focuses on allowing a human trainer to interactively shape an agent's policy via reinforcement signals. Specifically, the paper introduces \"Training an Agent Manually via Evaluative Reinforcement,\" or TAMER, a framework that enables such shaping. Differing from previous approaches to interactive shaping, a TAMER agent models the human's reinforcement and exploits its model by choosing actions expected to be most highly reinforced. Results from two domains demonstrate that lay users can train TAMER agents without defining an environmental reward function (as in an MDP) and indicate that human training within the TAMER framework can reduce sample complexity over autonomous learning algorithms.",
            "referenceCount": 23,
            "citationCount": 458,
            "influentialCitationCount": 53,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-09-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Knox2009InteractivelySA,\n author = {W. B. Knox and P. Stone},\n booktitle = {International Conference on Knowledge Capture},\n pages = {9-16},\n title = {Interactively shaping agents via human reinforcement: the TAMER framework},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e2230c5954d6806661bf81e4a3999a2802a85d11",
            "@type": "ScholarlyArticle",
            "paperId": "e2230c5954d6806661bf81e4a3999a2802a85d11",
            "corpusId": 9098305,
            "url": "https://www.semanticscholar.org/paper/e2230c5954d6806661bf81e4a3999a2802a85d11",
            "title": "Learning Parameterized Skills",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2133853511",
                "ArXiv": "1206.6398",
                "DBLP": "conf/icml/SilvaKB12",
                "CorpusId": 9098305
            },
            "abstract": "We introduce a method for constructing skills capable of solving tasks drawn from a distribution of parameterized reinforcement learning problems. The method draws example tasks from a distribution of interest and uses the corresponding learned policies to estimate the topology of the lower-dimensional piecewise-smooth manifold on which the skill policies lie. This manifold models how policy parameters change as task parameters vary. The method identifies the number of charts that compose the manifold and then applies non-linear regression in each chart to construct a parameterized skill by predicting policy parameters from task parameters. We evaluate our method on an underactuated simulated robotic arm tasked with learning to accurately throw darts at a parameterized target location.",
            "referenceCount": 86,
            "citationCount": 198,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2012-06-26",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Silva2012LearningPS,\n author = {Bruno C. da Silva and G. Konidaris and A. Barto},\n booktitle = {International Conference on Machine Learning},\n pages = {1443-1450},\n title = {Learning Parameterized Skills},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:15b26d8cb35d7e795c8832fe08794224ee1e9f84",
            "@type": "ScholarlyArticle",
            "paperId": "15b26d8cb35d7e795c8832fe08794224ee1e9f84",
            "corpusId": 6627476,
            "url": "https://www.semanticscholar.org/paper/15b26d8cb35d7e795c8832fe08794224ee1e9f84",
            "title": "The Option-Critic Architecture",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2964227312",
                "DBLP": "conf/aaai/BaconHP17",
                "ArXiv": "1609.05140",
                "DOI": "10.1609/aaai.v31i1.10916",
                "CorpusId": 6627476
            },
            "abstract": "\n \n Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging.We tackle this problem in the framework of options [Sutton,Precup and Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.\n \n",
            "referenceCount": 47,
            "citationCount": 846,
            "influentialCitationCount": 101,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10916/10775",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-09-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1609.05140"
            },
            "citationStyles": {
                "bibtex": "@Article{Bacon2016TheOA,\n author = {Pierre-Luc Bacon and J. Harb and Doina Precup},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {The Option-Critic Architecture},\n volume = {abs/1609.05140},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:41cca0b0a27ba363ca56e7033569aeb1922b0ac9",
            "@type": "ScholarlyArticle",
            "paperId": "41cca0b0a27ba363ca56e7033569aeb1922b0ac9",
            "corpusId": 52171619,
            "url": "https://www.semanticscholar.org/paper/41cca0b0a27ba363ca56e7033569aeb1922b0ac9",
            "title": "Recurrent World Models Facilitate Policy Evolution",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2953072278",
                "DBLP": "journals/corr/abs-1809-01999",
                "ArXiv": "1809.01999",
                "CorpusId": 52171619
            },
            "abstract": "A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of this paper is available at https://worldmodels.github.io",
            "referenceCount": 100,
            "citationCount": 679,
            "influentialCitationCount": 83,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ha2018RecurrentWM,\n author = {David R Ha and J. Schmidhuber},\n booktitle = {Neural Information Processing Systems},\n pages = {2455-2467},\n title = {Recurrent World Models Facilitate Policy Evolution},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a9a3ed69c94a3e1c08ef1f833d9199f57736238b",
            "@type": "ScholarlyArticle",
            "paperId": "a9a3ed69c94a3e1c08ef1f833d9199f57736238b",
            "corpusId": 6315299,
            "url": "https://www.semanticscholar.org/paper/a9a3ed69c94a3e1c08ef1f833d9199f57736238b",
            "title": "DeepMind Control Suite",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2781585732",
                "ArXiv": "1801.00690",
                "DBLP": "journals/corr/abs-1801-00690",
                "CorpusId": 6315299
            },
            "abstract": "The DeepMind Control Suite is a set of continuous control tasks with a standardised structure and interpretable rewards, intended to serve as performance benchmarks for reinforcement learning agents. The tasks are written in Python and powered by the MuJoCo physics engine, making them easy to use and modify. We include benchmarks for several learning algorithms. The Control Suite is publicly available at this https URL A video summary of all tasks is available at this http URL .",
            "referenceCount": 37,
            "citationCount": 697,
            "influentialCitationCount": 90,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-01-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1801.00690"
            },
            "citationStyles": {
                "bibtex": "@Article{Tassa2018DeepMindCS,\n author = {Yuval Tassa and Yotam Doron and Alistair Muldal and Tom Erez and Yazhe Li and Diego de Las Casas and D. Budden and A. Abdolmaleki and J. Merel and Andrew Lefrancq and T. Lillicrap and Martin A. Riedmiller},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {DeepMind Control Suite},\n volume = {abs/1801.00690},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:45b7b5514a65126d39a51d5a68da53e7aa244c1f",
            "@type": "ScholarlyArticle",
            "paperId": "45b7b5514a65126d39a51d5a68da53e7aa244c1f",
            "corpusId": 51763580,
            "url": "https://www.semanticscholar.org/paper/45b7b5514a65126d39a51d5a68da53e7aa244c1f",
            "title": "Understanding and Simplifying One-Shot Architecture Search",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2885820039",
                "DBLP": "conf/icml/BenderKZVL18",
                "CorpusId": 51763580
            },
            "abstract": "There is growing interest in automating neural network architecture design. Existing architecture search methods can be computationally expensive, requiring thousands of different architectures to be trained from scratch. Recent work has explored weight sharing across models to amortize the cost of training. Although previous methods reduced the cost of architecture search by orders of magnitude, they remain complex, requiring hypernetworks or reinforcement learning controllers. We aim to understand weight sharing for one-shot architecture search. With careful experimental analysis, we show that it is possible to efficiently identify promising architectures from a complex search space without either hypernetworks or RL.",
            "referenceCount": 29,
            "citationCount": 674,
            "influentialCitationCount": 107,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-07-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bender2018UnderstandingAS,\n author = {Gabriel Bender and Pieter-Jan Kindermans and Barret Zoph and Vijay Vasudevan and Quoc V. Le},\n booktitle = {International Conference on Machine Learning},\n pages = {549-558},\n title = {Understanding and Simplifying One-Shot Architecture Search},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:80159c6b24653d8d9797075d2b96dccc3c39a345",
            "@type": "ScholarlyArticle",
            "paperId": "80159c6b24653d8d9797075d2b96dccc3c39a345",
            "corpusId": 205210862,
            "url": "https://www.semanticscholar.org/paper/80159c6b24653d8d9797075d2b96dccc3c39a345",
            "title": "Reverse replay of behavioural sequences in hippocampal place cells during the awake state",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2115107366",
                "DOI": "10.1038/nature04587",
                "CorpusId": 205210862,
                "PubMed": "16474382"
            },
            "abstract": null,
            "referenceCount": 30,
            "citationCount": 1457,
            "influentialCitationCount": 87,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Biology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-03-30",
            "journal": {
                "name": "Nature",
                "volume": "440"
            },
            "citationStyles": {
                "bibtex": "@Article{Foster2006ReverseRO,\n author = {David J. Foster and M. Wilson},\n booktitle = {Nature},\n journal = {Nature},\n pages = {680-683},\n title = {Reverse replay of behavioural sequences in hippocampal place cells during the awake state},\n volume = {440},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:601a2d349fc26d7b82f905e924e2f91b0ac4b310",
            "@type": "ScholarlyArticle",
            "paperId": "601a2d349fc26d7b82f905e924e2f91b0ac4b310",
            "corpusId": 3463260,
            "url": "https://www.semanticscholar.org/paper/601a2d349fc26d7b82f905e924e2f91b0ac4b310",
            "title": "Distributed Prioritized Experience Replay",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963296584",
                "DBLP": "conf/iclr/HorganQBBHHS18",
                "ArXiv": "1803.00933",
                "CorpusId": 3463260
            },
            "abstract": "We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.",
            "referenceCount": 41,
            "citationCount": 609,
            "influentialCitationCount": 90,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1803.00933"
            },
            "citationStyles": {
                "bibtex": "@Article{Horgan2018DistributedPE,\n author = {Dan Horgan and John Quan and D. Budden and Gabriel Barth-Maron and Matteo Hessel and H. V. Hasselt and David Silver},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Distributed Prioritized Experience Replay},\n volume = {abs/1803.00933},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1028b3f1808b5bcc72b018d157db952bb8282205",
            "@type": "ScholarlyArticle",
            "paperId": "1028b3f1808b5bcc72b018d157db952bb8282205",
            "corpusId": 4242147,
            "url": "https://www.semanticscholar.org/paper/1028b3f1808b5bcc72b018d157db952bb8282205",
            "title": "Place navigation impaired in rats with hippocampal lesions",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 1982,
            "externalIds": {
                "MAG": "2017169966",
                "DOI": "10.1038/297681A0",
                "CorpusId": 4242147,
                "PubMed": "7088155"
            },
            "abstract": null,
            "referenceCount": 14,
            "citationCount": 6194,
            "influentialCitationCount": 195,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1982-06-24",
            "journal": {
                "name": "Nature",
                "volume": "297"
            },
            "citationStyles": {
                "bibtex": "@Article{Morris1982PlaceNI,\n author = {R. Morris and P. Garrud and J. Rawlins and J. O\u2019Keefe},\n booktitle = {Nature},\n journal = {Nature},\n pages = {681-683},\n title = {Place navigation impaired in rats with hippocampal lesions},\n volume = {297},\n year = {1982}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:45d3a2454ae38657c38429d2abbb3e65898d43cf",
            "@type": "ScholarlyArticle",
            "paperId": "45d3a2454ae38657c38429d2abbb3e65898d43cf",
            "corpusId": 43507282,
            "url": "https://www.semanticscholar.org/paper/45d3a2454ae38657c38429d2abbb3e65898d43cf",
            "title": "Dissociable Roles of Ventral and Dorsal Striatum in Instrumental Conditioning",
            "venue": "Science",
            "publicationVenue": {
                "id": "urn:research:f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
                "name": "Science",
                "alternate_names": null,
                "issn": "0193-4511",
                "url": "https://www.jstor.org/journal/science"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2129478155",
                "DOI": "10.1126/SCIENCE.1094285",
                "CorpusId": 43507282,
                "PubMed": "15087550"
            },
            "abstract": "Instrumental conditioning studies how animals and humans choose actions appropriate to the affective structure of an environment. According to recent reinforcement learning models, two distinct components are involved: a \u201ccritic,\u201d which learns to predict future reward, and an \u201cactor,\u201d which maintains information about the rewarding outcomes of actions to enable better ones to be chosen more frequently. We scanned human participants with functional magnetic resonance imaging while they engaged in instrumental conditioning. Our results suggest partly dissociable contributions of the ventral and dorsal striatum, with the former corresponding to the critic and the latter corresponding to the actor.",
            "referenceCount": 52,
            "citationCount": 2014,
            "influentialCitationCount": 86,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://pure.mpg.de/pubman/item/item_2614431_3/component/file_2622708/RD_Dissociable_2004.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2004-04-16",
            "journal": {
                "name": "Science",
                "volume": "304"
            },
            "citationStyles": {
                "bibtex": "@Article{O\u2019Doherty2004DissociableRO,\n author = {J. O\u2019Doherty and P. Dayan and J. Schultz and R. Deichmann and Karl J. Friston and R. Dolan},\n booktitle = {Science},\n journal = {Science},\n pages = {452 - 454},\n title = {Dissociable Roles of Ventral and Dorsal Striatum in Instrumental Conditioning},\n volume = {304},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:89c8aad71433f7638d2e2c009e1ea20e039f832d",
            "@type": "ScholarlyArticle",
            "paperId": "89c8aad71433f7638d2e2c009e1ea20e039f832d",
            "corpusId": 28328610,
            "url": "https://www.semanticscholar.org/paper/89c8aad71433f7638d2e2c009e1ea20e039f832d",
            "title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/abs-1712-05474",
                "MAG": "2776202271",
                "ArXiv": "1712.05474",
                "CorpusId": 28328610
            },
            "abstract": "We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at this http URL AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.",
            "referenceCount": 37,
            "citationCount": 738,
            "influentialCitationCount": 137,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-12-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1712.05474"
            },
            "citationStyles": {
                "bibtex": "@Article{Kolve2017AI2THORAI,\n author = {Eric Kolve and Roozbeh Mottaghi and Winson Han and Eli VanderBilt and Luca Weihs and Alvaro Herrasti and Matt Deitke and Kiana Ehsani and Daniel Gordon and Yuke Zhu and Aniruddha Kembhavi and A. Gupta and Ali Farhadi},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {AI2-THOR: An Interactive 3D Environment for Visual AI},\n volume = {abs/1712.05474},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3a56ad8d21bed73db98de27bf54b1f67ff383c17",
            "@type": "ScholarlyArticle",
            "paperId": "3a56ad8d21bed73db98de27bf54b1f67ff383c17",
            "corpusId": 10200383,
            "url": "https://www.semanticscholar.org/paper/3a56ad8d21bed73db98de27bf54b1f67ff383c17",
            "title": "Differential roles of human striatum and amygdala in associative learning",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2024910015",
                "DOI": "10.1038/nn.2904",
                "CorpusId": 10200383,
                "PubMed": "21909088"
            },
            "abstract": null,
            "referenceCount": 24,
            "citationCount": 308,
            "influentialCitationCount": 24,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc3268261?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-10-01",
            "journal": {
                "name": "Nature Neuroscience",
                "volume": "14"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2011DifferentialRO,\n author = {Jian Li and D. Schiller and G. Schoenbaum and E. Phelps and N. Daw},\n booktitle = {Nature Neuroscience},\n journal = {Nature Neuroscience},\n pages = {1250-1252},\n title = {Differential roles of human striatum and amygdala in associative learning},\n volume = {14},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7f77058976e2fe75e98280371962c43d98c98321",
            "@type": "ScholarlyArticle",
            "paperId": "7f77058976e2fe75e98280371962c43d98c98321",
            "corpusId": 46968933,
            "url": "https://www.semanticscholar.org/paper/7f77058976e2fe75e98280371962c43d98c98321",
            "title": "Adversarial Attack on Graph Structured Data",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2950317500",
                "DBLP": "journals/corr/abs-1806-02371",
                "ArXiv": "1806.02371",
                "CorpusId": 46968933
            },
            "abstract": "Deep learning on graph structures has shown exciting results in various applications. However, few attentions have been paid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this paper, we focus on the adversarial attacks that fool the model by modifying the combinatorial structure of data. We first propose a reinforcement learning based attack method that learns the generalizable attack policy, while only requiring prediction labels from the target classifier. Also, variants of genetic algorithms and gradient methods are presented in the scenario where prediction confidence or gradients are available. We use both synthetic and real-world data to show that, a family of Graph Neural Network models are vulnerable to these attacks, in both graph-level and node-level classification tasks. We also show such attacks can be used to diagnose the learned classifiers.",
            "referenceCount": 24,
            "citationCount": 595,
            "influentialCitationCount": 94,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-06-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dai2018AdversarialAO,\n author = {H. Dai and Hui Li and Tian Tian and Xin Huang and L. Wang and Jun Zhu and Le Song},\n booktitle = {International Conference on Machine Learning},\n pages = {1123-1132},\n title = {Adversarial Attack on Graph Structured Data},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3fab4d28be5ff73f42c8b307117aa8d680345ad9",
            "@type": "ScholarlyArticle",
            "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
            "corpusId": 5176587,
            "url": "https://www.semanticscholar.org/paper/3fab4d28be5ff73f42c8b307117aa8d680345ad9",
            "title": "Noisy Networks for Exploration",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2724169821",
                "ArXiv": "1706.10295",
                "DBLP": "journals/corr/FortunatoAPMOGM17",
                "CorpusId": 5176587
            },
            "abstract": "We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and $\\epsilon$-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.",
            "referenceCount": 49,
            "citationCount": 740,
            "influentialCitationCount": 92,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-06-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1706.10295"
            },
            "citationStyles": {
                "bibtex": "@Article{Fortunato2017NoisyNF,\n author = {Meire Fortunato and M. G. Azar and Bilal Piot and Jacob Menick and Ian Osband and Alex Graves and V. Mnih and R. Munos and D. Hassabis and O. Pietquin and C. Blundell and S. Legg},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Noisy Networks for Exploration},\n volume = {abs/1706.10295},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:269688bee08e2a61b7350806304a6654ba90d730",
            "@type": "ScholarlyArticle",
            "paperId": "269688bee08e2a61b7350806304a6654ba90d730",
            "corpusId": 14804899,
            "url": "https://www.semanticscholar.org/paper/269688bee08e2a61b7350806304a6654ba90d730",
            "title": "Handbook of Learning and Approximate Dynamic Programming",
            "venue": "IEEE Transactions on Automatic Control",
            "publicationVenue": {
                "id": "urn:research:1283a59c-0d1f-48c3-81d7-02172f597e70",
                "name": "IEEE Transactions on Automatic Control",
                "alternate_names": [
                    "IEEE Trans Autom Control"
                ],
                "issn": "0018-9286",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=9"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "1854776945",
                "DOI": "10.1109/9780470544785",
                "CorpusId": 14804899
            },
            "abstract": "Foreword. 1. ADP: goals, opportunities and principles. Part I: Overview. 2. Reinforcement learning and its relationship to supervised learning. 3. Model-based adaptive critic designs. 4. Guidance in the use of adaptive critics for control. 5. Direct neural dynamic programming. 6. The linear programming approach to approximate dynamic programming. 7. Reinforcement learning in large, high-dimensional state spaces. 8. Hierarchical decision making. Part II: Technical advances. 9. Improved temporal difference methods with linear function approximation. 10. Approximate dynamic programming for high-dimensional resource allocation problems. 11. Hierarchical approaches to concurrency, multiagency, and partial observability. 12. Learning and optimization - from a system theoretic perspective. 13. Robust reinforcement learning using integral-quadratic constraints. 14. Supervised actor-critic reinforcement learning. 15. BPTT and DAC - a common framework for comparison. Part III: Applications. 16. Near-optimal control via reinforcement learning. 17. Multiobjective control problems by reinforcement learning. 18. Adaptive critic based neural network for control-constrained agile missile. 19. Applications of approximate dynamic programming in power systems control. 20. Robust reinforcement learning for heating, ventilation, and air conditioning control of buildings. 21. Helicopter flight control using direct neural dynamic programming. 22. Toward dynamic stochastic optimal power flow. 23. Control, optimization, security, and self-healing of benchmark power systems.",
            "referenceCount": 57,
            "citationCount": 785,
            "influentialCitationCount": 35,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "IEEE Transactions on Automatic Control",
                "volume": "51"
            },
            "citationStyles": {
                "bibtex": "@Article{Si2006HandbookOL,\n author = {J. Si and A. Barto and Warrren B Powell and D. Wunsch},\n booktitle = {IEEE Transactions on Automatic Control},\n journal = {IEEE Transactions on Automatic Control},\n pages = {1730-1731},\n title = {Handbook of Learning and Approximate Dynamic Programming},\n volume = {51},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cc07fc48ce2a381e7f39235cef5fd10b939182c4",
            "@type": "ScholarlyArticle",
            "paperId": "cc07fc48ce2a381e7f39235cef5fd10b939182c4",
            "corpusId": 53025350,
            "url": "https://www.semanticscholar.org/paper/cc07fc48ce2a381e7f39235cef5fd10b939182c4",
            "title": "The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care",
            "venue": "Nature Network Boston",
            "publicationVenue": {
                "id": "urn:research:9e995b6d-f30b-4ab4-a13b-3dc2cc992f47",
                "name": "Nature Network Boston",
                "alternate_names": [
                    "Nat Netw Boston",
                    "Nat Med",
                    "Nature Medicine"
                ],
                "issn": "1744-7933",
                "url": "https://www.nature.com/nature/articles?code=archive_news"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2896893468",
                "DOI": "10.1038/s41591-018-0213-5",
                "CorpusId": 53025350,
                "PubMed": "30349085"
            },
            "abstract": null,
            "referenceCount": 44,
            "citationCount": 685,
            "influentialCitationCount": 56,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-10-22",
            "journal": {
                "name": "Nature Medicine",
                "volume": "24"
            },
            "citationStyles": {
                "bibtex": "@Article{Komorowski2018TheAI,\n author = {M. Komorowski and L. Celi and Omar Badawi and A. Gordon and Aldo A. Faisal},\n booktitle = {Nature Network Boston},\n journal = {Nature Medicine},\n pages = {1716 - 1720},\n title = {The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care},\n volume = {24},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c8c16a56d2a9520197da9a1546f517db5f19b204",
            "@type": "ScholarlyArticle",
            "paperId": "c8c16a56d2a9520197da9a1546f517db5f19b204",
            "corpusId": 15995898,
            "url": "https://www.semanticscholar.org/paper/c8c16a56d2a9520197da9a1546f517db5f19b204",
            "title": "Adversarial Attacks on Neural Network Policies",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/HuangPGDA17",
                "MAG": "2949103145",
                "ArXiv": "1702.02284",
                "CorpusId": 15995898
            },
            "abstract": "Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at this http URL.",
            "referenceCount": 23,
            "citationCount": 686,
            "influentialCitationCount": 72,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-02-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1702.02284"
            },
            "citationStyles": {
                "bibtex": "@Article{Huang2017AdversarialAO,\n author = {Sandy H. Huang and Nicolas Papernot and I. Goodfellow and Yan Duan and P. Abbeel},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Adversarial Attacks on Neural Network Policies},\n volume = {abs/1702.02284},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6a43d91c8d883e3463b358571125fa0ec7298b3a",
            "@type": "ScholarlyArticle",
            "paperId": "6a43d91c8d883e3463b358571125fa0ec7298b3a",
            "corpusId": 10296217,
            "url": "https://www.semanticscholar.org/paper/6a43d91c8d883e3463b358571125fa0ec7298b3a",
            "title": "Sample Efficient Actor-Critic with Experience Replay",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/iclr/0001BHMMKF17",
                "MAG": "2953009957",
                "ArXiv": "1611.01224",
                "CorpusId": 10296217
            },
            "abstract": "This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.",
            "referenceCount": 28,
            "citationCount": 677,
            "influentialCitationCount": 73,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-03",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.01224"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2016SampleEA,\n author = {Ziyun Wang and V. Bapst and N. Heess and Volodymyr Mnih and R. Munos and K. Kavukcuoglu and Nando de Freitas},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Sample Efficient Actor-Critic with Experience Replay},\n volume = {abs/1611.01224},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a762ae907b7dd71a59bd8bd98aba69dfe2de13a2",
            "@type": "ScholarlyArticle",
            "paperId": "a762ae907b7dd71a59bd8bd98aba69dfe2de13a2",
            "corpusId": 30099687,
            "url": "https://www.semanticscholar.org/paper/a762ae907b7dd71a59bd8bd98aba69dfe2de13a2",
            "title": "Emergence of Locomotion Behaviours in Rich Environments",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/HeessTSLMWTEWER17",
                "ArXiv": "1707.02286",
                "MAG": "2726187156",
                "CorpusId": 30099687
            },
            "abstract": "The reinforcement learning paradigm allows, in principle, for complex behaviours to be learned directly from simple reward signals. In practice, however, it is common to carefully hand-design the reward function to encourage a particular solution, or to derive it from demonstration data. In this paper explore how a rich environment can help to promote the learning of complex behavior. Specifically, we train agents in diverse environmental contexts, and find that this encourages the emergence of robust behaviours that perform well across a suite of tasks. We demonstrate this principle for locomotion -- behaviours that are known for their sensitivity to the choice of reward. We train several simulated bodies on a diverse set of challenging terrains and obstacles, using a simple reward function based on forward progress. Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required by the environment without explicit reward-based guidance. A visual depiction of highlights of the learned behavior can be viewed following this https URL .",
            "referenceCount": 31,
            "citationCount": 810,
            "influentialCitationCount": 62,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-07-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1707.02286"
            },
            "citationStyles": {
                "bibtex": "@Article{Heess2017EmergenceOL,\n author = {N. Heess and TB Dhruva and S. Sriram and Jay Lemmon and J. Merel and Greg Wayne and Yuval Tassa and Tom Erez and Ziyun Wang and S. Eslami and Martin A. Riedmiller and David Silver},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Emergence of Locomotion Behaviours in Rich Environments},\n volume = {abs/1707.02286},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a1c77da415e5b7cdb957c00dc7df2ef7a7dfd327",
            "@type": "ScholarlyArticle",
            "paperId": "a1c77da415e5b7cdb957c00dc7df2ef7a7dfd327",
            "corpusId": 8953026,
            "url": "https://www.semanticscholar.org/paper/a1c77da415e5b7cdb957c00dc7df2ef7a7dfd327",
            "title": "Context and behavioral processes in extinction.",
            "venue": "Learning & memory (Cold Spring Harbor, N.Y.)",
            "publicationVenue": {
                "id": "urn:research:c0bd3472-da10-46f9-8fb4-e0ca22eb094e",
                "name": "Learning & memory (Cold Spring Harbor, N.Y.)",
                "alternate_names": [
                    "Learning & Memory",
                    "Learn  mem (cold Spring Harb N.Y",
                    "Learn  Mem"
                ],
                "issn": "1072-0502",
                "url": "http://www.pubmedcentral.nih.gov/tocrender.fcgi?journal=103"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2145465887",
                "DOI": "10.1101/LM.78804",
                "CorpusId": 8953026,
                "PubMed": "15466298"
            },
            "abstract": "This article provides a selective review and integration of the behavioral literature on Pavlovian extinction. The first part reviews evidence that extinction does not destroy the original learning, but instead generates new learning that is especially context-dependent. The second part examines insights provided by research on several related behavioral phenomena (the interference paradigms, conditioned inhibition, and inhibition despite reinforcement). The final part examines four potential causes of extinction: the discrimination of a new reinforcement rate, generalization decrement, response inhibition, and violation of a reinforcer expectation. The data are consistent with behavioral models that emphasize the role of generalization decrement and expectation violation, but would be more so if those models were expanded to better accommodate the finding that extinction involves a context-modulated form of inhibitory learning.",
            "referenceCount": 124,
            "citationCount": 1643,
            "influentialCitationCount": 136,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://learnmem.cshlp.org/content/11/5/485.full.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2004-09-01",
            "journal": {
                "name": "Learning & memory",
                "volume": "11 5"
            },
            "citationStyles": {
                "bibtex": "@Article{Bouton2004ContextAB,\n author = {M. Bouton},\n booktitle = {Learning & memory (Cold Spring Harbor, N.Y.)},\n journal = {Learning & memory},\n pages = {\n          485-94\n        },\n title = {Context and behavioral processes in extinction.},\n volume = {11 5},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:84680b30a20775e5d319419a7f3f2a93e57c2a61",
            "@type": "ScholarlyArticle",
            "paperId": "84680b30a20775e5d319419a7f3f2a93e57c2a61",
            "corpusId": 11374605,
            "url": "https://www.semanticscholar.org/paper/84680b30a20775e5d319419a7f3f2a93e57c2a61",
            "title": "Value Iteration Networks",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2741197988",
                "DBLP": "conf/ijcai/TamarWTLA17",
                "ArXiv": "1602.02867",
                "DOI": "10.24963/ijcai.2017/700",
                "CorpusId": 11374605
            },
            "abstract": "We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.",
            "referenceCount": 65,
            "citationCount": 587,
            "influentialCitationCount": 88,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ijcai.org/proceedings/2017/0700.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-02-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tamar2016ValueIN,\n author = {Aviv Tamar and S. Levine and P. Abbeel and Yi Wu and G. Thomas},\n booktitle = {Neural Information Processing Systems},\n pages = {2146-2154},\n title = {Value Iteration Networks},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dbfb44b9fa32c669f84c859287332909ea6c6b7c",
            "@type": "ScholarlyArticle",
            "paperId": "dbfb44b9fa32c669f84c859287332909ea6c6b7c",
            "corpusId": 13929400,
            "url": "https://www.semanticscholar.org/paper/dbfb44b9fa32c669f84c859287332909ea6c6b7c",
            "title": "Neuronal Reward and Decision Signals: From Theories to Data.",
            "venue": "Physiological Reviews",
            "publicationVenue": {
                "id": "urn:research:d2a88b2a-7295-4af6-8f52-99424522515b",
                "name": "Physiological Reviews",
                "alternate_names": [
                    "Physiol Rev"
                ],
                "issn": "0031-9333",
                "url": "https://www.physiology.org/journal/physrev"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1944839424",
                "DOI": "10.1152/physrev.00023.2014",
                "CorpusId": 13929400,
                "PubMed": "26109341"
            },
            "abstract": "Rewards are crucial objects that induce learning, approach behavior, choices, and emotions. Whereas emotions are difficult to investigate in animals, the learning function is mediated by neuronal reward prediction error signals which implement basic constructs of reinforcement learning theory. These signals are found in dopamine neurons, which emit a global reward signal to striatum and frontal cortex, and in specific neurons in striatum, amygdala, and frontal cortex projecting to select neuronal populations. The approach and choice functions involve subjective value, which is objectively assessed by behavioral choices eliciting internal, subjective reward preferences. Utility is the formal mathematical characterization of subjective value and a prime decision variable in economic choice theory. It is coded as utility prediction error by phasic dopamine responses. Utility can incorporate various influences, including risk, delay, effort, and social interaction. Appropriate for formal decision mechanisms, rewards are coded as object value, action value, difference value, and chosen value by specific neurons. Although all reward, reinforcement, and decision variables are theoretical constructs, their neuronal signals constitute measurable physical implementations and as such confirm the validity of these concepts. The neuronal reward signals provide guidance for behavior while constraining the free will to act.",
            "referenceCount": 662,
            "citationCount": 796,
            "influentialCitationCount": 46,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2015-07-01",
            "journal": {
                "name": "Physiological reviews",
                "volume": "95 3"
            },
            "citationStyles": {
                "bibtex": "@Article{Schultz2015NeuronalRA,\n author = {W. Schultz},\n booktitle = {Physiological Reviews},\n journal = {Physiological reviews},\n pages = {\n          853-951\n        },\n title = {Neuronal Reward and Decision Signals: From Theories to Data.},\n volume = {95 3},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:481bd9ed53700ca2a1f1361fc18abc108639d78b",
            "@type": "ScholarlyArticle",
            "paperId": "481bd9ed53700ca2a1f1361fc18abc108639d78b",
            "corpusId": 1869663,
            "url": "https://www.semanticscholar.org/paper/481bd9ed53700ca2a1f1361fc18abc108639d78b",
            "title": "Inductive Logic Programming",
            "venue": "Lecture Notes in Computer Science",
            "publicationVenue": {
                "id": "urn:research:2f5d0e8a-faad-4f10-b323-2b2e3c439a78",
                "name": "Lecture Notes in Computer Science",
                "alternate_names": [
                    "LNCS",
                    "Transactions on Computational Systems Biology",
                    "Trans Comput Syst Biology",
                    "Lect Note Comput Sci"
                ],
                "issn": "0302-9743",
                "url": "http://www.springer.com/lncs"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2484864464",
                "DBLP": "conf/ilp/2013",
                "DOI": "10.1007/978-3-662-44923-3",
                "CorpusId": 1869663
            },
            "abstract": null,
            "referenceCount": 24,
            "citationCount": 573,
            "influentialCitationCount": 87,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://repositorio.inesctec.pt/bitstreams/9f4d007f-deb6-4d03-b6bd-06b06f129294/download",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2014-09-24",
            "journal": {
                "name": null,
                "volume": "8812"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Zaverucha2014InductiveLP,\n author = {Gerson Zaverucha and V. S. Costa and A. Paes},\n booktitle = {Lecture Notes in Computer Science},\n title = {Inductive Logic Programming},\n volume = {8812},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1c4927af526d5c28f7c2cfa492ece192d80a61d4",
            "@type": "ScholarlyArticle",
            "paperId": "1c4927af526d5c28f7c2cfa492ece192d80a61d4",
            "corpusId": 1923568,
            "url": "https://www.semanticscholar.org/paper/1c4927af526d5c28f7c2cfa492ece192d80a61d4",
            "title": "Policy Distillation",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1511.06295",
                "MAG": "3038067716",
                "DBLP": "journals/corr/RusuCGDKPMKH15",
                "CorpusId": 1923568
            },
            "abstract": "Abstract: Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.",
            "referenceCount": 28,
            "citationCount": 557,
            "influentialCitationCount": 58,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-19",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1511.06295"
            },
            "citationStyles": {
                "bibtex": "@Article{Rusu2015PolicyD,\n author = {Andrei A. Rusu and Sergio Gomez Colmenarejo and \u00c7aglar G\u00fcl\u00e7ehre and Guillaume Desjardins and J. Kirkpatrick and Razvan Pascanu and Volodymyr Mnih and K. Kavukcuoglu and R. Hadsell},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Policy Distillation},\n volume = {abs/1511.06295},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:33a2c0f3b9a0adc452a13d53f950c0c3c4abb11b",
            "@type": "ScholarlyArticle",
            "paperId": "33a2c0f3b9a0adc452a13d53f950c0c3c4abb11b",
            "corpusId": 202583612,
            "url": "https://www.semanticscholar.org/paper/33a2c0f3b9a0adc452a13d53f950c0c3c4abb11b",
            "title": "Emergent Tool Use From Multi-Agent Autocurricula",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1909.07528",
                "DBLP": "conf/iclr/BakerKMWPMM20",
                "MAG": "2973525135",
                "CorpusId": 202583612
            },
            "abstract": "Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.",
            "referenceCount": 79,
            "citationCount": 524,
            "influentialCitationCount": 48,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1909.07528"
            },
            "citationStyles": {
                "bibtex": "@Article{Baker2019EmergentTU,\n author = {Bowen Baker and I. Kanitscheider and Todor M. Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Emergent Tool Use From Multi-Agent Autocurricula},\n volume = {abs/1909.07528},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3a71c306eb6232658c9e5fd48aed1ef3befe5fbe",
            "@type": "ScholarlyArticle",
            "paperId": "3a71c306eb6232658c9e5fd48aed1ef3befe5fbe",
            "corpusId": 216559511,
            "url": "https://www.semanticscholar.org/paper/3a71c306eb6232658c9e5fd48aed1ef3befe5fbe",
            "title": "Planning to Explore via Self-Supervised World Models",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/icml/SekarRDAHP20",
                "ArXiv": "2005.05960",
                "MAG": "3025660841",
                "CorpusId": 216559511
            },
            "abstract": "Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at this https URL",
            "referenceCount": 61,
            "citationCount": 281,
            "influentialCitationCount": 36,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-05-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sekar2020PlanningTE,\n author = {Ramanan Sekar and Oleh Rybkin and Kostas Daniilidis and P. Abbeel and Danijar Hafner and Deepak Pathak},\n booktitle = {International Conference on Machine Learning},\n pages = {8583-8592},\n title = {Planning to Explore via Self-Supervised World Models},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:32c9f75a7751a7bf47472ccc3cf2703b1cb132a3",
            "@type": "ScholarlyArticle",
            "paperId": "32c9f75a7751a7bf47472ccc3cf2703b1cb132a3",
            "corpusId": 12688746,
            "url": "https://www.semanticscholar.org/paper/32c9f75a7751a7bf47472ccc3cf2703b1cb132a3",
            "title": "Two-process learning theory: Relationships between Pavlovian conditioning and instrumental learning.",
            "venue": "Psychology Review",
            "publicationVenue": {
                "id": "urn:research:dfb7f114-609c-4c34-9ee7-8cb1fc3e4a6b",
                "name": "Psychology Review",
                "alternate_names": [
                    "Psychol rev",
                    "Psychological review",
                    "Psychol Rev",
                    "Psychological Review"
                ],
                "issn": "1354-1129",
                "url": "http://www.apa.org/journals/rev/"
            },
            "year": 1967,
            "externalIds": {
                "MAG": "2039148875",
                "DOI": "10.1037/H0024475",
                "CorpusId": 12688746,
                "PubMed": "5342881"
            },
            "abstract": "The history of 2-process learning theory is described, and the logical and empirical validity of its major postulates is examined. The assumption of 2 acquisition processes requires the demonstration of an empirical interaction between 2 types of reinforcement contingencies and (a) response classes, (b) reinforcing stimulus classes, or (c) characteristics of the learned behavior itself. The mediation postulates of 2-process theory which argue that CRs are intimately involved in the control of instrumental responding are emphasized, and 2 major lines of evidence that stem uniquely from these postulates are examined : (a) the concurrent development and maintenance of instrumental responses and conditioned reflexes, and (b) the interaction between separately conducted Pavlovian conditioning contingencies and instrumental training contingencies in the control of instrumental behavior. The evidence from concurrent measurement studies provides, at the very best, only weak support for the mediational hypotheses of 2-process theory. In contrast, the evidence from interaction studies shows the strong mediating control of instrumental responses by Pavlovian conditioning procedures, and demonstrates the surprising power of Pavlovian concepts in predicting the outcomes of many kinds of interaction experiments.",
            "referenceCount": 101,
            "citationCount": 1434,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "1967-05-01",
            "journal": {
                "name": "Psychological review",
                "volume": "74 3"
            },
            "citationStyles": {
                "bibtex": "@Article{Rescorla1967TwoprocessLT,\n author = {R. Rescorla and R. Solomon},\n booktitle = {Psychology Review},\n journal = {Psychological review},\n pages = {\n          151-82\n        },\n title = {Two-process learning theory: Relationships between Pavlovian conditioning and instrumental learning.},\n volume = {74 3},\n year = {1967}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4ff04ef961094ba08587cb572eccc5382ac63d8f",
            "@type": "ScholarlyArticle",
            "paperId": "4ff04ef961094ba08587cb572eccc5382ac63d8f",
            "corpusId": 6517454,
            "url": "https://www.semanticscholar.org/paper/4ff04ef961094ba08587cb572eccc5382ac63d8f",
            "title": "The orbitofrontal cortex and reward.",
            "venue": "Cerebral Cortex",
            "publicationVenue": {
                "id": "urn:research:388efbe2-cd51-4399-93f6-ba85b8300840",
                "name": "Cerebral Cortex",
                "alternate_names": [
                    "Cereb Cortex"
                ],
                "issn": "1047-3211",
                "url": "https://academic.oup.com/cercor"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2134806750",
                "DOI": "10.1093/CERCOR/10.3.284",
                "CorpusId": 6517454,
                "PubMed": "10731223"
            },
            "abstract": "The primate orbitofrontal cortex contains the secondary taste cortex, in which the reward value of taste is represented. It also contains the secondary and tertiary olfactory cortical areas, in which information about the identity and also about the reward value of odors is represented. The orbitofrontal cortex also receives information about the sight of objects and faces from the temporal lobe cortical visual areas, and neurons in it learn and reverse the visual stimulus to which they respond when the association of the visual stimulus with a primary reinforcing stimulus (such as a taste reward) is reversed. However, the orbitofrontal cortex is involved in representing negative reinforcers (punishers) too, such as aversive taste, and in rapid stimulus-reinforcement association learning for both positive and negative primary reinforcers. In complementary neuroimaging studies in humans it is being found that areas of the orbitofrontal cortex (and connected subgenual cingulate cortex) are activated by pleasant touch, by painful touch, by rewarding and aversive taste, and by odor. Damage to the orbitofrontal cortex in humans can impair the learning and reversal of stimulus- reinforcement associations, and thus the correction of behavioral responses when these are no longer appropriate because previous reinforcement contingencies change. This evidence thus shows that the orbitofrontal cortex is involved in decoding and representing some primary reinforcers such as taste and touch; in learning and reversing associations of visual and other stimuli to these primary reinforcers; and in controlling and correcting reward-related and punishment-related behavior, and thus in emotion.",
            "referenceCount": 116,
            "citationCount": 1564,
            "influentialCitationCount": 81,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://academic.oup.com/cercor/article-pdf/10/3/284/9751041/100284.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2000-03-01",
            "journal": {
                "name": "Cerebral cortex",
                "volume": "10 3"
            },
            "citationStyles": {
                "bibtex": "@Article{Rolls2000TheOC,\n author = {E. Rolls},\n booktitle = {Cerebral Cortex},\n journal = {Cerebral cortex},\n pages = {\n          284-94\n        },\n title = {The orbitofrontal cortex and reward.},\n volume = {10 3},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:38688edefc7591ea2fc7d4294070e8bfe9d9ac3d",
            "@type": "ScholarlyArticle",
            "paperId": "38688edefc7591ea2fc7d4294070e8bfe9d9ac3d",
            "corpusId": 11675086,
            "url": "https://www.semanticscholar.org/paper/38688edefc7591ea2fc7d4294070e8bfe9d9ac3d",
            "title": "Learning Attractor Landscapes for Learning Motor Primitives",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2002,
            "externalIds": {
                "DBLP": "conf/nips/IjspeertNS02",
                "MAG": "2123967136",
                "CorpusId": 11675086
            },
            "abstract": "Many control problems take place in continuous state-action spaces, e.g., as in manipulator robotics, where the control objective is often defined as finding a desired trajectory that reaches a particular goal state. While reinforcement learning offers a theoretical framework to learn such control policies from scratch, its applicability to higher dimensional continuous state-action spaces remains rather limited to date. Instead of learning from scratch, in this paper we suggest to learn a desired complex control policy by transforming an existing simple canonical control policy. For this purpose, we represent canonical policies in terms of differential equations with well-defined attractor properties. By nonlinearly transforming the canonical attractor dynamics using techniques from nonparametric regression, almost arbitrary new nonlinear policies can be generated without losing the stability properties of the canonical system. We demonstrate our techniques in the context of learning a set of movement skills for a humanoid robot from demonstrations of a human teacher. Policies are acquired rapidly, and, due to the properties of well formulated differential equations, can be re-used and modified on-line under dynamic changes of the environment. The linear parameterization of nonparametric regression moreover lends itself to recognize and classify previously learned movement skills. Evaluations in simulations and on an actual 30 degree-of-freedom humanoid robot exemplify the feasibility and robustness of our approach.",
            "referenceCount": 9,
            "citationCount": 719,
            "influentialCitationCount": 89,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ijspeert2002LearningAL,\n author = {A. Ijspeert and J. Nakanishi and S. Schaal},\n booktitle = {Neural Information Processing Systems},\n pages = {1523-1530},\n title = {Learning Attractor Landscapes for Learning Motor Primitives},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4d6a87d76ec0c0379f0afbf24f84bba848c6246e",
            "@type": "ScholarlyArticle",
            "paperId": "4d6a87d76ec0c0379f0afbf24f84bba848c6246e",
            "corpusId": 3143757,
            "url": "https://www.semanticscholar.org/paper/4d6a87d76ec0c0379f0afbf24f84bba848c6246e",
            "title": "Policy search for motor primitives in robotics",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/nips/KoberP08",
                "MAG": "2167117957",
                "DOI": "10.1007/s10994-010-5223-6",
                "CorpusId": 3143757
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 731,
            "influentialCitationCount": 84,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2Fs10994-010-5223-6.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-12-08",
            "journal": {
                "name": "Machine Learning",
                "volume": "84"
            },
            "citationStyles": {
                "bibtex": "@Article{None,\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {171-203},\n title = {Policy search for motor primitives in robotics},\n volume = {84},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1e756eef31013bd185ea8ca17c4f304cbda57750",
            "@type": "ScholarlyArticle",
            "paperId": "1e756eef31013bd185ea8ca17c4f304cbda57750",
            "corpusId": 11794468,
            "url": "https://www.semanticscholar.org/paper/1e756eef31013bd185ea8ca17c4f304cbda57750",
            "title": "Learning and tuning fuzzy logic controllers through reinforcements",
            "venue": "IEEE Trans. Neural Networks",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1992,
            "externalIds": {
                "MAG": "2158316397",
                "DBLP": "journals/tnn/BerenjiK92",
                "DOI": "10.1109/72.159061",
                "CorpusId": 11794468,
                "PubMed": "18276471"
            },
            "abstract": "A method for learning and tuning a fuzzy logic controller based on reinforcements from a dynamic system is presented. It is shown that: the generalized approximate-reasoning-based intelligent control (GARIC) architecture learns and tunes a fuzzy logic controller even when only weak reinforcement, such as a binary failure signal, is available; introduces a new conjunction operator in computing the rule strengths of fuzzy control rules; introduces a new localized mean of maximum (LMOM) method in combining the conclusions of several firing control rules; and learns to produce real-valued control actions. Learning is achieved by integrating fuzzy inference into a feedforward network, which can then adaptively improve performance by using gradient descent methods. The GARIC architecture is applied to a cart-pole balancing system and demonstrates significant improvements in terms of the speed of learning and robustness to changes in the dynamic system's parameters over previous schemes for cart-pole balancing.",
            "referenceCount": 30,
            "citationCount": 1023,
            "influentialCitationCount": 54,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ntrs.nasa.gov/api/citations/19920019516/downloads/19920019516.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1992-09-01",
            "journal": {
                "name": "IEEE transactions on neural networks",
                "volume": "3 5"
            },
            "citationStyles": {
                "bibtex": "@Article{Berenji1992LearningAT,\n author = {H. Berenji and P. Khedkar},\n booktitle = {IEEE Trans. Neural Networks},\n journal = {IEEE transactions on neural networks},\n pages = {\n          724-40\n        },\n title = {Learning and tuning fuzzy logic controllers through reinforcements},\n volume = {3 5},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:435c755a6c573da0e213fa6dacbf5197c2f0e2e1",
            "@type": "ScholarlyArticle",
            "paperId": "435c755a6c573da0e213fa6dacbf5197c2f0e2e1",
            "corpusId": 4400823,
            "url": "https://www.semanticscholar.org/paper/435c755a6c573da0e213fa6dacbf5197c2f0e2e1",
            "title": "A cellular mechanism of reward-related learning",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2055523610",
                "DOI": "10.1038/35092560",
                "CorpusId": 4400823,
                "PubMed": "11544526"
            },
            "abstract": null,
            "referenceCount": 42,
            "citationCount": 772,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-09-06",
            "journal": {
                "name": "Nature",
                "volume": "413"
            },
            "citationStyles": {
                "bibtex": "@Article{Reynolds2001ACM,\n author = {J. Reynolds and B. Hyland and J. Wickens},\n booktitle = {Nature},\n journal = {Nature},\n pages = {67-70},\n title = {A cellular mechanism of reward-related learning},\n volume = {413},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:21d4345f5105d45072a665a8cd7354e4dc21dad6",
            "@type": "ScholarlyArticle",
            "paperId": "21d4345f5105d45072a665a8cd7354e4dc21dad6",
            "corpusId": 16676796,
            "url": "https://www.semanticscholar.org/paper/21d4345f5105d45072a665a8cd7354e4dc21dad6",
            "title": "Neuronal coding of prediction errors.",
            "venue": "Annual Review of Neuroscience",
            "publicationVenue": {
                "id": "urn:research:d9caa671-3be6-48bc-9710-f96334848b4c",
                "name": "Annual Review of Neuroscience",
                "alternate_names": [
                    "Annu Rev Neurosci"
                ],
                "issn": "0147-006X",
                "url": "https://www.annualreviews.org/journal/neuro"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2131045394",
                "DOI": "10.1146/ANNUREV.NEURO.23.1.473",
                "CorpusId": 16676796,
                "PubMed": "10845072"
            },
            "abstract": "Associative learning enables animals to anticipate the occurrence of important outcomes. Learning occurs when the actual outcome differs from the predicted outcome, resulting in a prediction error. Neurons in several brain structures appear to code prediction errors in relation to rewards, punishments, external stimuli, and behavioral reactions. In one form, dopamine neurons, norepinephrine neurons, and nucleus basalis neurons broadcast prediction errors as global reinforcement or teaching signals to large postsynaptic structures. In other cases, error signals are coded by selected neurons in the cerebellum, superior colliculus, frontal eye fields, parietal cortex, striatum, and visual system, where they influence specific subgroups of neurons. Prediction errors can be used in postsynaptic structures for the immediate selection of behavior or for synaptic changes underlying behavioral learning. The coding of prediction errors may represent a basic mode of brain function that may also contribute to the processing of sensory information and the short-term control of behavior.",
            "referenceCount": 101,
            "citationCount": 1412,
            "influentialCitationCount": 57,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Biology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Annual review of neuroscience",
                "volume": "23"
            },
            "citationStyles": {
                "bibtex": "@Article{Schultz2000NeuronalCO,\n author = {W. Schultz and A. Dickinson},\n booktitle = {Annual Review of Neuroscience},\n journal = {Annual review of neuroscience},\n pages = {\n          473-500\n        },\n title = {Neuronal coding of prediction errors.},\n volume = {23},\n year = {2000}\n}\n"
            }
        }
    }
]