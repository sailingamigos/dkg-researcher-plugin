[
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18604v1",
            "title": "Anaphor Assisted Document-Level Relation Extraction",
            "updated": "2023-10-28T06:11:18Z",
            "published": "2023-10-28T06:11:18Z",
            "summary": "Document-level relation extraction (DocRE) involves identifying relations\nbetween entities distributed in multiple sentences within a document. Existing\nmethods focus on building a heterogeneous document graph to model the internal\nstructure of an entity and the external interaction between entities. However,\nthere are two drawbacks in existing methods. On one hand, anaphor plays an\nimportant role in reasoning to identify relations between entities but is\nignored by these methods. On the other hand, these methods achieve\ncross-sentence entity interactions implicitly by utilizing a document or\nsentences as intermediate nodes. Such an approach has difficulties in learning\nfine-grained interactions between entities across different sentences,\nresulting in sub-optimal performance. To address these issues, we propose an\nAnaphor-Assisted (AA) framework for DocRE tasks. Experimental results on the\nwidely-used datasets demonstrate that our model achieves a new state-of-the-art\nperformance.",
            "author": [
                "Chonggang Lu",
                "Richong Zhang",
                "Kai Sun",
                "Jaein Kim",
                "Cunwang Zhang",
                "Yongyi Mao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18604v1",
                "http://arxiv.org/pdf/2310.18604v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18603v1",
            "title": "Large Language Models Are Better Adversaries: Exploring Generative\n  Clean-Label Backdoor Attacks Against Text Classifiers",
            "updated": "2023-10-28T06:11:07Z",
            "published": "2023-10-28T06:11:07Z",
            "summary": "Backdoor attacks manipulate model predictions by inserting innocuous triggers\ninto training and test data. We focus on more realistic and more challenging\nclean-label attacks where the adversarial training examples are correctly\nlabeled. Our attack, LLMBkd, leverages language models to automatically insert\ndiverse style-based triggers into texts. We also propose a poison selection\ntechnique to improve the effectiveness of both LLMBkd as well as existing\ntextual backdoor attacks. Lastly, we describe REACT, a baseline defense to\nmitigate backdoor attacks via antidote training examples. Our evaluations\ndemonstrate LLMBkd's effectiveness and efficiency, where we consistently\nachieve high attack success rates across a wide range of styles with little\neffort and no model training.",
            "author": [
                "Wencong You",
                "Zayd Hammoudeh",
                "Daniel Lowd"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18603v1",
                "http://arxiv.org/pdf/2310.18603v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18601v1",
            "title": "Online Decision Mediation",
            "updated": "2023-10-28T05:59:43Z",
            "published": "2023-10-28T05:59:43Z",
            "summary": "Consider learning a decision support assistant to serve as an intermediary\nbetween (oracle) expert behavior and (imperfect) human behavior: At each time,\nthe algorithm observes an action chosen by a fallible agent, and decides\nwhether to *accept* that agent's decision, *intervene* with an alternative, or\n*request* the expert's opinion. For instance, in clinical diagnosis,\nfully-autonomous machine behavior is often beyond ethical affordances, thus\nreal-world decision support is often limited to monitoring and forecasting.\nInstead, such an intermediary would strike a prudent balance between the former\n(purely prescriptive) and latter (purely descriptive) approaches, while\nproviding an efficient interface between human mistakes and expert feedback. In\nthis work, we first formalize the sequential problem of *online decision\nmediation* -- that is, of simultaneously learning and evaluating mediator\npolicies from scratch with *abstentive feedback*: In each round, deferring to\nthe oracle obviates the risk of error, but incurs an upfront penalty, and\nreveals the otherwise hidden expert action as a new training data point.\nSecond, we motivate and propose a solution that seeks to trade off (immediate)\nloss terms against (future) improvements in generalization error; in doing so,\nwe identify why conventional bandit algorithms may fail. Finally, through\nexperiments and sensitivities on a variety of datasets, we illustrate\nconsistent gains over applicable benchmarks on performance measures with\nrespect to the mediator policy, the learned model, and the decision-making\nsystem as a whole.",
            "author": [
                "Daniel Jarrett",
                "Alihan H\u00fcy\u00fck",
                "Mihaela van der Schaar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18601v1",
                "http://arxiv.org/pdf/2310.18601v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18600v1",
            "title": "MILDSum: A Novel Benchmark Dataset for Multilingual Summarization of\n  Indian Legal Case Judgments",
            "updated": "2023-10-28T05:51:57Z",
            "published": "2023-10-28T05:51:57Z",
            "summary": "Automatic summarization of legal case judgments is a practically important\nproblem that has attracted substantial research efforts in many countries. In\nthe context of the Indian judiciary, there is an additional complexity --\nIndian legal case judgments are mostly written in complex English, but a\nsignificant portion of India's population lacks command of the English\nlanguage. Hence, it is crucial to summarize the legal documents in Indian\nlanguages to ensure equitable access to justice. While prior research primarily\nfocuses on summarizing legal case judgments in their source languages, this\nstudy presents a pioneering effort toward cross-lingual summarization of\nEnglish legal documents into Hindi, the most frequently spoken Indian language.\nWe construct the first high-quality legal corpus comprising of 3,122 case\njudgments from prominent Indian courts in English, along with their summaries\nin both English and Hindi, drafted by legal practitioners. We benchmark the\nperformance of several diverse summarization approaches on our corpus and\ndemonstrate the need for further research in cross-lingual summarization in the\nlegal domain.",
            "author": [
                "Debtanu Datta",
                "Shubham Soni",
                "Rajdeep Mukherjee",
                "Saptarshi Ghosh"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18600v1",
                "http://arxiv.org/pdf/2310.18600v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18593v1",
            "title": "Fair Streaming Principal Component Analysis: Statistical and Algorithmic\n  Viewpoint",
            "updated": "2023-10-28T05:09:30Z",
            "published": "2023-10-28T05:09:30Z",
            "summary": "Fair Principal Component Analysis (PCA) is a problem setting where we aim to\nperform PCA while making the resulting representation fair in that the\nprojected distributions, conditional on the sensitive attributes, match one\nanother. However, existing approaches to fair PCA have two main problems:\ntheoretically, there has been no statistical foundation of fair PCA in terms of\nlearnability; practically, limited memory prevents us from using existing\napproaches, as they explicitly rely on full access to the entire data. On the\ntheoretical side, we rigorously formulate fair PCA using a new notion called\n\\emph{probably approximately fair and optimal} (PAFO) learnability. On the\npractical side, motivated by recent advances in streaming algorithms for\naddressing memory limitation, we propose a new setting called \\emph{fair\nstreaming PCA} along with a memory-efficient algorithm, fair noisy power method\n(FNPM). We then provide its {\\it statistical} guarantee in terms of\nPAFO-learnability, which is the first of its kind in fair PCA literature.\nLastly, we verify the efficacy and memory efficiency of our algorithm on\nreal-world datasets.",
            "author": [
                "Junghyun Lee",
                "Hanseul Cho",
                "Se-Young Yun",
                "Chulhee Yun"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18593v1",
                "http://arxiv.org/pdf/2310.18593v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18591v1",
            "title": "Inverse Decision Modeling: Learning Interpretable Representations of\n  Behavior",
            "updated": "2023-10-28T05:05:01Z",
            "published": "2023-10-28T05:05:01Z",
            "summary": "Decision analysis deals with modeling and enhancing decision processes. A\nprincipal challenge in improving behavior is in obtaining a transparent\ndescription of existing behavior in the first place. In this paper, we develop\nan expressive, unifying perspective on inverse decision modeling: a framework\nfor learning parameterized representations of sequential decision behavior.\nFirst, we formalize the forward problem (as a normative standard), subsuming\ncommon classes of control behavior. Second, we use this to formalize the\ninverse problem (as a descriptive model), generalizing existing work on\nimitation/reward learning -- while opening up a much broader class of research\nproblems in behavior representation. Finally, we instantiate this approach with\nan example (inverse bounded rational control), illustrating how this structure\nenables learning (interpretable) representations of (bounded) rationality --\nwhile naturally capturing intuitive notions of suboptimal actions, biased\nbeliefs, and imperfect knowledge of environments.",
            "author": [
                "Daniel Jarrett",
                "Alihan H\u00fcy\u00fck",
                "Mihaela van der Schaar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18591v1",
                "http://arxiv.org/pdf/2310.18591v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18590v2",
            "title": "Using Early Readouts to Mediate Featural Bias in Distillation",
            "updated": "2023-11-08T13:13:13Z",
            "published": "2023-10-28T04:58:15Z",
            "summary": "Deep networks tend to learn spurious feature-label correlations in real-world\nsupervised learning tasks. This vulnerability is aggravated in distillation,\nwhere a student model may have lesser representational capacity than the\ncorresponding teacher model. Often, knowledge of specific spurious correlations\nis used to reweight instances & rebalance the learning process. We propose a\nnovel early readout mechanism whereby we attempt to predict the label using\nrepresentations from earlier network layers. We show that these early readouts\nautomatically identify problem instances or groups in the form of confident,\nincorrect predictions. Leveraging these signals to modulate the distillation\nloss on an instance level allows us to substantially improve not only group\nfairness measures across benchmark datasets, but also overall accuracy of the\nstudent model. We also provide secondary analyses that bring insight into the\nrole of feature learning in supervision and distillation.",
            "author": [
                "Rishabh Tiwari",
                "Durga Sivasubramanian",
                "Anmol Mekala",
                "Ganesh Ramakrishnan",
                "Pradeep Shenoy"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18590v2",
                "http://arxiv.org/pdf/2310.18590v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18589v1",
            "title": "This Looks Like Those: Illuminating Prototypical Concepts Using Multiple\n  Visualizations",
            "updated": "2023-10-28T04:54:48Z",
            "published": "2023-10-28T04:54:48Z",
            "summary": "We present ProtoConcepts, a method for interpretable image classification\ncombining deep learning and case-based reasoning using prototypical parts.\nExisting work in prototype-based image classification uses a ``this looks like\nthat'' reasoning process, which dissects a test image by finding prototypical\nparts and combining evidence from these prototypes to make a final\nclassification. However, all of the existing prototypical part-based image\nclassifiers provide only one-to-one comparisons, where a single training image\npatch serves as a prototype to compare with a part of our test image. With\nthese single-image comparisons, it can often be difficult to identify the\nunderlying concept being compared (e.g., ``is it comparing the color or the\nshape?''). Our proposed method modifies the architecture of prototype-based\nnetworks to instead learn prototypical concepts which are visualized using\nmultiple image patches. Having multiple visualizations of the same prototype\nallows us to more easily identify the concept captured by that prototype (e.g.,\n``the test image and the related training patches are all the same shade of\nblue''), and allows our model to create richer, more interpretable visual\nexplanations. Our experiments show that our ``this looks like those'' reasoning\nprocess can be applied as a modification to a wide range of existing\nprototypical image classification networks while achieving comparable accuracy\non benchmark datasets.",
            "author": [
                "Chiyu Ma",
                "Brandon Zhao",
                "Chaofan Chen",
                "Cynthia Rudin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18589v1",
                "http://arxiv.org/pdf/2310.18589v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18588v1",
            "title": "Defining equations of $7$-dimensional model CR hypersurfaces and models\n  in $\\mathbb{C}^{N}$",
            "updated": "2023-10-28T04:51:42Z",
            "published": "2023-10-28T04:51:42Z",
            "summary": "We investigate a class of CR invariants of uniformly $2$-nondegenerate CR\nhypersurfaces, termed modified symbols, and develop methods for deriving\ndefining equations of CR hypersurfaces realizing different values of these\ninvariants. For hypersurfaces in $\\mathbb{C}^4$, we obtain the full\nclassification of modified symbols realizable by weighted homogeneous (with\nrespect to a natural weighting system) rigid CR hypersurfaces, and obtain\ndefining equations for the recently classified $7$-dimensional homogeneous\nmodified symbol models. Moreover, we characterize, up to CR equivalence, all\nweighted homogeneous rigid uniformly $2$-nondegenerate CR hypersurfaces in\n$\\mathbb{C}^{N}$ whose holomorphic infinitesimal symmetries generate a\ntransitive action on their Levi leaf spaces.",
            "author": [
                "Jan Gregorovi\u010d",
                "David Sykes"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18588v1",
                "http://arxiv.org/pdf/2310.18588v1"
            ],
            "primary_category": "math.CV",
            "category": [
                "math.CV",
                "math.DG",
                "32V05, 32V40, 53C30"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18587v1",
            "title": "Assessing and Improving Syntactic Adversarial Robustness of Pre-trained\n  Models for Code Translation",
            "updated": "2023-10-28T04:35:24Z",
            "published": "2023-10-28T04:35:24Z",
            "summary": "Context: Pre-trained models (PTMs) have demonstrated significant potential in\nautomatic code translation. However, the vulnerability of these models in\ntranslation tasks, particularly in terms of syntax, has not been extensively\ninvestigated. Objective: To fill this gap, our study aims to propose a novel\napproach CoTR to assess and improve the syntactic adversarial robustness of\nPTMs in code translation. Method: CoTR consists of two components: CoTR-A and\nCoTR-D. CoTR-A generates adversarial examples by transforming programs, while\nCoTR-D proposes a semantic distance-based sampling data augmentation method and\nadversarial training method to improve the model's robustness and\ngeneralization capabilities. The Pass@1 metric is used by CoTR to assess the\nperformance of PTMs, which is more suitable for code translation tasks and\noffers a more precise evaluation in real world scenarios. Results: The\neffectiveness of CoTR is evaluated through experiments on real world Java to\nPython datasets. The results demonstrate that CoTR-A can significantly reduce\nthe performance of existing PTMs, while CoTR-D effectively improves the\nrobustness of PTMs. Conclusion: Our study identifies the limitations of current\nPTMs, including large language models, in code translation tasks. It highlights\nthe potential of CoTR as an effective solution to enhance the robustness of\nPTMs for code translation tasks.",
            "author": [
                "Guang Yang",
                "Yu Zhou",
                "Xiangyu Zhang",
                "Xiang Chen",
                "Tingting Han",
                "Taolue Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18587v1",
                "http://arxiv.org/pdf/2310.18587v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18586v1",
            "title": "Optimal Transport for Kernel Gaussian Mixture Models",
            "updated": "2023-10-28T04:31:49Z",
            "published": "2023-10-28T04:31:49Z",
            "summary": "The Wasserstein distance from optimal mass transport (OMT) is a powerful\nmathematical tool with numerous applications that provides a natural measure of\nthe distance between two probability distributions. Several methods to\nincorporate OMT into widely used probabilistic models, such as Gaussian or\nGaussian mixture, have been developed to enhance the capability of modeling\ncomplex multimodal densities of real datasets. However, very few studies have\nexplored the OMT problems in a reproducing kernel Hilbert space (RKHS), wherein\nthe kernel trick is utilized to avoid the need to explicitly map input data\ninto a high-dimensional feature space. In the current study, we propose a\nWasserstein-type metric to compute the distance between two Gaussian mixtures\nin a RKHS via the kernel trick, i.e., kernel Gaussian mixture models.",
            "author": [
                "Jung Hun Oh",
                "Rena Elkin",
                "Anish Kumar Simhal",
                "Jiening Zhu",
                "Joseph O Deasy",
                "Allen Tannenbaum"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18586v1",
                "http://arxiv.org/pdf/2310.18586v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18582v1",
            "title": "Data-driven learning of the generalized Langevin equation with\n  state-dependent memory",
            "updated": "2023-10-28T04:10:39Z",
            "published": "2023-10-28T04:10:39Z",
            "summary": "We present a data-driven method to learn stochastic reduced models of complex\nsystems that retain a state-dependent memory beyond the standard generalized\nLangevin equation (GLE) with a homogeneous kernel. The constructed model\nnaturally encodes the heterogeneous energy dissipation by jointly learning a\nset of state features and the non-Markovian coupling among the features.\nNumerical results demonstrate the limitation of the standard GLE and the\nessential role of the broadly overlooked state-dependency nature in predicting\nmolecule kinetics related to conformation relaxation and transition.",
            "author": [
                "Pei Ge",
                "Zhongqiang Zhang",
                "Huan Lei"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18582v1",
                "http://arxiv.org/pdf/2310.18582v1"
            ],
            "primary_category": "physics.comp-ph",
            "category": [
                "physics.comp-ph",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18581v2",
            "title": "Accelerating LLaMA Inference by Enabling Intermediate Layer Decoding via\n  Instruction Tuning with LITE",
            "updated": "2023-11-07T05:44:17Z",
            "published": "2023-10-28T04:07:58Z",
            "summary": "Large Language Models (LLMs) have achieved remarkable performance across a\nwide variety of natural language tasks; however, their large size makes their\ninference slow and computationally expensive. Focusing on this problem, we\npropose to instruction tune LLMs with additional explicit losses from the\nintermediate layers (LITE) and show that it enables these layers to acquire\n'good' generation ability without affecting the generation ability of the final\nlayer. We perform 'dynamic confidence-based early exiting' at token level from\nthe intermediate layers which improves the efficiency of text generation\nwithout compromising the quality of the generation. We conduct comprehensive\nexperiments by instruction tuning LLaMA-2 models on the Alpaca dataset and\nholistically evaluate on four different human-instruction test sets. We show\nthat dynamic early exiting achieves consistent and considerable inference\ncomputation cost improvements (37.86% for 7B and 46.35% for 13B model) while\nmaintaining the generation quality of the responses. We further conduct a\nthorough analysis of the results over several important aspects, such as\ncomparing the semantic similarity of the outputs and dissecting the efficiency\nimprovements by comparing the number of tokens generated in the output. In\nsummary, our work contributes to improving the efficiency of LLM inference\nwhile maintaining the generation quality, a crucial step en route to enabling\ntheir widespread adoption.",
            "author": [
                "Neeraj Varshney",
                "Agneet Chatterjee",
                "Mihir Parmar",
                "Chitta Baral"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18581v2",
                "http://arxiv.org/pdf/2310.18581v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18579v1",
            "title": "Functional Group Induced Transformations in Stacking and Electron\n  Structure in Mo2CTx/NiS Heterostructures",
            "updated": "2023-10-28T03:56:36Z",
            "published": "2023-10-28T03:56:36Z",
            "summary": "The two-dimensional transition metal carbide/nitride family (MXenes) has\ngarnered significant attention due to their highly customizable surface\nfunctional groups. Leveraging modern material science techniques, the\ncustomizability of MXenes can be enhanced further through the construction of\nassociated heterostructures. As indicated by recent research, the Mo2CTx/NiS\nheterostructure has emerged as a promising candidate exhibiting superior\nphysical and chemical application potential. The geometrical structure of\nMo2CTx/NiS heterostructure is modeled and 6 possible configurations are\nvalidated by Density Functional Theory simulations. The variation in functional\ngroups leads to structural changes in Mo2CTx/NiS interfaces, primarily\nattributed to the competition between van der Waals and covalent interactions.\nThe presence of different functional groups results in significant band\nfluctuations near the Fermi level for Ni and Mo atoms, influencing the role of\natoms and electron's ability to escape near the interface. This, in turn,\nmodulates the strength of covalent interactions at the MXenes/NiS interface and\nalters the ease of dissociation of the MXenes/NiS complex. Notably, the\nMo2CO2/NiS(P6_3/mmc) heterostructure exhibits polymorphism, signifying that two\natomic arrangements can stabilize the structure. The transition process between\nthese polymorphs is also simulated, further indicating the modulation of the\nelectronic level of properties by a sliding operation.",
            "author": [
                "Jiamin Liu",
                "Guo Li",
                "Xinxu Zhang",
                "Jiahao Wei",
                "Hui Jia",
                "Yulong Wu",
                "Changlong Liu",
                "Yonghui Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18579v1",
                "http://arxiv.org/pdf/2310.18579v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18578v1",
            "title": "Simulations of the Visible, Infrared and Terahertz Properties of Doped\n  Cyclo[18]carbon Molecules Using Tuned Exchange-Correlation Potential",
            "updated": "2023-10-28T03:51:08Z",
            "published": "2023-10-28T03:51:08Z",
            "summary": "Cyclocarbon molecules are critical in understanding the carbon structure\nformation and the nature of the interaction between carbon atoms. In\ncyclocarbons, light elements such as H, O and N may interplay with rings to\nform doped cyclocarbon molecules. Such molecules show unique optical properties\nthat have never been reported before. In this study, density functional theory\nwith a tuned PBE functional (39% HF exchange) is employed to study the ground\nand excited states of a cyclo[18] carbon molecule (C18) and its doped variants\nC18M (M = H, Be, B, N, and O). The doping is shown to either make the UV-Vis\nspectra of C18 blue- or red-shifted depending on the spin brought by the\ndopant. Furthermore, introducing extra-atoms is found to cause the emergence of\na new set of infrared modes in the structure under investigation. Finally,\napplying the molecular dynamic simulations enables one to observe the terahertz\ncharacteristics of C18M with frequencies up to 1.5 THz due to the propagation\nof a particular pattern along the carbon ring.",
            "author": [
                "Liujia Min",
                "Aigen Li",
                "Xianghong Chen",
                "Yonghui Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18578v1",
                "http://arxiv.org/pdf/2310.18578v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18571v1",
            "title": "The Chow Ring Classes of $\\mathrm{PGL}_3$ Orbit Closures in\n  $\\mathbb{G}(1, 5)$",
            "updated": "2023-10-28T02:40:05Z",
            "published": "2023-10-28T02:40:05Z",
            "summary": "The space of all pencils of conics in the plane $\\mathbb{P} V$ (where $\\dim V\n= 3$) is a projective Grassmannian $\\mathbb{G} (1, \\mathbb{P} \\mathrm{Sym}^2\nV^*)$ and admits a natural $\\mathrm{PGL}(V)$ action. It is a classical theorem\nthat this action has exactly eight orbits, and in fact that the orbit of a\npencil $\\ell \\subset \\mathbb{P} \\mathrm{Sym}^2 V^*$ is determined completely by\nits position with respect to the Veronese surface $X \\subset \\mathbb{P}\n\\mathrm{Sym}^2V^*$ of rank 1 conics and its secant variety $S(X) \\subset\n\\mathbb{P} \\mathrm{Sym}^2 V^*$, which is the cubic fourfold of rank 2 conics.\nIn this paper, we present some geometric descriptions of these orbits. Then,\nusing a mixture of direct enumerative techniques and some Chern class\ncomputations, we present a calculation of the classes of the orbit closures in\nthe Chow ring of this Grassmannian (and consequently also of their degrees\nunder the Pl\\\"ucker embedding $\\mathbb{G} (1, \\mathbb{P} \\mathrm{Sym}^2\nV^*)\\hookrightarrow \\mathbb{P} \\Lambda^2 \\mathrm{Sym}^2 V^*$).",
            "author": [
                "Gaurav",
                "Goel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18571v1",
                "http://arxiv.org/pdf/2310.18571v1"
            ],
            "primary_category": "math.AG",
            "category": [
                "math.AG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18569v1",
            "title": "Enhancing Grasping Performance of Novel Objects through an Improved\n  Fine-Tuning Process",
            "updated": "2023-10-28T02:34:35Z",
            "published": "2023-10-28T02:34:35Z",
            "summary": "Grasping algorithms have evolved from planar depth grasping to utilizing\npoint cloud information, allowing for application in a wider range of\nscenarios. However, data-driven grasps based on models trained on basic\nopen-source datasets may not perform well on novel objects, which are often\nrequired in different scenarios, necessitating fine-tuning using new objects.\nThe data driving these algorithms essentially corresponds to the closing region\nof the hand in 6D pose, and due to the uniqueness of 6D pose, synthetic\nannotation or real-machine annotation methods are typically employed. Acquiring\nlarge amounts of data with real-machine annotation is challenging, making\nsynthetic annotation a common practice. However, obtaining annotated 6D pose\ndata using conventional methods is extremely time-consuming. Therefore, we\npropose a method to quickly acquire data for novel objects, enabling more\nefficient fine-tuning. Our method primarily samples grasp orientations to\ngenerate and annotate grasps. Experimental results demonstrate that our\nfine-tuning process for a new object is 400 \\% faster than other methods.\nFurthermore, we propose an optimized grasp annotation framework that accounts\nfor the effects of the gripper closing, making the annotations more reasonable.\nUpon acceptance of this paper, we will release our algorithm as open-source.",
            "author": [
                "Xiao Hu",
                "Xiangsheng Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18569v1",
                "http://arxiv.org/pdf/2310.18569v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18567v1",
            "title": "Accelerated degradation modeling considering long-range dependence and\n  unit-to-unit variability",
            "updated": "2023-10-28T02:28:51Z",
            "published": "2023-10-28T02:28:51Z",
            "summary": "Accelerated degradation testing (ADT) is an effective way to evaluate the\nreliability and lifetime of highly reliable products. Existing studies have\nshown that the degradation processes of some products are non-Markovian with\nlong-range dependence due to the interaction with environments. Besides, the\ndegradation processes of products from the same population generally vary from\neach other due to various uncertainties. These two aspects bring great\ndifficulty for ADT modeling. In this paper, we propose an improved ADT model\nconsidering both long-range dependence and unit-to-unit variability. To be\nspecific, fractional Brownian motion (FBM) is utilized to capture the\nlong-range dependence in the degradation process. The unit-to-unit variability\namong multiple products is captured by a random variable in the degradation\nrate function. To ensure the accuracy of the parameter estimations, a novel\nstatistical inference method based on expectation maximization (EM) algorithm\nis proposed, in which the maximization of the overall likelihood function is\nachieved. The effectiveness of the proposed method is fully verified by a\nsimulation case and a microwave case. The results show that the proposed model\nis more suitable for ADT modeling and analysis than existing ADT models.",
            "author": [
                "Shi-Shun Chen",
                "Xiao-Yang Li",
                "Wenrui Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18567v1",
                "http://arxiv.org/pdf/2310.18567v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18564v1",
            "title": "A General Framework for Robust G-Invariance in G-Equivariant Networks",
            "updated": "2023-10-28T02:27:34Z",
            "published": "2023-10-28T02:27:34Z",
            "summary": "We introduce a general method for achieving robust group-invariance in\ngroup-equivariant convolutional neural networks ($G$-CNNs), which we call the\n$G$-triple-correlation ($G$-TC) layer. The approach leverages the theory of the\ntriple-correlation on groups, which is the unique, lowest-degree polynomial\ninvariant map that is also complete. Many commonly used invariant maps - such\nas the max - are incomplete: they remove both group and signal structure. A\ncomplete invariant, by contrast, removes only the variation due to the actions\nof the group, while preserving all information about the structure of the\nsignal. The completeness of the triple correlation endows the $G$-TC layer with\nstrong robustness, which can be observed in its resistance to invariance-based\nadversarial attacks. In addition, we observe that it yields measurable\nimprovements in classification accuracy over standard Max $G$-Pooling in\n$G$-CNN architectures. We provide a general and efficient implementation of the\nmethod for any discretized group, which requires only a table defining the\ngroup's product structure. We demonstrate the benefits of this method for\n$G$-CNNs defined on both commutative and non-commutative groups - $SO(2)$,\n$O(2)$, $SO(3)$, and $O(3)$ (discretized as the cyclic $C8$, dihedral $D16$,\nchiral octahedral $O$ and full octahedral $O_h$ groups) - acting on\n$\\mathbb{R}^2$ and $\\mathbb{R}^3$ on both $G$-MNIST and $G$-ModelNet10\ndatasets.",
            "author": [
                "Sophia Sanborn",
                "Nina Miolane"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18564v1",
                "http://arxiv.org/pdf/2310.18564v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18557v1",
            "title": "Time-Dependent AGN Disc Winds I -- X-ray Irradiation",
            "updated": "2023-10-28T01:49:56Z",
            "published": "2023-10-28T01:49:56Z",
            "summary": "We study AGN line driven disc winds using time-dependent radiation\nhydrodynamics. The key criterion for determining wind launching is the coupling\nstrength of the UV radiation field via the spectral lines of the gas. The\nstrength of these lines in turn relies crucially on the gas ionization state,\ndetermined by the local X-ray intensity. We consider a suite of models where\nthe central ionizing radiation is affected by scattering, absorption and\nre-emission by the intervening gas. In a pure attenuation model, the disc\nlaunches an episodic wind, as previous studies have shown. Including scattering\nor re-emission tends to weaken the wind, lowering the mass flux and outflow\nvelocity and if sufficiently dominant, suppressing the outflow entirely.\nHowever, the exponential nature of radiative attenuation means only a modest,\nfactor of a few, increase in the absorption cross section can overcome the wind\nsuppression due to scattering and re-emission. We find mass outflow rates of\n$\\sim 20\\%$ or more of the assumed inflow rate through the disk, indicating\nthat radiation driven winds may significantly alter the structure of the\naccretion flow. The winds also supply a large, time-varying column of material\nabove the nominal constant disk scale height, which will determine the geometry\nof reprocessed emission from the central source. Our results suggest the need\nfor accurate photoionization modeling, radiation transport as well as accretion\ndisc physics, to study their effects on the AGN disc winds",
            "author": [
                "Sergei Dyda",
                "Shane W. Davis",
                "Daniel Proga"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18557v1",
                "http://arxiv.org/pdf/2310.18557v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18555v1",
            "title": "Group Robust Classification Without Any Group Information",
            "updated": "2023-10-28T01:29:18Z",
            "published": "2023-10-28T01:29:18Z",
            "summary": "Empirical risk minimization (ERM) is sensitive to spurious correlations in\nthe training data, which poses a significant risk when deploying systems\ntrained under this paradigm in high-stake applications. While the existing\nliterature focuses on maximizing group-balanced or worst-group accuracy,\nestimating these accuracies is hindered by costly bias annotations. This study\ncontends that current bias-unsupervised approaches to group robustness continue\nto rely on group information to achieve optimal performance. Firstly, these\nmethods implicitly assume that all group combinations are represented during\ntraining. To illustrate this, we introduce a systematic generalization task on\nthe MPI3D dataset and discover that current algorithms fail to improve the ERM\nbaseline when combinations of observed attribute values are missing. Secondly,\nbias labels are still crucial for effective model selection, restricting the\npracticality of these methods in real-world scenarios. To address these\nlimitations, we propose a revised methodology for training and validating\ndebiased models in an entirely bias-unsupervised manner. We achieve this by\nemploying pretrained self-supervised models to reliably extract bias\ninformation, which enables the integration of a logit adjustment training loss\nwith our validation criterion. Our empirical analysis on synthetic and\nreal-world tasks provides evidence that our approach overcomes the identified\nchallenges and consistently enhances robust accuracy, attaining performance\nwhich is competitive with or outperforms that of state-of-the-art methods,\nwhich, conversely, rely on bias labels for validation.",
            "author": [
                "Christos Tsirigotis",
                "Joao Monteiro",
                "Pau Rodriguez",
                "David Vazquez",
                "Aaron Courville"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18555v1",
                "http://arxiv.org/pdf/2310.18555v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18553v1",
            "title": "Affective Polarization in Social Networks",
            "updated": "2023-10-28T01:05:34Z",
            "published": "2023-10-28T01:05:34Z",
            "summary": "Affective polarization has grown dramatically in recent years, with surveys\nshowing that liberals and conservatives not only disagree on policy issues but\nalso dislike and distrust each other. While studies have implicated social\nmedia in amplifying polarization, there is a lack of agreement on the\nmechanisms driving affective polarization and methods to measure it. Our paper\naddresses these gaps. First, we directly measure affective polarization on\nsocial media by quantifying the emotional tone of reply interactions between\nusers. As predicted by affective polarization, in-group interactions between\nsame-partisanship users tend to be positive, while out-group interactions\nbetween opposite-partisanship users are characterized by negativity and\ntoxicity. Second, we show that affective polarization generalizes beyond the\nin-group/out-group dichotomy and can be considered a structural property of\nsocial networks. Specifically, we show that emotions vary with network distance\nbetween users, with closer interactions eliciting positive emotions and more\ndistant interactions leading to anger, disgust, and toxicity. These findings\nare consistent across diverse datasets and languages, spanning discussions on\ntopics such as the Covid-19 pandemic, abortion, and the 2017 French Election.\nOur research provides new insights into the complex social dynamics of\naffective polarization in the digital age and its implications for political\ndiscourse.",
            "author": [
                "Dan Feldman",
                "Ashwin Rao",
                "Zihao He",
                "Kristina Lerman"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18553v1",
                "http://arxiv.org/pdf/2310.18553v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.12882v1",
            "title": "Overview of Current Applications of Large Language Models in Various\n  Medical Specialities",
            "updated": "2023-10-28T01:01:30Z",
            "published": "2023-10-28T01:01:30Z",
            "summary": "This paper gives an overview of the latest applications of Large Language\nModels (LLMs) in the healthcare sector, highlighting their transformative role\nin enhancing medical care quality. By processing vast amounts of data from\ndiverse medical domains, LLMs have become pivotal in assisting doctors,\nhealthcare providers, and patients. We explore their utilization in various\nmedical specialties, such as cancer diagnostics, dentistry, nephrology,\ndermatology, etc. The paper includes the LLM methodologies applied in various\nmedical specialties, different data types in the medical domains and the\nrelevant input formatting for LLMs, along with practical use-cases of LLMs in\nthe healthcare domain.",
            "author": [
                "Ummara Mumtaz",
                "Awais Ahmed",
                "Summaya Mumtaz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12882v1",
                "http://arxiv.org/pdf/2311.12882v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18551v1",
            "title": "Modeling Shortest Paths in Polymeric Networks using Spatial Branching\n  Processes",
            "updated": "2023-10-28T00:43:55Z",
            "published": "2023-10-28T00:43:55Z",
            "summary": "Recent studies have established a connection between the macroscopic\nmechanical response of polymeric materials and the statistics of the shortest\npath (SP) length between distant nodes in the polymer network. Since these\nstatistics can be costly to compute and difficult to study theoretically, we\nintroduce a branching random walk (BRW) model to describe the SP statistics\nfrom the coarse-grained molecular dynamics (CGMD) simulations of polymer\nnetworks. We postulate that the first passage time (FPT) of the BRW to a given\ntermination site can be used to approximate the statistics of the SP between\ndistant nodes in the polymer network. We develop a theoretical framework for\nstudying the FPT of spatial branching processes and obtain an analytical\nexpression for estimating the FPT distribution as a function of the cross-link\ndensity. We demonstrate by extensive numerical calculations that the\ndistribution of the FPT of the BRW model agrees well with the SP distribution\nfrom the CGMD simulations. The theoretical estimate and the corresponding\nnumerical implementations of BRW provide an efficient way of approximating the\nSP distribution in a polymer network. Our results have the physical meaning\nthat by accounting for the realistic topology of polymer networks, extensive\nbond-breaking is expected to occur at a much smaller stretch than that expected\nfrom idealized models assuming periodic network structures. Our work presents\nthe first analysis of polymer networks as a BRW and sets the framework for\ndeveloping a generalizable spatial branching model for studying the macroscopic\nevolution of polymeric systems.",
            "author": [
                "Zhenyuan Zhang",
                "Shaswat Mohanty",
                "Jose Blanchet",
                "Wei Cai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18551v1",
                "http://arxiv.org/pdf/2310.18551v1"
            ],
            "primary_category": "math-ph",
            "category": [
                "math-ph",
                "math.MP",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18545v1",
            "title": "Identifying Conspiracy Theories News based on Event Relation Graph",
            "updated": "2023-10-28T00:27:21Z",
            "published": "2023-10-28T00:27:21Z",
            "summary": "Conspiracy theories, as a type of misinformation, are narratives that\nexplains an event or situation in an irrational or malicious manner. While most\nprevious work examined conspiracy theory in social media short texts, limited\nattention was put on such misinformation in long news documents. In this paper,\nwe aim to identify whether a news article contains conspiracy theories. We\nobserve that a conspiracy story can be made up by mixing uncorrelated events\ntogether, or by presenting an unusual distribution of relations between events.\nAchieving a contextualized understanding of events in a story is essential for\ndetecting conspiracy theories. Thus, we propose to incorporate an event\nrelation graph for each article, in which events are nodes, and four common\ntypes of event relations, coreference, temporal, causal, and subevent\nrelations, are considered as edges. Then, we integrate the event relation graph\ninto conspiracy theory identification in two ways: an event-aware language\nmodel is developed to augment the basic language model with the knowledge of\nevents and event relations via soft labels; further, a heterogeneous graph\nattention network is designed to derive a graph embedding based on hard labels.\nExperiments on a large benchmark dataset show that our approach based on event\nrelation graph improves both precision and recall of conspiracy theory\nidentification, and generalizes well for new unseen media sources.",
            "author": [
                "Yuanyuan Lei",
                "Ruihong Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18545v1",
                "http://arxiv.org/pdf/2310.18545v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18544v1",
            "title": "Discourse Structures Guided Fine-grained Propaganda Identification",
            "updated": "2023-10-28T00:18:19Z",
            "published": "2023-10-28T00:18:19Z",
            "summary": "Propaganda is a form of deceptive narratives that instigate or mislead the\npublic, usually with a political purpose. In this paper, we aim to identify\npropaganda in political news at two fine-grained levels: sentence-level and\ntoken-level. We observe that propaganda content is more likely to be embedded\nin sentences that attribute causality or assert contrast to nearby sentences,\nas well as seen in opinionated evaluation, speculation and discussions of\nfuture expectation. Hence, we propose to incorporate both local and global\ndiscourse structures for propaganda discovery and construct two teacher models\nfor identifying PDTB-style discourse relations between nearby sentences and\ncommon discourse roles of sentences in a news article respectively. We further\ndevise two methods to incorporate the two types of discourse structures for\npropaganda identification by either using teacher predicted probabilities as\nadditional features or soliciting guidance in a knowledge distillation\nframework. Experiments on the benchmark dataset demonstrate that leveraging\nguidance from discourse structures can significantly improve both precision and\nrecall of propaganda content identification.",
            "author": [
                "Yuanyuan Lei",
                "Ruihong Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18544v1",
                "http://arxiv.org/pdf/2310.18544v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18541v1",
            "title": "ReConTab: Regularized Contrastive Representation Learning for Tabular\n  Data",
            "updated": "2023-10-28T00:05:28Z",
            "published": "2023-10-28T00:05:28Z",
            "summary": "Representation learning stands as one of the critical machine learning\ntechniques across various domains. Through the acquisition of high-quality\nfeatures, pre-trained embeddings significantly reduce input space redundancy,\nbenefiting downstream pattern recognition tasks such as classification,\nregression, or detection. Nonetheless, in the domain of tabular data, feature\nengineering and selection still heavily rely on manual intervention, leading to\ntime-consuming processes and necessitating domain expertise. In response to\nthis challenge, we introduce ReConTab, a deep automatic representation learning\nframework with regularized contrastive learning. Agnostic to any type of\nmodeling task, ReConTab constructs an asymmetric autoencoder based on the same\nraw features from model inputs, producing low-dimensional representative\nembeddings. Specifically, regularization techniques are applied for raw feature\nselection. Meanwhile, ReConTab leverages contrastive learning to distill the\nmost pertinent information for downstream tasks. Experiments conducted on\nextensive real-world datasets substantiate the framework's capacity to yield\nsubstantial and robust performance improvements. Furthermore, we empirically\ndemonstrate that pre-trained embeddings can seamlessly integrate as easily\nadaptable features, enhancing the performance of various traditional methods\nsuch as XGBoost and Random Forest.",
            "author": [
                "Suiyao Chen",
                "Jing Wu",
                "Naira Hovakimyan",
                "Handong Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18541v1",
                "http://arxiv.org/pdf/2310.18541v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18538v1",
            "title": "Evaluating Cross-Domain Text-to-SQL Models and Benchmarks",
            "updated": "2023-10-27T23:36:14Z",
            "published": "2023-10-27T23:36:14Z",
            "summary": "Text-to-SQL benchmarks play a crucial role in evaluating the progress made in\nthe field and the ranking of different models. However, accurately matching a\nmodel-generated SQL query to a reference SQL query in a benchmark fails for\nvarious reasons, such as underspecified natural language queries, inherent\nassumptions in both model-generated and reference queries, and the\nnon-deterministic nature of SQL output under certain conditions. In this paper,\nwe conduct an extensive study of several prominent cross-domain text-to-SQL\nbenchmarks and re-evaluate some of the top-performing models within these\nbenchmarks, by both manually evaluating the SQL queries and rewriting them in\nequivalent expressions. Our evaluation reveals that attaining a perfect\nperformance on these benchmarks is unfeasible due to the multiple\ninterpretations that can be derived from the provided samples. Furthermore, we\nfind that the true performance of the models is underestimated and their\nrelative performance changes after a re-evaluation. Most notably, our\nevaluation reveals a surprising discovery: a recent GPT4-based model surpasses\nthe gold standard reference queries in the Spider benchmark in our human\nevaluation. This finding highlights the importance of interpreting benchmark\nevaluations cautiously, while also acknowledging the critical role of\nadditional independent evaluations in driving advancements in the field.",
            "author": [
                "Mohammadreza Pourreza",
                "Davood Rafiei"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18538v1",
                "http://arxiv.org/pdf/2310.18538v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.DB",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18536v1",
            "title": "Efficient Fully Bayesian Approach to Brain Activity Mapping with\n  Complex-Valued fMRI Data",
            "updated": "2023-10-27T23:25:51Z",
            "published": "2023-10-27T23:25:51Z",
            "summary": "Functional magnetic resonance imaging (fMRI) enables indirect detection of\nbrain activity changes via the blood-oxygen-level-dependent (BOLD) signal.\nConventional analysis methods mainly rely on the real-valued magnitude of these\nsignals. In contrast, research suggests that analyzing both real and imaginary\ncomponents of the complex-valued fMRI (cv-fMRI) signal provides a more holistic\napproach that can increase power to detect neuronal activation. We propose a\nfully Bayesian model for brain activity mapping with cv-fMRI data. Our model\naccommodates temporal and spatial dynamics. Additionally, we propose a\ncomputationally efficient sampling algorithm, which enhances processing speed\nthrough image partitioning. Our approach is shown to be computationally\nefficient via image partitioning and parallel computation while being\ncompetitive with state-of-the-art methods. We support these claims with both\nsimulated numerical studies and an application to real cv-fMRI data obtained\nfrom a finger-tapping experiment.",
            "author": [
                "Zhengxin Wang",
                "Daniel B. Rowe",
                "Xinyi Li",
                "D. Andrew Brown"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18536v1",
                "http://arxiv.org/pdf/2310.18536v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18532v1",
            "title": "SkipAnalyzer: An Embodied Agent for Code Analysis with Large Language\n  Models",
            "updated": "2023-10-27T23:17:42Z",
            "published": "2023-10-27T23:17:42Z",
            "summary": "We introduce SkipAnalyzer, the first large language model (LLM)-powered\nembodied agent for static code analysis. It can detect bugs, filter false\npositive warnings, and patch the detected bugs without human intervention.\nSkipAnalyzer consists of three components, 1) an LLM-based static bug detector\nthat scans source code and reports specific types of bugs, 2) an LLM-based\nfalse-positive filter that can identify false-positive bugs in the results of\nstatic bug detectors to improve detection accuracy, and 3) an LLM-based patch\ngenerator that can generate patches for the detected bugs above. As a\nproof-of-concept, SkipAnalyzer is built on ChatGPT, which has exhibited\noutstanding performance in various software engineering tasks. To evaluate\nSkipAnalyzer, we focus on two types of typical and critical bugs that are\ntargeted by static bug detection, i.e., Null Dereference and Resource Leak as\nsubjects. We employ Infer to aid the gathering of these two bug types from 10\nopen-source projects. Consequently, our experiment dataset contains 222\ninstances of Null Dereference bugs and 46 instances of Resource Leak bugs. Our\nstudy demonstrates that SkipAnalyzer achieves remarkable performance in the\nmentioned static analysis tasks, including bug detection, false-positive\nwarning removal, and bug repair. In static bug detection, SkipAnalyzer achieves\naccuracy values of up to 68.37% for detecting Null Dereference bugs and 76.95%\nfor detecting Resource Leak bugs, outperforming the current leading bug\ndetector, Infer. For removing false-positive warnings, SkipAnalyzer can reach a\nprecision of up to 93.88% for Null Dereference bugs and 63.33% for Resource\nLeak bugs. Additionally, SkipAnalyzer surpasses state-of-the-art false-positive\nwarning removal tools. Furthermore, in bug repair, SkipAnalyzer can generate\nsyntactically correct patches to fix its detected bugs with a success rate of\nup to 97.30%.",
            "author": [
                "Mohammad Mahdi Mohajer",
                "Reem Aleithan",
                "Nima Shiri Harzevili",
                "Moshi Wei",
                "Alvine Boaye Belle",
                "Hung Viet Pham",
                "Song Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18532v1",
                "http://arxiv.org/pdf/2310.18532v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18529v2",
            "title": "FPM-INR: Fourier ptychographic microscopy image stack reconstruction\n  using implicit neural representations",
            "updated": "2023-10-31T20:22:28Z",
            "published": "2023-10-27T23:13:49Z",
            "summary": "Image stacks provide invaluable 3D information in various biological and\npathological imaging applications. Fourier ptychographic microscopy (FPM)\nenables reconstructing high-resolution, wide field-of-view image stacks without\nz-stack scanning, thus significantly accelerating image acquisition. However,\nexisting FPM methods take tens of minutes to reconstruct and gigabytes of\nmemory to store a high-resolution volumetric scene, impeding fast\ngigapixel-scale remote digital pathology. While deep learning approaches have\nbeen explored to address this challenge, existing methods poorly generalize to\nnovel datasets and can produce unreliable hallucinations. This work presents\nFPM-INR, a compact and efficient framework that integrates physics-based\noptical models with implicit neural representations (INR) to represent and\nreconstruct FPM image stacks. FPM-INR is agnostic to system design or sample\ntypes and does not require external training data. In our demonstrated\nexperiments, FPM-INR substantially outperforms traditional FPM algorithms with\nup to a 25-fold increase in speed and an 80-fold reduction in memory usage for\ncontinuous image stack representations.",
            "author": [
                "Haowen Zhou",
                "Brandon Y. Feng",
                "Haiyun Guo",
                "Siyu Lin",
                "Mingshu Liang",
                "Christopher A. Metzler",
                "Changhuei Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18529v2",
                "http://arxiv.org/pdf/2310.18529v2"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18526v1",
            "title": "Sample based Explanations via Generalized Representers",
            "updated": "2023-10-27T22:54:47Z",
            "published": "2023-10-27T22:54:47Z",
            "summary": "We propose a general class of sample based explanations of machine learning\nmodels, which we term generalized representers. To measure the effect of a\ntraining sample on a model's test prediction, generalized representers use two\ncomponents: a global sample importance that quantifies the importance of the\ntraining point to the model and is invariant to test samples, and a local\nsample importance that measures similarity between the training sample and the\ntest point with a kernel. A key contribution of the paper is to show that\ngeneralized representers are the only class of sample based explanations\nsatisfying a natural set of axiomatic properties. We discuss approaches to\nextract global importances given a kernel, and also natural choices of kernels\ngiven modern non-linear models. As we show, many popular existing sample based\nexplanations could be cast as generalized representers with particular choices\nof kernels and approaches to extract global importances. Additionally, we\nconduct empirical comparisons of different generalized representers on two\nimage and two text classification datasets.",
            "author": [
                "Che-Ping Tsai",
                "Chih-Kuan Yeh",
                "Pradeep Ravikumar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18526v1",
                "http://arxiv.org/pdf/2310.18526v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18523v1",
            "title": "Using convolutional neural networks for stereological characterization\n  of 3D hetero-aggregates based on synthetic STEM data",
            "updated": "2023-10-27T22:49:08Z",
            "published": "2023-10-27T22:49:08Z",
            "summary": "The structural characterization of hetero-aggregates in 3D is of great\ninterest, e.g., for deriving process-structure or structure-property\nrelationships. However, since 3D imaging techniques are often difficult to\nperform as well as time and cost intensive, a characterization of\nhetero-aggregates based on 2D image data is desirable, but often non-trivial.\nTo overcome the issues of characterizing 3D structures from 2D measurements, a\nmethod is presented that relies on machine learning combined with methods of\nspatial stochastic modeling, where the latter are utilized for the generation\nof synthetic training data. This kind of training data has the advantage that\ntime-consuming experiments for the synthesis of differently structured\nmaterials followed by their 3D imaging can be avoided. More precisely, a\nparametric stochastic 3D model is presented, from which a wide spectrum of\nvirtual hetero-aggregates can be generated. Additionally, the virtual\nstructures are passed to a physics-based simulation tool in order to generate\nvirtual scanning transmission electron microscopy (STEM) images. The preset\nparameters of the 3D model together with the simulated STEM images serve as a\ndatabase for the training of convolutional neural networks, which can be used\nto determine the parameters of the underlying 3D model and, consequently, to\npredict 3D structures of hetero-aggregates from 2D STEM images. Furthermore, an\nerror analysis is performed to evaluate the prediction power of the trained\nneural networks with respect to structural descriptors, e.g. the\nhetero-coordination number.",
            "author": [
                "Lukas Fuchs",
                "Tom Kirstein",
                "Christoph Mahr",
                "Orkun Furat",
                "Valentin Baric",
                "Andreas Rosenauer",
                "Lutz Maedler",
                "Volker Schmidt"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18523v1",
                "http://arxiv.org/pdf/2310.18523v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18521v1",
            "title": "Membranes, holography, and quantum information",
            "updated": "2023-10-27T22:39:32Z",
            "published": "2023-10-27T22:39:32Z",
            "summary": "In this thesis, I study Interface Conformal Field Theories (ICFT) and their\nholographic dual, which is composed of two asymptotically Anti-de-Sitter (AdS)\nspaces glued through a thin gravitating membrane. I restrict the study to\nsimple minimal models, which allow for analytic control while providing\nuniversally applicable results. The analysis is set in 2D ICFT/3D gravity, but\nI expect much of the results to be generalizable to higher dimensions. I first\nconsider this system at equilibrium and at finite temperature. By solving the\nequations of motion in the bulk, I find the allowable solution landscape.\nClassifying the rich set of solutions among 3 thermodynamical phases, I draw\nthe phase diagram outlining the nature of the various phase transitions. I then\nexamine a simple out-of-equilibrium situation arising from connecting at an\ninterface two spatially infinite CFTs at different temperatures. Then a\n\"Non-Equilibrium Steady State\" (NESS) describes the growing region where the\ninteraction has settled into a stationary phase. I determine the holographic\ndual of this region, composed of two spinning planar black holes glued at the\nmembrane. I find an expression for the deformed out-of-equilibrium event\nhorizon. This geometry suggests that the field theory interface acts as a\nperfect scrambler, a property that until now seemed unique to black hole\nhorizons. Finally, I study the entanglement structure of the aforementioned\ngeometries by means of the Ryu-Takayanagi prescription. After reviewing a\nconstruction in the vacuum state, I present partial results for more general\ngeometries at finite temperature. For this purpose, I need to introduce\nnumerical methods. I outline the main difficulties in their application and\nconclude by mentioning the Quantum Null Energy Condition (QNEC), an inequality\nlinking entanglement entropy and energy, that can be used to test the\nconsistency of the models.",
            "author": [
                "Vassilis Papadopoulos"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18521v1",
                "http://arxiv.org/pdf/2310.18521v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18519v1",
            "title": "Practical trainable temporal post-processor for multi-state quantum\n  measurement",
            "updated": "2023-10-27T22:36:59Z",
            "published": "2023-10-27T22:36:59Z",
            "summary": "We develop and demonstrate a trainable temporal post-processor (TPP),\nharnessing a simple but versatile machine learning algorithm to provide optimal\nprocessing of quantum measurement data subject to arbitrary noise processes,\nfor the readout of an arbitrary number of quantum states. We demonstrate the\nTPP on the essential task of qubit state readout, which has historically relied\non temporal processing via matched filters in spite of their applicability only\nfor specific noise conditions. Our results show that the TPP can reliably\noutperform standard filtering approaches under complex readout conditions, such\nas high power readout. Using simulations of quantum measurement noise sources,\nwe show that this advantage relies on the TPP's ability to learn optimal linear\nfilters that account for general quantum noise correlations in data, such as\nthose due to quantum jumps, or correlated noise added by a phase-preserving\nquantum amplifier. Furthermore, for signals subject to Gaussian white noise\nprocesses, the TPP provides a linearly-scaling semi-analytic generalization of\nmatched filtering to an arbitrary number of states. The TPP can be efficiently,\nautonomously, and reliably trained on measurement data, and requires only\nlinear operations, making it ideal for FPGA implementations in cQED for\nreal-time processing of measurement data from general quantum systems.",
            "author": [
                "Saeed A. Khan",
                "Ryan Kaufman",
                "Boris Mesits",
                "Michael Hatridge",
                "Hakan E. T\u00fcreci"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18519v1",
                "http://arxiv.org/pdf/2310.18519v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18516v1",
            "title": "Operator is the Model",
            "updated": "2023-10-27T22:29:23Z",
            "published": "2023-10-27T22:29:23Z",
            "summary": "Koopman operator based models emerged as the leading methodology for machine\nlearning of dynamical systems. But their scope is much larger. In fact they\npresent a new take on modeling of physical systems, and even language. In this\narticle I present some of the underlying mathematical structures, applications,\nconnections to other methodologies such as transformer architectures",
            "author": [
                "Igor Mezi\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18516v1",
                "http://arxiv.org/pdf/2310.18516v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18513v1",
            "title": "The zero forcing numbers and propagation times of gear graphs and helm\n  graphs",
            "updated": "2023-10-27T22:10:06Z",
            "published": "2023-10-27T22:10:06Z",
            "summary": "Zero forcing is a dynamic coloring process on graphs. Initially, each vertex\nof a graph is assigned a color of either blue or white, and then a process\nbegins by which blue vertices force white vertices to become blue. The zero\nforcing number is the cardinality of the smallest set of initially blue\nvertices which can force the entire graph to become blue, and the propagation\ntime is the minimum number of steps in such a zero forcing process. In this\npaper we will determine the zero forcing numbers and propagation times of two\ninfinite classes of graphs called gear graphs and helm graphs.",
            "author": [
                "Sara Anderton",
                "Rilee Burden",
                "McKenzie Fontenot",
                "Noah Fredrickson",
                "Alexandria Kwon",
                "Sydney Le",
                "Kanno Mizozoe",
                "Erin Raign",
                "August Sangalli",
                "Houston Schuerger",
                "Andrew Schwartz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18513v1",
                "http://arxiv.org/pdf/2310.18513v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18512v2",
            "title": "Preventing Language Models From Hiding Their Reasoning",
            "updated": "2023-10-31T19:13:43Z",
            "published": "2023-10-27T22:02:29Z",
            "summary": "Large language models (LLMs) often benefit from intermediate steps of\nreasoning to generate answers to complex problems. When these intermediate\nsteps of reasoning are used to monitor the activity of the model, it is\nessential that this explicit reasoning is faithful, i.e. that it reflects what\nthe model is actually reasoning about. In this work, we focus on one potential\nway intermediate steps of reasoning could be unfaithful: encoded reasoning,\nwhere an LLM could encode intermediate steps of reasoning in the generated text\nin a way that is not understandable to human readers. We show that language\nmodels can be trained to make use of encoded reasoning to get higher\nperformance without the user understanding the intermediate steps of reasoning.\nWe argue that, as language models get stronger, this behavior becomes more\nlikely to appear naturally. Finally, we describe a methodology that enables the\nevaluation of defenses against encoded reasoning, and show that, under the\nright conditions, paraphrasing successfully prevents even the best encoding\nschemes we built from encoding more than 3 bits of information per KB of text.",
            "author": [
                "Fabien Roger",
                "Ryan Greenblatt"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18512v2",
                "http://arxiv.org/pdf/2310.18512v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18508v1",
            "title": "Lowering the reactor breakeven requirements for proton-Boron 11 fusion",
            "updated": "2023-10-27T21:57:37Z",
            "published": "2023-10-27T21:57:37Z",
            "summary": "Recently, it has been shown that altering the natural collisional power flow\nof the proton-Boron 11 (pB11) fusion reaction can significantly reduce the\nLawson product of ion density and confinement time required to achieve\nignition. However, these products are still onerous - on the order of $7 \\times\n10^{15}$ cm$^{-3}$s under the most optimistic scenarios. Fortunately, a\nbreakeven fusion power plant does not require an igniting plasma, but rather a\nreactor that produces more electrical power than it consumes. Here, we extend\nthe existing 0D power balance analysis to check the conditions on power plant\nbreakeven. We find that even for the base thermonuclear reaction, modern\nhigh-efficiency thermal engines should reduce the Lawson product to $1.2 \\times\n10^{15}$ cm$^{-3}$s. We then explore the impact of several potential\nimprovements, including fast proton heating, alpha power capture, direct\nconversion, and efficient heating. We find that such improvements could reduce\nthe required Lawson product by a further order of magnitude, bringing\naneutronic fusion to within target ITER design parameters.",
            "author": [
                "Ian E. Ochs",
                "Nathaniel J. Fisch"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18508v1",
                "http://arxiv.org/pdf/2310.18508v1"
            ],
            "primary_category": "physics.plasm-ph",
            "category": [
                "physics.plasm-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18507v2",
            "title": "Differentiable Simulator For Dynamic & Stochastic Optimal Gas & Power\n  Flows",
            "updated": "2023-10-31T14:55:14Z",
            "published": "2023-10-27T21:53:04Z",
            "summary": "In many power systems, particularly those isolated from larger\nintercontinental grids, operational dependence on natural gas becomes pivotal,\nespecially during fluctuations or unavailability of renewables coupled with\nuncertain consumption patterns. Efficient orchestration and inventive\nstrategies are imperative for the smooth functioning of these standalone\ngas-grid systems. This paper delves into the challenge of synchronized dynamic\nand stochastic optimization for independent transmission-level gas-grid\nsystems. Our approach's novelty lies in amalgamating the staggered-grid method\nfor the direct assimilation of gas-flow PDEs with an automated sensitivity\nanalysis facilitated by SciML/Julia, further enhanced by an intuitive linkage\nbetween gas and power grids via nodal flows. We initiate with a single pipe to\nestablish a versatile and expandable methodology, later showcasing its\neffectiveness with increasingly intricate examples.",
            "author": [
                "Criston Hyett",
                "Laurent Pagnier",
                "Jean Alisse",
                "Igal Goldshtein",
                "Lilah Saban",
                "Robert Ferrando",
                "Michael Chertkov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18507v2",
                "http://arxiv.org/pdf/2310.18507v2"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18505v1",
            "title": "Multi-fidelity Design of Porous Microstructures for Thermofluidic\n  Applications",
            "updated": "2023-10-27T21:51:11Z",
            "published": "2023-10-27T21:51:11Z",
            "summary": "As modern electronic devices are increasingly miniaturized and integrated,\ntheir performance relies more heavily on effective thermal management.\nTwo-phase cooling methods enhanced by porous surfaces, which capitalize on\nthin-film evaporation atop structured porous surfaces, are emerging as\npotential solutions. In such porous structures, the optimum heat dissipation\ncapacity relies on two competing objectives that depend on mass and heat\ntransfer. The computational costs of evaluating these objectives, the high\ndimensionality of the design space which a voxelated microstructure\nrepresentation, and the manufacturability constraints hinder the optimization\nprocess for thermal management. We address these challenges by developing a\ndata-driven framework for designing optimal porous microstructures for cooling\napplications. In our framework we leverage spectral density functions (SDFs) to\nencode the design space via a handful of interpretable variables and, in turn,\nefficiently search it. We develop physics-based formulas to quantify the\nthermofluidic properties and feasibility of candidate designs via offline\nsimulations. To decrease the reliance on expensive simulations, we generate\nmulti-fidelity data and build emulators to find Pareto-optimal designs. We\napply our approach to a canonical problem on evaporator wick design and obtain\nfin-like topologies in the optimal microstructures which are also\ncharacteristics often observed in industrial applications.",
            "author": [
                "Jonathan Tammer Eweis-LaBolle",
                "Chuanning Zhao",
                "Yoonjin Won",
                "Ramin Bostanabad"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18505v1",
                "http://arxiv.org/pdf/2310.18505v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18502v1",
            "title": "On the Automatic Generation and Simplification of Children's Stories",
            "updated": "2023-10-27T21:31:34Z",
            "published": "2023-10-27T21:31:34Z",
            "summary": "With recent advances in large language models (LLMs), the concept of\nautomatically generating children's educational materials has become\nincreasingly realistic. Working toward the goal of age-appropriate simplicity\nin generated educational texts, we first examine the ability of several popular\nLLMs to generate stories with properly adjusted lexical and readability levels.\nWe find that, in spite of the growing capabilities of LLMs, they do not yet\npossess the ability to limit their vocabulary to levels appropriate for younger\nage groups. As a second experiment, we explore the ability of state-of-the-art\nlexical simplification models to generalize to the domain of children's stories\nand, thus, create an efficient pipeline for their automatic generation. In\norder to test these models, we develop a dataset of child-directed lexical\nsimplification instances, with examples taken from the LLM-generated stories in\nour first experiment. We find that, while the strongest-performing current\nlexical simplification models do not perform as well on material designed for\nchildren due to their reliance on large language models behind the scenes, some\nmodels that still achieve fairly strong results on general data can mimic or\neven improve their performance on children-directed data with proper\nfine-tuning, which we conduct using our newly created child-directed\nsimplification dataset.",
            "author": [
                "Maria Valentini",
                "Jennifer Weber",
                "Jesus Salcido",
                "T\u00e9a Wright",
                "Eliana Colunga",
                "Katharina Kann"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18502v1",
                "http://arxiv.org/pdf/2310.18502v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18500v1",
            "title": "Designing Randomized Experiments to Predict Unit-Specific Treatment\n  Effects",
            "updated": "2023-10-27T21:29:48Z",
            "published": "2023-10-27T21:29:48Z",
            "summary": "Typically, a randomized experiment is designed to test a hypothesis about the\naverage treatment effect and sometimes hypotheses about treatment effect\nvariation. The results of such a study may then be used to inform policy and\npractice for units not in the study. In this paper, we argue that given this\nuse, randomized experiments should instead be designed to predict unit-specific\ntreatment effects in a well-defined population. We then consider how different\nsampling processes and models affect the bias, variance, and mean squared\nprediction error of these predictions. The results indicate, for example, that\nproblems of generalizability (differences between samples and populations) can\ngreatly affect bias both in predictive models and in measures of error in these\nmodels. We also examine when the average treatment effect estimate outperforms\nunit-specific treatment effect predictive models and implications of this for\nplanning studies.",
            "author": [
                "Elizabeth Tipton",
                "Michalis Mamakos"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18500v1",
                "http://arxiv.org/pdf/2310.18500v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18498v1",
            "title": "GPT-4 Vision on Medical Image Classification -- A Case Study on COVID-19\n  Dataset",
            "updated": "2023-10-27T21:28:36Z",
            "published": "2023-10-27T21:28:36Z",
            "summary": "This technical report delves into the application of GPT-4 Vision (GPT-4V) in\nthe nuanced realm of COVID-19 image classification, leveraging the\ntransformative potential of in-context learning to enhance diagnostic\nprocesses.",
            "author": [
                "Ruibo Chen",
                "Tianyi Xiong",
                "Yihan Wu",
                "Guodong Liu",
                "Zhengmian Hu",
                "Lichang Chen",
                "Yanshuo Chen",
                "Chenxi Liu",
                "Heng Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18498v1",
                "http://arxiv.org/pdf/2310.18498v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18496v1",
            "title": "How Well Do Feature-Additive Explainers Explain Feature-Additive\n  Predictors?",
            "updated": "2023-10-27T21:16:28Z",
            "published": "2023-10-27T21:16:28Z",
            "summary": "Surging interest in deep learning from high-stakes domains has precipitated\nconcern over the inscrutable nature of black box neural networks. Explainable\nAI (XAI) research has led to an abundance of explanation algorithms for these\nblack boxes. Such post hoc explainers produce human-comprehensible\nexplanations, however, their fidelity with respect to the model is not well\nunderstood - explanation evaluation remains one of the most challenging issues\nin XAI. In this paper, we ask a targeted but important question: can popular\nfeature-additive explainers (e.g., LIME, SHAP, SHAPR, MAPLE, and PDP) explain\nfeature-additive predictors? Herein, we evaluate such explainers on ground\ntruth that is analytically derived from the additive structure of a model. We\ndemonstrate the efficacy of our approach in understanding these explainers\napplied to symbolic expressions, neural networks, and generalized additive\nmodels on thousands of synthetic and several real-world tasks. Our results\nsuggest that all explainers eventually fail to correctly attribute the\nimportance of features, especially when a decision-making process involves\nfeature interactions.",
            "author": [
                "Zachariah Carmichael",
                "Walter J. Scheirer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18496v1",
                "http://arxiv.org/pdf/2310.18496v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18494v1",
            "title": "Knowledge-based in silico models and dataset for the comparative\n  evaluation of mammography AI for a range of breast characteristics, lesion\n  conspicuities and doses",
            "updated": "2023-10-27T21:14:30Z",
            "published": "2023-10-27T21:14:30Z",
            "summary": "To generate evidence regarding the safety and efficacy of artificial\nintelligence (AI) enabled medical devices, AI models need to be evaluated on a\ndiverse population of patient cases, some of which may not be readily\navailable. We propose an evaluation approach for testing medical imaging AI\nmodels that relies on in silico imaging pipelines in which stochastic digital\nmodels of human anatomy (in object space) with and without pathology are imaged\nusing a digital replica imaging acquisition system to generate realistic\nsynthetic image datasets. Here, we release M-SYNTH, a dataset of cohorts with\nfour breast fibroglandular density distributions imaged at different exposure\nlevels using Monte Carlo x-ray simulations with the publicly available Virtual\nImaging Clinical Trial for Regulatory Evaluation (VICTRE) toolkit. We utilize\nthe synthetic dataset to analyze AI model performance and find that model\nperformance decreases with increasing breast density and increases with higher\nmass density, as expected. As exposure levels decrease, AI model performance\ndrops with the highest performance achieved at exposure levels lower than the\nnominal recommended dose for the breast type.",
            "author": [
                "Elena Sizikova",
                "Niloufar Saharkhiz",
                "Diksha Sharma",
                "Miguel Lago",
                "Berkman Sahiner",
                "Jana G. Delfino",
                "Aldo Badano"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18494v1",
                "http://arxiv.org/pdf/2310.18494v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18492v1",
            "title": "Methodological challenges of scenario generation validation: a rear-end\n  crash-causation model for virtual safety assessment",
            "updated": "2023-10-27T21:12:53Z",
            "published": "2023-10-27T21:12:53Z",
            "summary": "Vehicle conflict and crash avoidance systems are becoming increasingly common\non our roads, aiming to save lives and mitigate injuries. System developers and\nother stakeholders need methods to assess the safety benefit of these systems\nbefore they are introduced. Using computer simulations for this is increasingly\ncommon, and a core component is the generation of crashes virtually. Crashes\ncan be generated in several ways; one is to use models of crash causation,\napplied to pre-crash kinematics from in-depth crash databases. However, these\nmodels are seldomly thoroughly validated. To address this research gap this\nstudy aims to validate a rear-end crash-causation-theory model based on four\ncrash-causation mechanisms: a) off-road glances, b) driving with too short\nheadway, c) drivers not braking with the maximum deceleration possible, and d)\nsleepy drivers (no reaction before the crash). Pre-crash kinematics from the\nGerman GIDAS in-depth crash database were used as a basis for validation. For\neach original crash, the evasive maneuver of the following vehicle was replaced\nwith the crash-causation model. Comparisons were then made between the\nestimated delta-v distribution of the original GIDAS data and the delta-v\ndistribution of the generated crashes. During the work, challenges with the\nvalidation process were identified and addressed; most notably a process for\ntransforming the generated crashes (which intrinsically include a large\nproportion of property-damage-only crashes) was developed, to mimic the sample\nselection process of GIDAS, addressing the GIDAS selection bias related to\ncensoring of low-severity crashes. Our results indicate that a sample selection\nbias transform is likely to substantially improve the comparability (and thus\nvalidation accuracy) of scenarios generated with a crash-causation model and\nGIDAS data.",
            "author": [
                "Jonas B\u00e4rgman",
                "Malin Sv\u00e4rd",
                "Simon Lundell",
                "Erik Hartelius"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18492v1",
                "http://arxiv.org/pdf/2310.18492v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18491v1",
            "title": "Publicly Detectable Watermarking for Language Models",
            "updated": "2023-10-27T21:08:51Z",
            "published": "2023-10-27T21:08:51Z",
            "summary": "We construct the first provable watermarking scheme for language models with\npublic detectability or verifiability: we use a private key for watermarking\nand a public key for watermark detection. Our protocol is the first\nwatermarking scheme that does not embed a statistical signal in generated text.\nRather, we directly embed a publicly-verifiable cryptographic signature using a\nform of rejection sampling. We show that our construction meets strong formal\nsecurity guarantees and preserves many desirable properties found in schemes in\nthe private-key watermarking setting. In particular, our watermarking scheme\nretains distortion-freeness and model agnosticity. We implement our scheme and\nmake empirical measurements over open models in the 7B parameter range. Our\nexperiments suggest that our watermarking scheme meets our formal claims while\npreserving text quality.",
            "author": [
                "Jaiden Fairoze",
                "Sanjam Garg",
                "Somesh Jha",
                "Saeed Mahloujifar",
                "Mohammad Mahmoody",
                "Mingyuan Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18491v1",
                "http://arxiv.org/pdf/2310.18491v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18487v1",
            "title": "Threshold Switching in CdTe Photovoltaics",
            "updated": "2023-10-27T21:01:07Z",
            "published": "2023-10-27T21:01:07Z",
            "summary": "With the ubiquitous acceptance of PVs, the number of devices manufactured\nannually is following an exponential trend. Yet, the manufacturing process of\nimportant brands of thin film solar cells involves a tedious and expensive step\nof laser scribing. The time-consuming and technologically involved laser\nscribing method remains widely used to contact the device electrodes. This work\nexamines an alternative method (threshold switching phenomenon) to create an\nenduring conductive path in cadmium telluride (CdTe) PV, which eliminates the\npitfalls in conventional scribing technology. The samples undergoing threshold\nswitching show a promising sign of the conductive path compared to the control\nsamples. This method could potentially lead to the manufacturing technology\nsaving time, money, and raw materials along with added reliability and\nefficiency.",
            "author": [
                "Suman Devkota",
                "Kwame A Nyako",
                "Brendan Kuzior",
                "Victor G. Karpov",
                "Daniel G. Georgiev",
                "Frank X. Li",
                "Pedro Cortes",
                "Vamsi Borra"
            ],
            "link": [
                "http://dx.doi.org/10.1149/10901.0003ecst",
                "http://arxiv.org/abs/2310.18487v1",
                "http://arxiv.org/pdf/2310.18487v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cond-mat.mes-hall",
                "cond-mat.other"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18484v1",
            "title": "Kinematic signatures of planet-disk interactions in VSI-turbulent\n  protoplanetary disks",
            "updated": "2023-10-27T20:55:51Z",
            "published": "2023-10-27T20:55:51Z",
            "summary": "Context. Planets are thought to form inside weakly ionized regions of\nprotoplanetary disks, where turbulence creates ideal conditions for solid\ngrowth. However, the nature of this turbulence is still uncertain. In this\nzone, vertical shear instability (VSI) can operate, inducing a low level of gas\nturbulence and large-scale motions. Resolving kinematic signatures of VSI may\nreveal the origin of turbulence in planet-forming disks. However, an\nexploration of kinematic signatures of the interplay between VSI and forming\nplanets is needed for a correct interpretation of radio interferometric\nobservations. Robust detection of VSI would lead to a deeper understanding of\nthe impact of gas turbulence on planet formation. Aims. The goal of this study\nis to explore the effect of VSI on the disk substructures triggered by an\nembedded massive planet. We focus on the impact of this interplay on CO\nkinematic observations with ALMA. Methods. We conduct global 3D hydrodynamical\nsimulations of VSI-unstable disks with and without embedded massive planets,\nexploring Saturn- and Jupiter-mass cases. We study the effect of planets on the\nVSI gas dynamics, comparing with viscous disks. Post-processing the simulations\nwith a radiative transfer code, we examine the kinematic signatures expected in\nCO molecular line emission, varying disk inclination. Further, we simulate ALMA\nhigh-resolution observations to test the observability of VSI and planetary\nsignatures. Results. The embedded planet dampens the VSI along a radial region,\nmost effective at the disk midplane. For the Saturn case, the VSI modes are\ndistorted by the planet's spirals producing mixed kinematic signatures. For the\nJupiter case, the planet's influence dominates the disk gas kinematics.\nConclusions. The presence of massive embedded planets can weaken the VSI\nlarge-scale gas flows, limiting its observability in CO kinematic observations.",
            "author": [
                "Marcelo Barraza-Alfaro",
                "Mario Flock",
                "Thomas Henning"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18484v1",
                "http://arxiv.org/pdf/2310.18484v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18479v1",
            "title": "Weighted Sampled Split Learning (WSSL): Balancing Privacy, Robustness,\n  and Fairness in Distributed Learning Environments",
            "updated": "2023-10-27T20:50:21Z",
            "published": "2023-10-27T20:50:21Z",
            "summary": "This study presents Weighted Sampled Split Learning (WSSL), an innovative\nframework tailored to bolster privacy, robustness, and fairness in distributed\nmachine learning systems. Unlike traditional approaches, WSSL disperses the\nlearning process among multiple clients, thereby safeguarding data\nconfidentiality. Central to WSSL's efficacy is its utilization of weighted\nsampling. This approach ensures equitable learning by tactically selecting\ninfluential clients based on their contributions. Our evaluation of WSSL\nspanned various client configurations and employed two distinct datasets: Human\nGait Sensor and CIFAR-10. We observed three primary benefits: heightened model\naccuracy, enhanced robustness, and maintained fairness across diverse client\ncompositions. Notably, our distributed frameworks consistently surpassed\ncentralized counterparts, registering accuracy peaks of 82.63% and 75.51% for\nthe Human Gait Sensor and CIFAR-10 datasets, respectively. These figures\ncontrast with the top accuracies of 81.12% and 58.60% achieved by centralized\nsystems. Collectively, our findings champion WSSL as a potent and scalable\nsuccessor to conventional centralized learning, marking it as a pivotal stride\nforward in privacy-focused, resilient, and impartial distributed machine\nlearning.",
            "author": [
                "Manish Osti",
                "Aashray Thakuri",
                "Basheer Qolomany",
                "Aos Mulahuwaish"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18479v1",
                "http://arxiv.org/pdf/2310.18479v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18475v1",
            "title": "Radiative decay branching ratio of the Hoyle state",
            "updated": "2023-10-27T20:42:25Z",
            "published": "2023-10-27T20:42:25Z",
            "summary": "The triple-alpha process is a vital reaction in nuclear astrophysics,\ncharacterized by two consecutive reactions\n($2\\alpha\\leftrightarrows{^{8}\\rm{Be}}(\\alpha,\\gamma){^{12}\\rm{C}}$) that drive\ncarbon formation. The second reaction occurs through the Hoyle state, a 7.65\nMeV excited state in ${^{12}\\rm{C}}$ with $J^{\\pi}=0^{+}$.The rate of the\nprocess depends on the radiative width, which can be determined by measuring\nthe branching ratio for electromagnetic decay. Recent measurements by Kib\\'edi,\n\\textit{et al.} conflicted with the adopted value and resulted in a significant\nincrease of nearly 50\\% in this branching ratio, directly affecting the\ntriple-alpha reaction. This work aims to utilize charged-particle spectroscopy\nwith magnetic selection as a means to accurately measure the total radiative\nbranching ratio ($\\Gamma_{\\rm{rad}}/\\Gamma$) of the Hoyle state in $^{12}{\\rm\nC}$. The Hoyle state in $^{12}{\\rm C}$ was populated via $^{12}\\rm{C}(\\alpha,\n\\alpha')^{12}\\rm{C^{*}}$ inelastic scattering. The scattered $\\alpha$-particles\nwere detected using a $\\Delta$E-E telescope, while the recoiled $^{12}{\\rm C}$\nions were identified in a magnetic spectrometer. A radiative branching ratio\nvalue of $\\Gamma_{\\rm{rad}}/\\Gamma\\times10^{4}=4.0\\pm0.3({\\rm\nstat.})\\pm0.16({\\rm syst.})$ was obtained. The radiative branching ratio for\nthe Hoyle state obtained in this work is in agreement with the original adopted\nvalue. Our result suggests that the proton-$\\gamma$-$\\gamma$ spectroscopy\nresult reported by Kib\\'edi \\textit{et al.} may be excluded.",
            "author": [
                "Zifeng Luo",
                "M. Barbui",
                "J. Bishop",
                "G. Chubarian",
                "V. Z. Goldberg",
                "E. Harris",
                "E. Koshchiy",
                "C. E. Parker",
                "M. Roosa",
                "A. Saastamoinen",
                "D. P. Scriven",
                "G. V. Rogachev"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18475v1",
                "http://arxiv.org/pdf/2310.18475v1"
            ],
            "primary_category": "nucl-ex",
            "category": [
                "nucl-ex",
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18472v1",
            "title": "Parameter-Efficient Methods for Metastases Detection from Clinical Notes",
            "updated": "2023-10-27T20:30:59Z",
            "published": "2023-10-27T20:30:59Z",
            "summary": "Understanding the progression of cancer is crucial for defining treatments\nfor patients. The objective of this study is to automate the detection of\nmetastatic liver disease from free-style computed tomography (CT) radiology\nreports. Our research demonstrates that transferring knowledge using three\napproaches can improve model performance. First, we utilize generic language\nmodels (LMs), pretrained in a self-supervised manner. Second, we use a\nsemi-supervised approach to train our model by automatically annotating a large\nunlabeled dataset; this approach substantially enhances the model's\nperformance. Finally, we transfer knowledge from related tasks by designing a\nmulti-task transfer learning methodology. We leverage the recent advancement of\nparameter-efficient LM adaptation strategies to improve performance and\ntraining efficiency. Our dataset consists of CT reports collected at Memorial\nSloan Kettering Cancer Center (MSKCC) over the course of 12 years. 2,641\nreports were manually annotated by domain experts; among them, 841 reports have\nbeen annotated for the presence of liver metastases. Our best model achieved an\nF1-score of 73.8%, a precision of 84%, and a recall of 65.8%.",
            "author": [
                "Maede Ashofteh Barabadi",
                "Xiaodan Zhu",
                "Wai Yip Chan",
                "Amber L. Simpson",
                "Richard K. G. Do"
            ],
            "link": [
                "http://dx.doi.org/10.21428/594757db.8bee12fd",
                "http://arxiv.org/abs/2310.18472v1",
                "http://arxiv.org/pdf/2310.18472v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18466v1",
            "title": "Integer Sequences: Irregular Arrays and Intra-Block Permutations",
            "updated": "2023-10-27T20:21:45Z",
            "published": "2023-10-27T20:21:45Z",
            "summary": "This article investigates integer sequences that partition the sequence into\nblocks of various lengths - irregular arrays. The main result of the article is\nexplicit formulas for numbering of irregular arrays. A generalization of Cantor\ndiagonal method is proposed. We also define and describe intra-block\npermutations of natural numbers. Generalizations of reluctant sequences are\nintroduced, namely generalized reluctant sequences and generalized reverse\nreluctant sequences. Explicit formulas are presented for these sequences. The\narticle provides numerous examples to illustrate all statements.",
            "author": [
                "Boris Putievskiy"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18466v1",
                "http://arxiv.org/pdf/2310.18466v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18463v1",
            "title": "PeTailor: Improving Large Language Model by Tailored Chunk Scorer in\n  Biomedical Triple Extraction",
            "updated": "2023-10-27T20:15:23Z",
            "published": "2023-10-27T20:15:23Z",
            "summary": "The automatic extraction of biomedical entities and their interaction from\nunstructured data remains a challenging task due to the limited availability of\nexpert-labeled standard datasets. In this paper, we introduce PETAI-LOR, a\nretrieval-based language framework that is augmented by tailored chunk scorer.\nUnlike previous retrieval-augmented language models (LM) that retrieve relevant\ndocuments by calculating the similarity between the input sentence and the\ncandidate document set, PETAILOR segments the sentence into chunks and\nretrieves the relevant chunk from our pre-computed chunk-based relational\nkey-value memory. Moreover, in order to comprehend the specific requirements of\nthe LM, PETAI-LOR adapt the tailored chunk scorer to the LM. We also introduce\nGM-CIHT, an expert annotated biomedical triple extraction dataset with more\nrelation types. This dataset is centered on the non-drug treatment and general\nbiomedical domain. Additionally, we investigate the efficacy of triple\nextraction models trained on general domains when applied to the biomedical\ndomain. Our experiments reveal that PETAI-LOR achieves state-of-the-art\nperformance on GM-CIHT",
            "author": [
                "Mingchen Li",
                "M. Chen",
                "Huixue Zhou",
                "Rui Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18463v1",
                "http://arxiv.org/pdf/2310.18463v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18461v1",
            "title": "Improved Lossless Coding for Storage and Transmission of Multichannel\n  Immersive Audio",
            "updated": "2023-10-27T20:14:00Z",
            "published": "2023-10-27T20:14:00Z",
            "summary": "In this paper, techniques for improving multichannel lossless coding are\nexamined. A method is proposed for the simultaneous coding of two or more\ndifferent renderings (mixes) of the same content. The signal model uses both\npast samples of the upmix, and the current time samples of downmix samples to\npredict the upmix. Model parameters are optimized via a general linear solver,\nand the prediction residual is Rice coded. Additionally, the use of an SVD\nprojection prior to residual coding is proposed. A comparison is made against\nvarious baselines, including FLAC. The proposed methods show improved\ncompression ratios for the storage and transmission of immersive audio.",
            "author": [
                "Toni Hirvonen",
                "Mahmoud Namazi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18461v1",
                "http://arxiv.org/pdf/2310.18461v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18460v1",
            "title": "Generalized Firefly Algorithm for Optimal Transmit Beamforming",
            "updated": "2023-10-27T20:13:26Z",
            "published": "2023-10-27T20:13:26Z",
            "summary": "This paper proposes a generalized Firefly Algorithm (FA) to solve an\noptimization framework having objective function and constraints as\nmultivariate functions of independent optimization variables. Four\nrepresentative examples of how the proposed generalized FA can be adopted to\nsolve downlink beamforming problems are shown for a classic transmit\nbeamforming, cognitive beamforming, reconfigurable-intelligent-surfaces-aided\n(RIS-aided) transmit beamforming, and RIS-aided wireless power transfer (WPT).\nComplexity analyzes indicate that in large-antenna regimes the proposed FA\napproaches require less computational complexity than their corresponding\ninterior point methods (IPMs) do, yet demand a higher complexity than the\niterative and the successive convex approximation (SCA) approaches do.\nSimulation results reveal that the proposed FA attains the same global optimal\nsolution as that of the IPM for an optimization problem in cognitive\nbeamforming. On the other hand, the proposed FA approaches outperform the\niterative, IPM and SCA in terms of obtaining better solution for optimization\nproblems, respectively, for a classic transmit beamforming, RIS-aided transmit\nbeamforming and RIS-aided WPT.",
            "author": [
                "Tuan Anh Le",
                "Xin-She Yang"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TWC.2023.3328713",
                "http://arxiv.org/abs/2310.18460v1",
                "http://arxiv.org/pdf/2310.18460v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18458v2",
            "title": "Do Not Harm Protected Groups in Debiasing Language Representation Models",
            "updated": "2023-11-11T22:29:58Z",
            "published": "2023-10-27T20:11:38Z",
            "summary": "Language Representation Models (LRMs) trained with real-world data may\ncapture and exacerbate undesired bias and cause unfair treatment of people in\nvarious demographic groups. Several techniques have been investigated for\napplying interventions to LRMs to remove bias in benchmark evaluations on, for\nexample, word embeddings. However, the negative side effects of debiasing\ninterventions are usually not revealed in the downstream tasks. We propose\nxGAP-DEBIAS, a set of evaluations on assessing the fairness of debiasing. In\nthis work, We examine four debiasing techniques on a real-world text\nclassification task and show that reducing biasing is at the cost of degrading\nperformance for all demographic groups, including those the debiasing\ntechniques aim to protect. We advocate that a debiasing technique should have\ngood downstream performance with the constraint of ensuring no harm to the\nprotected group.",
            "author": [
                "Chloe Qinyu Zhu",
                "Rickard Stureborg",
                "Brandon Fain"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18458v2",
                "http://arxiv.org/pdf/2310.18458v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18457v1",
            "title": "LLMSTEP: LLM proofstep suggestions in Lean",
            "updated": "2023-10-27T20:10:56Z",
            "published": "2023-10-27T20:10:56Z",
            "summary": "We present LLMSTEP, a tool for integrating a language model into the Lean\nproof assistant. LLMSTEP is a Lean 4 tactic that sends a user's proof state to\na server hosting a language model. The language model generates suggestions,\nwhich are checked in Lean and displayed to a user in their development\nenvironment. We provide a baseline language model, along with code for\nfine-tuning and evaluation to support further development. We provide server\nimplementations that run on CPU, a CUDA GPU, or a Google Colab notebook, as a\nstep towards fast, effective language model suggestions for any user.",
            "author": [
                "Sean Welleck",
                "Rahul Saha"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18457v1",
                "http://arxiv.org/pdf/2310.18457v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG",
                "I.2.2; I.2.5; I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18455v1",
            "title": "Approximate Heavy Tails in Offline (Multi-Pass) Stochastic Gradient\n  Descent",
            "updated": "2023-10-27T20:06:03Z",
            "published": "2023-10-27T20:06:03Z",
            "summary": "A recent line of empirical studies has demonstrated that SGD might exhibit a\nheavy-tailed behavior in practical settings, and the heaviness of the tails\nmight correlate with the overall performance. In this paper, we investigate the\nemergence of such heavy tails. Previous works on this problem only considered,\nup to our knowledge, online (also called single-pass) SGD, in which the\nemergence of heavy tails in theoretical findings is contingent upon access to\nan infinite amount of data. Hence, the underlying mechanism generating the\nreported heavy-tailed behavior in practical settings, where the amount of\ntraining data is finite, is still not well-understood. Our contribution aims to\nfill this gap. In particular, we show that the stationary distribution of\noffline (also called multi-pass) SGD exhibits 'approximate' power-law tails and\nthe approximation error is controlled by how fast the empirical distribution of\nthe training data converges to the true underlying data distribution in the\nWasserstein metric. Our main takeaway is that, as the number of data points\nincreases, offline SGD will behave increasingly 'power-law-like'. To achieve\nthis result, we first prove nonasymptotic Wasserstein convergence bounds for\noffline SGD to online SGD as the number of data points increases, which can be\ninteresting on their own. Finally, we illustrate our theory on various\nexperiments conducted on synthetic data and neural networks.",
            "author": [
                "Krunoslav Lehman Pavasovic",
                "Alain Durmus",
                "Umut Simsekli"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18455v1",
                "http://arxiv.org/pdf/2310.18455v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18454v1",
            "title": "T5 meets Tybalt: Author Attribution in Early Modern English Drama Using\n  Large Language Models",
            "updated": "2023-10-27T20:04:57Z",
            "published": "2023-10-27T20:04:57Z",
            "summary": "Large language models have shown breakthrough potential in many NLP domains.\nHere we consider their use for stylometry, specifically authorship\nidentification in Early Modern English drama. We find both promising and\nconcerning results; LLMs are able to accurately predict the author of\nsurprisingly short passages but are also prone to confidently misattribute\ntexts to specific authors. A fine-tuned t5-large model outperforms all tested\nbaselines, including logistic regression, SVM with a linear kernel, and cosine\ndelta, at attributing small passages. However, we see indications that the\npresence of certain authors in the model's pre-training data affects predictive\nresults in ways that are difficult to assess.",
            "author": [
                "Rebecca M. M. Hicke",
                "David Mimno"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18454v1",
                "http://arxiv.org/pdf/2310.18454v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18450v1",
            "title": "MixRep: Hidden Representation Mixup for Low-Resource Speech Recognition",
            "updated": "2023-10-27T19:48:00Z",
            "published": "2023-10-27T19:48:00Z",
            "summary": "In this paper, we present MixRep, a simple and effective data augmentation\nstrategy based on mixup for low-resource ASR. MixRep interpolates the feature\ndimensions of hidden representations in the neural network that can be applied\nto both the acoustic feature input and the output of each layer, which\ngeneralizes the previous MixSpeech method. Further, we propose to combine the\nmixup with a regularization along the time axis of the input, which is shown as\ncomplementary. We apply MixRep to a Conformer encoder of an E2E LAS\narchitecture trained with a joint CTC loss. We experiment on the WSJ dataset\nand subsets of the SWB dataset, covering reading and telephony conversational\nspeech. Experimental results show that MixRep consistently outperforms other\nregularization methods for low-resource ASR. Compared to a strong SpecAugment\nbaseline, MixRep achieves a +6.5\\% and a +6.7\\% relative WER reduction on the\neval92 set and the Callhome part of the eval'2000 set.",
            "author": [
                "Jiamin Xie",
                "John H. L. Hansen"
            ],
            "link": [
                "http://dx.doi.org/10.21437/Interspeech.2023-1216",
                "http://arxiv.org/abs/2310.18450v1",
                "http://arxiv.org/pdf/2310.18450v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18446v2",
            "title": "A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem",
            "updated": "2023-11-25T19:05:40Z",
            "published": "2023-10-27T19:42:23Z",
            "summary": "Optimal transportation is a fundamental topic that has attracted a great\namount of attention from machine learning community in the past decades. In\nthis paper, we consider an interesting discrete dynamic optimal transport\nproblem: can we efficiently update the optimal transport plan when the weights\nor the locations of the data points change? This problem is naturally motivated\nby several applications in machine learning. For example, we often need to\ncompute the optimal transportation cost between two different data sets; if\nsome change happens to a few data points, should we re-compute the high\ncomplexity cost function or update the cost by some efficient dynamic data\nstructure? We are aware that several dynamic maximum flow algorithms have been\nproposed before, however, the research on dynamic minimum cost flow problem is\nstill quite limited, to the best of our knowledge. We propose a novel 2D Skip\nOrthogonal List together with some dynamic tree techniques. Although our\nalgorithm is based on the conventional simplex method, it can efficiently\ncomplete each pivoting operation within $O(|V|)$ time with high probability\nwhere $V$ is the set of all supply and demand nodes. Since dynamic\nmodifications typically do not introduce significant changes, our algorithm\nrequires only a few simplex iterations in practice. So our algorithm is more\nefficient than re-computing the optimal transportation cost that needs at least\none traversal over all the $O(|E|) = O(|V|^2)$ variables in general cases. Our\nexperiments demonstrate that our algorithm significantly outperforms existing\nalgorithms in the dynamic scenarios.",
            "author": [
                "Xiaoyang Xu",
                "Hu Ding"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18446v2",
                "http://arxiv.org/pdf/2310.18446v2"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.AI",
                "cs.CG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18445v1",
            "title": "Signs of Similar Stellar Obliquity Distributions for Hot and Warm\n  Jupiters Orbiting Cool Stars",
            "updated": "2023-10-27T19:40:55Z",
            "published": "2023-10-27T19:40:55Z",
            "summary": "Transiting giant planets provide a natural opportunity to examine stellar\nobliquities, which offer clues about the origin and dynamical histories of\nclose-in planets. Hot Jupiters orbiting Sun-like stars show a tendency for\nobliquity alignment, which suggests that obliquities are rarely excited or that\ntidal realignment is common. However, the stellar obliquity distribution is\nless clear for giant planets at wider separations where realignment mechanisms\nare not expected to operate. In this work, we uniformly derive line-of-sight\ninclinations for 47 cool stars ($T_\\mathrm{eff}$ $<$ 6200 K) harboring\ntransiting hot and warm giant planets by combining rotation periods, stellar\nradii, and $v \\sin i$ measurements. Among the systems that show signs of\nspin-orbit misalignment in our sample, three are identified as being misaligned\nhere for the first time. Of particular interest are Kepler-1654, one of the\nlongest-period (1047 d; 2.0 AU) giant planets in a misaligned system, and\nKepler-30, a multi-planet misaligned system. By comparing the reconstructed\nunderlying inclination distributions, we find that the inferred minimum\nmisalignment distributions of hot Jupiters spanning $a/R_{*}$ = 3-20 ($\\approx$\n0.01-0.1 AU) and warm Jupiters spanning $a/R_{*}$ = 20-400 ($\\approx$ 0.1-1.9\nAU) are in good agreement. With 90$\\%$ confidence, at least 24$^{+9}_{-7}\\%$ of\nwarm Jupiters and 14$^{+7}_{-5}\\%$ of hot Jupiters around cool stars are\nmisaligned by at least 10$^\\circ$. Most stars harboring warm Jupiters are\ntherefore consistent with spin-orbit alignment. The similarity of hot and warm\nJupiter misalignment rates suggests that either the occasional misalignments\nare primordial and originate in misaligned disks, or the same underlying\nprocesses that create misaligned hot Jupiters also lead to misaligned warm\nJupiters.",
            "author": [
                "Marvin Morgan",
                "Brendan P. Bowler",
                "Quang H. Tran",
                "Erik Petigura",
                "Vighnesh Nagpal",
                "Sarah Blunt"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18445v1",
                "http://arxiv.org/pdf/2310.18445v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18440v1",
            "title": "Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement",
            "updated": "2023-10-27T19:27:59Z",
            "published": "2023-10-27T19:27:59Z",
            "summary": "Generative language models (LMs) are increasingly used for document\nclass-prediction tasks and promise enormous improvements in cost and\nefficiency. Existing research often examines simple classification tasks, but\nthe capability of LMs to classify on complex or specialized tasks is less well\nunderstood. We consider a highly complex task that is challenging even for\nhumans: the classification of legal reasoning according to jurisprudential\nphilosophy. Using a novel dataset of historical United States Supreme Court\nopinions annotated by a team of domain experts, we systematically test the\nperformance of a variety of LMs. We find that generative models perform poorly\nwhen given instructions (i.e. prompts) equal to the instructions presented to\nhuman annotators through our codebook. Our strongest results derive from\nfine-tuning models on the annotated dataset; the best performing model is an\nin-domain model, LEGAL-BERT. We apply predictions from this fine-tuned model to\nstudy historical trends in jurisprudence, an exercise that both aligns with\nprominent qualitative historical accounts and points to areas of possible\nrefinement in those accounts. Our findings generally sound a note of caution in\nthe use of generative LMs on complex tasks without fine-tuning and point to the\ncontinued relevance of human annotation-intensive classification methods.",
            "author": [
                "Rosamond Thalken",
                "Edward H. Stiglitz",
                "David Mimno",
                "Matthew Wilkens"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18440v1",
                "http://arxiv.org/pdf/2310.18440v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18438v1",
            "title": "Exploring Shape Embedding for Cloth-Changing Person Re-Identification\n  via 2D-3D Correspondences",
            "updated": "2023-10-27T19:26:30Z",
            "published": "2023-10-27T19:26:30Z",
            "summary": "Cloth-Changing Person Re-Identification (CC-ReID) is a common and realistic\nproblem since fashion constantly changes over time and people's aesthetic\npreferences are not set in stone. While most existing cloth-changing ReID\nmethods focus on learning cloth-agnostic identity representations from coarse\nsemantic cues (e.g. silhouettes and part segmentation maps), they neglect the\ncontinuous shape distributions at the pixel level. In this paper, we propose\nContinuous Surface Correspondence Learning (CSCL), a new shape embedding\nparadigm for cloth-changing ReID. CSCL establishes continuous correspondences\nbetween a 2D image plane and a canonical 3D body surface via pixel-to-vertex\nclassification, which naturally aligns a person image to the surface of a 3D\nhuman model and simultaneously obtains pixel-wise surface embeddings. We\nfurther extract fine-grained shape features from the learned surface embeddings\nand then integrate them with global RGB features via a carefully designed\ncross-modality fusion module. The shape embedding paradigm based on 2D-3D\ncorrespondences remarkably enhances the model's global understanding of human\nbody shape. To promote the study of ReID under clothing change, we construct 3D\nDense Persons (DP3D), which is the first large-scale cloth-changing ReID\ndataset that provides densely annotated 2D-3D correspondences and a precise 3D\nmesh for each person image, while containing diverse cloth-changing cases over\nall four seasons. Experiments on both cloth-changing and cloth-consistent ReID\nbenchmarks validate the effectiveness of our method.",
            "author": [
                "Yubin Wang",
                "Huimin Yu",
                "Yuming Yan",
                "Shuyi Song",
                "Biyang Liu",
                "Yichong Lu"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3581783.3611715",
                "http://arxiv.org/abs/2310.18438v1",
                "http://arxiv.org/pdf/2310.18438v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18435v1",
            "title": "Expanding the Set of Pragmatic Considerations in Conversational AI",
            "updated": "2023-10-27T19:21:50Z",
            "published": "2023-10-27T19:21:50Z",
            "summary": "Despite considerable performance improvements, current conversational AI\nsystems often fail to meet user expectations. We discuss several pragmatic\nlimitations of current conversational AI systems. We illustrate pragmatic\nlimitations with examples that are syntactically appropriate, but have clear\npragmatic deficiencies. We label our complaints as \"Turing Test Triggers\"\n(TTTs) as they indicate where current conversational AI systems fall short\ncompared to human behavior. We develop a taxonomy of pragmatic considerations\nintended to identify what pragmatic competencies a conversational AI system\nrequires and discuss implications for the design and evaluation of\nconversational AI systems.",
            "author": [
                "S. M. Seals",
                "Valerie L. Shalin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18435v1",
                "http://arxiv.org/pdf/2310.18435v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18432v1",
            "title": "Fully Relativistic Entanglement Harvesting",
            "updated": "2023-10-27T19:13:23Z",
            "published": "2023-10-27T19:13:23Z",
            "summary": "We study the protocol of entanglement harvesting when the particle detectors\nthat harvest entanglement from the field are replaced by fully relativistic\nquantum field theories. We show that two localized modes of the quantum field\ntheories are able to harvest the same amount of leading order entanglement as\ntwo non-relativistic particle detectors, thus implying that QFT probes can\ngenerally harvest more entanglement than particle detectors. These results\nlegitimize the use of particle detectors to study entanglement harvesting\nregardless of their internally non-relativistic nature.",
            "author": [
                "T. Rick Perche",
                "Jos\u00e9 Polo-G\u00f3mez",
                "Bruno de S. L. Torres",
                "Eduardo Mart\u00edn-Mart\u00ednez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18432v1",
                "http://arxiv.org/pdf/2310.18432v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "gr-qc",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18431v1",
            "title": "SDOH-NLI: a Dataset for Inferring Social Determinants of Health from\n  Clinical Notes",
            "updated": "2023-10-27T19:09:30Z",
            "published": "2023-10-27T19:09:30Z",
            "summary": "Social and behavioral determinants of health (SDOH) play a significant role\nin shaping health outcomes, and extracting these determinants from clinical\nnotes is a first step to help healthcare providers systematically identify\nopportunities to provide appropriate care and address disparities. Progress on\nusing NLP methods for this task has been hindered by the lack of high-quality\npublicly available labeled data, largely due to the privacy and regulatory\nconstraints on the use of real patients' information. This paper introduces a\nnew dataset, SDOH-NLI, that is based on publicly available notes and which we\nrelease publicly. We formulate SDOH extraction as a natural language inference\n(NLI) task, and provide binary textual entailment labels obtained from human\nraters for a cross product of a set of social history snippets as premises and\nSDOH factors as hypotheses. Our dataset differs from standard NLI benchmarks in\nthat our premises and hypotheses are obtained independently. We evaluate both\n\"off-the-shelf\" entailment models as well as models fine-tuned on our data, and\nhighlight the ways in which our dataset appears more challenging than commonly\nused NLI datasets.",
            "author": [
                "Adam D. Lelkes",
                "Eric Loreaux",
                "Tal Schuster",
                "Ming-Jun Chen",
                "Alvin Rajkomar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18431v1",
                "http://arxiv.org/pdf/2310.18431v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.02093v1",
            "title": "An Exploration on Integrated Sensing and Communication for the Future\n  Smart Internet of Things",
            "updated": "2023-10-27T19:03:10Z",
            "published": "2023-10-27T19:03:10Z",
            "summary": "Internet of Things (IoT) technologies are the foundation of a fully connected\nworld. Currently, IoT devices (or nodes) primarily use dedicated sensors to\nsense and collect data at large scales, and then transmit the data to target\nnodes or gateways through wireless communication for further processing and\nanalytics. In recent years, research efforts have been made to explore the\nfeasibility of using wireless communication for sensing (while assiduously\nimproving the transmission performance of wireless signals), in an attempt to\nachieve integrated sensing and communication (ISAC) for smart IoT of the\nfuture. In this paper, we leverage the capabilities of LoRa, a long-range IoT\ncommunication technology, to explore the possibility of using LoRa signals for\nboth sensing and communication. Based on LoRa, we propose ISAC designs in two\ntypical scenarios of smart IoT, and verify the feasibility and effectiveness of\nour designs in soil moisture monitoring and human presence detection.",
            "author": [
                "Zhaoxin Chang",
                "Fusang Zhang",
                "Daqing Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02093v1",
                "http://arxiv.org/pdf/2311.02093v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18430v1",
            "title": "MCRAGE: Synthetic Healthcare Data for Fairness",
            "updated": "2023-10-27T19:02:22Z",
            "published": "2023-10-27T19:02:22Z",
            "summary": "In the field of healthcare, electronic health records (EHR) serve as crucial\ntraining data for developing machine learning models for diagnosis, treatment,\nand the management of healthcare resources. However, medical datasets are often\nimbalanced in terms of sensitive attributes such as race/ethnicity, gender, and\nage. Machine learning models trained on class-imbalanced EHR datasets perform\nsignificantly worse in deployment for individuals of the minority classes\ncompared to samples from majority classes, which may lead to inequitable\nhealthcare outcomes for minority groups. To address this challenge, we propose\nMinority Class Rebalancing through Augmentation by Generative modeling\n(MCRAGE), a novel approach to augment imbalanced datasets using samples\ngenerated by a deep generative model. The MCRAGE process involves training a\nConditional Denoising Diffusion Probabilistic Model (CDDPM) capable of\ngenerating high-quality synthetic EHR samples from underrepresented classes. We\nuse this synthetic data to augment the existing imbalanced dataset, thereby\nachieving a more balanced distribution across all classes, which can be used to\ntrain an unbiased machine learning model. We measure the performance of MCRAGE\nversus alternative approaches using Accuracy, F1 score and AUROC. We provide\ntheoretical justification for our method in terms of recent convergence results\nfor DDPMs with minimal assumptions.",
            "author": [
                "Keira Behal",
                "Jiayi Chen",
                "Caleb Fikes",
                "Sophia Xiao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18430v1",
                "http://arxiv.org/pdf/2310.18430v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18427v1",
            "title": "Maximizing Equitable Reach and Accessibility of ETDs",
            "updated": "2023-10-27T18:57:27Z",
            "published": "2023-10-27T18:57:27Z",
            "summary": "This poster addresses accessibility issues of electronic theses and\ndissertations (ETDs) in digital libraries (DLs). ETDs are available primarily\nas PDF files, which present barriers to equitable access, especially for users\nwith visual impairments, cognitive or learning disabilities, or for anyone\nneeding more efficient and effective ways of finding relevant information\nwithin these long documents. We propose using AI techniques, including natural\nlanguage processing (NLP), computer vision, and text analysis, to convert PDFs\ninto machine-readable HTML documents with semantic tags and structure,\nextracting figures and tables, and generating summaries and keywords. Our goal\nis to increase the accessibility of ETDs and to make this important scholarship\navailable to a wider audience.",
            "author": [
                "William A. Ingram",
                "Jian Wu",
                "Edward A. Fox"
            ],
            "link": [
                "http://dx.doi.org/10.1109/JCDL57899.2023.00049",
                "http://arxiv.org/abs/2310.18427v1",
                "http://arxiv.org/pdf/2310.18427v1"
            ],
            "primary_category": "cs.DL",
            "category": [
                "cs.DL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18424v2",
            "title": "Fast Machine Learning Method with Vector Embedding on Orthonormal Basis\n  and Spectral Transform",
            "updated": "2023-11-13T16:48:01Z",
            "published": "2023-10-27T18:48:54Z",
            "summary": "This paper presents a novel fast machine learning method that leverages two\ntechniques: Vector Embedding on Orthonormal Basis (VEOB) and Spectral Transform\n(ST). The VEOB converts the original data encoding into a vector embedding with\ncoordinates projected onto orthonormal bases. The Singular Value Decomposition\n(SVD) technique is used to calculate the vector basis and projection\ncoordinates, leading to an enhanced distance measurement in the embedding space\nand facilitating data compression by preserving the projection vectors\nassociated with the largest singular values. On the other hand, ST transforms\nsequence of vector data into spectral space. By applying the Discrete Cosine\nTransform (DCT) and selecting the most significant components, it streamlines\nthe handling of lengthy vector sequences. The paper provides examples of word\nembedding, text chunk embedding, and image embedding, implemented in Julia\nlanguage with a vector database. It also investigates unsupervised learning and\nsupervised learning using this method, along with strategies for handling large\ndata volumes.",
            "author": [
                "Louis Yu Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18424v2",
                "http://arxiv.org/pdf/2310.18424v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NA",
                "math.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18417v1",
            "title": "Teacher Perception of Automatically Extracted Grammar Concepts for L2\n  Language Learning",
            "updated": "2023-10-27T18:17:29Z",
            "published": "2023-10-27T18:17:29Z",
            "summary": "One of the challenges in language teaching is how best to organize rules\nregarding syntax, semantics, or phonology in a meaningful manner. This not only\nrequires content creators to have pedagogical skills, but also have that\nlanguage's deep understanding. While comprehensive materials to develop such\ncurricula are available in English and some broadly spoken languages, for many\nother languages, teachers need to manually create them in response to their\nstudents' needs. This is challenging because i) it requires that such experts\nbe accessible and have the necessary resources, and ii) describing all the\nintricacies of a language is time-consuming and prone to omission. In this\nwork, we aim to facilitate this process by automatically discovering and\nvisualizing grammar descriptions. We extract descriptions from a natural text\ncorpus that answer questions about morphosyntax (learning of word order,\nagreement, case marking, or word formation) and semantics (learning of\nvocabulary). We apply this method for teaching two Indian languages, Kannada\nand Marathi, which, unlike English, do not have well-developed resources for\nsecond language learning. To assess the perceived utility of the extracted\nmaterial, we enlist the help of language educators from schools in North\nAmerica to perform a manual evaluation, who find the materials have potential\nto be used for their lesson preparation and learner evaluation.",
            "author": [
                "Aditi Chaudhary",
                "Arun Sampath",
                "Ashwin Sheshadri",
                "Antonios Anastasopoulos",
                "Graham Neubig"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18417v1",
                "http://arxiv.org/pdf/2310.18417v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18416v2",
            "title": "PolyMerge: A Novel Technique aimed at Dynamic HD Map Updates Leveraging\n  Polylines",
            "updated": "2023-10-31T13:55:02Z",
            "published": "2023-10-27T18:16:52Z",
            "summary": "Currently, High-Definition (HD) maps are a prerequisite for the stable\noperation of autonomous vehicles. Such maps contain information about all\nstatic road objects for the vehicle to consider during navigation, such as road\nedges, road lanes, crosswalks, and etc. To generate such an HD map, current\napproaches need to process pre-recorded environment data obtained from onboard\nsensors. However, recording such a dataset often requires a lot of time and\neffort. In addition, every time actual road environments are changed, a new\ndataset should be recorded to generate a relevant HD map.\n  This paper addresses a novel approach that allows to continuously generate or\nupdate the HD map using onboard sensor data. When there is no need to\npre-record the dataset, updating the HD map can be run in parallel with the\nmain autonomous vehicle navigation pipeline.\n  The proposed approach utilizes the VectorMapNet framework to generate vector\nroad object instances from a sensor data scan. The PolyMerge technique is aimed\nto merge new instances into previous ones, mitigating detection errors and,\ntherefore, generating or updating the HD map.\n  The performance of the algorithm was confirmed by comparison with ground\ntruth on the NuScenes dataset. Experimental results showed that the mean error\nfor different levels of environment complexity was comparable to the\nVectorMapNet single instance error.",
            "author": [
                "Mohamed Sayed",
                "Stepan Perminov",
                "Dzmitry Tsetserukou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18416v2",
                "http://arxiv.org/pdf/2310.18416v2"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18415v1",
            "title": "Overture POI data for the United Kingdom: a comprehensive, queryable\n  open data product",
            "updated": "2023-10-27T18:15:48Z",
            "published": "2023-10-27T18:15:48Z",
            "summary": "Point of Interest data that is comprehensive, globally-available and\nopen-access, is sparse, despite being important inputs for research in a number\nof application areas. New data from the Overture Maps Foundation offers\nsignificant potential in this arena, but accessing the data relies on\ncomputational resources beyond the skillset and capacity of the average\nresearcher. In this article, we provide a processed version of the Overture\nplaces (POI) dataset for the UK, in a fully-queryable format, and provide\naccompanying code through which to explore the data, and generate other\nnational subsets. In the article, we describe the construction and\ncharacteristics of the dataset, before considering how reliable it is\n(locational accuracy, attribute comprehensiveness), through direct comparison\nwith Geolytix supermarket data. This dataset can support new and important\nresearch projects in a variety of different thematic areas, and foster a\nnetwork of researchers to further evaluate its advantages and limitations.",
            "author": [
                "Patrick Ballantyne",
                "Cillian Berragan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18415v1",
                "http://arxiv.org/pdf/2310.18415v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18411v1",
            "title": "A general learning scheme for classical and quantum Ising machines",
            "updated": "2023-10-27T18:07:02Z",
            "published": "2023-10-27T18:07:02Z",
            "summary": "An Ising machine is any hardware specifically designed for finding the ground\nstate of the Ising model. Relevant examples are coherent Ising machines and\nquantum annealers. In this paper, we propose a new machine learning model that\nis based on the Ising structure and can be efficiently trained using gradient\ndescent. We provide a mathematical characterization of the training process,\nwhich is based upon optimizing a loss function whose partial derivatives are\nnot explicitly calculated but estimated by the Ising machine itself. Moreover,\nwe present some experimental results on the training and execution of the\nproposed learning model. These results point out new possibilities offered by\nIsing machines for different learning tasks. In particular, in the quantum\nrealm, the quantum resources are used for both the execution and the training\nof the model, providing a promising perspective in quantum machine learning.",
            "author": [
                "Ludwig Schmid",
                "Enrico Zardini",
                "Davide Pastorello"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18411v1",
                "http://arxiv.org/pdf/2310.18411v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18409v1",
            "title": "State-Action Similarity-Based Representations for Off-Policy Evaluation",
            "updated": "2023-10-27T18:00:57Z",
            "published": "2023-10-27T18:00:57Z",
            "summary": "In reinforcement learning, off-policy evaluation (OPE) is the problem of\nestimating the expected return of an evaluation policy given a fixed dataset\nthat was collected by running one or more different policies. One of the more\nempirically successful algorithms for OPE has been the fitted q-evaluation\n(FQE) algorithm that uses temporal difference updates to learn an action-value\nfunction, which is then used to estimate the expected return of the evaluation\npolicy. Typically, the original fixed dataset is fed directly into FQE to learn\nthe action-value function of the evaluation policy. Instead, in this paper, we\nseek to enhance the data-efficiency of FQE by first transforming the fixed\ndataset using a learned encoder, and then feeding the transformed dataset into\nFQE. To learn such an encoder, we introduce an OPE-tailored state-action\nbehavioral similarity metric, and use this metric and the fixed dataset to\nlearn an encoder that models this metric. Theoretically, we show that this\nmetric allows us to bound the error in the resulting OPE estimate. Empirically,\nwe show that other state-action similarity metrics lead to representations that\ncannot represent the action-value function of the evaluation policy, and that\nour state-action representation method boosts the data-efficiency of FQE and\nlowers OPE error relative to other OPE-based representation learning methods on\nchallenging OPE tasks. We also empirically show that the learned\nrepresentations significantly mitigate divergence of FQE under varying\ndistribution shifts. Our code is available here:\nhttps://github.com/Badger-RL/ROPE.",
            "author": [
                "Brahma S. Pavse",
                "Josiah P. Hanna"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18409v1",
                "http://arxiv.org/pdf/2310.18409v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18399v1",
            "title": "The spooky ghost of vectorization",
            "updated": "2023-10-27T18:00:04Z",
            "published": "2023-10-27T18:00:04Z",
            "summary": "An interesting mechanism for the formation of hairy black holes occurs when a\nvector field, non-minimally coupled to a source term, grows from a perturbation\nof the vacuum black hole, \\textit{aka} vectorization. Its study has, however,\nbeen lacking, in part due to the constant threat of ghost instabilities that\nhave plagued vector fields. In this work, we show evidence that, in a generic\nfamily of extended-vector-tensor theories where the vector field is\nnon-minimally coupled to the model's invariant (source term), a spherically\nsymmetric, vectorized black hole always suffers from ghost instabilities. These\nultimately turn the process of vectorization astrophysically unviable.",
            "author": [
                "Lorenzo Pizzuti",
                "Alexandre M. Pombo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18399v1",
                "http://arxiv.org/pdf/2310.18399v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc",
                "hep-th",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18400v1",
            "title": "Isotropic 3D topological phases with broken time reversal symmetry",
            "updated": "2023-10-27T18:00:04Z",
            "published": "2023-10-27T18:00:04Z",
            "summary": "Axial vectors, such as current or magnetization, are commonly used order\nparameters in time-reversal symmetry breaking systems. These vectors also break\nisotropy in three dimensional systems, lowering the spatial symmetry. We\ndemonstrate that it is possible to construct a fully isotropic and\ninversion-symmetric three-dimensional medium where time-reversal symmetry is\nsystematically broken. We devise a cubic crystal with scalar time-reversal\nsymmetry breaking, implemented by hopping through chiral magnetic clusters\nalong the crystal bonds. The presence of only the spatial symmetries of the\ncrystal -- finite rotation and inversion symmetry -- is sufficient to protect a\ntopological phase. The realization of this phase in amorphous systems with\naverage continuous rotation symmetry yields a statistical topological insulator\nphase. We demonstrate the topological nature of our model by constructing a\nbulk integer topological invariant, which guarantees gapless surface spectrum\non any surface with several overlapping Dirac nodes, analogous to crystalline\nmirror Chern insulators. We also show the expected transport properties of a\nthree-dimensional statistical topological insulator, which remains critical on\nthe surface for odd values of the invariant.",
            "author": [
                "Helene Spring",
                "Anton R. Akhmerov",
                "Daniel Varjas"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18400v1",
                "http://arxiv.org/pdf/2310.18400v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "cond-mat.dis-nn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18396v1",
            "title": "Circumnuclear Multi-phase Gas in the Circinus Galaxy. V. The Origin of\n  the X-Ray Polarization in the Circinus Galaxy",
            "updated": "2023-10-27T18:00:01Z",
            "published": "2023-10-27T18:00:01Z",
            "summary": "The Imaging X-ray Polarimetry Explorer (IXPE) detected X-ray polarization in\nthe nearest Seyfert 2 galaxy, the Circinus galaxy, for the first time. To\nreproduce the IXPE results, we computed the degree of polarization based on two\ntypes of radiative hydrodynamic simulations: a parsec-scale three-dimensional\nmodel and a sub-parsec-scale axisymmetric model with a higher spatial\nresolution. In a series of papers, we confirmed that these models naturally\nexplain the multi-wavelength observations of the Circinus galaxy from radio to\nX-rays. We used a Monte Carlo Simulation for Astrophysics and Cosmology code to\ncompute the linear polarization of continuum emission. We found that the degree\nof polarization based on the parsec-scale radiation-driven fountain model was\nsmaller than that observed with the IXPE. The degree of polarization based on\nthe sub-parsec-scale model depends on the hydrogen number density of the disk\n($d$), and the degree of polarization obtained from our simulation is\nconsistent with that observed with the IXPE in the case of $\\log\nd/\\mathrm{cm}^{-3} \\geq 13$. We investigate where the photons are Compton\nscattered and imply that the origin of the X-ray polarization in the Circinus\ngalaxy is the outflow inside $0.01 \\ \\mathrm{pc}$. In this case, the degree of\npolarization may change over a timescale of approximately ten years.",
            "author": [
                "Atsushi Tanimoto",
                "Keiichi Wada",
                "Yuki Kudoh",
                "Hirokazu Odaka",
                "Ryosuke Uematsu",
                "Shoji Ogawa"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18396v1",
                "http://arxiv.org/pdf/2310.18396v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18391v1",
            "title": "Entanglement in BF theory II: Edge-modes",
            "updated": "2023-10-27T18:00:00Z",
            "published": "2023-10-27T18:00:00Z",
            "summary": "We consider the entanglement entropy arising from edge-modes in Abelian\n$p$-form topological field theories in $d$ dimensions on arbitrary spatial\ntopology and across arbitrary entangling surfaces. We find a series of\ndescending area laws plus universal corrections proportional to the Betti\nnumbers of the entangling surface, which can be taken as a higher-dimensional\nversion of the \"topological entanglement entropy.\" Our calculation comes in two\nflavors: firstly, through an induced edge-mode theory appearing on the\nregulated entangling surface in a replica path integral and secondly through a\nmore rigorous definition of the entanglement entropy through an extended\nHilbert space. Along the way we establish several key results that are of their\nown merit. We explain how the edge-mode theory is a novel combination of\n$(p-1)$-form and $(d-p-2)$-form Maxwell theories linked by a chirality\ncondition, in what we coin a \"chiral mixed Maxwell theory.\" We explicitly\nevaluate the thermal partition function of this theory. Additionally we show\nthat the extended Hilbert space is completely organized into representations of\nan infinite-dimensional, centrally extended current algebra which naturally\ngeneralizes 2d Kac-Moody algebras to arbitrary dimension and topology. We\nconstruct the Verma modules and the representation characters of this algebra.\nLastly, we connect the two approaches, showing that the thermal partition\nfunction of the chiral mixed Maxwell theory is precisely an extended\nrepresentation character of our current algebra, establishing an exact\ncorrespondence of the edge-mode theory and the entanglement spectrum.",
            "author": [
                "Jackson R. Fliss",
                "Stathis Vitouladitis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18391v1",
                "http://arxiv.org/pdf/2310.18391v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18392v1",
            "title": "Shock cooling and breakout emission for optical flares associated with\n  gravitational wave events",
            "updated": "2023-10-27T18:00:00Z",
            "published": "2023-10-27T18:00:00Z",
            "summary": "The astrophysical origin of stellar-mass black hole (BH) mergers discovered\nthrough gravitational waves (GWs) is widely debated. Mergers in the disks of\nactive galactic nuclei (AGN) represent promising environments for at least a\nfraction of these events, with possible observational clues in the GW data. An\nadditional clue to unveil AGN merger environments is provided by possible\nelectromagnetic emission from post-merger accreting BHs. Associated with BH\nmergers in AGN disks, emission from shocks emerging around jets launched by\naccreting merger remnants is expected. In this paper we compute the properties\nof the emission produced during breakout and the subsequent adiabatic expansion\nphase of the shocks, and we then apply this model to optical flares suggested\nto be possibly associated with GW events. We find that the majority of the\nreported flares can be explained by the breakout and the shock cooling\nemission. If these events are real, then the merging locations of binaries are\nconstrained depending on the emission processes. If the optical flares are\nproduced by shock cooling emission, they would display moderate color\nevolution, possibly color variations among different events, a positive\ncorrelation between the delay time and the duration of flares, and accompanying\nbreakout emission in X-ray bands before the optical flares. If the breakout\nemission dominates the observed lightcurve, it is expected that the color is\ndistributed in a narrow range in the optical band, and the delay time from GW\nto electromagnetic emission is longer than $\\sim 2$ days. Hence, further\nexplorations of the distributions of delay times, color evolution of the\nflares, and associated X-ray emission will be useful to test the proposed\nemission model for the observed flares.",
            "author": [
                "Hiromichi Tagawa",
                "Shigeo S. Kimura",
                "Zolt\u00e1n Haiman",
                "Rosalba Perna",
                "Imre Bartos"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18392v1",
                "http://arxiv.org/pdf/2310.18392v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "astro-ph.GA",
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18313v1",
            "title": "FP8-LM: Training FP8 Large Language Models",
            "updated": "2023-10-27T17:59:51Z",
            "published": "2023-10-27T17:59:51Z",
            "summary": "In this paper, we explore FP8 low-bit data formats for efficient training of\nlarge language models (LLMs). Our key insight is that most variables, such as\ngradients and optimizer states, in LLM training can employ low-precision data\nformats without compromising model accuracy and requiring no changes to\nhyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision\nframework for training LLMs. This framework offers three levels of FP8\nutilization to streamline mixed-precision and distributed parallel training for\nLLMs. It gradually incorporates 8-bit gradients, optimizer states, and\ndistributed learning in an incremental manner. Experiment results show that,\nduring the training of GPT-175B model on H100 GPU platform, our FP8\nmixed-precision training framework not only achieved a remarkable 42% reduction\nin real memory usage but also ran 64% faster than the widely adopted BF16\nframework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer\nEngine by 17%. This largely reduces the training costs for large foundation\nmodels. Furthermore, our FP8 mixed-precision training methodology is generic.\nIt can be seamlessly applied to other tasks such as LLM instruction tuning and\nreinforcement learning with human feedback, offering savings in fine-tuning\nexpenses. Our FP8 low-precision training framework is open-sourced at\n{https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.",
            "author": [
                "Houwen Peng",
                "Kan Wu",
                "Yixuan Wei",
                "Guoshuai Zhao",
                "Yuxiang Yang",
                "Ze Liu",
                "Yifan Xiong",
                "Ziyue Yang",
                "Bolin Ni",
                "Jingcheng Hu",
                "Ruihang Li",
                "Miaosen Zhang",
                "Chen Li",
                "Jia Ning",
                "Ruizhe Wang",
                "Zheng Zhang",
                "Shuguang Liu",
                "Joe Chau",
                "Han Hu",
                "Peng Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18313v1",
                "http://arxiv.org/pdf/2310.18313v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18308v1",
            "title": "Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models",
            "updated": "2023-10-27T17:55:32Z",
            "published": "2023-10-27T17:55:32Z",
            "summary": "Generalist robot manipulators need to learn a wide variety of manipulation\nskills across diverse environments. Current robot training pipelines rely on\nhumans to provide kinesthetic demonstrations or to program simulation\nenvironments and to code up reward functions for reinforcement learning. Such\nhuman involvement is an important bottleneck towards scaling up robot learning\nacross diverse tasks and environments. We propose Generation to Simulation\n(Gen2Sim), a method for scaling up robot skill learning in simulation by\nautomating generation of 3D assets, task descriptions, task decompositions and\nreward functions using large pre-trained generative models of language and\nvision. We generate 3D assets for simulation by lifting open-world 2D\nobject-centric images to 3D using image diffusion models and querying LLMs to\ndetermine plausible physics parameters. Given URDF files of generated and\nhuman-developed assets, we chain-of-thought prompt LLMs to map these to\nrelevant task descriptions, temporal decompositions, and corresponding python\nreward functions for reinforcement learning. We show Gen2Sim succeeds in\nlearning policies for diverse long horizon tasks, where reinforcement learning\nwith non temporally decomposed reward functions fails. Gen2Sim provides a\nviable path for scaling up reinforcement learning for robot manipulators in\nsimulation, both by diversifying and expanding task and environment\ndevelopment, and by facilitating the discovery of reinforcement-learned\nbehaviors through temporal task decomposition in RL. Our work contributes\nhundreds of simulated assets, tasks and demonstrations, taking a step towards\nfully autonomous robotic manipulation skill acquisition in simulation.",
            "author": [
                "Pushkal Katara",
                "Zhou Xian",
                "Katerina Fragkiadaki"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18308v1",
                "http://arxiv.org/pdf/2310.18308v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18307v2",
            "title": "Torus knotted Reeb dynamics and the Calabi invariant",
            "updated": "2023-11-22T21:14:52Z",
            "published": "2023-10-27T17:55:29Z",
            "summary": "We establish the existence of a secondary Reeb orbit set with quantitative\naction and linking bounds for any contact form on the standard tight\nthree-sphere admitting the standard transverse positive $T(p,q)$ torus knot as\nan elliptic Reeb orbit with a canonically determined rotation number. This can\nbe interpreted through an ergodic lens for Reeb flows transverse to a surface\nof section. Our results also allow us to deduce an upper bound on the mean\naction of periodic orbits of naturally associated classes of area preserving\ndiffeomorphisms of the associated Seifert surfaces of genus $(p-1)(q-1)/2$ in\nterms of the Calabi invariant, without the need for genericity or Hamiltonian\nhypotheses. Our proofs utilize knot filtered embedded contact homology, which\nwas first introduced and computed by Hutchings for the standard transverse\nunknot in the irrational ellipsoids and further developed in our previous work.\nWe continue our development of nontoric methods for embedded contact homology\nand establish the knot filtration on the embedded contact homology chain\ncomplex of the standard tight three-sphere with respect to positive $T(p,q)$\ntorus knots, where there are nonvanishing differentials. We also obtain\nobstructions to the existence of exact symplectic cobordisms between positive\ntransverse torus knots.",
            "author": [
                "Jo Nelson",
                "Morgan Weiler"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18307v2",
                "http://arxiv.org/pdf/2310.18307v2"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT",
                "math.SG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18306v2",
            "title": "Supervised and Penalized Baseline Correction",
            "updated": "2023-11-14T22:36:17Z",
            "published": "2023-10-27T17:55:17Z",
            "summary": "Spectroscopic measurements can show distorted spectral shapes arising from a\nmixture of absorbing and scattering contributions. These distortions (or\nbaselines) often manifest themselves as non-constant offsets or low-frequency\noscillations. As a result, these baselines can adversely affect analytical and\nquantitative results. Baseline correction is an umbrella term where one applies\npre-processing methods to obtain baseline spectra (the unwanted distortions)\nand then remove the distortions by differencing. However, current state-of-the\nart baseline correction methods do not utilize analyte concentrations even if\nthey are available, or even if they contribute significantly to the observed\nspectral variability. We examine a class of state-of-the-art methods (penalized\nbaseline correction) and modify them such that they can accommodate a priori\nanalyte concentrations such that prediction can be enhanced. Performance will\nbe assessed on two near infra-red data sets across both classical penalized\nbaseline correction methods (without analyte information) and modified\npenalized baseline correction methods (leveraging analyte information).",
            "author": [
                "Erik Andries",
                "Ramin Nikzad-Langerodi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18306v2",
                "http://arxiv.org/pdf/2310.18306v2"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "eess.SP",
                "15, 62"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18305v1",
            "title": "Measurement and feedforward induced entanglement negativity transition",
            "updated": "2023-10-27T17:55:00Z",
            "published": "2023-10-27T17:55:00Z",
            "summary": "We study the interplay between measurement-induced dynamics and conditional\nunitary evolution in quantum systems. We numerically and analytically\ninvestigate commuting random measurement and feedforward (MFF) processes, and\nfind a sharp transition in their ability to generate entanglement negativity as\nthe number of MFF channels varies. We also establish a direct connection\nbetween these findings and transitions induced by random dephasing from an\nenvironment with broken time-reversal symmetry. In one variant of the problem,\nwe employ free probability theory to rigorously prove the transition's\nexistence. Furthermore, these MFF processes have dynamic circuit\nrepresentations that can be experimentally explored on current quantum\ncomputing platforms.",
            "author": [
                "Alireza Seif",
                "Yu-Xin Wang",
                "Ramis Movassagh",
                "Aashish A. Clerk"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18305v1",
                "http://arxiv.org/pdf/2310.18305v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.dis-nn",
                "cond-mat.mes-hall",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18301v4",
            "title": "Interactive Joint Planning for Autonomous Vehicles",
            "updated": "2023-11-22T22:25:54Z",
            "published": "2023-10-27T17:48:25Z",
            "summary": "In highly interactive driving scenarios, the actions of one agent greatly\ninfluences those of its neighbors. Planning safe motions for autonomous\nvehicles in such interactive environments, therefore, requires reasoning about\nthe impact of the ego's intended motion plan on nearby agents' behavior.\nDeep-learning-based models have recently achieved great success in trajectory\nprediction and many models in the literature allow for ego-conditioned\nprediction. However, leveraging ego-conditioned prediction remains challenging\nin downstream planning due to the complex nature of neural networks, limiting\nthe planner structure to simple ones, e.g., sampling-based planner. Despite\ntheir ability to generate fine-grained high-quality motion plans, it is\ndifficult for gradient-based planning algorithms, such as model predictive\ncontrol (MPC), to leverage ego-conditioned prediction due to their iterative\nnature and need for gradient. We present Interactive Joint Planning (IJP) that\nbridges MPC with learned prediction models in a computationally scalable manner\nto provide us the best of both the worlds. In particular, IJP jointly optimizes\nover the behavior of the ego and the surrounding agents and leverages\ndeep-learned prediction models as prediction priors that the join trajectory\noptimization tries to stay close to. Furthermore, by leveraging homotopy\nclasses, our joint optimizer searches over diverse motion plans to avoid\ngetting stuck at local minima. Closed-loop simulation result shows that IJP\nsignificantly outperforms the baselines that are either without joint\noptimization or running sampling-based planning.",
            "author": [
                "Yuxiao Chen",
                "Sushant Veer",
                "Peter Karkus",
                "Marco Pavone"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18301v4",
                "http://arxiv.org/pdf/2310.18301v4"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18297v3",
            "title": "Image Clustering Conditioned on Text Criteria",
            "updated": "2023-11-29T07:51:36Z",
            "published": "2023-10-27T17:35:01Z",
            "summary": "Classical clustering methods do not provide users with direct control of the\nclustering results, and the clustering results may not be consistent with the\nrelevant criterion that a user has in mind. In this work, we present a new\nmethodology for performing image clustering based on user-specified text\ncriteria by leveraging modern vision-language models and large language models.\nWe call our method Image Clustering Conditioned on Text Criteria (IC|TC), and\nit represents a different paradigm of image clustering. IC|TC requires a\nminimal and practical degree of human intervention and grants the user\nsignificant control over the clustering results in return. Our experiments show\nthat IC|TC can effectively cluster images with various criteria, such as human\naction, physical location, or the person's mood, while significantly\noutperforming baselines.",
            "author": [
                "Sehyun Kwon",
                "Jaeseung Park",
                "Minkyu Kim",
                "Jaewoong Cho",
                "Ernest K. Ryu",
                "Kangwook Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18297v3",
                "http://arxiv.org/pdf/2310.18297v3"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18296v1",
            "title": "Optical signatures of defects in BiFeO$_3$",
            "updated": "2023-10-27T17:31:54Z",
            "published": "2023-10-27T17:31:54Z",
            "summary": "Optical absorption in rhombohedral BiFeO$_3$ starts at photon energies below\nthe photoemission band gap of $\\approx$ 3 eV calculated from first principles.\nA shoulder at the absorption onset has so far been attributed to low-lying\nelectronic transitions or to oxygen vacancies. In this work optical spectra are\ncalculated ab initio to determine the nature of the optical transitions near\nthe absorption onset of pristine BiFeO$_3$, the effect of electron-hole\ninteraction, and the spectroscopic signatures of typical defects, i.e. doping\n(excess electrons or holes), intrinsic defects (oxygen and bismuth vacancies),\nand low-energy structural defects (ferroelectric domain walls).",
            "author": [
                "Sabine K\u00f6rbel"
            ],
            "link": [
                "http://dx.doi.org/10.1103/PhysRevMaterials.7.104402",
                "http://arxiv.org/abs/2310.18296v1",
                "http://arxiv.org/pdf/2310.18296v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18290v1",
            "title": "An Approach to Automatically generating Riddles aiding Concept\n  Attainment",
            "updated": "2023-10-27T17:28:23Z",
            "published": "2023-10-27T17:28:23Z",
            "summary": "One of the primary challenges in online learning environments, is to retain\nlearner engagement. Several different instructional strategies are proposed\nboth in online and offline environments to enhance learner engagement. The\nConcept Attainment Model is one such instructional strategy that focuses on\nlearners acquiring a deeper understanding of a concept rather than just its\ndictionary definition. This is done by searching and listing the properties\nused to distinguish examples from non-examples of various concepts. Our work\nattempts to apply the Concept Attainment Model to build conceptual riddles, to\ndeploy over online learning environments. The approach involves creating\nfactual triples from learning resources, classifying them based on their\nuniqueness to a concept into `Topic Markers' and `Common', followed by\ngenerating riddles based on the Concept Attainment Model's format and capturing\nall possible solutions to those riddles. The results obtained from the human\nevaluation of riddles prove encouraging.",
            "author": [
                "Niharika Sri Parasa",
                "Chaitali Diwan",
                "Srinath Srinivasa"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18290v1",
                "http://arxiv.org/pdf/2310.18290v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18288v3",
            "title": "Sustainable Concrete via Bayesian Optimization",
            "updated": "2023-11-20T16:17:38Z",
            "published": "2023-10-27T17:25:12Z",
            "summary": "Eight percent of global carbon dioxide emissions can be attributed to the\nproduction of cement, the main component of concrete, which is also the\ndominant source of CO2 emissions in the construction of data centers. The\ndiscovery of lower-carbon concrete formulae is therefore of high significance\nfor sustainability. However, experimenting with new concrete formulae is time\nconsuming and labor intensive, as one usually has to wait to record the\nconcrete's 28-day compressive strength, a quantity whose measurement can by its\ndefinition not be accelerated. This provides an opportunity for experimental\ndesign methodology like Bayesian Optimization (BO) to accelerate the search for\nstrong and sustainable concrete formulae. Herein, we 1) propose modeling steps\nthat make concrete strength amenable to be predicted accurately by a Gaussian\nprocess model with relatively few measurements, 2) formulate the search for\nsustainable concrete as a multi-objective optimization problem, and 3) leverage\nthe proposed model to carry out multi-objective BO with real-world strength\nmeasurements of the algorithmically proposed mixes. Our experimental results\nshow improved trade-offs between the mixtures' global warming potential (GWP)\nand their associated compressive strengths, compared to mixes based on current\nindustry practices. Our methods are open-sourced at\ngithub.com/facebookresearch/SustainableConcrete.",
            "author": [
                "Sebastian Ament",
                "Andrew Witte",
                "Nishant Garg",
                "Julius Kusuma"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18288v3",
                "http://arxiv.org/pdf/2310.18288v3"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18390v1",
            "title": "Entity Embeddings : Perspectives Towards an Omni-Modality Era for Large\n  Language Models",
            "updated": "2023-10-27T17:04:10Z",
            "published": "2023-10-27T17:04:10Z",
            "summary": "Large Language Models (LLMs) are evolving to integrate multiple modalities,\nsuch as text, image, and audio into a unified linguistic space. We envision a\nfuture direction based on this framework where conceptual entities defined in\nsequences of text can also be imagined as modalities. Such a formulation has\nthe potential to overcome the cognitive and computational limitations of\ncurrent models. Several illustrative examples of such potential implicit\nmodalities are given. Along with vast promises of the hypothesized structure,\nexpected challenges are discussed as well.",
            "author": [
                "Eren Unlu",
                "Unver Ciftci"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18390v1",
                "http://arxiv.org/pdf/2310.18390v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18275v1",
            "title": "The Pak--Postnikov and Naruse skew hook length formulas: a new proof",
            "updated": "2023-10-27T17:02:29Z",
            "published": "2023-10-27T17:02:29Z",
            "summary": "The classical hook length formula of enumerative combinatorics expresses the\nnumber of standard Young tableaux of a given partition shape as a single\nfraction. In recent years, two generalizations of this formula have emerged:\none by Pak and Postnikov, replacing the number by a (rational) generating\nfunction, and one by Naruse, which generalizes the setting from a partition to\na skew partition. Both generalizations appear to lie significantly deeper, with\nno simple proofs known. We combine them into a generating-function identity for\nskew partitions, and prove it in a fairly elementary way using recursion,\ndeterminants and simple combinatorics.",
            "author": [
                "Darij Grinberg",
                "Nazar Korniichuk",
                "Kostiantyn Molokanov",
                "Severyn Khomych"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18275v1",
                "http://arxiv.org/pdf/2310.18275v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05A17, 05E05, 15A15, 05A19"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18274v1",
            "title": "LipSim: A Provably Robust Perceptual Similarity Metric",
            "updated": "2023-10-27T16:59:51Z",
            "published": "2023-10-27T16:59:51Z",
            "summary": "Recent years have seen growing interest in developing and applying perceptual\nsimilarity metrics. Research has shown the superiority of perceptual metrics\nover pixel-wise metrics in aligning with human perception and serving as a\nproxy for the human visual system. On the other hand, as perceptual metrics\nrely on neural networks, there is a growing concern regarding their resilience,\ngiven the established vulnerability of neural networks to adversarial attacks.\nIt is indeed logical to infer that perceptual metrics may inherit both the\nstrengths and shortcomings of neural networks. In this work, we demonstrate the\nvulnerability of state-of-the-art perceptual similarity metrics based on an\nensemble of ViT-based feature extractors to adversarial attacks. We then\npropose a framework to train a robust perceptual similarity metric called\nLipSim (Lipschitz Similarity Metric) with provable guarantees. By leveraging\n1-Lipschitz neural networks as the backbone, LipSim provides guarded areas\naround each data point and certificates for all perturbations within an\n$\\ell_2$ ball. Finally, a comprehensive set of experiments shows the\nperformance of LipSim in terms of natural and certified scores and on the image\nretrieval application. The code is available at\nhttps://github.com/SaraGhazanfari/LipSim.",
            "author": [
                "Sara Ghazanfari",
                "Alexandre Araujo",
                "Prashanth Krishnamurthy",
                "Farshad Khorrami",
                "Siddharth Garg"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18274v1",
                "http://arxiv.org/pdf/2310.18274v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18268v1",
            "title": "PlantPlotGAN: A Physics-Informed Generative Adversarial Network for\n  Plant Disease Prediction",
            "updated": "2023-10-27T16:56:28Z",
            "published": "2023-10-27T16:56:28Z",
            "summary": "Monitoring plantations is crucial for crop management and producing healthy\nharvests. Unmanned Aerial Vehicles (UAVs) have been used to collect\nmultispectral images that aid in this monitoring. However, given the number of\nhectares to be monitored and the limitations of flight, plant disease signals\nbecome visually clear only in the later stages of plant growth and only if the\ndisease has spread throughout a significant portion of the plantation. This\nlimited amount of relevant data hampers the prediction models, as the\nalgorithms struggle to generalize patterns with unbalanced or unrealistic\naugmented datasets effectively. To address this issue, we propose PlantPlotGAN,\na physics-informed generative model capable of creating synthetic multispectral\nplot images with realistic vegetation indices. These indices served as a proxy\nfor disease detection and were used to evaluate if our model could help\nincrease the accuracy of prediction models. The results demonstrate that the\nsynthetic imagery generated from PlantPlotGAN outperforms state-of-the-art\nmethods regarding the Fr\\'echet inception distance. Moreover, prediction models\nachieve higher accuracy metrics when trained with synthetic and original\nimagery for earlier plant disease detection compared to the training processes\nbased solely on real imagery.",
            "author": [
                "Felipe A. Lopes",
                "Vasit Sagan",
                "Flavio Esposito"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18268v1",
                "http://arxiv.org/pdf/2310.18268v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18263v1",
            "title": "MalFake: A Multimodal Fake News Identification for Malayalam using\n  Recurrent Neural Networks and VGG-16",
            "updated": "2023-10-27T16:51:29Z",
            "published": "2023-10-27T16:51:29Z",
            "summary": "The amount of news being consumed online has substantially expanded in recent\nyears. Fake news has become increasingly common, especially in regional\nlanguages like Malayalam, due to the rapid publication and lack of editorial\nstandards on some online sites. Fake news may have a terrible effect on\nsociety, causing people to make bad judgments, lose faith in authorities, and\neven engage in violent behavior. When we take into the context of India, there\nare many regional languages, and fake news is spreading in every language.\nTherefore, providing efficient techniques for identifying false information in\nregional tongues is crucial. Until now, little to no work has been done in\nMalayalam, extracting features from multiple modalities to classify fake news.\nMultimodal approaches are more accurate in detecting fake news, as features\nfrom multiple modalities are extracted to build the deep learning\nclassification model. As far as we know, this is the first piece of work in\nMalayalam that uses multimodal deep learning to tackle false information.\nModels trained with more than one modality typically outperform models taught\nwith only one modality. Our study in the Malayalam language utilizing\nmultimodal deep learning is a significant step toward more effective\nmisinformation detection and mitigation.",
            "author": [
                "Adhish S. Sujan",
                "Ajitha. V",
                "Aleena Benny",
                "Amiya M. P.",
                "V. S. Anoop"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18263v1",
                "http://arxiv.org/pdf/2310.18263v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18262v2",
            "title": "Resolved-sideband cooling of a single $^9$Be$^+$ ion in a Penning trap",
            "updated": "2023-11-12T21:04:57Z",
            "published": "2023-10-27T16:50:14Z",
            "summary": "Manipulating individual trapped ions at the single quantum level has become\nstandard practice in radio-frequency ion traps, enabling applications from\nquantum information processing to precision metrology. The key ingredient is\nground-state cooling of the particle's motion through resolved-sideband laser\ncooling. Ultra-high-presicion experiments using Penning ion traps will greatly\nbenefit from the reduction of systematic errors offered by full motional\ncontrol, with applications to atomic masses and $g$-factor measurements,\ndeterminations of fundamental constants or related tests of fundamental\nphysics. In addition, it will allow to implement quantum logic spectroscopy, a\ntechnique that has enabled a new class of precision measurements in\nradio-frequency ion traps. Here we demonstrate resolved-sideband laser cooling\nof the axial motion of a single $^9$Be$^+$ ion in a cryogenic 5 Tesla Penning\ntrap system using a two-photon stimulated-Raman process, reaching a mean phonon\nnumber of $\\bar{n}_z = 0.10(4)$. This is a fundamental step in the\nimplementation of quantum logic spectroscopy for matter-antimatter comparison\ntests in the baryonic sector of the Standard Model and a key step towards\nimproved precision experiments in Penning traps operating at the quantum limit.",
            "author": [
                "Juan M. Cornejo",
                "Johannes Brombacher",
                "Julia A. Coenders",
                "Moritz von Boehn",
                "Teresa Meiners",
                "Malte Niemann",
                "Stefan Ulmer",
                "Christian Ospelkaus"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18262v2",
                "http://arxiv.org/pdf/2310.18262v2"
            ],
            "primary_category": "physics.atom-ph",
            "category": [
                "physics.atom-ph",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.03375v1",
            "title": "Edge AI Inference in Heterogeneous Constrained Computing: Feasibility\n  and Opportunities",
            "updated": "2023-10-27T16:46:59Z",
            "published": "2023-10-27T16:46:59Z",
            "summary": "The network edge's role in Artificial Intelligence (AI) inference processing\nis rapidly expanding, driven by a plethora of applications seeking\ncomputational advantages. These applications strive for data-driven efficiency,\nleveraging robust AI capabilities and prioritizing real-time responsiveness.\nHowever, as demand grows, so does system complexity. The proliferation of AI\ninference accelerators showcases innovation but also underscores challenges,\nparticularly the varied software and hardware configurations of these devices.\nThis diversity, while advantageous for certain tasks, introduces hurdles in\ndevice integration and coordination. In this paper, our objectives are\nthree-fold. Firstly, we outline the requirements and components of a framework\nthat accommodates hardware diversity. Next, we assess the impact of device\nheterogeneity on AI inference performance, identifying strategies to optimize\noutcomes without compromising service quality. Lastly, we shed light on the\nprevailing challenges and opportunities in this domain, offering insights for\nboth the research community and industry stakeholders.",
            "author": [
                "Roberto Morabito",
                "Mallik Tatipamula",
                "Sasu Tarkoma",
                "Mung Chiang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03375v1",
                "http://arxiv.org/pdf/2311.03375v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR",
                "cs.AI",
                "cs.DC",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18255v2",
            "title": "Van Vleck Analysis of Angularly Distorted Octahedra using\n  VanVleckCalculator",
            "updated": "2023-11-02T21:04:46Z",
            "published": "2023-10-27T16:40:58Z",
            "summary": "Van Vleck modes describe all possible displacements of\noctahedrally-coordinated ligands about a core atom. They are a useful\nanalytical tool for analysing the distortion of octahedra, particularly for the\nfirst-order Jahn-Teller distortion. Determination of Van Vleck modes of an\noctahedron is complicated by the presence of angular distortion of octahedra\nhowever. This problem is most commonly resolved by calculating the bond\ndistortion modes ($Q_2$, $Q_3$) along the bond axes of the octahedron,\ndisregarding the angular distortion and losing information on the octahedral\nshear modes ($Q_4$, $Q_5$, and $Q_6$) in the process. In this paper, the\nvalidity of assuming bond lengths to be orthogonal in order to calculate the\nvan Vleck modes is discussed, and a method is described for calculating Van\nVleck modes without disregarding the angular distortion. A Python code for\ndoing this, VanVleckCalculator, is introduced, and some examples of its use are\ngiven. Finally, we show that octahedral shear and angular distortion are often,\nbut not always, correlated, and propose a parameter as the shear fraction,\n$\\eta$. We demonstrate that $\\eta$ can be used to predict whether the values\nwill be correlated when varying a tuning parameter such as temperature or\npressure.",
            "author": [
                "Liam. A. V. Nagle-Cocco",
                "Si\u00e2n E. Dutton"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18255v2",
                "http://arxiv.org/pdf/2310.18255v2"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18251v1",
            "title": "A Self-Supervised Approach to Land Cover Segmentation",
            "updated": "2023-10-27T16:37:36Z",
            "published": "2023-10-27T16:37:36Z",
            "summary": "Land use/land cover change (LULC) maps are integral resources in earth\nscience and agricultural research. Due to the nature of such maps, the creation\nof LULC maps is often constrained by the time and human resources necessary to\naccurately annotate satellite imagery and remote sensing data. While computer\nvision models that perform semantic segmentation to create detailed labels from\nsuch data are not uncommon, litle research has been done on self-supervised and\nunsupervised approaches to labelling LULC maps without the use of ground-truth\nmasks. Here, we demonstrate a self-supervised method of land cover segmentation\nthat has no need for high-quality ground truth labels. The proposed deep\nlearning employs a frozen pre-trained ViT backbone transferred from DINO in a\nSTEGO architecture and is fine-tuned using a custom dataset consisting of very\nhigh resolution (VHR) sattelite imagery. After only 10 epochs of fine-tuning,\nan accuracy of roughly 52% was observed across 5 samples, signifying the\nfeasibility of self-supervised models for the automated labelling of VHR LULC\nmaps.",
            "author": [
                "Charles Moore",
                "Dakota Hester"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18251v1",
                "http://arxiv.org/pdf/2310.18251v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18249v1",
            "title": "Leveraging Machine Learning Models for Peptide-Protein Interaction\n  Prediction",
            "updated": "2023-10-27T16:36:06Z",
            "published": "2023-10-27T16:36:06Z",
            "summary": "Peptides play a pivotal role in a wide range of biological activities through\nparticipating in up to 40% protein-protein interactions in cellular processes.\nThey also demonstrate remarkable specificity and efficacy, making them\npromising candidates for drug development. However, predicting peptide-protein\ncomplexes by traditional computational approaches, such as Docking and\nMolecular Dynamics simulations, still remains a challenge due to high\ncomputational cost, flexible nature of peptides, and limited structural\ninformation of peptide-protein complexes. In recent years, the surge of\navailable biological data has given rise to the development of an increasing\nnumber of machine learning models for predicting peptide-protein interactions.\nThese models offer efficient solutions to address the challenges associated\nwith traditional computational approaches. Furthermore, they offer enhanced\naccuracy, robustness, and interpretability in their predictive outcomes. This\nreview presents a comprehensive overview of machine learning and deep learning\nmodels that have emerged in recent years for the prediction of peptide-protein\ninteractions.",
            "author": [
                "Song Yin",
                "Xuenan Mi",
                "Diwakar Shukla"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18249v1",
                "http://arxiv.org/pdf/2310.18249v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18248v1",
            "title": "Regenerations and applications",
            "updated": "2023-10-27T16:34:40Z",
            "published": "2023-10-27T16:34:40Z",
            "summary": "Chen-Gounelas-Liedtke recently introduced a powerful regeneration technique,\na process opposite to specialization, to prove existence results for rational\ncurves on projective $K3$ surfaces. We show that, for projective irreducible\nholomorphic symplectic manifolds, an analogous regeneration principle holds and\nprovides a very flexible tool to prove existence of uniruled divisors,\nsignificantly improving known results.",
            "author": [
                "Giovanni Mongardi",
                "Gianluca Pacienza"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18248v1",
                "http://arxiv.org/pdf/2310.18248v1"
            ],
            "primary_category": "math.AG",
            "category": [
                "math.AG",
                "14H45, 14J42"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18247v1",
            "title": "Guided Data Augmentation for Offline Reinforcement Learning and\n  Imitation Learning",
            "updated": "2023-10-27T16:34:00Z",
            "published": "2023-10-27T16:34:00Z",
            "summary": "Learning from demonstration (LfD) is a popular technique that uses expert\ndemonstrations to learn robot control policies. However, the difficulty in\nacquiring expert-quality demonstrations limits the applicability of LfD\nmethods: real-world data collection is often costly, and the quality of the\ndemonstrations depends greatly on the demonstrator's abilities and safety\nconcerns. A number of works have leveraged data augmentation (DA) to\ninexpensively generate additional demonstration data, but most DA works\ngenerate augmented data in a random fashion and ultimately produce highly\nsuboptimal data. In this work, we propose Guided Data Augmentation (GuDA), a\nhuman-guided DA framework that generates expert-quality augmented data. The key\ninsight of GuDA is that while it may be difficult to demonstrate the sequence\nof actions required to produce expert data, a user can often easily identify\nwhen an augmented trajectory segment represents task progress. Thus, the user\ncan impose a series of simple rules on the DA process to automatically generate\naugmented samples that approximate expert behavior. To extract a policy from\nGuDA, we use off-the-shelf offline reinforcement learning and behavior cloning\nalgorithms. We evaluate GuDA on a physical robot soccer task as well as\nsimulated D4RL navigation tasks, a simulated autonomous driving task, and a\nsimulated soccer task. Empirically, we find that GuDA enables learning from a\nsmall set of potentially suboptimal demonstrations and substantially\noutperforms a DA strategy that samples augmented data randomly.",
            "author": [
                "Nicholas E. Corrado",
                "Yuxiao Qu",
                "John U. Balis",
                "Adam Labiosa",
                "Josiah P. Hanna"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18247v1",
                "http://arxiv.org/pdf/2310.18247v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18241v1",
            "title": "$\u03b1$-Mutual Information: A Tunable Privacy Measure for Privacy\n  Protection in Data Sharing",
            "updated": "2023-10-27T16:26:14Z",
            "published": "2023-10-27T16:26:14Z",
            "summary": "This paper adopts Arimoto's $\\alpha$-Mutual Information as a tunable privacy\nmeasure, in a privacy-preserving data release setting that aims to prevent\ndisclosing private data to adversaries. By fine-tuning the privacy metric, we\ndemonstrate that our approach yields superior models that effectively thwart\nattackers across various performance dimensions. We formulate a general\ndistortion-based mechanism that manipulates the original data to offer privacy\nprotection. The distortion metrics are determined according to the data\nstructure of a specific experiment. We confront the problem expressed in the\nformulation by employing a general adversarial deep learning framework that\nconsists of a releaser and an adversary, trained with opposite goals. This\nstudy conducts empirical experiments on images and time-series data to verify\nthe functionality of $\\alpha$-Mutual Information. We evaluate the\nprivacy-utility trade-off of customized models and compare them to mutual\ninformation as the baseline measure. Finally, we analyze the consequence of an\nattacker's access to side information about private data and witness that\nadapting the privacy measure results in a more refined model than the\nstate-of-the-art in terms of resiliency against side information.",
            "author": [
                "MirHamed Jafarzadeh Asl",
                "Mohammadhadi Shateri",
                "Fabrice Labeau"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18241v1",
                "http://arxiv.org/pdf/2310.18241v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18240v1",
            "title": "Proxy Design: A Method for Involving Proxy Users to Speak on Behalf of\n  Vulnerable or Unreachable Users in Co-Design",
            "updated": "2023-10-27T16:24:54Z",
            "published": "2023-10-27T16:24:54Z",
            "summary": "Designing digital artifacts is not a linear, straightforward process. This is\nparticularly true when applying a user-centered design approach, or co-design,\nwith users who are unable to participate in the design process. Although the\nreduced participation of a particular user group may harm the end result, the\nliterature on solving this issue is sparse. In this article, proxy design is\noutlined as a method for involving a user group as proxy users to speak on\nbehalf of a group that is difficult to reach. We present a design ethnography\nspanning three years at a cancer rehabilitation clinic, where digital artifacts\nwere designed to be used collaboratively by nurses and patients. The empirical\ndata were analyzed using content analysis and consisted of 20 observation days\nat the clinic, six proxy design workshops, 21 telephone consultations between\npatients and nurses, and log data from the digital artifact. We show that\nsimulated consultations, with nurses roleplaying as proxies for patients\nignited and initiated the design process and enabled an efficient in-depth\nunderstanding of patients. Moreover, we reveal how proxy design as a method\nfurther expanded the design. We illustrate: (1) proxy design as a method for\ninitiating design, (2) proxy design as an embedded element in co-design and (3)\nsix design guidelines that should be considered when engaging in proxy design.\nThe main contribution is the conceptualization of proxy design as a method that\ncan ignite and initiate the co-design process when important users are\nunreachable, vulnerable or unable to represent themselves in the co-design\nprocess. Based on the empirical findings from a design ethnography that\ninvolved nurses as proxy users speaking on behalf of patients, the article\nshows that roleplaying in proxy design is a fitting way of initiating the\ndesign process, outlining proxy design as an embedded element of co-design.",
            "author": [
                "Anna Sigridur Islind",
                "Johan Lundin",
                "Katerina Cerna",
                "Tomas Lindroth",
                "Linda \u00c5keflo",
                "Gunnar Steineck"
            ],
            "link": [
                "http://dx.doi.org/10.1108/ITP-07-2021-0539",
                "http://arxiv.org/abs/2310.18240v1",
                "http://arxiv.org/pdf/2310.18240v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18239v1",
            "title": "Fine-Tuning Language Models Using Formal Methods Feedback",
            "updated": "2023-10-27T16:24:24Z",
            "published": "2023-10-27T16:24:24Z",
            "summary": "Although pre-trained language models encode generic knowledge beneficial for\nplanning and control, they may fail to generate appropriate control policies\nfor domain-specific tasks. Existing fine-tuning methods use human feedback to\naddress this limitation, however, sourcing human feedback is labor intensive\nand costly. We present a fully automated approach to fine-tune pre-trained\nlanguage models for applications in autonomous systems, bridging the gap\nbetween generic knowledge and domain-specific requirements while reducing cost.\nThe method synthesizes automaton-based controllers from pre-trained models\nguided by natural language task descriptions. These controllers are verifiable\nagainst independently provided specifications within a world model, which can\nbe abstract or obtained from a high-fidelity simulator. Controllers with high\ncompliance with the desired specifications receive higher ranks, guiding the\niterative fine-tuning process. We provide quantitative evidences, primarily in\nautonomous driving, to demonstrate the method's effectiveness across multiple\ntasks. The results indicate an improvement in percentage of specifications\nsatisfied by the controller from 60% to 90%.",
            "author": [
                "Yunhao Yang",
                "Neel P. Bhatt",
                "Tyler Ingebrand",
                "William Ward",
                "Steven Carr",
                "Zhangyang Wang",
                "Ufuk Topcu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18239v1",
                "http://arxiv.org/pdf/2310.18239v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.FL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18235v2",
            "title": "Davidsonian Scene Graph: Improving Reliability in Fine-grained\n  Evaluation for Text-to-Image Generation",
            "updated": "2023-10-30T16:00:49Z",
            "published": "2023-10-27T16:20:10Z",
            "summary": "Evaluating text-to-image models is notoriously difficult. A strong recent\napproach for assessing text-image faithfulness is based on QG/A (question\ngeneration and answering), which uses pre-trained foundational models to\nautomatically generate a set of questions and answers from the prompt, and\noutput images are scored based on whether these answers extracted with a visual\nquestion answering model are consistent with the prompt-based answers. This\nkind of evaluation is naturally dependent on the quality of the underlying QG\nand QA models. We identify and address several reliability challenges in\nexisting QG/A work: (a) QG questions should respect the prompt (avoiding\nhallucinations, duplications, and omissions) and (b) VQA answers should be\nconsistent (not asserting that there is no motorcycle in an image while also\nclaiming the motorcycle is blue). We address these issues with Davidsonian\nScene Graph (DSG), an empirically grounded evaluation framework inspired by\nformal semantics. DSG is an automatic, graph-based QG/A that is modularly\nimplemented to be adaptable to any QG/A module. DSG produces atomic and unique\nquestions organized in dependency graphs, which (i) ensure appropriate semantic\ncoverage and (ii) sidestep inconsistent answers. With extensive experimentation\nand human evaluation on a range of model configurations (LLM, VQA, and T2I), we\nempirically demonstrate that DSG addresses the challenges noted above. Finally,\nwe present DSG-1k, an open-sourced evaluation benchmark that includes 1,060\nprompts, covering a wide range of fine-grained semantic categories with a\nbalanced distribution. We release the DSG-1k prompts and the corresponding DSG\nquestions.",
            "author": [
                "Jaemin Cho",
                "Yushi Hu",
                "Roopal Garg",
                "Peter Anderson",
                "Ranjay Krishna",
                "Jason Baldridge",
                "Mohit Bansal",
                "Jordi Pont-Tuset",
                "Su Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18235v2",
                "http://arxiv.org/pdf/2310.18235v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18234v1",
            "title": "Edge AI-Based Vein Detector for Efficient Venipuncture in the\n  Antecubital Fossa",
            "updated": "2023-10-27T16:19:26Z",
            "published": "2023-10-27T16:19:26Z",
            "summary": "Assessing the condition and visibility of veins is a crucial step before\nobtaining intravenous access in the antecubital fossa, which is a common\nprocedure to draw blood or administer intravenous therapies (IV therapies).\nEven though medical practitioners are highly skilled at intravenous\ncannulation, they usually struggle to perform the procedure in patients with\nlow visible veins due to fluid retention, age, overweight, dark skin tone, or\ndiabetes. Recently, several investigations proposed combining Near Infrared\n(NIR) imaging and deep learning (DL) techniques for forearm vein segmentation.\nAlthough they have demonstrated compelling results, their use has been rather\nlimited owing to the portability and precision requirements to perform\nvenipuncture. In this paper, we aim to contribute to bridging this gap using\nthree strategies. First, we introduce a new NIR-based forearm vein segmentation\ndataset of 2,016 labelled images collected from 1,008 subjects with low visible\nveins. Second, we propose a modified U-Net architecture that locates veins\nspecifically in the antecubital fossa region of the examined patient. Finally,\na compressed version of the proposed architecture was deployed inside a\nbespoke, portable vein finder device after testing four common embedded\nmicrocomputers and four common quantization modalities. Experimental results\nshowed that the model compressed with Dynamic Range Quantization and deployed\non a Raspberry Pi 4B card produced the best execution time and precision\nbalance, with 5.14 FPS and 0.957 of latency and Intersection over Union (IoU),\nrespectively. These results show promising performance inside a\nresource-restricted low-cost device.",
            "author": [
                "Edwin Salcedo",
                "Patricia Pe\u00f1aloza"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-47640-2_24",
                "http://arxiv.org/abs/2310.18234v1",
                "http://arxiv.org/pdf/2310.18234v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18231v1",
            "title": "Well-posedness analysis of the Cahn-Hilliard-Biot model",
            "updated": "2023-10-27T16:12:17Z",
            "published": "2023-10-27T16:12:17Z",
            "summary": "We investigate the well-posedness of the recently proposed Cahn-Hilliard-Biot\nmodel. The model is a three-way coupled PDE of elliptic-parabolic nature, with\nseveral nonlinearities and the fourth order term known to the Cahn-Hilliard\nsystem. We show existence of weak solutions to the variational form of the\nequations and uniqueness under certain conditions of the material parameters\nand secondary consolidation, adding regularizing effects. Existence is shown by\ndiscretizing in space and applying ODE-theory (the Peano-Cauchy theorem) to\nprove existence of the discrete system, followed by compactness arguments to\nretain solutions of the continuous system. In addition, the continuous\ndependence of solutions on the data is established, in particular implying\nuniqueness. Both results build strongly on the inherent gradient flow structure\nof the model.",
            "author": [
                "Cedric Riethm\u00fcller",
                "Erlend Storvik",
                "Jakub Wiktor Both",
                "Florin Adrian Radu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18231v1",
                "http://arxiv.org/pdf/2310.18231v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18230v2",
            "title": "Deep Transformed Gaussian Processes",
            "updated": "2023-11-02T10:25:56Z",
            "published": "2023-10-27T16:09:39Z",
            "summary": "Transformed Gaussian Processes (TGPs) are stochastic processes specified by\ntransforming samples from the joint distribution from a prior process\n(typically a GP) using an invertible transformation; increasing the flexibility\nof the base process.\n  Furthermore, they achieve competitive results compared with Deep Gaussian\nProcesses (DGPs), which are another generalization constructed by a\nhierarchical concatenation of GPs. In this work, we propose a generalization of\nTGPs named Deep Transformed Gaussian Processes (DTGPs), which follows the trend\nof concatenating layers of stochastic processes. More precisely, we obtain a\nmulti-layer model in which each layer is a TGP. This generalization implies an\nincrement of flexibility with respect to both TGPs and DGPs. Exact inference in\nsuch a model is intractable. However, we show that one can use variational\ninference to approximate the required computations yielding a straightforward\nextension of the popular DSVI inference algorithm Salimbeni et al (2017). The\nexperiments conducted evaluate the proposed novel DTGPs in multiple regression\ndatasets, achieving good scalability and performance.",
            "author": [
                "Francisco Javier S\u00e1ez-Maldonado",
                "Juan Maro\u00f1as",
                "Daniel Hern\u00e1ndez-Lobato"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18230v2",
                "http://arxiv.org/pdf/2310.18230v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18229v1",
            "title": "Revising with a Backward Glance: Regressions and Skips during Reading as\n  Cognitive Signals for Revision Policies in Incremental Processing",
            "updated": "2023-10-27T16:08:15Z",
            "published": "2023-10-27T16:08:15Z",
            "summary": "In NLP, incremental processors produce output in instalments, based on\nincoming prefixes of the linguistic input. Some tokens trigger revisions,\ncausing edits to the output hypothesis, but little is known about why models\nrevise when they revise. A policy that detects the time steps where revisions\nshould happen can improve efficiency. Still, retrieving a suitable signal to\ntrain a revision policy is an open problem, since it is not naturally available\nin datasets. In this work, we investigate the appropriateness of regressions\nand skips in human reading eye-tracking data as signals to inform revision\npolicies in incremental sequence labelling. Using generalised mixed-effects\nmodels, we find that the probability of regressions and skips by humans can\npotentially serve as useful predictors for revisions in BiLSTMs and Transformer\nmodels, with consistent results for various languages.",
            "author": [
                "Brielen Madureira",
                "Pelin \u00c7elikkol",
                "David Schlangen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18229v1",
                "http://arxiv.org/pdf/2310.18229v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18228v2",
            "title": "First-principles molecular quantum electrodynamics theory at all\n  coupling strengths",
            "updated": "2023-11-08T17:37:46Z",
            "published": "2023-10-27T16:07:09Z",
            "summary": "The ever-growing intersection of quantum electrodynamics (QED) and molecular\nprocesses has shown remarkable and unanticipated advancements in altering\nmolecular properties and reactivity by exploiting light-matter couplings. In\nrecent years, multiple ab initio methods have been developed to compute the\neigenstates of molecular systems strongly coupled to cavities, ranging from the\nmean-field to quantum many-body methods. The quantum many-body methods, such as\ncoupled-cluster theories, usually rely on the quality of mean-field reference\nwavefunctions. Hence, developing efficient and physically reliable mean-filed\napproaches for molecular quantum electrodynamics problems is crucial. The\ncurrent widely used methods, such as QED Hartree-Fock and the self-consistent\ncounterpart, are limited to specific coupling regimes. In this work, we\ndeveloped a variational transformation-based molecular quantum electrodynamics\nmean-field method, namely VT-QEDHF, for light-matter interaction at arbitrary\ncoupling strength. The numerical benchmark demonstrates that the VT-QEDHF\nmethod naturally connects both QEDHF and self-consistent QEDHF methods at the\ntwo limits, showcasing the advantage of VT-QEHDF across all coupling strengths.",
            "author": [
                "Xinyang Li",
                "Yu Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18228v2",
                "http://arxiv.org/pdf/2310.18228v2"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18227v1",
            "title": "Quench dynamics in lattices above one dimension: the free fermionic case",
            "updated": "2023-10-27T16:02:19Z",
            "published": "2023-10-27T16:02:19Z",
            "summary": "We begin a systematic investigation of quench dynamics in higher-dimensional\nlattice systems considering the case of non-interacting fermions with conserved\nparticle number. We prepare the system in a translational-invariant\nnon-equilibrium initial state -- the simplest example being a classical\nconfiguration with fermions at fixed positions on the lattice -- and let it to\nevolve in time. We characterise the system's dynamics by measuring the\nentanglement between a finite connected region and its complement. We observe\nthe transmutation of entanglement entropy into thermodynamic entropy and\ninvestigate how this process depends on the shape and orientation of the region\nwith respect to the underlying lattice. Interestingly, we find that irregular\nregions display a distinctive multi-slope entanglement growth, while the\ndependence on the orientation angle is generically fairly weak. This is\nparticularly true for regions with a large (discrete) rotational symmetry\ngroup. The main tool of our analysis is the celebrated quasiparticle picture of\nCalabrese and Cardy, which we generalise to describe the case at hand.\nSpecifically, we show that for generic initial configurations (even when\nrestricting to classical ones) one has to allow for the production of\nmultiplets involving ${n>2}$ quasiparticles and carrying non-diagonal\ncorrelations. We obtain quantitatively accurate predictions -- tested against\nexact numerics -- and propose an efficient Monte Carlo-based scheme to evaluate\nthem for arbitrary connected regions of generic higher dimensional lattices.",
            "author": [
                "Molly Gibbins",
                "Arash Jafarizadeh",
                "Adam Smith",
                "Bruno Bertini"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18227v1",
                "http://arxiv.org/pdf/2310.18227v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.stat-mech",
                "cond-mat.str-el",
                "nlin.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18222v1",
            "title": "TBDLNet: a network for classifying multidrug-resistant and\n  drug-sensitive tuberculosis",
            "updated": "2023-10-27T15:51:33Z",
            "published": "2023-10-27T15:51:33Z",
            "summary": "This paper proposes applying a novel deep-learning model, TBDLNet, to\nrecognize CT images to classify multidrug-resistant and drug-sensitive\ntuberculosis automatically. The pre-trained ResNet50 is selected to extract\nfeatures. Three randomized neural networks are used to alleviate the\noverfitting problem. The ensemble of three RNNs is applied to boost the\nrobustness via majority voting. The proposed model is evaluated by five-fold\ncross-validation. Five indexes are selected in this paper, which are accuracy,\nsensitivity, precision, F1-score, and specificity. The TBDLNet achieves 0.9822\naccuracy, 0.9815 specificity, 0.9823 precision, 0.9829 sensitivity, and 0.9826\nF1-score, respectively. The TBDLNet is suitable for classifying\nmultidrug-resistant tuberculosis and drug-sensitive tuberculosis. It can detect\nmultidrug-resistant pulmonary tuberculosis as early as possible, which helps to\nadjust the treatment plan in time and improve the treatment effect.",
            "author": [
                "Ziquan Zhu",
                "Jing Tao",
                "Shuihua Wang",
                "Xin Zhang",
                "Yudong Zhang"
            ],
            "link": [
                "http://dx.doi.org/10.1002/ENG2.12815",
                "http://arxiv.org/abs/2310.18222v1",
                "http://arxiv.org/pdf/2310.18222v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18219v1",
            "title": "SWASTi-CME: A physics-based model to study CME evolution and its\n  interaction with Solar Wind",
            "updated": "2023-10-27T15:47:35Z",
            "published": "2023-10-27T15:47:35Z",
            "summary": "Coronal mass ejections (CMEs) are primary drivers of space weather and\nstudying their evolution in the inner heliosphere is vital to prepare for a\ntimely response. Solar wind streams, acting as background, influence their\npropagation in the heliosphere and associated geomagnetic storm activity. This\nstudy introduces SWASTi-CME, a newly developed MHD-based CME model integrated\ninto the Space Weather Adaptive SimulaTion (SWASTi) framework. It incorporates\na non-magnetized elliptic cone and a magnetized flux rope CME model. To\nvalidate the model's performance with in-situ observation at L1, two Carrington\nrotations were chosen: one during solar maxima with multiple CMEs, and one\nduring solar minima with a single CME. The study also presents a quantitative\nanalysis of CME-solar wind interaction using this model. To account for ambient\nsolar wind effects, two scenarios of different complexity in solar wind\nconditions were established. The results indicate that ambient conditions can\nsignificantly impact some of the CME properties in the inner heliosphere. We\nfound that the drag force on the CME front exhibits a variable nature,\nresulting in asymmetric deformation of the CME leading edge. Additionally, the\nstudy reveals that the impact on the distribution of CME internal pressure\nprimarily occurs during the initial stage, while the CME density distribution\nis affected throughout its propagation. Moreover, regardless of the ambient\nconditions, it was observed that after a certain propagation time (t), the CME\nvolume follows a non-fractal power-law expansion ($\\propto t^{3.03-3.33}$) due\nto the attainment of a balanced state with ambient.",
            "author": [
                "Prateek Mayank",
                "Bhargav Vaidya",
                "Wageesh Mishra",
                "D. Chakrabarty"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18219v1",
                "http://arxiv.org/pdf/2310.18219v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR",
                "astro-ph.EP",
                "physics.space-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18218v1",
            "title": "Random Fields from Quenched Disorder in an Archetype for Correlated\n  Electrons: the Parallel Spin Stripe Phase of\n  La$_{1.6-x}$Nd$_{0.4}$Sr$_x$CuO$_4$ at the 1/8 Anomaly",
            "updated": "2023-10-27T15:46:29Z",
            "published": "2023-10-27T15:46:29Z",
            "summary": "The parallel stripe phase is remarkable both in its own right, and in\nrelation to the other phases it co-exists with. Its inhomogeneous nature makes\nsuch states susceptible to random fields from quenched magnetic vacancies. We\nargue this is the case by introducing low concentrations of nonmagnetic Zn\nimpurities (0-10%) into La$_{1.6-x}$Nd$_{0.4}$Sr$_x$CuO$_4$ (Nd-LSCO) with $x$\n= 0.125 in single crystal form, well below the percolation threshold of $\\sim$\n41% for two-dimensional (2D) square lattice. Elastic neutron scattering\nmeasurements on these crystals show clear magnetic quasi-Bragg peaks at all Zn\ndopings. While all the Zn-doped crystals display order parameters that merge\ninto each other and the background at $\\sim$ 68 K, the temperature dependence\nof the order parameter as a function of Zn concentration is drastically\ndifferent. This result is consistent with meandering charge stripes within the\nparallel stripe phase, which are pinned in the presence of quenched magnetic\nvacancies. In turn it implies vacancies that preferentially occupy sites within\nthe charge stripes, and hence that can be very effective at disrupting\nsuperconductivity in Nd-LSCO ($x$ = 0.125), and, by extension, in all systems\nexhibiting parallel stripes.",
            "author": [
                "Q. Chen",
                "S. H. -Y. Huang",
                "Q. Ma",
                "E. M. Smith",
                "H. Sharron",
                "A. A. Aczel",
                "W. Tian",
                "B. D. Gaulin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18218v1",
                "http://arxiv.org/pdf/2310.18218v1"
            ],
            "primary_category": "cond-mat.supr-con",
            "category": [
                "cond-mat.supr-con",
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18217v1",
            "title": "Runtime Resolution of Feature Interactions through Adaptive Requirement\n  Weakening",
            "updated": "2023-10-27T15:45:30Z",
            "published": "2023-10-27T15:45:30Z",
            "summary": "The feature interaction problem occurs when two or more independently\ndeveloped components interact with each other in unanticipated ways, resulting\nin undesirable system behaviors. Feature interaction problems remain a\nchallenge for emerging domains in cyber-physical systems (CPS), such as the\nInternet of Things and autonomous drones. Existing techniques for resolving\nfeature interactions take a \"winner-takes-all\" approach, where one out of the\nconflicting features is selected as the most desirable one, and the rest are\ndisabled. However, when multiple of the conflicting features fulfill important\nsystem requirements, being forced to select one of them can result in an\nundesirable system outcome. In this paper, we propose a new resolution approach\nthat allows all of the conflicting features to continue to partially fulfill\ntheir requirements during the resolution process. In particular, our approach\nleverages the idea of adaptive requirement weakening, which involves one or\nmore features temporarily weakening their level of performance in order to\nco-exist with the other features in a consistent manner. Given feature\nrequirements specified in Signal Temporal Logic (STL), we propose an automated\nmethod and a runtime architecture for automatically weakening the requirements\nto resolve a conflict. We demonstrate our approach through case studies on\nfeature interactions in autonomous drones.",
            "author": [
                "Simon Chu",
                "Emma Shedden",
                "Changjian Zhang",
                "R\u00f4mulo Meira-G\u00f3es",
                "Gabriel A. Moreno",
                "David Garlan",
                "Eunsuk Kang"
            ],
            "link": [
                "http://dx.doi.org/10.1109/SEAMS59076.2023.00025",
                "http://arxiv.org/abs/2310.18217v1",
                "http://arxiv.org/pdf/2310.18217v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.FL",
                "cs.LO",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18212v1",
            "title": "Robustness of Algorithms for Causal Structure Learning to Hyperparameter\n  Choice",
            "updated": "2023-10-27T15:34:08Z",
            "published": "2023-10-27T15:34:08Z",
            "summary": "Hyperparameters play a critical role in machine learning. Hyperparameter\ntuning can make the difference between state-of-the-art and poor prediction\nperformance for any algorithm, but it is particularly challenging for structure\nlearning due to its unsupervised nature. As a result, hyperparameter tuning is\noften neglected in favour of using the default values provided by a particular\nimplementation of an algorithm. While there have been numerous studies on\nperformance evaluation of causal discovery algorithms, how hyperparameters\naffect individual algorithms, as well as the choice of the best algorithm for a\nspecific problem, has not been studied in depth before. This work addresses\nthis gap by investigating the influence of hyperparameters on causal structure\nlearning tasks. Specifically, we perform an empirical evaluation of\nhyperparameter selection for some seminal learning algorithms on datasets of\nvarying levels of complexity. We find that, while the choice of algorithm\nremains crucial to obtaining state-of-the-art performance, hyperparameter\nselection in ensemble settings strongly influences the choice of algorithm, in\nthat a poor choice of hyperparameters can lead to analysts using algorithms\nwhich do not give state-of-the-art performance for their data.",
            "author": [
                "Damian Machlanski",
                "Spyridon Samothrakis",
                "Paul Clarke"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18212v1",
                "http://arxiv.org/pdf/2310.18212v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18210v1",
            "title": "Molecular clouds in M51 from high-resolution extinction mapping",
            "updated": "2023-10-27T15:32:20Z",
            "published": "2023-10-27T15:32:20Z",
            "summary": "Here we present the cloud population extracted from M51, following the\napplication of our new high-resolution dust extinction technique to the galaxy\n(Faustino Vieira et al. 2023). With this technique, we are able to image the\ngas content of the entire disc of M51 down to 5 pc (0.14\"), which allows us to\nperform a statistical characterisation of well-resolved molecular cloud\nproperties across different large-scale dynamical environments and with\ngalactocentric distance. We find that cloud growth is promoted in regions in\nthe galaxy where shear is minimised; i.e. clouds can grow into higher masses\n(and surface densities) inside the spiral arms and molecular ring. We do not\ndetect any enhancement of high-mass star formation towards regions favourable\nto cloud growth, indicating that massive and/or dense clouds are not the sole\ningredient for high-mass star formation. We find that in the spiral arms there\nis a significant decline of cloud surface densities with increasing\ngalactocentric radius, whilst in the inter-arm regions they remain relatively\nconstant. We also find that the surface density distribution for spiral arm\nclouds has two distinct behaviours in the inner and outer galaxy, with average\ncloud surface densities at larger galactocentric radii becoming similar to\ninter-arm clouds. We propose that the tidal interaction between M51 and its\ncompanion (NGC 5195) - which heavily affects the nature of the spiral structure\n- might be the main factor behind this.",
            "author": [
                "Helena Faustino Vieira",
                "Ana Duarte-Cabral",
                "Timothy A. Davis",
                "Nicolas Peretto",
                "Matthew W. L. Smith",
                "Miguel Querejeta",
                "Dario Colombo",
                "Michael Anderson"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18210v1",
                "http://arxiv.org/pdf/2310.18210v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18208v2",
            "title": "ArcheType: A Novel Framework for Open-Source Column Type Annotation\n  using Large Language Models",
            "updated": "2023-11-06T13:16:27Z",
            "published": "2023-10-27T15:31:22Z",
            "summary": "Existing deep-learning approaches to semantic column type annotation (CTA)\nhave important shortcomings: they rely on semantic types which are fixed at\ntraining time; require a large number of training samples per type and incur\nlarge run-time inference costs; and their performance can degrade when\nevaluated on novel datasets, even when types remain constant. Large language\nmodels have exhibited strong zero-shot classification performance on a wide\nrange of tasks and in this paper we explore their use for CTA. We introduce\nArcheType, a simple, practical method for context sampling, prompt\nserialization, model querying, and label remapping, which enables large\nlanguage models to solve CTA problems in a fully zero-shot manner. We ablate\neach component of our method separately, and establish that improvements to\ncontext sampling and label remapping provide the most consistent gains.\nArcheType establishes a new state-of-the-art performance on zero-shot CTA\nbenchmarks (including three new domain-specific benchmarks which we release\nalong with this paper), and when used in conjunction with classical CTA\ntechniques, it outperforms a SOTA DoDuo model on the fine-tuned SOTAB\nbenchmark. Our code is available at https://github.com/penfever/ArcheType.",
            "author": [
                "Benjamin Feuer",
                "Yurong Liu",
                "Chinmay Hegde",
                "Juliana Freire"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18208v2",
                "http://arxiv.org/pdf/2310.18208v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18207v1",
            "title": "INA: An Integrative Approach for Enhancing Negotiation Strategies with\n  Reward-Based Dialogue System",
            "updated": "2023-10-27T15:31:16Z",
            "published": "2023-10-27T15:31:16Z",
            "summary": "In this paper, we propose a novel negotiation dialogue agent designed for the\nonline marketplace. Our agent is integrative in nature i.e, it possesses the\ncapability to negotiate on price as well as other factors, such as the addition\nor removal of items from a deal bundle, thereby offering a more flexible and\ncomprehensive negotiation experience. We create a new dataset called\nIntegrative Negotiation Dataset (IND) to enable this functionality. For this\ndataset creation, we introduce a new semi-automated data creation method, which\ncombines defining negotiation intents, actions, and intent-action simulation\nbetween users and the agent to generate potential dialogue flows. Finally, the\nprompting of GPT-J, a state-of-the-art language model, is done to generate\ndialogues for a given intent, with a human-in-the-loop process for post-editing\nand refining minor errors to ensure high data quality. We employ a set of novel\nrewards, specifically tailored for the negotiation task to train our\nNegotiation Agent, termed as the Integrative Negotiation Agent (INA). These\nrewards incentivize the chatbot to learn effective negotiation strategies that\ncan adapt to various contextual requirements and price proposals. By leveraging\nthe IND, we train our model and conduct experiments to evaluate the\neffectiveness of our reward-based dialogue system for negotiation. Our results\ndemonstrate that the proposed approach and reward system significantly enhance\nthe agent's negotiation capabilities. The INA successfully engages in\nintegrative negotiations, displaying the ability to dynamically adjust prices\nand negotiate the inclusion or exclusion of items in a bundle deal",
            "author": [
                "Zishan Ahmad",
                "Suman Saurabh",
                "Vaishakh Sreekanth Menon",
                "Asif Ekbal",
                "Roshni Ramnani",
                "Anutosh Maitra"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18207v1",
                "http://arxiv.org/pdf/2310.18207v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18206v1",
            "title": "FLSH -- Friendly Library for the Simulation of Humans",
            "updated": "2023-10-27T15:29:38Z",
            "published": "2023-10-27T15:29:38Z",
            "summary": "Computer models of humans are ubiquitous throughout computer animation and\ncomputer vision. However, these models rarely represent the dynamics of human\nmotion, as this requires adding a complex layer that solves body motion in\nresponse to external interactions and according to the laws of physics. FLSH is\na library that facilitates this task for researchers and developers who are not\ninterested in the nuisances of physics simulation, but want to easily integrate\ndynamic humans in their applications. FLSH provides easy access to three\nflavors of body physics, with different features and computational complexity:\nskeletal dynamics, full soft-tissue dynamics, and reduced-order modeling of\nsoft-tissue dynamics. In all three cases, the simulation models are built on\ntop of the pseudo-standard SMPL parametric body model.",
            "author": [
                "Pablo Ram\u00f3n",
                "Cristian Romero",
                "Javier Tapia",
                "Miguel A. Otaduy"
            ],
            "link": [
                "http://dx.doi.org/10.15221/23.20",
                "http://arxiv.org/abs/2310.18206v1",
                "http://arxiv.org/pdf/2310.18206v1"
            ],
            "primary_category": "cs.GR",
            "category": [
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18205v1",
            "title": "Lost in Translation, Found in Spans: Identifying Claims in Multilingual\n  Social Media",
            "updated": "2023-10-27T15:28:12Z",
            "published": "2023-10-27T15:28:12Z",
            "summary": "Claim span identification (CSI) is an important step in fact-checking\npipelines, aiming to identify text segments that contain a checkworthy claim or\nassertion in a social media post. Despite its importance to journalists and\nhuman fact-checkers, it remains a severely understudied problem, and the scarce\nresearch on this topic so far has only focused on English. Here we aim to\nbridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K\nreal-world claims collected from numerous social media platforms in five Indian\nlanguages and English. We report strong baselines with state-of-the-art\nencoder-only language models (e.g., XLM-R) and we demonstrate the benefits of\ntraining on multiple languages over alternative cross-lingual transfer methods\nsuch as zero-shot transfer, or training on translated data, from a\nhigh-resource language such as English. We evaluate generative large language\nmodels from the GPT series using prompting methods on the X-CLAIM dataset and\nwe find that they underperform the smaller encoder-only language models for\nlow-resource languages.",
            "author": [
                "Shubham Mittal",
                "Megha Sundriyal",
                "Preslav Nakov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18205v1",
                "http://arxiv.org/pdf/2310.18205v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18204v1",
            "title": "Competing magnetic orders in a bilayer Hubbard model with ultracold\n  atoms",
            "updated": "2023-10-27T15:26:41Z",
            "published": "2023-10-27T15:26:41Z",
            "summary": "Fermionic atoms in optical lattices have served as a compelling model system\nto study and emulate the physics of strongly-correlated matter. Driven by the\nadvances of high-resolution microscopy, the recent focus of research has been\non two-dimensional systems in which several quantum phases, such as\nanti-ferromagnetic Mott insulators for repulsive interactions and\ncharge-density waves for attractive interactions have been observed. However,\nthe aspired emulations of real materials, such as bilayer graphene, have to\ntake into account that their lattice structure composes of coupled layers and\ntherefore is not strictly two-dimensional. In this work, we realize a bilayer\nFermi-Hubbard model using ultracold atoms in an optical lattice and demonstrate\nthat the interlayer coupling controls a crossover between a planar\nanti-ferromagnetically ordered Mott insulator and a band insulator of\nspin-singlets along the bonds between the layers. Our work will enable the\nexploration of further fascinating properties of coupled-layer Hubbard models,\nsuch as theoretically predicted superconducting pairing mechanisms.",
            "author": [
                "Marcell Gall",
                "Nicola Wurz",
                "Jens Samland",
                "Chun Fai Chan",
                "Michael K\u00f6hl"
            ],
            "link": [
                "http://dx.doi.org/10.1038/s41586-020-03058-x",
                "http://arxiv.org/abs/2310.18204v1",
                "http://arxiv.org/pdf/2310.18204v1"
            ],
            "primary_category": "cond-mat.quant-gas",
            "category": [
                "cond-mat.quant-gas"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18203v1",
            "title": "3D atomic structure from a single XFEL pulse",
            "updated": "2023-10-27T15:25:57Z",
            "published": "2023-10-27T15:25:57Z",
            "summary": "X-ray Free Electron Lasers (XFEL) are the most advanced pulsed x-ray sources.\nTheir extraordinary pulse parameters promise unique applications. Indeed,\nseveral new methods have been developed at XFEL-s. However, no methods are\nknown, which would allow ab initio atomic level structure determination using\nonly a single XFEL pulse. Here, we present experimental results, demonstrating\nthe determination of the 3D atomic structure from data obtained during a single\n25 fs XFEL pulse. Parallel measurement of hundreds of Bragg reflections was\ndone by collecting Kossel line patterns of GaAs and GaP. With these\nmeasurements, we reached the ultimate temporal limit of the x-ray structure\nsolution possible today. These measurements open the way for studying\nnon-repeatable fast processes and structural transformations in crystals for\nexample measuring the atomic structure of matter at extremely non-ambient\nconditions or transient structures formed in irreversible physical, chemical,\nor biological processes. It would also facilitate time resolved pump-probe\nstructural studies making them significantly shorter than traditional serial\ncrystallography.",
            "author": [
                "G. Bortel",
                "M. Tegze",
                "M. Sikorski",
                "R. Bean",
                "J. Bielecki",
                "C. Kim",
                "J. Koliyadu",
                "F. Koua",
                "M. Ramilli",
                "A. Round",
                "T. Sato",
                "D. Zabelskii",
                "G. Faigel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18203v1",
                "http://arxiv.org/pdf/2310.18203v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18200v1",
            "title": "Invariants of Vanishing Brauer Classes",
            "updated": "2023-10-27T15:20:04Z",
            "published": "2023-10-27T15:20:04Z",
            "summary": "A specialization of a K3 surface with Picard rank one to a K3 with rank two\ndefines a vanishing class of order two in the Brauer group of the general K3\nsurface. We give the B-field invariants of this class. We apply this to the K3\ndouble plane defined by a cubic fourfold with a plane. The specialization of\nsuch a cubic fourfold whose group of codimension two cycles has rank two to one\nwhich has rank three induces such a specialization of the double planes. We\ndetermine the Picard lattice of the specialized double plane as well as the\nvanishing Brauer class and its relation to the natural \"Clifford\" Brauer class.\nThis provides more insight in the specializations. It allows us to explicitly\ndetermine the K3 surfaces associated to infinitely many of the conjecturally\nrational cubic fourfolds obtained as such specializations.",
            "author": [
                "Federica Galluzzi",
                "Bert van Geemen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18200v1",
                "http://arxiv.org/pdf/2310.18200v1"
            ],
            "primary_category": "math.AG",
            "category": [
                "math.AG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18199v1",
            "title": "Relative Transfer Function Vector Estimation for Acoustic Sensor\n  Networks Exploiting Covariance Matrix Structure",
            "updated": "2023-10-27T15:18:20Z",
            "published": "2023-10-27T15:18:20Z",
            "summary": "In many multi-microphone algorithms for noise reduction, an estimate of the\nrelative transfer function (RTF) vector of the target speaker is required. The\nstate-of-the-art covariance whitening (CW) method estimates the RTF vector as\nthe principal eigenvector of the whitened noisy covariance matrix, where\nwhitening is performed using an estimate of the noise covariance matrix. In\nthis paper, we consider an acoustic sensor network consisting of multiple\nmicrophone nodes. Assuming uncorrelated noise between the nodes but not within\nthe nodes, we propose two RTF vector estimation methods that leverage the\nblock-diagonal structure of the noise covariance matrix. The first method\nmodifies the CW method by considering only the diagonal blocks of the estimated\nnoise covariance matrix. In contrast, the second method only considers the\noff-diagonal blocks of the noisy covariance matrix, but cannot be solved using\na simple eigenvalue decomposition. When applying the estimated RTF vector in a\nminimum variance distortionless response beamformer, simulation results for\nreal-world recordings in a reverberant environment with multiple noise sources\nshow that the modified CW method performs slightly better than the CW method in\nterms of SNR improvement, while the off-diagonal selection method outperforms a\nbiased RTF vector estimate obtained as the principal eigenvector of the noisy\ncovariance matrix.",
            "author": [
                "Wiebke Middelberg",
                "Henri Gode",
                "Simon Doclo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18199v1",
                "http://arxiv.org/pdf/2310.18199v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16124v1",
            "title": "DiffAttack: Evasion Attacks Against Diffusion-Based Adversarial\n  Purification",
            "updated": "2023-10-27T15:17:50Z",
            "published": "2023-10-27T15:17:50Z",
            "summary": "Diffusion-based purification defenses leverage diffusion models to remove\ncrafted perturbations of adversarial examples and achieve state-of-the-art\nrobustness. Recent studies show that even advanced attacks cannot break such\ndefenses effectively, since the purification process induces an extremely deep\ncomputational graph which poses the potential problem of gradient obfuscation,\nhigh memory cost, and unbounded randomness. In this paper, we propose a unified\nframework DiffAttack to perform effective and efficient attacks against\ndiffusion-based purification defenses, including both DDPM and score-based\napproaches. In particular, we propose a deviated-reconstruction loss at\nintermediate diffusion steps to induce inaccurate density gradient estimation\nto tackle the problem of vanishing/exploding gradients. We also provide a\nsegment-wise forwarding-backwarding algorithm, which leads to memory-efficient\ngradient backpropagation. We validate the attack effectiveness of DiffAttack\ncompared with existing adaptive attacks on CIFAR-10 and ImageNet. We show that\nDiffAttack decreases the robust accuracy of models compared with SOTA attacks\nby over 20% on CIFAR-10 under $\\ell_\\infty$ attack $(\\epsilon=8/255)$, and over\n10% on ImageNet under $\\ell_\\infty$ attack $(\\epsilon=4/255)$. We conduct a\nseries of ablations studies, and we find 1) DiffAttack with the\ndeviated-reconstruction loss added over uniformly sampled time steps is more\neffective than that added over only initial/final steps, and 2) diffusion-based\npurification with a moderate diffusion length is more robust under DiffAttack.",
            "author": [
                "Mintong Kang",
                "Dawn Song",
                "Bo Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16124v1",
                "http://arxiv.org/pdf/2311.16124v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18196v1",
            "title": "On the properties of X-ray corona in Seyfert 1 galaxies",
            "updated": "2023-10-27T15:13:25Z",
            "published": "2023-10-27T15:13:25Z",
            "summary": "We carried out a uniform and systematic analysis of a sample of 112 nearby\nbright Seyfert 1 type AGN, the observations of which were carried out by the\n{\\it Nuclear Spectroscopic Telescope Array (NuSTAR)} between August 2013 and\nMay 2022. The main goal of this analysis is to investigate the nature of the\nX-ray corona in Seyfert 1 galaxies. From the physical model that fits the {\\it\nNuSTAR} spectra, we could constrain the high energy cut-off ($\\rm{E_{cut}}$)\nfor 73 sources in our sample. For those 73 sources, we fitted the\nComptonization model to estimate the temperature ($\\rm{kT_{e}}$) of their\ncorona. $\\rm{kT_{e}}$ could be constrained in 42 sources. We investigated for\npossible correlations between various properties of the corona obtained from\nphysical model fits to the observed spectra and between various coronal\nparameters and physical properties of the sources such as Eddington ratio and\nblack hole mass. We found (a) a strong correlation between $\\rm{E_{cut}}$ and\nthe photon index and (b) a significant negative correlation between\n$\\rm{kT_{e}}$ and the optical depth.",
            "author": [
                "Indrani Pal",
                "Anju A.",
                "H. Sreehari",
                "Gitika Rameshan",
                "C. S. Stalin",
                "Claudio Ricci",
                "Stefano Marchesi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18196v1",
                "http://arxiv.org/pdf/2310.18196v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18195v1",
            "title": "Nature of the photo-induced metallic state in monoclinic VO$_2$",
            "updated": "2023-10-27T15:08:53Z",
            "published": "2023-10-27T15:08:53Z",
            "summary": "The metal-insulator transition of VO$_2$, which in equilibrium is associated\nwith a structural phase transition, has been intensively studied for decades.\nIn particular, it is challenging to disentangle the role of Mott physics from\ndimerization effects in the insulating phase. Femtosecond time-resolved\nexperiments showed that optical excitations can induce a transient metallic\nstate in the dimerized phase, which is distinct from the known equilibrium\nphases. In this study, we combine non-equilibrium cluster dynamical mean-field\ntheory with realistic first principles modeling to clarify the nature of this\nlaser-induced metallic state. We show that the doublon-holon production by\nlaser pulses with polarization along the V-V dimers and the subsequent\ninter-orbital reshuffling of the photo-carriers leads to a population of\norbital-mixed states and the filling of the gap. The photo-induced metal state\nis qualitatively similar to a hot electronic state in the dimerized structure,\nand does not involve a collapse of the Mott gap.",
            "author": [
                "Jiyu Chen",
                "Francesco Petocchi",
                "Viktor Christiansson",
                "Philipp Werner"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18195v1",
                "http://arxiv.org/pdf/2310.18195v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18192v1",
            "title": "Artifact-Robust Graph-Based Learning in Digital Pathology",
            "updated": "2023-10-27T15:06:01Z",
            "published": "2023-10-27T15:06:01Z",
            "summary": "Whole slide images~(WSIs) are digitized images of tissues placed in glass\nslides using advanced scanners. The digital processing of WSIs is challenging\nas they are gigapixel images and stored in multi-resolution format. A common\nchallenge with WSIs is that perturbations/artifacts are inevitable during\nstoring the glass slides and digitizing them. These perturbations include\nmotion, which often arises from slide movement during placement, and changes in\nhue and brightness due to variations in staining chemicals and the quality of\ndigitizing scanners. In this work, a novel robust learning approach to account\nfor these artifacts is presented. Due to the size and resolution of WSIs and to\naccount for neighborhood information, graph-based methods are called for. We\nuse graph convolutional network~(GCN) to extract features from the graph\nrepresenting WSI. Through a denoiser {and pooling layer}, the effects of\nperturbations in WSIs are controlled and the output is followed by a\ntransformer for the classification of different grades of prostate cancer. To\ncompare the efficacy of the proposed approach, the model without denoiser is\ntrained and tested with WSIs without any perturbation and then different\nperturbations are introduced in WSIs and passed through the network with the\ndenoiser. The accuracy and kappa scores of the proposed model with prostate\ncancer dataset compared with non-robust algorithms show significant improvement\nin cancer diagnosis.",
            "author": [
                "Saba Heidari Gheshlaghi",
                "Milan Aryal",
                "Nasim Yahyasoltani",
                "Masoud Ganji"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18192v1",
                "http://arxiv.org/pdf/2310.18192v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18190v1",
            "title": "Photo-dynamics of quantum emitters in aluminum nitride",
            "updated": "2023-10-27T15:02:53Z",
            "published": "2023-10-27T15:02:53Z",
            "summary": "Aluminum nitride is a technologically important wide bandgap semiconductor\nwhich has been shown to host bright quantum emitters. In this paper, we probe\nthe photodynamics of quantum emitters in aluminum nitride using photon emission\ncorrelations and time-resolved spectroscopy. We identify that each emitter\ncontains as many as 6 internal energy levels with distinct laser\npower-dependent behaviors. Power-dependent shelving and de-shelving processes,\nsuch as optically induced ionization and recombination are considered,\nindicating complex optical dynamics associated with the spontaneous and\noptically pumped transitions. State population dynamics simulations\nqualitatively explain the temporal behaviours of the quantum emitters,\nrevealing that those with pump-dependent de-shelving processes can saturate at\nsignificantly higher intensities, resulting in bright room-temperature quantum\nlight emission.",
            "author": [
                "Yanzhao Guo",
                "John P. Hadden",
                "Rachel N. Clark",
                "Samuel G. Bishop",
                "Anthony J. Bennett"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18190v1",
                "http://arxiv.org/pdf/2310.18190v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18188v1",
            "title": "Discovery of the Zintl-phosphide BaCd$_{2}$P$_{2}$ as a long carrier\n  lifetime and stable solar absorber",
            "updated": "2023-10-27T15:01:23Z",
            "published": "2023-10-27T15:01:23Z",
            "summary": "Thin-film photovoltaics offers a path to significantly decarbonize our energy\nproduction. Unfortunately, current materials commercialized or under\ndevelopment as thin-film solar cell absorbers are far from optimal as they show\neither low power conversion efficiency or issues with earth-abundance and\nstability. Entirely new and disruptive materials platforms are rarely\ndiscovered as the search for new solar absorbers is traditionally slow and\nserendipitous. Here, we use first principles high-throughput screening to\naccelerate this process. We identify new solar absorbers among known inorganic\ncompounds using considerations on band gap, carrier transport, optical\nabsorption but also on intrinsic defects which can strongly limit the carrier\nlifetime and ultimately the solar cell efficiency. Screening about 40,000\nmaterials, we discover the Zintl-phosphide BaCd$_{2}$P$_{2}$ as a potential\nhigh-efficiency solar absorber. Follow-up experimental work confirms the\npredicted promises of BaCd$_{2}$P$_{2}$ highlighting an optimal band gap for\nvisible absorption, bright photoluminescence, and long carrier lifetime of up\nto 30 ns even for unoptimized powder samples. Importantly, BaCd$_{2}$P$_{2}$\ndoes not contain any critical elements and is highly stable in air and water.\nOur work opens an avenue for a new family of stable, earth-abundant,\nhigh-performance Zintl-based solar absorbers. It also demonstrates how recent\nadvances in first principles computation can accelerate the search of\nphotovoltaic materials by combining high-throughput screening with experiment.",
            "author": [
                "Zhenkun Yuan",
                "Diana Dahliah",
                "Muhammad Rubaiat Hasan",
                "Gideon Kassa",
                "Andrew Pike",
                "Shaham Quadir",
                "Romain Claes",
                "Cierra Chandler",
                "Yihuang Xiong",
                "Victoria Kyveryga",
                "Philip Yox",
                "Gian-Marco Rignanese",
                "Ismaila Dabo",
                "Andriy Zakutayev",
                "David P. Fenning",
                "Obadiah G. Reid",
                "Sage Bauers",
                "Jifeng Liu",
                "Kirill Kovnir",
                "Geoffroy Hautier"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18188v1",
                "http://arxiv.org/pdf/2310.18188v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18186v1",
            "title": "Model-free Posterior Sampling via Learning Rate Randomization",
            "updated": "2023-10-27T14:59:44Z",
            "published": "2023-10-27T14:59:44Z",
            "summary": "In this paper, we introduce Randomized Q-learning (RandQL), a novel\nrandomized model-free algorithm for regret minimization in episodic Markov\nDecision Processes (MDPs). To the best of our knowledge, RandQL is the first\ntractable model-free posterior sampling-based algorithm. We analyze the\nperformance of RandQL in both tabular and non-tabular metric space settings. In\ntabular MDPs, RandQL achieves a regret bound of order\n$\\widetilde{\\mathcal{O}}(\\sqrt{H^{5}SAT})$, where $H$ is the planning horizon,\n$S$ is the number of states, $A$ is the number of actions, and $T$ is the\nnumber of episodes. For a metric state-action space, RandQL enjoys a regret\nbound of order $\\widetilde{\\mathcal{O}}(H^{5/2} T^{(d_z+1)/(d_z+2)})$, where\n$d_z$ denotes the zooming dimension. Notably, RandQL achieves optimistic\nexploration without using bonuses, relying instead on a novel idea of learning\nrate randomization. Our empirical study shows that RandQL outperforms existing\napproaches on baseline exploration environments.",
            "author": [
                "Daniil Tiapkin",
                "Denis Belomestny",
                "Daniele Calandriello",
                "Eric Moulines",
                "Remi Munos",
                "Alexey Naumov",
                "Pierre Perrault",
                "Michal Valko",
                "Pierre Menard"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18186v1",
                "http://arxiv.org/pdf/2310.18186v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18185v1",
            "title": "Cooperative quantum tunneling of the magnetization in Fe-doped Li$_3$N",
            "updated": "2023-10-27T14:59:42Z",
            "published": "2023-10-27T14:59:42Z",
            "summary": "The spin-reversal in dilute Li$_2$(Li$_{1-x}$Fe$_{x}$)N with $x < 1$ % is\ndominated by resonant quantum tunneling of spatially well-separated states. We\nreport on the effect of finite couplings between those states that give rise to\ncooperative, simultaneous quantum tunneling of two spins. This phenomenon,\nknown as spin-spin cross relaxation, effectively elucidates the fine-structure\nobserved in isothermal magnetization loops, a previously unresolved aspect.\nTemperature and field-dependent magnetization measurements were conducted over\na range from T = 2 K to 300 K in applied fields of up to $\\mu_0H$ = 7 T.\nMagnetic dipole fields are computed numerically. Our findings affirm the\nabsence of stoichiometric defects in Li$_2$(Li$_{1-x}$Fe$_{x}$)N and underscore\nits exemplary suitability as a model system for investigating spin-reversal\nprocesses at the microscopic level. This is attributed to its comparatively\nsimple crystal structure, the availability of large single crystals, elevated\ncharacteristic energies, and well-defined energy levels",
            "author": [
                "M. Fix",
                "A. Jesche"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18185v1",
                "http://arxiv.org/pdf/2310.18185v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.04224v1",
            "title": "MELEP: A Novel Predictive Measure of Transferability in Multi-Label ECG\n  Analysis",
            "updated": "2023-10-27T14:57:10Z",
            "published": "2023-10-27T14:57:10Z",
            "summary": "We introduce MELEP, which stands for Muti-label Expected Log of Empirical\nPredictions, a novel measure to estimate how effective it is to transfer\nknowledge from a pre-trained model to a downstream task in a multi-label\nsettings. The measure is generic to work with new target data having a\ndifferent label set from source data. It is also computationally efficient,\nonly requires forward passing the downstream dataset through the pre-trained\nmodel once. To the best of our knowledge, we are the first to develop such a\ntransferability metric for multi-label ECG classification problems. Our\nexperiments show that MELEP can predict the performance of pre-trained\nconvolutional and recurrent deep neural networks, on small and imbalanced ECG\ndata. Specifically, strong correlation coefficients, with absolute values\nexceeding 0.6 in most cases, were observed between MELEP and the actual average\nF1 scores of the fine-tuned models.",
            "author": [
                "Cuong V. Nguyen",
                "Hieu Minh Duong",
                "Cuong D. Do"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04224v1",
                "http://arxiv.org/pdf/2311.04224v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18181v1",
            "title": "An Energy-Efficient Near-Data Processing Accelerator for DNNs that\n  Optimizes Data Accesses",
            "updated": "2023-10-27T14:49:47Z",
            "published": "2023-10-27T14:49:47Z",
            "summary": "The constant growth of DNNs makes them challenging to implement and run\nefficiently on traditional compute-centric architectures. Some accelerators\nhave attempted to add more compute units and on-chip buffers to solve the\nmemory wall problem without much success, and sometimes even worsening the\nissue since more compute units also require higher memory bandwidth. Prior\nworks have proposed the design of memory-centric architectures based on the\nNear-Data Processing (NDP) paradigm. NDP seeks to break the memory wall by\nmoving the computations closer to the memory hierarchy, reducing the data\nmovements and their cost as much as possible. The 3D-stacked memory is\nespecially appealing for DNN accelerators due to its high-density/low-energy\nstorage and near-memory computation capabilities to perform the DNN operations\nmassively in parallel. However, memory accesses remain as the main bottleneck\nfor running modern DNNs efficiently.\n  To improve the efficiency of DNN inference we present QeiHaN, a hardware\naccelerator that implements a 3D-stacked memory-centric weight storage scheme\nto take advantage of a logarithmic quantization of activations. In particular,\nsince activations of FC and CONV layers of modern DNNs are commonly represented\nas powers of two with negative exponents, QeiHaN performs an implicit in-memory\nbit-shifting of the DNN weights to reduce memory activity. Only the meaningful\nbits of the weights required for the bit-shift operation are accessed. Overall,\nQeiHaN reduces memory accesses by 25\\% compared to a standard memory\norganization. We evaluate QeiHaN on a popular set of DNNs. On average, QeiHaN\nprovides $4.3x$ speedup and $3.5x$ energy savings over a Neurocube-like\naccelerator.",
            "author": [
                "Bahareh Khabbazan",
                "Marc Riera",
                "Antonio Gonz\u00e1lez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18181v1",
                "http://arxiv.org/pdf/2310.18181v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18180v1",
            "title": "DPSS-based Codebook Design for Near-Field XL-MIMO Channel Estimation",
            "updated": "2023-10-27T14:49:12Z",
            "published": "2023-10-27T14:49:12Z",
            "summary": "Future sixth-generation (6G) systems are expected to leverage extremely\nlarge-scale multiple-input multiple-output (XL-MIMO) technology, which\nsignificantly expands the range of the near-field region. While accurate\nchannel estimation is essential for beamforming and data detection, the unique\ncharacteristics of near-field channels pose additional challenges to the\neffective acquisition of channel state information. In this paper, we propose a\nnovel codebook design, which allows efficient near-field channel estimation\nwith significantly reduced codebook size. Specifically, we consider the\neigen-problem based on the near-field electromagnetic wave transmission model.\nMoreover, we derive the general form of the eigenvectors associated with the\nnear-field channel matrix, revealing their noteworthy connection to the\ndiscrete prolate spheroidal sequence (DPSS). Based on the proposed near-field\ncodebook design, we further introduce a two-step channel estimation scheme.\nSimulation results demonstrate that the proposed codebook design not only\nachieves superior sparsification performance of near-field channels with a\nlower leakage effect, but also significantly improves the accuracy in\ncompressive sensing channel estimation.",
            "author": [
                "Shicong Liu",
                "Xianghao Yu",
                "Zhen Gao",
                "Derrick Wing Kwan Ng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18180v1",
                "http://arxiv.org/pdf/2310.18180v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18178v1",
            "title": "Deep3DSketch+\\+: High-Fidelity 3D Modeling from Single Free-hand\n  Sketches",
            "updated": "2023-10-27T14:45:19Z",
            "published": "2023-10-27T14:45:19Z",
            "summary": "The rise of AR/VR has led to an increased demand for 3D content. However, the\ntraditional method of creating 3D content using Computer-Aided Design (CAD) is\na labor-intensive and skill-demanding process, making it difficult to use for\nnovice users. Sketch-based 3D modeling provides a promising solution by\nleveraging the intuitive nature of human-computer interaction. However,\ngenerating high-quality content that accurately reflects the creator's ideas\ncan be challenging due to the sparsity and ambiguity of sketches. Furthermore,\nnovice users often find it challenging to create accurate drawings from\nmultiple perspectives or follow step-by-step instructions in existing methods.\nTo address this, we introduce a groundbreaking end-to-end approach in our work,\nenabling 3D modeling from a single free-hand sketch,\nDeep3DSketch+$\\backslash$+. The issue of sparsity and ambiguity using single\nsketch is resolved in our approach by leveraging the symmetry prior and\nstructural-aware shape discriminator. We conducted comprehensive experiments on\ndiverse datasets, including both synthetic and real data, to validate the\nefficacy of our approach and demonstrate its state-of-the-art (SOTA)\nperformance. Users are also more satisfied with results generated by our\napproach according to our user study. We believe our approach has the potential\nto revolutionize the process of 3D modeling by offering an intuitive and\neasy-to-use solution for novice users.",
            "author": [
                "Ying Zang",
                "Chaotao Ding",
                "Tianrun Chen",
                "Papa Mao",
                "Wenjun Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18178v1",
                "http://arxiv.org/pdf/2310.18178v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18173v2",
            "title": "Brochosome-inspired binary metastructures for pixel-by-pixel thermal\n  signature control",
            "updated": "2023-11-01T17:51:15Z",
            "published": "2023-10-27T14:40:46Z",
            "summary": "In nature, nano/micro-structured materials are adopted by various species to\ngenerate colors or achieve camouflage. Here, inspired by leafhopper-generated\nbrochosomes, we design binary metastructures composed of pixel twins to achieve\npixelated thermal signature control at the microscale. In the infrared range,\nthe pixel twins exhibit distinct emissivities, creating thermal counterparts of\n'0-1' binary states for storing and displaying information. In the visible\nrange, the engineered surface morphology of the pixel twins enables similar\nscattering behaviors, rendering the pixel twins visually indistinguishable,\nwhich achieves the camouflage of stored information. Compared to the previous\nwork based on plasmonic holographic metasurfaces, the brochosome-like pixel\ntwins are thermally driven, and their structure-enabled functions do not rely\non permittivities of any specific material. The unique combination of visible\ncamouflage and infrared display provides a systemic solution to the spatial\ncontrol of thermal signatures, and has important implications for optical\nsecurity, anticounterfeiting, and data encryption.",
            "author": [
                "Zhuo Li",
                "Lin Wang",
                "Xiu Liu",
                "Jiayu Li",
                "Hyeong Seok Yun",
                "Zexiao Wang",
                "Xu Zhang",
                "Tak-Sing Wong",
                "Sheng Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18173v2",
                "http://arxiv.org/pdf/2310.18173v2"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "physics.app-ph",
                "physics.bio-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.00718v1",
            "title": "Chat GPT Integrated with Voice Assistant as Learning Oral Chat-based\n  Constructive Communication to Improve Communicative Competence for EFL\n  earners",
            "updated": "2023-10-27T14:29:36Z",
            "published": "2023-10-27T14:29:36Z",
            "summary": "Chat GPT belongs to the category of Generative Pre-trained Transformer (GPT)\nlanguage models, which have received specialized training to produce text based\non natural language inputs. Its purpose is to imitate human-like conversation\nand can be implemented in multiple applications, such as chatbots, virtual\nassistants, and language translation systems, starting with an introduction to\nthe new trends and differences between artificial intelligence, machine\nlearning, and artificial neural networks, and highlighting the rigorous\nlanguage logic and powerful text generation capabilities of Chat GPT. This\npaper delves into how advances in artificial intelligence will shape e-learning\nin the coming decades, particularly in terms of Chat- GPT's ability to improve\nlearners' Communicative Competence when English is a second language. The\ncombination of new trends in artificial intelligence, mainly in the particular\ncase of English as a second language, and, at the academic level, chatbot\ntechnology, will be the next step in the replacement of the human academic\ncommunity by virtual assistants, apparently until a certain point. Despite the\ncontroversy, this very innovative solution will be able to bridge the gap\nbetween technology and education. Moreover, such innovative practices\nfacilitate communication by enabling its inclusion in various applications,\nincluding virtual assistants, chatbots, and language education. Keyword: Chat\nGPT, artificial intelligence, Communicative Competence, Communicative Language\nTeaching (CLT)",
            "author": [
                "Wei Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00718v1",
                "http://arxiv.org/pdf/2311.00718v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18169v1",
            "title": "Style Description based Text-to-Speech with Conditional Prosodic Layer\n  Normalization based Diffusion GAN",
            "updated": "2023-10-27T14:28:41Z",
            "published": "2023-10-27T14:28:41Z",
            "summary": "In this paper, we present a Diffusion GAN based approach (Prosodic Diff-TTS)\nto generate the corresponding high-fidelity speech based on the style\ndescription and content text as an input to generate speech samples within only\n4 denoising steps. It leverages the novel conditional prosodic layer\nnormalization to incorporate the style embeddings into the multi head attention\nbased phoneme encoder and mel spectrogram decoder based generator architecture\nto generate the speech. The style embedding is generated by fine tuning the\npretrained BERT model on auxiliary tasks such as pitch, speaking speed,\nemotion,gender classifications. We demonstrate the efficacy of our proposed\narchitecture on multi-speaker LibriTTS and PromptSpeech datasets, using\nmultiple quantitative metrics that measure generated accuracy and MOS.",
            "author": [
                "Neeraj Kumar",
                "Ankur Narang",
                "Brejesh Lall"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18169v1",
                "http://arxiv.org/pdf/2310.18169v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.CL",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18168v3",
            "title": "Personas as a Way to Model Truthfulness in Language Models",
            "updated": "2023-11-21T09:19:03Z",
            "published": "2023-10-27T14:27:43Z",
            "summary": "Large Language Models (LLMs) are trained on vast amounts of text from the\ninternet, which contains both factual and misleading information about the\nworld. Can language models discern truth from falsehood in this contradicting\ndata? Expanding on the view that LLMs can model different communicative agents,\nwe present the persona hypothesis: LLMs can cluster agents into personas using\ncommon features of their generations. For instance, a truthful persona is a\ngroup of agents that are likely to produce truthful text and that share similar\nfeatures like formal writing styles and scientific references. By modeling this\npersona, LLMs can generalize truthfulness beyond the specific contexts in which\neach agent generated the training text. For example, the model can infer that\nthe agent ``Wikipedia'' will behave truthfully on topics that were only\ngenerated by ``Science'' because they both belong to the truthful persona. We\nshow evidence for the persona hypothesis via two observations: (1) we can probe\nwhether a model's answer will be truthful before it is generated; (2)\nfinetuning a model on a set of facts improves its truthfulness on unseen\ntopics. Next, using arithmetics as a synthetic environment, we show that\nlanguage models can separate true and false statements, and generalize\ntruthfulness across agents; but only if agents in the training data share a\ntruthful generative process that enables the creation of a truthful persona.\nOverall, our findings suggest that models can exploit hierarchical structures\nin the data to learn abstract concepts like truthfulness.",
            "author": [
                "Nitish Joshi",
                "Javier Rando",
                "Abulhair Saparov",
                "Najoung Kim",
                "He He"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18168v3",
                "http://arxiv.org/pdf/2310.18168v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18167v1",
            "title": "MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading\n  Comprehension",
            "updated": "2023-10-27T14:24:06Z",
            "published": "2023-10-27T14:24:06Z",
            "summary": "The large language models have achieved superior performance on various\nnatural language tasks. One major drawback of such approaches is they are\nresource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a\nresource-efficient solution to fine-tune the pre-trained language models (PLMs)\nwhile keeping their weight frozen. Existing soft prompt methods mainly focus on\ndesigning the input-independent prompts that steer the model to fit the domain\nof the new dataset. Those methods often ignore the fine-grained information\nabout the task and context of the text. In this paper, we propose a multi-level\nprompt tuning (MPrompt) method for machine reading comprehension. It utilizes\nprompts at task-specific, domain-specific, and context-specific levels to\nenhance the comprehension of input semantics at different granularities. We\nalso propose an independence constraint to steer each domain-specific prompt to\nfocus on information within its domain to avoid redundancy. Moreover, we\npresent a prompt generator that incorporates context-related knowledge in the\nprompt generation to enhance contextual relevancy. We conducted extensive\nexperiments on 12 benchmarks of various QA formats and achieved an average\nimprovement of 1.94\\% over the state-of-the-art methods.",
            "author": [
                "Guoxin Chen",
                "Yiming Qian",
                "Bowen Wang",
                "Liangzhi Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18167v1",
                "http://arxiv.org/pdf/2310.18167v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18166v1",
            "title": "Functional Ownership through Fractional Uniqueness",
            "updated": "2023-10-27T14:22:00Z",
            "published": "2023-10-27T14:22:00Z",
            "summary": "Ownership and borrowing systems, designed to enforce safe memory management\nwithout the need for garbage collection, have been brought to the fore by the\nRust programming language. Rust also aims to bring some guarantees offered by\nfunctional programming into the realm of performant systems code, but the type\nsystem is largely separate from the ownership model, with type and borrow\nchecking happening in separate compilation phases. Recent models such as\nRustBelt and Oxide aim to formalise Rust in depth, but there is less focus on\nintegrating the basic ideas into more traditional type systems. An approach\ndesigned to expose an essential core for ownership and borrowing would open the\ndoor for functional languages to borrow concepts found in Rust and other\nownership frameworks, so that more programmers can enjoy their benefits.\n  One strategy for managing memory in a functional setting is through\nuniqueness types, but these offer a coarse-grained view: either a value has\nexactly one reference, and can be mutated safely, or it cannot, since other\nreferences may exist. Recent work demonstrates that linear and uniqueness types\ncan be combined in a single system to offer restrictions on program behaviour\nand guarantees about memory usage. We develop this connection further, showing\nthat just as graded type systems like those of Granule and Idris generalise\nlinearity, Rust's ownership model arises as a graded generalisation of\nuniqueness. We combine fractional permissions with grading to give the first\naccount of ownership and borrowing that smoothly integrates into a standard\ntype system alongside linearity and graded types, and extend Granule\naccordingly with these ideas.",
            "author": [
                "Daniel Marshall",
                "Dominic Orchard"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18166v1",
                "http://arxiv.org/pdf/2310.18166v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18165v1",
            "title": "Enhancing Enterprise Network Security: Comparing Machine-Level and\n  Process-Level Analysis for Dynamic Malware Detection",
            "updated": "2023-10-27T14:17:35Z",
            "published": "2023-10-27T14:17:35Z",
            "summary": "Analysing malware is important to understand how malicious software works and\nto develop appropriate detection and prevention methods. Dynamic analysis can\novercome evasion techniques commonly used to bypass static analysis and provide\ninsights into malware runtime activities. Much research on dynamic analysis\nfocused on investigating machine-level information (e.g., CPU, memory, network\nusage) to identify whether a machine is running malicious activities. A\nmalicious machine does not necessarily mean all running processes on the\nmachine are also malicious. If we can isolate the malicious process instead of\nisolating the whole machine, we could kill the malicious process, and the\nmachine can keep doing its job. Another challenge dynamic malware detection\nresearch faces is that the samples are executed in one machine without any\nbackground applications running. It is unrealistic as a computer typically runs\nmany benign (background) applications when a malware incident happens. Our\nexperiment with machine-level data shows that the existence of background\napplications decreases previous state-of-the-art accuracy by about 20.12% on\naverage. We also proposed a process-level Recurrent Neural Network (RNN)-based\ndetection model. Our proposed model performs better than the machine-level\ndetection model; 0.049 increase in detection rate and a false-positive rate\nbelow 0.1.",
            "author": [
                "Baskoro Adi Pratomo",
                "Toby Jackson",
                "Pete Burnap",
                "Andrew Hood",
                "Eirini Anthi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18165v1",
                "http://arxiv.org/pdf/2310.18165v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18164v1",
            "title": "Optimality of a refraction strategy in the optimal dividends problem\n  with absolutely continuous controls subject to Parisian ruin",
            "updated": "2023-10-27T14:17:33Z",
            "published": "2023-10-27T14:17:33Z",
            "summary": "We consider de Finetti's optimal dividends problem with absolutely continuous\nstrategies in a spectrally negative L\\'evy model with Parisian ruin as the\ntermination time. The problem considered is essentially a generalization of\nboth the control problems considered by Kyprianou, Loeffen & P\\'erez (2012) and\nby Renaud (2019). Using the language of scale functions for Parisian\nfluctuation theory, and under the assumption that the density of the L\\'evy\nmeasure is completely monotone, we prove that a refraction dividend strategy is\noptimal and we characterize the optimal threshold. In particular, we study the\neffect of the rate of Parisian implementation delays on this optimal threshold.",
            "author": [
                "F\u00e9lix Locas",
                "Jean-Fran\u00e7ois Renaud"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18164v1",
                "http://arxiv.org/pdf/2310.18164v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.03374v1",
            "title": "Generative AI for Software Metadata: Overview of the Information\n  Retrieval in Software Engineering Track at FIRE 2023",
            "updated": "2023-10-27T14:13:23Z",
            "published": "2023-10-27T14:13:23Z",
            "summary": "The Information Retrieval in Software Engineering (IRSE) track aims to\ndevelop solutions for automated evaluation of code comments in a machine\nlearning framework based on human and large language model generated labels. In\nthis track, there is a binary classification task to classify comments as\nuseful and not useful. The dataset consists of 9048 code comments and\nsurrounding code snippet pairs extracted from open source github C based\nprojects and an additional dataset generated individually by teams using large\nlanguage models. Overall 56 experiments have been submitted by 17 teams from\nvarious universities and software companies. The submissions have been\nevaluated quantitatively using the F1-Score and qualitatively based on the type\nof features developed, the supervised learning model used and their\ncorresponding hyper-parameters. The labels generated from large language models\nincrease the bias in the prediction model but lead to less over-fitted results.",
            "author": [
                "Srijoni Majumdar",
                "Soumen Paul",
                "Debjyoti Paul",
                "Ayan Bandyopadhyay",
                "Samiran Chattopadhyay",
                "Partha Pratim Das",
                "Paul D Clough",
                "Prasenjit Majumder"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03374v1",
                "http://arxiv.org/pdf/2311.03374v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.AI",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.04223v1",
            "title": "Dual band wireless transmission over 75-150GHz millimeter wave carriers\n  using frequency-locked laser pairs",
            "updated": "2023-10-27T14:12:38Z",
            "published": "2023-10-27T14:12:38Z",
            "summary": "We generate and transmit 75-GHz-bandwidth OFDM signals over the air using\nthree mutually frequency-locked lasers, achieving minimal frequency gap between\nthe wireless W and D bands using optical-assisted approaches, resulting in\n173.5 Gb/s detected capacity.",
            "author": [
                "Zichuan Zhou",
                "Amany Kassem",
                "James Seddon",
                "Eric Sillekens",
                "Izzat Darwazeh",
                "Polina Bayvel",
                "Zhixin Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04223v1",
                "http://arxiv.org/pdf/2311.04223v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.SY",
                "eess.SY",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18161v1",
            "title": "Lens Absorber Coupled MKIDs for Far Infrared Imaging Spectroscopy",
            "updated": "2023-10-27T14:10:55Z",
            "published": "2023-10-27T14:10:55Z",
            "summary": "Future generation of astronomical imaging spectrometers are targeting the far\ninfrared wavelengths to close the THz astronomy gap. Similar to lens antenna\ncoupled Microwave Kinetic Inductance Detectors (MKIDs), lens absorber coupled\nMKIDs are a candidate for highly sensitive large format detector arrays.\nHowever, the latter is more robust to misalignment and assembly issues at THz\nfrequencies due to its incoherent detection mechanism while requiring a less\ncomplex fabrication process. In this work, the performance of such detectors is\ninvestigated. The fabrication and sensitivity measurement of several lens\nabsorber coupled MKID array prototypes operating at 6.98 and 12 THz central\nfrequencies is on-going.",
            "author": [
                "Shahab O. Dabironezare",
                "Sven van Berkel",
                "Pierre M. Echternach",
                "Peter K. Day",
                "Charles M. Bradford",
                "Jochem J. A. Baselmans"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18161v1",
                "http://arxiv.org/pdf/2310.18161v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18155v1",
            "title": "Elevating Code-mixed Text Handling through Auditory Information of Words",
            "updated": "2023-10-27T14:03:30Z",
            "published": "2023-10-27T14:03:30Z",
            "summary": "With the growing popularity of code-mixed data, there is an increasing need\nfor better handling of this type of data, which poses a number of challenges,\nsuch as dealing with spelling variations, multiple languages, different\nscripts, and a lack of resources. Current language models face difficulty in\neffectively handling code-mixed data as they primarily focus on the semantic\nrepresentation of words and ignore the auditory phonetic features. This leads\nto difficulties in handling spelling variations in code-mixed text. In this\npaper, we propose an effective approach for creating language models for\nhandling code-mixed textual data using auditory information of words from\nSOUNDEX. Our approach includes a pre-training step based on\nmasked-language-modelling, which includes SOUNDEX representations (SAMLM) and a\nnew method of providing input data to the pre-trained model. Through\nexperimentation on various code-mixed datasets (of different languages) for\nsentiment, offensive and aggression classification tasks, we establish that our\nnovel language modeling approach (SAMLM) results in improved robustness towards\nadversarial attacks on code-mixed classification tasks. Additionally, our SAMLM\nbased approach also results in better classification results over the popular\nbaselines for code-mixed tasks. We use the explainability technique, SHAP\n(SHapley Additive exPlanations) to explain how the auditory features\nincorporated through SAMLM assist the model to handle the code-mixed text\neffectively and increase robustness against adversarial attacks\n\\footnote{Source code has been made available on\n\\url{https://github.com/20118/DefenseWithPhonetics},\n\\url{https://www.iitp.ac.in/~ai-nlp-ml/resources.html\\#Phonetics}}.",
            "author": [
                "Mamta",
                "Zishan Ahmad",
                "Asif Ekbal"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18155v1",
                "http://arxiv.org/pdf/2310.18155v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18154v1",
            "title": "Reaching high accuracy for energetic properties at second-order\n  perturbation cost by merging self-consistency and spin-opposite scaling",
            "updated": "2023-10-27T14:02:26Z",
            "published": "2023-10-27T14:02:26Z",
            "summary": "Quantum chemical methods dealing with challenging systems while retaining low\ncomputational costs have attracted attention. In particular, many efforts have\nbeen devoted to developing new methods based on the second-order perturbation\nthat may be the simplest correlated method beyond Hartree-Fock. We have\nrecently developed a self-consistent perturbation theory named one-body\nM{\\o}ller-Plesset second-order perturbation theory (OBMP2) and shown that it\ncan resolve issues caused by the non-iterative nature of standard perturbation\ntheory. In the present work, we extend the method by introducing the\nspin-opposite scaling to the double-excitation amplitudes, resulting in the\nO2BMP2 method. We assess the O2BMP2 performance on the triple-bond N2\ndissociation, singlet-triplet gaps, and ionization potentials. O2BMP2 performs\nmuch better than standard MP2 and reaches the accuracy of coupled-cluster\nmethods in all cases considered in this work.",
            "author": [
                "Nhan Tri Tran",
                "Hoang Thanh Nguyen",
                "Lan Nguyen Tran"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18154v1",
                "http://arxiv.org/pdf/2310.18154v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18152v2",
            "title": "Disentangled Representation Learning with Large Language Models for\n  Text-Attributed Graphs",
            "updated": "2023-11-06T12:54:14Z",
            "published": "2023-10-27T14:00:04Z",
            "summary": "Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs\nsuch as citation networks, e-commerce networks and social networks has\nattracted considerable attention in the web community. Recently, large language\nmodels (LLMs) have demonstrated exceptional capabilities across a wide range of\ntasks. However, the existing works focus on harnessing the potential of LLMs\nsolely relying on prompts to convey graph structure information to LLMs, thus\nsuffering from insufficient understanding of the complex structural\nrelationships within TAGs. To address this problem, in this paper we present\nthe Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the\nreasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model\nincorporates graph structure information through tailored disentangled graph\nneural network (GNN) layers, enabling LLMs to capture the intricate\nrelationships hidden in text-attributed graphs from multiple structural\nfactors. Furthermore, DGTL operates with frozen pre-trained LLMs, reducing\ncomputational costs and allowing much more flexibility in combining with\ndifferent LLM models. Experimental evaluations demonstrate the effectiveness of\nthe proposed DGTL model on achieving superior or comparable performance over\nstate-of-the-art baselines. Additionally, we also demonstrate that our DGTL\nmodel can offer natural language explanations for predictions, thereby\nsignificantly enhancing model interpretability.",
            "author": [
                "Yijian Qin",
                "Xin Wang",
                "Ziwei Zhang",
                "Wenwu Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18152v2",
                "http://arxiv.org/pdf/2310.18152v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18150v1",
            "title": "Event-Triggered Consensus for Continuous-Time Distributed Estimation",
            "updated": "2023-10-27T13:56:22Z",
            "published": "2023-10-27T13:56:22Z",
            "summary": "Distributed sensor networks have gained interest thanks to the developments\nin processing power and communications. Event-triggering mechanisms can be\nuseful in reducing communication between the nodes of the network, while still\nensuring an adequate behaviour of the system. However, very little attention\nhas been given to continuous-time systems in this context. In this work, we\npropose a strategy for distributed state estimation in sensor networks, based\non average dynamic consensus of the continuous measurements. While\ncommunication between nodes is discrete and heavily reduced due to the\nevent-triggering mechanism, our method ensures that the nodes are still able to\nproduce a continuous estimate of the global average measurement and the state\nof the plant, within some tuneable error bounds.",
            "author": [
                "Irene Perez-Salesa",
                "Rodrigo Aldana-Lopez",
                "Carlos Sagues"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.ifacol.2023.10.641",
                "http://arxiv.org/abs/2310.18150v1",
                "http://arxiv.org/pdf/2310.18150v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18146v3",
            "title": "Adaptive Out-Orientations with Applications",
            "updated": "2023-11-04T10:45:16Z",
            "published": "2023-10-27T13:51:53Z",
            "summary": "We give improved algorithms for maintaining edge-orientations of a\nfully-dynamic graph, such that the out-degree of each vertex is bounded. On one\nhand, we show how to orient the edges such that the out-degree of each vertex\nis proportional to the arboricity $\\alpha$ of the graph, in, either, an\namortised update time of $O(\\log^2 n \\log \\alpha)$, or a worst-case update time\nof $O(\\log^3 n \\log \\alpha)$. On the other hand, motivated by applications\nincluding dynamic maximal matching, we obtain a different trade-off, namely\neither $O(\\log n \\log \\alpha)$, amortised, or $O(\\log ^2 n \\log \\alpha)$,\nworst-case time, for the problem of maintaining an edge-orientation with at\nmost $O(\\alpha + \\log n)$ out-edges per vertex. Since our algorithms have\nupdate times with worst-case guarantees, the number of changes to the solution\n(i.e. the recourse) is naturally limited. Our algorithms adapt to the current\narboricity of the graph, and yield improvements over previous work: Firstly, we\nobtain an $O(\\varepsilon^{-6}\\log^3 n \\log \\rho)$ worst-case update time\nalgorithm for maintaining a $(1+\\varepsilon)$ approximation of the maximum\nsubgraph density, $\\rho$.\n  Secondly, we obtain an $O(\\varepsilon^{-6}\\log^3 n \\log \\alpha)$ worst-case\nupdate time algorithm for maintaining a $(1 + \\varepsilon) \\cdot OPT + 2$\napproximation of the optimal out-orientation of a graph with adaptive\narboricity $\\alpha$. This yields the first worst-case polylogarithmic dynamic\nalgorithm for decomposing into $O(\\alpha)$ forests.Thirdly, we obtain\narboricity-adaptive fully-dynamic deterministic algorithms for a variety, of\nproblems including maximal matching, $\\Delta+1$ coloring, and matrix vector\nmultiplication. All update times are worst-case $O(\\alpha+\\log^2n \\log\n\\alpha)$, where $\\alpha$ is the current arboricity of the graph.",
            "author": [
                "Chandra Chekuri",
                "Aleksander Bj\u00f8rn Christiansen",
                "Jacob Holm",
                "Ivor van der Hoog",
                "Kent Quanrud",
                "Eva Rotenberg",
                "Chris Schwiegelshohn"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18146v3",
                "http://arxiv.org/pdf/2310.18146v3"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18141v1",
            "title": "Unsupervised Representation Learning for Diverse Deformable Shape\n  Collections",
            "updated": "2023-10-27T13:45:30Z",
            "published": "2023-10-27T13:45:30Z",
            "summary": "We introduce a novel learning-based method for encoding and manipulating 3D\nsurface meshes. Our method is specifically designed to create an interpretable\nembedding space for deformable shape collections. Unlike previous 3D mesh\nautoencoders that require meshes to be in a 1-to-1 correspondence, our approach\nis trained on diverse meshes in an unsupervised manner. Central to our method\nis a spectral pooling technique that establishes a universal latent space,\nbreaking free from traditional constraints of mesh connectivity and shape\ncategories. The entire process consists of two stages. In the first stage, we\nemploy the functional map paradigm to extract point-to-point (p2p) maps between\na collection of shapes in an unsupervised manner. These p2p maps are then\nutilized to construct a common latent space, which ensures straightforward\ninterpretation and independence from mesh connectivity and shape category.\nThrough extensive experiments, we demonstrate that our method achieves\nexcellent reconstructions and produces more realistic and smoother\ninterpolations than baseline approaches.",
            "author": [
                "Sara Hahner",
                "Souhaib Attaiki",
                "Jochen Garcke",
                "Maks Ovsjanikov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18141v1",
                "http://arxiv.org/pdf/2310.18141v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18139v1",
            "title": "The impact of convective criteria on the properties of massive stars",
            "updated": "2023-10-27T13:41:00Z",
            "published": "2023-10-27T13:41:00Z",
            "summary": "We study the differences between models computed with Ledoux and\nSchwarzschild criteria on the internal structure, evolutionary track in the\nHertzsprung-Russell diagram (HRD), lifetimes, evolution of the surface\nabundances and velocities, and masses of the He and CO cores. We investigate\nthe consequences on the nature of the supernova (SN) progenitors and the type\nof SN events, as well as on the yields of light elements. We also study the\nimpact on the outputs of population synthesis models. Models with initial\nmasses between 7 and 120 M$_\\odot$ at solar metallicity ($Z$=0.014) and with an\ninitial rotation equal to 0 or 0.4 times the critical velocity at the zero-age\nmain sequence were computed with either the Schwarzschild or the Ledoux\ncriterion until the end of the C-burning phase. Models with initial masses\nbetween 15 and 32 M$_\\odot$ computed with the Schwarzschild criterion show\nlarger intermediate convective zones attached to the H-burning shell than\nmodels computed with the Ledoux criterion. Their CO cores and outer convective\nzones in the red supergiant (RSG) phase are also smaller. This impacts many\noutputs of stars during the core He-burning phase. Schwarzschild models have\nsmaller CO cores and outer convective zones in the RSG phase, and their\nblue-to-red supergiant ratio is much higher than for Ledoux models. They also\nproduce longer crossings of the Hertzsprung gap and favour blue loops. The\nupper luminosity of RSGs is little affected by the change in the convective\ncriterion. The maximum luminosity of RSG progenitors for type II-P SN events is\nlowered from 5.2 to 4.95 when the Ledoux criterion is used instead of the\nSchwarzschild criterion in non-rotating models. The Schwarzschild criterion\npredicts longer-lasting, less nitrogen-enriched, and faster-rotating Cepheids.\nRotational mixing decreases the differences between Schwarzschild and Ledoux\nmodels.",
            "author": [
                "Yves Sibony",
                "Cyril Georgy",
                "Sylvia Ekstr\u00f6m",
                "Georges Meynet"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18139v1",
                "http://arxiv.org/pdf/2310.18139v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.19903v1",
            "title": "A Multi-agent Reinforcement Learning Study of Emergence of Social\n  Classes out of Arbitrary Governance: The Role of Environment",
            "updated": "2023-10-27T13:31:53Z",
            "published": "2023-10-27T13:31:53Z",
            "summary": "There are several theories in economics regarding the roots or causes of\nprosperity in a society. One of these theories or hypotheses -- named geography\nhypothesis -- mentions that the reason why some countries are prosperous and\nsome others are poor is the geographical location of the countries in the world\nas makes their climate and environment favorable or unfavorable regarding\nnatural resources. Another competing hypothesis states that man-made\ninstitutions particularly inclusive political institutions are the reasons why\nsome countries are prosperous and some others are poor. On the other hand,\nthere is a specific political theory developed for the long-term social\ndevelopment in Iran -- named Arbitrary Rule and Aridisolatic Society which\nparticularly emphasizes on the role of aridity to shape arbitrary political and\neconomical institutions in Iran, without any functional social classes in the\nsociety. In this paper, by extending the AI-Economist -- a recently developed\ntwo-level multi-agent reinforcement learning environment -- I show that when\nthe central planner is ruling the environment by arbitrary rules, the society\nevolves through different paths in different environments. In the environment\nhaving band-like vertical isolated patches of natural resources, all mobile\nagents are equally exploited by the central planner and the central planner is\nalso not gaining any income, while in the society having more uniformly\ndistributed natural resources, the productivity and Maximin are higher and the\nsociety generates a heterogeneous stratified social structure. All these\nfindings provide a partial answer to the above debate and reconcile the role of\ngeography and political institutions on the long-term development in a region.",
            "author": [
                "Aslan S. Dizaji"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19903v1",
                "http://arxiv.org/pdf/2310.19903v1"
            ],
            "primary_category": "cs.MA",
            "category": [
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18135v1",
            "title": "Equivariant simplicial distributions and quantum contextuality",
            "updated": "2023-10-27T13:25:46Z",
            "published": "2023-10-27T13:25:46Z",
            "summary": "We introduce an equivariant version of contextuality with respect to a\nsymmetry group, which comes with natural applications to quantum theory. In the\nequivariant setting, we construct cohomology classes that can detect\ncontextuality. This framework is motivated by the earlier topological approach\nto contextuality producing cohomology classes that serve as computational\nprimitives in measurement-based quantum computing.",
            "author": [
                "Cihan Okay",
                "Igor Sikora"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18135v1",
                "http://arxiv.org/pdf/2310.18135v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "math.AT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18130v2",
            "title": "DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial\n  Issues",
            "updated": "2023-11-07T20:29:53Z",
            "published": "2023-10-27T13:23:02Z",
            "summary": "Controversy is a reflection of our zeitgeist, and an important aspect to any\ndiscourse. The rise of large language models (LLMs) as conversational systems\nhas increased public reliance on these systems for answers to their various\nquestions. Consequently, it is crucial to systematically examine how these\nmodels respond to questions that pertaining to ongoing debates. However, few\nsuch datasets exist in providing human-annotated labels reflecting the\ncontemporary discussions. To foster research in this area, we propose a novel\nconstruction of a controversial questions dataset, expanding upon the publicly\nreleased Quora Question Pairs Dataset. This dataset presents challenges\nconcerning knowledge recency, safety, fairness, and bias. We evaluate different\nLLMs using a subset of this dataset, illuminating how they handle controversial\nissues and the stances they adopt. This research ultimately contributes to our\nunderstanding of LLMs' interaction with controversial issues, paving the way\nfor improvements in their comprehension and handling of complex societal\ndebates.",
            "author": [
                "David Q. Sun",
                "Artem Abzaliev",
                "Hadas Kotek",
                "Zidi Xiu",
                "Christopher Klein",
                "Jason D. Williams"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18130v2",
                "http://arxiv.org/pdf/2310.18130v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18129v1",
            "title": "TabAttention: Learning Attention Conditionally on Tabular Data",
            "updated": "2023-10-27T13:21:37Z",
            "published": "2023-10-27T13:21:37Z",
            "summary": "Medical data analysis often combines both imaging and tabular data processing\nusing machine learning algorithms. While previous studies have investigated the\nimpact of attention mechanisms on deep learning models, few have explored\nintegrating attention modules and tabular data. In this paper, we introduce\nTabAttention, a novel module that enhances the performance of Convolutional\nNeural Networks (CNNs) with an attention mechanism that is trained\nconditionally on tabular data. Specifically, we extend the Convolutional Block\nAttention Module to 3D by adding a Temporal Attention Module that uses\nmulti-head self-attention to learn attention maps. Furthermore, we enhance all\nattention modules by integrating tabular data embeddings. Our approach is\ndemonstrated on the fetal birth weight (FBW) estimation task, using 92 fetal\nabdominal ultrasound video scans and fetal biometry measurements. Our results\nindicate that TabAttention outperforms clinicians and existing methods that\nrely on tabular and/or imaging data for FBW prediction. This novel approach has\nthe potential to improve computer-aided diagnosis in various clinical workflows\nwhere imaging and tabular data are combined. We provide a source code for\nintegrating TabAttention in CNNs at\nhttps://github.com/SanoScience/Tab-Attention.",
            "author": [
                "Michal K. Grzeszczyk",
                "Szymon P\u0142otka",
                "Beata Rebizant",
                "Katarzyna Kosi\u0144ska-Kaczy\u0144ska",
                "Micha\u0142 Lipa",
                "Robert Brawura-Biskupski-Samaha",
                "Przemys\u0142aw Korzeniowski",
                "Tomasz Trzci\u0144ski",
                "Arkadiusz Sitek"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-43990-2_33",
                "http://arxiv.org/abs/2310.18129v1",
                "http://arxiv.org/pdf/2310.18129v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18127v1",
            "title": "Ask more, know better: Reinforce-Learned Prompt Questions for Decision\n  Making with Large Language Models",
            "updated": "2023-10-27T13:19:19Z",
            "published": "2023-10-27T13:19:19Z",
            "summary": "Large language models (LLMs) demonstrate their promise in tackling\ncomplicated practical challenges by combining action-based policies with chain\nof thought (CoT) reasoning. Having high-quality prompts on hand, however, is\nvital to the framework's effectiveness. Currently, these prompts are\nhandcrafted utilizing extensive human labor, resulting in CoT policies that\nfrequently fail to generalize. Human intervention is also required in order to\ndevelop grounding functions that ensure low-level controllers appropriately\nprocess CoT reasoning. In this paper, we take the first step towards a fully\nintegrated end-to-end framework for task-solving in real settings employing\ncomplicated reasoning. To that purpose, we offer a new leader-follower bilevel\nframework capable of learning to ask relevant questions (prompts) and\nsubsequently undertaking reasoning to guide the learning of actions to be\nperformed in an environment. A good prompt should make introspective revisions\nbased on historical findings, leading the CoT to consider the anticipated\ngoals. A prompt-generator policy has its own aim in our system, allowing it to\nadapt to the action policy and automatically root the CoT process towards\noutputs that lead to decisive, high-performing actions. Meanwhile, the action\npolicy is learning how to use the CoT outputs to take specific actions. Our\nempirical data reveal that our system outperforms leading methods in agent\nlearning benchmarks such as Overcooked and FourRoom.",
            "author": [
                "Xue Yan",
                "Yan Song",
                "Xinyu Cui",
                "Filippos Christianos",
                "Haifeng Zhang",
                "David Henry Mguni",
                "Jun Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18127v1",
                "http://arxiv.org/pdf/2310.18127v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18125v1",
            "title": "An Elliott intertwining approach to classifying actions of C$^*$-tensor\n  categories",
            "updated": "2023-10-27T13:15:39Z",
            "published": "2023-10-27T13:15:39Z",
            "summary": "We introduce a categorical approach to classifying actions of C$^*$-tensor\ncategories $\\mathcal{C}$ on C$^*$-algebras up to cocycle conjugacy. We show\nthat, in this category, inductive limits exist and there is a natural notion of\napproximate unitary equivalence. Then, we generalise classical Elliott\nintertwining results to the $\\mathcal{C}$-equivariant case, in the same fashion\nas done by Szab\\'o for the group equivariant case in [39].",
            "author": [
                "Sergio Gir\u00f3n Pacheco",
                "Robert Neagu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18125v1",
                "http://arxiv.org/pdf/2310.18125v1"
            ],
            "primary_category": "math.OA",
            "category": [
                "math.OA",
                "math.QA",
                "46L35, 46L37, 46L55, 18D10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18122v2",
            "title": "OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization",
            "updated": "2023-11-11T20:52:04Z",
            "published": "2023-10-27T13:09:54Z",
            "summary": "Opinion summarization sets itself apart from other types of summarization\ntasks due to its distinctive focus on aspects and sentiments. Although certain\nautomated evaluation methods like ROUGE have gained popularity, we have found\nthem to be unreliable measures for assessing the quality of opinion summaries.\nIn this paper, we present OpinSummEval, a dataset comprising human judgments\nand outputs from 14 opinion summarization models. We further explore the\ncorrelation between 24 automatic metrics and human ratings across four\ndimensions. Our findings indicate that metrics based on neural networks\ngenerally outperform non-neural ones. However, even metrics built on powerful\nbackbones, such as BART and GPT-3/3.5, do not consistently correlate well\nacross all dimensions, highlighting the need for advancements in automated\nevaluation methods for opinion summarization. The code and data are publicly\navailable at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.",
            "author": [
                "Yuchen Shen",
                "Xiaojun Wan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18122v2",
                "http://arxiv.org/pdf/2310.18122v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18119v1",
            "title": "Towards a Unified Conversational Recommendation System: Multi-task\n  Learning via Contextualized Knowledge Distillation",
            "updated": "2023-10-27T13:06:24Z",
            "published": "2023-10-27T13:06:24Z",
            "summary": "In Conversational Recommendation System (CRS), an agent is asked to recommend\na set of items to users within natural language conversations. To address the\nneed for both conversational capability and personalized recommendations, prior\nworks have utilized separate recommendation and dialogue modules. However, such\napproach inevitably results in a discrepancy between recommendation results and\ngenerated responses. To bridge the gap, we propose a multi-task learning for a\nunified CRS, where a single model jointly learns both tasks via Contextualized\nKnowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate\nand soft gate. The former selectively gates between two task-specific teachers,\nwhile the latter integrates knowledge from both teachers. Our gates are\ncomputed on-the-fly in a context-specific manner, facilitating flexible\nintegration of relevant knowledge. Extensive experiments demonstrate that our\nsingle model significantly improves recommendation performance while enhancing\nfluency, and achieves comparable results in terms of diversity.",
            "author": [
                "Yeongseo Jung",
                "Eunseo Jung",
                "Lei Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18119v1",
                "http://arxiv.org/pdf/2310.18119v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18118v1",
            "title": "A Global Multi-Unit Calibration as a Method for Large Scale IoT\n  Particulate Matter Monitoring Systems Deployments",
            "updated": "2023-10-27T13:04:53Z",
            "published": "2023-10-27T13:04:53Z",
            "summary": "Scalable and effective calibration is a fundamental requirement for Low Cost\nAir Quality Monitoring Systems and will enable accurate and pervasive\nmonitoring in cities. Suffering from environmental interferences and\nfabrication variance, these devices need to encompass sensors specific and\ncomplex calibration processes for reaching a sufficient accuracy to be deployed\nas indicative measurement devices in Air Quality (AQ) monitoring networks.\nConcept and sensor drift often force calibration process to be frequently\nrepeated. These issues lead to unbearable calibration costs which denies their\nmassive deployment when accuracy is a concern. In this work, We propose a zero\ntransfer samples, global calibration methodology as a technological enabler for\nIoT AQ multisensory devices which relies on low cost Particulate Matter (PM)\nsensors. This methodology is based on field recorded responses from a limited\nnumber of IoT AQ multisensors units and machine learning concepts and can be\nuniversally applied to all units of the same type. A multi season test campaign\nshown that, when applied to different sensors, this methodology performances\nmatch those of state of the art methodology which requires to derive different\ncalibration parameters for each different unit. If confirmed, these results\nshow that, when properly derived, a global calibration law can be exploited for\na large number of networked devices with dramatic cost reduction eventually\nallowing massive deployment of accurate IoT AQ monitoring devices. Furthermore,\nthis calibration model could be easily embedded on board of the device or\nimplemented on the edge allowing immediate access to accurate readings for\npersonal exposure monitor applications as well as reducing long range data\ntransfer needs.",
            "author": [
                "Saverio De Vito",
                "Gerardo D Elia",
                "Sergio Ferlito",
                "Girolamo Di Francia",
                "Milos Davidovic",
                "Duska Kleut",
                "Danka Stojanovic",
                "Milena Jovasevic Stojanovic"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TIM.2023.3331428",
                "http://arxiv.org/abs/2310.18118v1",
                "http://arxiv.org/pdf/2310.18118v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18116v2",
            "title": "Direct Unsupervised Denoising",
            "updated": "2023-12-04T17:38:31Z",
            "published": "2023-10-27T13:02:12Z",
            "summary": "Traditional supervised denoisers are trained using pairs of noisy input and\nclean target images. They learn to predict a central tendency of the posterior\ndistribution over possible clean images. When, e.g., trained with the popular\nquadratic loss function, the network's output will correspond to the minimum\nmean square error (MMSE) estimate. Unsupervised denoisers based on Variational\nAutoEncoders (VAEs) have succeeded in achieving state-of-the-art results while\nrequiring only unpaired noisy data as training input. In contrast to the\ntraditional supervised approach, unsupervised denoisers do not directly produce\na single prediction, such as the MMSE estimate, but allow us to draw samples\nfrom the posterior distribution of clean solutions corresponding to the noisy\ninput. To approximate the MMSE estimate during inference, unsupervised methods\nhave to create and draw a large number of samples - a computationally expensive\nprocess - rendering the approach inapplicable in many situations. Here, we\npresent an alternative approach that trains a deterministic network alongside\nthe VAE to directly predict a central tendency. Our method achieves results\nthat surpass the results achieved by the unsupervised method at a fraction of\nthe computational cost.",
            "author": [
                "Benjamin Salmon",
                "Alexander Krull"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18116v2",
                "http://arxiv.org/pdf/2310.18116v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18115v1",
            "title": "First-order electronic phase transition in $\u03b1$-(BEDT-TTF)$_2$I$_3$\n  revealed by temperature-dependent generalized ellipsometry",
            "updated": "2023-10-27T12:59:16Z",
            "published": "2023-10-27T12:59:16Z",
            "summary": "The nature of correlation-driven metal-insulator transitions remains a\nlongstanding puzzle in solid-state physics. While some theories suggest a\nsecond-order character, various experimental observations in these materials\nindicate first-order phase transitions. Despite considerable progress over the\nlast decades in understanding the underlying driving mechanisms of\nmetal-insulator transitions, in particular the phase coexistence remains poorly\nunderstood on a microscopic scale. Here, we employ Mueller matrix spectroscopic\nand temperature-dependent ellipsometry to determine the anisotropic dielectric\nfunctions of the two-dimensional charge-transfer salt\n$\\alpha$-(BEDT-TTF)$_2$I$_3$ across its charge-order metal-insulator\ntransition. Our results offer valuable insights into temperature-dependent\nchanges of the dielectric functions along the different crystallographic axes.\nFurthermore, we apply an effective-medium approximation to quantify the\ncorrelation between the metal-to-insulator transition and the volume fraction\nof the metallic phase embedded within the insulating phase. Through this\ncomprehensive approach, generalized ellipsometry unravels the nature of the\ncorrelation-driven metal-insulator transition.",
            "author": [
                "Achyut Tiwari",
                "Bruno Gompf",
                "Dieter Schweitzer",
                "Martin Dressel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18115v1",
                "http://arxiv.org/pdf/2310.18115v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18110v1",
            "title": "A Control-Bounded Quadrature Leapfrog ADC",
            "updated": "2023-10-27T12:51:16Z",
            "published": "2023-10-27T12:51:16Z",
            "summary": "In this paper, the design flexibility of the control-bounded\nanalog-to-digital converter principle is demonstrated. A band-pass\nanalog-to-digital converter is considered as an application and case study. We\nshow how a low-pass control-bounded analog-to-digital converter can be\ntranslated into a band-pass version where the guaranteed stability, converter\nbandwidth, and signal-to-noise ratio are preserved while the center frequency\nfor conversion can be positioned freely. The proposed converter is validated\nwith behavioral simulations on several filter orders, center frequencies, and\noversampling ratios. Additionally, we consider an op-amp circuit realization\nwhere the effects of first-order op-amp non-idealities are shown. Finally,\nrobustness against component variations is demonstrated by Monte Carlo\nsimulations.",
            "author": [
                "Hampus Malmberg",
                "Fredrik Feyling",
                "Jose M. de la Rosa"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18110v1",
                "http://arxiv.org/pdf/2310.18110v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18109v1",
            "title": "Search for dark energy with neutron interferometry",
            "updated": "2023-10-27T12:49:01Z",
            "published": "2023-10-27T12:49:01Z",
            "summary": "We use previously obtained experimental results by neutron interferometry to\neffectively constrain the parameter space of several prominent dark energy\nmodels. This investigation encompasses the environment-dependent dilaton field,\na compelling contender for dark energy that emerges naturally within the strong\ncoupling limit of string theory, alongside symmetron and chameleon fields. Our\nstudy presents substantial improvements over previous constraints of the\ndilaton and symmetron fields, improving parameter constraints by several orders\nof magnitude. However, the analysis does not yield any new constraints on the\nchameleon field. Furthermore, we establish constraints for the projected\nneutron split interferometer, which has recently concluded a decisive\nproof-of-principle demonstration. Our symmetron simulations reveal that\ndepending on the parameter values there are multiple static solutions with\nincreasing number of nodes and increasing energy inside a cylindrical vacuum\nchamber. This agrees with results obtained earlier in the literature for\ninfinitely parallel plates. Interestingly, while these multiple solutions can\ncorrespond to domain walls forming inside the vacuum chamber, we also find\nsolutions that do not reach their vacuum expectation value inside the vacuum\nchamber, but display multiple nodes nonetheless.",
            "author": [
                "Hauke Fischer",
                "Christian K\u00e4ding",
                "Hartmut Lemmel",
                "Stephan Sponar",
                "Mario Pitschmann"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18109v1",
                "http://arxiv.org/pdf/2310.18109v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "astro-ph.CO",
                "gr-qc",
                "nucl-ex",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18106v1",
            "title": "Polygenic dynamics underlying the response of quantitative traits to\n  directional selection",
            "updated": "2023-10-27T12:44:53Z",
            "published": "2023-10-27T12:44:53Z",
            "summary": "We study the response of a quantitative trait to exponential directional\nselection in a finite haploid population at the genetic and the phenotypic\nlevel. We assume an infinite sites model, in which the number of new mutations\nper generation in the population follows a Poisson distribution (with mean\n$\\Theta$) and each mutation occurs at a new, previously monomorphic site.\nMutation effects are beneficial and drawn from a distribution. Sites are\nunlinked and contribute additively to the trait. Assuming that selection is\nstronger than random genetic drift, we model the initial phase of the dynamics\nby a supercritical Galton-Watson process. This enables us to obtain\ntime-dependent results. We show that the copy-number distribution of the mutant\nin generation n, conditioned on non-extinction until n, is described accurately\nby the deterministic increase from an initial distribution with mean 1. This\ndistribution is related to the absolutely continuous part $W^+$ of the random\nvariable, typically denoted $W$, that characterizes the stochasticity\naccumulating during the mutant's sweep. On this basis, we derive explicitly the\n(approximate) time dependence of the mutant frequency distribution, of the\nexpected mean and variance of the trait and of the expected number of\nsegregating sites. Unexpectedly, we obtain highly accurate approximations for\nall times, even for the quasi-stationary phase where we refine classical\nresults. In addition, we find that $\\Theta$ is the main determinant of the\npattern of adaptation at the genetic level, i.e., whether the initial\nallele-frequency dynamics are best described by sweep-like patterns at few loci\nor small allele-frequency shifts at many. The selection strength determines\nprimarily the rate of adaptation. The accuracy of our results is tested by\ncomprehensive simulations in a Wright-Fisher framework.",
            "author": [
                "Hannah G\u00f6tsch",
                "Reinhard B\u00fcrger"
            ],
            "link": [
                "http://dx.doi.org/10.1101/2023.02.23.529647",
                "http://arxiv.org/abs/2310.18106v1",
                "http://arxiv.org/pdf/2310.18106v1"
            ],
            "primary_category": "q-bio.PE",
            "category": [
                "q-bio.PE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18103v1",
            "title": "A Novel Application of Polynomial Solvers in mmWave Analog Radio\n  Beamforming",
            "updated": "2023-10-27T12:41:41Z",
            "published": "2023-10-27T12:41:41Z",
            "summary": "Beamforming is a signal processing technique where an array of antenna\nelements can be steered to transmit and receive radio signals in a specific\ndirection. The usage of millimeter wave (mmWave) frequencies and multiple input\nmultiple output (MIMO) beamforming are considered as the key innovations of 5th\nGeneration (5G) and beyond communication systems. The technique initially\nperforms a beam alignment procedure, followed by data transfer in the aligned\ndirections between the transmitter and the receiver. Traditionally, beam\nalignment involves periodical and exhaustive beam sweeping at both transmitter\nand the receiver, which is a slow process causing extra communication overhead\nwith MIMO and massive MIMO radio units. In applications such as beam tracking,\nangular velocity, beam steering etc., the beam alignment procedure is optimized\nby estimating the beam directions using first order polynomial approximations.\nRecent learning-based SOTA strategies for fast mmWave beam alignment also\nrequire exploration over exhaustive beam pairs during the training procedure,\ncausing overhead to learning strategies for higher antenna configurations. In\nthis work, we first optimize the beam alignment cost functions e.g. the data\nrate, to reduce the beam sweeping overhead by applying polynomial\napproximations of its partial derivatives which can then be solved as a system\nof polynomial equations using well-known tools from algebraic geometry. At this\npoint, a question arises: 'what is a good polynomial approximation?' In this\nwork, we attempt to obtain a 'good polynomial approximation'. Preliminary\nexperiments indicate that our estimated polynomial approximations attain a\nso-called sweet-spot in terms of the solver speed and accuracy, when evaluated\non test beamforming problems.",
            "author": [
                "Snehal Bhayani",
                "Praneeth Susarla",
                "S. S. Krishna Chaitanya Bulusu",
                "Olli Silven",
                "Markku Juntti",
                "Janne Heikkila"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18103v1",
                "http://arxiv.org/pdf/2310.18103v1"
            ],
            "primary_category": "cs.SC",
            "category": [
                "cs.SC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18102v1",
            "title": "Impact of Hydrogenation on the Stability and Mechanical Properties of\n  Amorphous Boron Nitride",
            "updated": "2023-10-27T12:39:40Z",
            "published": "2023-10-27T12:39:40Z",
            "summary": "Interconnect materials with ultralow dielectric constant, and good thermal\nand mechanical properties are crucial for the further miniaturization of\nelectronic devices. Recently, it has been demonstrated that ultrathin amorphous\nboron nitride (aBN) films have a very low dielectric constant, high density\n(above 2.1 g/cm3), high thermal stability, and mechanical properties. The\nexcellent properties of aBN derive from the nature and degree of disorder,\nwhich can be controlled at fabrication, allowing tuning of the physical\nproperties for desired applications. Here, we report an improvement in the\nstability and mechanical properties of amorphous boron nitride upon hydrogen\ndoping. With the introduction of a Gaussian approximation potential (GAP) for\natomistic simulations, we investigate the changing morphology of amorphous\nboron nitride with varying H doping concentrations. We found that for 8 at% of\nH doping, the concentration of $sp^3$-hybridized atoms reaches a maximum which\nleads to an improvement of thermal stability and mechanical properties by 20%.\nThese results will be a guideline for experimentalists and process engineers to\ntune the growth conditions of amorphous boron nitride films for numerous\napplications.",
            "author": [
                "Onurcan Kaya",
                "Luigi Colombo",
                "Aleandro Antidormi",
                "Marco A. Villena",
                "Mario Lanza",
                "Ivan Cole",
                "Stephan Roche"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18102v1",
                "http://arxiv.org/pdf/2310.18102v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18099v1",
            "title": "Enabling Acoustic Audience Feedback in Large Virtual Events",
            "updated": "2023-10-27T12:34:50Z",
            "published": "2023-10-27T12:34:50Z",
            "summary": "The COVID-19 pandemic shifted many events in our daily lives into the virtual\ndomain. While virtual conference systems provide an alternative to physical\nmeetings, larger events require a muted audience to avoid an accumulation of\nbackground noise and distorted audio. However, performing artists strongly rely\non the feedback of their audience. We propose a concept for a virtual audience\nframework which supports all participants with the ambience of a real audience.\nAudience feedback is collected locally, allowing users to express enthusiasm or\ndiscontent by selecting means such as clapping, whistling, booing, and\nlaughter. This feedback is sent as abstract information to a virtual audience\nserver. We broadcast the combined virtual audience feedback information to all\nparticipants, which can be synthesized as a single acoustic feedback by the\nclient. The synthesis can be done by turning the collective audience feedback\ninto a prompt that is fed to state-of-the-art models such as AudioGen. This\nway, each user hears a single acoustic feedback sound of the entire virtual\nevent, without requiring to unmute or risk hearing distorted, unsynchronized\nfeedback.",
            "author": [
                "Tamay Aykut",
                "Markus Hofbauer",
                "Christopher Kuhn",
                "Eckehard Steinbach",
                "Bernd Girod"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18099v1",
                "http://arxiv.org/pdf/2310.18099v1"
            ],
            "primary_category": "cs.MM",
            "category": [
                "cs.MM",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18098v1",
            "title": "Mind the Gap: Automated Corpus Creation for Enthymeme Detection and\n  Reconstruction in Learner Arguments",
            "updated": "2023-10-27T12:33:40Z",
            "published": "2023-10-27T12:33:40Z",
            "summary": "Writing strong arguments can be challenging for learners. It requires to\nselect and arrange multiple argumentative discourse units (ADUs) in a logical\nand coherent way as well as to decide which ADUs to leave implicit, so called\nenthymemes. However, when important ADUs are missing, readers might not be able\nto follow the reasoning or understand the argument's main point. This paper\nintroduces two new tasks for learner arguments: to identify gaps in arguments\n(enthymeme detection) and to fill such gaps (enthymeme reconstruction).\nApproaches to both tasks may help learners improve their argument quality. We\nstudy how corpora for these tasks can be created automatically by deleting ADUs\nfrom an argumentative text that are central to the argument and its quality,\nwhile maintaining the text's naturalness. Based on the ICLEv3 corpus of\nargumentative learner essays, we create 40,089 argument instances for enthymeme\ndetection and reconstruction. Through manual studies, we provide evidence that\nthe proposed corpus creation process leads to the desired quality reduction,\nand results in arguments that are similarly natural to those written by\nlearners. Finally, first baseline approaches to enthymeme detection and\nreconstruction demonstrate the corpus' usefulness.",
            "author": [
                "Maja Stahl",
                "Nick D\u00fcsterhus",
                "Mei-Hua Chen",
                "Henning Wachsmuth"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18098v1",
                "http://arxiv.org/pdf/2310.18098v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18097v1",
            "title": "A graph database for feature characterization of dislocation networks",
            "updated": "2023-10-27T12:33:13Z",
            "published": "2023-10-27T12:33:13Z",
            "summary": "Three-dimensional dislocation networks control the mechanical properties such\nas strain hardening of crystals. Due to the complexity of dislocation networks\nand their temporal evolution, analysis tools are needed that fully resolve the\ndynamic processes of the intrinsic dislocation graph structure. We propose the\nuse of a graph database for the analysis of three-dimensional dislocation\nnetworks obtained from discrete dislocation dynamics simulations. This makes it\npossible to extract (sub-)graphs and their features with relative ease. That\nallows for a more holistic view of the evolution of dislocation networks and\nfor the extraction of homogenized graph features to be incorporated into\ncontinuum formulation. As an illustration, we describe the static and dynamic\nanalysis of spatio-temporal dislocation graphs as well as graph feature\nanalysis.",
            "author": [
                "Balduin Katzer",
                "Daniel Betsche",
                "Klemens B\u00f6hm",
                "Daniel Weygand",
                "Katrin Schulz"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.scriptamat.2023.115841",
                "http://arxiv.org/abs/2310.18097v1",
                "http://arxiv.org/pdf/2310.18097v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18090v1",
            "title": "Probabilistic Constellation Shaping for OFDM-Based ISAC Signaling",
            "updated": "2023-10-27T12:22:22Z",
            "published": "2023-10-27T12:22:22Z",
            "summary": "Integrated Sensing and Communications (ISAC) has garnered significant\nattention as a promising technology for the upcoming sixth-generation wireless\ncommunication systems (6G). In pursuit of this goal, a common strategy is that\na unified waveform, such as Orthogonal Frequency Division Multiplexing (OFDM),\nshould serve dual-functional roles by enabling simultaneous sensing and\ncommunications (S&C) operations. However, the sensing performance of an OFDM\ncommunication signal is substantially affected by the randomness of the data\nsymbols mapped from bit streams. Therefore, achieving a balance between\npreserving communication capability (i.e., the randomness) while improving\nsensing performance remains a challenging task. To cope with this issue, in\nthis paper we analyze the ambiguity function of the OFDM communication signal\nmodulated by random data. Subsequently, a probabilistic constellation shaping\n(PCS) method is proposed to devise the probability distributions of\nconstellation points, which is able to strike a scalable S&C tradeoff of the\nrandom transmitted signal. Finally, the superiority of the proposed PCS method\nover conventional uniformly distributed constellations is validated through\nnumerical simulations.",
            "author": [
                "Zhen Du",
                "Fan Liu",
                "Yifeng Xiong",
                "Tony Xiao Han",
                "Weijie Yuan",
                "Yuanhao Cui",
                "Changhua Yao",
                "Yonina C. Eldar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18090v1",
                "http://arxiv.org/pdf/2310.18090v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18089v1",
            "title": "Lost in Translation -- Multilingual Misinformation and its Evolution",
            "updated": "2023-10-27T12:21:55Z",
            "published": "2023-10-27T12:21:55Z",
            "summary": "Misinformation and disinformation are growing threats in the digital age,\nspreading rapidly across languages and borders. This paper investigates the\nprevalence and dynamics of multilingual misinformation through an analysis of\nover 250,000 unique fact-checks spanning 95 languages. First, we find that\nwhile the majority of misinformation claims are only fact-checked once, 11.7%,\ncorresponding to more than 21,000 claims, are checked multiple times. Using\nfact-checks as a proxy for the spread of misinformation, we find 33% of\nrepeated claims cross linguistic boundaries, suggesting that some\nmisinformation permeates language barriers. However, spreading patterns exhibit\nstrong homophily, with misinformation more likely to spread within the same\nlanguage. To study the evolution of claims over time and mutations across\nlanguages, we represent fact-checks with multilingual sentence embeddings and\ncluster semantically similar claims. We analyze the connected components and\nshortest paths connecting different versions of a claim finding that claims\ngradually drift over time and undergo greater alteration when traversing\nlanguages. Overall, this novel investigation of multilingual misinformation\nprovides key insights. It quantifies redundant fact-checking efforts,\nestablishes that some claims diffuse across languages, measures linguistic\nhomophily, and models the temporal and cross-lingual evolution of claims. The\nfindings advocate for expanded information sharing between fact-checkers\nglobally while underscoring the importance of localized verification.",
            "author": [
                "Dorian Quelle",
                "Calvin Cheng",
                "Alexandre Bovet",
                "Scott A. Hale"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18089v1",
                "http://arxiv.org/pdf/2310.18089v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CY",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18088v1",
            "title": "iRED: A disaggregated P4-AQM fully implemented in programmable data\n  plane hardware",
            "updated": "2023-10-27T12:14:22Z",
            "published": "2023-10-27T12:14:22Z",
            "summary": "Routers employ queues to temporarily hold packets when the scheduler cannot\nimmediately process them. Congestion occurs when the arrival rate of packets\nexceeds the processing capacity, leading to increased queueing delay. Over\ntime, Active Queue Management (AQM) strategies have focused on directly\ndraining packets from queues to alleviate congestion and reduce queuing delay.\nOn Programmable Data Plane (PDP) hardware, AQMs traditionally reside in the\nEgress pipeline due to the availability of queue delay information there. We\nargue that this approach wastes the router's resources because the dropped\npacket has already consumed the entire pipeline of the device. In this work, we\npropose ingress Random Early Detection (iRED), a more efficient approach that\naddresses the Egress drop problem. iRED is a disaggregated P4-AQM fully\nimplemented in programmable data plane hardware and also supports Low Latency,\nLow Loss, and Scalable Throughput (L4S) framework, saving device pipeline\nresources by dropping packets in the Ingress block. To evaluate iRED, we\nconducted three experiments using a Tofino2 programmable switch: i) An in-depth\nanalysis of state-of-the-art AQMs on PDP hardware, using 12 different network\nconfigurations varying in bandwidth, Round-Trip Time (RTT), and Maximum\nTransmission Unit (MTU). The results demonstrate that iRED can significantly\nreduce router resource consumption, with up to a 10x reduction in memory usage,\n12x fewer processing cycles, and 8x less power consumption for the same traffic\nload; ii) A performance evaluation regarding the L4S framework. The results\nprove that iRED achieves fairness in bandwidth usage for different types of\ntraffic (classic and scalable); iii) A comprehensive analysis of the QoS in a\nreal setup of a DASH) technology. iRED demonstrated up to a 2.34x improvement\nin FPS and a 4.77x increase in the video player buffer fill.",
            "author": [
                "Leandro C. de Almeida",
                "Rafael Pasquini",
                "Chrysa Papagianni",
                "F\u00e1bio L. Verdi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18088v1",
                "http://arxiv.org/pdf/2310.18088v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18087v1",
            "title": "A Chebyshev Confidence Guided Source-Free Domain Adaptation Framework\n  for Medical Image Segmentation",
            "updated": "2023-10-27T12:12:06Z",
            "published": "2023-10-27T12:12:06Z",
            "summary": "Source-free domain adaptation (SFDA) aims to adapt models trained on a\nlabeled source domain to an unlabeled target domain without the access to\nsource data. In medical imaging scenarios, the practical significance of SFDA\nmethods has been emphasized due to privacy concerns. Recent State-of-the-art\nSFDA methods primarily rely on self-training based on pseudo-labels (PLs).\nUnfortunately, PLs suffer from accuracy deterioration caused by domain shift,\nand thus limit the effectiveness of the adaptation process. To address this\nissue, we propose a Chebyshev confidence guided SFDA framework to accurately\nassess the reliability of PLs and generate self-improving PLs for\nself-training. The Chebyshev confidence is estimated by calculating probability\nlower bound of the PL confidence, given the prediction and the corresponding\nuncertainty. Leveraging the Chebyshev confidence, we introduce two\nconfidence-guided denoising methods: direct denoising and prototypical\ndenoising. Additionally, we propose a novel teacher-student joint training\nscheme (TJTS) that incorporates a confidence weighting module to improve PLs\niteratively. The TJTS, in collaboration with the denoising methods, effectively\nprevents the propagation of noise and enhances the accuracy of PLs. Extensive\nexperiments in diverse domain scenarios validate the effectiveness of our\nproposed framework and establish its superiority over state-of-the-art SFDA\nmethods. Our paper contributes to the field of SFDA by providing a novel\napproach for precisely estimating the reliability of pseudo-labels and a\nframework for obtaining high-quality PLs, resulting in improved adaptation\nperformance.",
            "author": [
                "Jiesi Hu",
                "Yanwu Yang",
                "Xutao Guo",
                "Jinghua Wang",
                "Ting Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18087v1",
                "http://arxiv.org/pdf/2310.18087v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18085v1",
            "title": "FPGA-Based Implicit-Explicit Real-time Simulation Solver for Railway\n  Wireless Power Transfer with Nonlinear Magnetic Coupling Components",
            "updated": "2023-10-27T12:09:58Z",
            "published": "2023-10-27T12:09:58Z",
            "summary": "Railway Wireless Power Transfer (WPT) is a promising non-contact power supply\nsolution, but constructing prototypes for controller testing can be both costly\nand unsafe. Real-time hardware-in-the-loop simulation is an effective and\nsecure testing tool, but simulating the dynamic charging process of railway WPT\nsystems is challenging due to the continuous changes in the nonlinear magnetic\ncoupling components. To address this challenge, we propose an FPGA-based\nhalf-step implicit-explicit (IMEX) simulation solver. The proposed solver\nadopts an IMEX algorithm to solve the piecewise linear and nonlinear parts of\nthe system separately, which enables FPGAs to solve nonlinear components while\nachieving high numerical stability. Additionally, we divide a complete\nintegration step into two half-steps to reduce computational time delays. Our\nproposed method offers a promising solution for the real-time simulation of\nrailway WPT systems. The novelty of our approach lies in the use of the IMEX\nalgorithm and the half-step integration method, which significantly improves\nthe accuracy and efficiency of the simulation. Our simulations and experiments\ndemonstrate the effectiveness and accuracy of the proposed solver, which\nprovides a new approach for simulating and optimizing railway WPT systems with\nnonlinear magnetic coupling components.",
            "author": [
                "Han Xu",
                "Yangbin Zeng",
                "Jialin Zheng",
                "Kainan Chen",
                "Weicheng Liu",
                "Zhengming Zhao"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TTE.2023.3332583",
                "http://arxiv.org/abs/2310.18085v1",
                "http://arxiv.org/pdf/2310.18085v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18077v1",
            "title": "Detrimental Contexts in Open-Domain Question Answering",
            "updated": "2023-10-27T11:45:16Z",
            "published": "2023-10-27T11:45:16Z",
            "summary": "For knowledge intensive NLP tasks, it has been widely accepted that accessing\nmore information is a contributing factor to improvements in the model's\nend-to-end performance. However, counter-intuitively, too much context can have\na negative impact on the model when evaluated on common question answering (QA)\ndatasets. In this paper, we analyze how passages can have a detrimental effect\non retrieve-then-read architectures used in question answering. Our empirical\nevidence indicates that the current read architecture does not fully leverage\nthe retrieved passages and significantly degrades its performance when using\nthe whole passages compared to utilizing subsets of them. Our findings\ndemonstrate that model accuracy can be improved by 10% on two popular QA\ndatasets by filtering out detrimental passages. Additionally, these outcomes\nare attained by utilizing existing retrieval methods without further training\nor data. We further highlight the challenges associated with identifying the\ndetrimental passages. First, even with the correct context, the model can make\nan incorrect prediction, posing a challenge in determining which passages are\nmost influential. Second, evaluation typically considers lexical matching,\nwhich is not robust to variations of correct answers. Despite these\nlimitations, our experimental results underscore the pivotal role of\nidentifying and removing these detrimental passages for the context-efficient\nretrieve-then-read pipeline. Code and data are available at\nhttps://github.com/xfactlab/emnlp2023-damaging-retrieval",
            "author": [
                "Philhoon Oh",
                "James Thorne"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18077v1",
                "http://arxiv.org/pdf/2310.18077v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18076v1",
            "title": "Knowledge Corpus Error in Question Answering",
            "updated": "2023-10-27T11:44:06Z",
            "published": "2023-10-27T11:44:06Z",
            "summary": "Recent works in open-domain question answering (QA) have explored generating\ncontext passages from large language models (LLMs), replacing the traditional\nretrieval step in the QA pipeline. However, it is not well understood why\ngenerated passages can be more effective than retrieved ones. This study\nrevisits the conventional formulation of QA and introduces the concept of\nknowledge corpus error. This error arises when the knowledge corpus used for\nretrieval is only a subset of the entire string space, potentially excluding\nmore helpful passages that exist outside the corpus. LLMs may mitigate this\nshortcoming by generating passages in a larger space. We come up with an\nexperiment of paraphrasing human-annotated gold context using LLMs to observe\nknowledge corpus error empirically. Our results across three QA benchmarks\nreveal an increased performance (10% - 13%) when using paraphrased passage,\nindicating a signal for the existence of knowledge corpus error. Our code is\navailable at https://github.com/xfactlab/emnlp2023-knowledge-corpus-error",
            "author": [
                "Yejoon Lee",
                "Philhoon Oh",
                "James Thorne"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18076v1",
                "http://arxiv.org/pdf/2310.18076v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18075v4",
            "title": "DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking",
            "updated": "2023-11-24T09:18:27Z",
            "published": "2023-10-27T11:43:46Z",
            "summary": "Inspired by the dual-process theory of human cognition, we introduce DUMA, a\nnovel conversational agent framework that embodies a dual-mind mechanism\nthrough the utilization of two generative Large Language Models (LLMs)\ndedicated to fast and slow thinking respectively. The fast thinking model\nserves as the primary interface for external interactions and initial response\ngeneration, evaluating the necessity for engaging the slow thinking model based\non the complexity of the complete response. When invoked, the slow thinking\nmodel takes over the conversation, engaging in meticulous planning, reasoning,\nand tool utilization to provide a well-analyzed response. This dual-mind\nconfiguration allows for a seamless transition between intuitive responses and\ndeliberate problem-solving processes based on the situation. We have\nconstructed a conversational agent to handle online inquiries in the real\nestate industry. The experiment proves that our method balances effectiveness\nand efficiency, and has a significant improvement compared to the baseline.",
            "author": [
                "Xiaoyu Tian",
                "Liangyu Chen",
                "Na Liu",
                "Yaxuan Liu",
                "Wei Zou",
                "Kaijiang Chen",
                "Ming Cui"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18075v4",
                "http://arxiv.org/pdf/2310.18075v4"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18073v1",
            "title": "A Scalable Framework for Table of Contents Extraction from Complex ESG\n  Annual Reports",
            "updated": "2023-10-27T11:40:32Z",
            "published": "2023-10-27T11:40:32Z",
            "summary": "Table of contents (ToC) extraction centres on structuring documents in a\nhierarchical manner. In this paper, we propose a new dataset, ESGDoc,\ncomprising 1,093 ESG annual reports from 563 companies spanning from 2001 to\n2022. These reports pose significant challenges due to their diverse structures\nand extensive length. To address these challenges, we propose a new framework\nfor Toc extraction, consisting of three steps: (1) Constructing an initial tree\nof text blocks based on reading order and font sizes; (2) Modelling each tree\nnode (or text block) independently by considering its contextual information\ncaptured in node-centric subtree; (3) Modifying the original tree by taking\nappropriate action on each tree node (Keep, Delete, or Move). This\nconstruction-modelling-modification (CMM) process offers several benefits. It\neliminates the need for pairwise modelling of section headings as in previous\napproaches, making document segmentation practically feasible. By incorporating\nstructured information, each section heading can leverage both local and\nlong-distance context relevant to itself. Experimental results show that our\napproach outperforms the previous state-of-the-art baseline with a fraction of\nrunning time. Our framework proves its scalability by effectively handling\ndocuments of any length.",
            "author": [
                "Xinyu Wang",
                "Lin Gui",
                "Yulan He"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18073v1",
                "http://arxiv.org/pdf/2310.18073v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18070v1",
            "title": "Multi-grained Evidence Inference for Multi-choice Reading Comprehension",
            "updated": "2023-10-27T11:36:18Z",
            "published": "2023-10-27T11:36:18Z",
            "summary": "Multi-choice Machine Reading Comprehension (MRC) is a major and challenging\ntask for machines to answer questions according to provided options. Answers in\nmulti-choice MRC cannot be directly extracted in the given passages, and\nessentially require machines capable of reasoning from accurate extracted\nevidence. However, the critical evidence may be as simple as just one word or\nphrase, while it is hidden in the given redundant, noisy passage with multiple\nlinguistic hierarchies from phrase, fragment, sentence until the entire\npassage. We thus propose a novel general-purpose model enhancement which\nintegrates multi-grained evidence comprehensively, named Multi-grained evidence\ninferencer (Mugen), to make up for the inability. Mugen extracts three\ndifferent granularities of evidence: coarse-, middle- and fine-grained\nevidence, and integrates evidence with the original passages, achieving\nsignificant and consistent performance improvement on four multi-choice MRC\nbenchmarks.",
            "author": [
                "Yilin Zhao",
                "Hai Zhao",
                "Sufeng Duan"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TASLP.2023.3313885",
                "http://arxiv.org/abs/2310.18070v1",
                "http://arxiv.org/pdf/2310.18070v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18064v1",
            "title": "New Fast Transform for Orthogonal Frequency Division Multiplexing",
            "updated": "2023-10-27T11:26:42Z",
            "published": "2023-10-27T11:26:42Z",
            "summary": "In this paper, a new fast and low complexity transform is introduced for\northogonal frequency division multiplexing (OFDM) wireless systems. The new\ntransform combines the effects of fast complex-Walsh-Hadamard transform (CHT)\nand the fast Fourier transform (FFT) into a single unitary transform named in\nthis paper as the complex transition transform (CTT). The development of a new\nalgorithm for fast calculation of the CT transform called FCT is found to have\nall the desirable properties such as in-place computation, simple indexing\nscheme and considerably lower arithmetic complexity than existing algorithms.\nFurthermore, a new OFDM system using the FCT algorithm is introduced and its\nperformance has been evaluated. The proposed CT-OFDM achieves a noticeable\nreduction in peak-to-average-power-ratio (PAPR) and a significant improvement\nin the bit-error-rate (BER) performance compared with the conventional OFDM.",
            "author": [
                "Said Boussakta",
                "Mounir T. Hamood",
                "Mohammed Sh. Ahmed"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18064v1",
                "http://arxiv.org/pdf/2310.18064v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18063v1",
            "title": "\"Honey, Tell Me What's Wrong\", Global Explanation of Textual\n  Discriminative Models through Cooperative Generation",
            "updated": "2023-10-27T11:26:27Z",
            "published": "2023-10-27T11:26:27Z",
            "summary": "The ubiquity of complex machine learning has raised the importance of\nmodel-agnostic explanation algorithms. These methods create artificial\ninstances by slightly perturbing real instances, capturing shifts in model\ndecisions. However, such methods rely on initial data and only provide\nexplanations of the decision for these. To tackle these problems, we propose\nTherapy, the first global and model-agnostic explanation method adapted to text\nwhich requires no input dataset. Therapy generates texts following the\ndistribution learned by a classifier through cooperative generation. Because it\ndoes not rely on initial samples, it allows to generate explanations even when\ndata is absent (e.g., for confidentiality reasons). Moreover, conversely to\nexisting methods that combine multiple local explanations into a global one,\nTherapy offers a global overview of the model behavior on the input space. Our\nexperiments show that although using no input data to generate samples, Therapy\nprovides insightful information about features used by the classifier that is\ncompetitive with the ones from methods relying on input samples and outperforms\nthem when input samples are not specific to the studied model.",
            "author": [
                "Antoine Chaffin",
                "Julien Delaunay"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18063v1",
                "http://arxiv.org/pdf/2310.18063v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18062v1",
            "title": "Group Actions from Algebraic Flops",
            "updated": "2023-10-27T11:25:03Z",
            "published": "2023-10-27T11:25:03Z",
            "summary": "This paper constructs derived autoequivalences associated to an algebraic\nflopping contraction \\(X\\to X_{\\con}, \\) where \\(X\\) is quasi-projective with\nonly mild singularities. These functors are constructed naturally using\nbimodule cones, and we prove these cones are locally two-sided tilting\ncomplexes by using the local-global properties and a key commutative diagram.\nThe main result is that these autoequivalences combine to give an action of the\nfundamental group of an associated infinite hyperplane arrangement on the\nderived category of \\(X.\\) This generalises and simplifies \\cite{DW3}, by\nfinally removing reliance on subgroups, and it also lifts many other results\nfrom the complete local setting.",
            "author": [
                "Caroline Namanya"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18062v1",
                "http://arxiv.org/pdf/2310.18062v1"
            ],
            "primary_category": "math.AG",
            "category": [
                "math.AG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18055v2",
            "title": "On some integrable models in inhomogeneous space",
            "updated": "2023-11-08T15:02:55Z",
            "published": "2023-10-27T11:09:33Z",
            "summary": "The purpose of this work is to build a framework that allows for an in-depth\nstudy of various generalisations to inhomogeneous space of models of\nBorodin-Ferrari, Dieker-Warren, Nordenstam, Warren-Windridge of interacting\nparticles in interlacing arrays, both in discrete and continuous time,\ninvolving both Bernoulli and geometric jumps. The models can in addition be\neither time-inhomogeneous or particle-inhomogeneous. We show that the\ncorrelation functions of these models are determinantal and using this we prove\na short-time asymptotic for these dynamics to the discrete Bessel point\nprocess. We moreover prove a number of closely related results. We prove that\nthe autonomous, inhomogeneous in space and time, TASEP-like and pushTASEP-like\nparticle systems on the edges of the array have explicit transition kernels and\nthat from any deterministic initial condition their distributions are marginals\nof a determinantal measure. We prove a novel duality relation between dynamics\nin inhomogeneous space and dynamics with inhomogeneities on the level of the\narray. We extend the work of Nordenstam on the shuffling algorithm for domino\ntilings of the Aztec diamond and its relation to push-block dynamics in\ninterlacing arrays to general weights on the tilings and then connect this, for\na special class of weights, back to our previous results. We also consider\nnon-intersecting walks in inhomogeneous space and time with fixed starting and\nend points and obtain a formula for their correlation functions, involving\namong other ingredients, an explicit Riemann-Hilbert problem. We then prove a\nlimit theorem for the bottom lines in this line-ensemble, under some technical\nconditions. The main computational tool throughout this work is a natural\ngeneralisation of a Toeplitz matrix, that we call inhomogeneous Toeplitz-like\nmatrix $\\mathsf{T}_{\\mathbf{f}}$ with (a possibly matrix-valued) symbol\n$\\mathbf{f}$.",
            "author": [
                "Theodoros Assiotis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18055v2",
                "http://arxiv.org/pdf/2310.18055v2"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math-ph",
                "math.CO",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18053v2",
            "title": "Phylogenetic invariants: straightforward from the general Markov to\n  equivariant models",
            "updated": "2023-11-09T06:48:48Z",
            "published": "2023-10-27T11:03:39Z",
            "summary": "In the last decade, some algebraic tools have been successfully applied to\nphylogenetic reconstruction. These tools are mainly based on the knowledge of\nequations describing algebraic varieties associated to phylogenetic trees\nevolving under Markov processes of molecular substitution, the so called\nphylogenetic invariants. Although the theory involved allows to explicitly\nobtain these equations for all equivariant models (which include some of the\nmost popular nucleotide substitution models), practical uses of these algebraic\ntools have been restricted to the case of the general Markov model. Arguably,\none of the reasons for this restriction is that knowledge of linear\nrepresentation theory is required before making these equations explicit.\n  With the aim of enlarging the practical uses of algebraic phylogenetics, in\nthis paper we prove that phylogenetic invariants for trees evolving under\nequivariant models can be derived from phylogenetic invariants for the general\nMarkov model, without the need of representation theory. Our main result states\nthat the algebraic variety corresponding to a phylogenetic tree evolving under\nan equivariant model is an irreducible component of the variety corresponding\nto the same tree under the general Markov model cut with the linear space\ndefined by the model. We also prove that, for any equivariant model, those\nphylogenetic invariants that are relevant for practical uses (e.g. tree\nreconstruction) can be simply deduced from a single rank constraint on the\nmatrices obtained by flattening the joint distribution at the leaves of the\ntree. This condition can be easily tested from singular values of the matrices\nand extends our results from trees to phylogenetic networks.",
            "author": [
                "Marta Casanellas",
                "Jes\u00fas Fern\u00e1ndez-S\u00e1nchez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18053v2",
                "http://arxiv.org/pdf/2310.18053v2"
            ],
            "primary_category": "q-bio.PE",
            "category": [
                "q-bio.PE",
                "math.AG",
                "14J99, 92D15, 05C85, 62R01"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18052v1",
            "title": "Economics for the Global Economic Order: The Tragedy of Epic Fail\n  Equilibria",
            "updated": "2023-10-27T11:00:44Z",
            "published": "2023-10-27T11:00:44Z",
            "summary": "This paper casts within a unified economic framework some key challenges for\nthe global economic order: de-globalization; the rising impracticability of\nglobal cooperation; and the increasingly confrontational nature of Great Power\ncompetition. In these, economics has been weaponised in the service of national\ninterest. This need be no bad thing. History provides examples where greater\nopenness and freer trade emerge from nations seeking only to advance their own\nself-interests. But the cases described in the paper provide mixed signals. We\nfind that some developments do draw on a growing zero-sum perception to\neconomic and political engagement. That zero-sum explanation alone, however, is\ncrucially inadequate. Self-serving nations, even when believing the world\nzero-sum, have under certain circumstances produced outcomes that have\nbenefited all. In other circumstances, perfectly-predicted losses have instead\nresulted on all sides. Such lose-lose outcomes -- epic fail equilibria --\ngeneralize the Prisoner's Dilemma game and are strictly worse than zero-sum. In\nour analysis, Third Nations -- those not frontline in Great Power rivalry --\ncan serve an essential role in averting epic fail outcomes. The policy\nimplication is that Third Nations need to provide platforms that will gently\nand unobtrusively nudge Great Powers away from epic-fail equilibria and towards\ninadvertent cooperation.",
            "author": [
                "Shiro Armstrong",
                "Danny Quah"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18052v1",
                "http://arxiv.org/pdf/2310.18052v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18049v1",
            "title": "Text Augmented Spatial-aware Zero-shot Referring Image Segmentation",
            "updated": "2023-10-27T10:52:50Z",
            "published": "2023-10-27T10:52:50Z",
            "summary": "In this paper, we study a challenging task of zero-shot referring image\nsegmentation. This task aims to identify the instance mask that is most related\nto a referring expression without training on pixel-level annotations. Previous\nresearch takes advantage of pre-trained cross-modal models, e.g., CLIP, to\nalign instance-level masks with referring expressions. %Yet, CLIP only\nconsiders image-text pair level alignment, which neglects fine-grained image\nregion and complex sentence matching. Yet, CLIP only considers the global-level\nalignment of image-text pairs, neglecting fine-grained matching between the\nreferring sentence and local image regions. To address this challenge, we\nintroduce a Text Augmented Spatial-aware (TAS) zero-shot referring image\nsegmentation framework that is training-free and robust to various visual\nencoders. TAS incorporates a mask proposal network for instance-level mask\nextraction, a text-augmented visual-text matching score for mining the\nimage-text correlation, and a spatial rectifier for mask post-processing.\nNotably, the text-augmented visual-text matching score leverages a $P$ score\nand an $N$-score in addition to the typical visual-text matching score. The\n$P$-score is utilized to close the visual-text domain gap through a surrogate\ncaptioning model, where the score is computed between the surrogate\nmodel-generated texts and the referring expression. The $N$-score considers the\nfine-grained alignment of region-text pairs via negative phrase mining,\nencouraging the masked image to be repelled from the mined distracting phrases.\nExtensive experiments are conducted on various datasets, including RefCOCO,\nRefCOCO+, and RefCOCOg. The proposed method clearly outperforms\nstate-of-the-art zero-shot referring image segmentation methods.",
            "author": [
                "Yucheng Suo",
                "Linchao Zhu",
                "Yi Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18049v1",
                "http://arxiv.org/pdf/2310.18049v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18046v1",
            "title": "ViCLEVR: A Visual Reasoning Dataset and Hybrid Multimodal Fusion Model\n  for Visual Question Answering in Vietnamese",
            "updated": "2023-10-27T10:44:50Z",
            "published": "2023-10-27T10:44:50Z",
            "summary": "In recent years, Visual Question Answering (VQA) has gained significant\nattention for its diverse applications, including intelligent car assistance,\naiding visually impaired individuals, and document image information retrieval\nusing natural language queries. VQA requires effective integration of\ninformation from questions and images to generate accurate answers. Neural\nmodels for VQA have made remarkable progress on large-scale datasets, with a\nprimary focus on resource-rich languages like English. To address this, we\nintroduce the ViCLEVR dataset, a pioneering collection for evaluating various\nvisual reasoning capabilities in Vietnamese while mitigating biases. The\ndataset comprises over 26,000 images and 30,000 question-answer pairs (QAs),\neach question annotated to specify the type of reasoning involved. Leveraging\nthis dataset, we conduct a comprehensive analysis of contemporary visual\nreasoning systems, offering valuable insights into their strengths and\nlimitations. Furthermore, we present PhoVIT, a comprehensive multimodal fusion\nthat identifies objects in images based on questions. The architecture\neffectively employs transformers to enable simultaneous reasoning over textual\nand visual data, merging both modalities at an early model stage. The\nexperimental findings demonstrate that our proposed model achieves\nstate-of-the-art performance across four evaluation metrics. The accompanying\ncode and dataset have been made publicly accessible at\n\\url{https://github.com/kvt0012/ViCLEVR}. This provision seeks to stimulate\nadvancements within the research community, fostering the development of more\nmultimodal fusion algorithms, specifically tailored to address the nuances of\nlow-resource languages, exemplified by Vietnamese.",
            "author": [
                "Khiem Vinh Tran",
                "Hao Phu Phan",
                "Kiet Van Nguyen",
                "Ngan Luu Thuy Nguyen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18046v1",
                "http://arxiv.org/pdf/2310.18046v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18042v1",
            "title": "Sui Lutris: A Blockchain Combining Broadcast and Consensus",
            "updated": "2023-10-27T10:40:11Z",
            "published": "2023-10-27T10:40:11Z",
            "summary": "Sui Lutris is the first smart-contract platform to sustainably achieve\nsub-second finality. It achieves this significant decrease in latency by\nemploying consensusless agreement not only for simple payments but for a large\nvariety of transactions. Unlike prior work, Sui Lutris neither compromises\nexpressiveness nor throughput and can run perpetually without restarts. Sui\nLutris achieves this by safely integrating consensuless agreement with a\nhigh-throughput consensus protocol that is invoked out of the critical finality\npath but makes sure that when a transaction is at risk of inconsistent\nconcurrent accesses its settlement is delayed until the total ordering is\nresolved. Building such a hybrid architecture is especially delicate during\nreconfiguration events, where the system needs to preserve the safety of the\nconsensusless path without compromising the long-term liveness of potentially\nmisconfigured clients. We thus develop a novel reconfiguration protocol, the\nfirst to show the safe and efficient reconfiguration of a consensusless\nblockchain. Sui Lutris is currently running in production as part of a major\nsmart-contract platform. Combined with the Move Programming language it enables\nthe safe execution of smart contracts that expose objects as a first-class\nresource. In our experiments Sui Lutris achieves latency lower than 0.5 seconds\nfor throughput up to 5,000 certificates per second (150k ops/s with bundling),\ncompared to the state-of-the-art real-world consensus latencies of 3 seconds.\nFurthermore, it gracefully handles validators crash-recovery and does not\nsuffer visible performance degradation during reconfiguration.",
            "author": [
                "Same Blackshear",
                "Andrey Chursin",
                "George Danezis",
                "Anastasios Kichidis",
                "Lefteris Kokoris-Kogias",
                "Xun Li",
                "Mark Logan",
                "Ashok Menon",
                "Todd Nowacki",
                "Alberto Sonnino",
                "Brandon Williams",
                "Lu Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18042v1",
                "http://arxiv.org/pdf/2310.18042v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18038v1",
            "title": "On General Language Understanding",
            "updated": "2023-10-27T10:36:54Z",
            "published": "2023-10-27T10:36:54Z",
            "summary": "Natural Language Processing prides itself to be an empirically-minded, if not\noutright empiricist field, and yet lately it seems to get itself into\nessentialist debates on issues of meaning and measurement (\"Do Large Language\nModels Understand Language, And If So, How Much?\"). This is not by accident:\nHere, as everywhere, the evidence underspecifies the understanding. As a\nremedy, this paper sketches the outlines of a model of understanding, which can\nground questions of the adequacy of current methods of measurement of model\nquality. The paper makes three claims: A) That different language use situation\ntypes have different characteristics, B) That language understanding is a\nmultifaceted phenomenon, bringing together individualistic and social\nprocesses, and C) That the choice of Understanding Indicator marks the limits\nof benchmarking, and the beginnings of considerations of the ethics of NLP use.",
            "author": [
                "David Schlangen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18038v1",
                "http://arxiv.org/pdf/2310.18038v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.06280v1",
            "title": "A Data-driven Deep Learning Approach for Bitcoin Price Forecasting",
            "updated": "2023-10-27T10:35:47Z",
            "published": "2023-10-27T10:35:47Z",
            "summary": "Bitcoin as a cryptocurrency has been one of the most important digital coins\nand the first decentralized digital currency. Deep neural networks, on the\nother hand, has shown promising results recently; however, we require huge\namount of high-quality data to leverage their power. There are some techniques\nsuch as augmentation that can help us with increasing the dataset size, but we\ncannot exploit them on historical bitcoin data. As a result, we propose a\nshallow Bidirectional-LSTM (Bi-LSTM) model, fed with feature engineered data\nusing our proposed method to forecast bitcoin closing prices in a daily time\nframe. We compare the performance with that of other forecasting methods, and\nshow that with the help of the proposed feature engineering method, a shallow\ndeep neural network outperforms other popular price forecasting models.",
            "author": [
                "Parth Daxesh Modi",
                "Kamyar Arshi",
                "Pertami J. Kunz",
                "Abdelhak M. Zoubir"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06280v1",
                "http://arxiv.org/pdf/2311.06280v1"
            ],
            "primary_category": "q-fin.ST",
            "category": [
                "q-fin.ST",
                "cs.AI",
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18036v1",
            "title": "Fast and simple unrooted dynamic forests",
            "updated": "2023-10-27T10:28:24Z",
            "published": "2023-10-27T10:28:24Z",
            "summary": "A dynamic forest data structure maintains a forest (and associated data like\nedge weights) under edge insertions and deletions. Dynamic forests are widely\nused to solve online and offline graph problems. Well-known examples of dynamic\nforest data structures are link-cut trees [Sleator and Tarjan '83] and top\ntrees [Alstrup, Holm, de Lichtenberg, and Thorup '05], both of which need O(log\nn) time per operation. While top trees are more flexible and arguably easier to\nuse, link-cut trees are faster in practice [Tarjan and Werneck '10].\n  In this paper, we propose an alternative to link-cut trees. Our data\nstructure is based on search trees on trees (STTs, also known as elimination\ntrees) and an STT algorithm [Berendsohn and Kozma '22] based on the classical\nSplay trees [Sleator and Tarjan '85]. While link-cut trees maintain a hierarchy\nof binary search trees, we maintain a single STT. Most of the complexity of our\ndata structure lies in the implementation of the STT rotation primitive, which\ncan easily be reused, simplifying the development of new STT-based approaches.\n  We implement several variants of our data structure in the Rust programming\nlanguage, along with an implementation of link-cut trees for comparison.\nExperimental evaluation suggests that our algorithms are faster when the\ndynamic forest is unrooted, while link-cut trees are faster for rooted dynamic\nforests.",
            "author": [
                "Benjamin Aram Berendsohn"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18036v1",
                "http://arxiv.org/pdf/2310.18036v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18026v1",
            "title": "Symmetry-Based Quantum Circuit Mapping",
            "updated": "2023-10-27T10:04:34Z",
            "published": "2023-10-27T10:04:34Z",
            "summary": "Quantum circuit mapping is a crucial process in the quantum circuit\ncompilation pipeline, facilitating the transformation of a logical quantum\ncircuit into a list of instructions directly executable on a target quantum\nsystem. Recent research has introduced a post-compilation step known as\nremapping, which seeks to reconfigure the initial circuit mapping to mitigate\nquantum circuit errors arising from system variability. As quantum processors\ncontinue to scale in size, the efficiency of quantum circuit mapping and the\noverall compilation process has become of paramount importance. In this work,\nwe introduce a quantum circuit remapping algorithm that leverages the intrinsic\nsymmetries in quantum processors, making it well-suited for large-scale quantum\nsystems. This algorithm identifies all topologically equivalent circuit\nmappings by constraining the search space using symmetries and accelerates the\nscoring of each mapping using vector computation. Notably, this symmetry-based\ncircuit remapping algorithm exhibits linear scaling with the number of qubits\nin the target quantum hardware and is proven to be optimal in terms of its time\ncomplexity. Moreover, we conduct a comparative analysis against existing\nmethods in the literature, demonstrating the superior performance of our\nsymmetry-based method on state-of-the-art quantum hardware architectures and\nhighlighting the practical utility of our algorithm, particularly for quantum\nprocessors with millions of qubits.",
            "author": [
                "Di Yu",
                "Kun Fang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18026v1",
                "http://arxiv.org/pdf/2310.18026v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.PL",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18025v1",
            "title": "Large language models for aspect-based sentiment analysis",
            "updated": "2023-10-27T10:03:21Z",
            "published": "2023-10-27T10:03:21Z",
            "summary": "Large language models (LLMs) offer unprecedented text completion\ncapabilities. As general models, they can fulfill a wide range of roles,\nincluding those of more specialized models. We assess the performance of GPT-4\nand GPT-3.5 in zero shot, few shot and fine-tuned settings on the aspect-based\nsentiment analysis (ABSA) task. Fine-tuned GPT-3.5 achieves a state-of-the-art\nF1 score of 83.8 on the joint aspect term extraction and polarity\nclassification task of the SemEval-2014 Task 4, improving upon InstructABSA\n[@scaria_instructabsa_2023] by 5.7%. However, this comes at the price of 1000\ntimes more model parameters and thus increased inference cost. We discuss the\nthe cost-performance trade-offs of different models, and analyze the typical\nerrors that they make. Our results also indicate that detailed prompts improve\nperformance in zero-shot and few-shot settings but are not necessary for\nfine-tuned models. This evidence is relevant for practioners that are faced\nwith the choice of prompt engineering versus fine-tuning when using LLMs for\nABSA.",
            "author": [
                "Paul F. Simmering",
                "Paavo Huoviala"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18025v1",
                "http://arxiv.org/pdf/2310.18025v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18387v2",
            "title": "OffMix-3L: A Novel Code-Mixed Dataset in Bangla-English-Hindi for\n  Offensive Language Identification",
            "updated": "2023-11-25T13:13:01Z",
            "published": "2023-10-27T09:59:35Z",
            "summary": "Code-mixing is a well-studied linguistic phenomenon when two or more\nlanguages are mixed in text or speech. Several works have been conducted on\nbuilding datasets and performing downstream NLP tasks on code-mixed data.\nAlthough it is not uncommon to observe code-mixing of three or more languages,\nmost available datasets in this domain contain code-mixed data from only two\nlanguages. In this paper, we introduce OffMix-3L, a novel offensive language\nidentification dataset containing code-mixed data from three different\nlanguages. We experiment with several models on this dataset and observe that\nBanglishBERT outperforms other transformer-based models and GPT-3.5.",
            "author": [
                "Dhiman Goswami",
                "Md Nishat Raihan",
                "Antara Mahmud",
                "Antonios Anastasopoulos",
                "Marcos Zampieri"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18387v2",
                "http://arxiv.org/pdf/2310.18387v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18023v2",
            "title": "SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment\n  Analysis",
            "updated": "2023-11-29T10:33:26Z",
            "published": "2023-10-27T09:59:24Z",
            "summary": "Code-mixing is a well-studied linguistic phenomenon when two or more\nlanguages are mixed in text or speech. Several datasets have been build with\nthe goal of training computational models for code-mixing. Although it is very\ncommon to observe code-mixing with multiple languages, most datasets available\ncontain code-mixed between only two languages. In this paper, we introduce\nSentMix-3L, a novel dataset for sentiment analysis containing code-mixed data\nbetween three languages Bangla, English, and Hindi. We carry out a\ncomprehensive evaluation using SentMix-3L. We show that zero-shot prompting\nwith GPT-3.5 outperforms all transformer-based models on SentMix-3L.",
            "author": [
                "Md Nishat Raihan",
                "Dhiman Goswami",
                "Antara Mahmud",
                "Antonios Anastasopoulos",
                "Marcos Zampieri"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18023v2",
                "http://arxiv.org/pdf/2310.18023v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18021v3",
            "title": "FormalGeo: The First Step Toward Human-like IMO-level Geometric\n  Automated Reasoning",
            "updated": "2023-11-28T07:00:35Z",
            "published": "2023-10-27T09:55:12Z",
            "summary": "This is the first paper in a series of work we have accomplished over the\npast three years. In this paper, we have constructed a complete and compatible\nformal plane geometry system. This will serve as a crucial bridge between\nIMO-level plane geometry challenges and readable AI automated reasoning. Within\nthis formal framework, we have been able to seamlessly integrate modern AI\nmodels with our formal system. AI is now capable of providing deductive\nreasoning solutions to IMO-level plane geometry problems, just like handling\nother natural languages, and these proofs are readable, traceable, and\nverifiable. We propose the geometry formalization theory (GFT) to guide the\ndevelopment of the geometry formal system. Based on the GFT, we have\nestablished the FormalGeo, which consists of 88 geometric predicates and 196\ntheorems. It can represent, validate, and solve IMO-level geometry problems. we\nalso have crafted the FGPS (formal geometry problem solver) in Python. It\nserves as both an interactive assistant for verifying problem-solving processes\nand an automated problem solver. We've annotated the formalgeo7k and\nformalgeo-imo datasets. The former contains 6,891 (expand to 133,818 through\ndata augmentation) geometry problems, while the latter includes 18 (expand to\n2,627 and continuously increasing) IMO-level challenging geometry problems. All\nannotated problems include detailed formal language descriptions and solutions.\nImplementation of the formal system and experiments validate the correctness\nand utility of the GFT. The backward depth-first search method only yields a\n2.42% problem-solving failure rate, and we can incorporate deep learning\ntechniques to achieve lower one. The source code of FGPS and datasets are\navailable at https://github.com/BitSecret/FGPS.",
            "author": [
                "Xiaokai Zhang",
                "Na Zhu",
                "Yiming He",
                "Jia Zou",
                "Qike Huang",
                "Xiaoxiao Jin",
                "Yanjun Guo",
                "Chenyang Mao",
                "Zhe Zhu",
                "Dengfeng Yue",
                "Fangzhen Zhu",
                "Yang Li",
                "Yifan Wang",
                "Yiwen Huang",
                "Runan Wang",
                "Cheng Qin",
                "Zhenbing Zeng",
                "Shaorong Xie",
                "Xiangfeng Luo",
                "Tuo Leng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18021v3",
                "http://arxiv.org/pdf/2310.18021v3"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18019v1",
            "title": "Temperature Monitoring of Agricultural Areas in a Secure Data Room",
            "updated": "2023-10-27T09:49:52Z",
            "published": "2023-10-27T09:49:52Z",
            "summary": "Agricultural production is highly dependent on naturally occurring\nenvironmental conditions like change of seasons and the weather. Especially in\nfruit and wine growing, late frosts occurring shortly after the crops have\nsprouted have the potential to cause massive damage to plants [L1,L2] [1]. In\nthis article we present a cost-efficient temperature monitoring system for\ndetecting and reacting to late frosts to prevent crop failures. The proposed\nsolution includes a data space where Internet of Things (IoT) devices can form\na cyber-physical system (CPS) to interact with their nearby environment and\nsecurely exchange data. Based on this data, more accurate predictions can be\nmade in the future using machine learning (ML), which will further contribute\nto minimising economic damage caused by crop failures.",
            "author": [
                "Thomas Ederer",
                "Martin Ivancsits",
                "Igor Ivki\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18019v1",
                "http://arxiv.org/pdf/2310.18019v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18018v1",
            "title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination\n  for each Benchmark",
            "updated": "2023-10-27T09:48:29Z",
            "published": "2023-10-27T09:48:29Z",
            "summary": "In this position paper, we argue that the classical evaluation on Natural\nLanguage Processing (NLP) tasks using annotated benchmarks is in trouble. The\nworst kind of data contamination happens when a Large Language Model (LLM) is\ntrained on the test split of a benchmark, and then evaluated in the same\nbenchmark. The extent of the problem is unknown, as it is not straightforward\nto measure. Contamination causes an overestimation of the performance of a\ncontaminated model in a target benchmark and associated task with respect to\ntheir non-contaminated counterparts. The consequences can be very harmful, with\nwrong scientific conclusions being published while other correct ones are\ndiscarded. This position paper defines different levels of data contamination\nand argues for a community effort, including the development of automatic and\nsemi-automatic measures to detect when data from a benchmark was exposed to a\nmodel, and suggestions for flagging papers with conclusions that are\ncompromised by data contamination.",
            "author": [
                "Oscar Sainz",
                "Jon Ander Campos",
                "Iker Garc\u00eda-Ferrero",
                "Julen Etxaniz",
                "Oier Lopez de Lacalle",
                "Eneko Agirre"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18018v1",
                "http://arxiv.org/pdf/2310.18018v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18012v1",
            "title": "Vision-Based Reconfigurable Intelligent Surface Beam Tracking for mmWave\n  Communications",
            "updated": "2023-10-27T09:40:09Z",
            "published": "2023-10-27T09:40:09Z",
            "summary": "Reconfigurable intelligent surfaces have emerged as a technology with the\npotential to enhance wireless communication performance for 5G and beyond.\nHowever, the technology comes with challenges in areas such as complexity,\npower consumption, and cost. This paper demonstrates a computer vision-based\nreconfigurable intelligent surface beamforming algorithm that addresses\ncomplexity and cost issues and analyzes the multipath components that arise\nfrom the insertion of such a device into the wireless channel. The results show\nthat a reconfigurable intelligent surface can provide an additional multipath\ncomponent. The power of this additional path can be critical in blockage\nscenarios, and a capacity increase can be perceived in both line-of-sight and\nnon line-of-sight scenarios.",
            "author": [
                "Juan Sanchez",
                "Xuesong Cai",
                "Fredrik Tufvesson"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18012v1",
                "http://arxiv.org/pdf/2310.18012v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18011v1",
            "title": "Data journeys in popular science: Producing climate change and COVID-19\n  data visualizations at Scientific American",
            "updated": "2023-10-27T09:39:06Z",
            "published": "2023-10-27T09:39:06Z",
            "summary": "Vast amounts of (open) data are increasingly used to make arguments about\ncrisis topics such as climate change and global pandemics. Data visualizations\nare central to bringing these viewpoints to broader publics. However,\nvisualizations often conceal the many contexts involved in their production,\nranging from decisions made in research labs about collecting and sharing data\nto choices made in editorial rooms about which data stories to tell. In this\npaper, we examine how data visualizations about climate change and COVID-19 are\nproduced in popular science magazines, using Scientific American, an\nestablished English-language popular science magazine, as a case study. To do\nthis, we apply the analytical concept of \"data journeys\" (Leonelli, 2020) in a\nmixed methods study that centers on interviews with Scientific American staff\nand is supplemented by a visualization analysis of selected charts. In\nparticular, we discuss the affordances of working with open data, the role of\ncollaborative data practices, and how the magazine works to counter\nmisinformation and increase transparency. This work provides a theoretical\ncontribution by testing and expanding the concept of data journeys as an\nanalytical framework, as well as practical contributions by providing insight\ninto the data (visualization) practices of science communicators.",
            "author": [
                "Kathleen Gregory",
                "Laura Koesten",
                "Regina Schuster",
                "Torsten M\u00f6ller",
                "Sarah Davies"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18011v1",
                "http://arxiv.org/pdf/2310.18011v1"
            ],
            "primary_category": "cs.DL",
            "category": [
                "cs.DL",
                "cs.HC",
                "physics.pop-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18010v1",
            "title": "The 7 faces of quantum NP",
            "updated": "2023-10-27T09:36:11Z",
            "published": "2023-10-27T09:36:11Z",
            "summary": "When it comes to NP, its natural definition, its wide applicability across\nscientific disciplines, and its timeless relevance, the writing is on the wall:\nThere can be only one. Quantum NP, on the other hand, is clearly the apple that\nfell far from the tree of NP. Two decades since the first definitions of\nquantum NP started rolling in, quantum complexity theorists face a stark\nreality: There's QMA, QCMA, QMA1, QMA(2), StoqMA, and NQP. In this article\naimed at a general theoretical computer science audience, I survey these\nvarious definitions of quantum NP, their strengths and weaknesses, and why most\nof them, for better or worse, actually appear to fit naturally into the\ncomplexity zoo.",
            "author": [
                "Sevag Gharibian"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18010v1",
                "http://arxiv.org/pdf/2310.18010v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18009v1",
            "title": "ProcNet: Deep Predictive Coding Model for Robust-to-occlusion Visual\n  Segmentation and Pose Estimation",
            "updated": "2023-10-27T09:34:57Z",
            "published": "2023-10-27T09:34:57Z",
            "summary": "Systems involving human-robot collaboration necessarily require that steps be\ntaken to ensure safety of the participating human. This is usually achievable\nif accurate, reliable estimates of the human's pose are available. In this\npaper, we present a deep Predictive Coding (PC) model supporting visual\nsegmentation, which we extend to pursue pose estimation. The model is designed\nto offer robustness to the type of transient occlusion naturally occurring when\nhuman and robot are operating in close proximity to one another. Impact on\nperformance of relevant model parameters is assessed, and comparison to an\nalternate pose estimation model (NVIDIA's PoseCNN) illustrates efficacy of the\nproposed approach.",
            "author": [
                "Michael Zechmair",
                "Alban Bornet",
                "Yannick Morel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18009v1",
                "http://arxiv.org/pdf/2310.18009v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18007v1",
            "title": "LHAASO J2108+5157 as a Molecular Cloud Illuminated by a Supernova\n  Remnant",
            "updated": "2023-10-27T09:33:21Z",
            "published": "2023-10-27T09:33:21Z",
            "summary": "The search for Galactic PeVatrons - astrophysical accelerators of cosmic rays\nto PeV energies - has entered a new phase in recent years with the discovery of\nthe first Ultra-High-Energy (UHE, $E>100$ TeV) gamma-ray sources by the HAWC\nand LHAASO experiments. Establishing whether the emission is leptonic or\nhadronic in nature, however, requires multiwavelength data and modelling\nstudies. Among the currently known UHE sources, LHAASO J2108+5157 is an\nenigmatic source without clear association to a plausible accelerator, yet\nspatially coincident with molecular clouds. We investigate the scenario of a\nmolecular cloud illuminated by cosmic rays accelerated in a nearby supernova\nremnant (SNR) as an explanation for LHAASO J2108+5157. We aim to constrain the\nrequired properties of the SNR as well as which of the clouds identified in the\nvicinity is the most likely association. We use a model for cosmic ray\nacceleration in SNRs, their transport through the interstellar medium and\nsubsequent interaction with molecular material, to predict the corresponding\ngamma-ray emission. The parameter space of SNR properties is explored to find\nthe most plausible parameter combination that can account for the gamma-ray\nspectrum of LHAASO J2108+5157. In the case that a SNR is illuminating the\ncloud, we find that it must be young ($<10$ kyr) and located within $40-60$ pc\nof the cloud. A SN scenario with a low Sedov time is preferred, with a maximum\nproton energy of 3 PeV assumed. No SNRs matching these properties are currently\nknown, although an as yet undetected SNR remains feasible. The galactic CR sea\nis insufficient to solely account for the observed flux, such that a PeVatron\naccelerator must be present in the vicinity.",
            "author": [
                "A. M. W. Mitchell"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18007v1",
                "http://arxiv.org/pdf/2310.18007v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18004v1",
            "title": "Text2Bundle: Towards Personalized Query-based Bundle Generation",
            "updated": "2023-10-27T09:24:38Z",
            "published": "2023-10-27T09:24:38Z",
            "summary": "Bundle generation aims to provide a bundle of items for the user, and has\nbeen widely studied and applied on online service platforms. Existing bundle\ngeneration methods mainly utilized user's preference from historical\ninteractions in common recommendation paradigm, and ignored the potential\ntextual query which is user's current explicit intention. There can be a\nscenario in which a user proactively queries a bundle with some natural\nlanguage description, the system should be able to generate a bundle that\nexactly matches the user's intention through the user's query and preferences.\nIn this work, we define this user-friendly scenario as Query-based Bundle\nGeneration task and propose a novel framework Text2Bundle that leverages both\nthe user's short-term interests from the query and the user's long-term\npreferences from the historical interactions. Our framework consists of three\nmodules: (1) a query interest extractor that mines the user's fine-grained\ninterests from the query; (2) a unified state encoder that learns the current\nbundle context state and the user's preferences based on historical interaction\nand current query; and (3) a bundle generator that generates personalized and\ncomplementary bundles using a reinforcement learning with specifically designed\nrewards. We conduct extensive experiments on three real-world datasets and\ndemonstrate the effectiveness of our framework compared with several\nstate-of-the-art methods.",
            "author": [
                "Shixuan Zhu",
                "Chuan Cui",
                "JunTong Hu",
                "Qi Shen",
                "Yu Ji",
                "Zhihua Wei"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18004v1",
                "http://arxiv.org/pdf/2310.18004v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18002v1",
            "title": "Radio outburst from a massive (proto)star. II. A portrait in space and\n  time of the expanding radio jet from S255 NIRS3",
            "updated": "2023-10-27T09:17:21Z",
            "published": "2023-10-27T09:17:21Z",
            "summary": "Observations indicate that the accretion process in star formation may occur\nthrough accretion outbursts. This phenomenon has also now been detected in a\nfew young massive (proto)stars (>8 Msun). The recent outburst at radio\nwavelengths of the massive (proto)star S255 NIRS3 has been interpreted by us as\nexpansion of a thermal jet, fed by the infalling material. To follow up on our\nprevious study and confirm our interpretation, we monitored the source for more\nthan 1 yr in six bands from 1.5 GHz to 45.5 GHz and, after ~1.5 yr, with the\nAtacama Large Millimeter/submillimeter Array at two epochs, which made it\npossible to detect the proper motions of the jet lobes. The prediction of our\nprevious study is confirmed by the new results. The radio jet is found to\nexpand, while the flux, after an initial exponential increase, appears to\nstabilise and eventually decline. The radio flux measured during our monitoring\nis attributed to a single NE lobe, However, from 2019 a second lobe has been\nemerging to the SW, probably powered by the same accretion outburst, although\nwith a delay of at least a couple of years. Flux densities at >6 GHz were\nsatisfactorily fitted with a jet model, whereas those below 6 GHz are clearly\nunderestimated by the model. This indicates that non-thermal emission becomes\ndominant at long wavelengths. Our results suggest that thermal jets can be a\ndirect consequence of accretion events, when yearly flux variations are\ndetected. The end of the accretion outburst is mirrored in the radio jet, as ~1\nyr after the onset of the radio outburst, the inner radius of the jet began to\nincrease while the jet mass stopped growing, as expected if the powering\nmechanism of the jet is quenched. Our findings support a tight connection\nbetween accretion and ejection in massive stars, consistent with a formation\nprocess involving a disk-jet system similar to that of low-mass stars.",
            "author": [
                "R. Cesaroni",
                "L. Moscadelli",
                "A. Caratti o Garatti",
                "J. Eisloeffel",
                "R. Fedriani",
                "R. Neri",
                "T. Ray",
                "A. Sanna",
                "B. Stecklum"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18002v1",
                "http://arxiv.org/pdf/2310.18002v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17997v1",
            "title": "Deep Learning Enables Large Depth-of-Field Images for\n  Sub-Diffraction-Limit Scanning Superlens Microscopy",
            "updated": "2023-10-27T09:16:56Z",
            "published": "2023-10-27T09:16:56Z",
            "summary": "Scanning electron microscopy (SEM) is indispensable in diverse applications\nranging from microelectronics to food processing because it provides large\ndepth-of-field images with a resolution beyond the optical diffraction limit.\nHowever, the technology requires coating conductive films on insulator samples\nand a vacuum environment. We use deep learning to obtain the mapping\nrelationship between optical super-resolution (OSR) images and SEM domain\nimages, which enables the transformation of OSR images into SEM-like large\ndepth-of-field images. Our custom-built scanning superlens microscopy (SSUM)\nsystem, which requires neither coating samples by conductive films nor a vacuum\nenvironment, is used to acquire the OSR images with features down to ~80 nm.\nThe peak signal-to-noise ratio (PSNR) and structural similarity index measure\nvalues indicate that the deep learning method performs excellently in\nimage-to-image translation, with a PSNR improvement of about 0.74 dB over the\noptical super-resolution images. The proposed method provides a high level of\ndetail in the reconstructed results, indicating that it has broad applicability\nto chip-level defect detection, biological sample analysis, forensics, and\nvarious other fields.",
            "author": [
                "Hui Sun",
                "Hao Luo",
                "Feifei Wang",
                "Qingjiu Chen",
                "Meng Chen",
                "Xiaoduo Wang",
                "Haibo Yu",
                "Guanglie Zhang",
                "Lianqing Liu",
                "Jianping Wang",
                "Dapeng Wu",
                "Wen Jung Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17997v1",
                "http://arxiv.org/pdf/2310.17997v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "cs.AI",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17993v1",
            "title": "Ultrafast switchable spin-orbit coupling for silicon spin qubits via\n  spin valves",
            "updated": "2023-10-27T09:06:17Z",
            "published": "2023-10-27T09:06:17Z",
            "summary": "Recent experimental breakthroughs, particularly for single-qubit and\ntwo-qubit gates exceeding the error correction threshold, highlight silicon\nspin qubits as leading candidates for fault-tolerant quantum computation. In\nthe existing architecture, intrinsic or synthetic spin-orbit coupling (SOC) is\ncritical in various aspects, including electrical control, addressability,\nscalability, etc. However, the high-fidelity SWAP operation and quantum state\ntransfer (QST) between spin qubits, crucial for qubit-qubit connectivity,\nrequire the switchable nature of SOC which is rarely considered. Here, we\npropose a flexible architecture based on spin valves by electrically changing\nits magnetization orientation within sub-nanoseconds to generate ultrafast\nswitchable SOC. Based on the switchable SOC architecture, both SWAP operation\nof neighbor spin qubits and resonant QST between distant spins can be realized\nwith fidelity exceeding 99% while considering the realistic experimental\nparameters. Benefiting from the compatible processes with the modern\nsemiconductor industry and experimental advances in spin valves and spin\nqubits, our results pave the way for future construction of silicon-based\nquantum chips.",
            "author": [
                "Ranran Cai",
                "Fang-Ge Li",
                "Bao-Chuan Wang",
                "Hai-Ou Li",
                "Gang Cao",
                "Guo-Ping Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17993v1",
                "http://arxiv.org/pdf/2310.17993v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17991v1",
            "title": "Effects of impurity band on multiphoton photocurrent from InGaN and GaN\n  photodetectors",
            "updated": "2023-10-27T09:03:42Z",
            "published": "2023-10-27T09:03:42Z",
            "summary": "Multiphoton absorption of wide band-gap semiconductors has shown great\nprospects in many fundamental researches and practical applications. With\nintensity-modulated femtosecond lasers by acousto-optic frequency shifters,\nphotocurrents and yellow luminescence induced by two-photon absorption of InGaN\nand GaN photodetectors are investigated experimentally. Photocurrent from InGaN\ndetector shows nearly perfect quadratic dependence on excitation intensity,\nwhile that in GaN detector shows cubic and higher order dependence. Yellow\nluminescence from both detectors show sub-quadratic dependence on excitation\nintensity. Highly nonlinear photocurrent from GaN is ascribed to absorption of\nadditional photons by long-lived electrons in traps and impurity bands. Our\ninvestigation indicates that InGaN can serve as a superior detector for\nmultiphoton absorption, absent of linear and higher order process, while GaN,\nwhich suffers from absorption by trapped electrons and impurity bands, must be\nused with caution.",
            "author": [
                "Chuanliang Wang",
                "Ahsan Ali",
                "Jinlei Wu",
                "Wei Huang",
                "Hai Lu",
                "Khadga Jung Karki"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17991v1",
                "http://arxiv.org/pdf/2310.17991v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17988v1",
            "title": "SCAN-MUSIC: An Efficient Super-resolution Algorithm for Single Snapshot\n  Wide-band Line Spectral Estimation",
            "updated": "2023-10-27T09:01:15Z",
            "published": "2023-10-27T09:01:15Z",
            "summary": "We propose an efficient algorithm for reconstructing one-dimensional\nwide-band line spectra from their Fourier data in a bounded interval\n$[-\\Omega,\\Omega]$. While traditional subspace methods such as MUSIC achieve\nsuper-resolution for closely separated line spectra, their computational cost\nis high, particularly for wide-band line spectra. To address this issue, we\nproposed a scalable algorithm termed SCAN-MUSIC that scans the spectral domain\nusing a fixed Gaussian window and then reconstructs the line spectra falling\ninto the window at each time. For line spectra with cluster structure, we\nfurther refine the proposed algorithm using the annihilating filter technique.\nBoth algorithms can significantly reduce the computational complexity of the\nstandard MUSIC algorithm with a moderate loss of resolution. Moreover, in terms\nof speed, their performance is comparable to the state-of-the-art algorithms,\nwhile being more reliable for reconstructing line spectra with cluster\nstructure. The algorithms are supplemented with theoretical analyses of error\nestimates, sampling complexity, computational complexity, and computational\nlimit.",
            "author": [
                "Zetao Fei",
                "Hai Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17988v1",
                "http://arxiv.org/pdf/2310.17988v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17983v1",
            "title": "Major floods of the V\u00e9subie and Roya Rivers (Alps, France) in October\n  2020: hydrogeomorphological caracterisation and management perspectives",
            "updated": "2023-10-27T08:55:26Z",
            "published": "2023-10-27T08:55:26Z",
            "summary": "On October 2nd, 2020, under the combined effect of the winter Alex storm\nformed off the Brittany coast, and a strong Mediterranean episode, very\nintensive rainfalls affected in the south eastern France, both Roya and\nV{\\'e}subie catchments (locally up to 600 mm in 24h). This paroxysmal event\nwith a heavy human toll (10 dead, 8 missing) generated extreme flash floods\nover a large part of the hydrographic network. The result is an almost\ngeneralized fluvial metamorphosis of rivers, from sinuous single-thread\nchannels to braided channels. The characterization of morphological effects of\nthese floods is based on a diachronic aerial picture analysis highlighting a\nstrong increase of the active channel width (up to 900%) reaching -- or even\npushing back in few sectors -- front limits of the valley bottom. In the\nV{\\'e}subie, the 2D morphological effect of the Alex storm was 10 times higher\nthan that of the 100-yrs return period flood of November 1997. Comparison of\ndigital terrain models (DEM) before- and after-flood also allows us to foresee\nthe altitudinal variations (erosion/deposition) that affected beds and their\nriverine margins. The analysis of the impacts caused by these floods changes\nthe perception of the ``freedom space'' of these alpine rivers, which now must\nbe taken into account in the perspective of resilient reconstruction.",
            "author": [
                "Gabriel Melun",
                "Fr\u00e9d\u00e9ric Li\u00e9bault",
                "Guillaume Piton",
                "Margot Chapuis",
                "Paul Passy",
                "Damien Kuss",
                "C\u00e9line Martins"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17983v1",
                "http://arxiv.org/pdf/2310.17983v1"
            ],
            "primary_category": "physics.geo-ph",
            "category": [
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17982v1",
            "title": "Historical synthesis supporting Duhem's demonstration of the\n  impossibility of the `experimentum crucis' of comparison of the speed of\n  light in air and water",
            "updated": "2023-10-27T08:51:53Z",
            "published": "2023-10-27T08:51:53Z",
            "summary": "In 1850, Foucault was the first to compare experimentally the velocity of\nlight in air to its velocity in water. And as the wave and projectile theories\ncompeting at the time predicted inverse results to such a measurement, Foucault\n''declared the emission system incompatible with the reality of the facts''.\nYet, in his Physical Theory, Duhem uses this very example to illustrate his\nepistemological demonstration of the impossibility of the 'experimentum crucis'\nin physics. Then, the ambition of the present article is to augment Duhem's\ndemonstration with historical evidence relating precisely to the question of\nthe speed of light in water. We will review the four major opinions on the\nnature of light developed between 1637 and 1801; all leading to theories\nconcluding that the ratio of sines is constant and equal to the ratio of the\nvelocities in the two media, but all being defended by at least two authors\nconcluding to inverse ratios of those velocities -- and thus to opposite\npredictions on the result of an experiment such as Foucault's. The convergence\nof historical evidence will then confirm that the nature of light, considered\nin isolation, is in no way constrained by the measurement of its speed in\nwater.",
            "author": [
                "Olivier Morizot"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17982v1",
                "http://arxiv.org/pdf/2310.17982v1"
            ],
            "primary_category": "physics.hist-ph",
            "category": [
                "physics.hist-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17976v2",
            "title": "Does Role-Playing Chatbots Capture the Character Personalities?\n  Assessing Personality Traits for Role-Playing Chatbots",
            "updated": "2023-10-30T03:13:15Z",
            "published": "2023-10-27T08:42:18Z",
            "summary": "The emergence of large-scale pretrained language models has revolutionized\nthe capabilities of new AI application, especially in the realm of crafting\nchatbots with distinct personas. Given the \"stimulus-response\" nature of\nchatbots, this paper unveils an innovative open-ended interview-style approach\nfor personality assessment on role-playing chatbots, which offers a richer\ncomprehension of their intrinsic personalities. We conduct personality\nassessments on 32 role-playing chatbots created by the ChatHaruhi library,\nacross both the Big Five and MBTI dimensions, and measure their alignment with\nhuman perception. Evaluation results underscore that modern role-playing\nchatbots based on LLMs can effectively portray personality traits of\ncorresponding characters, with an alignment rate of 82.8% compared with\nhuman-perceived personalities. Besides, we also suggest potential strategies\nfor shaping chatbots' personalities. Hence, this paper serves as a cornerstone\nstudy for role-playing chatbots that intersects computational linguistics and\npsychology. Our resources are available at\nhttps://github.com/LC1332/Chat-Haruhi-Suzumiya",
            "author": [
                "Xintao Wang",
                "Quan Tu",
                "Yaying Fei",
                "Ziang Leng",
                "Cheng Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17976v2",
                "http://arxiv.org/pdf/2310.17976v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.02091v1",
            "title": "Deciphering circulating tumor cells binding in a microfluidic system\n  thanks to a parameterized mathematical model",
            "updated": "2023-10-27T08:41:20Z",
            "published": "2023-10-27T08:41:20Z",
            "summary": "Metastatic spread is a crucial process in which some questions remain\nunanswered. In this work, we focus on tumor cells circulating in the\nbloodstream, so-called Circulating Tumor Cells (CTCs). We aim to characterize\ntheir trajectories under the influence of hemodynamic forces and adhesion\nforces resulting from interaction with an endothelial layer using in vitro\nmeasurements performed with a microfluidic device. This essential step in tumor\nspread precedes intravascular arrest and metastatic extravasation. Our strategy\nis based on a differential equation model - a Poiseuille model for the fluid\nvelocity and an ODE system for the cell adhesion model - and allows us to\nseparate the two phenomena underlying cell motion: transport of the cell\nthrough the fluid and adhesion to the endothelial layer. A robust calibration\nprocedure enables us to characterize the dynamics. Our strategy reveals the\nexpected role of the glycoprotein CD44 compared to the integrin ITGB1 in the\ndeceleration of CTCs and quantifies the strong impact of the fluid velocity in\nthe protein binding.",
            "author": [
                "Giorgia Ciavolella",
                "Julien Granet",
                "Nael Osmani",
                "Jacky Goetz",
                "Christele Etchegaray",
                "Annabelle Collin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02091v1",
                "http://arxiv.org/pdf/2311.02091v1"
            ],
            "primary_category": "q-bio.TO",
            "category": [
                "q-bio.TO",
                "physics.bio-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17975v1",
            "title": "A Bayesian Method to Mitigate the Effects of Unmodelled Time-Varying\n  Systematics for 21-cm Cosmology Experiments",
            "updated": "2023-10-27T08:40:46Z",
            "published": "2023-10-27T08:40:46Z",
            "summary": "Radio observations of the neutral hydrogen signal from the Cosmic Dawn and\nEpoch of Reionisation have helped to provide constraints on the properties of\nthe first stars and galaxies. Since this global 21-cm cosmological signal from\nthe Cosmic Dawn is effectively constant on observing timescales and since\neffects resulting from systematics will vary with time, the effects of these\nsystematics can be mitigated without the need for a model of the systematic. We\npresent a method to account for unmodelled time-varying systematics in 21-cm\nradio cosmology experiments using a squared-exponential Gaussian process kernel\nto account for correlations between time bins in a fully Bayesian way. We find\nby varying the model parameters of a simulated systematic that the Gaussian\nprocess method improves our ability to recover the signal parameters by\nwidening the posterior in the presence of a systematic and reducing the bias in\nthe mean fit parameters. When varying the amplitude of a model sinusoidal\nsystematic between 0.25 and 2.00 times the 21-cm signal amplitude and the\nperiod between 0.5 and 4.0 times the signal width, we find on average a 5%\nimprovement in the root mean squared error of the fitted signal. We can use the\nfitted Gaussian process hyperparameters to identify the presence of a\nsystematic in the data, demonstrating the method's utility as a diagnostic\ntool. Furthermore, we can use Gaussian process regression to calculate a mean\nfit to the residuals over time, providing a basis for producing a model of the\ntime-varying systematic.",
            "author": [
                "Christian J. Kirkham",
                "Dominic J. Anstey",
                "Eloy de Lera Acedo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17975v1",
                "http://arxiv.org/pdf/2310.17975v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17974v1",
            "title": "FaultSeg Swin-UNETR: Transformer-Based Self-Supervised Pretraining Model\n  for Fault Recognition",
            "updated": "2023-10-27T08:38:59Z",
            "published": "2023-10-27T08:38:59Z",
            "summary": "This paper introduces an approach to enhance seismic fault recognition\nthrough self-supervised pretraining. Seismic fault interpretation holds great\nsignificance in the fields of geophysics and geology. However, conventional\nmethods for seismic fault recognition encounter various issues, including\ndependence on data quality and quantity, as well as susceptibility to\ninterpreter subjectivity. Currently, automated fault recognition methods\nproposed based on small synthetic datasets experience performance degradation\nwhen applied to actual seismic data. To address these challenges, we have\nintroduced the concept of self-supervised learning, utilizing a substantial\namount of relatively easily obtainable unlabeled seismic data for pretraining.\nSpecifically, we have employed the Swin Transformer model as the core network\nand employed the SimMIM pretraining task to capture unique features related to\ndiscontinuities in seismic data. During the fine-tuning phase, inspired by edge\ndetection techniques, we have also refined the structure of the Swin-UNETR\nmodel, enabling multiscale decoding and fusion for more effective fault\ndetection. Experimental results demonstrate that our proposed method attains\nstate-of-the-art performance on the Thebe dataset, as measured by the OIS and\nODS metrics.",
            "author": [
                "Zeren Zhang",
                "Ran Chen",
                "Jinwen Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17974v1",
                "http://arxiv.org/pdf/2310.17974v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17969v1",
            "title": "Quantitative recurrence for $T,T^{-1}$ tranformation",
            "updated": "2023-10-27T08:31:59Z",
            "published": "2023-10-27T08:31:59Z",
            "summary": "We are interested in the study of the asymptotic behaviour of return times in\nsmall balls for the $T,T^{-1}$-transformation. We exhibit different asymptotic\nbehaviours (different scaling, different limit point process) depending on the\nrespective dimensions of the measures of the two underlying dynamical systems.\nIt behaves either as for the direct product of the underlying systems, or as\nfor the Z-extension of the driving system (also studied in this article), or as\na more sophisticated process.",
            "author": [
                "Fran\u00e7oise P\u00e8ne",
                "Benoit Saussol"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17969v1",
                "http://arxiv.org/pdf/2310.17969v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17961v1",
            "title": "Complete lowest order radiative corrections in semi-inclusive scattering\n  of polarized particles",
            "updated": "2023-10-27T08:19:40Z",
            "published": "2023-10-27T08:19:40Z",
            "summary": "The lowest order radiative corrections to the cross section and asymmetries\nmeasured in experiments on semi-inclusive deep inelastic scattering of\npolarized particles were calculated. Both exact and leading log expressions\nwere presented and discussed for the total correction that include the\ncontributions from the processes of (i) real photon emission with\nsemi-inclusive processes, (ii) loop diagrams, and (iii) real photon emission\nwith exclusive processes. Radiative corrections to the Sivers and Collins\nasymmetries in $\\pi^+$ electroproduction were studied numerically within the\nkinematical conditions of modern experimental environments at Jefferson\nLaboratory (JLab). The Wandzura-Wilczek approximation for the semi-inclusive\nstructure functions and MAID2007 parameterization for the six amplitudes of\nexclusive processes were used in numeric analyses. The results show that (i)\nradiative effects can generate a correction comparable to the size of Sivers\nand Collins asymmetries at the Born level, (ii) there is good agreement between\nthe exact and leading-order corrections, (iii) external functions (that is,\nother than the Sivers and Collins functions in the respective asymmetries) can\ngenerate a contribution to the radiative correction up to 20\\%, and (iv) there\nexists a strong dependence of the radiative correction on the models for\nsemi-inclusive and exclusive structure functions.",
            "author": [
                "I. Akushevich",
                "H. Avakian",
                "A. Ilyichev",
                "S. Srednyak"
            ],
            "link": [
                "http://dx.doi.org/10.1140/epja/s10050-023-01150-0",
                "http://arxiv.org/abs/2310.17961v1",
                "http://arxiv.org/pdf/2310.17961v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17956v2",
            "title": "Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General\n  Healthcare",
            "updated": "2023-11-01T07:10:23Z",
            "published": "2023-10-27T08:05:21Z",
            "summary": "Large Language Models (LLMs) have introduced a new era of proficiency in\ncomprehending complex healthcare and biomedical topics. However, there is a\nnoticeable lack of models in languages other than English and models that can\ninterpret multi-modal input, which is crucial for global healthcare\naccessibility. In response, this study introduces Qilin-Med-VL, the first\nChinese large vision-language model designed to integrate the analysis of\ntextual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer\n(ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum\ntraining process that includes feature alignment and instruction tuning. This\nmethod enhances the model's ability to generate medical captions and answer\ncomplex medical queries. We also release ChiMed-VL, a dataset consisting of\nmore than 1M image-text pairs. This dataset has been carefully curated to\nenable detailed and comprehensive interpretation of medical data using various\ntypes of images.",
            "author": [
                "Junling Liu",
                "Ziming Wang",
                "Qichen Ye",
                "Dading Chong",
                "Peilin Zhou",
                "Yining Hua"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17956v2",
                "http://arxiv.org/pdf/2310.17956v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17955v1",
            "title": "Near-to mid-IR spectral purity transfer with a tunable frequency comb:\n  methanol frequency metrology over a record frequency span",
            "updated": "2023-10-27T08:03:48Z",
            "published": "2023-10-27T08:03:48Z",
            "summary": "We report the development and operation of a frequency-comb-assisted\nhigh-resolution mid-infrared molecular spectrometer combining high spectral\npurity, SI-traceability, wide tunability and high sensitivity. An optical\nfrequency comb is used to transfer the spectral purity of a SI-traceable 1.54\n$\\mu$m metrology-grade frequency reference to a 10.3 $\\mu$m quantum cascade\nlaser (QCL). The near-infrared reference is operated at the French\ntime/frequency metrology institute, calibrated there to primary frequency\nstandards, and transferred to Laboratoire de Physique des Lasers via the\nREFIMEVE fiber network. The QCL exhibits a sub-10 --15 frequency stability from\n0.1 to 10 s and its frequency is traceable to the SI with a total uncertainty\nbetter than 4 x 10 --14 after 1-s averaging time. We have developed the\ninstrumentation allowing comb modes to be continuously tuned over 9 GHz\nresulting in a QCL of record spectral purity uninterruptedly tunable at the\nprecision of the reference over an unprecedented span of 1.4 GHz. We have used\nour apparatus to conduct sub-Doppler spectroscopy of methanol in a multi-pass\ncell, demonstrating state-of-art frequency uncertainties down to the few\nkilohertz level. We have observed weak intensity resonances unreported so far,\nresolved subtle doublets never seen before and brought to light discrepancies\nwith the HITRAN database. This demonstrates the potential of our apparatus for\nprobing subtle internal molecular processes, building accurate spectroscopic\nmodels of polyatomic molecules of atmospheric or astrophysical interest, and\ncarrying out precise spectroscopic tests of fundamental physics.",
            "author": [
                "D B A Tran",
                "Olivier Lopez",
                "M Manceau",
                "A Goncharov",
                "Michel Abgrall",
                "H Alvarez-Martinez",
                "R Le Targat",
                "E Cantin",
                "P. -E Pottie",
                "A Amy-Klein",
                "B Darqui\u00e9"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17955v1",
                "http://arxiv.org/pdf/2310.17955v1"
            ],
            "primary_category": "physics.atom-ph",
            "category": [
                "physics.atom-ph",
                "physics.ins-det",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17954v1",
            "title": "Multivessel Coronary Artery Segmentation and Stenosis Localisation using\n  Ensemble Learning",
            "updated": "2023-10-27T08:03:12Z",
            "published": "2023-10-27T08:03:12Z",
            "summary": "Coronary angiography analysis is a common clinical task performed by\ncardiologists to diagnose coronary artery disease (CAD) through an assessment\nof atherosclerotic plaque's accumulation. This study introduces an end-to-end\nmachine learning solution developed as part of our solution for the MICCAI 2023\nAutomatic Region-based Coronary Artery Disease diagnostics using x-ray\nangiography imagEs (ARCADE) challenge, which aims to benchmark solutions for\nmultivessel coronary artery segmentation and potential stenotic lesion\nlocalisation from X-ray coronary angiograms. We adopted a robust baseline model\ntraining strategy to progressively improve performance, comprising five\nsuccessive stages of binary class pretraining, multivessel segmentation,\nfine-tuning using class frequency weighted dataloaders, fine-tuning using\nF1-based curriculum learning strategy (F1-CLS), and finally multi-target\nangiogram view classifier-based collective adaptation. Unlike many other\nmedical imaging procedures, this task exhibits a notable degree of\ninterobserver variability. %, making it particularly amenable to automated\nanalysis. Our ensemble model combines the outputs from six baseline models\nusing the weighted ensembling approach, which our analysis shows is found to\ndouble the predictive accuracy of the proposed solution. The final prediction\nwas further refined, targeting the correction of misclassified blobs. Our\nsolution achieved a mean F1 score of $37.69\\%$ for coronary artery\nsegmentation, and $39.41\\%$ for stenosis localisation, positioning our team in\nthe 5th position on both leaderboards. This work demonstrates the potential of\nautomated tools to aid CAD diagnosis, guide interventions, and improve the\naccuracy of stent injections in clinical settings.",
            "author": [
                "Muhammad Bilal",
                "Dinis Martinho",
                "Reiner Sim",
                "Adnan Qayyum",
                "Hunaid Vohra",
                "Massimo Caputo",
                "Taofeek Akinosho",
                "Sofiat Abioye",
                "Zaheer Khan",
                "Waleed Niaz",
                "Junaid Qadir"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17954v1",
                "http://arxiv.org/pdf/2310.17954v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17953v1",
            "title": "Whisper-MCE: Whisper Model Finetuned for Better Performance with Mixed\n  Languages",
            "updated": "2023-10-27T08:01:55Z",
            "published": "2023-10-27T08:01:55Z",
            "summary": "Recently Whisper has approached human-level robustness and accuracy in\nEnglish automatic speech recognition (ASR), while in minor language and mixed\nlanguage speech recognition, there remains a compelling need for further\nimprovement. In this work, we present the impressive results of Whisper-MCE,\nour finetuned Whisper model, which was trained using our self-collected\ndataset, Mixed Cantonese and English audio dataset (MCE). Meanwhile,\nconsidering word error rate (WER) poses challenges when it comes to evaluating\nits effectiveness in minor language and mixed-language contexts, we present a\nnovel rating mechanism. By comparing our model to the baseline whisper-large-v2\nmodel, we demonstrate its superior ability to accurately capture the content of\nthe original audio, achieve higher recognition accuracy, and exhibit faster\nrecognition speed. Notably, our model outperforms other existing models in the\nspecific task of recognizing mixed language.",
            "author": [
                "Peng Xie",
                "XingYuan Liu",
                "ZiWei Chen",
                "Kani Chen",
                "Yang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17953v1",
                "http://arxiv.org/pdf/2310.17953v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.CL",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17952v2",
            "title": "Shape-centered Representation Learning for Visible-Infrared Person\n  Re-identification",
            "updated": "2023-10-30T01:37:18Z",
            "published": "2023-10-27T07:57:24Z",
            "summary": "Current Visible-Infrared Person Re-Identification (VI-ReID) methods\nprioritize extracting distinguishing appearance features, ignoring the natural\nresistance of body shape against modality changes. Initially, we gauged the\ndiscriminative potential of shapes by a straightforward concatenation of shape\nand appearance features. However, two unresolved issues persist in the\nutilization of shape features. One pertains to the dependence on auxiliary\nmodels for shape feature extraction in the inference phase, along with the\nerrors in generated infrared shapes due to the intrinsic modality disparity.\nThe other issue involves the inadequately explored correlation between shape\nand appearance features. To tackle the aforementioned challenges, we propose\nthe Shape-centered Representation Learning framework (ScRL), which focuses on\nlearning shape features and appearance features associated with shapes.\nSpecifically, we devise the Shape Feature Propagation (SFP), facilitating\ndirect extraction of shape features from original images with minimal\ncomplexity costs during inference. To restitute inaccuracies in infrared body\nshapes at the feature level, we present the Infrared Shape Restitution (ISR).\nFurthermore, to acquire appearance features related to shape, we design the\nAppearance Feature Enhancement (AFE), which accentuates identity-related\nfeatures while suppressing identity-unrelated features guided by shape\nfeatures. Extensive experiments are conducted to validate the effectiveness of\nthe proposed ScRL. Achieving remarkable results, the Rank-1 (mAP) accuracy\nattains 76.1%, 71.2%, 92.4% (72.6%, 52.9%, 86.7%) on the SYSU-MM01, HITSZ-VCM,\nRegDB datasets respectively, outperforming existing state-of-the-art methods.",
            "author": [
                "Shuang Li",
                "Jiaxu Leng",
                "Ji Gan",
                "Mengjingcheng Mo",
                "Xinbo Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17952v2",
                "http://arxiv.org/pdf/2310.17952v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17950v1",
            "title": "Resilient Intraparticle Entanglement and its Manifestation in Spin\n  Dynamics of Disordered Dirac Matter",
            "updated": "2023-10-27T07:44:55Z",
            "published": "2023-10-27T07:44:55Z",
            "summary": "Topological quantum matter exhibits novel transport phenomena driven by\nentanglement between internal degrees of freedom, as for instance generated by\nspin-orbit coupling effects. Here we report on a direct connection between the\nmechanism driving spin relaxation and the intertwined dynamics between spin and\nsublattice degrees of freedom in disordered graphene. Beyond having a direct\nobservable consequence, such intraparticle entanglement is shown to be\nresilient to disorder, pointing towards a novel resource for quantum\ninformation processing.",
            "author": [
                "Jorge Martinez Romeral",
                "Aron W. Cummings",
                "Stephan Roche"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17950v1",
                "http://arxiv.org/pdf/2310.17950v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17949v2",
            "title": "Instance Segmentation under Occlusions via Location-aware Copy-Paste\n  Data Augmentation",
            "updated": "2023-11-21T05:55:10Z",
            "published": "2023-10-27T07:44:25Z",
            "summary": "Occlusion is a long-standing problem in computer vision, particularly in\ninstance segmentation. ACM MMSports 2023 DeepSportRadar has introduced a\ndataset that focuses on segmenting human subjects within a basketball context\nand a specialized evaluation metric for occlusion scenarios. Given the modest\nsize of the dataset and the highly deformable nature of the objects to be\nsegmented, this challenge demands the application of robust data augmentation\ntechniques and wisely-chosen deep learning architectures. Our work (ranked 1st\nin the competition) first proposes a novel data augmentation technique, capable\nof generating more training samples with wider distribution. Then, we adopt a\nnew architecture - Hybrid Task Cascade (HTC) framework with CBNetV2 as backbone\nand MaskIoU head to improve segmentation performance. Furthermore, we employ a\nStochastic Weight Averaging (SWA) training strategy to improve the model's\ngeneralization. As a result, we achieve a remarkable occlusion score (OM) of\n0.533 on the challenge dataset, securing the top-1 position on the leaderboard.\nSource code is available at this\nhttps://github.com/nguyendinhson-kaist/MMSports23-Seg-AutoID.",
            "author": [
                "Son Nguyen",
                "Mikel Lainsa",
                "Hung Dao",
                "Daeyoung Kim",
                "Giang Nguyen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17949v2",
                "http://arxiv.org/pdf/2310.17949v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17946v1",
            "title": "The Stokes Vector Measurement: A Paradigm Shift in Electric-Magnetic\n  Light Distinction",
            "updated": "2023-10-27T07:41:07Z",
            "published": "2023-10-27T07:41:07Z",
            "summary": "The multipolar expansion of the electromagnetic field plays a key role in the\nstudy of light-matter interactions. All the information about the radiation and\ncoupling between the incident wavefield and the object is embodied in the\nelectric and magnetic scattering coefficients $\\{a_{\\ell m}, b_{\\ell m} \\}$ of\nthis expansion. However, the experimental determination of $\\{a_{\\ell m},\nb_{\\ell m} \\}$ requires measuring the components of the scattered\nelectromagnetic field in all directions, something that is enormously\nchallenging. In this Letter, we demonstrate that a single measurement of the\nStokes vector at an angle of choice unlocks fundamental Nanophotonics\nmagnitudes that are concealed in the scattered field. The unveiled quantities\nare: $\\left[|a_{\\ell m}|^2, |b_{\\ell m}|^2, \\Re \\{ a_{\\ell m} b^*_{\\ell m} \\},\n\\Im \\{ a_{\\ell m} b^*_{\\ell m} \\} \\right]$. Strikingly, our Stokes polarimetry\napproach allows for distinguishing between the magnetic and electric nature of\nthe radiated electromagnetic field. Thereby, our findings, supported by exact\nanalytical theory, can find applications across all branches of Nanophotonics\nand Optics, and greatly facilitate routine light-scattering measurements.",
            "author": [
                "Jorge Olmos-Trigo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17946v1",
                "http://arxiv.org/pdf/2310.17946v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17945v1",
            "title": "A Comprehensive and Reliable Feature Attribution Method: Double-sided\n  Remove and Reconstruct (DoRaR)",
            "updated": "2023-10-27T07:40:45Z",
            "published": "2023-10-27T07:40:45Z",
            "summary": "The limited transparency of the inner decision-making mechanism in deep\nneural networks (DNN) and other machine learning (ML) models has hindered their\napplication in several domains. In order to tackle this issue, feature\nattribution methods have been developed to identify the crucial features that\nheavily influence decisions made by these black box models. However, many\nfeature attribution methods have inherent downsides. For example, one category\nof feature attribution methods suffers from the artifacts problem, which feeds\nout-of-distribution masked inputs directly through the classifier that was\noriginally trained on natural data points. Another category of feature\nattribution method finds explanations by using jointly trained feature\nselectors and predictors. While avoiding the artifacts problem, this new\ncategory suffers from the Encoding Prediction in the Explanation (EPITE)\nproblem, in which the predictor's decisions rely not on the features, but on\nthe masks that selects those features. As a result, the credibility of\nattribution results is undermined by these downsides. In this research, we\nintroduce the Double-sided Remove and Reconstruct (DoRaR) feature attribution\nmethod based on several improvement methods that addresses these issues. By\nconducting thorough testing on MNIST, CIFAR10 and our own synthetic dataset, we\ndemonstrate that the DoRaR feature attribution method can effectively bypass\nthe above issues and can aid in training a feature selector that outperforms\nother state-of-the-art feature attribution methods. Our code is available at\nhttps://github.com/dxq21/DoRaR.",
            "author": [
                "Dong Qin",
                "George Amariucai",
                "Daji Qiao",
                "Yong Guan",
                "Shen Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17945v1",
                "http://arxiv.org/pdf/2310.17945v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17943v1",
            "title": "Low-Complexity and Information-Theoretic Optimal Memory AMP for Coded\n  Generalized MIMO",
            "updated": "2023-10-27T07:38:49Z",
            "published": "2023-10-27T07:38:49Z",
            "summary": "This paper considers a generalized multiple-input multiple-output (GMIMO)\nwith practical assumptions, such as massive antennas, practical channel coding,\narbitrary input distributions, and general right-unitarily-invariant channel\nmatrices (covering Rayleigh fading, certain ill-conditioned and correlated\nchannel matrices). Orthogonal/vector approximate message passing (OAMP/VAMP)\nhas been proved to be information-theoretically optimal in GMIMO, but it is\nlimited to high complexity. Meanwhile, low-complexity memory approximate\nmessage passing (MAMP) was shown to be Bayes optimal in GMIMO, but channel\ncoding was ignored. Therefore, how to design a low-complexity and\ninformation-theoretic optimal receiver for GMIMO is still an open issue. In\nthis paper, we propose an information-theoretic optimal MAMP receiver for coded\nGMIMO, whose achievable rate analysis and optimal coding principle are provided\nto demonstrate its information-theoretic optimality. Specifically, state\nevolution (SE) for MAMP is intricately multi-dimensional because of the nature\nof local memory detection. To this end, a fixed-point consistency lemma is\nproposed to derive the simplified variational SE (VSE) for MAMP, based on which\nthe achievable rate of MAMP is calculated, and the optimal coding principle is\nderived to maximize the achievable rate. Subsequently, we prove the\ninformation-theoretic optimality of MAMP. Numerical results show that the\nfinite-length performances of MAMP with optimized LDPC codes are about 1.0 -\n2.7 dB away from the associated constrained capacities. It is worth noting that\nMAMP can achieve the same performance as OAMP/VAMP with 0.4% of the time\nconsumption for large-scale systems.",
            "author": [
                "Yufei Chen",
                "Lei Liu",
                "Yuhao Chi",
                "Ying Li",
                "Zhaoyang Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17943v1",
                "http://arxiv.org/pdf/2310.17943v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17940v4",
            "title": "Unified Segment-to-Segment Framework for Simultaneous Sequence\n  Generation",
            "updated": "2023-11-30T08:26:16Z",
            "published": "2023-10-27T07:34:51Z",
            "summary": "Simultaneous sequence generation is a pivotal task for real-time scenarios,\nsuch as streaming speech recognition, simultaneous machine translation and\nsimultaneous speech translation, where the target sequence is generated while\nreceiving the source sequence. The crux of achieving high-quality generation\nwith low latency lies in identifying the optimal moments for generating,\naccomplished by learning a mapping between the source and target sequences.\nHowever, existing methods often rely on task-specific heuristics for different\nsequence types, limiting the model's capacity to adaptively learn the\nsource-target mapping and hindering the exploration of multi-task learning for\nvarious simultaneous tasks. In this paper, we propose a unified\nsegment-to-segment framework (Seg2Seg) for simultaneous sequence generation,\nwhich learns the mapping in an adaptive and unified manner. During the process\nof simultaneous generation, the model alternates between waiting for a source\nsegment and generating a target segment, making the segment serve as the\nnatural bridge between the source and target. To accomplish this, Seg2Seg\nintroduces a latent segment as the pivot between source to target and explores\nall potential source-target mappings via the proposed expectation training,\nthereby learning the optimal moments for generating. Experiments on multiple\nsimultaneous generation tasks demonstrate that Seg2Seg achieves\nstate-of-the-art performance and exhibits better generality across various\ntasks.",
            "author": [
                "Shaolei Zhang",
                "Yang Feng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17940v4",
                "http://arxiv.org/pdf/2310.17940v4"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17936v1",
            "title": "Transformers as Graph-to-Graph Models",
            "updated": "2023-10-27T07:21:37Z",
            "published": "2023-10-27T07:21:37Z",
            "summary": "We argue that Transformers are essentially graph-to-graph models, with\nsequences just being a special case. Attention weights are functionally\nequivalent to graph edges. Our Graph-to-Graph Transformer architecture makes\nthis ability explicit, by inputting graph edges into the attention weight\ncomputations and predicting graph edges with attention-like functions, thereby\nintegrating explicit graphs into the latent graphs learned by pretrained\nTransformers. Adding iterative graph refinement provides a joint embedding of\ninput, output, and latent graphs, allowing non-autoregressive graph prediction\nto optimise the complete graph without any bespoke pipeline or decoding\nstrategy. Empirical results show that this architecture achieves\nstate-of-the-art accuracies for modelling a variety of linguistic structures,\nintegrating very effectively with the latent linguistic representations learned\nby pretraining.",
            "author": [
                "James Henderson",
                "Alireza Mohammadshahi",
                "Andrei C. Coman",
                "Lesly Miculicich"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17936v1",
                "http://arxiv.org/pdf/2310.17936v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17933v1",
            "title": "A barycenter-based approach for the multi-model ensembling of\n  subseasonal forecasts",
            "updated": "2023-10-27T07:12:03Z",
            "published": "2023-10-27T07:12:03Z",
            "summary": "Ensemble forecasts and their combination are explored from the perspective of\na probability space. Manipulating ensemble forecasts as discrete probability\ndistributions, multi-model ensembles (MMEs) are reformulated as barycenters of\nthese distributions. Barycenters are defined with respect to a given distance.\nThe barycenter with respect to the L2-distance is shown to be equivalent to the\npooling method. Then, the barycenter-based approach is extended to a different\ndistance with interesting properties in the distribution space: the Wasserstein\ndistance. Another interesting feature of the barycenter approach is the\npossibility to give different weights to the ensembles and so to naturally\nbuild weighted MME.\n  As a proof of concept, the L2- and the Wasserstein-barycenters are applied to\ncombine two models from the S2S database, namely the European Centre\nMedium-Range Weather Forecasts (ECMWF) and the National Centers for\nEnvironmental Prediction (NCEP) models. The performance of the two (weighted-)\nMMEs are evaluated for the prediction of weekly 2m-temperature over Europe for\nseven winters. The weights given to the models in the barycenters are optimized\nwith respect to two metrics, the CRPS and the proportion of skilful forecasts.\nThese weights have an important impact on the skill of the two barycenter-based\nMMEs. Although the ECMWF model has an overall better performance than NCEP, the\nbarycenter-ensembles are generally able to outperform both. However, the best\nMME method, but also the weights, are dependent on the metric. These results\nconstitute a promising first implementation of this methodology before moving\nto combination of more models.",
            "author": [
                "Camille Le Coz",
                "Alexis Tantet",
                "R\u00e9mi Flamary",
                "Riwal Plougonven"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17933v1",
                "http://arxiv.org/pdf/2310.17933v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18385v1",
            "title": "Matching of Descriptive Labels to Glossary Descriptions",
            "updated": "2023-10-27T07:09:04Z",
            "published": "2023-10-27T07:09:04Z",
            "summary": "Semantic text similarity plays an important role in software engineering\ntasks in which engineers are requested to clarify the semantics of descriptive\nlabels (e.g., business terms, table column names) that are often consists of\ntoo short or too generic words and appears in their IT systems. We formulate\nthis type of problem as a task of matching descriptive labels to glossary\ndescriptions. We then propose a framework to leverage an existing semantic text\nsimilarity measurement (STS) and augment it using semantic label enrichment and\nset-based collective contextualization where the former is a method to retrieve\nsentences relevant to a given label and the latter is a method to compute\nsimilarity between two contexts each of which is derived from a set of texts\n(e.g., column names in the same table). We performed an experiment on two\ndatasets derived from publicly available data sources. The result indicated\nthat the proposed methods helped the underlying STS correctly match more\ndescriptive labels with the descriptions.",
            "author": [
                "Toshihiro Takahashi",
                "Takaaki Tateishi",
                "Michiaki Tatsubori"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18385v1",
                "http://arxiv.org/pdf/2310.18385v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17924v1",
            "title": "SOUL: Towards Sentiment and Opinion Understanding of Language",
            "updated": "2023-10-27T06:48:48Z",
            "published": "2023-10-27T06:48:48Z",
            "summary": "Sentiment analysis is a well-established natural language processing task,\nwith sentiment polarity classification being one of its most popular and\nrepresentative tasks. However, despite the success of pre-trained language\nmodels in this area, they often fall short of capturing the broader\ncomplexities of sentiment analysis. To address this issue, we propose a new\ntask called Sentiment and Opinion Understanding of Language (SOUL). SOUL aims\nto evaluate sentiment understanding through two subtasks: Review Comprehension\n(RC) and Justification Generation (JG). RC seeks to validate statements that\nfocus on subjective information based on a review text, while JG requires\nmodels to provide explanations for their sentiment predictions. To enable\ncomprehensive evaluation, we annotate a new dataset comprising 15,028\nstatements from 3,638 reviews. Experimental results indicate that SOUL is a\nchallenging task for both small and large language models, with a performance\ngap of up to 27% when compared to human performance. Furthermore, evaluations\nconducted with both human experts and GPT-4 highlight the limitations of the\nsmall language model in generating reasoning-based justifications. These\nfindings underscore the challenging nature of the SOUL task for existing\nmodels, emphasizing the need for further advancements in sentiment analysis to\naddress its complexities. The new dataset and code are available at\nhttps://github.com/DAMO-NLP-SG/SOUL.",
            "author": [
                "Yue Deng",
                "Wenxuan Zhang",
                "Sinno Jialin Pan",
                "Lidong Bing"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17924v1",
                "http://arxiv.org/pdf/2310.17924v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17923v1",
            "title": "Dynamic Grasping of Unknown Objects with a Multi-Fingered Hand",
            "updated": "2023-10-27T06:37:33Z",
            "published": "2023-10-27T06:37:33Z",
            "summary": "An important prerequisite for autonomous robots is their ability to reliably\ngrasp a wide variety of objects. Most state-of-the-art systems employ\nspecialized or simple end-effectors, such as two-jaw grippers, which severely\nlimit the range of objects to manipulate. Additionally, they conventionally\nrequire a structured and fully predictable environment while the vast majority\nof our world is complex, unstructured, and dynamic. This paper presents an\nimplementation to overcome both issues. Firstly, the integration of a\nfive-finger hand enhances the variety of possible grasps and manipulable\nobjects. This kinematically complex end-effector is controlled by a deep\nlearning based generative grasping network. The required virtual model of the\nunknown target object is iteratively completed by processing visual sensor\ndata. Secondly, this visual feedback is employed to realize closed-loop servo\ncontrol which compensates for external disturbances. Our experiments on real\nhardware confirm the system's capability to reliably grasp unknown dynamic\ntarget objects without a priori knowledge of their trajectories. To the best of\nour knowledge, this is the first method to achieve dynamic multi-fingered\ngrasping for unknown objects. A video of the experiments is available at\nhttps://youtu.be/Ut28yM1gnvI.",
            "author": [
                "Yannick Burkhardt",
                "Qian Feng",
                "Karan Sharma",
                "Zhaopeng Chen",
                "Alois Knoll"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17923v1",
                "http://arxiv.org/pdf/2310.17923v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17918v1",
            "title": "Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection\n  Method",
            "updated": "2023-10-27T06:22:14Z",
            "published": "2023-10-27T06:22:14Z",
            "summary": "Large Language Models (LLMs) have shown great potential in Natural Language\nProcessing (NLP) tasks. However, recent literature reveals that LLMs generate\nnonfactual responses intermittently, which impedes the LLMs' reliability for\nfurther utilization. In this paper, we propose a novel self-detection method to\ndetect which questions that a LLM does not know that are prone to generate\nnonfactual results. Specifically, we first diversify the textual expressions\nfor a given question and collect the corresponding answers. Then we examine the\ndivergencies between the generated answers to identify the questions that the\nmodel may generate falsehoods. All of the above steps can be accomplished by\nprompting the LLMs themselves without referring to any other external\nresources. We conduct comprehensive experiments and demonstrate the\neffectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT,\nand GPT-4.",
            "author": [
                "Yukun Zhao",
                "Lingyong Yan",
                "Weiwei Sun",
                "Guoliang Xing",
                "Chong Meng",
                "Shuaiqiang Wang",
                "Zhicong Cheng",
                "Zhaochun Ren",
                "Dawei Yin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17918v1",
                "http://arxiv.org/pdf/2310.17918v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17914v1",
            "title": "3D-Aware Visual Question Answering about Parts, Poses and Occlusions",
            "updated": "2023-10-27T06:15:30Z",
            "published": "2023-10-27T06:15:30Z",
            "summary": "Despite rapid progress in Visual question answering (VQA), existing datasets\nand models mainly focus on testing reasoning in 2D. However, it is important\nthat VQA models also understand the 3D structure of visual scenes, for example\nto support tasks like navigation or manipulation. This includes an\nunderstanding of the 3D object pose, their parts and occlusions. In this work,\nwe introduce the task of 3D-aware VQA, which focuses on challenging questions\nthat require a compositional reasoning over the 3D structure of visual scenes.\nWe address 3D-aware VQA from both the dataset and the model perspective. First,\nwe introduce Super-CLEVR-3D, a compositional reasoning dataset that contains\nquestions about object parts, their 3D poses, and occlusions. Second, we\npropose PO3D-VQA, a 3D-aware VQA model that marries two powerful ideas:\nprobabilistic neural symbolic program execution for reasoning and deep neural\nnetworks with 3D generative representations of objects for robust visual\nrecognition. Our experimental results show our model PO3D-VQA outperforms\nexisting methods significantly, but we still observe a significant performance\ngap compared to 2D VQA benchmarks, indicating that 3D-aware VQA remains an\nimportant open research area.",
            "author": [
                "Xingrui Wang",
                "Wufei Ma",
                "Zhuowan Li",
                "Adam Kortylewski",
                "Alan Yuille"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17914v1",
                "http://arxiv.org/pdf/2310.17914v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17912v1",
            "title": "Restoring the Broken Covenant Between Compilers and Deep Learning\n  Accelerators",
            "updated": "2023-10-27T06:14:45Z",
            "published": "2023-10-27T06:14:45Z",
            "summary": "Deep learning accelerators address the computational demands of Deep Neural\nNetworks (DNNs), departing from the traditional Von Neumann execution model.\nThey leverage specialized hardware to align with the application domain's\nstructure. Compilers for these accelerators face distinct challenges compared\nto those for general-purpose processors. These challenges include exposing and\nmanaging more micro-architectural features, handling software-managed scratch\npads for on-chip storage, explicitly managing data movement, and matching DNN\nlayers with varying hardware capabilities. These complexities necessitate a new\napproach to compiler design, as traditional compilers mainly focused on\ngenerating fine-grained instruction sequences while abstracting\nmicro-architecture details. This paper introduces the Architecture Covenant\nGraph (ACG), an abstract representation of an architectural structure's\ncomponents and their programmable capabilities. By enabling the compiler to\nwork with the ACG, it allows for adaptable compilation workflows when making\nchanges to accelerator design, reducing the need for a complete compiler\nredevelopment. Codelets, which express DNN operation functionality and evolve\ninto execution mappings on the ACG, are key to this process. The Covenant\ncompiler efficiently targets diverse deep learning accelerators, achieving\n93.8% performance compared to state-of-the-art, hand-tuned DNN layer\nimplementations when compiling 14 DNN layers from various models on two\ndifferent architectures.",
            "author": [
                "Sean Kinzer",
                "Soroush Ghodrati",
                "Rohan Mahapatra",
                "Byung Hoon Ahn",
                "Edwin Mascarenhas",
                "Xiaolong Li",
                "Janarbek Matai",
                "Liang Zhang",
                "Hadi Esmaeilzadeh"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17912v1",
                "http://arxiv.org/pdf/2310.17912v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17911v1",
            "title": "Hyper-Skin: A Hyperspectral Dataset for Reconstructing Facial\n  Skin-Spectra from RGB Images",
            "updated": "2023-10-27T06:10:35Z",
            "published": "2023-10-27T06:10:35Z",
            "summary": "We introduce Hyper-Skin, a hyperspectral dataset covering wide range of\nwavelengths from visible (VIS) spectrum (400nm - 700nm) to near-infrared (NIR)\nspectrum (700nm - 1000nm), uniquely designed to facilitate research on facial\nskin-spectra reconstruction. By reconstructing skin spectra from RGB images,\nour dataset enables the study of hyperspectral skin analysis, such as melanin\nand hemoglobin concentrations, directly on the consumer device. Overcoming\nlimitations of existing datasets, Hyper-Skin consists of diverse facial skin\ndata collected with a pushbroom hyperspectral camera. With 330 hyperspectral\ncubes from 51 subjects, the dataset covers the facial skin from different\nangles and facial poses. Each hyperspectral cube has dimensions of\n1024$\\times$1024$\\times$448, resulting in millions of spectra vectors per\nimage. The dataset, carefully curated in adherence to ethical guidelines,\nincludes paired hyperspectral images and synthetic RGB images generated using\nreal camera responses. We demonstrate the efficacy of our dataset by showcasing\nskin spectra reconstruction using state-of-the-art models on 31 bands of\nhyperspectral data resampled in the VIS and NIR spectrum. This Hyper-Skin\ndataset would be a valuable resource to NeurIPS community, encouraging the\ndevelopment of novel algorithms for skin spectral reconstruction while\nfostering interdisciplinary collaboration in hyperspectral skin analysis\nrelated to cosmetology and skin's well-being. Instructions to request the data\nand the related benchmarking codes are publicly available at:\n\\url{https://github.com/hyperspectral-skin/Hyper-Skin-2023}.",
            "author": [
                "Pai Chet Ng",
                "Zhixiang Chi",
                "Yannick Verdie",
                "Juwei Lu",
                "Konstantinos N. Plataniotis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17911v1",
                "http://arxiv.org/pdf/2310.17911v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17905v1",
            "title": "Small antimicrobial resistance proteins (SARPs): Small proteins\n  conferring antimicrobial resistance",
            "updated": "2023-10-27T05:37:56Z",
            "published": "2023-10-27T05:37:56Z",
            "summary": "Small open reading frames are understudied as they have been historically\nexcluded from genome annotations. However, evidence for the functional\nsignificance of small proteins in various cellular processes accumulates.\nProteins with less than 70 residues can also confer resistance to antimicrobial\ncompounds, including intracellularly-acting protein toxins, membrane-acting\nantimicrobial peptides and various small-molecule antibiotics. Such herein\ncoined Small Antimicrobial Resistance Proteins (SARPs) have emerged on\nevolutionary timescales or can be enriched from protein libraries using\nlaboratory evolution. Our review consolidates existing knowledge on SARPs and\nhighlights recent advancements in proteomics and genomics that reveal pervasive\ntranslation of unannotated genetic regions into small proteins that show\nfeatures of known SARPs. The potential contribution of small proteins to\nantimicrobial resistance is awaiting exploration.",
            "author": [
                "Rianne C. Prins",
                "Sonja Billerbeck"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17905v1",
                "http://arxiv.org/pdf/2310.17905v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17903v1",
            "title": "Pitfalls in Language Models for Code Intelligence: A Taxonomy and Survey",
            "updated": "2023-10-27T05:32:57Z",
            "published": "2023-10-27T05:32:57Z",
            "summary": "Modern language models (LMs) have been successfully employed in source code\ngeneration and understanding, leading to a significant increase in research\nfocused on learning-based code intelligence, such as automated bug repair, and\ntest case generation. Despite their great potential, language models for code\nintelligence (LM4Code) are susceptible to potential pitfalls, which hinder\nrealistic performance and further impact their reliability and applicability in\nreal-world deployment. Such challenges drive the need for a comprehensive\nunderstanding - not just identifying these issues but delving into their\npossible implications and existing solutions to build more reliable language\nmodels tailored to code intelligence. Based on a well-defined systematic\nresearch approach, we conducted an extensive literature review to uncover the\npitfalls inherent in LM4Code. Finally, 67 primary studies from top-tier venues\nhave been identified. After carefully examining these studies, we designed a\ntaxonomy of pitfalls in LM4Code research and conducted a systematic study to\nsummarize the issues, implications, current solutions, and challenges of\ndifferent pitfalls for LM4Code systems. We developed a comprehensive\nclassification scheme that dissects pitfalls across four crucial aspects: data\ncollection and labeling, system design and learning, performance evaluation,\nand deployment and maintenance. Through this study, we aim to provide a roadmap\nfor researchers and practitioners, facilitating their understanding and\nutilization of LM4Code in reliable and trustworthy ways.",
            "author": [
                "Xinyu She",
                "Yue Liu",
                "Yanjie Zhao",
                "Yiling He",
                "Li Li",
                "Chakkrit Tantithamthavorn",
                "Zhan Qin",
                "Haoyu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17903v1",
                "http://arxiv.org/pdf/2310.17903v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17902v1",
            "title": "CPIA Dataset: A Comprehensive Pathological Image Analysis Dataset for\n  Self-supervised Learning Pre-training",
            "updated": "2023-10-27T05:32:16Z",
            "published": "2023-10-27T05:32:16Z",
            "summary": "Pathological image analysis is a crucial field in computer-aided diagnosis,\nwhere deep learning is widely applied. Transfer learning using pre-trained\nmodels initialized on natural images has effectively improved the downstream\npathological performance. However, the lack of sophisticated domain-specific\npathological initialization hinders their potential. Self-supervised learning\n(SSL) enables pre-training without sample-level labels, which has great\npotential to overcome the challenge of expensive annotations. Thus, studies\nfocusing on pathological SSL pre-training call for a comprehensive and\nstandardized dataset, similar to the ImageNet in computer vision. This paper\npresents the comprehensive pathological image analysis (CPIA) dataset, a\nlarge-scale SSL pre-training dataset combining 103 open-source datasets with\nextensive standardization. The CPIA dataset contains 21,427,877 standardized\nimages, covering over 48 organs/tissues and about 100 kinds of diseases, which\nincludes two main data types: whole slide images (WSIs) and characteristic\nregions of interest (ROIs). A four-scale WSI standardization process is\nproposed based on the uniform resolution in microns per pixel (MPP), while the\nROIs are divided into three scales artificially. This multi-scale dataset is\nbuilt with the diagnosis habits under the supervision of experienced senior\npathologists. The CPIA dataset facilitates a comprehensive pathological\nunderstanding and enables pattern discovery explorations. Additionally, to\nlaunch the CPIA dataset, several state-of-the-art (SOTA) baselines of SSL\npre-training and downstream evaluation are specially conducted. The CPIA\ndataset along with baselines is available at\nhttps://github.com/zhanglab2021/CPIA_Dataset.",
            "author": [
                "Nan Ying",
                "Yanli Lei",
                "Tianyi Zhang",
                "Shangqing Lyu",
                "Chunhui Li",
                "Sicheng Chen",
                "Zeyu Liu",
                "Yu Zhao",
                "Guanglei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17902v1",
                "http://arxiv.org/pdf/2310.17902v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17897v1",
            "title": "Event Generation and Consistence Test for Physics with Sliced\n  Wasserstein Distance",
            "updated": "2023-10-27T05:08:25Z",
            "published": "2023-10-27T05:08:25Z",
            "summary": "In the field of modern high-energy physics research, there is a growing\nemphasis on utilizing deep learning techniques to optimize event simulation,\nthereby expanding the statistical sample size for more accurate physical\nanalysis. Traditional simulation methods often encounter challenges when\ndealing with complex physical processes and high-dimensional data\ndistributions, resulting in slow performance. To overcome these limitations, we\npropose a solution based on deep learning with the sliced Wasserstein distance\nas the loss function. Our method shows its ability on high precision and\nlarge-scale simulations, and demonstrates its effectiveness in handling complex\nphysical processes. By employing an advanced transformer learning architecture,\nwe initiate the learning process from a Monte Carlo sample, and generate\nhigh-dimensional data while preserving all original distribution features. The\ngenerated data samples have passed the consistence test, that is developed to\ncalculate the confidence of the high-dimentional distributions of the generated\ndata samples through permutation tests. This fast simulation strategy, enabled\nby deep learning, holds significant potential not only for increasing sample\nsizes and reducing statistical uncertainties but also for applications in\nnumerical integration, which is crucial in partial wave analysis,\nhigh-precision sample checks, and other related fields. It opens up new\npossibilities for improving event simulation in high-energy physics research.",
            "author": [
                "Chu-Cheng Pan",
                "Xiang Dong",
                "Yu-Chang Sun",
                "Ao-Yan Cheng",
                "Ao-Bo Wang",
                "Yu-Xuan Hu",
                "Hao Cai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17897v1",
                "http://arxiv.org/pdf/2310.17897v1"
            ],
            "primary_category": "physics.comp-ph",
            "category": [
                "physics.comp-ph",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17894v1",
            "title": "Natural Language Interfaces for Tabular Data Querying and Visualization:\n  A Survey",
            "updated": "2023-10-27T05:01:20Z",
            "published": "2023-10-27T05:01:20Z",
            "summary": "The emergence of natural language processing has revolutionized the way users\ninteract with tabular data, enabling a shift from traditional query languages\nand manual plotting to more intuitive, language-based interfaces. The rise of\nlarge language models (LLMs) such as ChatGPT and its successors has further\nadvanced this field, opening new avenues for natural language processing\ntechniques. This survey presents a comprehensive overview of natural language\ninterfaces for tabular data querying and visualization, which allow users to\ninteract with data using natural language queries. We introduce the fundamental\nconcepts and techniques underlying these interfaces with a particular emphasis\non semantic parsing, the key technology facilitating the translation from\nnatural language to SQL queries or data visualization commands. We then delve\ninto the recent advancements in Text-to-SQL and Text-to-Vis problems from the\nperspectives of datasets, methodologies, metrics, and system designs. This\nincludes a deep dive into the influence of LLMs, highlighting their strengths,\nlimitations, and potential for future improvements. Through this survey, we aim\nto provide a roadmap for researchers and practitioners interested in developing\nand applying natural language interfaces for data interaction in the era of\nlarge language models.",
            "author": [
                "Weixu Zhang",
                "Yifei Wang",
                "Yuanfeng Song",
                "Victor Junqiu Wei",
                "Yuxing Tian",
                "Yiyan Qi",
                "Jonathan H. Chan",
                "Raymond Chi-Wing Wong",
                "Haiqin Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17894v1",
                "http://arxiv.org/pdf/2310.17894v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17893v1",
            "title": "Nuclear structure study of $^{20-23}$Na isotopes with ab initio no-core\n  shell-model",
            "updated": "2023-10-27T04:58:07Z",
            "published": "2023-10-27T04:58:07Z",
            "summary": "We have done a systematic no-core shell-model study of $^{20-23}$Na isotopes.\nThe low-energy spectra of these sodium isotopes consisting of natural and\nun-natural parity states were reported, considering three realistic\ninteractions: inside nonlocal outside Yukawa (INOY), charge-dependent Bonn 2000\n(CDB2K), and the chiral next-to-next-to-next-to-leading order (N$^3$LO). We\nalso present the mirror energy differences in the low-energy spectra of $|T_z|$\n= 1/2 mirror pair ($^{21}$Na - $^{21}$Ne). Apart from the energy spectra, we\nhave also reported the electromagnetic transition strengths and moments.\nFinally, considering all three realistic interactions, we report the\npoint-proton radii and neutron skin thicknesses.",
            "author": [
                "Chandan Sarma",
                "Praveen C. Srivastava"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17893v1",
                "http://arxiv.org/pdf/2310.17893v1"
            ],
            "primary_category": "nucl-th",
            "category": [
                "nucl-th",
                "nucl-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17892v1",
            "title": "Prompt-NER: Zero-shot Named Entity Recognition in Astronomy Literature\n  via Large Language Models",
            "updated": "2023-10-27T04:50:16Z",
            "published": "2023-10-27T04:50:16Z",
            "summary": "This study delves into the application of Large Language Models (LLMs) for\nNamed Entity Recognition (NER) tasks in the field of astronomy literature. To\nenhance the zero-shot recognition capabilities of LLMs for astronomical named\nentities, we propose a strategy called Prompt-NER. Prompt-NER includes five\nprompt elements: Task Descriptions, Entity Definitions, Task Emphasis, Task\nExamples, and Second Conversation. To assess the effectiveness of the\nPrompt-NER strategy, we utilize three representative LLMs (Claude-2, GPT-3.5,\nand LLaMA-2-70b) to identify telescope and celestial object named entities in\nastronomical literature. Our experiments are conducted based on two distinct\ndatasets. The first dataset comprises 30 original PDF documents, which we split\ninto paragraphs in sequential order, resulting in a second dataset consisting\nof 30 paragraph collections. Additionally, we incorporate 30 astronomical\ntelegrams to diversify our experiments and assess the performance of LLMs based\non Prompt-NER on concise, complete texts. Our experimental results indicate\nthat the Prompt-NER strategy enables LLMs to effectively accomplish NER tasks\nin the field of astronomy, even without prior astronomical knowledge during\ntraining. We carefully analyze the experimental results, including the\nmechanism of different prompt elements and the influence of different features\nof long and short texts on their respective experimental results. This research\nprovides experience for zero-shot NER tasks in astronomical literature and\nsuggests future work in this area.",
            "author": [
                "Wujun Shao",
                "Yaohua Hu",
                "Pengli Ji",
                "Xiaoran Yan",
                "Dongwei Fan",
                "Rui Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17892v1",
                "http://arxiv.org/pdf/2310.17892v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17891v1",
            "title": "Inadmissibility and Transience",
            "updated": "2023-10-27T04:49:40Z",
            "published": "2023-10-27T04:49:40Z",
            "summary": "We discuss the relation between the statistical question of inadmissibility\nand the probabilistic question of transience. Brown (1971) proved the\nmathematical link between the admissibility of the mean of a Gaussian\ndistribution and the recurrence of a Brownian motion, which holds for\n$\\mathbb{R}^{2}$ but not for $\\mathbb{R}^{3}$ in Euclidean space. We extend\nthis result to symmetric, non-Gaussian distributions, without assuming the\nexistence of moments. As an application, we prove that the relation between the\ninadmissibility of the predictive density of a Cauchy distribution under a\nuniform prior and the transience of the Cauchy process differs from dimensions\n$\\mathbb{R}^{1}$ to $\\mathbb{R}^{2}$. We also show that there exists an extreme\nmodel that is inadmissible in $\\mathbb{R}^{1}$.",
            "author": [
                "Kosaku Takanashi",
                "Kenichiro McAlinn"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17891v1",
                "http://arxiv.org/pdf/2310.17891v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "math.PR",
                "stat.TH",
                "62C15"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17890v1",
            "title": "Submodel Partitioning in Hierarchical Federated Learning: Algorithm\n  Design and Convergence Analysis",
            "updated": "2023-10-27T04:42:59Z",
            "published": "2023-10-27T04:42:59Z",
            "summary": "Hierarchical federated learning (HFL) has demonstrated promising scalability\nadvantages over the traditional \"star-topology\" architecture-based federated\nlearning (FL). However, HFL still imposes significant computation,\ncommunication, and storage burdens on the edge, especially when training a\nlarge-scale model over resource-constrained Internet of Things (IoT) devices.\nIn this paper, we propose hierarchical independent submodel training (HIST), a\nnew FL methodology that aims to address these issues in hierarchical settings.\nThe key idea behind HIST is a hierarchical version of model partitioning, where\nwe partition the global model into disjoint submodels in each round, and\ndistribute them across different cells, so that each cell is responsible for\ntraining only one partition of the full model. This enables each client to save\ncomputation/storage costs while alleviating the communication loads throughout\nthe hierarchy. We characterize the convergence behavior of HIST for non-convex\nloss functions under mild assumptions, showing the impact of several attributes\n(e.g., number of cells, local and global aggregation frequency) on the\nperformance-efficiency tradeoff. Finally, through numerical experiments, we\nverify that HIST is able to save communication costs by a wide margin while\nachieving the same target testing accuracy.",
            "author": [
                "Wenzhi Fang",
                "Dong-Jun Han",
                "Christopher G. Brinton"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17890v1",
                "http://arxiv.org/pdf/2310.17890v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17889v1",
            "title": "Towards optimal multimode fiber imaging by leveraging input polarization\n  and conditional generative adversarial networks",
            "updated": "2023-10-27T04:39:23Z",
            "published": "2023-10-27T04:39:23Z",
            "summary": "Deep learning techniques provide a plausible route towards achieving\npractical imaging through multimode fibers. However, the results produced by\nthese methods are often influenced by physical factors like temperature, fiber\nlength, external perturbations, and polarization state of the input light. The\nimpact of other factors, except input light polarization, has been discussed in\nthe literature for imaging applications. The input polarization has been\nconsidered by researchers while looking at the characterization and control of\npolarization in multimode fibers. Here, we show experimentally that the state\nof polarization of light, being injected at multimode fiber input, affects the\nfidelity of reconstructed images from speckle patterns. Certain polarization\nstates produce high-quality images at fiber output, while some yield degraded\nresults. We have designed a conditional generative adversarial network~(CGAN)\nfor image regeneration at various degrees of input light polarization. We\ndemonstrate that in the case of multimode fibers that are held fixed, optimal\nimaging can be achieved by leveraging our CGAN model with the input light\npolarization state, where the fidelity of images is maximum. Our work exhibits\nhigh average structural similarity index values exceeding 0.9, surpassing the\npreviously reported value of 0.8772. We also show that the model can be\ngeneralized to image adequately for all input light polarization states when\nthe fiber has bends or twists. We anticipate our work will be a stepping stone\ntoward developing high-resolution and less invasive multimode fiber endoscopes.",
            "author": [
                "Jawaria Maqbool",
                "Syed Talal Hassan",
                "M. Imran Cheema"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17889v1",
                "http://arxiv.org/pdf/2310.17889v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17888v1",
            "title": "Large Language Models as Subpopulation Representative Models: A Review",
            "updated": "2023-10-27T04:31:27Z",
            "published": "2023-10-27T04:31:27Z",
            "summary": "Of the many commercial and scientific opportunities provided by large\nlanguage models (LLMs; including Open AI's ChatGPT, Meta's LLaMA, and\nAnthropic's Claude), one of the more intriguing applications has been the\nsimulation of human behavior and opinion. LLMs have been used to generate human\nsimulcra to serve as experimental participants, survey respondents, or other\nindependent agents, with outcomes that often closely parallel the observed\nbehavior of their genuine human counterparts. Here, we specifically consider\nthe feasibility of using LLMs to estimate subpopulation representative models\n(SRMs). SRMs could provide an alternate or complementary way to measure public\nopinion among demographic, geographic, or political segments of the population.\nHowever, the introduction of new technology to the socio-technical\ninfrastructure does not come without risk. We provide an overview of behavior\nelicitation techniques for LLMs, and a survey of existing SRM implementations.\nWe offer frameworks for the analysis, development, and practical implementation\nof LLMs as SRMs, consider potential risks, and suggest directions for future\nwork.",
            "author": [
                "Gabriel Simmons",
                "Christopher Hare"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17888v1",
                "http://arxiv.org/pdf/2310.17888v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17884v1",
            "title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models\n  via Contextual Integrity Theory",
            "updated": "2023-10-27T04:15:30Z",
            "published": "2023-10-27T04:15:30Z",
            "summary": "The interactive use of large language models (LLMs) in AI assistants (at\nwork, home, etc.) introduces a new set of inference-time privacy risks: LLMs\nare fed different types of information from multiple sources in their inputs\nand are expected to reason about what to share in their outputs, for what\npurpose and with whom, within a given context. In this work, we draw attention\nto the highly critical yet overlooked notion of contextual privacy by proposing\nConfAIde, a benchmark designed to identify critical weaknesses in the privacy\nreasoning capabilities of instruction-tuned LLMs. Our experiments show that\neven the most capable models such as GPT-4 and ChatGPT reveal private\ninformation in contexts that humans would not, 39% and 57% of the time,\nrespectively. This leakage persists even when we employ privacy-inducing\nprompts or chain-of-thought reasoning. Our work underscores the immediate need\nto explore novel inference-time privacy-preserving approaches, based on\nreasoning and theory of mind.",
            "author": [
                "Niloofar Mireshghallah",
                "Hyunwoo Kim",
                "Xuhui Zhou",
                "Yulia Tsvetkov",
                "Maarten Sap",
                "Reza Shokri",
                "Yejin Choi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17884v1",
                "http://arxiv.org/pdf/2310.17884v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17882v1",
            "title": "Machine Learning Infused Distributed Optimization for Coordinating\n  Virtual Power Plant Assets",
            "updated": "2023-10-27T04:11:13Z",
            "published": "2023-10-27T04:11:13Z",
            "summary": "Amid the increasing interest in the deployment of Distributed Energy\nResources (DERs), the Virtual Power Plant (VPP) has emerged as a pivotal tool\nfor aggregating diverse DERs and facilitating their participation in wholesale\nenergy markets. These VPP deployments have been fueled by the Federal Energy\nRegulatory Commission's Order 2222, which makes DERs and VPPs competitive\nacross market segments. However, the diversity and decentralized nature of DERs\npresent significant challenges to the scalable coordination of VPP assets. To\naddress efficiency and speed bottlenecks, this paper presents a novel machine\nlearning-assisted distributed optimization to coordinate VPP assets. Our\nmethod, named LOOP-MAC(Learning to Optimize the Optimization Process for\nMulti-agent Coordination), adopts a multi-agent coordination perspective where\neach VPP agent manages multiple DERs and utilizes neural network approximators\nto expedite the solution search. The LOOP-MAC method employs a gauge map to\nguarantee strict compliance with local constraints, effectively reducing the\nneed for additional post-processing steps. Our results highlight the advantages\nof LOOP-MAC, showcasing accelerated solution times per iteration and\nsignificantly reduced convergence times. The LOOP-MAC method outperforms\nconventional centralized and distributed optimization methods in optimization\ntasks that require repetitive and sequential execution.",
            "author": [
                "Meiyi Li",
                "Javad Mohammadi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17882v1",
                "http://arxiv.org/pdf/2310.17882v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17881v1",
            "title": "Signs of the rates in the Lindblad master equations can always be\n  arbitrarily determined",
            "updated": "2023-10-27T04:06:50Z",
            "published": "2023-10-27T04:06:50Z",
            "summary": "Determining the Markovianity and non-Markovianity of a quantum process is a\ncritical problem in the theory of open quantum systems, as their behaviors\ndiffer significantly in terms of complexity. It is well recognized that a\nquantum process is Markovian if and only if the quantum master equation can be\nwritten in the standard Lindblad form with all rates nonnegative for all time.\nHowever, here we present a striking result that \\textit{any} finite-dimensional\nopen quantum system dynamics can be described by a quantum master equation in\nthe Lindblad form with all rates nonnegative for all time. In fact, it can be\nshown that one can arbitrarily decide the sign of the rates in any case at any\ntime interval. Note that here we take an unconventional approach where the\nquantum master equation we construct will in general be state-dependent, which\nmeans that the Hamiltonian, jump operators and rates will all depend on the\ncurrent state of the density matrix $\\rho(t)$. Our findings raise serious\nquestions on the current criterion in determining Markovianity and\nnon-Markovianity in open quantum system dynamics.",
            "author": [
                "Le Hu",
                "Andrew N. Jordan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17881v1",
                "http://arxiv.org/pdf/2310.17881v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17877v1",
            "title": "ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for\n  Consistent Data-to-Text Generation",
            "updated": "2023-10-27T03:39:51Z",
            "published": "2023-10-27T03:39:51Z",
            "summary": "We present ASPIRO, an approach for structured data verbalisation into short\ntemplate sentences in zero to few-shot settings. Unlike previous methods, our\napproach prompts large language models (LLMs) to directly produce\nentity-agnostic templates, rather than relying on LLMs to faithfully copy the\ngiven example entities, or validating/crafting the templates manually. We\nincorporate LLM re-prompting, triggered by algorithmic parsing checks, as well\nas the PARENT metric induced consistency validation to identify and rectify\ntemplate generation problems in real-time. ASPIRO, compared to direct LLM\noutput, averages 66\\% parsing error rate reduction in generated verbalisations\nof RDF triples on the DART dataset. Our best 5-shot text-davinci-003 setup,\nscoring BLEU of 50.62, METEOR of 45.16, BLEURT of 0.82, NUBIA of 0.87, and\nPARENT of 0.8962 on the Rel2Text dataset, competes effectively with recent\nfine-tuned pre-trained language models.",
            "author": [
                "Martin Vejvar",
                "Yasutaka Fujimoto"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17877v1",
                "http://arxiv.org/pdf/2310.17877v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17876v2",
            "title": "TarGEN: Targeted Data Generation with Large Language Models",
            "updated": "2023-10-30T19:08:56Z",
            "published": "2023-10-27T03:32:17Z",
            "summary": "The rapid advancement of large language models (LLMs) has sparked interest in\ndata synthesis techniques, aiming to generate diverse and high-quality\nsynthetic datasets. However, these synthetic datasets often suffer from a lack\nof diversity and added noise. In this paper, we present TarGEN, a multi-step\nprompting strategy for generating high-quality synthetic datasets utilizing a\nLLM. An advantage of TarGEN is its seedless nature; it does not require\nspecific task instances, broadening its applicability beyond task replication.\nWe augment TarGEN with a method known as self-correction empowering LLMs to\nrectify inaccurately labeled instances during dataset creation, ensuring\nreliable labels. To assess our technique's effectiveness, we emulate 8 tasks\nfrom the SuperGLUE benchmark and finetune various language models, including\nencoder-only, encoder-decoder, and decoder-only models on both synthetic and\noriginal training sets. Evaluation on the original test set reveals that models\ntrained on datasets generated by TarGEN perform approximately 1-2% points\nbetter than those trained on original datasets (82.84% via syn. vs. 81.12% on\nog. using Flan-T5). When incorporating instruction tuning, the performance\nincreases to 84.54% on synthetic data vs. 81.49% on original data by Flan-T5. A\ncomprehensive analysis of the synthetic dataset compared to the original\ndataset reveals that the synthetic dataset demonstrates similar or higher\nlevels of dataset complexity and diversity. Furthermore, the synthetic dataset\ndisplays a bias level that aligns closely with the original dataset. Finally,\nwhen pre-finetuned on our synthetic SuperGLUE dataset, T5-3B yields impressive\nresults on the OpenLLM leaderboard, surpassing the model trained on the\nSelf-Instruct dataset by 4.14% points. We hope that TarGEN can be helpful for\nquality data generation and reducing the human efforts to create complex\nbenchmarks.",
            "author": [
                "Himanshu Gupta",
                "Kevin Scaria",
                "Ujjwala Anantheswaran",
                "Shreyas Verma",
                "Mihir Parmar",
                "Saurabh Arjun Sawant",
                "Chitta Baral",
                "Swaroop Mishra"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17876v2",
                "http://arxiv.org/pdf/2310.17876v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17875v1",
            "title": "Siamese-DETR for Generic Multi-Object Tracking",
            "updated": "2023-10-27T03:32:05Z",
            "published": "2023-10-27T03:32:05Z",
            "summary": "The ability to detect and track the dynamic objects in different scenes is\nfundamental to real-world applications, e.g., autonomous driving and robot\nnavigation. However, traditional Multi-Object Tracking (MOT) is limited to\ntracking objects belonging to the pre-defined closed-set categories. Recently,\nOpen-Vocabulary MOT (OVMOT) and Generic MOT (GMOT) are proposed to track\ninterested objects beyond pre-defined categories with the given text prompt and\ntemplate image. However, the expensive well pre-trained (vision-)language model\nand fine-grained category annotations are required to train OVMOT models. In\nthis paper, we focus on GMOT and propose a simple but effective method,\nSiamese-DETR, for GMOT. Only the commonly used detection datasets (e.g., COCO)\nare required for training. Different from existing GMOT methods, which train a\nSingle Object Tracking (SOT) based detector to detect interested objects and\nthen apply a data association based MOT tracker to get the trajectories, we\nleverage the inherent object queries in DETR variants. Specifically: 1) The\nmulti-scale object queries are designed based on the given template image,\nwhich are effective for detecting different scales of objects with the same\ncategory as the template image; 2) A dynamic matching training strategy is\nintroduced to train Siamese-DETR on commonly used detection datasets, which\ntakes full advantage of provided annotations; 3) The online tracking pipeline\nis simplified through a tracking-by-query manner by incorporating the tracked\nboxes in previous frame as additional query boxes. The complex data association\nis replaced with the much simpler Non-Maximum Suppression (NMS). Extensive\nexperimental results show that Siamese-DETR surpasses existing MOT methods on\nGMOT-40 dataset by a large margin.",
            "author": [
                "Qiankun Liu",
                "Yichen Li",
                "Yuqi Jiang",
                "Ying Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17875v1",
                "http://arxiv.org/pdf/2310.17875v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17872v1",
            "title": "User Association and Resource Allocation in Large Language Model Based\n  Mobile Edge Computing System over Wireless Communications",
            "updated": "2023-10-27T03:20:49Z",
            "published": "2023-10-27T03:20:49Z",
            "summary": "In the rapidly evolving landscape of large language models (LLMs) and mobile\nedge computing, the need for efficient service delivery to mobile users with\nconstrained computational resources has become paramount. Addressing this, our\npaper delves into a collaborative framework for model training where user data\nand model adapters are shared with servers to optimize performance. Within this\nframework, users initially update the first several layers of the adapters\nwhile freezing the other layers of them, leveraging their local datasets. Once\nthis step is complete, these partially trained parameters are transmitted to\nservers. The servers, equipped with more robust computational capabilities,\nthen update the subsequent layers. After this training, they send the enhanced\nparameters back to the users. This collaborative training approach ensures that\nmobile users with limited computational capacities can still benefit from\nadvanced LLM services without being burdened by exhaustive computations.\nCentral to our methodology is the DASHF algorithm, which encapsulates the\nDinkelbach algorithm, alternating optimization, semidefinite relaxation (SDR),\nthe Hungarian method, and a pioneering fractional programming technique from\nour recent IEEE JSAC paper \"Human-Centric Resource Allocation in the Metaverse\nover Wireless Communications\". The crux of DASHF is its capability to\nreformulate an optimization problem as Quadratically Constrained Quadratic\nProgramming (QCQP) via meticulously crafted transformations, making it solvable\nby SDR and the Hungarian algorithm. Through extensive simulations, we\ndemonstrate the effectiveness of the DASHF algorithm, offering significant\ninsights for the advancement of collaborative LLM service deployments.",
            "author": [
                "Liangxin Qian",
                "Jun Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17872v1",
                "http://arxiv.org/pdf/2310.17872v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17868v1",
            "title": "Resource Allocation for Near-Field Communications: Fundamentals, Tools,\n  and Outlooks",
            "updated": "2023-10-27T03:06:56Z",
            "published": "2023-10-27T03:06:56Z",
            "summary": "Extremely large-scale multiple-input-multiple output (XL-MIMO) is a promising\ntechnology to achieve high spectral efficiency (SE) and energy efficiency (EE)\nin future wireless systems. The larger array aperture of XL-MIMO makes\ncommunication scenarios closer to the near-field region. Therefore, near-field\nresource allocation is essential in realizing the above key performance\nindicators (KPIs). Moreover, the overall performance of XL-MIMO systems heavily\ndepends on the channel characteristics of the selected users, eliminating\ninterference between users through beamforming, power control, etc. The above\nresource allocation issue constitutes a complex joint multi-objective\noptimization problem since many variables and parameters must be optimized,\nincluding the spatial degree of freedom, rate, power allocation, and\ntransmission technique. In this article, we review the basic properties of\nnear-field communications and focus on the corresponding \"resource allocation\"\nproblems. First, we identify available resources in near-field communication\nsystems and highlight their distinctions from far-field communications. Then,\nwe summarize optimization tools, such as numerical techniques and machine\nlearning methods, for addressing near-field resource allocation, emphasizing\ntheir strengths and limitations. Finally, several important research directions\nof near-field communications are pointed out for further investigation.",
            "author": [
                "Bokai Xu",
                "Jiayi Zhang",
                "Hongyang Du",
                "Zhe Wang",
                "Yuanwei Liu",
                "Dusit Niyato",
                "Bo Ai",
                "Khaled B. Letaief"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17868v1",
                "http://arxiv.org/pdf/2310.17868v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17867v1",
            "title": "Reproducibility in Multiple Instance Learning: A Case For Algorithmic\n  Unit Tests",
            "updated": "2023-10-27T03:05:11Z",
            "published": "2023-10-27T03:05:11Z",
            "summary": "Multiple Instance Learning (MIL) is a sub-domain of classification problems\nwith positive and negative labels and a \"bag\" of inputs, where the label is\npositive if and only if a positive element is contained within the bag, and\notherwise is negative. Training in this context requires associating the\nbag-wide label to instance-level information, and implicitly contains a causal\nassumption and asymmetry to the task (i.e., you can't swap the labels without\nchanging the semantics). MIL problems occur in healthcare (one malignant cell\nindicates cancer), cyber security (one malicious executable makes an infected\ncomputer), and many other tasks. In this work, we examine five of the most\nprominent deep-MIL models and find that none of them respects the standard MIL\nassumption. They are able to learn anti-correlated instances, i.e., defaulting\nto \"positive\" labels until seeing a negative counter-example, which should not\nbe possible for a correct MIL model. We suspect that enhancements and other\nworks derived from these models will share the same issue. In any context in\nwhich these models are being used, this creates the potential for learning\nincorrect models, which creates risk of operational failure. We identify and\ndemonstrate this problem via a proposed \"algorithmic unit test\", where we\ncreate synthetic datasets that can be solved by a MIL respecting model, and\nwhich clearly reveal learning that violates MIL assumptions. The five evaluated\nmethods each fail one or more of these tests. This provides a model-agnostic\nway to identify violations of modeling assumptions, which we hope will be\nuseful for future development and evaluation of MIL models.",
            "author": [
                "Edward Raff",
                "James Holt"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17867v1",
                "http://arxiv.org/pdf/2310.17867v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17866v1",
            "title": "Wavelength Dependences of the Optical/UV and X-ray Luminosity\n  Correlations of Quasars",
            "updated": "2023-10-27T03:03:52Z",
            "published": "2023-10-27T03:03:52Z",
            "summary": "The inter-band correlations between optical/UV and X-ray luminosities of\nactive galactic nuclei (AGN) are important for understanding the disc-coronal\nconnection, as well as using AGN as standard candles for cosmology. It is\nconventional to measure the X-ray luminosity at rest frame 2 keV and compare to\nthe UV luminosity at the rest-frame 2500 \\AA, but the wavelength-dependence was\nnever well explored. In this work, we adopt a well-defined sample of 1169\nunobscured quasars in the redshift range 0.13 - 4.51, and apply the\ndirect-correlation method to explore how the correlation with the 2 keV\nluminosity changes at different optical/UV wavelengths, from 1280 - 5550 \\AA\\\nwhere the spectral quality is high. We find that the luminosity at all UV\ncontinuum wavelengths correlates with the X-ray luminosity similarly to that at\n2500 \\AA, and that these correlations are better than at the optical\nwavelengths. Strong self-correlation is also found in the broadband optical/UV\ncontinuum, supporting the scenario that it is dominated by the disc emission.\nCorrelations of various emission lines are also investigated (e.g. C IV, C\nIII], Mg II, H$\\beta$, [O III]$\\lambda\\lambda 4959/5007$), including the\nBaldwin effect and correlations involving line-widths. We find the forms of\nthese line correlations are different, and they are also different from their\nunderlying continua, suggesting various complexities in the line-generation\nprocess. We discuss these results in the disc-wind scenario. Our study confirms\nthat the rest-frame 2500 \\AA\\ is a good wavelength to represent the optical/UV\ncontinual properties of quasars, and shows the advantages of the\ndirect-correlation method.",
            "author": [
                "Chichuan Jin",
                "Elisabeta Lusso",
                "Martin Ward",
                "Chris Done",
                "Riccardo Middei"
            ],
            "link": [
                "http://dx.doi.org/10.1093/mnras/stad3193",
                "http://arxiv.org/abs/2310.17866v1",
                "http://arxiv.org/pdf/2310.17866v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17865v1",
            "title": "Asymmetric Geometry of Total Grassmannians",
            "updated": "2023-10-27T03:02:19Z",
            "published": "2023-10-27T03:02:19Z",
            "summary": "Metrics in Grassmannians, or distances between subspaces of same dimension,\nhave many applications. However, usual extensions to the Total Grassmannian of\nsubspaces of different dimensions lack useful properties or give little\ninformation. Dimensional asymmetries call for the use of asymmetric metrics,\nwhich arise naturally in this space. Their geometry reflects containment\nrelations of subspaces, and minimal geodesics link subspaces of distinct\ndimensions. In particular, the Fubini-Study metric extends as an asymmetric\nangle with useful properties, that can be computed in arbitrary bases or via\nGrassmann algebra. It has a nice geometric interpretation, as does its sine, an\nasymmetric Binet-Cauchy metric.",
            "author": [
                "Andr\u00e9 L. G. Mandolesi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17865v1",
                "http://arxiv.org/pdf/2310.17865v1"
            ],
            "primary_category": "math.MG",
            "category": [
                "math.MG",
                "14M15 (Primary) 15A75, 51K99 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17864v1",
            "title": "TorchAudio 2.1: Advancing speech recognition, self-supervised learning,\n  and audio processing components for PyTorch",
            "updated": "2023-10-27T03:00:51Z",
            "published": "2023-10-27T03:00:51Z",
            "summary": "TorchAudio is an open-source audio and speech processing library built for\nPyTorch. It aims to accelerate the research and development of audio and speech\ntechnologies by providing well-designed, easy-to-use, and performant PyTorch\ncomponents. Its contributors routinely engage with users to understand their\nneeds and fulfill them by developing impactful features. Here, we survey\nTorchAudio's development principles and contents and highlight key features we\ninclude in its latest version (2.1): self-supervised learning pre-trained\npipelines and training recipes, high-performance CTC decoders, speech\nrecognition models and training recipes, advanced media I/O capabilities, and\ntools for performing forced alignment, multi-channel speech enhancement, and\nreference-less speech assessment. For a selection of these features, through\nempirical studies, we demonstrate their efficacy and show that they achieve\ncompetitive or state-of-the-art performance.",
            "author": [
                "Jeff Hwang",
                "Moto Hira",
                "Caroline Chen",
                "Xiaohui Zhang",
                "Zhaoheng Ni",
                "Guangzhi Sun",
                "Pingchuan Ma",
                "Ruizhe Huang",
                "Vineel Pratap",
                "Yuekai Zhang",
                "Anurag Kumar",
                "Chin-Yun Yu",
                "Chuang Zhu",
                "Chunxi Liu",
                "Jacob Kahn",
                "Mirco Ravanelli",
                "Peng Sun",
                "Shinji Watanabe",
                "Yangyang Shi",
                "Yumeng Tao",
                "Robin Scheibler",
                "Samuele Cornell",
                "Sean Kim",
                "Stavros Petridis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17864v1",
                "http://arxiv.org/pdf/2310.17864v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18380v1",
            "title": "Experimental demonstration of picometer level signal extraction with\n  time-delay interferometry technique",
            "updated": "2023-10-27T02:23:44Z",
            "published": "2023-10-27T02:23:44Z",
            "summary": "In this work, we have built an experimental setup to simulate the clock noise\ntransmission with two spacecrafts and two optical links, and further\ndemonstrated the extraction of picometer level signal drowned by the large\nlaser frequency noise and clock noise with the data post-processing method.\nLaser frequency noise is almost eliminated by using the idea of time-delay\ninterferometry (TDI) to construct an equal arm interferometer. Clock\nasynchronism and clock jitter noise are significantly suppressed by laser\nsideband transmitting the clock noise using an electro-optic modulator (EOM).\nExperimental results show a reduction in laser frequency noise by approximately\n10^5 and clock noise by 10^2, recovering a weak displacement signal with an\naverage amplitude about 60 picometer and period 1 second. This work has\nachieved the principle verification of the noise reduction function of TDI\ntechnique to some extent, serving the data processing research of space-borne\ngravitational wave detection.",
            "author": [
                "Mingyang Xu",
                "Yujie Tan",
                "Yurong Liang",
                "Jiawen Zhi",
                "Xiaoyang Guo",
                "Dan Luo",
                "Panpan Wang",
                "Hanzhong Wu",
                "Chenggang Shao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18380v1",
                "http://arxiv.org/pdf/2310.18380v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "gr-qc",
                "physics.ins-det"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17857v1",
            "title": "From Values to Opinions: Predicting Human Behaviors and Stances Using\n  Value-Injected Large Language Models",
            "updated": "2023-10-27T02:18:10Z",
            "published": "2023-10-27T02:18:10Z",
            "summary": "Being able to predict people's opinions on issues and behaviors in realistic\nscenarios can be helpful in various domains, such as politics and marketing.\nHowever, conducting large-scale surveys like the European Social Survey to\nsolicit people's opinions on individual issues can incur prohibitive costs.\nLeveraging prior research showing influence of core human values on individual\ndecisions and actions, we propose to use value-injected large language models\n(LLM) to predict opinions and behaviors. To this end, we present Value\nInjection Method (VIM), a collection of two methods -- argument generation and\nquestion answering -- designed to inject targeted value distributions into LLMs\nvia fine-tuning. We then conduct a series of experiments on four tasks to test\nthe effectiveness of VIM and the possibility of using value-injected LLMs to\npredict opinions and behaviors of people. We find that LLMs value-injected with\nvariations of VIM substantially outperform the baselines. Also, the results\nsuggest that opinions and behaviors can be better predicted using\nvalue-injected LLMs than the baseline approaches.",
            "author": [
                "Dongjun Kang",
                "Joonsuk Park",
                "Yohan Jo",
                "JinYeong Bak"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17857v1",
                "http://arxiv.org/pdf/2310.17857v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18379v1",
            "title": "Influence of EOM sideband modulation noise on space-borne gravitational\n  wave detection",
            "updated": "2023-10-27T02:08:11Z",
            "published": "2023-10-27T02:08:11Z",
            "summary": "Clock noise is one of the dominant noises in the space-borne gravitational\nwave (GW) detection. To suppress this noise, the clock noise-calibrated\ntime-delay-interferometry (TDI) technique is proposed. In this technique, an\ninter-spacecraft clock tone transfer chain is necessary to obtain the\ncomparison information of the clock noises in two spacecraft, during which an\nelectro-optic-modulator (EOM) is critical and used to modulate the clock noise\nto the laser phase. Since the EOM sideband modulation process introduces\nmodulation noise, it is significant to put forward the corresponding\nrequirements and assess whether the commercial EOM meets. In this work, based\non the typical Michelson TDI algorithm and the fundamental noise requirement of\nGW detectors, the analytic expression of the modulation noise requirement is\nstrictly derived, which relax the component indicator need compared to the\nexisting commonly used rough assessments. Furthermore, a commercial EOM\n(iXblue-NIR-10 GHz) is tested, and the experimental results show that it can\nmeet the requirement of the typical GW detection mission LISA in whole\nscientific bandwidth by taking the optimal combination of the data stream. Even\nwhen the displacement measurement accuracy of LISA is improved to 1 pm/\n$\\mathrm{Hz^{1/2}}$ in the future, it still meets the demand.",
            "author": [
                "Mingyang Xu",
                "Yujie Tan",
                "Hanzhong Wu",
                "Panpan Wang",
                "Hao Yan",
                "Yurong Liang",
                "Chenggang Shao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18379v1",
                "http://arxiv.org/pdf/2310.18379v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "gr-qc",
                "physics.ins-det"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17844v1",
            "title": "Adaptive operator learning for infinite-dimensional Bayesian inverse\n  problems",
            "updated": "2023-10-27T01:50:33Z",
            "published": "2023-10-27T01:50:33Z",
            "summary": "The fundamental computational issues in Bayesian inverse problems (BIPs)\ngoverned by partial differential equations (PDEs) stem from the requirement of\nrepeated forward model evaluations. A popular strategy to reduce such cost is\nto replace expensive model simulations by computationally efficient\napproximations using operator learning, motivated by recent progresses in deep\nlearning. However, using the approximated model directly may introduce a\nmodeling error, exacerbating the already ill-posedness of inverse problems.\nThus, balancing between accuracy and efficiency is essential for the effective\nimplementation of such approaches. To this end, we develop an adaptive operator\nlearning framework that can reduce modeling error gradually by forcing the\nsurrogate to be accurate in local areas. This is accomplished by fine-tuning\nthe pre-trained approximate model during the inversion process with adaptive\npoints selected by a greedy algorithm, which requires only a few forward model\nevaluations. To validate our approach, we adopt DeepOnet to construct the\nsurrogate and use unscented Kalman inversion (UKI) to approximate the solution\nof BIPs, respectively. Furthermore, we present rigorous convergence guarantee\nin the linear case using the framework of UKI. We test the approach on several\nbenchmarks, including the Darcy flow, the heat source inversion problem, and\nthe reaction diffusion problems. Numerical results demonstrate that our method\ncan significantly reduce computational costs while maintaining inversion\naccuracy.",
            "author": [
                "Zhiwei Gao",
                "Liang Yan",
                "Tao Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17844v1",
                "http://arxiv.org/pdf/2310.17844v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "stat.CO",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17838v1",
            "title": "Real-time Animation Generation and Control on Rigged Models via Large\n  Language Models",
            "updated": "2023-10-27T01:36:35Z",
            "published": "2023-10-27T01:36:35Z",
            "summary": "We introduce a novel method for real-time animation control and generation on\nrigged models using natural language input. First, we embed a large language\nmodel (LLM) in Unity to output structured texts that can be parsed into diverse\nand realistic animations. Second, we illustrate LLM's potential to enable\nflexible state transition between existing animations. We showcase the\nrobustness of our approach through qualitative results on various rigged models\nand motions.",
            "author": [
                "Han Huang",
                "Fernanda De La Torre",
                "Cathy Mengying Fang",
                "Andrzej Banburski-Fahey",
                "Judith Amores",
                "Jaron Lanier"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17838v1",
                "http://arxiv.org/pdf/2310.17838v1"
            ],
            "primary_category": "cs.GR",
            "category": [
                "cs.GR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18378v1",
            "title": "Ontology Revision based on Pre-trained Language Models",
            "updated": "2023-10-27T00:52:01Z",
            "published": "2023-10-27T00:52:01Z",
            "summary": "Ontology revision aims to seamlessly incorporate new information into an\nexisting ontology and plays a crucial role in tasks such as ontology evolution,\nontology maintenance, and ontology alignment. Similar to repair single\nontologies, resolving logical incoherence in the task of ontology revision is\nalso important and meaningful since incoherence is a main potential factor to\ncause inconsistency and reasoning with an inconsistent ontology will obtain\nmeaningless answers. To deal with this problem, various ontology revision\nmethods have been proposed to define revision operators and design ranking\nstrategies for axioms in an ontology. However, they rarely consider axiom\nsemantics which provides important information to differentiate axioms. On the\nother hand, pre-trained models can be utilized to encode axiom semantics, and\nhave been widely applied in many natural language processing tasks and\nontology-related ones in recent years. Therefore, in this paper, we define four\nscoring functions to rank axioms based on a pre-trained model by considering\nvarious information from a rebuttal ontology and its corresponding reliable\nontology. Based on such a scoring function, we propose an ontology revision\nalgorithm to deal with unsatisfiable concepts at once. If it is hard to resolve\nall unsatisfiable concepts in a rebuttal ontology together, an adapted revision\nalgorithm is designed to deal with them group by group. We conduct experiments\nover 19 ontology pairs and compare our algorithms and scoring functions with\nexisting ones. According to the experiments, it shows that our algorithms could\nachieve promising performance. The adapted revision algorithm could improve the\nefficiency largely, and at most 96% time could be saved for some ontology\npairs. Some of our scoring functions help a revision algorithm obtain better\nresults in many cases, especially for the challenging pairs.",
            "author": [
                "Qiu Ji",
                "Guilin Qi",
                "Yuxin Ye",
                "Jiaye Li",
                "Site Li",
                "Jianjie Ren",
                "Songtao Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18378v1",
                "http://arxiv.org/pdf/2310.18378v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18377v1",
            "title": "Large-scale Foundation Models and Generative AI for BigData Neuroscience",
            "updated": "2023-10-27T00:44:40Z",
            "published": "2023-10-27T00:44:40Z",
            "summary": "Recent advances in machine learning have made revolutionary breakthroughs in\ncomputer games, image and natural language understanding, and scientific\ndiscovery. Foundation models and large-scale language models (LLMs) have\nrecently achieved human-like intelligence thanks to BigData. With the help of\nself-supervised learning (SSL) and transfer learning, these models may\npotentially reshape the landscapes of neuroscience research and make a\nsignificant impact on the future. Here we present a mini-review on recent\nadvances in foundation models and generative AI models as well as their\napplications in neuroscience, including natural language and speech, semantic\nmemory, brain-machine interfaces (BMIs), and data augmentation. We argue that\nthis paradigm-shift framework will open new avenues for many neuroscience\nresearch directions and discuss the accompanying challenges and opportunities.",
            "author": [
                "Ran Wang",
                "Zhe Sage Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18377v1",
                "http://arxiv.org/pdf/2310.18377v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC",
                "cs.AI",
                "cs.HC",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17826v1",
            "title": "OmniFill: Domain-Agnostic Form Filling Suggestions Using Multi-Faceted\n  Context",
            "updated": "2023-10-27T00:25:17Z",
            "published": "2023-10-27T00:25:17Z",
            "summary": "Predictive suggestion systems offer contextually-relevant text entry\ncompletions. Existing approaches, like autofill, often excel in\nnarrowly-defined domains but fail to generalize to arbitrary workflows. We\nintroduce a conceptual framework to analyze the compound demands of a\nparticular suggestion context, yielding unique opportunities for large language\nmodels (LLMs) to infer suggestions for a wide range of domain-agnostic\nform-filling tasks that were out of reach with prior approaches. We explore\nthese opportunities in OmniFill, a prototype that collects multi-faceted\ncontext including browsing and text entry activity to construct an LLM prompt\nthat offers suggestions in situ for arbitrary structured text entry interfaces.\nThrough a user study with 18 participants, we found that OmniFill offered\nvaluable suggestions and we identified four themes that characterize users'\nbehavior and attitudes: an \"opportunistic scrapbooking\" approach; a trust\nplaced in the system; value in partial success; and a need for visibility into\nprompt context.",
            "author": [
                "Timothy J. Aveni",
                "Armando Fox",
                "Bj\u00f6rn Hartmann"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17826v1",
                "http://arxiv.org/pdf/2310.17826v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18376v1",
            "title": "SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL\n  Translation",
            "updated": "2023-10-27T00:13:59Z",
            "published": "2023-10-27T00:13:59Z",
            "summary": "In recent years, there has been growing interest in text-to-SQL translation,\nwhich is the task of converting natural language questions into executable SQL\nqueries. This technology is important for its potential to democratize data\nextraction from databases. However, some of its key hurdles include domain\ngeneralisation, which is the ability to adapt to previously unseen databases,\nand alignment of natural language questions with the corresponding SQL queries.\nTo overcome these challenges, we introduce SQLformer, a novel Transformer\narchitecture specifically crafted to perform text-to-SQL translation tasks. Our\nmodel predicts SQL queries as abstract syntax trees (ASTs) in an autoregressive\nway, incorporating structural inductive bias in the encoder and decoder layers.\nThis bias, guided by database table and column selection, aids the decoder in\ngenerating SQL query ASTs represented as graphs in a Breadth-First Search\ncanonical order. Comprehensive experiments illustrate the state-of-the-art\nperformance of SQLformer in the challenging text-to-SQL Spider benchmark. Our\nimplementation is available at https://github.com/AdrianBZG/SQLformer",
            "author": [
                "Adri\u00e1n Bazaga",
                "Pietro Li\u00f2",
                "Gos Micklem"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18376v1",
                "http://arxiv.org/pdf/2310.18376v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17824v2",
            "title": "Ab-initio study of the energy competition between \u0393and K valleys\n  in bilayer transition metal dichalcogenides",
            "updated": "2023-10-30T17:23:01Z",
            "published": "2023-10-27T00:06:56Z",
            "summary": "Moir\\'e engineering in two-dimensional van der Waals bilayer crystals has\nemerged as a flexible platform for controlling strongly correlated electron\nsystems. The competition between valleys for the band extremum energy position\nin the parent layers is crucial in deciding the qualitative nature of the\nmoir\\'e Hamiltonian since it controls the physics of the moir\\'e minibands.\nHere we use density functional theory to examine the competition between K and\n$\\Gamma$ for the valence band maximum in homo- and hetero-bilayers formed from\nthe transition metal dichalcogenides (TMD), MX\\{_2} where M=Mo,W and X=S,Se,Te.\nWe shed light on how the competition is influenced by interlayer separation,\nwhich can be modified by applying pressure, by external gate-defined electric\nfields, and by transition metal atom d-orbital correlations. Our findings are\nrelated to several recent experiments, and contribute to the development of\ndesign rules for moir\\'{e} materials.",
            "author": [
                "Sam Olin",
                "Erekle Jmukhadze",
                "Allan H. MacDonald",
                "Wei-Cheng Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17824v2",
                "http://arxiv.org/pdf/2310.17824v2"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17819v2",
            "title": "Multiplexed Processing of Quantum Information Across an Ultra-wide\n  Optical Bandwidth",
            "updated": "2023-10-30T21:03:00Z",
            "published": "2023-10-26T23:50:20Z",
            "summary": "Quantum information processing is the foundation of quantum technology.\nProtocols of quantum information share secrets between two distant parties for\nsecure communication (quantum key distribution), teleport quantum states, and\nstand at the heart of quantum computation. While various protocols of quantum\ncommunication have already been realized, and even commercialized, their\ncommunication speed is generally low, limited by the narrow electronic\nbandwidth of the measurement apparatus in the MHz-to-GHz range, which is\norders-of-magnitude lower than the optical bandwidth of available quantum\noptical sources (10-100 THz). We present and demonstrate an efficient method to\nprocess quantum information with such broadband sources in parallel over\nmultiplexed frequency channels using parametric homodyne detection for\nsimultaneous measurement of all the channels. Specifically, we propose two\nbasic protocols: A multiplexed Continuous-Variable Quantum Key Distribution\n(CV-QKD) and A multiplexed continuous-variable quantum teleportation protocol.\nWe demonstrate the multiplexed CV-QKD protocol in a proof-of-principle\nexperiment, where we successfully carry out QKD over 23 uncorrelated spectral\nchannels and show the ability to detect eavesdropping in any of them. These\nmultiplexed methods (and similar) will enable to carry out quantum processing\nin parallel over hundreds of channels, potentially increasing the throughput of\nquantum protocols by orders of magnitude",
            "author": [
                "Alon Eldan",
                "Ofek Gilon",
                "Asher Lagimi",
                "Elai Forman",
                "Avi Pe'er"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17819v2",
                "http://arxiv.org/pdf/2310.17819v2"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17813v1",
            "title": "A Spectral Condition for Feature Learning",
            "updated": "2023-10-26T23:17:39Z",
            "published": "2023-10-26T23:17:39Z",
            "summary": "The push to train ever larger neural networks has motivated the study of\ninitialization and training at large network width. A key challenge is to scale\ntraining so that a network's internal representations evolve nontrivially at\nall widths, a process known as feature learning. Here, we show that feature\nlearning is achieved by scaling the spectral norm of weight matrices and their\nupdates like $\\sqrt{\\texttt{fan-out}/\\texttt{fan-in}}$, in contrast to widely\nused but heuristic scalings based on Frobenius norm and entry size. Our\nspectral scaling analysis also leads to an elementary derivation of\n\\emph{maximal update parametrization}. All in all, we aim to provide the reader\nwith a solid conceptual understanding of feature learning in neural networks.",
            "author": [
                "Greg Yang",
                "James B. Simon",
                "Jeremy Bernstein"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17813v1",
                "http://arxiv.org/pdf/2310.17813v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17812v2",
            "title": "Prospects for thermalization of microwave-shielded ultracold molecules",
            "updated": "2023-11-11T18:12:14Z",
            "published": "2023-10-26T23:06:47Z",
            "summary": "We study anisotropic thermalization in dilute gases of microwave shielded\npolar molecular fermions. For collision energies above the threshold regime, we\nfind that thermalization is suppressed due to a strong preference for forward\nscattering and a reduction in total cross section with energy, significantly\nreducing the efficiency of evaporative cooling. We perform close-coupling\ncalculations on the effective potential energy surface derived by Deng et al.\n[Phys. Rev. Lett. 130, 183001 (2023)], to obtain accurate 2-body elastic\ndifferential cross sections across a range of collision energies. We use\nGaussian process regression to obtain a global representation of the\ndifferential cross section, over a wide range of collision angles and energies.\nThe route to equilibrium is then analyzed with cross-dimensional\nrethermalization experiments, quantified by a measure of collisional efficiency\ntoward achieving thermalization.",
            "author": [
                "Reuben R. W. Wang",
                "John L. Bohn"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17812v2",
                "http://arxiv.org/pdf/2310.17812v2"
            ],
            "primary_category": "cond-mat.quant-gas",
            "category": [
                "cond-mat.quant-gas",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17811v2",
            "title": "Style-Aware Radiology Report Generation with RadGraph and Few-Shot\n  Prompting",
            "updated": "2023-10-31T17:07:17Z",
            "published": "2023-10-26T23:06:38Z",
            "summary": "Automatically generated reports from medical images promise to improve the\nworkflow of radiologists. Existing methods consider an image-to-report modeling\ntask by directly generating a fully-fledged report from an image. However, this\nconflates the content of the report (e.g., findings and their attributes) with\nits style (e.g., format and choice of words), which can lead to clinically\ninaccurate reports. To address this, we propose a two-step approach for\nradiology report generation. First, we extract the content from an image; then,\nwe verbalize the extracted content into a report that matches the style of a\nspecific radiologist. For this, we leverage RadGraph -- a graph representation\nof reports -- together with large language models (LLMs). In our quantitative\nevaluations, we find that our approach leads to beneficial performance. Our\nhuman evaluation with clinical raters highlights that the AI-generated reports\nare indistinguishably tailored to the style of individual radiologist despite\nleveraging only a few examples as context.",
            "author": [
                "Benjamin Yan",
                "Ruochen Liu",
                "David E. Kuo",
                "Subathra Adithan",
                "Eduardo Pontes Reis",
                "Stephen Kwak",
                "Vasantha Kumar Venugopal",
                "Chloe P. O'Connell",
                "Agustina Saenz",
                "Pranav Rajpurkar",
                "Michael Moor"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17811v2",
                "http://arxiv.org/pdf/2310.17811v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17807v1",
            "title": "Clover: Closed-Loop Verifiable Code Generation",
            "updated": "2023-10-26T22:58:19Z",
            "published": "2023-10-26T22:58:19Z",
            "summary": "The use of large language models for code generation is a rapidly growing\ntrend in software development. However, without effective methods for ensuring\nthe correctness of generated code, this trend could lead to any number of\nundesirable outcomes. In this paper, we lay out a vision for addressing this\nchallenge: the Clover paradigm, short for Closed-Loop Verifiable Code\nGeneration, which reduces correctness checking to the more accessible problem\nof consistency checking. At the core of Clover lies a checker that performs\nconsistency checks among code, docstrings, and formal annotations. The checker\nis implemented using a novel integration of formal verification tools and large\nlanguage models. We provide a theoretical analysis to support our thesis that\nClover should be effective at consistency checking. We also empirically\ninvestigate its feasibility on a hand-designed dataset (CloverBench) featuring\nannotated Dafny programs at a textbook level of difficulty. Experimental\nresults show that for this dataset, (i) LLMs are reasonably successful at\nautomatically generating formal specifications; and (ii) our consistency\nchecker achieves a promising acceptance rate (up to 87%) for correct instances\nwhile maintaining zero tolerance for incorrect ones (no false positives).",
            "author": [
                "Chuyue Sun",
                "Ying Sheng",
                "Oded Padon",
                "Clark Barrett"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17807v1",
                "http://arxiv.org/pdf/2310.17807v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.03373v1",
            "title": "Unscrambling the Rectification of Adversarial Attacks Transferability\n  across Computer Networks",
            "updated": "2023-10-26T22:36:24Z",
            "published": "2023-10-26T22:36:24Z",
            "summary": "Convolutional neural networks (CNNs) models play a vital role in achieving\nstate-of-the-art performances in various technological fields. CNNs are not\nlimited to Natural Language Processing (NLP) or Computer Vision (CV) but also\nhave substantial applications in other technological domains, particularly in\ncybersecurity. The reliability of CNN's models can be compromised because of\ntheir susceptibility to adversarial attacks, which can be generated\neffortlessly, easily applied, and transferred in real-world scenarios.\n  In this paper, we present a novel and comprehensive method to improve the\nstrength of attacks and assess the transferability of adversarial examples in\nCNNs when such strength changes, as well as whether the transferability\nproperty issue exists in computer network applications. In the context of our\nstudy, we initially examined six distinct modes of attack: the Carlini and\nWagner (C&W), Fast Gradient Sign Method (FGSM), Iterative Fast Gradient Sign\nMethod (I-FGSM), Jacobian-based Saliency Map (JSMA), Limited-memory Broyden\nfletcher Goldfarb Shanno (L-BFGS), and Projected Gradient Descent (PGD) attack.\nWe applied these attack techniques on two popular datasets: the CIC and UNSW\ndatasets. The outcomes of our experiment demonstrate that an improvement in\ntransferability occurs in the targeted scenarios for FGSM, JSMA, LBFGS, and\nother attacks. Our findings further indicate that the threats to security posed\nby adversarial examples, even in computer network applications, necessitate the\ndevelopment of novel defense mechanisms to enhance the security of DL-based\ntechniques.",
            "author": [
                "Ehsan Nowroozi",
                "Samaneh Ghelichkhani",
                "Imran Haider",
                "Ali Dehghantanha"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03373v1",
                "http://arxiv.org/pdf/2311.03373v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17802v1",
            "title": "TIMELINE: Exhaustive Annotation of Temporal Relations Supporting the\n  Automatic Ordering of Events in News Articles",
            "updated": "2023-10-26T22:23:38Z",
            "published": "2023-10-26T22:23:38Z",
            "summary": "Temporal relation extraction models have thus far been hindered by a number\nof issues in existing temporal relation-annotated news datasets, including: (1)\nlow inter-annotator agreement due to the lack of specificity of their\nannotation guidelines in terms of what counts as a temporal relation; (2) the\nexclusion of long-distance relations within a given document (those spanning\nacross different paragraphs); and (3) the exclusion of events that are not\ncentred on verbs. This paper aims to alleviate these issues by presenting a new\nannotation scheme that clearly defines the criteria based on which temporal\nrelations should be annotated. Additionally, the scheme includes events even if\nthey are not expressed as verbs (e.g., nominalised events). Furthermore, we\npropose a method for annotating all temporal relations -- including\nlong-distance ones -- which automates the process, hence reducing time and\nmanual effort on the part of annotators. The result is a new dataset, the\nTIMELINE corpus, in which improved inter-annotator agreement was obtained, in\ncomparison with previously reported temporal relation datasets. We report the\nresults of training and evaluating baseline temporal relation extraction models\non the new corpus, and compare them with results obtained on the widely used\nMATRES corpus.",
            "author": [
                "Sarah Alsayyahi",
                "Riza Batista-Navarro"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17802v1",
                "http://arxiv.org/pdf/2310.17802v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17801v1",
            "title": "Image Prior and Posterior Conditional Probability Representation for\n  Efficient Damage Assessment",
            "updated": "2023-10-26T22:17:37Z",
            "published": "2023-10-26T22:17:37Z",
            "summary": "It is important to quantify Damage Assessment (DA) for Human Assistance and\nDisaster Response (HADR) applications. In this paper, to achieve efficient and\nscalable DA in HADR, an image prior and posterior conditional probability\n(IP2CP) is developed as an effective computational imaging representation.\nEquipped with the IP2CP representation, the matching pre- and post-disaster\nimages are effectively encoded into one image that is then processed using deep\nlearning approaches to determine the damage levels. Two scenarios of crucial\nimportance for the practical use of DA in HADR applications are examined:\npixel-wise semantic segmentation and patch-based contrastive learning-based\nglobal damage classification. Results achieved by IP2CP in both scenarios\ndemonstrate promising performances, showing that our IP2CP-based methods within\nthe deep learning framework can effectively achieve data and computational\nefficiency, which is of utmost importance for the DA in HADR applications.",
            "author": [
                "Jie Wei",
                "Weicong Feng",
                "Erik Blasch",
                "Erika Ardiles-Cruz",
                "Haibin Ling"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17801v1",
                "http://arxiv.org/pdf/2310.17801v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "I.4.6, I.5.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17800v1",
            "title": "Interacting Diffusion Processes for Event Sequence Forecasting",
            "updated": "2023-10-26T22:17:25Z",
            "published": "2023-10-26T22:17:25Z",
            "summary": "Neural Temporal Point Processes (TPPs) have emerged as the primary framework\nfor predicting sequences of events that occur at irregular time intervals, but\ntheir sequential nature can hamper performance for long-horizon forecasts. To\naddress this, we introduce a novel approach that incorporates a diffusion\ngenerative model. The model facilitates sequence-to-sequence prediction,\nallowing multi-step predictions based on historical event sequences. In\ncontrast to previous approaches, our model directly learns the joint\nprobability distribution of types and inter-arrival times for multiple events.\nThis allows us to fully leverage the high dimensional modeling capability of\nmodern generative models. Our model is composed of two diffusion processes, one\nfor the time intervals and one for the event types. These processes interact\nthrough their respective denoising functions, which can take as input\nintermediate representations from both processes, allowing the model to learn\ncomplex interactions. We demonstrate that our proposal outperforms\nstate-of-the-art baselines for long-horizon forecasting of TPP.",
            "author": [
                "Mai Zeng",
                "Florence Regol",
                "Mark Coates"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17800v1",
                "http://arxiv.org/pdf/2310.17800v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17798v1",
            "title": "Maximum entropy-based modeling of community-level hazard responses for\n  civil infrastructures",
            "updated": "2023-10-26T22:02:06Z",
            "published": "2023-10-26T22:02:06Z",
            "summary": "Perturbed by natural hazards, community-level infrastructure networks operate\nlike many-body systems, with behaviors emerging from coupling individual\ncomponent dynamics with group correlations and interactions. It follows that we\ncan borrow methods from statistical physics to study the response of\ninfrastructure systems to natural disasters. This study aims to construct a\njoint probability distribution model to describe the post-hazard state of\ninfrastructure networks and propose an efficient surrogate model of the joint\ndistribution for large-scale systems. Specifically, we present maximum entropy\nmodeling of the regional impact of natural hazards on civil infrastructures.\nProvided with the current state of knowledge, the principle of maximum entropy\nyields the ``most unbiased`` joint distribution model for the performances of\ninfrastructures. In the general form, the model can handle multivariate\nperformance states and higher-order correlations. In a particular yet typical\nscenario of binary performance state variables with knowledge of their mean and\npairwise correlation, the joint distribution reduces to the Ising model in\nstatistical physics. In this context, we propose using a dichotomized Gaussian\nmodel as an efficient surrogate for the maximum entropy model, facilitating the\napplication to large systems. Using the proposed method, we investigate the\nseismic collective behavior of a large-scale road network (with 8,694 nodes and\n26,964 links) in San Francisco, showcasing the non-trivial collective behaviors\nof infrastructure systems.",
            "author": [
                "Xiaolei Chu",
                "Ziqi Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17798v1",
                "http://arxiv.org/pdf/2310.17798v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17796v2",
            "title": "ControlLLM: Augment Language Models with Tools by Searching on Graphs",
            "updated": "2023-10-30T17:30:47Z",
            "published": "2023-10-26T21:57:21Z",
            "summary": "We present ControlLLM, a novel framework that enables large language models\n(LLMs) to utilize multi-modal tools for solving complex real-world tasks.\nDespite the remarkable performance of LLMs, they still struggle with tool\ninvocation due to ambiguous user prompts, inaccurate tool selection and\nparameterization, and inefficient tool scheduling. To overcome these\nchallenges, our framework comprises three key components: (1) a \\textit{task\ndecomposer} that breaks down a complex task into clear subtasks with\nwell-defined inputs and outputs; (2) a \\textit{Thoughts-on-Graph (ToG)\nparadigm} that searches the optimal solution path on a pre-built tool graph,\nwhich specifies the parameter and dependency relations among different tools;\nand (3) an \\textit{execution engine with a rich toolbox} that interprets the\nsolution path and runs the tools efficiently on different computational\ndevices. We evaluate our framework on diverse tasks involving image, audio, and\nvideo processing, demonstrating its superior accuracy, efficiency, and\nversatility compared to existing methods. The code is at\nhttps://github.com/OpenGVLab/ControlLLM .",
            "author": [
                "Zhaoyang Liu",
                "Zeqiang Lai",
                "Zhangwei Gao",
                "Erfei Cui",
                "Zhiheng Li",
                "Xizhou Zhu",
                "Lewei Lu",
                "Qifeng Chen",
                "Yu Qiao",
                "Jifeng Dai",
                "Wenhai Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17796v2",
                "http://arxiv.org/pdf/2310.17796v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17793v1",
            "title": "\"You Are An Expert Linguistic Annotator\": Limits of LLMs as Analyzers of\n  Abstract Meaning Representation",
            "updated": "2023-10-26T21:47:59Z",
            "published": "2023-10-26T21:47:59Z",
            "summary": "Large language models (LLMs) show amazing proficiency and fluency in the use\nof language. Does this mean that they have also acquired insightful linguistic\nknowledge about the language, to an extent that they can serve as an \"expert\nlinguistic annotator\"? In this paper, we examine the successes and limitations\nof the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning\nstructure, focusing on the Abstract Meaning Representation (AMR; Banarescu et\nal. 2013) parsing formalism, which provides rich graphical representations of\nsentence meaning structure while abstracting away from surface forms. We\ncompare models' analysis of this semantic structure across two settings: 1)\ndirect production of AMR parses based on zero- and few-shot prompts, and 2)\nindirect partial reconstruction of AMR via metalinguistic natural language\nqueries (e.g., \"Identify the primary event of this sentence, and the predicate\ncorresponding to that event.\"). Across these settings, we find that models can\nreliably reproduce the basic format of AMR, and can often capture core event,\nargument, and modifier structure -- however, model outputs are prone to\nfrequent and major errors, and holistic analysis of parse acceptability shows\nthat even with few-shot demonstrations, models have virtually 0% success in\nproducing fully accurate parses. Eliciting natural language responses produces\nsimilar patterns of errors. Overall, our findings indicate that these models\nout-of-the-box can capture aspects of semantic structure, but there remain key\nlimitations in their ability to support fully accurate semantic analyses or\nparses.",
            "author": [
                "Allyson Ettinger",
                "Jena D. Hwang",
                "Valentina Pyatkin",
                "Chandra Bhagavatula",
                "Yejin Choi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17793v1",
                "http://arxiv.org/pdf/2310.17793v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18375v1",
            "title": "CMOS-based Single-Cycle In-Memory XOR/XNOR",
            "updated": "2023-10-26T21:43:01Z",
            "published": "2023-10-26T21:43:01Z",
            "summary": "Big data applications are on the rise, and so is the number of data centers.\nThe ever-increasing massive data pool needs to be periodically backed up in a\nsecure environment. Moreover, a massive amount of securely backed-up data is\nrequired for training binary convolutional neural networks for image\nclassification. XOR and XNOR operations are essential for large-scale data copy\nverification, encryption, and classification algorithms. The disproportionate\nspeed of existing compute and memory units makes the von Neumann architecture\ninefficient to perform these Boolean operations. Compute-in-memory (CiM) has\nproved to be an optimum approach for such bulk computations. The existing\nCiM-based XOR/XNOR techniques either require multiple cycles for computing or\nadd to the complexity of the fabrication process. Here, we propose a CMOS-based\nhardware topology for single-cycle in-memory XOR/XNOR operations. Our design\nprovides at least 2 times improvement in the latency compared with other\nexisting CMOS-compatible solutions. We verify the proposed system through\ncircuit/system-level simulations and evaluate its robustness using a 5000-point\nMonte Carlo variation analysis. This all-CMOS design paves the way for\npractical implementation of CiM XOR/XNOR at scaled technology nodes.",
            "author": [
                "Shamiul Alam",
                "Jack Hutchins",
                "Nikhil Shukla",
                "Kazi Asifuzzaman",
                "Ahmedullah Aziz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18375v1",
                "http://arxiv.org/pdf/2310.18375v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17788v1",
            "title": "Utilizing Language Models for Energy Load Forecasting",
            "updated": "2023-10-26T21:36:06Z",
            "published": "2023-10-26T21:36:06Z",
            "summary": "Energy load forecasting plays a crucial role in optimizing resource\nallocation and managing energy consumption in buildings and cities. In this\npaper, we propose a novel approach that leverages language models for energy\nload forecasting. We employ prompting techniques to convert energy consumption\ndata into descriptive sentences, enabling fine-tuning of language models. By\nadopting an autoregressive generating approach, our proposed method enables\npredictions of various horizons of future energy load consumption. Through\nextensive experiments on real-world datasets, we demonstrate the effectiveness\nand accuracy of our proposed method. Our results indicate that utilizing\nlanguage models for energy load forecasting holds promise for enhancing energy\nefficiency and facilitating intelligent decision-making in energy systems.",
            "author": [
                "Hao Xue",
                "Flora D. Salim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17788v1",
                "http://arxiv.org/pdf/2310.17788v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17787v1",
            "title": "Evaluation of large language models using an Indian language LGBTI+\n  lexicon",
            "updated": "2023-10-26T21:32:24Z",
            "published": "2023-10-26T21:32:24Z",
            "summary": "Large language models (LLMs) are typically evaluated on the basis of\ntask-based benchmarks such as MMLU. Such benchmarks do not examine responsible\nbehaviour of LLMs in specific contexts. This is particularly true in the LGBTI+\ncontext where social stereotypes may result in variation in LGBTI+ terminology.\nTherefore, domain-specific lexicons or dictionaries may be useful as a\nrepresentative list of words against which the LLM's behaviour needs to be\nevaluated. This paper presents a methodology for evaluation of LLMs using an\nLGBTI+ lexicon in Indian languages. The methodology consists of four steps:\nformulating NLP tasks relevant to the expected behaviour, creating prompts that\ntest LLMs, using the LLMs to obtain the output and, finally, manually\nevaluating the results. Our qualitative analysis shows that the three LLMs we\nexperiment on are unable to detect underlying hateful content. Similarly, we\nobserve limitations in using machine translation as means to evaluate natural\nlanguage understanding in languages other than English. The methodology\npresented in this paper can be useful for LGBTI+ lexicons in other languages as\nwell as other domain-specific lexicons. The work done in this paper opens\navenues for responsible behaviour of LLMs, as demonstrated in the context of\nprevalent social perception of the LGBTI+ community.",
            "author": [
                "Aditya Joshi",
                "Shruta Rawat",
                "Alpana Dange"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17787v1",
                "http://arxiv.org/pdf/2310.17787v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17782v1",
            "title": "Bidisperse beds sheared by viscous fluids: Grain segregation and bed\n  hardening",
            "updated": "2023-10-26T21:16:25Z",
            "published": "2023-10-26T21:16:25Z",
            "summary": "When a granular bed is sheared by a fluid that flows above a critical limit,\nit undergoes a complex motion that varies along time: it can contain fluid-\n(bedload) and solid-like (creep) regions, being prone to strain hardening and,\nin case of polydispersity, segregation. In this paper, we investigate\nexperimentally the short- and long-time evolution of a bidisperse bed sheared\nby a viscous liquid. Different from previous experiments, the density ratio\nbetween grains and fluid is 2.7, close to values found in rivers and oceans. We\nshow the existence of diffusive, advective and constrained regions, that most\nof segregation occurs during the very first stages of the flow, and that bed\nhardening becomes stronger while bedload and creep weaken along time. We obtain\nthe segregation rates, their evolution along time, their variation with the\napplied shearing, and the time evolution of creeping and bedload. Finally, we\npropose characteristic times for the segregation of large particles and bed\nhardening. Our results shed light on the complex motion of sheared beds\nexisting in nature, such as river beds and creeping lands.",
            "author": [
                "Jaime Oswaldo Gonzalez Maya",
                "Fernando David C\u00fa\u00f1ez Benalc\u00e1zar",
                "Erick de Moraes Franklin"
            ],
            "link": [
                "http://dx.doi.org/10.1063/5.0168415",
                "http://arxiv.org/abs/2310.17782v1",
                "http://arxiv.org/pdf/2310.17782v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn",
                "cond-mat.soft",
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17781v1",
            "title": "Design and implementation of a seismic Newtonian-noise cancellation\n  system for the Virgo gravitational-wave detector",
            "updated": "2023-10-26T21:13:55Z",
            "published": "2023-10-26T21:13:55Z",
            "summary": "Terrestrial gravity perturbations caused by seismic fields produce the\nso-called Newtonian noise in gravitational-wave detectors, which is predicted\nto limit their sensitivity in the upcoming observing runs. In the past, this\nnoise was seen as an infrastructural limitation, i.e., something that cannot be\novercome without major investments to improve a detector's infrastructure.\nHowever, it is possible to have at least an indirect estimate of this noise by\nusing the data from a large number of seismometers deployed around a detector's\nsuspended test masses. The noise estimate can be subtracted from the\ngravitational-wave data; a process called Newtonian-noise cancellation (NNC).\nIn this article, we present the design and implementation of the first NNC\nsystem at the Virgo detector as part of its AdV+ upgrade. It uses data from 110\nvertical geophones deployed inside the Virgo buildings in optimized array\nconfigurations. We use a separate tiltmeter channel to test the pipeline in a\nproof-of-principle. The system has been running with good performance over\nmonths.",
            "author": [
                "Soumen Koley",
                "Jan Harms",
                "Annalisa Allocca",
                "Enrico Calloni",
                "Rosario De Rosa",
                "Luciano Errico",
                "Marina Esposito",
                "Francesca Badaracco",
                "Luca Rei",
                "Alessandro Bertolini",
                "Tomasz Bulik",
                "Marek Cieslar",
                "Mateusz Pietrzak",
                "Mariusz Suchenek",
                "Irene Fiori",
                "Andrea Paoli",
                "Maria Concetta Tringali",
                "Paolo Ruggi",
                "Stefan Hild",
                "Ayatri Singha",
                "Bartosz Idzkowski",
                "Maciej Suchinski",
                "Alain Masserot",
                "Loic Rolland",
                "Benoit Mours",
                "Federico Paoletti"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17781v1",
                "http://arxiv.org/pdf/2310.17781v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc",
                "astro-ph.IM",
                "physics.ins-det"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17780v1",
            "title": "AutoCT: Automated CT registration, segmentation, and quantification",
            "updated": "2023-10-26T21:09:47Z",
            "published": "2023-10-26T21:09:47Z",
            "summary": "The processing and analysis of computed tomography (CT) imaging is important\nfor both basic scientific development and clinical applications. In AutoCT, we\nprovide a comprehensive pipeline that integrates an end-to-end automatic\npreprocessing, registration, segmentation, and quantitative analysis of 3D CT\nscans. The engineered pipeline enables atlas-based CT segmentation and\nquantification leveraging diffeomorphic transformations through efficient\nforward and inverse mappings. The extracted localized features from the\ndeformation field allow for downstream statistical learning that may facilitate\nmedical diagnostics. On a lightweight and portable software platform, AutoCT\nprovides a new toolkit for the CT imaging community to underpin the deployment\nof artificial intelligence-driven applications.",
            "author": [
                "Zhe Bai",
                "Abdelilah Essiari",
                "Talita Perciano",
                "Kristofer E. Bouchard"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17780v1",
                "http://arxiv.org/pdf/2310.17780v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17775v2",
            "title": "Functional Limit Theorems for Local Functionals of Dynamic Point\n  Processes",
            "updated": "2023-11-27T02:43:39Z",
            "published": "2023-10-26T20:55:50Z",
            "summary": "We establish functional limit theorems for local, additive, interaction\nfunctions of temporally evolving point processes. The dynamics are those of a\nspatial Poisson process on the flat torus with points subject to a birth-death\nmechanism, and which move according to Brownian motion while alive. The results\nreveal the existence of a phase diagram describing at least three distinct\nstructures for the limiting processes, depending on the extent of the local\ninteractions and the speed of the Brownian motions. The proofs, which identify\nthree different limits, rely heavily on Malliavin-Stein bounds on a\nrepresentation of the dynamic point process via a distributionally equivalent\nmarked point process.",
            "author": [
                "Efe Onaran",
                "Omer Bobrowski",
                "Robert J. Adler"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17775v2",
                "http://arxiv.org/pdf/2310.17775v2"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "60G55, 60F17 (Primary) 60D05, 60G15 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17774v1",
            "title": "Words, Subwords, and Morphemes: What Really Matters in the\n  Surprisal-Reading Time Relationship?",
            "updated": "2023-10-26T20:55:29Z",
            "published": "2023-10-26T20:55:29Z",
            "summary": "An important assumption that comes with using LLMs on psycholinguistic data\nhas gone unverified. LLM-based predictions are based on subword tokenization,\nnot decomposition of words into morphemes. Does that matter? We carefully test\nthis by comparing surprisal estimates using orthographic, morphological, and\nBPE tokenization against reading time data. Our results replicate previous\nfindings and provide evidence that in the aggregate, predictions using BPE\ntokenization do not suffer relative to morphological and orthographic\nsegmentation. However, a finer-grained analysis points to potential issues with\nrelying on BPE-based tokenization, as well as providing promising results\ninvolving morphologically-aware surprisal estimates and suggesting a new method\nfor evaluating morphological prediction.",
            "author": [
                "Sathvik Nair",
                "Philip Resnik"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17774v1",
                "http://arxiv.org/pdf/2310.17774v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17770v1",
            "title": "GROOViST: A Metric for Grounding Objects in Visual Storytelling",
            "updated": "2023-10-26T20:27:16Z",
            "published": "2023-10-26T20:27:16Z",
            "summary": "A proper evaluation of stories generated for a sequence of images -- the task\ncommonly referred to as visual storytelling -- must consider multiple aspects,\nsuch as coherence, grammatical correctness, and visual grounding. In this work,\nwe focus on evaluating the degree of grounding, that is, the extent to which a\nstory is about the entities shown in the images. We analyze current metrics,\nboth designed for this purpose and for general vision-text alignment. Given\ntheir observed shortcomings, we propose a novel evaluation tool, GROOViST, that\naccounts for cross-modal dependencies, temporal misalignments (the fact that\nthe order in which entities appear in the story and the image sequence may not\nmatch), and human intuitions on visual grounding. An additional advantage of\nGROOViST is its modular design, where the contribution of each component can be\nassessed and interpreted individually.",
            "author": [
                "Aditya K Surikuchi",
                "Sandro Pezzelle",
                "Raquel Fern\u00e1ndez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17770v1",
                "http://arxiv.org/pdf/2310.17770v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17769v2",
            "title": "Social Contract AI: Aligning AI Assistants with Implicit Group Norms",
            "updated": "2023-12-03T17:42:33Z",
            "published": "2023-10-26T20:27:03Z",
            "summary": "We explore the idea of aligning an AI assistant by inverting a model of\nusers' (unknown) preferences from observed interactions. To validate our\nproposal, we run proof-of-concept simulations in the economic ultimatum game,\nformalizing user preferences as policies that guide the actions of simulated\nplayers. We find that the AI assistant accurately aligns its behavior to match\nstandard policies from the economic literature (e.g., selfish, altruistic).\nHowever, the assistant's learned policies lack robustness and exhibit limited\ngeneralization in an out-of-distribution setting when confronted with a\ncurrency (e.g., grams of medicine) that was not included in the assistant's\ntraining distribution. Additionally, we find that when there is inconsistency\nin the relationship between language use and an unknown policy (e.g., an\naltruistic policy combined with rude language), the assistant's learning of the\npolicy is slowed. Overall, our preliminary results suggest that developing\nsimulation frameworks in which AI assistants need to infer preferences from\ndiverse users can provide a valuable approach for studying practical alignment\nquestions.",
            "author": [
                "Jan-Philipp Fr\u00e4nken",
                "Sam Kwok",
                "Peixuan Ye",
                "Kanishk Gandhi",
                "Dilip Arumugam",
                "Jared Moore",
                "Alex Tamkin",
                "Tobias Gerstenberg",
                "Noah D. Goodman"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17769v2",
                "http://arxiv.org/pdf/2310.17769v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17766v1",
            "title": "Minibatch Markov chain Monte Carlo Algorithms for Fitting Gaussian\n  Processes",
            "updated": "2023-10-26T20:20:29Z",
            "published": "2023-10-26T20:20:29Z",
            "summary": "Gaussian processes (GPs) are a highly flexible, nonparametric statistical\nmodel that are commonly used to fit nonlinear relationships or account for\ncorrelation between observations. However, the computational load of fitting a\nGaussian process is $\\mathcal{O}(n^3)$ making them infeasible for use on large\ndatasets. To make GPs more feasible for large datasets, this research focuses\non the use of minibatching to estimate GP parameters. Specifically, we outline\nboth approximate and exact minibatch Markov chain Monte Carlo algorithms that\nsubstantially reduce the computation of fitting a GP by only considering small\nsubsets of the data at a time. We demonstrate and compare this methodology\nusing various simulations and real datasets.",
            "author": [
                "Matthew J Heaton",
                "Jacob A. Johnson"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17766v1",
                "http://arxiv.org/pdf/2310.17766v1"
            ],
            "primary_category": "stat.CO",
            "category": [
                "stat.CO",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17765v1",
            "title": "Autonomous convergence of STM control parameters using Bayesian\n  Optimization",
            "updated": "2023-10-26T20:15:48Z",
            "published": "2023-10-26T20:15:48Z",
            "summary": "Scanning Tunneling microscopy (STM) is a widely used tool for atomic imaging\nof novel materials and its surface energetics. However, the optimization of the\nimaging conditions is a tedious process due to the extremely sensitive\ntip-surface interaction, and thus limits the throughput efficiency. Here we\ndeploy a machine learning (ML) based framework to achieve optimal-atomically\nresolved imaging conditions in real time. The experimental workflow leverages\nBayesian optimization (BO) method to rapidly improve the image quality, defined\nby the peak intensity in the Fourier space. The outcome of the BO prediction is\nincorporated into the microscope controls, i.e., the current setpoint and the\ntip bias, to dynamically improve the STM scan conditions. We present strategies\nto either selectively explore or exploit across the parameter space. As a\nresult, suitable policies are developed for autonomous convergence of the\ncontrol-parameters. The ML-based framework serves as a general workflow\nmethodology across a wide range of materials.",
            "author": [
                "Ganesh Narasimha",
                "Saban Hus",
                "Arpan Biswas",
                "Rama Vasudevan",
                "Maxim Ziatdinov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17765v1",
                "http://arxiv.org/pdf/2310.17765v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph",
                "cond-mat.mtrl-sci",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18374v1",
            "title": "Forum on immune digital twins: a meeting report",
            "updated": "2023-10-26T20:14:35Z",
            "published": "2023-10-26T20:14:35Z",
            "summary": "Medical digital twins are computational models of human biology relevant to a\ngiven medical condition, which can be tailored to an individual patient,\nthereby predicting the course of disease and individualized treatments, an\nimportant goal of personalized medicine. The immune system, which has a central\nrole in many diseases, is highly heterogeneous between individuals, and thus\nposes a major challenge for this technology. If medical digital twins are to\nfaithfully capture the characteristics of a patient's immune system, we need to\nanswer many questions, such as: What do we need to know about the immune system\nto build mathematical models that reflect features of an individual? What data\ndo we need to collect across the different scales of immune system action? What\nare the right modeling paradigms to properly capture immune system complexity?\nIn February 2023, an international group of experts convened in Lake Nona, FL\nfor two days to discuss these and other questions related to digital twins of\nthe immune system. The group consisted of clinicians, immunologists,\nbiologists, and mathematical modelers, representative of the interdisciplinary\nnature of medical digital twin development. A video recording of the entire\nevent is available. This paper presents a synopsis of the discussions, brief\ndescriptions of ongoing digital twin projects at different stages of progress.\nIt also proposes a 5-year action plan for further developing this technology.\nThe main recommendations are to identify and pursue a small number of promising\nuse cases, to develop stimulation-specific assays of immune function in a\nclinical setting, and to develop a database of existing computational immune\nmodels, as well as advanced modeling technology and infrastructure.",
            "author": [
                "Reinhard Laubenbacher",
                "Fred Adler",
                "Gary An",
                "Filippo Castiglione",
                "Stephen Eubank",
                "Luis L. Fonseca",
                "James Glazier",
                "Tomas Helikar",
                "Marti Jett-Tilton",
                "Denise Kirschner",
                "Paul Macklin",
                "Borna Mehrad",
                "Beth Moore",
                "Virginia Pasour",
                "Ilya Shmulevich",
                "Amber Smith",
                "Isabel Voigt",
                "Thomas E. Yankeelov",
                "Tjalf Ziemssen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18374v1",
                "http://arxiv.org/pdf/2310.18374v1"
            ],
            "primary_category": "q-bio.OT",
            "category": [
                "q-bio.OT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17760v1",
            "title": "Novel Models for Multiple Dependent Heteroskedastic Time Series",
            "updated": "2023-10-26T20:07:25Z",
            "published": "2023-10-26T20:07:25Z",
            "summary": "Functional magnetic resonance imaging or functional MRI (fMRI) is a very\npopular tool used for differing brain regions by measuring brain activity. It\nis affected by physiological noise, such as head and brain movement in the\nscanner from breathing, heart beats, or the subject fidgeting. The purpose of\nthis paper is to propose a novel approach to handling fMRI data for infants\nwith high volatility caused by sudden head movements. Another purpose is to\nevaluate the volatility modelling performance of multiple dependent fMRI time\nseries data. The models examined in this paper are AR and GARCH and the\nmodelling performance is evaluated by several statistical performance measures.\nThe conclusions of this paper are that multiple dependent fMRI series data can\nbe fitted with AR + GARCH model if the multiple fMRI data have many sudden head\nmovements. The GARCH model can capture the shared volatility clustering caused\nby head movements across brain regions. However, the multiple fMRI data without\nmany head movements have fitted AR + GARCH model with different performance.\nThe conclusions are supported by statistical tests and measures. This paper\nhighlights the difference between the proposed approach from traditional\napproaches when estimating model parameters and modelling conditional variances\non multiple dependent time series. In the future, the proposed approach can be\napplied to other research fields, such as financial economics, and signal\nprocessing. Code is available at \\url{https://github.com/13204942/STAT40710}.",
            "author": [
                "Fangyijie Wang",
                "Michael Salter-Townshend"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17760v1",
                "http://arxiv.org/pdf/2310.17760v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17759v1",
            "title": "Optimal Guarantees for Algorithmic Reproducibility and Gradient\n  Complexity in Convex Optimization",
            "updated": "2023-10-26T19:56:52Z",
            "published": "2023-10-26T19:56:52Z",
            "summary": "Algorithmic reproducibility measures the deviation in outputs of machine\nlearning algorithms upon minor changes in the training process. Previous work\nsuggests that first-order methods would need to trade-off convergence rate\n(gradient complexity) for better reproducibility. In this work, we challenge\nthis perception and demonstrate that both optimal reproducibility and\nnear-optimal convergence guarantees can be achieved for smooth convex\nminimization and smooth convex-concave minimax problems under various\nerror-prone oracle settings. Particularly, given the inexact initialization\noracle, our regularization-based algorithms achieve the best of both worlds -\noptimal reproducibility and near-optimal gradient complexity - for minimization\nand minimax optimization. With the inexact gradient oracle, the near-optimal\nguarantees also hold for minimax optimization. Additionally, with the\nstochastic gradient oracle, we show that stochastic gradient descent ascent is\noptimal in terms of both reproducibility and gradient complexity. We believe\nour results contribute to an enhanced understanding of the\nreproducibility-convergence trade-off in the context of convex optimization.",
            "author": [
                "Liang Zhang",
                "Junchi Yang",
                "Amin Karbasi",
                "Niao He"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17759v1",
                "http://arxiv.org/pdf/2310.17759v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17758v2",
            "title": "Graph Neural Networks for Enhanced Decoding of Quantum LDPC Codes",
            "updated": "2023-11-06T20:44:45Z",
            "published": "2023-10-26T19:56:25Z",
            "summary": "In this work, we propose a fully differentiable iterative decoder for quantum\nlow-density parity-check (LDPC) codes. The proposed algorithm is composed of\nclassical belief propagation (BP) decoding stages and intermediate graph neural\nnetwork (GNN) layers. Both component decoders are defined over the same sparse\ndecoding graph enabling a seamless integration and scalability to large codes.\nThe core idea is to use the GNN component between consecutive BP runs, so that\nthe knowledge from the previous BP run, if stuck in a local minima caused by\ntrapping sets or short cycles in the decoding graph, can be leveraged to better\ninitialize the next BP run. By doing so, the proposed decoder can learn to\ncompensate for sub-optimal BP decoding graphs that result from the design\nconstraints of quantum LDPC codes. Since the entire decoder remains\ndifferentiable, gradient descent-based training is possible. We compare the\nerror rate performance of the proposed decoder against various post-processing\nmethods such as random perturbation, enhanced feedback, augmentation, and\nordered-statistics decoding (OSD) and show that a carefully designed training\nprocess lowers the error-floor significantly. As a result, our proposed decoder\noutperforms the former three methods using significantly fewer post-processing\nattempts. The source code of our experiments is available online.",
            "author": [
                "Anqi Gong",
                "Sebastian Cammerer",
                "Joseph M. Renes"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17758v2",
                "http://arxiv.org/pdf/2310.17758v2"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17755v1",
            "title": "Alzheimers Disease Diagnosis by Deep Learning Using MRI-Based Approaches",
            "updated": "2023-10-26T19:48:08Z",
            "published": "2023-10-26T19:48:08Z",
            "summary": "The most frequent kind of dementia of the nervous system, Alzheimer's\ndisease, weakens several brain processes (such as memory) and eventually\nresults in death. The clinical study uses magnetic resonance imaging to\ndiagnose AD. Deep learning algorithms are capable of pattern recognition and\nfeature extraction from the inputted raw data. As early diagnosis and stage\ndetection are the most crucial elements in enhancing patient care and treatment\noutcomes, deep learning algorithms for MRI images have recently allowed for\ndiagnosing a medical condition at the beginning stage and identifying\nparticular symptoms of Alzheimer's disease. As a result, we aimed to analyze\nfive specific studies focused on AD diagnosis using MRI-based deep learning\nalgorithms between 2021 and 2023 in this study. To completely illustrate the\ndifferences between these techniques and comprehend how deep learning\nalgorithms function, we attempted to explore selected approaches in depth.",
            "author": [
                "Sarasadat Foroughipoor",
                "Kimia Moradi",
                "Hamidreza Bolhasani"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17755v1",
                "http://arxiv.org/pdf/2310.17755v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17753v1",
            "title": "Bin Assignment and Decentralized Path Planning for Multi-Robot Parcel\n  Sorting",
            "updated": "2023-10-26T19:47:49Z",
            "published": "2023-10-26T19:47:49Z",
            "summary": "At modern warehouses, mobile robots transport packages and drop them into\ncollection bins/chutes based on shipping destinations grouped by, e.g., the ZIP\ncode. System throughput, measured as the number of packages sorted per unit of\ntime, determines the efficiency of the warehouse. This research develops a\nscalable, high-throughput multi-robot parcel sorting solution, decomposing the\ntask into two related processes, bin assignment and offline/online multi-robot\npath planning, and optimizing both. Bin assignment matches collection bins with\npackage types to minimize traveling costs. Subsequently, robots are assigned to\npick up and drop packages into assigned bins. Multiple highly effective bin\nassignment algorithms are proposed that can work with an arbitrary planning\nalgorithm. We propose a decentralized path planning routine using only local\ninformation to route the robots over a carefully constructed directed road\nnetwork for multi-robot path planning. Our decentralized planner, provably\nprobabilistically deadlock-free, consistently delivers near-optimal results on\npar with some top-performing centralized planners while significantly reducing\ncomputation times by orders of magnitude. Extensive simulations show that our\noverall framework delivers promising performances.",
            "author": [
                "Teng Guo",
                "Jingjin Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17753v1",
                "http://arxiv.org/pdf/2310.17753v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17752v1",
            "title": "PockEngine: Sparse and Efficient Fine-tuning in a Pocket",
            "updated": "2023-10-26T19:46:11Z",
            "published": "2023-10-26T19:46:11Z",
            "summary": "On-device learning and efficient fine-tuning enable continuous and\nprivacy-preserving customization (e.g., locally fine-tuning large language\nmodels on personalized data). However, existing training frameworks are\ndesigned for cloud servers with powerful accelerators (e.g., GPUs, TPUs) and\nlack the optimizations for learning on the edge, which faces challenges of\nresource limitations and edge hardware diversity. We introduce PockEngine: a\ntiny, sparse and efficient engine to enable fine-tuning on various edge\ndevices. PockEngine supports sparse backpropagation: it prunes the backward\ngraph and sparsely updates the model with measured memory saving and latency\nreduction while maintaining the model quality. Secondly, PockEngine is\ncompilation first: the entire training graph (including forward, backward and\noptimization steps) is derived at compile-time, which reduces the runtime\noverhead and brings opportunities for graph transformations. PockEngine also\nintegrates a rich set of training graph optimizations, thus can further\naccelerate the training cost, including operator reordering and backend\nswitching. PockEngine supports diverse applications, frontends and hardware\nbackends: it flexibly compiles and tunes models defined in\nPyTorch/TensorFlow/Jax and deploys binaries to mobile CPU/GPU/DSPs. We\nevaluated PockEngine on both vision models and large language models.\nPockEngine achieves up to 15 $\\times$ speedup over off-the-shelf TensorFlow\n(Raspberry Pi), 5.6 $\\times$ memory saving back-propagation (Jetson AGX Orin).\nRemarkably, PockEngine enables fine-tuning LLaMav2-7B on NVIDIA Jetson AGX Orin\nat 550 tokens/s, 7.9$\\times$ faster than the PyTorch.",
            "author": [
                "Ligeng Zhu",
                "Lanxiang Hu",
                "Ji Lin",
                "Wei-Chen Wang",
                "Wei-Ming Chen",
                "Chuang Gan",
                "Song Han"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3613424.3614307",
                "http://arxiv.org/abs/2310.17752v1",
                "http://arxiv.org/pdf/2310.17752v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17750v1",
            "title": "A Framework for Automated Measurement of Responsible AI Harms in\n  Generative AI Applications",
            "updated": "2023-10-26T19:45:06Z",
            "published": "2023-10-26T19:45:06Z",
            "summary": "We present a framework for the automated measurement of responsible AI (RAI)\nmetrics for large language models (LLMs) and associated products and services.\nOur framework for automatically measuring harms from LLMs builds on existing\ntechnical and sociotechnical expertise and leverages the capabilities of\nstate-of-the-art LLMs, such as GPT-4. We use this framework to run through\nseveral case studies investigating how different LLMs may violate a range of\nRAI-related principles. The framework may be employed alongside domain-specific\nsociotechnical expertise to create measurements for new harm areas in the\nfuture. By implementing this framework, we aim to enable more advanced harm\nmeasurement efforts and further the responsible use of LLMs.",
            "author": [
                "Ahmed Magooda",
                "Alec Helyar",
                "Kyle Jackson",
                "David Sullivan",
                "Chad Atalla",
                "Emily Sheng",
                "Dan Vann",
                "Richard Edgar",
                "Hamid Palangi",
                "Roman Lutz",
                "Hongliang Kong",
                "Vincent Yun",
                "Eslam Kamal",
                "Federico Zarfati",
                "Hanna Wallach",
                "Sarah Bird",
                "Mei Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17750v1",
                "http://arxiv.org/pdf/2310.17750v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17749v1",
            "title": "Salespeople vs SalesBot: Exploring the Role of Educational Value in\n  Conversational Recommender Systems",
            "updated": "2023-10-26T19:44:06Z",
            "published": "2023-10-26T19:44:06Z",
            "summary": "Making big purchases requires consumers to research or consult a salesperson\nto gain domain expertise. However, existing conversational recommender systems\n(CRS) often overlook users' lack of background knowledge, focusing solely on\ngathering preferences. In this work, we define a new problem space for\nconversational agents that aim to provide both product recommendations and\neducational value through mixed-type mixed-initiative dialog. We introduce\nSalesOps, a framework that facilitates the simulation and evaluation of such\nsystems by leveraging recent advancements in large language models (LLMs). We\nbuild SalesBot and ShopperBot, a pair of LLM-powered agents that can simulate\neither side of the framework. A comprehensive human study compares SalesBot\nagainst professional salespeople, revealing that although SalesBot approaches\nprofessional performance in terms of fluency and informativeness, it lags\nbehind in recommendation quality. We emphasize the distinct limitations both\nface in providing truthful information, highlighting the challenges of ensuring\nfaithfulness in the CRS context. We release our code and make all data\navailable.",
            "author": [
                "Lidiya Murakhovs'ka",
                "Philippe Laban",
                "Tian Xie",
                "Caiming Xiong",
                "Chien-Sheng Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17749v1",
                "http://arxiv.org/pdf/2310.17749v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17746v1",
            "title": "Memory Efficient Multithreaded Incremental Segmented Sieve Algorithm",
            "updated": "2023-10-26T19:37:46Z",
            "published": "2023-10-26T19:37:46Z",
            "summary": "Prime numbers are fundamental in number theory and play a significant role in\nvarious areas, from pure mathematics to practical applications, including\ncryptography. In this contribution, we introduce a multithreaded implementation\nof the Segmented Sieve algorithm. In our implementation, instead of handling\nlarge prime ranges in one iteration, the sieving process is broken down\nincrementally, which theoretically eliminates the challenges of working with\nlarge numbers, and can reduce memory usage, providing overall more efficient\nmulti-core utilization over extended computations.",
            "author": [
                "Evan Ning",
                "David Kaeli"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17746v1",
                "http://arxiv.org/pdf/2310.17746v1"
            ],
            "primary_category": "cs.PF",
            "category": [
                "cs.PF",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17745v1",
            "title": "Solving the two membranes problem for different operators by iterations\n  of the obstacle problem",
            "updated": "2023-10-26T19:37:33Z",
            "published": "2023-10-26T19:37:33Z",
            "summary": "In this paper we analyze iterations of the obstacle problem for two different\noperators. We solve iteratively the obstacle problem from above or below for\ntwo different differential operators with obstacles given by the previous\nfunctions in the iterative process. When we start the iterations with a super\nor a subsolution of one of the operators this procedure generates two monotone\nsequences of functions that we show that converge to a solution to the two\nmembranes problem for the two different operators. We perform our analysis in\nboth the variational and the viscosity settings.",
            "author": [
                "Irene Gonzalvez",
                "Alfredo Miranda",
                "Julio D. Rossi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17745v1",
                "http://arxiv.org/pdf/2310.17745v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17743v2",
            "title": "StyleBART: Decorate Pretrained Model with Style Adapters for\n  Unsupervised Stylistic Headline Generation",
            "updated": "2023-11-13T06:38:53Z",
            "published": "2023-10-26T19:31:22Z",
            "summary": "Stylistic headline generation is the task to generate a headline that not\nonly summarizes the content of an article, but also reflects a desired style\nthat attracts users. As style-specific article-headline pairs are scarce,\nprevious researches focus on unsupervised approaches with a standard headline\ngeneration dataset and mono-style corpora. In this work, we follow this line\nand propose StyleBART, an unsupervised approach for stylistic headline\ngeneration. Our method decorates the pretrained BART model with adapters that\nare responsible for different styles and allows the generation of headlines\nwith diverse styles by simply switching the adapters. Different from previous\nworks, StyleBART separates the task of style learning and headline generation,\nmaking it possible to freely combine the base model and the style adapters\nduring inference. We further propose an inverse paraphrasing task to enhance\nthe style adapters. Extensive automatic and human evaluations show that\nStyleBART achieves new state-of-the-art performance in the unsupervised\nstylistic headline generation task, producing high-quality headlines with the\ndesired style.",
            "author": [
                "Hanqing Wang",
                "Yajing Luo",
                "Boya Xiong",
                "Guanhua Chen",
                "Yun Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17743v2",
                "http://arxiv.org/pdf/2310.17743v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17742v1",
            "title": "BERT-PIN: A BERT-based Framework for Recovering Missing Data Segments in\n  Time-series Load Profiles",
            "updated": "2023-10-26T19:30:31Z",
            "published": "2023-10-26T19:30:31Z",
            "summary": "Inspired by the success of the Transformer model in natural language\nprocessing and computer vision, this paper introduces BERT-PIN, a Bidirectional\nEncoder Representations from Transformers (BERT) powered Profile Inpainting\nNetwork. BERT-PIN recovers multiple missing data segments (MDSs) using load and\ntemperature time-series profiles as inputs. To adopt a standard Transformer\nmodel structure for profile inpainting, we segment the load and temperature\nprofiles into line segments, treating each segment as a word and the entire\nprofile as a sentence. We incorporate a top candidates selection process in\nBERT-PIN, enabling it to produce a sequence of probability distributions, based\non which users can generate multiple plausible imputed data sets, each\nreflecting different confidence levels. We develop and evaluate BERT-PIN using\nreal-world dataset for two applications: multiple MDSs recovery and demand\nresponse baseline estimation. Simulation results show that BERT-PIN outperforms\nthe existing methods in accuracy while is capable of restoring multiple MDSs\nwithin a longer window. BERT-PIN, served as a pre-trained model, can be\nfine-tuned for conducting many downstream tasks, such as classification and\nsuper resolution.",
            "author": [
                "Yi Hu",
                "Kai Ye",
                "Hyeonjin Kim",
                "Ning Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17742v1",
                "http://arxiv.org/pdf/2310.17742v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17739v1",
            "title": "Deep Quantum Circuit Simulations of Low-Energy Nuclear States",
            "updated": "2023-10-26T19:10:58Z",
            "published": "2023-10-26T19:10:58Z",
            "summary": "Numerical simulation is an important method for verifying the quantum\ncircuits used to simulate low-energy nuclear states. However, real-world\napplications of quantum computing for nuclear theory often generate deep\nquantum circuits that place demanding memory and processing requirements on\nconventional simulation methods. Here, we present advances in high-performance\nnumerical simulations of deep quantum circuits to efficiently verify the\naccuracy of low-energy nuclear physics applications. Our approach employs\nseveral novel methods for accelerating the numerical simulation including 1-\nand 2-qubit gate fusion techniques as well as management of simulated\nmid-circuit measurements to verify state preparation circuits. We test these\nmethods across a variety of high-performance computing systems and our results\nshow that circuits up to 21 qubits and more than 115,000,000 gates can be\nefficiently simulated.",
            "author": [
                "Ang Li",
                "Alessandro Baroni",
                "Ionel Stetcu",
                "Travis S. Humble"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17739v1",
                "http://arxiv.org/pdf/2310.17739v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17737v1",
            "title": "ArchBERT: Bi-Modal Understanding of Neural Architectures and Natural\n  Languages",
            "updated": "2023-10-26T18:58:52Z",
            "published": "2023-10-26T18:58:52Z",
            "summary": "Building multi-modal language models has been a trend in the recent years,\nwhere additional modalities such as image, video, speech, etc. are jointly\nlearned along with natural languages (i.e., textual information). Despite the\nsuccess of these multi-modal language models with different modalities, there\nis no existing solution for neural network architectures and natural languages.\nProviding neural architectural information as a new modality allows us to\nprovide fast architecture-2-text and text-2-architecture retrieval/generation\nservices on the cloud with a single inference. Such solution is valuable in\nterms of helping beginner and intermediate ML users to come up with better\nneural architectures or AutoML approaches with a simple text query. In this\npaper, we propose ArchBERT, a bi-modal model for joint learning and\nunderstanding of neural architectures and natural languages, which opens up new\navenues for research in this area. We also introduce a pre-training strategy\nnamed Masked Architecture Modeling (MAM) for a more generalized joint learning.\nMoreover, we introduce and publicly release two new bi-modal datasets for\ntraining and validating our methods. The ArchBERT's performance is verified\nthrough a set of numerical experiments on different downstream tasks such as\narchitecture-oriented reasoning, question answering, and captioning\n(summarization). Datasets, codes, and demos are available supplementary\nmaterials.",
            "author": [
                "Mohammad Akbari",
                "Saeed Ranjbar Alvar",
                "Behnam Kamranian",
                "Amin Banitalebi-Dehkordi",
                "Yong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17737v1",
                "http://arxiv.org/pdf/2310.17737v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17734v1",
            "title": "Investigating Multilingual Coreference Resolution by Universal\n  Annotations",
            "updated": "2023-10-26T18:50:04Z",
            "published": "2023-10-26T18:50:04Z",
            "summary": "Multilingual coreference resolution (MCR) has been a long-standing and\nchallenging task. With the newly proposed multilingual coreference dataset,\nCorefUD (Nedoluzhko et al., 2022), we conduct an investigation into the task by\nusing its harmonized universal morphosyntactic and coreference annotations.\nFirst, we study coreference by examining the ground truth data at different\nlinguistic levels, namely mention, entity and document levels, and across\ndifferent genres, to gain insights into the characteristics of coreference\nacross multiple languages. Second, we perform an error analysis of the most\nchallenging cases that the SotA system fails to resolve in the CRAC 2022 shared\ntask using the universal annotations. Last, based on this analysis, we extract\nfeatures from universal morphosyntactic annotations and integrate these\nfeatures into a baseline system to assess their potential benefits for the MCR\ntask. Our results show that our best configuration of features improves the\nbaseline by 0.9% F1 score.",
            "author": [
                "Haixia Chai",
                "Michael Strube"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17734v1",
                "http://arxiv.org/pdf/2310.17734v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17732v1",
            "title": "GNN-GMVO: Graph Neural Networks for Optimizing Gross Merchandise Value\n  in Similar Item Recommendation",
            "updated": "2023-10-26T18:43:16Z",
            "published": "2023-10-26T18:43:16Z",
            "summary": "Similar item recommendation is a critical task in the e-Commerce industry,\nwhich helps customers explore similar and relevant alternatives based on their\ninterested products. Despite the traditional machine learning models, Graph\nNeural Networks (GNNs), by design, can understand complex relations like\nsimilarity between products. However, in contrast to their wide usage in\nretrieval tasks and their focus on optimizing the relevance, the current GNN\narchitectures are not tailored toward maximizing revenue-related objectives\nsuch as Gross Merchandise Value (GMV), which is one of the major business\nmetrics for e-Commerce companies. In addition, defining accurate edge relations\nin GNNs is non-trivial in large-scale e-Commerce systems, due to the\nheterogeneity nature of the item-item relationships. This work aims to address\nthese issues by designing a new GNN architecture called GNN-GMVO (Graph Neural\nNetwork - Gross Merchandise Value Optimizer). This model directly optimizes GMV\nwhile considering the complex relations between items. In addition, we propose\na customized edge construction method to tailor the model toward similar item\nrecommendation task and alleviate the noisy and complex item-item relations. In\nour comprehensive experiments on three real-world datasets, we show higher\nprediction performance and expected GMV for top ranked items recommended by our\nmodel when compared with selected state-of-the-art benchmark models.",
            "author": [
                "Ramin Giahi",
                "Reza Yousefi Maragheh",
                "Nima Farrokhsiar",
                "Jianpeng Xu",
                "Jason Cho",
                "Evren Korpeoglu",
                "Sushant Kumar",
                "Kannan Achan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17732v1",
                "http://arxiv.org/pdf/2310.17732v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17723v1",
            "title": "ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training\n  Quantization Framework for W8A8 Transformers",
            "updated": "2023-10-26T18:34:41Z",
            "published": "2023-10-26T18:34:41Z",
            "summary": "Quantization techniques are pivotal in reducing the memory and computational\ndemands of deep neural network inference. Existing solutions, such as\nZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook\ncrucial memory-bounded operators and the complexities of per-token\nquantization. Addressing these gaps, we present a novel, fully\nhardware-enhanced robust optimized post-training W8A8 quantization framework,\nZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and\ncompute-intensive operators, aiming for optimal hardware performance.\nAdditionally, it offers flexibility by allowing specific INT8 modules to switch\nto FP16/BF16 mode, enhancing accuracy.",
            "author": [
                "Zhewei Yao",
                "Reza Yazdani Aminabadi",
                "Stephen Youn",
                "Xiaoxia Wu",
                "Elton Zheng",
                "Yuxiong He"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17723v1",
                "http://arxiv.org/pdf/2310.17723v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17722v1",
            "title": "Large Language Models as Generalizable Policies for Embodied Tasks",
            "updated": "2023-10-26T18:32:05Z",
            "published": "2023-10-26T18:32:05Z",
            "summary": "We show that large language models (LLMs) can be adapted to be generalizable\npolicies for embodied visual tasks. Our approach, called Large LAnguage model\nReinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take\nas input text instructions and visual egocentric observations and output\nactions directly in the environment. Using reinforcement learning, we train\nLLaRP to see and act solely through environmental interactions. We show that\nLLaRP is robust to complex paraphrasings of task instructions and can\ngeneralize to new tasks that require novel optimal behavior. In particular, on\n1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other\ncommon learned baselines or zero-shot applications of LLMs. Finally, to aid the\ncommunity in studying language conditioned, massively multi-task, embodied AI\nproblems we release a novel benchmark, Language Rearrangement, consisting of\n150,000 training and 1,000 testing tasks for language-conditioned\nrearrangement. Video examples of LLaRP in unseen Language Rearrangement\ninstructions are at https://llm-rl.github.io.",
            "author": [
                "Andrew Szot",
                "Max Schwarzer",
                "Harsh Agrawal",
                "Bogdan Mazoure",
                "Walter Talbott",
                "Katherine Metcalf",
                "Natalie Mackraz",
                "Devon Hjelm",
                "Alexander Toshev"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17722v1",
                "http://arxiv.org/pdf/2310.17722v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17721v1",
            "title": "From Transcripts to Insights: Uncovering Corporate Risks Using\n  Generative AI",
            "updated": "2023-10-26T18:30:37Z",
            "published": "2023-10-26T18:30:37Z",
            "summary": "We explore the value of generative AI tools, such as ChatGPT, in helping\ninvestors uncover dimensions of corporate risk. We develop and validate\nfirm-level measures of risk exposure to political, climate, and AI-related\nrisks. Using the GPT 3.5 model to generate risk summaries and assessments from\nthe context provided by earnings call transcripts, we show that GPT-based\nmeasures possess significant information content and outperform the existing\nrisk measures in predicting (abnormal) firm-level volatility and firms' choices\nsuch as investment and innovation. Importantly, information in risk assessments\ndominates that in risk summaries, establishing the value of general AI\nknowledge. We also find that generative AI is effective at detecting emerging\nrisks, such as AI risk, which has soared in recent quarters. Our measures\nperform well both within and outside the GPT's training window and are priced\nin equity markets. Taken together, an AI-based approach to risk measurement\nprovides useful insights to users of corporate disclosures at a low cost.",
            "author": [
                "Alex Kim",
                "Maximilian Muhn",
                "Valeri Nikolaev"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17721v1",
                "http://arxiv.org/pdf/2310.17721v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "cs.AI",
                "cs.CL",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17720v1",
            "title": "Advancing Brain Tumor Detection: A Thorough Investigation of CNNs,\n  Clustering, and SoftMax Classification in the Analysis of MRI Images",
            "updated": "2023-10-26T18:27:20Z",
            "published": "2023-10-26T18:27:20Z",
            "summary": "Brain tumors pose a significant global health challenge due to their high\nprevalence and mortality rates across all age groups. Detecting brain tumors at\nan early stage is crucial for effective treatment and patient outcomes. This\nstudy presents a comprehensive investigation into the use of Convolutional\nNeural Networks (CNNs) for brain tumor detection using Magnetic Resonance\nImaging (MRI) images. The dataset, consisting of MRI scans from both healthy\nindividuals and patients with brain tumors, was processed and fed into the CNN\narchitecture. The SoftMax Fully Connected layer was employed to classify the\nimages, achieving an accuracy of 98%. To evaluate the CNN's performance, two\nother classifiers, Radial Basis Function (RBF) and Decision Tree (DT), were\nutilized, yielding accuracy rates of 98.24% and 95.64%, respectively. The study\nalso introduced a clustering method for feature extraction, improving CNN's\naccuracy. Sensitivity, Specificity, and Precision were employed alongside\naccuracy to comprehensively evaluate the network's performance. Notably, the\nSoftMax classifier demonstrated the highest accuracy among the categorizers,\nachieving 99.52% accuracy on test data. The presented research contributes to\nthe growing field of deep learning in medical image analysis. The combination\nof CNNs and MRI data offers a promising tool for accurately detecting brain\ntumors, with potential implications for early diagnosis and improved patient\ncare.",
            "author": [
                "Jonayet Miah",
                "Duc M Cao",
                "Md Abu Sayed3",
                "Md Siam Taluckder",
                "Md Sabbirul Haque",
                "Fuad Mahmud"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17720v1",
                "http://arxiv.org/pdf/2310.17720v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17716v1",
            "title": "Unifying (Quantum) Statistical and Parametrized (Quantum) Algorithms",
            "updated": "2023-10-26T18:23:21Z",
            "published": "2023-10-26T18:23:21Z",
            "summary": "Kearns' statistical query (SQ) oracle (STOC'93) lends a unifying perspective\nfor most classical machine learning algorithms. This ceases to be true in\nquantum learning, where many settings do not admit, neither an SQ analog nor a\nquantum statistical query (QSQ) analog. In this work, we take inspiration from\nKearns' SQ oracle and Valiant's weak evaluation oracle (TOCT'14) and establish\na unified perspective bridging the statistical and parametrized learning\nparadigms in a novel way. We explore the problem of learning from an evaluation\noracle, which provides an estimate of function values, and introduce an\nextensive yet intuitive framework that yields unconditional lower bounds for\nlearning from evaluation queries and characterizes the query complexity for\nlearning linear function classes. The framework is directly applicable to the\nQSQ setting and virtually all algorithms based on loss function optimization.\n  Our first application is to extend prior results on the learnability of\noutput distributions of quantum circuits and Clifford unitaries from the SQ to\nthe (multi-copy) QSQ setting, implying exponential separations between learning\nstabilizer states from (multi-copy) QSQs versus from quantum samples. Our\nsecond application is to analyze some popular quantum machine learning (QML)\nsettings. We gain an intuitive picture of the hardness of many QML tasks which\ngoes beyond existing methods such as barren plateaus and the statistical\ndimension, and contains crucial setting-dependent implications. Our framework\nnot only unifies the perspective of cost concentration with that of the\nstatistical dimension in a unified language but exposes their connectedness and\nsimilarity.",
            "author": [
                "Alexander Nietner"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17716v1",
                "http://arxiv.org/pdf/2310.17716v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17715v1",
            "title": "Outlier Dimensions Encode Task-Specific Knowledge",
            "updated": "2023-10-26T18:22:13Z",
            "published": "2023-10-26T18:22:13Z",
            "summary": "Representations from large language models (LLMs) are known to be dominated\nby a small subset of dimensions with exceedingly high variance. Previous works\nhave argued that although ablating these outlier dimensions in LLM\nrepresentations hurts downstream performance, outlier dimensions are\ndetrimental to the representational quality of embeddings. In this study, we\ninvestigate how fine-tuning impacts outlier dimensions and show that 1) outlier\ndimensions that occur in pre-training persist in fine-tuned models and 2) a\nsingle outlier dimension can complete downstream tasks with a minimal error\nrate. Our results suggest that outlier dimensions can encode crucial\ntask-specific knowledge and that the value of a representation in a single\noutlier dimension drives downstream model decisions.",
            "author": [
                "William Rudman",
                "Catherine Chen",
                "Carsten Eickhoff"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17715v1",
                "http://arxiv.org/pdf/2310.17715v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17714v1",
            "title": "Nearest Neighbor Search over Vectorized Lexico-Syntactic Patterns for\n  Relation Extraction from Financial Documents",
            "updated": "2023-10-26T18:19:56Z",
            "published": "2023-10-26T18:19:56Z",
            "summary": "Relation extraction (RE) has achieved remarkable progress with the help of\npre-trained language models. However, existing RE models are usually incapable\nof handling two situations: implicit expressions and long-tail relation\nclasses, caused by language complexity and data sparsity. Further, these\napproaches and models are largely inaccessible to users who don't have direct\naccess to large language models (LLMs) and/or infrastructure for supervised\ntraining or fine-tuning. Rule-based systems also struggle with implicit\nexpressions. Apart from this, Real world financial documents such as various\n10-X reports (including 10-K, 10-Q, etc.) of publicly traded companies pose\nanother challenge to rule-based systems in terms of longer and complex\nsentences. In this paper, we introduce a simple approach that consults training\nrelations at test time through a nearest-neighbor search over dense vectors of\nlexico-syntactic patterns and provides a simple yet effective means to tackle\nthe above issues. We evaluate our approach on REFinD and show that our method\nachieves state-of-the-art performance. We further show that it can provide a\ngood start for human in the loop setup when a small number of annotations are\navailable and it is also beneficial when domain experts can provide high\nquality patterns.",
            "author": [
                "Pawan Kumar Rajpoot",
                "Ankur Parikh"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17714v1",
                "http://arxiv.org/pdf/2310.17714v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17711v1",
            "title": "Is Explanation the Cure? Misinformation Mitigation in the Short Term and\n  Long Term",
            "updated": "2023-10-26T18:12:02Z",
            "published": "2023-10-26T18:12:02Z",
            "summary": "With advancements in natural language processing (NLP) models, automatic\nexplanation generation has been proposed to mitigate misinformation on social\nmedia platforms in addition to adding warning labels to identified fake news.\nWhile many researchers have focused on generating good explanations, how these\nexplanations can really help humans combat fake news is under-explored. In this\nstudy, we compare the effectiveness of a warning label and the state-of-the-art\ncounterfactual explanations generated by GPT-4 in debunking misinformation. In\na two-wave, online human-subject study, participants (N = 215) were randomly\nassigned to a control group in which false contents are shown without any\nintervention, a warning tag group in which the false claims were labeled, or an\nexplanation group in which the false contents were accompanied by GPT-4\ngenerated explanations. Our results show that both interventions significantly\ndecrease participants' self-reported belief in fake claims in an equivalent\nmanner for the short-term and long-term. We discuss the implications of our\nfindings and directions for future NLP-based misinformation debunking\nstrategies.",
            "author": [
                "Yi-Li Hsu",
                "Shih-Chieh Dai",
                "Aiping Xiong",
                "Lun-Wei Ku"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17711v1",
                "http://arxiv.org/pdf/2310.17711v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17710v1",
            "title": "Peccei-Quinn Inflation at the Pole and Axion Kinetic Misalignment",
            "updated": "2023-10-26T18:11:41Z",
            "published": "2023-10-26T18:11:41Z",
            "summary": "We propose a minimal extension of the Standard Model with the Peccei-Quinn\n(PQ) scalar field and explain the relic density of the QCD axion through the\nkinetic misalignment with a relatively small axion decay constant. To this\npurpose, we consider a slow-roll inflation from the radial component of the PQ\nfield with the PQ conserving potential near the pole of its kinetic term and\ninvestigate the post-inflationary dynamics of the PQ field for reheating. The\nangular mode of the PQ field, identified with the QCD axion, receives a nonzero\nvelocity during inflation due to the PQ violating potential, evolving with an\napproximately conserved Noether PQ charge. We determine the reheating\ntemperature from the perturbative decays and scattering processes of the\ninflaton and obtain dark radiation from the axions produced from the inflaton\nscattering at a testable level in the future Cosmic Microwave Background\nexperiments. We show the correlation between the reheating temperature, the\ninitial velocity of the axion and the axion decay constant, realizing the axion\nkinetic misalignment for the correct relic density.",
            "author": [
                "Hyun Min Lee",
                "Adriana G. Menkara",
                "Myeong-Jung Seong",
                "Jun-Ho Song"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17710v1",
                "http://arxiv.org/pdf/2310.17710v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17705v1",
            "title": "A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered\n  by Semantic Communication",
            "updated": "2023-10-26T18:05:22Z",
            "published": "2023-10-26T18:05:22Z",
            "summary": "Generative AI applications are recently catering to a vast user base by\ncreating diverse and high-quality AI-generated content (AIGC). With the\nproliferation of mobile devices and rapid growth of mobile traffic, providing\nubiquitous access to high-quality AIGC services via wireless communication\nnetworks is becoming the future direction for AIGC products. However, it is\nchallenging to provide optimal AIGC services in wireless networks with unstable\nchannels, limited bandwidth resources, and unevenly distributed computational\nresources. To tackle these challenges, we propose a semantic communication\n(SemCom)-empowered AIGC (SemAIGC) generation and transmission framework, where\nonly semantic information of the content rather than all the binary bits should\nbe extracted and transmitted by using SemCom. Specifically, SemAIGC integrates\ndiffusion-based models within the semantic encoder and decoder for efficient\ncontent generation and flexible adjustment of the computing workload of both\ntransmitter and receiver. Meanwhile, we devise a resource-aware workload\ntrade-off (ROOT) scheme into the SemAIGC framework to intelligently decide\ntransmitter/receiver workload, thus adjusting the utilization of computational\nresource according to service requirements. Simulations verify the superiority\nof our proposed SemAIGC framework in terms of latency and content quality\ncompared to conventional approaches.",
            "author": [
                "Runze Cheng",
                "Yao Sun",
                "Dusit Niyato",
                "Lan Zhang",
                "Lei Zhang",
                "Muhammad Ali Imran"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17705v1",
                "http://arxiv.org/pdf/2310.17705v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.AI",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17704v1",
            "title": "Effects of Finite Material Size On Axion-magnon Conversion",
            "updated": "2023-10-26T18:04:53Z",
            "published": "2023-10-26T18:04:53Z",
            "summary": "Magnetic materials are particularly favorable targets for detecting axions\ninteracting with electrons because the collective excitation of electron spins,\nthe magnon, can be excited through the axion-magnon conversion process. It is\noften assumed that only the zero-momentum uniformly precessing magnetostatic\n(Kittel) mode of the magnon is excited. This is justified if the de Broglie\nwavelength of the axion is much longer than the size of the target magnetic\nmaterial. However, if the de Broglie wavelength is shorter, finite-momentum\nmagnon modes can also be excited. We systematically analyze the target material\nsize dependence of the axion-magnon conversion rate. We discuss the importance\nof these effects in the detection of relativistic axions as well as in the\ndetection of axion dark matter of relatively heavy mass with large material\nsize.",
            "author": [
                "So Chigusa",
                "Asuka Ito",
                "Kazunori Nakayama",
                "Volodymyr Takhistov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17704v1",
                "http://arxiv.org/pdf/2310.17704v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "astro-ph.CO",
                "astro-ph.HE",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17703v2",
            "title": "The impact of responding to patient messages with large language model\n  assistance",
            "updated": "2023-11-29T19:10:58Z",
            "published": "2023-10-26T18:03:46Z",
            "summary": "Documentation burden is a major contributor to clinician burnout, which is\nrising nationally and is an urgent threat to our ability to care for patients.\nArtificial intelligence (AI) chatbots, such as ChatGPT, could reduce clinician\nburden by assisting with documentation. Although many hospitals are actively\nintegrating such systems into electronic medical record systems, AI chatbots\nutility and impact on clinical decision-making have not been studied for this\nintended use. We are the first to examine the utility of large language models\nin assisting clinicians draft responses to patient questions. In our two-stage\ncross-sectional study, 6 oncologists responded to 100 realistic synthetic\ncancer patient scenarios and portal messages developed to reflect common\nmedical situations, first manually, then with AI assistance.\n  We find AI-assisted responses were longer, less readable, but provided\nacceptable drafts without edits 58% of time. AI assistance improved efficiency\n77% of time, with low harm risk (82% safe). However, 7.7% unedited AI responses\ncould severely harm. In 31% cases, physicians thought AI drafts were\nhuman-written. AI assistance led to more patient education recommendations,\nfewer clinical actions than manual responses. Results show promise for AI to\nimprove clinician efficiency and patient care through assisting documentation,\nif used judiciously. Monitoring model outputs and human-AI interaction remains\ncrucial for safe implementation.",
            "author": [
                "Shan Chen",
                "Marco Guevara",
                "Shalini Moningi",
                "Frank Hoebers",
                "Hesham Elhalawani",
                "Benjamin H. Kann",
                "Fallon E. Chipidza",
                "Jonathan Leeman",
                "Hugo J. W. L. Aerts",
                "Timothy Miller",
                "Guergana K. Savova",
                "Raymond H. Mak",
                "Maryam Lustberg",
                "Majid Afshar",
                "Danielle S. Bitterman"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17703v2",
                "http://arxiv.org/pdf/2310.17703v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17694v1",
            "title": "Where do stars explode in the ISM? -- The distribution of dense gas\n  around massive stars and supernova remnants in M33",
            "updated": "2023-10-26T18:00:01Z",
            "published": "2023-10-26T18:00:01Z",
            "summary": "Star formation in galaxies is regulated by turbulence, outflows, gas heating\nand cloud dispersal -- processes which depend sensitively on the properties of\nthe interstellar medium (ISM) into which supernovae (SNe) explode.\nUnfortunately, direct measurements of ISM environments around SNe remain\nscarce, as SNe are rare and often distant. Here we demonstrate a new approach:\nmapping the ISM around the massive stars that are soon to explode. This\nprovides a much larger census of explosion sites than possible with only SNe,\nand allows comparison with sensitive, high-resolution maps of the atomic and\nmolecular gas from the Jansky VLA and ALMA. In the well-resolved Local Group\nspiral M33, we specifically observe the environments of red supergiants (RSGs,\nprogenitors of Type II SNe), Wolf-Rayet stars (WRs, tracing stars $>$30\nM$_{\\odot}$, and possibly future stripped-envelope SNe), and supernova remnants\n(SNRs, locations where SNe have exploded). We find that massive stars evolve\nnot only in dense, molecular-dominated gas (with younger stars in denser gas),\nbut also a substantial fraction ($\\sim$45\\% of WRs; higher for RSGs) evolve in\nlower-density, atomic-gas-dominated, inter-cloud media. We show that these\nmeasurements are consistent with expectations from different stellar-age tracer\nmaps, and can be useful for validating SN feedback models in numerical\nsimulations of galaxies. Along with the discovery of a 20-pc diameter molecular\ngas cavity around a WR, these findings re-emphasize the importance of\npre-SN/correlated-SN feedback evacuating the dense gas around massive stars\nbefore explosion, and the need for high-resolution (down to pc-scale) surveys\nof the multi-phase ISM in nearby galaxies.",
            "author": [
                "Sumit K. Sarbadhicary",
                "Jordan Wagner",
                "Eric W. Koch",
                "Ness Mayker Chen",
                "Adam K. Leroy",
                "Natalia Lah\u00e9n",
                "Erik Rosolowsky",
                "Kathryn F. Neugent",
                "Chang-Goo Kim",
                "Laura Chomiuk",
                "Julianne J. Dalcanton",
                "Laura A. Lopez",
                "Nickolas M. Pingel",
                "Remy Indebetouw",
                "Thomas G. Williams",
                "Elizabeth Tarantino",
                "Jennifer Donovan Meyer",
                "Evan D. Skillman",
                "Adam Smercina",
                "Amanda A. Kepley",
                "Eric J. Murphy",
                "Jay Strader",
                "Tony Wong",
                "Sne\u017eana Stanimirovi\u0107",
                "Vicente Villanueva",
                "Fabian Walter",
                "Juergen Ott",
                "Jeremy Darling",
                "Julia Roman-Duval",
                "Claire E. Murray"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17694v1",
                "http://arxiv.org/pdf/2310.17694v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17690v1",
            "title": "Non-contrastive sentence representations via self-supervision",
            "updated": "2023-10-26T18:00:00Z",
            "published": "2023-10-26T18:00:00Z",
            "summary": "Sample contrastive methods, typically referred to simply as contrastive are\nthe foundation of most unsupervised methods to learn text and sentence\nembeddings. On the other hand, a different class of self-supervised loss\nfunctions and methods have been considered in the computer vision community and\nreferred to as dimension contrastive. In this paper, we thoroughly compare this\nclass of methods with the standard baseline for contrastive sentence\nembeddings, SimCSE. We find that self-supervised embeddings trained using\ndimension contrastive objectives can outperform SimCSE on downstream tasks\nwithout needing auxiliary loss functions.",
            "author": [
                "Marco Farina",
                "Duccio Pappadopulo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17690v1",
                "http://arxiv.org/pdf/2310.17690v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17688v2",
            "title": "Managing AI Risks in an Era of Rapid Progress",
            "updated": "2023-11-12T14:33:20Z",
            "published": "2023-10-26T17:59:06Z",
            "summary": "In this short consensus paper, we outline risks from upcoming, advanced AI\nsystems. We examine large-scale social harms and malicious uses, as well as an\nirreversible loss of human control over autonomous AI systems. In light of\nrapid and continuing AI progress, we propose urgent priorities for AI R&D and\ngovernance.",
            "author": [
                "Yoshua Bengio",
                "Geoffrey Hinton",
                "Andrew Yao",
                "Dawn Song",
                "Pieter Abbeel",
                "Yuval Noah Harari",
                "Ya-Qin Zhang",
                "Lan Xue",
                "Shai Shalev-Shwartz",
                "Gillian Hadfield",
                "Jeff Clune",
                "Tegan Maharaj",
                "Frank Hutter",
                "At\u0131l\u0131m G\u00fcne\u015f Baydin",
                "Sheila McIlraith",
                "Qiqi Gao",
                "Ashwin Acharya",
                "David Krueger",
                "Anca Dragan",
                "Philip Torr",
                "Stuart Russell",
                "Daniel Kahneman",
                "Jan Brauner",
                "S\u00f6ren Mindermann"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17688v2",
                "http://arxiv.org/pdf/2310.17688v2"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17646v1",
            "title": "Do Graph Neural Networks Dream of Landau Damping? Insights from Kinetic\n  Simulations of a Plasma Sheet Model",
            "updated": "2023-10-26T17:58:12Z",
            "published": "2023-10-26T17:58:12Z",
            "summary": "We explore the possibility of fully replacing a plasma physics kinetic\nsimulator with a graph neural network-based simulator. We focus on this class\nof surrogate models given the similarity between their message-passing update\nmechanism and the traditional physics solver update, and the possibility of\nenforcing known physical priors into the graph construction and update. We show\nthat our model learns the kinetic plasma dynamics of the one-dimensional plasma\nmodel, a predecessor of contemporary kinetic plasma simulation codes, and\nrecovers a wide range of well-known kinetic plasma processes, including plasma\nthermalization, electrostatic fluctuations about thermal equilibrium, and the\ndrag on a fast sheet and Landau damping. We compare the performance against the\noriginal plasma model in terms of run-time, conservation laws, and temporal\nevolution of key physical quantities. The limitations of the model are\npresented and possible directions for higher-dimensional surrogate models for\nkinetic plasmas are discussed.",
            "author": [
                "Diogo D Carvalho",
                "Diogo R Ferreira",
                "Luis O Silva"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17646v1",
                "http://arxiv.org/pdf/2310.17646v1"
            ],
            "primary_category": "physics.plasm-ph",
            "category": [
                "physics.plasm-ph",
                "cs.LG",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17644v1",
            "title": "torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free\n  Deep Learning Studies: A Case Study on NLP",
            "updated": "2023-10-26T17:57:15Z",
            "published": "2023-10-26T17:57:15Z",
            "summary": "Reproducibility in scientific work has been becoming increasingly important\nin research communities such as machine learning, natural language processing,\nand computer vision communities due to the rapid development of the research\ndomains supported by recent advances in deep learning. In this work, we present\na significantly upgraded version of torchdistill, a modular-driven coding-free\ndeep learning framework significantly upgraded from the initial release, which\nsupports only image classification and object detection tasks for reproducible\nknowledge distillation experiments. To demonstrate that the upgraded framework\ncan support more tasks with third-party libraries, we reproduce the GLUE\nbenchmark results of BERT models using a script based on the upgraded\ntorchdistill, harmonizing with various Hugging Face libraries. All the 27\nfine-tuned BERT models and configurations to reproduce the results are\npublished at Hugging Face, and the model weights have already been widely used\nin research communities. We also reimplement popular small-sized models and new\nknowledge distillation methods and perform additional experiments for computer\nvision tasks.",
            "author": [
                "Yoshitomo Matsubara"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17644v1",
                "http://arxiv.org/pdf/2310.17644v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17641v2",
            "title": "Criteria for Davies Irreducibility of Markovian Quantum Dynamics",
            "updated": "2023-11-07T20:36:22Z",
            "published": "2023-10-26T17:55:46Z",
            "summary": "The dynamics of Markovian open quantum systems are described by Lindblad\nmaster equations, generating a quantum dynamical semigroup. An important\nconcept for such systems is (Davies) irreducibility, i.e., the question whether\nthere exist non-trivial invariant subspaces. Steady states of irreducible\nsystems are unique and faithful, i.e., they have full rank. In the 1970s,\nFrigerio showed that a system is irreducible if the Lindblad operators span a\nself-adjoint set with trivial commutant. We discuss a more general and powerful\nalgebraic criterion, showing that a system is irreducible if and only if the\nmultiplicative algebra generated by the Lindblad operators $L_a$ and the\noperator $iH+\\sum_a L^\\dagger_aL_a$, involving the Hamiltonian $H$, is the\nentire operator space. Examples for two-level systems, show that a change of\nHamiltonian terms as well as the addition or removal of dissipators can render\na reducible system irreducible and vice versa. Examples for many-body systems\nshow that a large class of spin chains can be rendered irreducible by\ndissipators on just one or two sites. Additionally, we discuss the decisive\ndifferences between (Davies) reducibility and Evans reducibility for quantum\nchannels and dynamical semigroups which has lead to some confusion in the\nrecent physics literature, especially, in the context of boundary-driven\nsystems. We give a criterion for quantum reducibility in terms of associated\nclassical Markov processes and, lastly, discuss the relation of the main result\nto the stabilization of pure states and argue that systems with local Lindblad\noperators cannot stabilize pure Fermi-sea states.",
            "author": [
                "Yikang Zhang",
                "Thomas Barthel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17641v2",
                "http://arxiv.org/pdf/2310.17641v2"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.stat-mech",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17639v2",
            "title": "In-Context Learning Dynamics with Random Binary Sequences",
            "updated": "2023-11-27T21:05:54Z",
            "published": "2023-10-26T17:54:52Z",
            "summary": "Large language models (LLMs) trained on huge corpora of text datasets\ndemonstrate intriguing capabilities, achieving state-of-the-art performance on\ntasks they were not explicitly trained for. The precise nature of LLM\ncapabilities is often mysterious, and different prompts can elicit different\ncapabilities through in-context learning. We propose a framework that enables\nus to analyze in-context learning dynamics to understand latent concepts\nunderlying LLMs' behavioral patterns. This provides a more nuanced\nunderstanding than success-or-failure evaluation benchmarks, but does not\nrequire observing internal activations as a mechanistic interpretation of\ncircuits would. Inspired by the cognitive science of human randomness\nperception, we use random binary sequences as context and study dynamics of\nin-context learning by manipulating properties of context data, such as\nsequence length. In the latest GPT-3.5+ models, we find emergent abilities to\ngenerate seemingly random numbers and learn basic formal languages, with\nstriking in-context learning dynamics where model outputs transition sharply\nfrom seemingly random behaviors to deterministic repetition.",
            "author": [
                "Eric J. Bigelow",
                "Ekdeep Singh Lubana",
                "Robert P. Dick",
                "Hidenori Tanaka",
                "Tomer D. Ullman"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17639v2",
                "http://arxiv.org/pdf/2310.17639v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17638v1",
            "title": "Generative Fractional Diffusion Models",
            "updated": "2023-10-26T17:53:24Z",
            "published": "2023-10-26T17:53:24Z",
            "summary": "We generalize the continuous time framework for score-based generative models\nfrom an underlying Brownian motion (BM) to an approximation of fractional\nBrownian motion (FBM). We derive a continuous reparameterization trick and the\nreverse time model by representing FBM as a stochastic integral over a family\nof Ornstein-Uhlenbeck processes to define generative fractional diffusion\nmodels (GFDM) with driving noise converging to a non-Markovian process of\ninfinite quadratic variation. The Hurst index $H\\in(0,1)$ of FBM enables\ncontrol of the roughness of the distribution transforming path. To the best of\nour knowledge, this is the first attempt to build a generative model upon a\nstochastic process with infinite quadratic variation.",
            "author": [
                "Gabriel Nobis",
                "Marco Aversa",
                "Maximilian Springenberg",
                "Michael Detzel",
                "Stefano Ermon",
                "Shinichi Nakajima",
                "Roderick Murray-Smith",
                "Sebastian Lapuschkin",
                "Christoph Knochenhauer",
                "Luis Oala",
                "Wojciech Samek"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17638v1",
                "http://arxiv.org/pdf/2310.17638v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML",
                "I.2.4; F.4.1; G.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17633v1",
            "title": "Growth of cancer stem cell driven tumors: staged invasion, linear\n  determinacy, and the tumor invasion paradox",
            "updated": "2023-10-26T17:51:09Z",
            "published": "2023-10-26T17:51:09Z",
            "summary": "We study growth of solid tumors in a partial differential equation model\nintroduced by Hillen et al for the interaction between tumor cells (TCs) and\ncancer stem cells (CSCs). We find that invasion into the cancer-free state may\nbe separated into two regimes, depending on the death rate of tumor cells. In\nthe first, staged invasion regime, invasion into the cancer-free state is lead\nby tumor cells, which are then subsequently invaded at a slower speed by cancer\nstem cells. In the second, TC extinction regime, cancer stem cells directly\ninvade the cancer-free state. Relying on recent results establishing front\nselection propagation under marginal stability assumptions, we use geometric\nsingular perturbation theory to establish existence and selection properties of\nfront solutions which describe both the primary and secondary invasion\nprocesses. With rigorous predictions for the invasion speeds, we are then able\nto heuristically predict how the total cancer mass as a function of time\ndepends on the TC death rate, finding in some situations a tumor invasion\nparadox, in which increasing the TC death rate leads to an increase in the\ntotal cancer mass. Our methods give a general approach for verifying linear\ndeterminacy of spreading speeds of invasion fronts in systems with fast-slow\nstructure.",
            "author": [
                "Montie Avery"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17633v1",
                "http://arxiv.org/pdf/2310.17633v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "math.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17632v1",
            "title": "DeepShaRM: Multi-View Shape and Reflectance Map Recovery Under Unknown\n  Lighting",
            "updated": "2023-10-26T17:50:10Z",
            "published": "2023-10-26T17:50:10Z",
            "summary": "Geometry reconstruction of textureless, non-Lambertian objects under unknown\nnatural illumination (i.e., in the wild) remains challenging as correspondences\ncannot be established and the reflectance cannot be expressed in simple\nanalytical forms. We derive a novel multi-view method, DeepShaRM, that achieves\nstate-of-the-art accuracy on this challenging task. Unlike past methods that\nformulate this as inverse-rendering, i.e., estimation of reflectance,\nillumination, and geometry from images, our key idea is to realize that\nreflectance and illumination need not be disentangled and instead estimated as\na compound reflectance map. We introduce a novel deep reflectance map\nestimation network that recovers the camera-view reflectance maps from the\nsurface normals of the current geometry estimate and the input multi-view\nimages. The network also explicitly estimates per-pixel confidence scores to\nhandle global light transport effects. A deep shape-from-shading network then\nupdates the geometry estimate expressed with a signed distance function using\nthe recovered reflectance maps. By alternating between these two, and, most\nimportant, by bypassing the ill-posed problem of reflectance and illumination\ndecomposition, the method accurately recovers object geometry in these\nchallenging settings. Extensive experiments on both synthetic and real-world\ndata clearly demonstrate its state-of-the-art accuracy.",
            "author": [
                "Kohei Yamashita",
                "Shohei Nobuhara",
                "Ko Nishino"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17632v1",
                "http://arxiv.org/pdf/2310.17632v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17631v1",
            "title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
            "updated": "2023-10-26T17:48:58Z",
            "published": "2023-10-26T17:48:58Z",
            "summary": "Evaluating Large Language Models (LLMs) in open-ended scenarios is\nchallenging because existing benchmarks and metrics can not measure them\ncomprehensively. To address this problem, we propose to fine-tune LLMs as\nscalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in\nopen-ended benchmarks. We first propose a comprehensive, large-scale,\nhigh-quality dataset containing task seeds, LLMs-generated answers, and\nGPT-4-generated judgments for fine-tuning high-performance judges, as well as a\nnew benchmark for evaluating the judges. We train JudgeLM at different scales\nfrom 7B, 13B, to 33B parameters, and conduct a systematic analysis of its\ncapabilities and behaviors. We then analyze the key biases in fine-tuning LLM\nas a judge and consider them as position bias, knowledge bias, and format bias.\nTo address these issues, JudgeLM introduces a bag of techniques including swap\naugmentation, reference support, and reference drop, which clearly enhance the\njudge's performance. JudgeLM obtains the state-of-the-art judge performance on\nboth the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM\nis efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8\nA100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an\nagreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM\nalso demonstrates extended capabilities in being judges of the single answer,\nmultimodal models, multiple answers, and multi-turn chat.",
            "author": [
                "Lianghui Zhu",
                "Xinggang Wang",
                "Xinlong Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17631v1",
                "http://arxiv.org/pdf/2310.17631v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17630v1",
            "title": "InstOptima: Evolutionary Multi-objective Instruction Optimization via\n  Large Language Model-based Instruction Operators",
            "updated": "2023-10-26T17:48:45Z",
            "published": "2023-10-26T17:48:45Z",
            "summary": "Instruction-based language modeling has received significant attention in\npretrained language models. However, the efficiency of instruction engineering\nremains low and hinders the development of instruction studies. Recent studies\nhave focused on automating instruction generation, but they primarily aim to\nimprove performance without considering other crucial objectives that impact\ninstruction quality, such as instruction length and perplexity. Therefore, we\npropose a novel approach (i.e., InstOptima) that treats instruction generation\nas an evolutionary multi-objective optimization problem. In contrast to text\nedition-based methods, our approach utilizes a large language model (LLM) to\nsimulate instruction operators, including mutation and crossover. Furthermore,\nwe introduce an objective-guided mechanism for these operators, allowing the\nLLM to comprehend the objectives and enhance the quality of the generated\ninstructions. Experimental results demonstrate improved fine-tuning performance\nand the generation of a diverse set of high-quality instructions.",
            "author": [
                "Heng Yang",
                "Ke Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17630v1",
                "http://arxiv.org/pdf/2310.17630v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17626v1",
            "title": "A Survey on Transferability of Adversarial Examples across Deep Neural\n  Networks",
            "updated": "2023-10-26T17:45:26Z",
            "published": "2023-10-26T17:45:26Z",
            "summary": "The emergence of Deep Neural Networks (DNNs) has revolutionized various\ndomains, enabling the resolution of complex tasks spanning image recognition,\nnatural language processing, and scientific problem-solving. However, this\nprogress has also exposed a concerning vulnerability: adversarial examples.\nThese crafted inputs, imperceptible to humans, can manipulate machine learning\nmodels into making erroneous predictions, raising concerns for safety-critical\napplications. An intriguing property of this phenomenon is the transferability\nof adversarial examples, where perturbations crafted for one model can deceive\nanother, often with a different architecture. This intriguing property enables\n\"black-box\" attacks, circumventing the need for detailed knowledge of the\ntarget model. This survey explores the landscape of the adversarial\ntransferability of adversarial examples. We categorize existing methodologies\nto enhance adversarial transferability and discuss the fundamental principles\nguiding each approach. While the predominant body of research primarily\nconcentrates on image classification, we also extend our discussion to\nencompass other vision tasks and beyond. Challenges and future prospects are\ndiscussed, highlighting the importance of fortifying DNNs against adversarial\nvulnerabilities in an evolving landscape.",
            "author": [
                "Jindong Gu",
                "Xiaojun Jia",
                "Pau de Jorge",
                "Wenqain Yu",
                "Xinwei Liu",
                "Avery Ma",
                "Yuan Xun",
                "Anjun Hu",
                "Ashkan Khakzar",
                "Zhijiang Li",
                "Xiaochun Cao",
                "Philip Torr"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17626v1",
                "http://arxiv.org/pdf/2310.17626v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17625v1",
            "title": "The metallicity dependence and evolutionary times of merging binary\n  black holes: Combined constraints from individual gravitational-wave\n  detections and the stochastic background",
            "updated": "2023-10-26T17:44:11Z",
            "published": "2023-10-26T17:44:11Z",
            "summary": "The advent of gravitational-wave astronomy is now allowing for the study of\ncompact binary merger demographics throughout the Universe. This information\ncan be leveraged as tools for understanding massive stars, their environments,\nand their evolution. One active question is the nature of compact binary\nformation: the environmental and chemical conditions required for black hole\nbirth and the time delays experienced by binaries before they merge.\nGravitational-wave events detected today, however, primarily occur at low or\nmoderate redshifts due to current interferometer sensitivity, therefore\nlimiting our ability to probe the high redshift behavior of these quantities.\nIn this work, we circumvent this limitation by using an additional source of\ninformation: observational limits on the gravitational-wave background from\nunresolved binaries in the distant Universe. Using current gravitational-wave\ndata from the first three observing runs of LIGO-Virgo-KAGRA, we combine\ncatalogs of directly detected binaries and limits on the stochastic background\nto constrain the time-delay distribution and metallicity dependence of binary\nblack hole evolution. Looking to the future, we also explore how these\nconstraints will be improved at the Advanced LIGO A+ sensitivity. We conclude\nthat, although binary black hole formation cannot be strongly constrained with\ntoday's data, the future detection (or a non-detection) of the\ngravitational-wave background with Advanced LIGO A+ will carry strong\nimplications for the evolution of binary black holes.",
            "author": [
                "Kevin Turbang",
                "Max Lalleman",
                "Thomas A. Callister",
                "Nick van Remortel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17625v1",
                "http://arxiv.org/pdf/2310.17625v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17623v2",
            "title": "Proving Test Set Contamination in Black Box Language Models",
            "updated": "2023-11-24T01:45:16Z",
            "published": "2023-10-26T17:43:13Z",
            "summary": "Large language models are trained on vast amounts of internet data, prompting\nconcerns and speculation that they have memorized public benchmarks. Going from\nspeculation to proof of contamination is challenging, as the pretraining data\nused by proprietary models are often not publicly accessible. We show that it\nis possible to provide provable guarantees of test set contamination in\nlanguage models without access to pretraining data or model weights. Our\napproach leverages the fact that when there is no data contamination, all\norderings of an exchangeable benchmark should be equally likely. In contrast,\nthe tendency for language models to memorize example order means that a\ncontaminated language model will find certain canonical orderings to be much\nmore likely than others. Our test flags potential contamination whenever the\nlikelihood of a canonically ordered benchmark dataset is significantly higher\nthan the likelihood after shuffling the examples. We demonstrate that our\nprocedure is sensitive enough to reliably prove test set contamination in\nchallenging situations, including models as small as 1.4 billion parameters, on\nsmall test sets of only 1000 examples, and datasets that appear only a few\ntimes in the pretraining corpus. Using our test, we audit five popular publicly\naccessible language models for test set contamination and find little evidence\nfor pervasive contamination.",
            "author": [
                "Yonatan Oren",
                "Nicole Meister",
                "Niladri Chatterji",
                "Faisal Ladhak",
                "Tatsunori B. Hashimoto"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17623v2",
                "http://arxiv.org/pdf/2310.17623v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17621v1",
            "title": "A spectral element solution of the Poisson equation with shifted\n  boundary polynomial corrections: influence of the surrogate to true boundary\n  mapping and an asymptotically preserving Robin formulation",
            "updated": "2023-10-26T17:40:59Z",
            "published": "2023-10-26T17:40:59Z",
            "summary": "We present a new high-order accurate spectral element solution to the\ntwo-dimensional scalar Poisson equation subject to a general Robin boundary\ncondition. The solution is based on a simplified version of the shifted\nboundary method employing a continuous arbitrary order $hp$-Galerkin spectral\nelement method as the numerical discretization procedure. The simplification\nrelies on a polynomial correction to avoid explicitly evaluating high-order\npartial derivatives from the Taylor series expansion, which traditionally have\nbeen used within the shifted boundary method. In this setting, we apply an\nextrapolation and novel interpolation approach to project the basis functions\nfrom the true domain onto the approximate surrogate domain. The resulting\nsolution provides a method that naturally incorporates curved geometrical\nfeatures of the domain, overcomes complex and cumbersome mesh generation, and\navoids problems with small-cut-cells. Dirichlet, Neumann, and general Robin\nboundary conditions are enforced weakly through: i) a generalized Nitsche's\nmethod and ii) a generalized Aubin's method. For this, a consistent asymptotic\npreserving formulation of the embedded Robin formulations is presented.\n  We present several numerical experiments and analysis of the algorithmic\nproperties of the different weak formulations. With this, we include\nconvergence studies under polynomial, $p$, increase of the basis functions,\nmesh, $h$, refinement, and matrix conditioning to highlight the spectral and\nalgebraic convergence features, respectively. This is done to assess the\ninfluence of errors across variational formulations, polynomial order, mesh\nsize, and mappings between the true and surrogate boundaries.",
            "author": [
                "Jens Visbech",
                "Allan Peter Engsig-Karup",
                "Mario Ricchiuto"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17621v1",
                "http://arxiv.org/pdf/2310.17621v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17618v1",
            "title": "Applications of emulation and Bayesian methods in heavy-ion physics",
            "updated": "2023-10-26T17:39:15Z",
            "published": "2023-10-26T17:39:15Z",
            "summary": "Heavy-ion collisions provide a window into the properties of many-body\nsystems of deconfined quarks and gluons. Understanding the collective\nproperties of quarks and gluons is possible by comparing models of heavy-ion\ncollisions to measurements of the distribution of particles produced at the end\nof the collisions. These model-to-data comparisons are extremely challenging,\nhowever, because of the complexity of the models, the large amount of\nexperimental data, and their uncertainties. Bayesian inference provides a\nrigorous statistical framework to constrain the properties of nuclear matter by\nsystematically comparing models and measurements.\n  This review covers model emulation and Bayesian methods as applied to\nmodel-to-data comparisons in heavy-ion collisions. Replacing the model outputs\n(observables) with Gaussian process emulators is key to the Bayesian approach\ncurrently used in the field, and both current uses of emulators and related\nrecent developments are reviewed. The general principles of Bayesian inference\nare then discussed along with other Bayesian methods, followed by a systematic\ncomparison of seven recent Bayesian analyses that studied quark-gluon plasma\nproperties, such as the shear and bulk viscosities. The latter comparison is\nused to illustrate sources of differences in analyses, and what it can teach us\nfor future studies.",
            "author": [
                "Jean-Fran\u00e7ois Paquet"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17618v1",
                "http://arxiv.org/pdf/2310.17618v1"
            ],
            "primary_category": "nucl-th",
            "category": [
                "nucl-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17617v2",
            "title": "Experimental prospects for indirect BSM searches in\n  $e^{-}e^{+}\\rightarrow q\\bar{q}$ ($q=c,b$) processes at Higgs Factories",
            "updated": "2023-10-27T16:23:52Z",
            "published": "2023-10-26T17:38:54Z",
            "summary": "This contribution explores the ability to probe BSM physics by using the\nexperimental prospects for measuring the forward-backward asymmetry ($A_{FB}$)\nin $e^{+}e^{-}\\rightarrow b\\bar{b}$ and $e^{+}e^{-}\\rightarrow c\\bar{c}$\nprocesses at the baseline energy points of ILC: 250 and 500 GeV. The studies\nare based on the full simulation samples and reconstruction chains from the ILD\nconcept group. The BSM models studied are two different types of gauge-Higgs\nunification (GHU) models that predict BSM Z$^\\prime$ resonances at the TeV\nscale.",
            "author": [
                "J. P. Marquez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17617v2",
                "http://arxiv.org/pdf/2310.17617v2"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17616v1",
            "title": "Verifying Programs with Logic and Extended Proof Rules: Deep Embedding\n  v.s. Shallow Embedding",
            "updated": "2023-10-26T17:37:26Z",
            "published": "2023-10-26T17:37:26Z",
            "summary": "Many foundational program verification tools have been developed to build\nmachine-checked program correctness proofs, a majority of which are based on\nHoare logic. Their program logics, their assertion languages, and their\nunderlying programming languages can be formalized by either a shallow\nembedding or a deep embedding. Tools like Iris and early versions of Verified\nSoftware Toolchain (VST) choose different shallow embeddings to formalize their\nprogram logics. But the pros and cons of these different embeddings were not\nyet well studied. Therefore, we want to study the impact of the program logic's\nembedding on logic's proof rules in this paper. This paper considers a set of\nuseful extended proof rules, and four different logic embeddings: one deep\nembedding and three common shallow embeddings. We prove the validity of these\nextended rules under these embeddings and discuss their main challenges.\nFurthermore, we propose a method to lift existing shallowly embedded logics to\ndeeply embedded ones to greatly simplify proofs of extended rules in specific\nproof systems. We evaluate our results on two existing verification tools. We\nlift the originally shallowly embedded VST to our deeply embedded VST to\nsupport extended rules, and we implement Iris-CF and deeply embedded Iris-Imp\nbased on the Iris framework to evaluate our theory in real verification\nprojects.",
            "author": [
                "Zhongye Wang",
                "Qinxiang Cao",
                "Yichen Tao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17616v1",
                "http://arxiv.org/pdf/2310.17616v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17611v1",
            "title": "Uncovering Meanings of Embeddings via Partial Orthogonality",
            "updated": "2023-10-26T17:34:32Z",
            "published": "2023-10-26T17:34:32Z",
            "summary": "Machine learning tools often rely on embedding text as vectors of real\nnumbers. In this paper, we study how the semantic structure of language is\nencoded in the algebraic structure of such embeddings. Specifically, we look at\na notion of ``semantic independence'' capturing the idea that, e.g.,\n``eggplant'' and ``tomato'' are independent given ``vegetable''. Although such\nexamples are intuitive, it is difficult to formalize such a notion of semantic\nindependence. The key observation here is that any sensible formalization\nshould obey a set of so-called independence axioms, and thus any algebraic\nencoding of this structure should also obey these axioms. This leads us\nnaturally to use partial orthogonality as the relevant algebraic structure. We\ndevelop theory and methods that allow us to demonstrate that partial\northogonality does indeed capture semantic independence. Complementary to this,\nwe also introduce the concept of independence preserving embeddings where\nembeddings preserve the conditional independence structures of a distribution,\nand we prove the existence of such embeddings and approximations to them.",
            "author": [
                "Yibo Jiang",
                "Bryon Aragam",
                "Victor Veitch"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17611v1",
                "http://arxiv.org/pdf/2310.17611v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17609v1",
            "title": "LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset",
            "updated": "2023-10-26T17:32:55Z",
            "published": "2023-10-26T17:32:55Z",
            "summary": "As an important component of intelligent legal systems, legal case retrieval\nplays a critical role in ensuring judicial justice and fairness. However, the\ndevelopment of legal case retrieval technologies in the Chinese legal system is\nrestricted by three problems in existing datasets: limited data size, narrow\ndefinitions of legal relevance, and naive candidate pooling strategies used in\ndata sampling. To alleviate these issues, we introduce LeCaRDv2, a large-scale\nLegal Case Retrieval Dataset (version 2). It consists of 800 queries and 55,192\ncandidates extracted from 4.3 million criminal case documents. To the best of\nour knowledge, LeCaRDv2 is one of the largest Chinese legal case retrieval\ndatasets, providing extensive coverage of criminal charges. Additionally, we\nenrich the existing relevance criteria by considering three key aspects:\ncharacterization, penalty, procedure. This comprehensive criteria enriches the\ndataset and may provides a more holistic perspective. Furthermore, we propose a\ntwo-level candidate set pooling strategy that effectively identify potential\ncandidates for each query case. It's important to note that all cases in the\ndataset have been annotated by multiple legal experts specializing in criminal\nlaw. Their expertise ensures the accuracy and reliability of the annotations.\nWe evaluate several state-of-the-art retrieval models at LeCaRDv2,\ndemonstrating that there is still significant room for improvement in legal\ncase retrieval. The details of LeCaRDv2 can be found at the anonymous website\nhttps://github.com/anonymous1113243/LeCaRDv2.",
            "author": [
                "Haitao Li",
                "Yunqiu Shao",
                "Yueyue Wu",
                "Qingyao Ai",
                "Yixiao Ma",
                "Yiqun Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17609v1",
                "http://arxiv.org/pdf/2310.17609v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17607v1",
            "title": "2-Form U(1) Spin Liquids: Classical Model and Quantum Aspects",
            "updated": "2023-10-26T17:30:23Z",
            "published": "2023-10-26T17:30:23Z",
            "summary": "We introduce a novel geometrically frustrated classical Ising model, dubbed\nthe \"spin vorticity model\", whose ground state manifold is a novel classical\nspin liquid, a \"2-form Coulomb phase\". We study the thermodynamics of this\nmodel both analytically and numerically, exposing the presence of algebraically\ndecaying correlations and demonstrating an extensive ground state entropy, and\ngive a comprehensive account of its ground state properties and excitations.\nEach classical ground state may be decomposed into collections of closed\n2-dimensional membranes, supporting fractionalized string excitations attached\nto the boundaries of open membranes. At finite temperature, the model can then\nbe described as a gas of closed strings in a background of fluctuating\nmembranes. We demonstrate that the emergent gauge structure of the\nlow-temperature phase is naturally captured in the formalism of 2-form\nelectrodynamics, which describes 1-dimensional charged strings coupled to a\nrank-2 anti-symmetric gauge field. After establishing the classical spin\nvorticity model, we consider perturbing it with quantum exchange interactions,\nfrom which we derive an effective membrane exchange model of the quantum\ndynamics of these membranes, which maps to a frustrated 2-form U(1) lattice\ngauge theory. We show the existence of a fine-tuned Rokhsar-Kivelson point\nwhere the quantum ground state is an equal weight superposition of all\nclassical ground state configurations. We further demonstrate how to quantize\nthe string excitations, by coupling a 1-form string field to the emergent\n2-form U(1) gauge field, thus mapping a quantum spin model to a 2-form\ngauge-Higgs model. We discuss the stability of the gapless deconfined phase of\nthis gauge theory and the possibility of realizing a novel class of phases of\nquantum matter: 2-form U(1) quantum spin liquids.",
            "author": [
                "Kristian Tyn Kai Chung",
                "Michel J. P. Gingras"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17607v1",
                "http://arxiv.org/pdf/2310.17607v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17606v1",
            "title": "Using State-of-the-Art Speech Models to Evaluate Oral Reading Fluency in\n  Ghana",
            "updated": "2023-10-26T17:30:13Z",
            "published": "2023-10-26T17:30:13Z",
            "summary": "This paper reports on a set of three recent experiments utilizing large-scale\nspeech models to evaluate the oral reading fluency (ORF) of students in Ghana.\nWhile ORF is a well-established measure of foundational literacy, assessing it\ntypically requires one-on-one sessions between a student and a trained\nevaluator, a process that is time-consuming and costly. Automating the\nevaluation of ORF could support better literacy instruction, particularly in\neducation contexts where formative assessment is uncommon due to large class\nsizes and limited resources. To our knowledge, this research is among the first\nto examine the use of the most recent versions of large-scale speech models\n(Whisper V2 wav2vec2.0) for ORF assessment in the Global South.\n  We find that Whisper V2 produces transcriptions of Ghanaian students reading\naloud with a Word Error Rate of 13.5. This is close to the model's average WER\non adult speech (12.8) and would have been considered state-of-the-art for\nchildren's speech transcription only a few years ago. We also find that when\nthese transcriptions are used to produce fully automated ORF scores, they\nclosely align with scores generated by expert human graders, with a correlation\ncoefficient of 0.96. Importantly, these results were achieved on a\nrepresentative dataset (i.e., students with regional accents, recordings taken\nin actual classrooms), using a free and publicly available speech model out of\nthe box (i.e., no fine-tuning). This suggests that using large-scale speech\nmodels to assess ORF may be feasible to implement and scale in lower-resource,\nlinguistically diverse educational contexts.",
            "author": [
                "Owen Henkel",
                "Hannah Horne-Robinson",
                "Libby Hills",
                "Bill Roberts",
                "Joshua McGrane"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17606v1",
                "http://arxiv.org/pdf/2310.17606v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17604v1",
            "title": "Characterization of acetonitrile ice irradiated by X-rays employing the\n  PROCODA code: II. Desorption processes",
            "updated": "2023-10-26T17:27:55Z",
            "published": "2023-10-26T17:27:55Z",
            "summary": "In this work, we focus on the study of radiation induced desorption processes\nthat occurred in acetonitrile ice irradiated by broadband X-rays (6 eV to 2\nkeV) monitored by FTIR spectroscopy at different radiation fluences. In a\nprevious work, we used the PROCODA code to derive the chemical evolution of the\nice. Here, we have obtained that the acetonitrile desorbed column density is at\nleast two orders of magnitude larger than the desorbed column densities of\ndaughter or granddaughter molecular species at chemical equilibrium stage. This\nindicates that total desorption column density is mainly governed by the father\nmolecule, as also previously hypothesized in experimental studies. This occurs\nbasically because the acetonitrile column density is larger than the other\nones. In particular, at chemical equilibrium acetonitrile desorption column\ndensity represents almost 98\\% of the total, while it is close to 1\\% for H, CN\nand CH$_2$, the species with larger molecular desorption percentages at\nchemical equilibrium. Another derived quantity is what we called intrinsic\ndesorption rate, which is a number per second for individual species. Some of\nthe larger intrinsic desorption rates were: CH$_3$CN ($6.2\\times 10^{-6}$), CN\n($6.2\\times 10^{-6}$), H ($5.7\\times 10^{-6}$), CH$_2$ ($5.7\\times 10^{-6}$)\nand C$_2$N$_2$ ($4.4\\times 10^{-6}$). These results help to put constrain in\nastrochemical models and can be also useful to clarify some astronomical radio\nobservations.",
            "author": [
                "G. A. Carvalho",
                "S. Pilling",
                "S. Gerasimenko"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17604v1",
                "http://arxiv.org/pdf/2310.17604v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "astro-ph.GA",
                "astro-ph.IM",
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17600v1",
            "title": "The sparse circular law, revisited",
            "updated": "2023-10-26T17:23:35Z",
            "published": "2023-10-26T17:23:35Z",
            "summary": "Let $A_n$ be an $n\\times n$ matrix with iid entries distributed as Bernoulli\nrandom variables with parameter $p = p_n$. Rudelson and Tikhomirov, in a\nbeautiful and celebrated paper, show that the distribution of eigenvalues of\n$A_n \\cdot (pn)^{-1/2}$ is approximately uniform on the unit disk as\n$n\\rightarrow \\infty$ as long as $pn \\rightarrow \\infty$, which is the natural\nnecessary condition.\n  In this paper we give a much simpler proof of this result, in its full\ngenerality, using a perspective we developed in our recent proof of the\nexistence of the limiting spectral law when $pn$ is bounded. One feature of our\nproof is that it avoids the use of $\\epsilon$-nets entirely and, instead,\nproceeds by studying the evolution of the singular values of the shifted\nmatrices $A_n-zI$ as we incrementally expose the randomness in the matrix.",
            "author": [
                "Ashwin Sah",
                "Julian Sahasrabudhe",
                "Mehtaab Sawhney"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17600v1",
                "http://arxiv.org/pdf/2310.17600v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17598v1",
            "title": "A fresh look at the nested soft-collinear subtraction scheme: NNLO QCD\n  corrections to $N$-gluon final states in $q\\bar{q}$ annihilation",
            "updated": "2023-10-26T17:20:57Z",
            "published": "2023-10-26T17:20:57Z",
            "summary": "We describe how the nested soft-collinear subtraction scheme [1] can be used\nto compute the next-to-next-to-leading order (NNLO) QCD corrections to the\nproduction of an arbitrary number of gluonic jets in hadron collisions. We show\nthat the infrared subtraction terms can be combined into recurring structures\nthat in many cases are simple iterations of those terms known from\nnext-to-leading order. The way that these recurring structures are identified\nand computed is fairly general, and can be applied to any partonic process. As\nan example, we explicitly demonstrate the cancellation of all singularities in\nthe fully-differential cross section for the $q\\bar{q} \\to X + N g $ process at\nNNLO in QCD. The finite remainder of the NNLO QCD contribution, which arises\nupon cancellation of all $\\epsilon$-poles, is expressed via relatively simple\nformulas, which can be implemented in a numerical code in a straightforward\nway. Our approach can be extended to describe arbitrary processes at NNLO in\nQCD; the largest remaining challenge at this point is the combinatorics of\nquark and gluon collinear limits.",
            "author": [
                "Federica Devoto",
                "Kirill Melnikov",
                "Raoul R\u00f6ntsch",
                "Chiara Signorile-Signorile",
                "Davide Maria Tagliabue"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17598v1",
                "http://arxiv.org/pdf/2310.17598v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17597v1",
            "title": "Engineering Waveguide Nonlinear Effective Length via Low Index Thin\n  Films",
            "updated": "2023-10-26T17:20:55Z",
            "published": "2023-10-26T17:20:55Z",
            "summary": "Novel photonic nanowires were fabricated using low-index materials and tested\nin the near-infrared spectrum to assess their nonlinear optical properties. In\nthis work, we argue the need to redefine the standard nonlinear figure of merit\nin terms of nonlinear phase shift and optical transmission for a given\npropagation distance. According to this new metric, our devices largely\noutperform all established platforms for devices with a linear footprint in the\nrange of 50 to 500 um, which is demonstrated to be an outstanding technological\ngap. For 85 fs pulses, with carrier wavelength at 1480nm and sub-uW power\nlevels, a spectral broadening exceeding 80% of the initial bandwidth was\nrecorded over a propagation length of just 50 um. Leveraging on CMOS-compatible\nprocesses and well-established materials such as silicon, silica, and indium\ntin oxide, our devices bring great promise for developing alternative\nall-optical devices with unparalleled nonlinear performances within the\naforementioned range.",
            "author": [
                "Wallace Jaffray",
                "Farhan Ali",
                "Sven Stengel",
                "Ziheng Guo",
                "Sebastian A. Schulz",
                "Andrea Di Falco",
                "Marcello Ferrera"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17597v1",
                "http://arxiv.org/pdf/2310.17597v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17595v2",
            "title": "Model-theoretic properties of nilpotent groups and Lie algebras",
            "updated": "2023-11-20T14:01:42Z",
            "published": "2023-10-26T17:14:53Z",
            "summary": "We give a systematic study of the model theory of generic nilpotent groups\nand Lie algebras. We show that the Fra\\\"iss\\'e limit of 2-nilpotent groups of\nexponent $p$ studied by Baudisch is 2-dependent and NSOP$_{1}$. We prove that\nthe class of $c$-nilpotent Lie algebras over an arbitrary field, in a language\nwith predicates for a Lazard series, is closed under free amalgamation. We show\nthat for $2 < c$, the generic $c$-nilpotent Lie algebra over $\\mathbb{F}_{p}$\nis strictly NSOP$_{4}$ and $c$-dependent. Via the Lazard correspondence, we\nobtain the same result for $c$-nilpotent groups of exponent $p$, for an odd\nprime $p > c$.",
            "author": [
                "Christian d'Elb\u00e9e",
                "Isabel M\u00fcller",
                "Nicholas Ramsey",
                "Daoud Siniora"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17595v2",
                "http://arxiv.org/pdf/2310.17595v2"
            ],
            "primary_category": "math.LO",
            "category": [
                "math.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17593v1",
            "title": "Time-dependence of SrVO$_3$ thermionic electron emission properties",
            "updated": "2023-10-26T17:13:39Z",
            "published": "2023-10-26T17:13:39Z",
            "summary": "Thermionic electron emission cathodes are critical components of various high\npower and high frequency vacuum electronic devices, electron microscopes,\ne-beam lithographic devices, and thermionic energy converters, which all demand\nan efficient and long-lasting low work function cathode. Single phase,\npolycrystalline perovskite oxide SrVO$_3$, with its intrinsic low effective\nwork function and facile synthesis process, is a promising cathode candidate,\nwhere previous works have shown evidence of an effective work function as low\nas 2.3 eV. However, assessment of the stability over time under conditions\nrelevant for operation and the related interplay of evolving surface chemistry\nwith emission performance are still missing, and necessary for understanding\nhow to best prepare, process and operate SrVO$_3$ cathodes. In this work, we\nstudy the vacuum activation process of SrVO$_3$ and find it has promising\nemission stability over 15 days of continuous high temperature operation. We\nfind that SrVO$_3$ shows surface Sr and O segregation during operation, which\nwe hypothesize is needed to create a positive surface dipole, leading to low\neffective work function. Emission repeatability from cyclic heating and cooling\nsuggests the promising stability of the low effective work function surface,\nand additional observations of drift-free emission during one hour of\ncontinuous emission testing at high temperature further demonstrates its\nexcellent performance stability.",
            "author": [
                "Md Sariful Sheikh",
                "Ryan Jacobs",
                "Dane Morgan",
                "John Booske"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17593v1",
                "http://arxiv.org/pdf/2310.17593v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17591v1",
            "title": "Lil-Bevo: Explorations of Strategies for Training Language Models in\n  More Humanlike Ways",
            "updated": "2023-10-26T17:13:07Z",
            "published": "2023-10-26T17:13:07Z",
            "summary": "We present Lil-Bevo, our submission to the BabyLM Challenge. We pretrained\nour masked language models with three ingredients: an initial pretraining with\nmusic data, training on shorter sequences before training on longer ones, and\nmasking specific tokens to target some of the BLiMP subtasks. Overall, our\nbaseline models performed above chance, but far below the performance levels of\nlarger LLMs trained on more data. We found that training on short sequences\nperformed better than training on longer sequences.Pretraining on music may\nhelp performance marginally, but, if so, the effect seems small. Our targeted\nMasked Language Modeling augmentation did not seem to improve model performance\nin general, but did seem to help on some of the specific BLiMP tasks that we\nwere targeting (e.g., Negative Polarity Items). Training performant LLMs on\nsmall amounts of data is a difficult but potentially informative task. While\nsome of our techniques showed some promise, more work is needed to explore\nwhether they can improve performance more than the modest gains here. Our code\nis available at https://github.com/venkatasg/Lil-Bevo and out models at\nhttps://huggingface.co/collections/venkatasg/babylm-653591cdb66f4bf68922873a",
            "author": [
                "Venkata S Govindarajan",
                "Juan Diego Rodriguez",
                "Kaj Bostrom",
                "Kyle Mahowald"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17591v1",
                "http://arxiv.org/pdf/2310.17591v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17590v1",
            "title": "Noise-Free Score Distillation",
            "updated": "2023-10-26T17:12:26Z",
            "published": "2023-10-26T17:12:26Z",
            "summary": "Score Distillation Sampling (SDS) has emerged as the de facto approach for\ntext-to-content generation in non-image domains. In this paper, we reexamine\nthe SDS process and introduce a straightforward interpretation that demystifies\nthe necessity for large Classifier-Free Guidance (CFG) scales, rooted in the\ndistillation of an undesired noise term. Building upon our interpretation, we\npropose a novel Noise-Free Score Distillation (NFSD) process, which requires\nminimal modifications to the original SDS framework. Through this streamlined\ndesign, we achieve more effective distillation of pre-trained text-to-image\ndiffusion models while using a nominal CFG scale. This strategic choice allows\nus to prevent the over-smoothing of results, ensuring that the generated data\nis both realistic and complies with the desired prompt. To demonstrate the\nefficacy of NFSD, we provide qualitative examples that compare NFSD and SDS, as\nwell as several other methods.",
            "author": [
                "Oren Katzir",
                "Or Patashnik",
                "Daniel Cohen-Or",
                "Dani Lischinski"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17590v1",
                "http://arxiv.org/pdf/2310.17590v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17589v1",
            "title": "An Open Source Data Contamination Report for Llama Series Models",
            "updated": "2023-10-26T17:11:42Z",
            "published": "2023-10-26T17:11:42Z",
            "summary": "Data contamination in language model evaluation is increasingly prevalent as\nthe popularity of large language models. It allows models to \"cheat\" via\nmemorisation instead of displaying true capabilities. Therefore, contamination\nanalysis has became an crucial part of reliable model evaluation to validate\nresults. However, existing contamination analysis is usually conducted\ninternally by LLM developers and often lacks transparency and completeness.\nThis paper present an open source data contamination reports for the Llama\nseries models. We analyse six popular multi-choice QA benchmarks and quantify\ntheir overlapping with the training set of Llama. Various levels of\ncontamination ranging from 1\\% to 8.7\\% are found across benchmarks. Our\ncomparison also reveals that Llama models can gain over 5\\% higher accuracy on\ncontaminated subsets versus clean subsets. Data and code are available at:\nhttps://github.com/liyucheng09/Contamination_Detector.",
            "author": [
                "Yucheng Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17589v1",
                "http://arxiv.org/pdf/2310.17589v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17588v1",
            "title": "PAC-tuning:Fine-tuning Pretrained Language Models with PAC-driven\n  Perturbed Gradient Descent",
            "updated": "2023-10-26T17:09:13Z",
            "published": "2023-10-26T17:09:13Z",
            "summary": "Fine-tuning pretrained language models (PLMs) for downstream tasks is a\nlarge-scale optimization problem, in which the choice of the training algorithm\ncritically determines how well the trained model can generalize to unseen test\ndata, especially in the context of few-shot learning. To achieve good\ngeneralization performance and avoid overfitting, techniques such as data\naugmentation and pruning are often applied. However, adding these\nregularizations necessitates heavy tuning of the hyperparameters of\noptimization algorithms, such as the popular Adam optimizer. In this paper, we\npropose a two-stage fine-tuning method, PAC-tuning, to address this\noptimization challenge. First, based on PAC-Bayes training, PAC-tuning directly\nminimizes the PAC-Bayes generalization bound to learn proper parameter\ndistribution. Second, PAC-tuning modifies the gradient by injecting noise with\nthe variance learned in the first stage into the model parameters during\ntraining, resulting in a variant of perturbed gradient descent (PGD). In the\npast, the few-shot scenario posed difficulties for PAC-Bayes training because\nthe PAC-Bayes bound, when applied to large models with limited training data,\nmight not be stringent. Our experimental results across 5 GLUE benchmark tasks\ndemonstrate that PAC-tuning successfully handles the challenges of fine-tuning\ntasks and outperforms strong baseline methods by a visible margin, further\nconfirming the potential to apply PAC training for any other settings where the\nAdam optimizer is currently used for training.",
            "author": [
                "Guangliang Liu",
                "Zhiyu Xue",
                "Xitong Zhang",
                "Kristen Marie Johnson",
                "Rongrong Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17588v1",
                "http://arxiv.org/pdf/2310.17588v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17586v1",
            "title": "Global Voices, Local Biases: Socio-Cultural Prejudices across Languages",
            "updated": "2023-10-26T17:07:50Z",
            "published": "2023-10-26T17:07:50Z",
            "summary": "Human biases are ubiquitous but not uniform: disparities exist across\nlinguistic, cultural, and societal borders. As large amounts of recent\nliterature suggest, language models (LMs) trained on human data can reflect and\noften amplify the effects of these social biases. However, the vast majority of\nexisting studies on bias are heavily skewed towards Western and European\nlanguages. In this work, we scale the Word Embedding Association Test (WEAT) to\n24 languages, enabling broader studies and yielding interesting findings about\nLM bias. We additionally enhance this data with culturally relevant information\nfor each language, capturing local contexts on a global scale. Further, to\nencompass more widely prevalent societal biases, we examine new bias dimensions\nacross toxicity, ableism, and more. Moreover, we delve deeper into the Indian\nlinguistic landscape, conducting a comprehensive regional bias analysis across\nsix prevalent Indian languages. Finally, we highlight the significance of these\nsocial biases and the new dimensions through an extensive comparison of\nembedding methods, reinforcing the need to address them in pursuit of more\nequitable language models. All code, data and results are available here:\nhttps://github.com/iamshnoo/weathub.",
            "author": [
                "Anjishnu Mukherjee",
                "Chahat Raj",
                "Ziwei Zhu",
                "Antonios Anastasopoulos"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17586v1",
                "http://arxiv.org/pdf/2310.17586v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17585v1",
            "title": "Boosting Biomolecular Switch Efficiency With Quantum Coherence",
            "updated": "2023-10-26T17:07:45Z",
            "published": "2023-10-26T17:07:45Z",
            "summary": "The resource theory of quantum thermodynamics has emerged as a powerful tool\nfor exploring the out-of-equilibrium dynamics of microscopic and highly\ncorrelated systems. Recently, it has been employed in photoisomerization, a\nmechanism facilitating vision through the isomerism of the photo receptor\nprotein rhodopsin, to elucidate the fundamental limits of efficiency inherent\nin this physical process. Limited attention has been given to the impact of\nenergetic quantum coherences in this process, as these coherences do not\ninfluence the energy level populations within an individual molecule subjected\nto thermal operations. However, a specific type of energetic quantum coherences\ncan impact the energy level populations in the scenario involving two or more\nmolecules. In this study, we examine the case of two molecules undergoing\nphotoisomerization to show that energetic quantum coherence can function as a\nresource that amplifies the efficiency of photoisomerization. These insights\noffer evidence for the role of energetic quantum coherence as a key resource in\nthe realm of quantum thermodynamics at mesoscopic scales.",
            "author": [
                "Mattheus Burkhard",
                "Onur Pusuluk",
                "Tristan Farrow"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17585v1",
                "http://arxiv.org/pdf/2310.17585v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17582v1",
            "title": "Convergence of flow-based generative models via proximal gradient\n  descent in Wasserstein space",
            "updated": "2023-10-26T17:06:23Z",
            "published": "2023-10-26T17:06:23Z",
            "summary": "Flow-based generative models enjoy certain advantages in computing the data\ngeneration and the likelihood, and have recently shown competitive empirical\nperformance. Compared to the accumulating theoretical studies on related\nscore-based diffusion models, analysis of flow-based models, which are\ndeterministic in both forward (data-to-noise) and reverse (noise-to-data)\ndirections, remain sparse. In this paper, we provide a theoretical guarantee of\ngenerating data distribution by a progressive flow model, the so-called JKO\nflow model, which implements the Jordan-Kinderleherer-Otto (JKO) scheme in a\nnormalizing flow network. Leveraging the exponential convergence of the\nproximal gradient descent (GD) in Wasserstein space, we prove the\nKullback-Leibler (KL) guarantee of data generation by a JKO flow model to be\n$O(\\varepsilon^2)$ when using $N \\lesssim \\log (1/\\varepsilon)$ many JKO steps\n($N$ Residual Blocks in the flow) where $\\varepsilon $ is the error in the\nper-step first-order condition. The assumption on data density is merely a\nfinite second moment, and the theory extends to data distributions without\ndensity and when there are inversion errors in the reverse process where we\nobtain KL-$W_2$ mixed error guarantees. The non-asymptotic convergence rate of\nthe JKO-type $W_2$-proximal GD is proved for a general class of convex\nobjective functionals that includes the KL divergence as a special case, which\ncan be of independent interest.",
            "author": [
                "Xiuyuan Cheng",
                "Jianfeng Lu",
                "Yixin Tan",
                "Yao Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17582v1",
                "http://arxiv.org/pdf/2310.17582v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "math.OC",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18373v1",
            "title": "Can LLMs Grade Short-answer Reading Comprehension Questions :\n  Foundational Literacy Assessment in LMICs",
            "updated": "2023-10-26T17:05:40Z",
            "published": "2023-10-26T17:05:40Z",
            "summary": "This paper presents emerging evidence of using generative large language\nmodels (i.e., GPT-4) to reliably evaluate short-answer reading comprehension\nquestions. Specifically, we explore how various configurations of generative\n(LLMs) are able to evaluate student responses from a new dataset, drawn from a\nbattery of reading assessments conducted with over 150 students in Ghana. As\nthis dataset is novel and hence not used in training runs of GPT, it offers an\nopportunity to test for domain shift and evaluate the generalizability of\ngenerative LLMs, which are predominantly designed and trained on data from\nhigh-income North American countries. We found that GPT-4, with minimal prompt\nengineering performed extremely well on evaluating the novel dataset (Quadratic\nWeighted Kappa 0.923, F1 0.88), substantially outperforming transfer-learning\nbased approaches, and even exceeding expert human raters (Quadratic Weighted\nKappa 0.915, F1 0.87). To the best of our knowledge, our work is the first to\nempirically evaluate the performance of generative LLMs on short-answer reading\ncomprehension questions, using real student data, and suggests that generative\nLLMs have the potential to reliably evaluate foundational literacy. Currently\nthe assessment of formative literacy and numeracy is infrequent in many low and\nmiddle-income countries (LMICs) due to the cost and operational complexities of\nconducting them at scale. Automating the grading process for reading assessment\ncould enable wider usage, and in turn improve decision-making regarding\ncurricula, school management, and teaching practice at the classroom level.\nImportantly, in contrast transfer learning based approaches, generative LLMs\ngeneralize well and the technical barriers to their use are low, making them\nmore feasible to implement and scale in lower resource educational contexts.",
            "author": [
                "Owen Henkel",
                "Libby Hills",
                "Bill Roberts",
                "Joshua McGrane"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18373v1",
                "http://arxiv.org/pdf/2310.18373v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17579v1",
            "title": "BLIS-Net: Classifying and Analyzing Signals on Graphs",
            "updated": "2023-10-26T17:03:14Z",
            "published": "2023-10-26T17:03:14Z",
            "summary": "Graph neural networks (GNNs) have emerged as a powerful tool for tasks such\nas node classification and graph classification. However, much less work has\nbeen done on signal classification, where the data consists of many functions\n(referred to as signals) defined on the vertices of a single graph. These tasks\nrequire networks designed differently from those designed for traditional GNN\ntasks. Indeed, traditional GNNs rely on localized low-pass filters, and signals\nof interest may have intricate multi-frequency behavior and exhibit long range\ninteractions. This motivates us to introduce the BLIS-Net (Bi-Lipschitz\nScattering Net), a novel GNN that builds on the previously introduced geometric\nscattering transform. Our network is able to capture both local and global\nsignal structure and is able to capture both low-frequency and high-frequency\ninformation. We make several crucial changes to the original geometric\nscattering architecture which we prove increase the ability of our network to\ncapture information about the input signal and show that BLIS-Net achieves\nsuperior performance on both synthetic and real-world data sets based on\ntraffic flow and fMRI data.",
            "author": [
                "Charles Xu",
                "Laney Goldman",
                "Valentina Guo",
                "Benjamin Hollander-Bodie",
                "Maedee Trank-Greene",
                "Ian Adelstein",
                "Edward De Brouwer",
                "Rex Ying",
                "Smita Krishnaswamy",
                "Michael Perlmutter"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17579v1",
                "http://arxiv.org/pdf/2310.17579v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17577v2",
            "title": "Global Structure-Aware Diffusion Process for Low-Light Image Enhancement",
            "updated": "2023-10-27T08:26:49Z",
            "published": "2023-10-26T17:01:52Z",
            "summary": "This paper studies a diffusion-based framework to address the low-light image\nenhancement problem. To harness the capabilities of diffusion models, we delve\ninto this intricate process and advocate for the regularization of its inherent\nODE-trajectory. To be specific, inspired by the recent research that low\ncurvature ODE-trajectory results in a stable and effective diffusion process,\nwe formulate a curvature regularization term anchored in the intrinsic\nnon-local structures of image data, i.e., global structure-aware\nregularization, which gradually facilitates the preservation of complicated\ndetails and the augmentation of contrast during the diffusion process. This\nincorporation mitigates the adverse effects of noise and artifacts resulting\nfrom the diffusion process, leading to a more precise and flexible enhancement.\nTo additionally promote learning in challenging regions, we introduce an\nuncertainty-guided regularization technique, which wisely relaxes constraints\non the most extreme regions of the image. Experimental evaluations reveal that\nthe proposed diffusion-based framework, complemented by rank-informed\nregularization, attains distinguished performance in low-light enhancement. The\noutcomes indicate substantial advancements in image quality, noise suppression,\nand contrast amplification in comparison with state-of-the-art methods. We\nbelieve this innovative approach will stimulate further exploration and\nadvancement in low-light image processing, with potential implications for\nother applications of diffusion models. The code is publicly available at\nhttps://github.com/jinnh/GSAD.",
            "author": [
                "Jinhui Hou",
                "Zhiyu Zhu",
                "Junhui Hou",
                "Hui Liu",
                "Huanqiang Zeng",
                "Hui Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17577v2",
                "http://arxiv.org/pdf/2310.17577v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17576v1",
            "title": "1D-Touch: NLP-Assisted Coarse Text Selection via a Semi-Direct Gesture",
            "updated": "2023-10-26T17:01:22Z",
            "published": "2023-10-26T17:01:22Z",
            "summary": "Existing text selection techniques on touchscreen focus on improving the\ncontrol for moving the carets. Coarse-grained text selection on word and phrase\nlevels has not received much support beyond word-snapping and entity\nrecognition. We introduce 1D-Touch, a novel text selection method that\ncomplements the carets-based sub-word selection by facilitating the selection\nof semantic units of words and above. This method employs a simple vertical\nslide gesture to expand and contract a selection area from a word. The\nexpansion can be by words or by semantic chunks ranging from sub-phrases to\nsentences. This technique shifts the concept of text selection, from defining a\nrange by locating the first and last words, towards a dynamic process of\nexpanding and contracting a textual semantic entity. To understand the effects\nof our approach, we prototyped and tested two variants: WordTouch, which offers\na straightforward word-by-word expansion, and ChunkTouch, which leverages NLP\nto chunk text into syntactic units, allowing the selection to grow by\nsemantically meaningful units in response to the sliding gesture. Our\nevaluation, focused on the coarse-grained selection tasks handled by 1D-Touch,\nshows a 20% improvement over the default word-snapping selection method on\nAndroid.",
            "author": [
                "Peiling Jiang",
                "Li Feng",
                "Fuling Sun",
                "Parakrant Sarkar",
                "Haijun Xia",
                "Can Liu"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3626483",
                "http://arxiv.org/abs/2310.17576v1",
                "http://arxiv.org/pdf/2310.17576v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17573v1",
            "title": "Wave transformations near a coronal magnetic null-point",
            "updated": "2023-10-26T16:59:20Z",
            "published": "2023-10-26T16:59:20Z",
            "summary": "We investigate the viability of MHD waves, in particular acoustic p-modes, in\ncausing strong current accumulation at the null points. We begin with a\nthree-dimensional numerical setup incorporating a gravitationally stratifed\nsolar atmosphere and an axially symmetric magnetic feld including a coronal\nmagnetic null point. To excite waves, we employ wave drivers mimicking global\np-modes. We found that most of the vertical velocity transmits through the\nAlfv\\'en acoustic equipartition layer maintaining acoustic nature while a small\nfraction generates fast waves via the mode conversion process. The fast waves\nundergo almost total refection at the transition region due to sharp gradients\nin density and Alfv\\'en speed. There are only weak signatures of Alfv\\'en wave\ngeneration near the transition region due to fast-to-Alfv\\'en mode conversion.\nSince the slow waves propagate with the local sound speed, they are not much\nafected by the density gradients at the transition region and undergo secondary\nmode conversion and transmission at the Alfv\\'en-acoustic equipartition layer\nsurrounding the null point, leading to fast wave focusing at the null point.\nThese fast waves have associated perturbations in current density, showing\noscillatory signatures compatible with the second harmonic of the driving\nfrequency which could result in resistive heating and enhanced intensity in the\npresence of fnite resistivity. We conclude that MHD waves could be a potential\nsource for oscillatory current dissipation around the magnetic null point. We\nconjecture that besides oscillatory magnetic reconnection, global p-modes could\nlead to the formation of various quasiperiodic energetic events.",
            "author": [
                "Nitin Yadav",
                "Rony Keppens"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17573v1",
                "http://arxiv.org/pdf/2310.17573v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17570v1",
            "title": "DiffS2UT: A Semantic Preserving Diffusion Model for Textless Direct\n  Speech-to-Speech Translation",
            "updated": "2023-10-26T16:58:14Z",
            "published": "2023-10-26T16:58:14Z",
            "summary": "While Diffusion Generative Models have achieved great success on image\ngeneration tasks, how to efficiently and effectively incorporate them into\nspeech generation especially translation tasks remains a non-trivial problem.\nSpecifically, due to the low information density of speech data, the\ntransformed discrete speech unit sequence is much longer than the corresponding\ntext transcription, posing significant challenges to existing auto-regressive\nmodels. Furthermore, it is not optimal to brutally apply discrete diffusion on\nthe speech unit sequence while disregarding the continuous space structure,\nwhich will degrade the generation performance significantly. In this paper, we\npropose a novel diffusion model by applying the diffusion forward process in\nthe \\textit{continuous} speech representation space, while employing the\ndiffusion backward process in the \\textit{discrete} speech unit space. In this\nway, we preserve the semantic structure of the continuous speech representation\nspace in the diffusion process and integrate the continuous and discrete\ndiffusion models. We conduct extensive experiments on the textless direct\nspeech-to-speech translation task, where the proposed method achieves\ncomparable results to the computationally intensive auto-regressive baselines\n(500 steps on average) with significantly fewer decoding steps (50 steps).",
            "author": [
                "Yongxin Zhu",
                "Zhujin Gao",
                "Xinyuan Zhou",
                "Zhongyi Ye",
                "Linli Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17570v1",
                "http://arxiv.org/pdf/2310.17570v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17568v1",
            "title": "Navigating to Success in Multi-Modal Human-Robot Collaboration: Analysis\n  and Corpus Release",
            "updated": "2023-10-26T16:56:01Z",
            "published": "2023-10-26T16:56:01Z",
            "summary": "Human-guided robotic exploration is a useful approach to gathering\ninformation at remote locations, especially those that might be too risky,\ninhospitable, or inaccessible for humans. Maintaining common ground between the\nremotely-located partners is a challenge, one that can be facilitated by\nmulti-modal communication. In this paper, we explore how participants utilized\nmultiple modalities to investigate a remote location with the help of a robotic\npartner. Participants issued spoken natural language instructions and received\nfrom the robot: text-based feedback, continuous 2D LIDAR mapping, and\nupon-request static photographs. We noticed that different strategies were\nadopted in terms of use of the modalities, and hypothesize that these\ndifferences may be correlated with success at several exploration sub-tasks. We\nfound that requesting photos may have improved the identification and counting\nof some key entities (doorways in particular) and that this strategy did not\nhinder the amount of overall area exploration. Future work with larger samples\nmay reveal the effects of more nuanced photo and dialogue strategies, which can\ninform the training of robotic agents. Additionally, we announce the release of\nour unique multi-modal corpus of human-robot communication in an exploration\ncontext: SCOUT, the Situated Corpus on Understanding Transactions.",
            "author": [
                "Stephanie M. Lukin",
                "Kimberly A. Pollard",
                "Claire Bonial",
                "Taylor Hudson",
                "Ron Arstein",
                "Clare Voss",
                "David Traum"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17568v1",
                "http://arxiv.org/pdf/2310.17568v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.CL",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17567v1",
            "title": "Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models",
            "updated": "2023-10-26T16:55:05Z",
            "published": "2023-10-26T16:55:05Z",
            "summary": "With LLMs shifting their role from statistical modeling of language to\nserving as general-purpose AI agents, how should LLM evaluations change?\nArguably, a key ability of an AI agent is to flexibly combine, as needed, the\nbasic skills it has learned. The capability to combine skills plays an\nimportant role in (human) pedagogy and also in a paper on emergence phenomena\n(Arora & Goyal, 2023).\n  This work introduces Skill-Mix, a new evaluation to measure ability to\ncombine skills. Using a list of $N$ skills the evaluator repeatedly picks\nrandom subsets of $k$ skills and asks the LLM to produce text combining that\nsubset of skills. Since the number of subsets grows like $N^k$, for even modest\n$k$ this evaluation will, with high probability, require the LLM to produce\ntext significantly different from any text in the training set. The paper\ndevelops a methodology for (a) designing and administering such an evaluation,\nand (b) automatic grading (plus spot-checking by humans) of the results using\nGPT-4 as well as the open LLaMA-2 70B model.\n  Administering a version of to popular chatbots gave results that, while\ngenerally in line with prior expectations, contained surprises. Sizeable\ndifferences exist among model capabilities that are not captured by their\nranking on popular LLM leaderboards (\"cramming for the leaderboard\").\nFurthermore, simple probability calculations indicate that GPT-4's reasonable\nperformance on $k=5$ is suggestive of going beyond \"stochastic parrot\" behavior\n(Bender et al., 2021), i.e., it combines skills in ways that it had not seen\nduring training.\n  We sketch how the methodology can lead to a Skill-Mix based eco-system of\nopen evaluations for AI capabilities of future models.",
            "author": [
                "Dingli Yu",
                "Simran Kaur",
                "Arushi Gupta",
                "Jonah Brown-Cohen",
                "Anirudh Goyal",
                "Sanjeev Arora"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17567v1",
                "http://arxiv.org/pdf/2310.17567v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17563v1",
            "title": "Breakthroughs in Cool Star Physics with the Line Emission Mapper X-ray\n  Probe",
            "updated": "2023-10-26T16:50:31Z",
            "published": "2023-10-26T16:50:31Z",
            "summary": "We outline some of the highlights of the scientific case for the advancement\nof stellar high energy physics using the Line Emission Mapper X-ray Probe ({\\it\nLEM}). The key to advancements with LEM lie in its large effective area -- up\nto 100 times that of the {\\it Chandra} MEG -- and 1~eV spectral resolution. The\nlarge effective area opens up for the first time the ability to study\ntime-dependent phenomena on their natural timescales at high resolution, such\nas flares and coronal mass ejections, and also opens the sky to much fainter\ntargets than available to {\\it Chandra} or {\\it XMM-Newton}.",
            "author": [
                "Jeremy J. Drake",
                "Juli\u00e1n Alvarado Gomez",
                "Costanza Argiroffi",
                "Ettore Flaccomio",
                "Cecilia Garraffo",
                "Nicolas Grosso",
                "Nazma Islam",
                "Margarita Karovska",
                "Vinay L. Kashyap",
                "Kristina Monsch",
                "Jan-Uwe Ness",
                "Salvatore Sciortino",
                "Bradford Wargelin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17563v1",
                "http://arxiv.org/pdf/2310.17563v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "astro-ph.HE",
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17561v1",
            "title": "Bifurcations and loss jumps in RNN training",
            "updated": "2023-10-26T16:49:44Z",
            "published": "2023-10-26T16:49:44Z",
            "summary": "Recurrent neural networks (RNNs) are popular machine learning tools for\nmodeling and forecasting sequential data and for inferring dynamical systems\n(DS) from observed time series. Concepts from DS theory (DST) have variously\nbeen used to further our understanding of both, how trained RNNs solve complex\ntasks, and the training process itself. Bifurcations are particularly important\nphenomena in DS, including RNNs, that refer to topological (qualitative)\nchanges in a system's dynamical behavior as one or more of its parameters are\nvaried. Knowing the bifurcation structure of an RNN will thus allow to deduce\nmany of its computational and dynamical properties, like its sensitivity to\nparameter variations or its behavior during training. In particular,\nbifurcations may account for sudden loss jumps observed in RNN training that\ncould severely impede the training process. Here we first mathematically prove\nfor a particular class of ReLU-based RNNs that certain bifurcations are indeed\nassociated with loss gradients tending toward infinity or zero. We then\nintroduce a novel heuristic algorithm for detecting all fixed points and\nk-cycles in ReLU-based RNNs and their existence and stability regions, hence\nbifurcation manifolds in parameter space. In contrast to previous numerical\nalgorithms for finding fixed points and common continuation methods, our\nalgorithm provides exact results and returns fixed points and cycles up to high\norders with surprisingly good scaling behavior. We exemplify the algorithm on\nthe analysis of the training process of RNNs, and find that the recently\nintroduced technique of generalized teacher forcing completely avoids certain\ntypes of bifurcations in training. Thus, besides facilitating the DST analysis\nof trained RNNs, our algorithm provides a powerful instrument for analyzing the\ntraining process itself.",
            "author": [
                "Lukas Eisenmann",
                "Zahra Monfared",
                "Niclas Alexander G\u00f6ring",
                "Daniel Durstewitz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17561v1",
                "http://arxiv.org/pdf/2310.17561v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "math.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17559v1",
            "title": "Instability of computer vision models is a necessary result of the task\n  itself",
            "updated": "2023-10-26T16:48:36Z",
            "published": "2023-10-26T16:48:36Z",
            "summary": "Adversarial examples resulting from instability of current computer vision\nmodels are an extremely important topic due to their potential to compromise\nany application. In this paper we demonstrate that instability is inevitable\ndue to a) symmetries (translational invariance) of the data, b) the categorical\nnature of the classification task, and c) the fundamental discrepancy of\nclassifying images as objects themselves. The issue is further exacerbated by\nnon-exhaustive labelling of the training data. Therefore we conclude that\ninstability is a necessary result of how the problem of computer vision is\ncurrently formulated. While the problem cannot be eliminated, through the\nanalysis of the causes, we have arrived at ways how it can be partially\nalleviated. These include i) increasing the resolution of images, ii) providing\ncontextual information for the image, iii) exhaustive labelling of training\ndata, and iv) preventing attackers from frequent access to the computer vision\nsystem.",
            "author": [
                "Oliver Turnbull",
                "George Cevora"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17559v1",
                "http://arxiv.org/pdf/2310.17559v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17558v1",
            "title": "Towards Matching Phones and Speech Representations",
            "updated": "2023-10-26T16:47:52Z",
            "published": "2023-10-26T16:47:52Z",
            "summary": "Learning phone types from phone instances has been a long-standing problem,\nwhile still being open. In this work, we revisit this problem in the context of\nself-supervised learning, and pose it as the problem of matching cluster\ncentroids to phone embeddings. We study two key properties that enable\nmatching, namely, whether cluster centroids of self-supervised representations\nreduce the variability of phone instances and respect the relationship among\nphones. We then use the matching result to produce pseudo-labels and introduce\na new loss function for improving self-supervised representations. Our\nexperiments show that the matching result captures the relationship among\nphones. Training the new loss function jointly with the regular self-supervised\nlosses, such as APC and CPC, significantly improves the downstream phone\nclassification.",
            "author": [
                "Gene-Ping Yang",
                "Hao Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17558v1",
                "http://arxiv.org/pdf/2310.17558v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17556v1",
            "title": "Efficient Numerical Algorithm for Large-Scale Damped Natural Gradient\n  Descent",
            "updated": "2023-10-26T16:46:13Z",
            "published": "2023-10-26T16:46:13Z",
            "summary": "We propose a new algorithm for efficiently solving the damped Fisher matrix\nin large-scale scenarios where the number of parameters significantly exceeds\nthe number of available samples. This problem is fundamental for natural\ngradient descent and stochastic reconfiguration. Our algorithm is based on\nCholesky decomposition and is generally applicable. Benchmark results show that\nthe algorithm is significantly faster than existing methods.",
            "author": [
                "Yixiao Chen",
                "Hao Xie",
                "Han Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17556v1",
                "http://arxiv.org/pdf/2310.17556v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NA",
                "math.NA",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17555v1",
            "title": "Interactive Robot Learning from Verbal Correction",
            "updated": "2023-10-26T16:46:12Z",
            "published": "2023-10-26T16:46:12Z",
            "summary": "The ability to learn and refine behavior after deployment has become ever\nmore important for robots as we design them to operate in unstructured\nenvironments like households. In this work, we design a new learning system\nbased on large language model (LLM), OLAF, that allows everyday users to teach\na robot using verbal corrections when the robot makes mistakes, e.g., by saying\n\"Stop what you're doing. You should move closer to the cup.\" A key feature of\nOLAF is its ability to update the robot's visuomotor neural policy based on the\nverbal feedback to avoid repeating mistakes in the future. This is in contrast\nto existing LLM-based robotic systems, which only follow verbal commands or\ncorrections but not learn from them. We demonstrate the efficacy of our design\nin experiments where a user teaches a robot to perform long-horizon\nmanipulation tasks both in simulation and on physical hardware, achieving on\naverage 20.0% improvement in policy success rate. Videos and more results are\nat https://ut-austin-rpl.github.io/olaf/",
            "author": [
                "Huihan Liu",
                "Alice Chen",
                "Yuke Zhu",
                "Adith Swaminathan",
                "Andrey Kolobov",
                "Ching-An Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17555v1",
                "http://arxiv.org/pdf/2310.17555v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17551v1",
            "title": "Unpacking the Ethical Value Alignment in Big Models",
            "updated": "2023-10-26T16:45:40Z",
            "published": "2023-10-26T16:45:40Z",
            "summary": "Big models have greatly advanced AI's ability to understand, generate, and\nmanipulate information and content, enabling numerous applications. However, as\nthese models become increasingly integrated into everyday life, their inherent\nethical values and potential biases pose unforeseen risks to society. This\npaper provides an overview of the risks and challenges associated with big\nmodels, surveys existing AI ethics guidelines, and examines the ethical\nimplications arising from the limitations of these models. Taking a normative\nethics perspective, we propose a reassessment of recent normative guidelines,\nhighlighting the importance of collaborative efforts in academia to establish a\nunified and universal AI ethics framework. Furthermore, we investigate the\nmoral inclinations of current mainstream LLMs using the Moral Foundation\ntheory, analyze existing alignment algorithms, and outline the unique\nchallenges encountered in aligning ethical values within them. To address these\nchallenges, we introduce a novel conceptual paradigm for aligning the ethical\nvalues of big models and discuss promising research directions for alignment\ncriteria, evaluation, and method, representing an initial step towards the\ninterdisciplinary construction of the ethically aligned AI\n  This paper is a modified English version of our Chinese paper\nhttps://crad.ict.ac.cn/cn/article/doi/10.7544/issn1000-1239.202330553, intended\nto help non-Chinese native speakers better understand our work.",
            "author": [
                "Xiaoyuan Yi",
                "Jing Yao",
                "Xiting Wang",
                "Xing Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17551v1",
                "http://arxiv.org/pdf/2310.17551v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17549v1",
            "title": "Scalar Cosmological Perturbations from Quantum Gravitational\n  Entanglement",
            "updated": "2023-10-26T16:44:58Z",
            "published": "2023-10-26T16:44:58Z",
            "summary": "A major challenge at the interface of quantum gravity and cosmology is to\nexplain how the large-scale structure of the Universe emerges from physics at\nthe Planck scale. In this letter, we take an important step in this direction\nby extracting the dynamics of scalar isotropic cosmological perturbations from\nfull quantum gravity, as described by the causally complete Barrett-Crane group\nfield theory model. From the perspective of the underlying quantum gravity\ntheory, cosmological perturbations are represented as nearest-neighbor two-body\nentanglement of group field theory quanta. Their effective dynamics is obtained\nvia mean-field methods and described relationally with respect to a physical\nLorentz frame causally coupled to the quantum geometry. We quantitatively study\nthese effective dynamical equations and show that at low energies they are\nperfectly consistent with those of General Relativity, while for\ntrans-Planckian scales quantum effects become important. These results\ntherefore not only provide crucial insights into the potentially purely quantum\ngravitational nature of cosmological perturbations, but also offer rich\nphenomenological implications for the physics of the early Universe.",
            "author": [
                "Alexander F. Jercher",
                "Luca Marchetti",
                "Andreas G. A. Pithis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17549v1",
                "http://arxiv.org/pdf/2310.17549v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17547v1",
            "title": "Hopf algebras from poset growth models",
            "updated": "2023-10-26T16:43:19Z",
            "published": "2023-10-26T16:43:19Z",
            "summary": "We give a framework for growth models on posets which simultaneously\ngeneralizes the Classical Sequential Growth models for posets from causal set\ntheory and the tree growth models of natural growth and simple tree classes,\nthe latter of which also appear as solutions of combinatorial Dyson-Schwinger\nequations in quantum field theory. We prove which cases of the Classical\nSequential Growth models give subHopf algebras of the Hopf algebra of posets,\nin analogy to a characterization due to Foissy in the Dyson-Schwinger case. We\nfind a family of generating sets for the Connes-Moscovici Hopf algebra.",
            "author": [
                "Karen Yeats",
                "Stav Zalel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17547v1",
                "http://arxiv.org/pdf/2310.17547v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "gr-qc",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17546v1",
            "title": "A changepoint approach to modelling non-stationary soil moisture\n  dynamics",
            "updated": "2023-10-26T16:43:03Z",
            "published": "2023-10-26T16:43:03Z",
            "summary": "Soil moisture dynamics provide an indicator of soil health that scientists\nmodel via soil drydown curves. The typical modeling process requires the soil\nmoisture time series to be manually separated into drydown segments and then\nexponential decay models are fitted to them independently. Sensor development\nover recent years means that experiments that were previously conducted over a\nfew field campaigns can now be scaled to months or even years, often at a\nhigher sampling rate. Manual identification of drydown segments is no longer\npractical. To better meet the challenge of increasing data size, this paper\nproposes a novel changepoint-based approach to automatically identify\nstructural changes in the soil drying process, and estimate the parameters\ncharacterizing the drying processes simultaneously. A simulation study is\ncarried out to assess the performance of the method. The results demonstrate\nits ability to identify structural changes and retrieve key parameters of\ninterest to soil scientists. The method is applied to hourly soil moisture time\nseries from the NEON data portal to investigate the temporal dynamics of soil\nmoisture drydown. We recover known relationships previously identified\nmanually, alongside delivering new insights into the temporal variability\nacross soil types and locations.",
            "author": [
                "Mengyi Gong",
                "Rebecca Killick",
                "Christopher Nemeth",
                "John Quinton"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17546v1",
                "http://arxiv.org/pdf/2310.17546v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP",
                "stat.ME",
                "62M10, 62P12"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17543v2",
            "title": "On invariant distributions of Feller Markov chains with applications to\n  dynamical systems with random switching",
            "updated": "2023-10-28T08:11:59Z",
            "published": "2023-10-26T16:39:02Z",
            "summary": "We introduce simple conditions ensuring that invariant distributions of a\nFeller Markov chain on a compact Riemannian manifold are absolutely continuous\nwith a lower semi-continuous, continuous or smooth density with respect to the\nRiemannian measure. This is applied to Markov chains obtained by random\ncomposition of maps and to piecewise deterministic Markov processes obtained by\nrandom switching between flows.",
            "author": [
                "Michel Bena\u00efm",
                "Oliver Tough"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17543v2",
                "http://arxiv.org/pdf/2310.17543v2"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "60, 37"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17532v1",
            "title": "Distributed Consensus in Content Centric Networking",
            "updated": "2023-10-26T16:19:31Z",
            "published": "2023-10-26T16:19:31Z",
            "summary": "We describe a method to achieve distributed consensus in a Content Centric\nNetwork using the PAXOS algorithm. Consensus is necessary, for example, if\nmultiple writers wish to agree on the current version number of a CCNx name or\nif multiple distributed systems wish to elect a leader for fast transaction\nprocessing. We describe two forms of protocols, one using standard CCNx\nInterest request and Content Object response, and the second using a CCNx Push\nrequest and response. We further divide the protocols in to those using the\nCCNx 0.x protocol where Content Object name may continue Interest names and the\nCCNx 1.0 protocol where Content Object names exactly match Interest names.",
            "author": [
                "Marc Mosko"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17532v1",
                "http://arxiv.org/pdf/2310.17532v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17530v1",
            "title": "Evaluating Bias and Fairness in Gender-Neutral Pretrained\n  Vision-and-Language Models",
            "updated": "2023-10-26T16:19:19Z",
            "published": "2023-10-26T16:19:19Z",
            "summary": "Pretrained machine learning models are known to perpetuate and even amplify\nexisting biases in data, which can result in unfair outcomes that ultimately\nimpact user experience. Therefore, it is crucial to understand the mechanisms\nbehind those prejudicial biases to ensure that model performance does not\nresult in discriminatory behaviour toward certain groups or populations. In\nthis work, we define gender bias as our case study. We quantify bias\namplification in pretraining and after fine-tuning on three families of\nvision-and-language models. We investigate the connection, if any, between the\ntwo learning stages, and evaluate how bias amplification reflects on model\nperformance. Overall, we find that bias amplification in pretraining and after\nfine-tuning are independent. We then examine the effect of continued\npretraining on gender-neutral data, finding that this reduces group\ndisparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without\nsignificantly compromising task performance.",
            "author": [
                "Laura Cabello",
                "Emanuele Bugliarello",
                "Stephanie Brandl",
                "Desmond Elliott"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17530v1",
                "http://arxiv.org/pdf/2310.17530v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17526v2",
            "title": "Can large language models replace humans in the systematic review\n  process? Evaluating GPT-4's efficacy in screening and extracting data from\n  peer-reviewed and grey literature in multiple languages",
            "updated": "2023-10-27T12:14:27Z",
            "published": "2023-10-26T16:18:30Z",
            "summary": "Systematic reviews are vital for guiding practice, research, and policy, yet\nthey are often slow and labour-intensive. Large language models (LLMs) could\noffer a way to speed up and automate systematic reviews, but their performance\nin such tasks has not been comprehensively evaluated against humans, and no\nstudy has tested GPT-4, the biggest LLM so far. This pre-registered study\nevaluates GPT-4's capability in title/abstract screening, full-text review, and\ndata extraction across various literature types and languages using a\n'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human\nperformance in most tasks, results were skewed by chance agreement and dataset\nimbalance. After adjusting for these, there was a moderate level of performance\nfor data extraction, and - barring studies that used highly reliable prompts -\nscreening performance levelled at none to moderate for different stages and\nlanguages. When screening full-text literature using highly reliable prompts,\nGPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing key\nstudies using highly reliable prompts improved its performance even more. Our\nfindings indicate that, currently, substantial caution should be used if LLMs\nare being used to conduct systematic reviews, but suggest that, for certain\nsystematic review tasks delivered under reliable prompts, LLMs can rival human\nperformance.",
            "author": [
                "Qusai Khraisha",
                "Sophie Put",
                "Johanna Kappenberg",
                "Azza Warraitch",
                "Kristin Hadfield"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17526v2",
                "http://arxiv.org/pdf/2310.17526v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17523v2",
            "title": "Adaptive Resource Management for Edge Network Slicing using Incremental\n  Multi-Agent Deep Reinforcement Learning",
            "updated": "2023-10-27T09:13:06Z",
            "published": "2023-10-26T16:16:08Z",
            "summary": "Multi-access edge computing provides local resources in mobile networks as\nthe essential means for meeting the demands of emerging ultra-reliable\nlow-latency communications. At the edge, dynamic computing requests require\nadvanced resource management for adaptive network slicing, including resource\nallocations, function scaling and load balancing to utilize only the necessary\nresources in resource-constraint networks. Recent solutions are designed for a\nstatic number of slices. Therefore, the painful process of optimization is\nrequired again with any update on the number of slices. In addition, these\nsolutions intend to maximize instant rewards, neglecting long-term resource\nscheduling. Unlike these efforts, we propose an algorithmic approach based on\nmulti-agent deep deterministic policy gradient (MADDPG) for optimizing resource\nmanagement for edge network slicing. Our objective is two-fold: (i) maximizing\nlong-term network slicing benefits in terms of delay and energy consumption,\nand (ii) adapting to slice number changes. Through simulations, we demonstrate\nthat MADDPG outperforms benchmark solutions including a static slicing-based\none from the literature, achieving stable and high long-term performance.\nAdditionally, we leverage incremental learning to facilitate a dynamic number\nof edge slices, with enhanced performance compared to pre-trained base models.\nRemarkably, this approach yields superior reward performance while saving\napproximately 90% of training time costs.",
            "author": [
                "Haiyuan Li",
                "Yuelin Liu",
                "Xueqing Zhou",
                "Xenofon Vasilakos",
                "Reza Nejabati",
                "Shuangyi Yan",
                "Dimitra Simeonidou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17523v2",
                "http://arxiv.org/pdf/2310.17523v2"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17519v2",
            "title": "FLARE: Fast Learning of Animatable and Relightable Mesh Avatars",
            "updated": "2023-10-27T09:11:32Z",
            "published": "2023-10-26T16:13:00Z",
            "summary": "Our goal is to efficiently learn personalized animatable 3D head avatars from\nvideos that are geometrically accurate, realistic, relightable, and compatible\nwith current rendering systems. While 3D meshes enable efficient processing and\nare highly portable, they lack realism in terms of shape and appearance. Neural\nrepresentations, on the other hand, are realistic but lack compatibility and\nare slow to train and render. Our key insight is that it is possible to\nefficiently learn high-fidelity 3D mesh representations via differentiable\nrendering by exploiting highly-optimized methods from traditional computer\ngraphics and approximating some of the components with neural networks. To that\nend, we introduce FLARE, a technique that enables the creation of animatable\nand relightable mesh avatars from a single monocular video. First, we learn a\ncanonical geometry using a mesh representation, enabling efficient\ndifferentiable rasterization and straightforward animation via learned\nblendshapes and linear blend skinning weights. Second, we follow\nphysically-based rendering and factor observed colors into intrinsic albedo,\nroughness, and a neural representation of the illumination, allowing the\nlearned avatars to be relit in novel scenes. Since our input videos are\ncaptured on a single device with a narrow field of view, modeling the\nsurrounding environment light is non-trivial. Based on the split-sum\napproximation for modeling specular reflections, we address this by\napproximating the pre-filtered environment map with a multi-layer perceptron\n(MLP) modulated by the surface roughness, eliminating the need to explicitly\nmodel the light. We demonstrate that our mesh-based avatar formulation,\ncombined with learned deformation, material, and lighting MLPs, produces\navatars with high-quality geometry and appearance, while also being efficient\nto train and render compared to existing approaches.",
            "author": [
                "Shrisha Bharadwaj",
                "Yufeng Zheng",
                "Otmar Hilliges",
                "Michael J. Black",
                "Victoria Fernandez-Abrevaya"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3618401",
                "http://arxiv.org/abs/2310.17519v2",
                "http://arxiv.org/pdf/2310.17519v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00012v1",
            "title": "Multi-fidelity uncertainty quantification for homogenization problems in\n  structure-property relationships from crystal plasticity finite elements",
            "updated": "2023-10-26T16:11:39Z",
            "published": "2023-10-26T16:11:39Z",
            "summary": "Crystal plasticity finite element method (CPFEM) has been an integrated\ncomputational materials engineering (ICME) workhorse to study materials\nbehaviors and structure-property relationships for the last few decades. These\nrelations are mappings from the microstructure space to the materials\nproperties space. Due to the stochastic and random nature of microstructures,\nthere is always some uncertainty associated with materials properties, for\nexample, in homogenized stress-strain curves. For critical applications with\nstrong reliability needs, it is often desirable to quantify the\nmicrostructure-induced uncertainty in the context of structure-property\nrelationships. However, this uncertainty quantification (UQ) problem often\nincurs a large computational cost because many statistically equivalent\nrepresentative volume elements (SERVEs) are needed. In this paper, we apply a\nmulti-level Monte Carlo (MLMC) method to CPFEM to study the uncertainty in\nstress-strain curves, given an ensemble of SERVEs at multiple mesh resolutions.\nBy using the information at coarse meshes, we show that it is possible to\napproximate the response at fine meshes with a much reduced computational cost.\nWe focus on problems where the model output is multi-dimensional, which\nrequires us to track multiple quantities of interest (QoIs) at the same time.\nOur numerical results show that MLMC can accelerate UQ tasks around 2.23x,\ncompared to the classical Monte Carlo (MC) method, which is widely known as the\nensemble average in the CPFEM literature.",
            "author": [
                "Anh Tran",
                "Pieterjan Robbe",
                "Theron Rodgers",
                "Hojun Lim"
            ],
            "link": [
                "http://dx.doi.org/10.1007/s11837-023-06182-x",
                "http://arxiv.org/abs/2312.00012v1",
                "http://arxiv.org/pdf/2312.00012v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cond-mat.mtrl-sci",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17514v1",
            "title": "The Validity of Evaluation Results: Assessing Concurrence Across\n  Compositionality Benchmarks",
            "updated": "2023-10-26T16:11:04Z",
            "published": "2023-10-26T16:11:04Z",
            "summary": "NLP models have progressed drastically in recent years, according to numerous\ndatasets proposed to evaluate performance. Questions remain, however, about how\nparticular dataset design choices may impact the conclusions we draw about\nmodel capabilities. In this work, we investigate this question in the domain of\ncompositional generalization. We examine the performance of six modeling\napproaches across 4 datasets, split according to 8 compositional splitting\nstrategies, ranking models by 18 compositional generalization splits in total.\nOur results show that: i) the datasets, although all designed to evaluate\ncompositional generalization, rank modeling approaches differently; ii)\ndatasets generated by humans align better with each other than they with\nsynthetic datasets, or than synthetic datasets among themselves; iii)\ngenerally, whether datasets are sampled from the same source is more predictive\nof the resulting model ranking than whether they maintain the same\ninterpretation of compositionality; and iv) which lexical items are used in the\ndata can strongly impact conclusions. Overall, our results demonstrate that\nmuch work remains to be done when it comes to assessing whether popular\nevaluation datasets measure what they intend to measure, and suggest that\nelucidating more rigorous standards for establishing the validity of evaluation\nsets could benefit the field.",
            "author": [
                "Kaiser Sun",
                "Adina Williams",
                "Dieuwke Hupkes"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17514v1",
                "http://arxiv.org/pdf/2310.17514v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17513v2",
            "title": "The Expressive Power of Low-Rank Adaptation",
            "updated": "2023-10-27T02:36:44Z",
            "published": "2023-10-26T16:08:33Z",
            "summary": "Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that\nleverages low-rank adaptation of weight matrices, has emerged as a prevalent\ntechnique for fine-tuning pre-trained models such as large language models and\ndiffusion models. Despite its huge success in practice, the theoretical\nunderpinnings of LoRA have largely remained unexplored. This paper takes the\nfirst step to bridge this gap by theoretically analyzing the expressive power\nof LoRA. We prove that, for fully connected neural networks, LoRA can adapt any\nmodel $f$ to accurately represent any smaller target model $\\overline{f}$ if\nLoRA-rank $\\geq(\\text{width of }f) \\times \\frac{\\text{depth of\n}\\overline{f}}{\\text{depth of }f}$. We also quantify the approximation error\nwhen LoRA-rank is lower than the threshold. For Transformer networks, we show\nany model can be adapted to a target model of the same size with\nrank-$(\\frac{\\text{embedding size}}{2})$ LoRA adapters.",
            "author": [
                "Yuchen Zeng",
                "Kangwook Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17513v2",
                "http://arxiv.org/pdf/2310.17513v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17512v1",
            "title": "CompeteAI: Understanding the Competition Behaviors in Large Language\n  Model-based Agents",
            "updated": "2023-10-26T16:06:20Z",
            "published": "2023-10-26T16:06:20Z",
            "summary": "Large language models (LLMs) have been widely used as agents to complete\ndifferent tasks, such as personal assistance or event planning. While most work\nhas focused on cooperation and collaboration between agents, little work\nexplores competition, another important mechanism that fosters the development\nof society and economy. In this paper, we seek to examine the competition\nbehaviors in LLM-based agents. We first propose a general framework to study\nthe competition between agents. Then, we implement a practical competitive\nenvironment using GPT-4 to simulate a virtual town with two types of agents,\nincluding restaurant agents and customer agents. Specifically, restaurant\nagents compete with each other to attract more customers, where the competition\nfosters them to transform, such as cultivating new operating strategies. The\nresults of our experiments reveal several interesting findings ranging from\nsocial learning to Matthew Effect, which aligns well with existing sociological\nand economic theories. We believe that competition between agents deserves\nfurther investigation to help us understand society better. The code will be\nreleased soon.",
            "author": [
                "Qinlin Zhao",
                "Jindong Wang",
                "Yixuan Zhang",
                "Yiqiao Jin",
                "Kaijie Zhu",
                "Hao Chen",
                "Xing Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17512v1",
                "http://arxiv.org/pdf/2310.17512v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.HC",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17506v1",
            "title": "Predicting Patient No-Shows in Community Health Clinics: A Case Study in\n  Designing a Data Analytic Product",
            "updated": "2023-10-26T15:56:43Z",
            "published": "2023-10-26T15:56:43Z",
            "summary": "The data science revolution has highlighted the varying roles that data\nanalytic products can play in a different industries and applications. There\nhas been particular interest in using analytic products coupled with\nalgorithmic prediction models to aid in human decision-making. However,\ndetailed descriptions of the decision-making process that leads to the design\nand development of analytic products are lacking in the statistical literature,\nmaking it difficult to accumulate a body of knowledge where students interested\nin the field of data science may look to learn about this process. In this\npaper, we present a case study describing the development of an analytic\nproduct for predicting whether patients will show up for scheduled appointments\nat a community health clinic. We consider the stakeholders involved and their\ninterests, along with the real-world analytical and technical trade-offs\ninvolved in developing and deploying the product. Our goal here is to highlight\nthe decisions made and evaluate them in the context of possible alternatives.\nWe find that although this case study has some unique characteristics, there\nare lessons to be learned that could translate to other settings and\napplications.",
            "author": [
                "Roger D. Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17506v1",
                "http://arxiv.org/pdf/2310.17506v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17502v1",
            "title": "Controllable Generation of Artificial Speaker Embeddings through\n  Discovery of Principal Directions",
            "updated": "2023-10-26T15:54:12Z",
            "published": "2023-10-26T15:54:12Z",
            "summary": "Customizing voice and speaking style in a speech synthesis system with\nintuitive and fine-grained controls is challenging, given that little data with\nappropriate labels is available. Furthermore, editing an existing human's voice\nalso comes with ethical concerns. In this paper, we propose a method to\ngenerate artificial speaker embeddings that cannot be linked to a real human\nwhile offering intuitive and fine-grained control over the voice and speaking\nstyle of the embeddings, without requiring any labels for speaker or style. The\nartificial and controllable embeddings can be fed to a speech synthesis system,\nconditioned on embeddings of real humans during training, without sacrificing\nprivacy during inference.",
            "author": [
                "Florian Lux",
                "Pascal Tilli",
                "Sarina Meyer",
                "Ngoc Thang Vu"
            ],
            "link": [
                "http://dx.doi.org/10.21437/Interspeech.2023-858",
                "http://arxiv.org/abs/2310.17502v1",
                "http://arxiv.org/pdf/2310.17502v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.LG",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17499v1",
            "title": "The IMS Toucan System for the Blizzard Challenge 2023",
            "updated": "2023-10-26T15:53:29Z",
            "published": "2023-10-26T15:53:29Z",
            "summary": "For our contribution to the Blizzard Challenge 2023, we improved on the\nsystem we submitted to the Blizzard Challenge 2021. Our approach entails a\nrule-based text-to-phoneme processing system that includes rule-based\ndisambiguation of homographs in the French language. It then transforms the\nphonemes to spectrograms as intermediate representations using a fast and\nefficient non-autoregressive synthesis architecture based on Conformer and\nGlow. A GAN based neural vocoder that combines recent state-of-the-art\napproaches converts the spectrogram to the final wave. We carefully designed\nthe data processing, training, and inference procedures for the challenge data.\nOur system identifier is G. Open source code and demo are available.",
            "author": [
                "Florian Lux",
                "Julia Koch",
                "Sarina Meyer",
                "Thomas Bott",
                "Nadja Schauffler",
                "Pavel Denisov",
                "Antje Schweitzer",
                "Ngoc Thang Vu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17499v1",
                "http://arxiv.org/pdf/2310.17499v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17497v1",
            "title": "Branching Particle Systems with Mutually Catalytic Interactions",
            "updated": "2023-10-26T15:52:40Z",
            "published": "2023-10-26T15:52:40Z",
            "summary": "We study a continuous time Mutually Catalytic Branching model on the\n$\\mathbb{Z}^{d}$. The model describes the behavior of two different populations\nof particles, performing random walk on the lattice in the presence of\nbranching, that is, each particle dies at a certain rate and is replaced by a\nrandom number of offsprings. The branching rate of a particle in one population\nis proportional to the number of particles of another population at the same\nsite. We study the long time behavior for this model, in particular,\ncoexistence and non-coexistence of two populations in the long run. Finally, we\nconstruct a sequence of renormalized processes and use duality techniques to\ninvestigate its limiting behavior.",
            "author": [
                "Alexandra Jamchi Fugenfirov",
                "Leonid Mytnik"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17497v1",
                "http://arxiv.org/pdf/2310.17497v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17491v1",
            "title": "FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine\n  Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation\n  Models with Mobile Edge Computing",
            "updated": "2023-10-26T15:47:44Z",
            "published": "2023-10-26T15:47:44Z",
            "summary": "The emergence of foundation models, including language and vision models, has\nreshaped AI's landscape, offering capabilities across various applications.\nDeploying and fine-tuning these large models, like GPT-3 and BERT, presents\nchallenges, especially in the current foundation model era. We introduce\nEmulator-Assisted Tuning (EAT) combined with Parameter-Efficient Fine-Tuning\n(PEFT) to form Parameter-Efficient Emulator-Assisted Tuning (PEAT). Further, we\nexpand this into federated learning as Federated PEAT (FedPEAT). FedPEAT uses\nadapters, emulators, and PEFT for federated model tuning, enhancing model\nprivacy and memory efficiency. Adapters adjust pre-trained models, while\nemulators give a compact representation of original models, addressing both\nprivacy and efficiency. Adaptable to various neural networks, our approach also\nuses deep reinforcement learning for hyper-parameter optimization. We tested\nFedPEAT in a unique scenario with a server participating in collaborative\nfederated tuning, showcasing its potential in tackling foundation model\nchallenges.",
            "author": [
                "Terence Jie Chua",
                "Wenhan Yu",
                "Jun Zhao",
                "Kwok-Yan Lam"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17491v1",
                "http://arxiv.org/pdf/2310.17491v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17490v3",
            "title": "Improving Zero-shot Reader by Reducing Distractions from Irrelevant\n  Documents in Open-Domain Question Answering",
            "updated": "2023-11-14T06:49:33Z",
            "published": "2023-10-26T15:45:12Z",
            "summary": "Large language models (LLMs) enable zero-shot approaches in open-domain\nquestion answering (ODQA), yet with limited advancements as the reader is\ncompared to the retriever. This study aims at the feasibility of a zero-shot\nreader that addresses the challenges of computational cost and the need for\nlabeled data. We find that LLMs are distracted due to irrelevant documents in\nthe retrieved set and the overconfidence of the generated answers when they are\nexploited as zero-shot readers. To tackle these problems, we mitigate the\nimpact of such documents via Distraction-aware Answer Selection (DAS) with a\nnegation-based instruction and score adjustment for proper answer selection.\nExperimental results show that our approach successfully handles distraction\nacross diverse scenarios, enhancing the performance of zero-shot readers.\nFurthermore, unlike supervised readers struggling with unseen data, zero-shot\nreaders demonstrate outstanding transferability without any training.",
            "author": [
                "Sukmin Cho",
                "Jeongyeon Seo",
                "Soyeong Jeong",
                "Jong C. Park"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17490v3",
                "http://arxiv.org/pdf/2310.17490v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17489v1",
            "title": "Bias in Evaluation Processes: An Optimization-Based Model",
            "updated": "2023-10-26T15:45:01Z",
            "published": "2023-10-26T15:45:01Z",
            "summary": "Biases with respect to socially-salient attributes of individuals have been\nwell documented in evaluation processes used in settings such as admissions and\nhiring. We view such an evaluation process as a transformation of a\ndistribution of the true utility of an individual for a task to an observed\ndistribution and model it as a solution to a loss minimization problem subject\nto an information constraint. Our model has two parameters that have been\nidentified as factors leading to biases: the resource-information trade-off\nparameter in the information constraint and the risk-averseness parameter in\nthe loss function. We characterize the distributions that arise from our model\nand study the effect of the parameters on the observed distribution. The\noutputs of our model enrich the class of distributions that can be used to\ncapture variation across groups in the observed evaluations. We empirically\nvalidate our model by fitting real-world datasets and use it to study the\neffect of interventions in a downstream selection task. These results\ncontribute to an understanding of the emergence of bias in evaluation processes\nand provide tools to guide the deployment of interventions to mitigate biases.",
            "author": [
                "L. Elisa Celis",
                "Amit Kumar",
                "Anay Mehrotra",
                "Nisheeth K. Vishnoi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17489v1",
                "http://arxiv.org/pdf/2310.17489v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.AI",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17488v2",
            "title": "LightLM: A Lightweight Deep and Narrow Language Model for Generative\n  Recommendation",
            "updated": "2023-10-30T02:50:17Z",
            "published": "2023-10-26T15:44:57Z",
            "summary": "This paper presents LightLM, a lightweight Transformer-based language model\nfor generative recommendation. While Transformer-based generative modeling has\ngained importance in various AI sub-fields such as NLP and vision, generative\nrecommendation is still in its infancy due to its unique demand on personalized\ngenerative modeling. Existing works on generative recommendation often use\nNLP-oriented Transformer architectures such as T5, GPT, LLaMA and M6, which are\nheavy-weight and are not specifically designed for recommendation tasks.\nLightLM tackles the issue by introducing a light-weight deep and narrow\nTransformer architecture, which is specifically tailored for direct generation\nof recommendation items. This structure is especially apt for straightforward\ngenerative recommendation and stems from the observation that language model\ndoes not have to be too wide for this task, as the input predominantly consists\nof short tokens that are well-suited for the model's capacity. We also show\nthat our devised user and item ID indexing methods, i.e., Spectral\nCollaborative Indexing (SCI) and Graph Collaborative Indexing (GCI), enables\nthe deep and narrow Transformer architecture to outperform large-scale language\nmodels for recommendation. Besides, to address the hallucination problem of\ngenerating items as output, we propose the constrained generation process for\ngenerative recommenders. Experiments on real-world datasets show that LightLM\noutperforms various competitive baselines in terms of both recommendation\naccuracy and efficiency. The code can be found at\nhttps://github.com/dongyuanjushi/LightLM.",
            "author": [
                "Kai Mei",
                "Yongfeng Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17488v2",
                "http://arxiv.org/pdf/2310.17488v2"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17487v1",
            "title": "High Transmission in 120-degree Sharp Bends of Inversion-symmetric and\n  Inversion-asymmetric Photonic Crystal Waveguides",
            "updated": "2023-10-26T15:44:10Z",
            "published": "2023-10-26T15:44:10Z",
            "summary": "Bending loss is one of the serious problems for constructing nanophotonic\nintegrated circuits. Recently, many works reported that valley photonic\ncrystals (VPhCs) enable significantly high transmission via 120-degree sharp\nbends. However, it is unclear whether the high bend-transmission results\ndirectly from the valley-photonic effects, which are based on the breaking of\ninversion symmetry. In this study, we conduct a series of comparative numerical\nand experimental investigations of bend-transmission in various triangular PhCs\nwith and without inversion symmetry and reveal that the high bend-transmission\nis solely determined by the domain-wall configuration and independent of the\nexistence of the inversion symmetry. Preliminary analysis of the polarization\ndistribution indicates that high bend-transmissions are closely related to the\nappearance of local topological polarization singularities near the bending\nsection. Our work demonstrates that high transmission can be achieved in a much\nwider family of PhC waveguides, which may provide novel designs for low-loss\nnanophotonic integrated circuits with enhanced flexibility and a new\nunderstanding of the nature of valley-photonics",
            "author": [
                "Wei Dai",
                "Taiki Yoda",
                "Yuto Moritake",
                "Masaaki Ono",
                "Eiichi Kuramochi",
                "Masaya Notomi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17487v1",
                "http://arxiv.org/pdf/2310.17487v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17472v1",
            "title": "Soft supersymmetry breaking as the sole origin of neutrino masses and\n  lepton number violation",
            "updated": "2023-10-26T15:21:38Z",
            "published": "2023-10-26T15:21:38Z",
            "summary": "We discuss a scenario in which the supergravity induced soft terms,\nconventionally used for breaking supersymmetry, also lead to non-zero Majorana\nneutrino masses. The soft terms lead to the spontaneous violation of the lepton\nnumber at the gravitino mass scale $m_{3/2}$ which in turn leads to (i) the\nMajorana masses of ${\\cal O} (m_{3/2})$ for the right-handed neutrinos and (ii)\nthe $R$-parity breaking at the same scale. The former contributes to light\nneutrino masses through the type I seesaw mechanism, while the latter adds to\nit through neutrino-neutralino mixing. Both contributions can scale inversely\nwith respect to $m_{3/2}$ given that gaugino and Higgsino masses are also of\norder $m_{3/2}$. Together, these two contributions adequately explain observed\nneutrino masses and mixing. One realization of the scenario also naturally\nleads to a $\\mu$ parameter of ${\\cal O} (m_{3/2})$. Despite the lepton number\nsymmetry breaking close to the weak scale, the Majoron in the model exhibits\nvery weak coupling to leptons, satisfying existing constraints on\nMajoron-lepton interactions. The right-handed neutrinos in the model have a\nlarge coupling to Higgsinos. This coupling and the relatively large heavy-light\nneutrino mixing induced through the seesaw mechanism can lead to the observable\nsignals at colliders in terms of displaced vertices.",
            "author": [
                "Anjan S. Joshipura",
                "Ketan M. Patel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17472v1",
                "http://arxiv.org/pdf/2310.17472v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17471v1",
            "title": "Foundation Model Based Native AI Framework in 6G with Cloud-Edge-End\n  Collaboration",
            "updated": "2023-10-26T15:19:40Z",
            "published": "2023-10-26T15:19:40Z",
            "summary": "Future wireless communication networks are in a position to move beyond\ndata-centric, device-oriented connectivity and offer intelligent, immersive\nexperiences based on task-oriented connections, especially in the context of\nthe thriving development of pre-trained foundation models (PFM) and the\nevolving vision of 6G native artificial intelligence (AI). Therefore,\nredefining modes of collaboration between devices and servers and constructing\nnative intelligence libraries become critically important in 6G. In this paper,\nwe analyze the challenges of achieving 6G native AI from the perspectives of\ndata, intelligence, and networks. Then, we propose a 6G native AI framework\nbased on foundation models, provide a customization approach for intent-aware\nPFM, present a construction of a task-oriented AI toolkit, and outline a novel\ncloud-edge-end collaboration paradigm. As a practical use case, we apply this\nframework for orchestration, achieving the maximum sum rate within a wireless\ncommunication system, and presenting preliminary evaluation results. Finally,\nwe outline research directions for achieving native AI in 6G.",
            "author": [
                "Xiang Chen",
                "Zhiheng Guo",
                "Xijun Wang",
                "Howard H. Yang",
                "Chenyuan Feng",
                "Junshen Su",
                "Sihui Zheng",
                "Tony Q. S. Quek"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17471v1",
                "http://arxiv.org/pdf/2310.17471v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.DC",
                "cs.LG",
                "cs.NI",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17470v1",
            "title": "Adaptive Digital Twin for UAV-Assisted Integrated Sensing,\n  Communication, and Computation Networks",
            "updated": "2023-10-26T15:17:15Z",
            "published": "2023-10-26T15:17:15Z",
            "summary": "In this paper, we study a digital twin (DT)-empowered integrated sensing,\ncommunication, and computation network. Specifically, the users perform radar\nsensing and computation offloading on the same spectrum, while unmanned aerial\nvehicles (UAVs) are deployed to provide edge computing service. We first\nformulate a multi-objective optimization problem to minimize the beampattern\nperformance of multi-input multi-output (MIMO) radars and the computation\noffloading energy consumption simultaneously. Then, we explore the prediction\ncapability of DT to provide intelligent offloading decision, where the DT\nestimation deviation is considered. To track this challenge, we reformulate the\noriginal problem as a multi-agent Markov decision process and design a\nmulti-agent proximal policy optimization (MAPPO) framework to achieve a\nflexible learning policy. Furthermore, the Beta-policy and attention mechanism\nare used to improve the training performance. Numerical results show that the\nproposed method is able to balance the performance tradeoff between sensing and\ncomputation functions, while reducing the energy consumption compared with the\nexisting studies.",
            "author": [
                "Bin Li",
                "Wenshuai Liu",
                "Wancheng Xie",
                "Ning Zhang",
                "Yan Zhang"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TGCN.2023.3298039",
                "http://arxiv.org/abs/2310.17470v1",
                "http://arxiv.org/pdf/2310.17470v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17468v1",
            "title": "Cross-modal Active Complementary Learning with Self-refining\n  Correspondence",
            "updated": "2023-10-26T15:15:11Z",
            "published": "2023-10-26T15:15:11Z",
            "summary": "Recently, image-text matching has attracted more and more attention from\nacademia and industry, which is fundamental to understanding the latent\ncorrespondence across visual and textual modalities. However, most existing\nmethods implicitly assume the training pairs are well-aligned while ignoring\nthe ubiquitous annotation noise, a.k.a noisy correspondence (NC), thereby\ninevitably leading to a performance drop. Although some methods attempt to\naddress such noise, they still face two challenging problems: excessive\nmemorizing/overfitting and unreliable correction for NC, especially under high\nnoise. To address the two problems, we propose a generalized Cross-modal Robust\nComplementary Learning framework (CRCL), which benefits from a novel Active\nComplementary Loss (ACL) and an efficient Self-refining Correspondence\nCorrection (SCC) to improve the robustness of existing methods. Specifically,\nACL exploits active and complementary learning losses to reduce the risk of\nproviding erroneous supervision, leading to theoretically and experimentally\ndemonstrated robustness against NC. SCC utilizes multiple self-refining\nprocesses with momentum correction to enlarge the receptive field for\ncorrecting correspondences, thereby alleviating error accumulation and\nachieving accurate and stable corrections. We carry out extensive experiments\non three image-text benchmarks, i.e., Flickr30K, MS-COCO, and CC152K, to verify\nthe superior robustness of our CRCL against synthetic and real-world noisy\ncorrespondences.",
            "author": [
                "Yang Qin",
                "Yuan Sun",
                "Dezhong Peng",
                "Joey Tianyi Zhou",
                "Xi Peng",
                "Peng Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17468v1",
                "http://arxiv.org/pdf/2310.17468v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17464v1",
            "title": "Superconductivity in a new layered cobalt oxychalcogenide\n  Na$_{6}$Co$_{3}$Se$_{6}$O$_{3}$ with a 3$d^{5}$ triangular lattice",
            "updated": "2023-10-26T15:11:24Z",
            "published": "2023-10-26T15:11:24Z",
            "summary": "Unconventional superconductivity in bulk materials under ambient pressure is\nextremely rare among the 3$d$ transition-metal compounds outside the layered\ncuprates and iron-based family. It is predominantly linked to highly\nanisotropic electronic properties and quasi-two-dimensional (2D) Fermi\nsurfaces. To date, the only known example of the Co-based exotic superconductor\nwas the hydrated layered cobaltate, Na$_{x}$CoO$_{2}\\cdot$ yH$_{2}$O, and its\nsuperconductivity is realized in the vicinity of a spin-1/2 Mott state.\nHowever, the nature of the superconductivity in these materials is still an\nactive subject of debate, and therefore, finding new class of superconductors\nwill help unravel the mysteries of their unconventional superconductivity. Here\nwe report the discovery of unconventional superconductivity at $\\sim$ 6.3 K in\nour newly synthesized layered compound Na$_{6}$Co$_{3}$Se$_{6}$O$_{3}$, in\nwhich the edge-shared CoSe$_{6}$ octahedra form [CoSe$_{2}$] layers with a\nperfect triangular lattice of Co ions. It is the first 3$d$ transition-metal\noxychalcogenide superconductor with distinct structural and chemical\ncharacteristics. Despite its relatively low $T_{c}$, material exhibits\nextremely high superconducting upper critical fields, $\\mu_{0}H_{c2}(0)$, which\nfar exceeds the Pauli paramagnetic limit by a factor of 3 - 4. First-principles\ncalculations show that Na$_{6}$Co$_{3}$Se$_{6}$O$_{3}$ is a rare example of\nnegative charge transfer superconductor. This new cobalt oxychalcogenide with a\ngeometrical frustration among Co spins, shows great potential as a highly\nappealing candidate for the realization of high-$T_{c}$ and/or unconventional\nsuperconductivity beyond the well-established Cu- and Fe-based superconductor\nfamilies, and opened a new field in physics and chemistry of low-dimensional\nsuperconductors.",
            "author": [
                "Jingwen Cheng",
                "Jianli Bai",
                "Binbin Ruan",
                "Pinyu Liu",
                "Yu Huang",
                "Qingxin Dong",
                "Yifei Huang",
                "Yingrui Sun",
                "Cundong Li",
                "Libo Zhang",
                "Qiaoyu Liu",
                "Wenliang Zhu",
                "Zhian Ren",
                "Genfu Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17464v1",
                "http://arxiv.org/pdf/2310.17464v1"
            ],
            "primary_category": "cond-mat.supr-con",
            "category": [
                "cond-mat.supr-con",
                "cond-mat.mtrl-sci",
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17458v1",
            "title": "Coalitional Bargaining via Reinforcement Learning: An Application to\n  Collaborative Vehicle Routing",
            "updated": "2023-10-26T15:04:23Z",
            "published": "2023-10-26T15:04:23Z",
            "summary": "Collaborative Vehicle Routing is where delivery companies cooperate by\nsharing their delivery information and performing delivery requests on behalf\nof each other. This achieves economies of scale and thus reduces cost,\ngreenhouse gas emissions, and road congestion. But which company should partner\nwith whom, and how much should each company be compensated? Traditional game\ntheoretic solution concepts, such as the Shapley value or nucleolus, are\ndifficult to calculate for the real-world problem of Collaborative Vehicle\nRouting due to the characteristic function scaling exponentially with the\nnumber of agents. This would require solving the Vehicle Routing Problem (an\nNP-Hard problem) an exponential number of times. We therefore propose to model\nthis problem as a coalitional bargaining game where - crucially - agents are\nnot given access to the characteristic function. Instead, we implicitly reason\nabout the characteristic function, and thus eliminate the need to evaluate the\nVRP an exponential number of times - we only need to evaluate it once. Our\ncontribution is that our decentralised approach is both scalable and considers\nthe self-interested nature of companies. The agents learn using a modified\nIndependent Proximal Policy Optimisation. Our RL agents outperform a strong\nheuristic bot. The agents correctly identify the optimal coalitions 79% of the\ntime with an average optimality gap of 4.2% and reduction in run-time of 62%.",
            "author": [
                "Stephen Mak",
                "Liming Xu",
                "Tim Pearce",
                "Michael Ostroumov",
                "Alexandra Brintrup"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17458v1",
                "http://arxiv.org/pdf/2310.17458v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17452v1",
            "title": "Resummation schemes for High-Electric-Charge Objects leading to improved\n  experimental mass limits",
            "updated": "2023-10-26T15:00:23Z",
            "published": "2023-10-26T15:00:23Z",
            "summary": "High-Electric-Charge compact Objects (HECOs) appear in several theoretical\nparticle physics models beyond the Standard Model, and are actively searched\nfor in current colliders, such as the Large Hadron Collider at CERN. In such\nsearches, mass bounds of these objects have been placed, using Drell-Yan and\nphoton-fusion processes at tree level so far. However, such mass-bound\nestimates are not reliable, given that, as a result of the large values of the\nelectric charge of the HECO, perturbative Quantum Electrodynamics calculations\nbreak down. In this work, we perform a Dyson-Schwinger resummation scheme (as\nopposed to lattice strong-coupling approach), which makes the computation of\nthe pertinent HECO-production cross sections reliable, thus allowing us to\nextract improved mass bounds for such objects from ATLAS and MoEDAL searches.",
            "author": [
                "Jean Alexandre",
                "Nick E. Mavromatos",
                "Vasiliki A. Mitsou",
                "Emanuela Musumeci"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17452v1",
                "http://arxiv.org/pdf/2310.17452v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "hep-ex",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17451v1",
            "title": "Generating by Understanding: Neural Visual Generation with Logical\n  Symbol Groundings",
            "updated": "2023-10-26T15:00:21Z",
            "published": "2023-10-26T15:00:21Z",
            "summary": "Despite the great success of neural visual generative models in recent years,\nintegrating them with strong symbolic knowledge reasoning systems remains a\nchallenging task. The main challenges are two-fold: one is symbol assignment,\ni.e. bonding latent factors of neural visual generators with meaningful symbols\nfrom knowledge reasoning systems. Another is rule learning, i.e. learning new\nrules, which govern the generative process of the data, to augment the\nknowledge reasoning systems. To deal with these symbol grounding problems, we\npropose a neural-symbolic learning approach, Abductive Visual Generation\n(AbdGen), for integrating logic programming systems with neural visual\ngenerative models based on the abductive learning framework. To achieve\nreliable and efficient symbol assignment, the quantized abduction method is\nintroduced for generating abduction proposals by the nearest-neighbor lookups\nwithin semantic codebooks. To achieve precise rule learning, the contrastive\nmeta-abduction method is proposed to eliminate wrong rules with positive cases\nand avoid less-informative rules with negative cases simultaneously.\nExperimental results on various benchmark datasets show that compared to the\nbaselines, AbdGen requires significantly fewer instance-level labeling\ninformation for symbol assignment. Furthermore, our approach can effectively\nlearn underlying logical generative rules from data, which is out of the\ncapability of existing approaches.",
            "author": [
                "Yifei Peng",
                "Yu Jin",
                "Zhexu Luo",
                "Yao-Xiang Ding",
                "Wang-Zhou Dai",
                "Zhong Ren",
                "Kun Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17451v1",
                "http://arxiv.org/pdf/2310.17451v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17450v1",
            "title": "Rapid Generation of Kilonova Light Curves Using Conditional Variational\n  Autoencoder",
            "updated": "2023-10-26T15:00:00Z",
            "published": "2023-10-26T15:00:00Z",
            "summary": "The discovery of the optical counterpart, along with the gravitational waves\nfrom GW170817, of the first binary neutron star merger, opened up a new era for\nmulti-messenger astrophysics. Combining the GW data with the optical\ncounterpart, also known as AT2017gfo, classified as a kilonova, has revealed\nthe nature of compact binary merging systems by extracting enriched information\nabout the total binary mass, the mass ratio, the system geometry, and the\nequation of state. Even though the detection of kilonova brought about a\nrevolution in the domain of multi-messenger astronomy, since there has been\nonly one kilonova from a gravitational wave detected binary neutron star merger\nevent so far, this limits the exact understanding of the origin and propagation\nof the kilonova. Here, we use a conditional variational autoencoder trained on\nlight curve data from two kilonova models having different temporal lengths,\nand consequently, generate kilonova light curves rapidly based on physical\nparameters of our choice with good accuracy. Once trained, the time scale for\nlight curve generation is of the order of a few milliseconds, thus speeding up\ngenerating light curves by $1000$ times compared to the simulation. The mean\nsquared error between the generated and original light curves is typically\n$0.015$ with a maximum of $0.08$ for each set of considered physical parameter;\nwhile having a maximum of $\\approx0.6$ error across the whole parameter space.\nHence, implementing this technique provides fast and reliably accurate results.",
            "author": [
                "Surojit Saha",
                "Michael J. Williams",
                "Laurence Datrier",
                "Fergus Hayes",
                "Matt Nicholl",
                "Albert K. H. Kong",
                "Martin Hendry",
                "IK Siong Heng",
                "Gavin P. Lamb",
                "En-Tzu Lin",
                "Daniel Williams"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17450v1",
                "http://arxiv.org/pdf/2310.17450v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17448v1",
            "title": "Dialect Adaptation and Data Augmentation for Low-Resource ASR: TalTech\n  Systems for the MADASR 2023 Challenge",
            "updated": "2023-10-26T14:57:08Z",
            "published": "2023-10-26T14:57:08Z",
            "summary": "This paper describes Tallinn University of Technology (TalTech) systems\ndeveloped for the ASRU MADASR 2023 Challenge. The challenge focuses on\nautomatic speech recognition of dialect-rich Indian languages with limited\ntraining audio and text data. TalTech participated in two tracks of the\nchallenge: Track 1 that allowed using only the provided training data and Track\n3 which allowed using additional audio data. In both tracks, we relied on\nwav2vec2.0 models. Our methodology diverges from the traditional procedure of\nfinetuning pretrained wav2vec2.0 models in two key points: firstly, through the\nimplementation of the aligned data augmentation technique to enhance the\nlinguistic diversity of the training data, and secondly, via the application of\ndeep prefix tuning for dialect adaptation of wav2vec2.0 models. In both tracks,\nour approach yielded significant improvements over the provided baselines,\nachieving the lowest word error rates across all participating teams.",
            "author": [
                "Tanel Alum\u00e4e",
                "Jiaming Kong",
                "Daniil Robnikov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17448v1",
                "http://arxiv.org/pdf/2310.17448v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17442v1",
            "title": "Quasisymmetries of finitely ramified Julia sets",
            "updated": "2023-10-26T14:52:48Z",
            "published": "2023-10-26T14:52:48Z",
            "summary": "We develop a theory of quasisymmetries for finitely ramified fractals, with\napplications to finitely ramified Julia sets. We prove that certain finitely\nramified fractals admit a naturally defined class of \"undistorted metrics\" that\nare all quasi-equivalent. As a result, piecewise-defined homeomorphisms of such\na fractal that locally preserve the cell structure are quasisymmetries. This\nimmediately gives a solution to the quasisymmetric uniformization problem for\ntopologically rigid fractals such as the Sierpi\\'nski triangle. We show that\nour theory applies to many finitely ramified Julia sets, and we prove that any\nconnected Julia set for a hyperbolic unicritical polynomial has infinitely many\nquasisymmetries, generalizing a result of Lyubich and Merenkov. We also prove\nthat the quasisymmetry group of the Julia set for the rational function\n$1-z^{-2}$ is infinite, and we show that the quasisymmetry groups for the Julia\nsets of a broad class of polynomials contain Thompson's group $F$.",
            "author": [
                "James Belk",
                "Bradley Forrest"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17442v1",
                "http://arxiv.org/pdf/2310.17442v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "math.GR",
                "37F10 (Primary) 20F38, 28A80, 30C62 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17438v1",
            "title": "Localisation-to-delocalisation transition of moir\u00e9 excitons in\n  WSe$_2$/MoSe$_2$ heterostructures",
            "updated": "2023-10-26T14:48:59Z",
            "published": "2023-10-26T14:48:59Z",
            "summary": "Moir\\'{e} excitons (MXs) are electron-hole pairs localised by the periodic\n(moir\\'{e}) potential forming in two-dimensional heterostructures (HSs). MXs\ncan be exploited, $e.g.$, for creating nanoscale-ordered quantum emitters and\nachieving or probing strongly correlated electronic phases at relatively high\ntemperatures. Here, we studied the exciton properties of a WSe$_2$/MoSe$_2$ HS\nfrom $T$=6 K to room temperature using time-resolved and continuous-wave\nmicro-photoluminescence, also under magnetic field. The exciton dynamics and\nemission lineshape evolution with temperature show clear signatures that MXs\nde-trap from the moir\\'{e} potential and turn into free interlayer excitons\n(IXs) at $T\\gtrsim$120 K. The MX-to-IX transition is also apparent from the\nexciton magnetic moment reversing its sign when the moir\\'{e} potential is not\ncapable to localise excitons at elevated temperatures. Concomitantly, the\nexciton formation and decay times reduce drastically. Thus, our findings\nestablish the conditions for a truly confined nature of the exciton states in a\nmoir\\'{e} superlattice with increasing temperature.",
            "author": [
                "Elena Blundo",
                "Federico Tuzi",
                "Salvatore Cianci",
                "Marzia Cuccu",
                "Katarzyna Olkowska-Pucko",
                "\u0141ucja Kipczak",
                "Giorgio Contestabile",
                "Antonio Miriametro",
                "Marco Felici",
                "Giorgio Pettinari",
                "Takashi Taniguchi",
                "Kenji Watanabe",
                "Adam Babi\u0144ski",
                "Maciej R. Molas",
                "Antonio Polimeni"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17438v1",
                "http://arxiv.org/pdf/2310.17438v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cond-mat.mes-hall"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17437v1",
            "title": "Sign Languague Recognition without frame-sequencing constraints: A proof\n  of concept on the Argentinian Sign Language",
            "updated": "2023-10-26T14:47:11Z",
            "published": "2023-10-26T14:47:11Z",
            "summary": "Automatic sign language recognition (SLR) is an important topic within the\nareas of human-computer interaction and machine learning. On the one hand, it\nposes a complex challenge that requires the intervention of various knowledge\nareas, such as video processing, image processing, intelligent systems and\nlinguistics. On the other hand, robust recognition of sign language could\nassist in the translation process and the integration of hearing-impaired\npeople, as well as the teaching of sign language for the hearing population.\n  SLR systems usually employ Hidden Markov Models, Dynamic Time Warping or\nsimilar models to recognize signs. Such techniques exploit the sequential\nordering of frames to reduce the number of hypothesis. This paper presents a\ngeneral probabilistic model for sign classification that combines\nsub-classifiers based on different types of features such as position, movement\nand handshape. The model employs a bag-of-words approach in all classification\nsteps, to explore the hypothesis that ordering is not essential for\nrecognition. The proposed model achieved an accuracy rate of 97% on an\nArgentinian Sign Language dataset containing 64 classes of signs and 3200\nsamples, providing some evidence that indeed recognition without ordering is\npossible.",
            "author": [
                "Franco Ronchetti",
                "Facundo Manuel Quiroga",
                "C\u00e9sar Estrebou",
                "Laura Lanzarini",
                "Alejandro Rosete"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-319-47955-2_28",
                "http://arxiv.org/abs/2310.17437v1",
                "http://arxiv.org/pdf/2310.17437v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17683v1",
            "title": "Sliceformer: Make Multi-head Attention as Simple as Sorting in\n  Discriminative Tasks",
            "updated": "2023-10-26T14:43:07Z",
            "published": "2023-10-26T14:43:07Z",
            "summary": "As one of the most popular neural network modules, Transformer plays a\ncentral role in many fundamental deep learning models, e.g., the ViT in\ncomputer vision and the BERT and GPT in natural language processing. The\neffectiveness of the Transformer is often attributed to its multi-head\nattention (MHA) mechanism. In this study, we discuss the limitations of MHA,\nincluding the high computational complexity due to its ``query-key-value''\narchitecture and the numerical issue caused by its softmax operation.\nConsidering the above problems and the recent development tendency of the\nattention layer, we propose an effective and efficient surrogate of the\nTransformer, called Sliceformer. Our Sliceformer replaces the classic MHA\nmechanism with an extremely simple ``slicing-sorting'' operation, i.e.,\nprojecting inputs linearly to a latent space and sorting them along different\nfeature dimensions (or equivalently, called channels). For each feature\ndimension, the sorting operation implicitly generates an implicit attention map\nwith sparse, full-rank, and doubly-stochastic structures. We consider different\nimplementations of the slicing-sorting operation and analyze their impacts on\nthe Sliceformer. We test the Sliceformer in the Long-Range Arena benchmark,\nimage classification, text classification, and molecular property prediction,\ndemonstrating its advantage in computational complexity and universal\neffectiveness in discriminative tasks. Our Sliceformer achieves comparable or\nbetter performance with lower memory cost and faster speed than the Transformer\nand its variants. Moreover, the experimental results reveal that applying our\nSliceformer can empirically suppress the risk of mode collapse when\nrepresenting data. The code is available at\n\\url{https://github.com/SDS-Lab/sliceformer}.",
            "author": [
                "Shen Yuan",
                "Hongteng Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17683v1",
                "http://arxiv.org/pdf/2310.17683v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17430v1",
            "title": "A near-autonomous and incremental intrusion detection system through\n  active learning of known and unknown attacks",
            "updated": "2023-10-26T14:37:54Z",
            "published": "2023-10-26T14:37:54Z",
            "summary": "Intrusion detection is a traditional practice of security experts, however,\nthere are several issues which still need to be tackled. Therefore, in this\npaper, after highlighting these issues, we present an architecture for a hybrid\nIntrusion Detection System (IDS) for an adaptive and incremental detection of\nboth known and unknown attacks. The IDS is composed of supervised and\nunsupervised modules, namely, a Deep Neural Network (DNN) and the K-Nearest\nNeighbors (KNN) algorithm, respectively. The proposed system is near-autonomous\nsince the intervention of the expert is minimized through the active learning\n(AL) approach. A query strategy for the labeling process is presented, it aims\nat teaching the supervised module to detect unknown attacks and improve the\ndetection of the already-known attacks. This teaching is achieved through\nsliding windows (SW) in an incremental fashion where the DNN is retrained when\nthe data is available over time, thus rendering the IDS adaptive to cope with\nthe evolutionary aspect of the network traffic. A set of experiments was\nconducted on the CICIDS2017 dataset in order to evaluate the performance of the\nIDS, promising results were obtained.",
            "author": [
                "Lynda Boukela",
                "Gongxuan Zhang",
                "Meziane Yacoub",
                "Samia Bouzefrane"
            ],
            "link": [
                "http://dx.doi.org/10.1109/SPAC53836.2021.9539947",
                "http://arxiv.org/abs/2310.17430v1",
                "http://arxiv.org/pdf/2310.17430v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "68"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17429v1",
            "title": "LSA64: An Argentinian Sign Language Dataset",
            "updated": "2023-10-26T14:37:01Z",
            "published": "2023-10-26T14:37:01Z",
            "summary": "Automatic sign language recognition is a research area that encompasses\nhuman-computer interaction, computer vision and machine learning. Robust\nautomatic recognition of sign language could assist in the translation process\nand the integration of hearing-impaired people, as well as the teaching of sign\nlanguage to the hearing population. Sign languages differ significantly in\ndifferent countries and even regions, and their syntax and semantics are\ndifferent as well from those of written languages. While the techniques for\nautomatic sign language recognition are mostly the same for different\nlanguages, training a recognition system for a new language requires having an\nentire dataset for that language. This paper presents a dataset of 64 signs\nfrom the Argentinian Sign Language (LSA). The dataset, called LSA64, contains\n3200 videos of 64 different LSA signs recorded by 10 subjects, and is a first\nstep towards building a comprehensive research-level dataset of Argentinian\nsigns, specifically tailored to sign language recognition or other machine\nlearning tasks. The subjects that performed the signs wore colored gloves to\nease the hand tracking and segmentation steps, allowing experiments on the\ndataset to focus specifically on the recognition of signs. We also present a\npre-processed version of the dataset, from which we computed statistics of\nmovement, position and handshape of the signs.",
            "author": [
                "Franco Ronchetti",
                "Facundo Manuel Quiroga",
                "C\u00e9sar Estrebou",
                "Laura Lanzarini",
                "Alejandro Rosete"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17429v1",
                "http://arxiv.org/pdf/2310.17429v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "I.5.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17428v1",
            "title": "''Fifty Shades of Bias'': Normative Ratings of Gender Bias in GPT\n  Generated English Text",
            "updated": "2023-10-26T14:34:06Z",
            "published": "2023-10-26T14:34:06Z",
            "summary": "Language serves as a powerful tool for the manifestation of societal belief\nsystems. In doing so, it also perpetuates the prevalent biases in our society.\nGender bias is one of the most pervasive biases in our society and is seen in\nonline and offline discourses. With LLMs increasingly gaining human-like\nfluency in text generation, gaining a nuanced understanding of the biases these\nsystems can generate is imperative. Prior work often treats gender bias as a\nbinary classification task. However, acknowledging that bias must be perceived\nat a relative scale; we investigate the generation and consequent receptivity\nof manual annotators to bias of varying degrees. Specifically, we create the\nfirst dataset of GPT-generated English text with normative ratings of gender\nbias. Ratings were obtained using Best--Worst Scaling -- an efficient\ncomparative annotation framework. Next, we systematically analyze the variation\nof themes of gender biases in the observed ranking and show that\nidentity-attack is most closely related to gender bias. Finally, we show the\nperformance of existing automated models trained on related concepts on our\ndataset.",
            "author": [
                "Rishav Hada",
                "Agrima Seth",
                "Harshita Diddee",
                "Kalika Bali"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17428v1",
                "http://arxiv.org/pdf/2310.17428v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17427v1",
            "title": "Handshape recognition for Argentinian Sign Language using ProbSom",
            "updated": "2023-10-26T14:32:44Z",
            "published": "2023-10-26T14:32:44Z",
            "summary": "Automatic sign language recognition is an important topic within the areas of\nhuman-computer interaction and machine learning. On the one hand, it poses a\ncomplex challenge that requires the intervention of various knowledge areas,\nsuch as video processing, image processing, intelligent systems and\nlinguistics. On the other hand, robust recognition of sign language could\nassist in the translation process and the integration of hearing-impaired\npeople.\n  This paper offers two main contributions: first, the creation of a database\nof handshapes for the Argentinian Sign Language (LSA), which is a topic that\nhas barely been discussed so far. Secondly, a technique for image processing,\ndescriptor extraction and subsequent handshape classification using a\nsupervised adaptation of self-organizing maps that is called ProbSom. This\ntechnique is compared to others in the state of the art, such as Support Vector\nMachines (SVM), Random Forests, and Neural Networks.\n  The database that was built contains 800 images with 16 LSA handshapes, and\nis a first step towards building a comprehensive database of Argentinian signs.\nThe ProbSom-based neural classifier, using the proposed descriptor, achieved an\naccuracy rate above 90%.",
            "author": [
                "Franco Ronchetti",
                "Facundo Manuel Quiroga",
                "C\u00e9sar Estrebou",
                "Laura Lanzarini"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17427v1",
                "http://arxiv.org/pdf/2310.17427v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17425v1",
            "title": "Detecting Abrupt Change of Channel Covariance Matrix in IRS-Assisted\n  Communication",
            "updated": "2023-10-26T14:30:31Z",
            "published": "2023-10-26T14:30:31Z",
            "summary": "The knowledge of channel covariance matrices is crucial to the design of\nintelligent reflecting surface (IRS) assisted communication. However, channel\ncovariance matrices may change suddenly in practice. This letter focuses on the\ndetection of the above change in IRS-assisted communication. Specifically, we\nconsider the uplink communication system consisting of a single-antenna user\n(UE), an IRS, and a multi-antenna base station (BS). We first categorize two\ntypes of channel covariance matrix changes based on their impact on system\ndesign: Type I change, which denotes the change in the BS receive covariance\nmatrix, and Type II change, which denotes the change in the IRS\ntransmit/receive covariance matrix. Secondly, a powerful method is proposed to\ndetect whether a Type I change occurs, a Type II change occurs, or no change\noccurs. The effectiveness of our proposed scheme is verified by numerical\nresults.",
            "author": [
                "Runnan Liu",
                "Liang Liu",
                "Yin Xu",
                "Dazhi He",
                "Wenjun Zhang",
                "Chang Wen Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17425v1",
                "http://arxiv.org/pdf/2310.17425v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17419v2",
            "title": "AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image\n  Detectors",
            "updated": "2023-11-03T03:26:29Z",
            "published": "2023-10-26T14:23:45Z",
            "summary": "Deep generative models can create remarkably photorealistic fake images while\nraising concerns about misinformation and copyright infringement, known as\ndeepfake threats. Deepfake detection technique is developed to distinguish\nbetween real and fake images, where the existing methods typically train\nclassifiers in the image domain or various feature domains. However, the\ngeneralizability of deepfake detection against emerging and more advanced\ngenerative models remains challenging. In this paper, inspired by the zero-shot\nadvantages of Vision-Language Models (VLMs), we propose a novel approach using\nVLMs (e.g. InstructBLIP) and prompt tuning techniques to improve the deepfake\ndetection accuracy over unseen data. We formulate deepfake detection as a\nvisual question answering problem, and tune soft prompts for InstructBLIP to\ndistinguish a query image is real or fake. We conduct full-spectrum experiments\non datasets from 3 held-in and 13 held-out generative models, covering modern\ntext-to-image generation, image editing and image attacks. Results demonstrate\nthat (1) the deepfake detection accuracy can be significantly and consistently\nimproved (from 54.6% to 91.31%, in average accuracy over unseen data) using\npretrained vision-language models with prompt tuning; (2) our superior\nperformance is at less cost of trainable parameters, resulting in an effective\nand efficient solution for deepfake detection. Code and models can be found at\nhttps://github.com/nctu-eva-lab/AntifakePrompt.",
            "author": [
                "You-Ming Chang",
                "Chen Yeh",
                "Wei-Chen Chiu",
                "Ning Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17419v2",
                "http://arxiv.org/pdf/2310.17419v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17418v1",
            "title": "Circuit as Set of Points",
            "updated": "2023-10-26T14:22:43Z",
            "published": "2023-10-26T14:22:43Z",
            "summary": "As the size of circuit designs continues to grow rapidly, artificial\nintelligence technologies are being extensively used in Electronic Design\nAutomation (EDA) to assist with circuit design. Placement and routing are the\nmost time-consuming parts of the physical design process, and how to quickly\nevaluate the placement has become a hot research topic. Prior works either\ntransformed circuit designs into images using hand-crafted methods and then\nused Convolutional Neural Networks (CNN) to extract features, which are limited\nby the quality of the hand-crafted methods and could not achieve end-to-end\ntraining, or treated the circuit design as a graph structure and used Graph\nNeural Networks (GNN) to extract features, which require time-consuming\npreprocessing. In our work, we propose a novel perspective for circuit design\nby treating circuit components as point clouds and using Transformer-based\npoint cloud perception methods to extract features from the circuit. This\napproach enables direct feature extraction from raw data without any\npreprocessing, allows for end-to-end training, and results in high performance.\nExperimental results show that our method achieves state-of-the-art performance\nin congestion prediction tasks on both the CircuitNet and ISPD2015 datasets, as\nwell as in design rule check (DRC) violation prediction tasks on the CircuitNet\ndataset. Our method establishes a bridge between the relatively mature point\ncloud perception methods and the fast-developing EDA algorithms, enabling us to\nleverage more collective intelligence to solve this task. To facilitate the\nresearch of open EDA design, source codes and pre-trained models are released\nat https://github.com/hustvl/circuitformer.",
            "author": [
                "Jialv Zou",
                "Xinggang Wang",
                "Jiahao Guo",
                "Wenyu Liu",
                "Qian Zhang",
                "Chang Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17418v1",
                "http://arxiv.org/pdf/2310.17418v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17415v1",
            "title": "PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word\n  Tokenization on Downstream Applications",
            "updated": "2023-10-26T14:20:44Z",
            "published": "2023-10-26T14:20:44Z",
            "summary": "Large protein language models are adept at capturing the underlying\nevolutionary information in primary structures, offering significant practical\nvalue for protein engineering. Compared to natural language models, protein\namino acid sequences have a smaller data volume and a limited combinatorial\nspace. Choosing an appropriate vocabulary size to optimize the pre-trained\nmodel is a pivotal issue. Moreover, despite the wealth of benchmarks and\nstudies in the natural language community, there remains a lack of a\ncomprehensive benchmark for systematically evaluating protein language model\nquality. Given these challenges, PETA trained language models with 14 different\nvocabulary sizes under three tokenization methods. It conducted thousands of\ntests on 33 diverse downstream datasets to assess the models' transfer learning\ncapabilities, incorporating two classification heads and three random seeds to\nmitigate potential biases. Extensive experiments indicate that vocabulary sizes\nbetween 50 and 200 optimize the model, whereas sizes exceeding 800\ndetrimentally affect the model's representational performance. Our code, model\nweights and datasets are available at\nhttps://github.com/ginnm/ProteinPretraining.",
            "author": [
                "Yang Tan",
                "Mingchen Li",
                "Pan Tan",
                "Ziyi Zhou",
                "Huiqun Yu",
                "Guisheng Fan",
                "Liang Hong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17415v1",
                "http://arxiv.org/pdf/2310.17415v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17413v1",
            "title": "Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases",
            "updated": "2023-10-26T14:19:48Z",
            "published": "2023-10-26T14:19:48Z",
            "summary": "We propose a comprehensive study of one-stage elicitation techniques for\nquerying a large pre-trained generative transformer (GPT-3.5-turbo) in the\nrhetorical role prediction task of legal cases. This task is known as requiring\ntextual context to be addressed. Our study explores strategies such as zero-few\nshots, task specification with definitions and clarification of annotation\nambiguities, textual context and reasoning with general prompts and specific\nquestions. We show that the number of examples, the definition of labels, the\npresentation of the (labelled) textual context and specific questions about\nthis context have a positive influence on the performance of the model. Given\nnon-equivalent test set configurations, we observed that prompting with a few\nlabelled examples from direct context can lead the model to a better\nperformance than a supervised fined-tuned multi-class classifier based on the\nBERT encoder (weighted F1 score of = 72%). But there is still a gap to reach\nthe performance of the best systems = 86%) in the LegalEval 2023 task which, on\nthe other hand, require dedicated resources, architectures and training.",
            "author": [
                "Anas Belfathi",
                "Nicolas Hernandez",
                "Laura Monceaux"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17413v1",
                "http://arxiv.org/pdf/2310.17413v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17412v1",
            "title": "Molecular dynamics simulation of W Silicon Emitting Centers formation by\n  Ga ion implantation",
            "updated": "2023-10-26T14:17:24Z",
            "published": "2023-10-26T14:17:24Z",
            "summary": "Silicon Emitting Centers (SEC) constitute promising candidates for quantum\ntelecommunication technologies. Their operation depends on the fabrication of\nlight emitting defect centers such as the triinterstitial Si complex, the\nW-Center. In this paper the formation of Si tri-interstitial clusters after Ga\nion beam bombardment on pure silicon substrates and a subsequent annealing\nstage is investigated using molecular dynamics (MD) simulations. This study\naims to understand the dynamic formation process of W centers after Ga\nimplantation and annealing in order to assist the focused ion beam and\nannealing experimental systems. A new tri-interstitial cluster identification\nmethod is proposed which considers the configuration of the clusters in the Si\nlattice in order to identify the defects which will act as candidates for the W\ncenter. This method successfully identifies W center defect candidates in an\nideal system. The number of tri-interstitial clusters increases and spread\ndeeper into the Si for higher energies and their probability of generation\nincreases until a limiting Ga dose. Furthermore, annealing can eliminate a lot\nof the unwanted defects maintaining at the same time the number of the\ntri-interstitial clusters, leading to isolated clusters with less distorted\nlocal environment.",
            "author": [
                "Christos Gennetidis",
                "Patrice Chantrenne",
                "Thomas Wood"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17412v1",
                "http://arxiv.org/pdf/2310.17412v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17410v1",
            "title": "Synthesizing Efficiently Monitorable Formulas in Metric Temporal Logic",
            "updated": "2023-10-26T14:13:15Z",
            "published": "2023-10-26T14:13:15Z",
            "summary": "In runtime verification, manually formalizing a specification for monitoring\nsystem executions is a tedious and error-prone process. To address this issue,\nwe consider the problem of automatically synthesizing formal specifications\nfrom system executions. To demonstrate our approach, we consider the popular\nspecification language Metric Temporal Logic (MTL), which is particularly\ntailored towards specifying temporal properties for cyber-physical systems\n(CPS). Most of the classical approaches for synthesizing temporal logic\nformulas aim at minimizing the size of the formula. However, for efficiency in\nmonitoring, along with the size, the amount of \"lookahead\" required for the\nspecification becomes relevant, especially for safety-critical applications. We\nformalize this notion and devise a learning algorithm that synthesizes concise\nformulas having bounded lookahead. To do so, our algorithm reduces the\nsynthesis task to a series of satisfiability problems in Linear Real Arithmetic\n(LRA) and generates MTL formulas from their satisfying assignments. The\nreduction uses a novel encoding of a popular MTL monitoring procedure using\nLRA. Finally, we implement our algorithm in a tool called TEAL and demonstrate\nits ability to synthesize efficiently monitorable MTL formulas in a CPS\napplication.",
            "author": [
                "Ritam Raha",
                "Rajarshi Roy",
                "Nathanael Fijalkow",
                "Daniel Neider",
                "Guillermo A. Perez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17410v1",
                "http://arxiv.org/pdf/2310.17410v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17409v1",
            "title": "MIGHTEE: multi-wavelength counterparts in the COSMOS field",
            "updated": "2023-10-26T14:10:52Z",
            "published": "2023-10-26T14:10:52Z",
            "summary": "In this paper we combine the Early Science radio continuum data from the\nMeerKAT International GHz Tiered Extragalactic Exploration (MIGHTEE) Survey,\nwith optical and near-infrared data and release the cross-matched catalogues.\nThe radio data used in this work covers $0.86$ deg$^2$ of the COSMOS field,\nreaches a thermal noise of $1.7$ $\\mu$Jy/beam and contains $6102$ radio\ncomponents. We visually inspect and cross-match the radio sample with optical\nand near-infrared data from the Hyper Suprime-Cam (HSC) and UltraVISTA surveys.\nThis allows the properties of active galactic nuclei and star-forming\npopulations of galaxies to be probed out to $z \\approx 5$. Additionally, we use\nthe likelihood ratio method to automatically cross-match the radio and optical\ncatalogues and compare this to the visually cross-matched catalogue. We find\nthat 94 per cent of our radio source catalogue can be matched with this method,\nwith a reliability of $95$ per cent. We proceed to show that visual\nclassification will still remain an essential process for the cross-matching of\ncomplex and extended radio sources. In the near future, the MIGHTEE survey will\nbe expanded in area to cover a total of $\\sim$20~deg$^2$; thus the combination\nof automated and visual identification will be critical. We compare redshift\ndistribution of SFG and AGN to the SKADS and T-RECS simulations and find more\nAGN than predicted at $z \\sim 1$.",
            "author": [
                "I. H. Whittam",
                "M. Prescott",
                "C. L. Hale",
                "M . J. Jarvis",
                "I. Heywood",
                "Fangxia An",
                "M. Glowacki",
                "N. Maddox",
                "L. Marchetti",
                "L. K. Morabito",
                "N. J. Adams",
                "R. A. A. Bowler",
                "P. W. Hatfield",
                "R. G. Varadaraj",
                "J. Collier",
                "B. Frank",
                "A. R. Taylor",
                "M. G. Santos",
                "M. Vaccari",
                "J. Afonso",
                "Y. Ao",
                "J. Delhaize",
                "K. Knowles",
                "S. Kolwa",
                "S. M. Randriamampandry",
                "Z. Randriamanakoto",
                "O. Smirnov",
                "D. J. B. Smith",
                "S. V. White"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17409v1",
                "http://arxiv.org/pdf/2310.17409v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17408v2",
            "title": "Tackling the Matrix Multiplication Micro-kernel Generation with Exo",
            "updated": "2023-10-27T08:28:03Z",
            "published": "2023-10-26T14:09:57Z",
            "summary": "The optimization of the matrix multiplication (or GEMM) has been a need\nduring the last decades. This operation is considered the flagship of current\nlinear algebra libraries such as BLIS, OpenBLAS, or Intel OneAPI because of its\nwidespread use in a large variety of scientific applications. The GEMM is\nusually implemented following the GotoBLAS philosophy, which tiles the GEMM\noperands and uses a series of nested loops for performance improvement. These\napproaches extract the maximum computational power of the architectures through\nsmall pieces of hardware-oriented, high-performance code called micro-kernel.\nHowever, this approach forces developers to generate, with a non-negligible\neffort, a dedicated micro-kernel for each new hardware.\n  In this work, we present a step-by-step procedure for generating\nmicro-kernels with the Exo compiler that performs close to (or even better\nthan) manually developed microkernels written with intrinsic functions or\nassembly language. Our solution also improves the portability of the generated\ncode, since a hardware target is fully specified by a concise library-based\ndescription of its instructions.",
            "author": [
                "Adri\u00e1n Castell\u00f3",
                "Julian Bellavita",
                "Grace Dinh",
                "Yuka Ikarashi",
                "H\u00e9ctor Mart\u00ednez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17408v2",
                "http://arxiv.org/pdf/2310.17408v2"
            ],
            "primary_category": "cs.MS",
            "category": [
                "cs.MS",
                "cs.CL",
                "cs.PF"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17407v1",
            "title": "Meaning and understanding in large language models",
            "updated": "2023-10-26T14:06:14Z",
            "published": "2023-10-26T14:06:14Z",
            "summary": "Can a machine understand the meanings of natural language? Recent\ndevelopments in the generative large language models (LLMs) of artificial\nintelligence have led to the belief that traditional philosophical assumptions\nabout machine understanding of language need to be revised. This article\ncritically evaluates the prevailing tendency to regard machine language\nperformance as mere syntactic manipulation and the simulation of understanding,\nwhich is only partial and very shallow, without sufficient referential\ngrounding in the world. The aim is to highlight the conditions crucial to\nattributing natural language understanding to state-of-the-art LLMs, where it\ncan be legitimately argued that LLMs not only use syntax but also semantics,\ntheir understanding not being simulated but duplicated; and determine how they\nground the meanings of linguistic expressions.",
            "author": [
                "Vladim\u00edr Havl\u00edk"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17407v1",
                "http://arxiv.org/pdf/2310.17407v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17401v1",
            "title": "Energy Efficient Robust Beamforming for Vehicular ISAC with Imperfect\n  Channel Estimation",
            "updated": "2023-10-26T13:53:11Z",
            "published": "2023-10-26T13:53:11Z",
            "summary": "This paper investigates robust beamforming for system-centric energy\nefficiency (EE) optimization in the vehicular integrated sensing and\ncommunication (ISAC) system, where the mobility of vehicles poses significant\nchallenges to channel estimation. To obtain the optimal beamforming under\nchannel uncertainty, we first formulate an optimization problem for maximizing\nthe system EE under bounded channel estimation errors. Next, fractional\nprogramming and semidefinite relaxation (SDR) are utilized to relax the rank-1\nconstraints. We further use Schur complement and S-Procedure to transform\nCramer-Rao bound (CRB) and channel estimation error constraints into convex\nforms, respectively. Based on the Lagrangian dual function and\nKarush-Kuhn-Tucker (KKT) conditions, it is proved that the optimal beamforming\nsolution is rank-1. Finally, we present comprehensive simulation results to\ndemonstrate two key findings: 1) the proposed algorithm exhibits a favorable\nconvergence rate, and 2) the approach effectively mitigates the impact of\nchannel estimation errors.",
            "author": [
                "Hanwen Zhang",
                "Haijian Sun",
                "Tianyi He",
                "Weiming Xiang",
                "Rose Qingyang Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17401v1",
                "http://arxiv.org/pdf/2310.17401v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17389v1",
            "title": "ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in\n  Real-World User-AI Conversation",
            "updated": "2023-10-26T13:35:41Z",
            "published": "2023-10-26T13:35:41Z",
            "summary": "Despite remarkable advances that large language models have achieved in\nchatbots, maintaining a non-toxic user-AI interactive environment has become\nincreasingly critical nowadays. However, previous efforts in toxicity detection\nhave been mostly based on benchmarks derived from social media content, leaving\nthe unique challenges inherent to real-world user-AI interactions\ninsufficiently explored. In this work, we introduce ToxicChat, a novel\nbenchmark based on real user queries from an open-source chatbot. This\nbenchmark contains the rich, nuanced phenomena that can be tricky for current\ntoxicity detection models to identify, revealing a significant domain\ndifference compared to social media content. Our systematic evaluation of\nmodels trained on existing toxicity datasets has shown their shortcomings when\napplied to this unique domain of ToxicChat. Our work illuminates the\npotentially overlooked challenges of toxicity detection in real-world user-AI\nconversations. In the future, ToxicChat can be a valuable resource to drive\nfurther advancements toward building a safe and healthy environment for user-AI\ninteractions.",
            "author": [
                "Zi Lin",
                "Zihan Wang",
                "Yongqi Tong",
                "Yangkun Wang",
                "Yuxin Guo",
                "Yujia Wang",
                "Jingbo Shang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17389v1",
                "http://arxiv.org/pdf/2310.17389v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17388v1",
            "title": "Architecture Design of a Networked Music Performance Platform for a\n  Chamber Choir",
            "updated": "2023-10-26T13:35:34Z",
            "published": "2023-10-26T13:35:34Z",
            "summary": "This paper describes an architecture design process for Networked Music\nPerformance (NMP) platform for medium-sized conducted music ensembles, based on\nremote rehearsals of Academic Choir of Gdansk University of Technology. The\nissues of real-time remote communication, in-person music performance, and NMP\nare described. Three iterative steps defining and extending the architecture of\nthe NMP platform with additional features to enhance its utility in remote\nrehearsals are presented. The first iteration uses a regular video conferencing\nplatform, the second iteration uses dedicated NMP devices and tools, and the\nthird iteration adds video transmission and utilizes professional low-latency\naudio and video workstations. For each iteration, the platform architecture is\ndefined and deployed with simultaneous usability tests. Its strengths and\nweaknesses are identified through qualitative and quantitative measurements -\nstatistical analysis shows a significant improvement in rehearsal quality after\neach iteration. The final optimal architecture is described and concluded with\nguidelines for creating NMP systems for said music ensembles.",
            "author": [
                "Jan Cychnerski",
                "Bart\u0142omiej Mr\u00f3z"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-15743-1_40",
                "http://arxiv.org/abs/2310.17388v1",
                "http://arxiv.org/pdf/2310.17388v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17682v1",
            "title": "Scale-invariant phase transition of disordered bosons in one dimension",
            "updated": "2023-10-26T13:30:12Z",
            "published": "2023-10-26T13:30:12Z",
            "summary": "The disorder-induced quantum phase transition between superfluid and\nnon-superfluid states of bosonic particles in one dimension is generally\nexpected to be of the Berezinskii-Kosterlitz-Thouless (BKT) type. Here, we show\nthat hard-core lattice bosons with integrable power-law hopping decaying with\ndistance as $1/r^\\alpha$ - corresponding in spin language to a $XY$ model with\npower-law couplings - undergo a non-BKT continuous phase transition instead. We\nuse exact quantum Monte-Carlo methods to determine the phase diagram for\ndifferent values of the exponent $\\alpha$, focusing on the regime $\\alpha > 2$.\nWe find that the scaling of the superfluid stiffness with the system size is\nscale-invariant at the transition point for any $\\alpha\\leq 3$ - a behavior\nincompatible with the BKT scenario and typical of continuous phase transitions\nin higher dimension. By scaling analysis near the transition point, we find\nthat our data are consistent with a correlation length exponent satisfying the\nHarris bound $\\nu \\geq 2$ and demonstrate a new universal behavior of\ndisordered bosons in one dimension. For $\\alpha>3$ our data are consistent with\na BKT scenario where the liquid is pinned by infinitesimal disorder.",
            "author": [
                "Tanul Gupta",
                "Guido Masella",
                "Francesco Mattiotti",
                "Nikolay V. Prokof'ev",
                "Guido Pupillo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17682v1",
                "http://arxiv.org/pdf/2310.17682v1"
            ],
            "primary_category": "cond-mat.quant-gas",
            "category": [
                "cond-mat.quant-gas",
                "cond-mat.dis-nn",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17379v1",
            "title": "YOLO-BEV: Generating Bird's-Eye View in the Same Way as 2D Object\n  Detection",
            "updated": "2023-10-26T13:16:27Z",
            "published": "2023-10-26T13:16:27Z",
            "summary": "Vehicle perception systems strive to achieve comprehensive and rapid visual\ninterpretation of their surroundings for improved safety and navigation. We\nintroduce YOLO-BEV, an efficient framework that harnesses a unique surrounding\ncameras setup to generate a 2D bird's-eye view of the vehicular environment. By\nstrategically positioning eight cameras, each at a 45-degree interval, our\nsystem captures and integrates imagery into a coherent 3x3 grid format, leaving\nthe center blank, providing an enriched spatial representation that facilitates\nefficient processing. In our approach, we employ YOLO's detection mechanism,\nfavoring its inherent advantages of swift response and compact model structure.\nInstead of leveraging the conventional YOLO detection head, we augment it with\na custom-designed detection head, translating the panoramically captured data\ninto a unified bird's-eye view map of ego car. Preliminary results validate the\nfeasibility of YOLO-BEV in real-time vehicular perception tasks. With its\nstreamlined architecture and potential for rapid deployment due to minimized\nparameters, YOLO-BEV poses as a promising tool that may reshape future\nperspectives in autonomous driving systems.",
            "author": [
                "Chang Liu",
                "Liguo Zhou",
                "Yanliang Huang",
                "Alois Knoll"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17379v1",
                "http://arxiv.org/pdf/2310.17379v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17372v1",
            "title": "Dialogue-based generation of self-driving simulation scenarios using\n  Large Language Models",
            "updated": "2023-10-26T13:07:01Z",
            "published": "2023-10-26T13:07:01Z",
            "summary": "Simulation is an invaluable tool for developing and evaluating controllers\nfor self-driving cars. Current simulation frameworks are driven by\nhighly-specialist domain specific languages, and so a natural language\ninterface would greatly enhance usability. But there is often a gap, consisting\nof tacit assumptions the user is making, between a concise English utterance\nand the executable code that captures the user's intent. In this paper we\ndescribe a system that addresses this issue by supporting an extended\nmultimodal interaction: the user can follow up prior instructions with\nrefinements or revisions, in reaction to the simulations that have been\ngenerated from their utterances so far. We use Large Language Models (LLMs) to\nmap the user's English utterances in this interaction into domain-specific\ncode, and so we explore the extent to which LLMs capture the context\nsensitivity that's necessary for computing the speaker's intended message in\ndiscourse.",
            "author": [
                "Antonio Valerio Miceli-Barone",
                "Alex Lascarides",
                "Craig Innes"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17372v1",
                "http://arxiv.org/pdf/2310.17372v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17369v2",
            "title": "Language and Mental Health: Measures of Emotion Dynamics from Text as\n  Linguistic Biosocial Markers",
            "updated": "2023-11-04T17:19:29Z",
            "published": "2023-10-26T13:00:26Z",
            "summary": "Research in psychopathology has shown that, at an aggregate level, the\npatterns of emotional change over time -- emotion dynamics -- are indicators of\none's mental health. One's patterns of emotion change have traditionally been\ndetermined through self-reports of emotions; however, there are known issues\nwith accuracy, bias, and ease of data collection. Recent approaches to\ndetermining emotion dynamics from one's everyday utterances addresses many of\nthese concerns, but it is not yet known whether these measures of utterance\nemotion dynamics (UED) correlate with mental health diagnoses. Here, for the\nfirst time, we study the relationship between tweet emotion dynamics and mental\nhealth disorders. We find that each of the UED metrics studied varied by the\nuser's self-disclosed diagnosis. For example: average valence was significantly\nhigher (i.e., more positive text) in the control group compared to users with\nADHD, MDD, and PTSD. Valence variability was significantly lower in the control\ngroup compared to ADHD, depression, bipolar disorder, MDD, PTSD, and OCD but\nnot PPD. Rise and recovery rates of valence also exhibited significant\ndifferences from the control. This work provides important early evidence for\nhow linguistic cues pertaining to emotion dynamics can play a crucial role as\nbiosocial markers for mental illnesses and aid in the understanding, diagnosis,\nand management of mental health disorders.",
            "author": [
                "Daniela Teodorescu",
                "Tiffany Cheng",
                "Alona Fyshe",
                "Saif M. Mohammad"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17369v2",
                "http://arxiv.org/pdf/2310.17369v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17365v2",
            "title": "Optimal entanglement generation in GHZ-type states",
            "updated": "2023-11-21T02:38:16Z",
            "published": "2023-10-26T12:57:01Z",
            "summary": "The entanglement production is key for many applications in the realm of\nquantum information, but so is the identification of processes that allow to\ncreate entanglement in a fast and sustained way. Most of the advances in this\ndirection have been circumscribed to bipartite systems only, and the rate of\nentanglement in multipartite system has been much less explored.Here we\ncontribute to the identification of processes that favor the fastest and\nsustained generation of tripartite entanglement in a class of 3-qubit GHZ-type\nstates. By considering a three-party interaction Hamiltonian, we analyse the\ndynamics of the 3-tangle and the entanglement rate to identify the optimal\nlocal operations that supplement the Hamiltonian evolution in order to speed-up\nthe generation of three-way entanglement, and to prevent its decay below a\npredetermined threshold value. The appropriate local operation that maximizes\nthe speed at which a highly-entangled state is reached has the advantage of\nrequiring access to only one of the qubits, yet depends on the actual state of\nthe system. Other universal (state-independent) local operations are found that\nconform schemes to maintain a sufficiently high amount of 3-tangle. Our results\nexpand our understanding of entanglement rates to multipartite systems, and\noffer guidance regarding the strategies that improve the efficiency in various\nquantum information processing tasks.",
            "author": [
                "N. Giovenale",
                "L. Hernandez-Martinez",
                "A. P. Majtey",
                "A. Vald\u00e9s-Hern\u00e1ndez"
            ],
            "link": [
                "http://dx.doi.org/10.1088/1751-8121/ad0a44",
                "http://arxiv.org/abs/2310.17365v2",
                "http://arxiv.org/pdf/2310.17365v2"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17359v1",
            "title": "SE(3) Diffusion Model-based Point Cloud Registration for Robust 6D\n  Object Pose Estimation",
            "updated": "2023-10-26T12:47:26Z",
            "published": "2023-10-26T12:47:26Z",
            "summary": "In this paper, we introduce an SE(3) diffusion model-based point cloud\nregistration framework for 6D object pose estimation in real-world scenarios.\nOur approach formulates the 3D registration task as a denoising diffusion\nprocess, which progressively refines the pose of the source point cloud to\nobtain a precise alignment with the model point cloud. Training our framework\ninvolves two operations: An SE(3) diffusion process and an SE(3) reverse\nprocess. The SE(3) diffusion process gradually perturbs the optimal rigid\ntransformation of a pair of point clouds by continuously injecting noise\n(perturbation transformation). By contrast, the SE(3) reverse process focuses\non learning a denoising network that refines the noisy transformation\nstep-by-step, bringing it closer to the optimal transformation for accurate\npose estimation. Unlike standard diffusion models used in linear Euclidean\nspaces, our diffusion model operates on the SE(3) manifold. This requires\nexploiting the linear Lie algebra $\\mathfrak{se}(3)$ associated with SE(3) to\nconstrain the transformation transitions during the diffusion and reverse\nprocesses. Additionally, to effectively train our denoising network, we derive\na registration-specific variational lower bound as the optimization objective\nfor model learning. Furthermore, we show that our denoising network can be\nconstructed with a surrogate registration model, making our approach applicable\nto different deep registration networks. Extensive experiments demonstrate that\nour diffusion registration framework presents outstanding pose estimation\nperformance on the real-world TUD-L, LINEMOD, and Occluded-LINEMOD datasets.",
            "author": [
                "Haobo Jiang",
                "Mathieu Salzmann",
                "Zheng Dang",
                "Jin Xie",
                "Jian Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17359v1",
                "http://arxiv.org/pdf/2310.17359v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17356v1",
            "title": "Sky Imager-Based Forecast of Solar Irradiance Using Machine Learning",
            "updated": "2023-10-26T12:44:45Z",
            "published": "2023-10-26T12:44:45Z",
            "summary": "Ahead-of-time forecasting of the output power of power plants is essential\nfor the stability of the electricity grid and ensuring uninterrupted service.\nHowever, forecasting renewable energy sources is difficult due to the chaotic\nbehavior of natural energy sources. This paper presents a new approach to\nestimate short-term solar irradiance from sky images. The~proposed algorithm\nextracts features from sky images and use learning-based techniques to estimate\nthe solar irradiance. The~performance of proposed machine learning (ML)\nalgorithm is evaluated using two publicly available datasets of sky images.\nThe~datasets contain over 350,000 images for an interval of 16 years, from 2004\nto 2020, with the corresponding global horizontal irradiance (GHI) of each\nimage as the ground truth. Compared to the state-of-the-art computationally\nheavy algorithms proposed in the literature, our approach achieves competitive\nresults with much less computational complexity for both nowcasting and\nforecasting up to 4 h ahead of time.",
            "author": [
                "Anas Al-lahham",
                "Obaidah Theeb",
                "Khaled Elalem",
                "Tariq A. Alshawi",
                "Saleh A. Alshebeili"
            ],
            "link": [
                "http://dx.doi.org/10.3390/electronics9101700",
                "http://arxiv.org/abs/2310.17356v1",
                "http://arxiv.org/pdf/2310.17356v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17355v2",
            "title": "Exploring the Trie of Rules: a fast data structure for the\n  representation of association rules",
            "updated": "2023-11-21T13:27:01Z",
            "published": "2023-10-26T12:44:33Z",
            "summary": "Association rule mining techniques can generate a large volume of sequential\ndata when implemented on transactional databases. Extracting insights from a\nlarge set of association rules has been found to be a challenging process. When\nexamining a ruleset, the fundamental question is how to summarise and represent\nmeaningful mined knowledge efficiently. Many algorithms and strategies have\nbeen developed to address issue of knowledge extraction; however, the\neffectiveness of this process can be limited by the data structures. A better\ndata structure can sufficiently affect the speed of the knowledge extraction\nprocess. This paper proposes a novel data structure, called the Trie of rules,\nfor storing a ruleset that is generated by association rule mining. The\nresulting data structure is a prefix-tree graph structure made of pre-mined\nrules. This graph stores the rules as paths within the prefix-tree in a way\nthat similar rules overlay each other. Each node in the tree represents a rule\nwhere a consequent is this node, and an antecedent is a path from this node to\nthe root of the tree. The evaluation showed that the proposed representation\ntechnique is promising. It compresses a ruleset with almost no data loss and\nbenefits in terms of time for basic operations such as searching for a specific\nrule and sorting, which is the base for many knowledge discovery methods.\nMoreover, our method demonstrated a significant improvement in traversing time,\nachieving an 8-fold increase compared to traditional data structures.",
            "author": [
                "Mikhail Kudriavtsev",
                "Marija Bezbradica",
                "Andrew McCarren"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17355v2",
                "http://arxiv.org/pdf/2310.17355v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "I.2.4; H.3.3; E.1"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17353v1",
            "title": "Cultural Adaptation of Recipes",
            "updated": "2023-10-26T12:39:20Z",
            "published": "2023-10-26T12:39:20Z",
            "summary": "Building upon the considerable advances in Large Language Models (LLMs), we\nare now equipped to address more sophisticated tasks demanding a nuanced\nunderstanding of cross-cultural contexts. A key example is recipe adaptation,\nwhich goes beyond simple translation to include a grasp of ingredients,\nculinary techniques, and dietary preferences specific to a given culture. We\nintroduce a new task involving the translation and cultural adaptation of\nrecipes between Chinese and English-speaking cuisines. To support this\ninvestigation, we present CulturalRecipes, a unique dataset comprised of\nautomatically paired recipes written in Mandarin Chinese and English. This\ndataset is further enriched with a human-written and curated test set. In this\nintricate task of cross-cultural recipe adaptation, we evaluate the performance\nof various methods, including GPT-4 and other LLMs, traditional machine\ntranslation, and information retrieval techniques. Our comprehensive analysis\nincludes both automatic and human evaluation metrics. While GPT-4 exhibits\nimpressive abilities in adapting Chinese recipes into English, it still lags\nbehind human expertise when translating English recipes into Chinese. This\nunderscores the multifaceted nature of cultural adaptations. We anticipate that\nthese insights will significantly contribute to future research on\nculturally-aware language models and their practical application in culturally\ndiverse contexts.",
            "author": [
                "Yong Cao",
                "Yova Kementchedjhieva",
                "Ruixiang Cui",
                "Antonia Karamolegkou",
                "Li Zhou",
                "Megan Dare",
                "Lucia Donatelli",
                "Daniel Hershcovich"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17353v1",
                "http://arxiv.org/pdf/2310.17353v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17352v1",
            "title": "Experimental determination of the glass transition temperature in a very\n  narrow temperature range by Temperature Modulated Optical Refractometry",
            "updated": "2023-10-26T12:37:59Z",
            "published": "2023-10-26T12:37:59Z",
            "summary": "Latest since the landmark studies of Kovacs and co-workers on the glass\ntransition of polymers, it is clear that thermally induced volume changes are\nof central importance for the understanding of the nature of the glass\ntransition. Due to the kinetic background of the canonical (thermal) glass\ntransition, it does not seem possible to derive a well-defined glass transition\ntemperature T_g based on susceptibilities such as the thermal volume expansion\ncoefficient, \\beta(T), being strongly coupled to the glass transition process.\nTherefore, in practice, T_g is for example defined via the inflection point of\nthe step-like \\beta(T) curve. In this publication, we propose to use a\nthermo-optical feature, preceding the glass transition in the high-temperature\nphase, to determine the glass transition temperature T_g of a model polymer in\na rather narrow temperature interval.",
            "author": [
                "Andreas Klingler",
                "Bernd Wetzel",
                "Jan-Kristian Krueger"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17352v1",
                "http://arxiv.org/pdf/2310.17352v1"
            ],
            "primary_category": "cond-mat.soft",
            "category": [
                "cond-mat.soft",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17348v1",
            "title": "Network Intrusion Detection with Edge-Directed Graph Multi-Head\n  Attention Networks",
            "updated": "2023-10-26T12:30:11Z",
            "published": "2023-10-26T12:30:11Z",
            "summary": "A network intrusion usually involves a number of network locations. Data flow\n(including the data generated by intrusion behaviors) among these locations\n(usually represented by IP addresses) naturally forms a graph. Thus, graph\nneural networks (GNNs) have been used in the construction of intrusion\ndetection models in recent years since they have an excellent ability to\ncapture graph topological features of intrusion data flow. However, existing\nGNN models treat node mean aggregation equally in node information aggregation.\nIn reality, the correlations of nodes and their neighbors as well as the linked\nedges are different. Assigning higher weights to nodes and edges with high\nsimilarity can highlight the correlation among them, which will enhance the\naccuracy and expressiveness of the model. To this end, this paper proposes\nnovel Edge-Directed Graph Multi-Head Attention Networks (EDGMAT) for network\nintrusion detection. The proposed EDGMAT model introduces a multi-head\nattention mechanism into the intrusion detection model. Additional weight\nlearning is realized through the combination of a multi-head attention\nmechanism and edge features. Weighted aggregation makes better use of the\nrelationship between different network traffic data. Experimental results on\nfour recent NIDS benchmark datasets show that the performance of EDGMAT in\nterms of weighted F1-Score is significantly better than that of four\nstate-of-the-art models in multi-class detection tasks.",
            "author": [
                "Xiang Li",
                "Jing Zhang",
                "Yali Yuan",
                "Cangqi Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17348v1",
                "http://arxiv.org/pdf/2310.17348v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17346v1",
            "title": "Extended Signaling Methods for Reduced Video Decoder Power Consumption\n  Using Green Metadata",
            "updated": "2023-10-26T12:26:13Z",
            "published": "2023-10-26T12:26:13Z",
            "summary": "In this paper, we discuss one aspect of the latest MPEG standard edition on\nenergy-efficient media consumption, also known as Green Metadata (ISO/IEC\n232001-11), which is the interactive signaling for remote decoder-power\nreduction for peer-to-peer video conferencing. In this scenario, the receiver\nof a video, e.g., a battery-driven portable device, can send a dedicated\nrequest to the sender which asks for a video bitstream representation that is\nless complex to decode and process. Consequently, the receiver saves energy and\nextends operating times. We provide an overview on latest studies from the\nliterature dealing with energy-saving aspects, which motivate the extension of\nthe legacy Green Metadata standard. Furthermore, we explain the newly\nintroduced syntax elements and verify their effectiveness by performing\ndedicated experiments. We show that the integration of these syntax elements\ncan lead to dynamic energy savings of up to 90% for software video decoding and\n80% for hardware video decoding, respectively.",
            "author": [
                "Christian Herglotz",
                "Matthias Kr\u00e4nzler",
                "Xixue Chu",
                "Edouard Francois",
                "Yong He",
                "Andr\u00e9 Kaup"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17346v1",
                "http://arxiv.org/pdf/2310.17346v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17345v2",
            "title": "Total light deflection in the gravitational field of solar system bodies",
            "updated": "2023-11-01T14:40:51Z",
            "published": "2023-10-26T12:24:53Z",
            "summary": "The total light deflection represents a concept, which allows one to decide\nwhich multipoles need to be implemented in the light trajectory for a given\nastrometric accuracy. The fundamental quantity of total light deflection is the\ntangent vector of the light trajectory at future infinity. It is found that\nthis tangent vector is naturally given by Chebyshev polynomials. It is just\nthis remarkable fact, which allows to determine strict upper limits of total\nlight deflection for each individual multipole of solar system bodies. Special\ncare is taken about the gauge terms. It is found that these gauge terms vanish\nat spatial infinity. The results are applied to the case of light deflection in\nthe gravitational fields of Jupiter and Saturn.",
            "author": [
                "Sven Zschocke"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17345v2",
                "http://arxiv.org/pdf/2310.17345v2"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17342v1",
            "title": "ACT-SQL: In-Context Learning for Text-to-SQL with\n  Automatically-Generated Chain-of-Thought",
            "updated": "2023-10-26T12:16:25Z",
            "published": "2023-10-26T12:16:25Z",
            "summary": "Recently Large Language Models (LLMs) have been proven to have strong\nabilities in various domains and tasks. We study the problem of prompt\ndesigning in the text-to-SQL task and attempt to improve the LLMs' reasoning\nability when generating SQL queries. Besides the trivial few-shot in-context\nlearning setting, we design our chain-of-thought (CoT) prompt with a similar\nmethod to schema linking. We provide a method named ACT-SQL to automatically\ngenerate auto-CoT exemplars and thus the whole process doesn't need manual\nlabeling. Our approach is cost-saving since we only use the LLMs' API call once\nwhen generating one SQL query. Furthermore, we extend our in-context learning\nmethod to the multi-turn text-to-SQL task. The experiment results show that the\nLLMs' performance can benefit from our ACT-SQL approach. Our approach achieves\nSOTA performance on the Spider dev set among existing in-context learning\napproaches.",
            "author": [
                "Hanchong Zhang",
                "Ruisheng Cao",
                "Lu Chen",
                "Hongshen Xu",
                "Kai Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17342v1",
                "http://arxiv.org/pdf/2310.17342v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17341v3",
            "title": "De-novo Chemical Reaction Generation by Means of Temporal Convolutional\n  Neural Networks",
            "updated": "2023-11-01T23:27:13Z",
            "published": "2023-10-26T12:15:56Z",
            "summary": "We present here a combination of two networks, Recurrent Neural Networks\n(RNN) and Temporarily Convolutional Neural Networks (TCN) in de novo reaction\ngeneration using the novel Reaction Smiles-like representation of reactions\n(CGRSmiles) with atom mapping directly incorporated. Recurrent Neural Networks\nare known for their autoregressive properties and are frequently used in\nlanguage modelling with direct application to SMILES generation. The relatively\nnovel TCNs possess similar properties with wide receptive field while obeying\nthe causality required for natural language processing (NLP). The combination\nof both latent representations expressed through TCN and RNN results in an\noverall better performance compared to RNN alone. Additionally, it is shown\nthat different fine-tuning protocols have a profound impact on generative scope\nof the model when applied on a dataset of interest via transfer learning.",
            "author": [
                "Andrei Buin",
                "Hung Yi Chiang",
                "S. Andrew Gadsden",
                "Faraz A. Alderson"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17341v3",
                "http://arxiv.org/pdf/2310.17341v3"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17335v1",
            "title": "A multi-artifact EEG denoising by frequency-based deep learning",
            "updated": "2023-10-26T12:01:47Z",
            "published": "2023-10-26T12:01:47Z",
            "summary": "Electroencephalographic (EEG) signals are fundamental to neuroscience\nresearch and clinical applications such as brain-computer interfaces and\nneurological disorder diagnosis. These signals are typically a combination of\nneurological activity and noise, originating from various sources, including\nphysiological artifacts like ocular and muscular movements. Under this setting,\nwe tackle the challenge of distinguishing neurological activity from\nnoise-related sources. We develop a novel EEG denoising model that operates in\nthe frequency domain, leveraging prior knowledge about noise spectral features\nto adaptively compute optimal convolutional filters for noise separation. The\nmodel is trained to learn an empirical relationship connecting the spectral\ncharacteristics of noise and noisy signal to a non-linear transformation which\nallows signal denoising. Performance evaluation on the EEGdenoiseNet dataset\nshows that the proposed model achieves optimal results according to both\ntemporal and spectral metrics. The model is found to remove physiological\nartifacts from input EEG data, thus achieving effective EEG denoising. Indeed,\nthe model performance either matches or outperforms that achieved by benchmark\nmodels, proving to effectively remove both muscle and ocular artifacts without\nthe need to perform any training on the particular type of artifact.",
            "author": [
                "Matteo Gabardi",
                "Aurora Saibene",
                "Francesca Gasparini",
                "Daniele Rizzo",
                "Fabio Antonio Stella"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17335v1",
                "http://arxiv.org/pdf/2310.17335v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17334v1",
            "title": "Bayesian Optimization for Personalized Dose-Finding Trials with\n  Combination Therapies",
            "updated": "2023-10-26T12:01:05Z",
            "published": "2023-10-26T12:01:05Z",
            "summary": "Identification of optimal dose combinations in early phase dose-finding\ntrials is challenging, due to the trade-off between precisely estimating the\nmany parameters required to flexibly model the dose-response surface, and the\nsmall sample sizes in early phase trials. Existing methods often restrict the\nsearch to pre-defined dose combinations, which may fail to identify regions of\noptimality in the dose combination space. These difficulties are even more\npertinent in the context of personalized dose-finding, where patient\ncharacteristics are used to identify tailored optimal dose combinations. To\novercome these challenges, we propose the use of Bayesian optimization for\nfinding optimal dose combinations in standard (\"one size fits all\") and\npersonalized multi-agent dose-finding trials. Bayesian optimization is a method\nfor estimating the global optima of expensive-to-evaluate objective functions.\nThe objective function is approximated by a surrogate model, commonly a\nGaussian process, paired with a sequential design strategy to select the next\npoint via an acquisition function. This work is motivated by an\nindustry-sponsored problem, where focus is on optimizing a dual-agent therapy\nin a setting featuring minimal toxicity. To compare the performance of the\nstandard and personalized methods under this setting, simulation studies are\nperformed for a variety of scenarios. Our study concludes that taking a\npersonalized approach is highly beneficial in the presence of heterogeneity.",
            "author": [
                "James Willard",
                "Shirin Golchi",
                "Erica E. M. Moodie",
                "Bruno Boulanger",
                "Bradley P. Carlin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17334v1",
                "http://arxiv.org/pdf/2310.17334v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17333v1",
            "title": "Arabic Fine-Grained Entity Recognition",
            "updated": "2023-10-26T11:59:45Z",
            "published": "2023-10-26T11:59:45Z",
            "summary": "Traditional NER systems are typically trained to recognize coarse-grained\nentities, and less attention is given to classifying entities into a hierarchy\nof fine-grained lower-level subtypes. This article aims to advance Arabic NER\nwith fine-grained entities. We chose to extend Wojood (an open-source Nested\nArabic Named Entity Corpus) with subtypes. In particular, four main entity\ntypes in Wojood, geopolitical entity (GPE), location (LOC), organization (ORG),\nand facility (FAC), are extended with 31 subtypes. To do this, we first revised\nWojood's annotations of GPE, LOC, ORG, and FAC to be compatible with the LDC's\nACE guidelines, which yielded 5, 614 changes. Second, all mentions of GPE, LOC,\nORG, and FAC (~44K) in Wojood are manually annotated with the LDC's ACE\nsub-types. We refer to this extended version of Wojood as WojoodF ine. To\nevaluate our annotations, we measured the inter-annotator agreement (IAA) using\nboth Cohen's Kappa and F1 score, resulting in 0.9861 and 0.9889, respectively.\nTo compute the baselines of WojoodF ine, we fine-tune three pre-trained Arabic\nBERT encoders in three settings: flat NER, nested NER and nested NER with\nsubtypes and achieved F1 score of 0.920, 0.866, and 0.885, respectively. Our\ncorpus and models are open-source and available at\nhttps://sina.birzeit.edu/wojood/.",
            "author": [
                "Haneen Liqreina",
                "Mustafa Jarrar",
                "Mohammed Khalilia",
                "Ahmed Oumar El-Shangiti",
                "Muhammad AbdulMageed"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17333v1",
                "http://arxiv.org/pdf/2310.17333v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17330v1",
            "title": "CQM: Curriculum Reinforcement Learning with a Quantized World Model",
            "updated": "2023-10-26T11:50:58Z",
            "published": "2023-10-26T11:50:58Z",
            "summary": "Recent curriculum Reinforcement Learning (RL) has shown notable progress in\nsolving complex tasks by proposing sequences of surrogate tasks. However, the\nprevious approaches often face challenges when they generate curriculum goals\nin a high-dimensional space. Thus, they usually rely on manually specified goal\nspaces. To alleviate this limitation and improve the scalability of the\ncurriculum, we propose a novel curriculum method that automatically defines the\nsemantic goal space which contains vital information for the curriculum\nprocess, and suggests curriculum goals over it. To define the semantic goal\nspace, our method discretizes continuous observations via vector\nquantized-variational autoencoders (VQ-VAE) and restores the temporal relations\nbetween the discretized observations by a graph. Concurrently, ours suggests\nuncertainty and temporal distance-aware curriculum goals that converges to the\nfinal goals over the automatically composed goal space. We demonstrate that the\nproposed method allows efficient explorations in an uninformed environment with\nraw goal examples only. Also, ours outperforms the state-of-the-art curriculum\nRL methods on data efficiency and performance, in various goal-reaching tasks\neven with ego-centric visual inputs.",
            "author": [
                "Seungjae Lee",
                "Daesol Cho",
                "Jonghae Park",
                "H. Jin Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17330v1",
                "http://arxiv.org/pdf/2310.17330v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17327v1",
            "title": "Near-Field Positioning and Attitude Sensing Based on Electromagnetic\n  Propagation Modeling",
            "updated": "2023-10-26T11:47:26Z",
            "published": "2023-10-26T11:47:26Z",
            "summary": "Positioning and sensing over wireless networks are imperative for many\nemerging applications. However, traditional wireless channel models cannot be\nused for sensing the attitude of the user equipment (UE), since they\nover-simplify the UE as a point target. In this paper, a comprehensive\nelectromagnetic propagation modeling (EPM) based on electromagnetic theory is\ndeveloped to precisely model the near-field channel. For the noise-free case,\nthe EPM model establishes the non-linear functional dependence of observed\nsignals on both the position and attitude of the UE. To address the difficulty\nin the non-linear coupling, we first propose to divide the distance domain into\nthree regions, separated by the defined Phase ambiguity distance and Spacing\nconstraint distance. Then, for each region, we obtain the closed-form solutions\nfor joint position and attitude estimation with low complexity. Next, to\ninvestigate the impact of random noise on the joint estimation performance, the\nZiv-Zakai bound (ZZB) is derived to yield useful insights. The expected\nCram\\'er-Rao bound (ECRB) is further provided to obtain the simplified\nclosed-form expressions for the performance lower bounds. Our numerical results\ndemonstrate that the derived ZZB can provide accurate predictions of the\nperformance of estimators in all signal-to-noise ratio (SNR) regimes. More\nimportantly, we achieve the millimeter-level accuracy in position estimation\nand attain the 0.1-level accuracy in attitude estimation.",
            "author": [
                "Ang Chen",
                "Li Chen",
                "Yunfei Chen",
                "Nan Zhao",
                "Changsheng You"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17327v1",
                "http://arxiv.org/pdf/2310.17327v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17325v1",
            "title": "C-Disentanglement: Discovering Causally-Independent Generative Factors\n  under an Inductive Bias of Confounder",
            "updated": "2023-10-26T11:44:42Z",
            "published": "2023-10-26T11:44:42Z",
            "summary": "Representation learning assumes that real-world data is generated by a few\nsemantically meaningful generative factors (i.e., sources of variation) and\naims to discover them in the latent space. These factors are expected to be\ncausally disentangled, meaning that distinct factors are encoded into separate\nlatent variables, and changes in one factor will not affect the values of the\nothers. Compared to statistical independence, causal disentanglement allows\nmore controllable data generation, improved robustness, and better\ngeneralization. However, most existing work assumes unconfoundedness in the\ndiscovery process, that there are no common causes to the generative factors\nand thus obtain only statistical independence. In this paper, we recognize the\nimportance of modeling confounders in discovering causal generative factors.\nUnfortunately, such factors are not identifiable without proper inductive bias.\nWe fill the gap by introducing a framework entitled Confounded-Disentanglement\n(C-Disentanglement), the first framework that explicitly introduces the\ninductive bias of confounder via labels from domain expertise. In addition, we\naccordingly propose an approach to sufficiently identify the causally\ndisentangled factors under any inductive bias of the confounder. We conduct\nextensive experiments on both synthetic and real-world datasets. Our method\ndemonstrates competitive results compared to various SOTA baselines in\nobtaining causally disentangled features and downstream tasks under domain\nshifts.",
            "author": [
                "Xiaoyu Liu",
                "Jiaxin Yuan",
                "Bang An",
                "Yuancheng Xu",
                "Yifan Yang",
                "Furong Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17325v1",
                "http://arxiv.org/pdf/2310.17325v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17315v1",
            "title": "Nabra: Syrian Arabic Dialects with Morphological Annotations",
            "updated": "2023-10-26T11:23:05Z",
            "published": "2023-10-26T11:23:05Z",
            "summary": "This paper presents Nabra, a corpora of Syrian Arabic dialects with\nmorphological annotations. A team of Syrian natives collected more than 6K\nsentences containing about 60K words from several sources including social\nmedia posts, scripts of movies and series, lyrics of songs and local proverbs\nto build Nabra. Nabra covers several local Syrian dialects including those of\nAleppo, Damascus, Deir-ezzur, Hama, Homs, Huran, Latakia, Mardin, Raqqah, and\nSuwayda. A team of nine annotators annotated the 60K tokens with full\nmorphological annotations across sentence contexts. We trained the annotators\nto follow methodological annotation guidelines to ensure unique morpheme\nannotations, and normalized the annotations. F1 and kappa agreement scores\nranged between 74% and 98% across features, showing the excellent quality of\nNabra annotations. Our corpora are open-source and publicly available as part\nof the Currasat portal https://sina.birzeit.edu/currasat.",
            "author": [
                "Amal Nayouf",
                "Tymaa Hammouda",
                "Mustafa Jarrar",
                "Fadi Zaraket",
                "Mohamad-Bassam Kurdy"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17315v1",
                "http://arxiv.org/pdf/2310.17315v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17314v1",
            "title": "The covariant Langevin equation",
            "updated": "2023-10-26T11:21:59Z",
            "published": "2023-10-26T11:21:59Z",
            "summary": "The covariant form of the multivariable diffusion-drift process is described\nby the covariant Fokker--Planck equation using the standard toolbox of Riemann\ngeometry. The covariant form of the equivalent Langevin stochastic equation is\nlong sought after. We start from the simplest covariant Stratonovich stochastic\ndifferential equation depending on the local orthogonal frame (cf. vielbein).\nWe show that this stochastic differential equation (Graham, 1977) becomes the\ndesired covariant Langevin equation but only if we impose an additional\ncovariant constraint: the vectors of the frame must be divergence-free.",
            "author": [
                "Lajos Di\u00f3si"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17314v1",
                "http://arxiv.org/pdf/2310.17314v1"
            ],
            "primary_category": "cond-mat.stat-mech",
            "category": [
                "cond-mat.stat-mech",
                "gr-qc",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17312v1",
            "title": "An Ensemble Method Based on the Combination of Transformers with\n  Convolutional Neural Networks to Detect Artificially Generated Text",
            "updated": "2023-10-26T11:17:03Z",
            "published": "2023-10-26T11:17:03Z",
            "summary": "Thanks to the state-of-the-art Large Language Models (LLMs), language\ngeneration has reached outstanding levels. These models are capable of\ngenerating high quality content, thus making it a challenging task to detect\ngenerated text from human-written content. Despite the advantages provided by\nNatural Language Generation, the inability to distinguish automatically\ngenerated text can raise ethical concerns in terms of authenticity.\nConsequently, it is important to design and develop methodologies to detect\nartificial content. In our work, we present some classification models\nconstructed by ensembling transformer models such as Sci-BERT, DeBERTa and\nXLNet, with Convolutional Neural Networks (CNNs). Our experiments demonstrate\nthat the considered ensemble architectures surpass the performance of the\nindividual transformer models for classification. Furthermore, the proposed\nSciBERT-CNN ensemble model produced an F1-score of 98.36% on the ALTA shared\ntask 2023 data.",
            "author": [
                "Vijini Liyanage",
                "Davide Buscaldi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17312v1",
                "http://arxiv.org/pdf/2310.17312v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18371v1",
            "title": "In-Context Ability Transfer for Question Decomposition in Complex QA",
            "updated": "2023-10-26T11:11:07Z",
            "published": "2023-10-26T11:11:07Z",
            "summary": "Answering complex questions is a challenging task that requires question\ndecomposition and multistep reasoning for arriving at the solution. While\nexisting supervised and unsupervised approaches are specialized to a certain\ntask and involve training, recently proposed prompt-based approaches offer\ngeneralizable solutions to tackle a wide variety of complex question-answering\n(QA) tasks. However, existing prompt-based approaches that are effective for\ncomplex QA tasks involve expensive hand annotations from experts in the form of\nrationales and are not generalizable to newer complex QA scenarios and tasks.\nWe propose, icat (In-Context Ability Transfer) which induces reasoning\ncapabilities in LLMs without any LLM fine-tuning or manual annotation of\nin-context samples. We transfer the ability to decompose complex questions to\nsimpler questions or generate step-by-step rationales to LLMs, by careful\nselection from available data sources of related tasks. We also propose an\nautomated uncertainty-aware exemplar selection approach for selecting examples\nfrom transfer data sources. Finally, we conduct large-scale experiments on a\nvariety of complex QA tasks involving numerical reasoning, compositional\ncomplex QA, and heterogeneous complex QA which require decomposed reasoning. We\nshow that ICAT convincingly outperforms existing prompt-based solutions without\ninvolving any model training, showcasing the benefits of re-using existing\nabilities.",
            "author": [
                "Venktesh V",
                "Sourangshu Bhattacharya",
                "Avishek Anand"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18371v1",
                "http://arxiv.org/pdf/2310.18371v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17308v1",
            "title": "Wild Bootstrap for Counting Process-Based Statistics",
            "updated": "2023-10-26T11:07:24Z",
            "published": "2023-10-26T11:07:24Z",
            "summary": "The wild bootstrap is a popular resampling method in the context of\ntime-to-event data analyses. Previous works established the large sample\nproperties of it for applications to different estimators and test statistics.\nIt can be used to justify the accuracy of inference procedures such as\nhypothesis tests or time-simultaneous confidence bands. This paper consists of\ntwo parts: in Part~I, a general framework is developed in which the large\nsample properties are established in a unified way by using martingale\nstructures. The framework includes most of the well-known non- and\nsemiparametric statistical methods in time-to-event analysis and parametric\napproaches. In Part II, the Fine-Gray proportional sub-hazards model\nexemplifies the theory for inference on cumulative incidence functions given\nthe covariates. The model falls within the framework if the data are\ncensoring-complete. A simulation study demonstrates the reliability of the\nmethod and an application to a data set about hospital-acquired infections\nillustrates the statistical procedure.",
            "author": [
                "Marina T. Dietrich",
                "Dennis Dobler",
                "Mathisca C. M. de Gunst"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17308v1",
                "http://arxiv.org/pdf/2310.17308v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "math.ST",
                "stat.TH",
                "62G09, 62G20, 62N02, 62N03, 62F03, 62F12, 62F25, 62F40"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17680v3",
            "title": "CodeFusion: A Pre-trained Diffusion Model for Code Generation",
            "updated": "2023-11-01T17:30:47Z",
            "published": "2023-10-26T11:06:15Z",
            "summary": "Imagine a developer who can only change their last line of code, how often\nwould they have to start writing a function from scratch before it is correct?\nAuto-regressive models for code generation from natural language have a similar\nlimitation: they do not easily allow reconsidering earlier tokens generated. We\nintroduce CodeFusion, a pre-trained diffusion code generation model that\naddresses this limitation by iteratively denoising a complete program\nconditioned on the encoded natural language. We evaluate CodeFusion on the task\nof natural language to code generation for Bash, Python, and Microsoft Excel\nconditional formatting (CF) rules. Experiments show that CodeFusion (75M\nparameters) performs on par with state-of-the-art auto-regressive systems\n(350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and\ntop-5 accuracy due to its better balance in diversity versus quality.",
            "author": [
                "Mukul Singh",
                "Jos\u00e9 Cambronero",
                "Sumit Gulwani",
                "Vu Le",
                "Carina Negreanu",
                "Gust Verbruggen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17680v3",
                "http://arxiv.org/pdf/2310.17680v3"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.AI",
                "cs.CL",
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17306v3",
            "title": "FormaT5: Abstention and Examples for Conditional Table Formatting with\n  Natural Language",
            "updated": "2023-11-01T17:31:30Z",
            "published": "2023-10-26T11:05:15Z",
            "summary": "Formatting is an important property in tables for visualization,\npresentation, and analysis. Spreadsheet software allows users to automatically\nformat their tables by writing data-dependent conditional formatting (CF)\nrules. Writing such rules is often challenging for users as it requires them to\nunderstand and implement the underlying logic. We present FormaT5, a\ntransformer-based model that can generate a CF rule given the target table and\na natural language description of the desired formatting logic. We find that\nuser descriptions for these tasks are often under-specified or ambiguous,\nmaking it harder for code generation systems to accurately learn the desired\nrule in a single step. To tackle this problem of under-specification and\nminimise argument errors, FormaT5 learns to predict placeholders though an\nabstention objective. These placeholders can then be filled by a second model\nor, when examples of rows that should be formatted are available, by a\nprogramming-by-example system. To evaluate FormaT5 on diverse and real\nscenarios, we create an extensive benchmark of 1053 CF tasks, containing\nreal-world descriptions collected from four different sources. We release our\nbenchmarks to encourage research in this area. Abstention and filling allow\nFormaT5 to outperform 8 different neural approaches on our benchmarks, both\nwith and without examples. Our results illustrate the value of building\ndomain-specific learning systems.",
            "author": [
                "Mukul Singh",
                "Jos\u00e9 Cambronero",
                "Sumit Gulwani",
                "Vu Le",
                "Carina Negreanu",
                "Elnaz Nouri",
                "Mohammad Raza",
                "Gust Verbruggen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17306v3",
                "http://arxiv.org/pdf/2310.17306v3"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.DB",
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17305v1",
            "title": "Spontaneously sliding multipole spin density waves in cold atoms",
            "updated": "2023-10-26T11:00:04Z",
            "published": "2023-10-26T11:00:04Z",
            "summary": "We report on the observation of spontaneously drifting coupled spin and\nquadrupolar density waves in the ground state of laser driven Rubidium atoms.\nThese laser-cooled atomic ensembles exhibit spontaneous magnetism via light\nmediated interactions when submitted to optical feedback by a retro-reflecting\nmirror. Drift direction and chirality of the waves arise from spontaneous\nsymmetry breaking. The observations demonstrate a novel transport process in\nout-of-equilibrium magnetic systems.",
            "author": [
                "G. Labeyrie",
                "J. G. M. Walker",
                "G. R. M. Robb",
                "R. Kaiser",
                "T. Ackemann"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17305v1",
                "http://arxiv.org/pdf/2310.17305v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.stat-mech",
                "nlin.PS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17304v1",
            "title": "Static Semantics Reconstruction for Enhancing JavaScript-WebAssembly\n  Multilingual Malware Detection",
            "updated": "2023-10-26T10:59:45Z",
            "published": "2023-10-26T10:59:45Z",
            "summary": "The emergence of WebAssembly allows attackers to hide the malicious\nfunctionalities of JavaScript malware in cross-language interoperations, termed\nJavaScript-WebAssembly multilingual malware (JWMM). However, existing\nanti-virus solutions based on static program analysis are still limited to\nmonolingual code. As a result, their detection effectiveness decreases\nsignificantly against JWMM. The detection of JWMM is challenging due to the\ncomplex interoperations and semantic diversity between JavaScript and\nWebAssembly. To bridge this gap, we present JWBinder, the first technique aimed\nat enhancing the static detection of JWMM. JWBinder performs a\nlanguage-specific data-flow analysis to capture the cross-language\ninteroperations and then characterizes the functionalities of JWMM through a\nunified high-level structure called Inter-language Program Dependency Graph.\nThe extensive evaluation on one of the most representative real-world\nanti-virus platforms, VirusTotal, shows that \\system effectively enhances\nanti-virus systems from various vendors and increases the overall successful\ndetection rate against JWMM from 49.1\\% to 86.2\\%. Additionally, we assess the\nside effects and runtime overhead of JWBinder, corroborating its practical\nviability in real-world applications.",
            "author": [
                "Yifan Xia",
                "Ping He",
                "Xuhong Zhang",
                "Peiyu Liu",
                "Shouling Ji",
                "Wenhai Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17304v1",
                "http://arxiv.org/pdf/2310.17304v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17303v1",
            "title": "Demonstration-Regularized RL",
            "updated": "2023-10-26T10:54:47Z",
            "published": "2023-10-26T10:54:47Z",
            "summary": "Incorporating expert demonstrations has empirically helped to improve the\nsample efficiency of reinforcement learning (RL). This paper quantifies\ntheoretically to what extent this extra information reduces RL's sample\ncomplexity. In particular, we study the demonstration-regularized reinforcement\nlearning that leverages the expert demonstrations by KL-regularization for a\npolicy learned by behavior cloning. Our findings reveal that using\n$N^{\\mathrm{E}}$ expert demonstrations enables the identification of an optimal\npolicy at a sample complexity of order\n$\\widetilde{\\mathcal{O}}(\\mathrm{Poly}(S,A,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$\nin finite and $\\widetilde{\\mathcal{O}}(\\mathrm{Poly}(d,H)/(\\varepsilon^2\nN^{\\mathrm{E}}))$ in linear Markov decision processes, where $\\varepsilon$ is\nthe target precision, $H$ the horizon, $A$ the number of action, $S$ the number\nof states in the finite case and $d$ the dimension of the feature space in the\nlinear case. As a by-product, we provide tight convergence guarantees for the\nbehaviour cloning procedure under general assumptions on the policy classes.\nAdditionally, we establish that demonstration-regularized methods are provably\nefficient for reinforcement learning from human feedback (RLHF). In this\nrespect, we provide theoretical evidence showing the benefits of\nKL-regularization for RLHF in tabular and linear MDPs. Interestingly, we avoid\npessimism injection by employing computationally feasible regularization to\nhandle reward estimation uncertainty, thus setting our approach apart from the\nprior works.",
            "author": [
                "Daniil Tiapkin",
                "Denis Belomestny",
                "Daniele Calandriello",
                "Eric Moulines",
                "Alexey Naumov",
                "Pierre Perrault",
                "Michal Valko",
                "Pierre Menard"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17303v1",
                "http://arxiv.org/pdf/2310.17303v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17302v1",
            "title": "Mott transition and pseudogap of the square-lattice Hubbard model:\n  results from center-focused cellular dynamical mean-field theory",
            "updated": "2023-10-26T10:46:49Z",
            "published": "2023-10-26T10:46:49Z",
            "summary": "The recently proposed center-focused post-processing procedure [Phys. Rev.\nResearch 2, 033476 (2020)] of cellular dynamical mean-field theory suggests\nthat central sites of large impurity clusters are closer to the exact solution\nof the Hubbard model than the edge sites. In this paper, we systematically\ninvestigate results in the spirit of this center-focused scheme for several\ncluster sizes up to $8\\times 8$ in and out of particle-hole symmetry. First we\nanalyze the metal-insulator crossovers and transitions of the half-filled\nHubbard model on a simple square lattice. We find that the critical interaction\nof the crossover is reduced with increasing cluster sizes and the critical\ntemperature abruptly drops for the $4\\times 4$ cluster. Second, for this\ncluster size, we apply the center-focused scheme to a system with more\nrealistic tight-binding parameters, investigating its pseudogap regime as a\nfunction of temperature and doping, where we find doping dependent\nmetal-insulator crossovers, Lifshitz transitions and a strongly renormalized\nFermi-liquid regime. Additionally to diagnosing the real space origin of the\nsuppressed antinodal spectral weight in the pseudogap regime, we can infer\nhints towards underlying charge ordering tendencies.",
            "author": [
                "Michael Meixner",
                "Henri Menke",
                "Marcel Klett",
                "Sarah Heinzelmann",
                "Sabine Andergassen",
                "Philipp Hansmann",
                "Thomas Sch\u00e4fer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17302v1",
                "http://arxiv.org/pdf/2310.17302v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17300v1",
            "title": "Comparing Photorealistic and Animated Embodied Conversational Agents in\n  Serious Games: An Empirical Study on User Experience",
            "updated": "2023-10-26T10:45:26Z",
            "published": "2023-10-26T10:45:26Z",
            "summary": "Embodied conversational agents (ECAs) are paradigms of conversational user\ninterfaces in the form of embodied characters. While ECAs offer various\nmanipulable features, this paper focuses on a study conducted to explore two\ndistinct levels of presentation realism. The two agent versions are\nphotorealistic and animated. The study aims to provide insights and design\nsuggestions for speech-enabled ECAs within serious game environments. A\nwithin-subjects, two-by-two factorial design was employed for this research\nwith a cohort of 36 participants balanced for gender. The results showed that\nboth the photorealistic and the animated versions were perceived as highly\nusable, with overall mean scores of 5.76 and 5.71, respectively. However, 69.4\nper cent of the participants stated they preferred the photorealistic version,\n25 per cent stated they preferred the animated version and 5.6 per cent had no\nstated preference. The photorealistic agents were perceived as more realistic\nand human-like, while the animated characters made the task feel more like a\ngame. Even though the agents' realism had no significant effect on usability,\nit positively influenced participants' perceptions of the agent. This research\naims to lay the groundwork for future studies on ECA realism's impact in\nserious games across diverse contexts.",
            "author": [
                "Danai Korre"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17300v1",
                "http://arxiv.org/pdf/2310.17300v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI",
                "cs.CL",
                "cs.MM",
                "I.2.1; H.5.2; K.3.1"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17295v1",
            "title": "Normal Forms for Elements of the ${}^*$-Continuous Kleene Algebras\n  $K\\mathop{\\otimes_{\\cal R}} C_2'$",
            "updated": "2023-10-26T10:23:02Z",
            "published": "2023-10-26T10:23:02Z",
            "summary": "The tensor product $K \\mathop{\\otimes_{\\cal R}} C_2'$ of the\n${}^*$-continuous Kleene algebra $K$ with the polycyclic ${}^*$-continuous\nKleene algebra $C_2'$ over two bracket pairs contains a copy of the fixed-point\nclosure of $K$: the centralizer of $C_2'$ in $K \\mathop{\\otimes_{\\cal R}}\nC_2'$. We prove a representation of elements of $K\\mathop{\\otimes_{\\cal R}}\nC_2'$ by automata \\`a la Kleene and refine it by normal form theorems that\nrestrict the occurrences of brackets on paths through the automata. This is a\nfoundation for a calculus of context-free expressions. We also show that $C_2'$\nvalidates a relativized form of the ``completeness property'' that\ndistinguishes the bra-ket ${}^*$-continuous Kleene algebra $C_2$ from the\npolycyclic one.",
            "author": [
                "Mark Hopkins",
                "Hans Lei\u00df"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17295v1",
                "http://arxiv.org/pdf/2310.17295v1"
            ],
            "primary_category": "cs.FL",
            "category": [
                "cs.FL",
                "F.4.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17294v3",
            "title": "Scale-Adaptive Feature Aggregation for Efficient Space-Time Video\n  Super-Resolution",
            "updated": "2023-11-27T06:21:03Z",
            "published": "2023-10-26T10:18:51Z",
            "summary": "The Space-Time Video Super-Resolution (STVSR) task aims to enhance the visual\nquality of videos, by simultaneously performing video frame interpolation (VFI)\nand video super-resolution (VSR). However, facing the challenge of the\nadditional temporal dimension and scale inconsistency, most existing STVSR\nmethods are complex and inflexible in dynamically modeling different motion\namplitudes. In this work, we find that choosing an appropriate processing scale\nachieves remarkable benefits in flow-based feature propagation. We propose a\nnovel Scale-Adaptive Feature Aggregation (SAFA) network that adaptively selects\nsub-networks with different processing scales for individual samples.\nExperiments on four public STVSR benchmarks demonstrate that SAFA achieves\nstate-of-the-art performance. Our SAFA network outperforms recent\nstate-of-the-art methods such as TMNet and VideoINR by an average improvement\nof over 0.5dB on PSNR, while requiring less than half the number of parameters\nand only 1/3 computational costs.",
            "author": [
                "Zhewei Huang",
                "Ailin Huang",
                "Xiaotao Hu",
                "Chen Hu",
                "Jun Xu",
                "Shuchang Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17294v3",
                "http://arxiv.org/pdf/2310.17294v3"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17290v1",
            "title": "RIO: A Benchmark for Reasoning Intention-Oriented Objects in Open\n  Environments",
            "updated": "2023-10-26T10:15:21Z",
            "published": "2023-10-26T10:15:21Z",
            "summary": "Intention-oriented object detection aims to detect desired objects based on\nspecific intentions or requirements. For instance, when we desire to \"lie down\nand rest\", we instinctively seek out a suitable option such as a \"bed\" or a\n\"sofa\" that can fulfill our needs. Previous work in this area is limited either\nby the number of intention descriptions or by the affordance vocabulary\navailable for intention objects. These limitations make it challenging to\nhandle intentions in open environments effectively. To facilitate this\nresearch, we construct a comprehensive dataset called Reasoning\nIntention-Oriented Objects (RIO). In particular, RIO is specifically designed\nto incorporate diverse real-world scenarios and a wide range of object\ncategories. It offers the following key features: 1) intention descriptions in\nRIO are represented as natural sentences rather than a mere word or verb\nphrase, making them more practical and meaningful; 2) the intention\ndescriptions are contextually relevant to the scene, enabling a broader range\nof potential functionalities associated with the objects; 3) the dataset\ncomprises a total of 40,214 images and 130,585 intention-object pairs. With the\nproposed RIO, we evaluate the ability of some existing models to reason\nintention-oriented objects in open environments.",
            "author": [
                "Mengxue Qu",
                "Yu Wu",
                "Wu Liu",
                "Xiaodan Liang",
                "Jingkuan Song",
                "Yao Zhao",
                "Yunchao Wei"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17290v1",
                "http://arxiv.org/pdf/2310.17290v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17287v1",
            "title": "Ready for O4 II: GRANDMA Observations of Swift GRBs during eight-weeks\n  of Spring 2022",
            "updated": "2023-10-26T10:08:14Z",
            "published": "2023-10-26T10:08:14Z",
            "summary": "We present a campaign designed to train the GRANDMA network and its\ninfrastructure to follow up on transient alerts and detect their early\nafterglows. In preparation for O4 II campaign, we focused on GRB alerts as they\nare expected to be an electromagnetic counterpart of gravitational-wave events.\nOur goal was to improve our response to the alerts and start prompt\nobservations as soon as possible to better prepare the GRANDMA network for the\nfourth observational run of LIGO-Virgo-Kagra (which started at the end of May\n2023), and future missions such as SM. To receive, manage and send out\nobservational plans to our partner telescopes we set up dedicated\ninfrastructure and a rota of follow-up adcates were organized to guarantee\nround-the-clock assistance to our telescope teams. To ensure a great number of\nobservations, we focused on Swift GRBs whose localization errors were generally\nsmaller than the GRANDMA telescopes' field of view. This allowed us to bypass\nthe transient identification process and focus on the reaction time and\nefficiency of the network. During 'Ready for O4 II', 11 Swift/INTEGRAL GRB\ntriggers were selected, nine fields had been observed, and three afterglows\nwere detected (GRB 220403B, GRB 220427A, GRB 220514A), with 17 GRANDMA\ntelescopes and 17 amateur astronomers from the citizen science project\nKilonova-Catcher. Here we highlight the GRB 220427A analysis where our\nlong-term follow-up of the host galaxy allowed us to obtain a photometric\nredshift of $z=0.82\\pm0.09$, its lightcurve elution, fit the decay slope of the\nafterglows, and study the properties of the host galaxy.",
            "author": [
                "I. Tosta e Melo",
                "J. -G. Ducoin",
                "Z. Vidadi",
                "C. Andrade",
                "V. Rupchandani",
                "S. Agayeva",
                "J. Abdelhadi",
                "L. Abe",
                "O. Aguerre-Chariol",
                "V. Aivazyan",
                "S. Alishov",
                "S. Antier",
                "J. -M. Bai",
                "A. Baransky",
                "S. Bednarz",
                "Ph. Bendjoya",
                "Z. Benkhaldoun",
                "S. Beradze",
                "M. A. Bizouard",
                "U. Bhardwaj",
                "M. Blazek",
                "M. Bo\u00ebr",
                "E. Broens",
                "O. Burkhonov",
                "N. Christensen",
                "J. Cooke",
                "W. Corradi",
                "M. W. Coughlin",
                "T. Culino",
                "F. Daigne",
                "D. Dornic",
                "P. -A. Duverne",
                "S. Ehgamberdiev",
                "L. Eymar",
                "A. Fouad",
                "M. Freeberg",
                "B. Gendre",
                "F. Guo",
                "P. Gokuldass",
                "N. Guessoum",
                "E. Gurbanov",
                "R. Hainich",
                "E. Hasanov",
                "P. Hello",
                "R. Inasaridze",
                "A. Iskandar",
                "N. Ismailov",
                "A. Janati",
                "T. Jegou du Laz",
                "D. A. Kann",
                "S. Karpov",
                "R. W. Kiendrebeogo",
                "A. Klotz",
                "R. Kneip",
                "N. Kochiashvili",
                "A. Kaeouach",
                "K. Kruiswijk",
                "M. Lamoureux",
                "N. Leroy",
                "W. L. Lin",
                "J. Mao",
                "D. Marchais",
                "M. Ma\u0161ek",
                "T. Midavaine",
                "A. Moller",
                "D. Morris",
                "R. Natsvlishvili",
                "F. Navarete",
                "A Nicuesa Guelbenzu",
                "K. Noonan",
                "K. Noysena",
                "A. Oksanen",
                "N. B. Orange",
                "C. Pellouin",
                "J. Peloton",
                "H. W. Peng",
                "M. Pilloix",
                "A. Popowicz",
                "T. Pradier",
                "O. Pyshna",
                "G. Raaijmakers",
                "Y. Rajabov",
                "A. Rau",
                "C. Rinner",
                "J. -P. Rivet",
                "A. S. Ryh",
                "M. Sabil",
                "T. Sadibekova",
                "N. Sasaki",
                "M. Serrau",
                "A. Simon",
                "A. Shokry",
                "K. Smith",
                "O. Sokoliuk",
                "X. Song",
                "A. Takey",
                "P. Thierry",
                "Y. Tillayev",
                "D. Turpin",
                "A. de Ugarte Postigo",
                "V. Vasylenko",
                "D. Vernet",
                "L. Wang",
                "F. Vachier",
                "J. P. Vignes",
                "X. F. Wang",
                "X. Zeng",
                "J. Zhang",
                "Y. Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17287v1",
                "http://arxiv.org/pdf/2310.17287v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17284v1",
            "title": "Learning to Abstract with Nonparametric Variational Information\n  Bottleneck",
            "updated": "2023-10-26T10:04:31Z",
            "published": "2023-10-26T10:04:31Z",
            "summary": "Learned representations at the level of characters, sub-words, words and\nsentences, have each contributed to advances in understanding different NLP\ntasks and linguistic phenomena. However, learning textual embeddings is costly\nas they are tokenization specific and require different models to be trained\nfor each level of abstraction. We introduce a novel language representation\nmodel which can learn to compress to different levels of abstraction at\ndifferent layers of the same model. We apply Nonparametric Variational\nInformation Bottleneck (NVIB) to stacked Transformer self-attention layers in\nthe encoder, which encourages an information-theoretic compression of the\nrepresentations through the model. We find that the layers within the model\ncorrespond to increasing levels of abstraction and that their representations\nare more linguistically informed. Finally, we show that NVIB compression\nresults in a model which is more robust to adversarial perturbations.",
            "author": [
                "Melika Behjati",
                "Fabio Fehr",
                "James Henderson"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17284v1",
                "http://arxiv.org/pdf/2310.17284v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17679v1",
            "title": "Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score\n  Search and Grow-Shrink Trees",
            "updated": "2023-10-26T10:03:12Z",
            "published": "2023-10-26T10:03:12Z",
            "summary": "Learning graphical conditional independence structures is an important\nmachine learning problem and a cornerstone of causal discovery. However, the\naccuracy and execution time of learning algorithms generally struggle to scale\nto problems with hundreds of highly connected variables -- for instance,\nrecovering brain networks from fMRI data. We introduce the best order score\nsearch (BOSS) and grow-shrink trees (GSTs) for learning directed acyclic graphs\n(DAGs) in this paradigm. BOSS greedily searches over permutations of variables,\nusing GSTs to construct and score DAGs from permutations. GSTs efficiently\ncache scores to eliminate redundant calculations. BOSS achieves\nstate-of-the-art performance in accuracy and execution time, comparing\nfavorably to a variety of combinatorial and gradient-based learning algorithms\nunder a broad range of conditions. To demonstrate its practicality, we apply\nBOSS to two sets of resting-state fMRI data: simulated data with\npseudo-empirical noise distributions derived from randomized empirical fMRI\ncortical signals and clinical data from 3T fMRI scans processed into cortical\nparcels. BOSS is available for use within the TETRAD project which includes\nPython and R wrappers.",
            "author": [
                "Bryan Andrews",
                "Joseph Ramsey",
                "Ruben Sanchez-Romero",
                "Jazmin Camchong",
                "Erich Kummerfeld"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17679v1",
                "http://arxiv.org/pdf/2310.17679v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17281v1",
            "title": "BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point\n  Clouds",
            "updated": "2023-10-26T10:02:33Z",
            "published": "2023-10-26T10:02:33Z",
            "summary": "We present a surprisingly simple and efficient method for self-supervision of\n3D backbone on automotive Lidar point clouds. We design a contrastive loss\nbetween features of Lidar scans captured in the same scene. Several such\napproaches have been proposed in the literature from PointConstrast, which uses\na contrast at the level of points, to the state-of-the-art TARL, which uses a\ncontrast at the level of segments, roughly corresponding to objects. While the\nformer enjoys a great simplicity of implementation, it is surpassed by the\nlatter, which however requires a costly pre-processing. In BEVContrast, we\ndefine our contrast at the level of 2D cells in the Bird's Eye View plane.\nResulting cell-level representations offer a good trade-off between the\npoint-level representations exploited in PointContrast and segment-level\nrepresentations exploited in TARL: we retain the simplicity of PointContrast\n(cell representations are cheap to compute) while surpassing the performance of\nTARL in downstream semantic segmentation.",
            "author": [
                "Corentin Sautier",
                "Gilles Puy",
                "Alexandre Boulch",
                "Renaud Marlet",
                "Vincent Lepetit"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17281v1",
                "http://arxiv.org/pdf/2310.17281v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17279v1",
            "title": "Automatic Logical Forms improve fidelity in Table-to-Text generation",
            "updated": "2023-10-26T10:00:24Z",
            "published": "2023-10-26T10:00:24Z",
            "summary": "Table-to-text systems generate natural language statements from structured\ndata like tables. While end-to-end techniques suffer from low factual\ncorrectness (fidelity), a previous study reported gains when using manual\nlogical forms (LF) that represent the selected content and the semantics of the\ntarget text. Given the manual step, it was not clear whether automatic LFs\nwould be effective, or whether the improvement came from content selection\nalone. We present TlT which, given a table and a selection of the content,\nfirst produces LFs and then the textual statement. We show for the first time\nthat automatic LFs improve quality, with an increase in fidelity of 30 points\nover a comparable system not using LFs. Our experiments allow to quantify the\nremaining challenges for high factual correctness, with automatic selection of\ncontent coming first, followed by better Logic-to-Text generation and, to a\nlesser extent, better Table-to-Logic parsing.",
            "author": [
                "I\u00f1igo Alonso",
                "Eneko Agirre"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.eswa.2023.121869",
                "http://arxiv.org/abs/2310.17279v1",
                "http://arxiv.org/pdf/2310.17279v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17277v1",
            "title": "Risk-sensitive control, single controller games and linear programming",
            "updated": "2023-10-26T09:59:21Z",
            "published": "2023-10-26T09:59:21Z",
            "summary": "This article recalls the recent work on a linear programming formulation of\ninfinite horizon risk-sensitive control via its equivalence with a single\ncontroller game, using a classic work of Vrieze. This is then applied to a\nconstrained risk-sensitive control problem with a risk-sensitive cost and\nrisk-sensitive constraint. This facilitates a Lagrange multiplier based\nresolution thereof. In the process, this leads to an unconstrained linear\nprogram and its dual, parametrized by a parameter that is a surrogate for\nLagrange multiplier. This also opens up the possibility of a primal - dual type\nnumerical scheme wherein the linear program is a subroutine within the\nsubgradient ascent based update rule for the Lagrange multiplier. This\nequivalent unconstrained risk-sensitive control formulation does not seem\nobvious without the linear programming equivalents as intermediaries. We also\ndiscuss briefly other related algorithmic possibilities for future research.",
            "author": [
                "Vivek Shripad Borkar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17277v1",
                "http://arxiv.org/pdf/2310.17277v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "93E20"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17275v1",
            "title": "NIGHT: a compact, near-infrared, high-resolution spectrograph to survey\n  helium in exoplanet systems",
            "updated": "2023-10-26T09:53:07Z",
            "published": "2023-10-26T09:53:07Z",
            "summary": "Among highly irradiated exoplanets, some have been found to undergo\nsignificant hydrodynamic expansion traced by atmospheric escape. To better\nunderstand these processes in the context of planetary evolution, we propose\nNIGHT (the Near-Infrared Gatherer of Helium Transits). NIGHT is a\nhigh-resolution spectrograph dedicated to surveying and temporally monitoring\nHe I triplet absorption at 1083nm in stellar and planetary atmospheres. In this\npaper, we outline our scientific objectives, requirements, and cost-efficient\ndesign. Our simulations, based on previous detections and modelling using the\ncurrent exoplanet population, determine our requirements and survey targets.\nWith a spectral resolution of 70,000 on a 2-meter telescope, NIGHT can\naccurately resolve the helium triplet and detect 1% peak absorption in 118\nknown exoplanets in a single transit. Additionally, it can search for\nthree-sigma temporal variations of 0.4% in 66 exoplanets in-between two\ntransits. These are conservative estimates considering the ongoing detections\nof transiting planets amenable to atmospheric characterisation. We find that\ninstrumental stability at 40m/s, less stringent than for radial velocity\nmonitoring, is sufficient for transmission spectroscopy in He I. As such, NIGHT\ncan utilize mostly off-the-shelf components, ensuring cost-efficiency. A\nfibre-fed system allows for flexibility as a visitor instrument on a variety of\ntelescopes, making it ideal for follow-up observations after JWST or\nground-based detections. Over a few years of surveying, NIGHT could offer\ndetailed insights into the mechanisms shaping the hot Neptune desert and\nclose-in planet population by significantly expanding the statistical sample of\nplanets with known evaporating atmospheres. First light is expected in 2024.",
            "author": [
                "C. Farret Jentink",
                "V. Bourrier",
                "C. Lovis",
                "R. Allart",
                "B. Chazelas",
                "M. Lendl",
                "X. Dumusque",
                "F. Pepe"
            ],
            "link": [
                "http://dx.doi.org/10.1093/mnras/stad3285",
                "http://arxiv.org/abs/2310.17275v1",
                "http://arxiv.org/pdf/2310.17275v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP",
                "astro-ph.IM",
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17273v3",
            "title": "Looping in the Human: Collaborative and Explainable Bayesian\n  Optimization",
            "updated": "2023-11-06T19:25:58Z",
            "published": "2023-10-26T09:50:31Z",
            "summary": "Like many optimizers, Bayesian optimization often falls short of gaining user\ntrust due to opacity. While attempts have been made to develop human-centric\noptimizers, they typically assume user knowledge is well-specified and\nerror-free, employing users mainly as supervisors of the optimization process.\nWe relax these assumptions and propose a more balanced human-AI partnership\nwith our Collaborative and Explainable Bayesian Optimization (CoExBO)\nframework. Instead of explicitly requiring a user to provide a knowledge model,\nCoExBO employs preference learning to seamlessly integrate human insights into\nthe optimization, resulting in algorithmic suggestions that resonate with user\npreference. CoExBO explains its candidate selection every iteration to foster\ntrust, empowering users with a clearer grasp of the optimization. Furthermore,\nCoExBO offers a no-harm guarantee, allowing users to make mistakes; even with\nextreme adversarial interventions, the algorithm converges asymptotically to a\nvanilla Bayesian optimization. We validate CoExBO's efficacy through human-AI\nteaming experiments in lithium-ion battery design, highlighting substantial\nimprovements over conventional methods.",
            "author": [
                "Masaki Adachi",
                "Brady Planden",
                "David A. Howey",
                "Krikamol Muandet",
                "Michael A. Osborne",
                "Siu Lun Chau"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17273v3",
                "http://arxiv.org/pdf/2310.17273v3"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.HC",
                "stat.ML",
                "62C10, 62F15"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17271v1",
            "title": "Understanding the Role of Input Token Characters in Language Models: How\n  Does Information Loss Affect Performance?",
            "updated": "2023-10-26T09:47:50Z",
            "published": "2023-10-26T09:47:50Z",
            "summary": "Understanding how and what pre-trained language models (PLMs) learn about\nlanguage is an open challenge in natural language processing. Previous work has\nfocused on identifying whether they capture semantic and syntactic information,\nand how the data or the pre-training objective affects their performance.\nHowever, to the best of our knowledge, no previous work has specifically\nexamined how information loss in input token characters affects the performance\nof PLMs. In this study, we address this gap by pre-training language models\nusing small subsets of characters from individual tokens. Surprisingly, we find\nthat pre-training even under extreme settings, i.e. using only one character of\neach token, the performance retention in standard NLU benchmarks and probing\ntasks compared to full-token models is high. For instance, a model pre-trained\nonly on single first characters from tokens achieves performance retention of\napproximately $90$\\% and $77$\\% of the full-token model in SuperGLUE and GLUE\ntasks, respectively.",
            "author": [
                "Ahmed Alajrami",
                "Katerina Margatina",
                "Nikolaos Aletras"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17271v1",
                "http://arxiv.org/pdf/2310.17271v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17264v1",
            "title": "Variance of ML-based software fault predictors: are we really improving\n  fault prediction?",
            "updated": "2023-10-26T09:31:32Z",
            "published": "2023-10-26T09:31:32Z",
            "summary": "Software quality assurance activities become increasingly difficult as\nsoftware systems become more and more complex and continuously grow in size.\nMoreover, testing becomes even more expensive when dealing with large-scale\nsystems. Thus, to effectively allocate quality assurance resources, researchers\nhave proposed fault prediction (FP) which utilizes machine learning (ML) to\npredict fault-prone code areas. However, ML algorithms typically make use of\nstochastic elements to increase the prediction models' generalizability and\nefficiency of the training process. These stochastic elements, also known as\nnondeterminism-introducing (NI) factors, lead to variance in the training\nprocess and as a result, lead to variance in prediction accuracy and training\ntime. This variance poses a challenge for reproducibility in research. More\nimportantly, while fault prediction models may have shown good performance in\nthe lab (e.g., often-times involving multiple runs and averaging outcomes),\nhigh variance of results can pose the risk that these models show low\nperformance when applied in practice. In this work, we experimentally analyze\nthe variance of a state-of-the-art fault prediction approach. Our experimental\nresults indicate that NI factors can indeed cause considerable variance in the\nfault prediction models' accuracy. We observed a maximum variance of 10.10% in\nterms of the per-class accuracy metric. We thus, also discuss how to deal with\nsuch variance.",
            "author": [
                "Xhulja Shahini",
                "Domenic Bubel",
                "Andreas Metzger"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17264v1",
                "http://arxiv.org/pdf/2310.17264v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17259v1",
            "title": "O-band QKD link over a multiple ONT loaded carrier-grade GPON for FTTH\n  applications",
            "updated": "2023-10-26T09:18:02Z",
            "published": "2023-10-26T09:18:02Z",
            "summary": "We have successfully integrated an O-band commercial Quantum-Key-Distribution\n(QKD) system over a lit GPON testbed that replicates a carrier-grade\nFiber-to-the-Home (FTTH) optical access network with multiple ONTs to emulate\nreal-life FTTH operational deployments.",
            "author": [
                "N. Makris",
                "A. Ntanos",
                "A. Papageorgopoulos",
                "A. Stathis",
                "P. Konteli",
                "I. Tsoni",
                "G. Giannoulis",
                "F. Setaki",
                "T. Stathopoulos",
                "G. Lyberopoulos",
                "H. Avramopoulos",
                "G. T. Kanellos",
                "D. Syvridis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17259v1",
                "http://arxiv.org/pdf/2310.17259v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17255v2",
            "title": "Generalizing to Unseen Domains in Diabetic Retinopathy Classification",
            "updated": "2023-10-27T04:34:33Z",
            "published": "2023-10-26T09:11:55Z",
            "summary": "Diabetic retinopathy (DR) is caused by long-standing diabetes and is among\nthe fifth leading cause for visual impairments. The process of early diagnosis\nand treatments could be helpful in curing the disease, however, the detection\nprocedure is rather challenging and mostly tedious. Therefore, automated\ndiabetic retinopathy classification using deep learning techniques has gained\ninterest in the medical imaging community. Akin to several other real-world\napplications of deep learning, the typical assumption of i.i.d data is also\nviolated in DR classification that relies on deep learning. Therefore,\ndeveloping DR classification methods robust to unseen distributions is of great\nvalue. In this paper, we study the problem of generalizing a model to unseen\ndistributions or domains (a.k.a domain generalization) in DR classification. To\nthis end, we propose a simple and effective domain generalization (DG) approach\nthat achieves self-distillation in vision transformers (ViT) via a novel\nprediction softening mechanism. This prediction softening is an adaptive convex\ncombination one-hot labels with the model's own knowledge. We perform extensive\nexperiments on challenging open-source DR classification datasets under both\nmulti-source and single-source DG settings with three different ViT backbones\nto establish the efficacy and applicability of our approach against competing\nmethods. For the first time, we report the performance of several\nstate-of-the-art DG methods on open-source DR classification datasets after\nconducting thorough experiments. Finally, our method is also capable of\ndelivering improved calibration performance than other methods, showing its\nsuitability for safety-critical applications, including healthcare. We hope\nthat our contributions would investigate more DG research across the medical\nimaging community.",
            "author": [
                "Chamuditha Jayanga Galappaththige",
                "Gayal Kuruppu",
                "Muhammad Haris Khan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17255v2",
                "http://arxiv.org/pdf/2310.17255v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17247v1",
            "title": "Grokking Beyond Neural Networks: An Empirical Exploration with Model\n  Complexity",
            "updated": "2023-10-26T08:47:42Z",
            "published": "2023-10-26T08:47:42Z",
            "summary": "In some settings neural networks exhibit a phenomenon known as grokking,\nwhere they achieve perfect or near-perfect accuracy on the validation set long\nafter the same performance has been achieved on the training set. In this\npaper, we discover that grokking is not limited to neural networks but occurs\nin other settings such as Gaussian process (GP) classification, GP regression\nand linear regression. We also uncover a mechanism by which to induce grokking\non algorithmic datasets via the addition of dimensions containing spurious\ninformation. The presence of the phenomenon in non-neural architectures\nprovides evidence that grokking is not specific to SGD or weight norm\nregularisation. Instead, grokking may be possible in any setting where solution\nsearch is guided by complexity and error. Based on this insight and further\ntrends we see in the training trajectories of a Bayesian neural network (BNN)\nand GP regression model, we make progress towards a more general theory of\ngrokking. Specifically, we hypothesise that the phenomenon is governed by the\naccessibility of certain regions in the error and complexity landscapes.",
            "author": [
                "Jack Miller",
                "Charles O'Neill",
                "Thang Bui"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17247v1",
                "http://arxiv.org/pdf/2310.17247v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17245v1",
            "title": "CROP: Conservative Reward for Model-based Offline Policy Optimization",
            "updated": "2023-10-26T08:45:23Z",
            "published": "2023-10-26T08:45:23Z",
            "summary": "Offline reinforcement learning (RL) aims to optimize policy using collected\ndata without online interactions. Model-based approaches are particularly\nappealing for addressing offline RL challenges due to their capability to\nmitigate the limitations of offline data through data generation using models.\nPrior research has demonstrated that introducing conservatism into the model or\nQ-function during policy optimization can effectively alleviate the prevalent\ndistribution drift problem in offline RL. However, the investigation into the\nimpacts of conservatism in reward estimation is still lacking. This paper\nproposes a novel model-based offline RL algorithm, Conservative Reward for\nmodel-based Offline Policy optimization (CROP), which conservatively estimates\nthe reward in model training. To achieve a conservative reward estimation, CROP\nsimultaneously minimizes the estimation error and the reward of random actions.\nTheoretical analysis shows that this conservative reward mechanism leads to a\nconservative policy evaluation and helps mitigate distribution drift.\nExperiments on D4RL benchmarks showcase that the performance of CROP is\ncomparable to the state-of-the-art baselines. Notably, CROP establishes an\ninnovative connection between offline and online RL, highlighting that offline\nRL problems can be tackled by adopting online RL techniques to the empirical\nMarkov decision process trained with a conservative reward. The source code is\navailable with https://github.com/G0K0URURI/CROP.git.",
            "author": [
                "Hao Li",
                "Xiao-Hu Zhou",
                "Xiao-Liang Xie",
                "Shi-Qi Liu",
                "Zhen-Qiu Feng",
                "Xiao-Yin Liu",
                "Mei-Jiang Gui",
                "Tian-Yu Xiang",
                "De-Xing Huang",
                "Bo-Xian Yao",
                "Zeng-Guang Hou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17245v1",
                "http://arxiv.org/pdf/2310.17245v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17242v2",
            "title": "Slow and Non-Equilibrium Dynamics due to Electronic Ferroelectricity in\n  a Strongly-Correlated Molecular Conductor",
            "updated": "2023-11-03T08:56:45Z",
            "published": "2023-10-26T08:39:38Z",
            "summary": "Using a combination of resistance fluctuation (noise) and dielectric\nspectroscopy we investigate the nature of relaxor-type electronic\nferroelectricity in the organic conductor $\\kappa$-(BETS)$_2$Mn[N(CN)$_2$]$_3$,\na system representative for a wider class of materials, where strong\ncorrelations of electrons on a lattice of dimerized molecules results in an\ninsulating ground state. The two complementary spectroscopies reveal a distinct\nlow-frequency dynamics. By dielectric spectroscopy we detect an intrinsic\nrelaxation that is typical for relaxor ferroelectrics below the\nmetal-to-insulator transition at $T_{\\rm{MI}}\\sim 25\\,$K. Resistance noise\nspectroscopy reveals fluctuating two-level processes above $T_{\\rm MI}$ which\nstrongly couple to the applied electric field, a signature of fluctuating polar\nnanoregions (PNR), i.e. clusters of quantum electric dipoles fluctuating\ncollectively. The PNR preform above the metal insulator transition. Upon\ncooling through $T_{\\rm MI}$, a drastic increase of the low-frequency\n$1/f$-type fluctuations and slowing down of the charge carrier dynamics is\naccompanied by the onset of strong non-equilibrium dynamics indicating a glassy\ntransition of interacting dipolar clusters, the scaling properties of which are\nconsistent with a droplet model. The freezing of nano-scale polar clusters and\nnon-equilibrium dynamics is suggested to be a common feature of organic\nrelaxor-type electronic ferroelectrics and needs to be considered in\ntheoretical models describing these materials.",
            "author": [
                "Tatjana Thomas",
                "Yassine Agarmani",
                "Steffi Hartmann",
                "Mark Kartsovnik",
                "Natalia Kushch",
                "Stephen M. Winter",
                "Sebastian Schmid",
                "Peter Lunkenheimer",
                "Michael Lang",
                "Jens Mueller"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17242v2",
                "http://arxiv.org/pdf/2310.17242v2"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17241v1",
            "title": "Various questions around finitely positively expansive dynamical systems",
            "updated": "2023-10-26T08:39:06Z",
            "published": "2023-10-26T08:39:06Z",
            "summary": "It is well-known that when a positively expansive dynamical system is\ninvertible then its underlying space is finite. C.Morales has introduced a\ndecade ago a natural way to generalize positive expansiveness, by introducing\nother properties that he called positive $n$-expansiveness, for all $n \\ge 1$,\npositive $1$-expansiveness being identical to positive expansiveness. Contrary\nto positive expansiveness, positive $n$-expansiveness for $n>1$ does not\nenforce that the space is finite when the system is invertible. In the present\npaper we call finitely positively expansive dynamical systems as the ones which\nare positively $n$-expansive for some integer $n$, and prove several results on\nthis class of systems. In particular, the well-known result quoted above is\ntrue if we add the constraint of shadowing property, while it is not if this\nproperty is replaced with minimality. Furthermore, finitely positively\nexpansive systems cannot occur on certain topological spaces such as the\ninterval, when the system is assumed to be invertible finite positive\nexpansiveness implies zero topological entropy. Overall we show that the class\nof finitely positively expansive dynamical systems is quite rich and leave\nseveral questions open for further research.",
            "author": [
                "Silv\u00e8re Gangloff",
                "Pierre Guillon",
                "Piotr Oprocha"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17241v1",
                "http://arxiv.org/pdf/2310.17241v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17238v1",
            "title": "Joint Entity and Relation Extraction with Span Pruning and Hypergraph\n  Neural Networks",
            "updated": "2023-10-26T08:36:39Z",
            "published": "2023-10-26T08:36:39Z",
            "summary": "Entity and Relation Extraction (ERE) is an important task in information\nextraction. Recent marker-based pipeline models achieve state-of-the-art\nperformance, but still suffer from the error propagation issue. Also, most of\ncurrent ERE models do not take into account higher-order interactions between\nmultiple entities and relations, while higher-order modeling could be\nbeneficial.In this work, we propose HyperGraph neural network for ERE\n($\\hgnn{}$), which is built upon the PL-marker (a state-of-the-art marker-based\npipleline model). To alleviate error propagation,we use a high-recall pruner\nmechanism to transfer the burden of entity identification and labeling from the\nNER module to the joint module of our model. For higher-order modeling, we\nbuild a hypergraph, where nodes are entities (provided by the span pruner) and\nrelations thereof, and hyperedges encode interactions between two different\nrelations or between a relation and its associated subject and object entities.\nWe then run a hypergraph neural network for higher-order inference by applying\nmessage passing over the built hypergraph. Experiments on three widely used\nbenchmarks (\\acef{}, \\ace{} and \\scierc{}) for ERE task show significant\nimprovements over the previous state-of-the-art PL-marker.",
            "author": [
                "Zhaohui Yan",
                "Songlin Yang",
                "Wei Liu",
                "Kewei Tu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17238v1",
                "http://arxiv.org/pdf/2310.17238v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17233v1",
            "title": "EMMA-X: An EM-like Multilingual Pre-training Algorithm for Cross-lingual\n  Representation Learning",
            "updated": "2023-10-26T08:31:00Z",
            "published": "2023-10-26T08:31:00Z",
            "summary": "Expressing universal semantics common to all languages is helpful in\nunderstanding the meanings of complex and culture-specific sentences. The\nresearch theme underlying this scenario focuses on learning universal\nrepresentations across languages with the usage of massive parallel corpora.\nHowever, due to the sparsity and scarcity of parallel data, there is still a\nbig challenge in learning authentic ``universals'' for any two languages. In\nthis paper, we propose EMMA-X: an EM-like Multilingual pre-training Algorithm,\nto learn (X)Cross-lingual universals with the aid of excessive multilingual\nnon-parallel data. EMMA-X unifies the cross-lingual representation learning\ntask and an extra semantic relation prediction task within an EM framework.\nBoth the extra semantic classifier and the cross-lingual sentence encoder\napproximate the semantic relation of two sentences, and supervise each other\nuntil convergence. To evaluate EMMA-X, we conduct experiments on XRETE, a newly\nintroduced benchmark containing 12 widely studied cross-lingual tasks that\nfully depend on sentence-level representations. Results reveal that EMMA-X\nachieves state-of-the-art performance. Further geometric analysis of the built\nrepresentation space with three requirements demonstrates the superiority of\nEMMA-X over advanced models.",
            "author": [
                "Ping Guo",
                "Xiangpeng Wei",
                "Yue Hu",
                "Baosong Yang",
                "Dayiheng Liu",
                "Fei Huang",
                "Jun Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17233v1",
                "http://arxiv.org/pdf/2310.17233v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17230v1",
            "title": "Codebook Features: Sparse and Discrete Interpretability for Neural\n  Networks",
            "updated": "2023-10-26T08:28:48Z",
            "published": "2023-10-26T08:28:48Z",
            "summary": "Understanding neural networks is challenging in part because of the dense,\ncontinuous nature of their hidden states. We explore whether we can train\nneural networks to have hidden states that are sparse, discrete, and more\ninterpretable by quantizing their continuous features into what we call\ncodebook features. Codebook features are produced by finetuning neural networks\nwith vector quantization bottlenecks at each layer, producing a network whose\nhidden features are the sum of a small number of discrete vector codes chosen\nfrom a larger codebook. Surprisingly, we find that neural networks can operate\nunder this extreme bottleneck with only modest degradation in performance. This\nsparse, discrete bottleneck also provides an intuitive way of controlling\nneural network behavior: first, find codes that activate when the desired\nbehavior is present, then activate those same codes during generation to elicit\nthat behavior. We validate our approach by training codebook Transformers on\nseveral different datasets. First, we explore a finite state machine dataset\nwith far more hidden states than neurons. In this setting, our approach\novercomes the superposition problem by assigning states to distinct codes, and\nwe find that we can make the neural network behave as if it is in a different\nstate by activating the code for that state. Second, we train Transformer\nlanguage models with up to 410M parameters on two natural language datasets. We\nidentify codes in these models representing diverse, disentangled concepts\n(ranging from negative emotions to months of the year) and find that we can\nguide the model to generate different topics by activating the appropriate\ncodes during inference. Overall, codebook features appear to be a promising\nunit of analysis and control for neural networks and interpretability. Our\ncodebase and models are open-sourced at\nhttps://github.com/taufeeque9/codebook-features.",
            "author": [
                "Alex Tamkin",
                "Mohammad Taufeeque",
                "Noah D. Goodman"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17230v1",
                "http://arxiv.org/pdf/2310.17230v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17228v2",
            "title": "TST$^\\mathrm{R}$: Target Similarity Tuning Meets the Real World",
            "updated": "2023-10-28T08:24:06Z",
            "published": "2023-10-26T08:27:36Z",
            "summary": "Target similarity tuning (TST) is a method of selecting relevant examples in\nnatural language (NL) to code generation through large language models (LLMs)\nto improve performance. Its goal is to adapt a sentence embedding model to have\nthe similarity between two NL inputs match the similarity between their\nassociated code outputs. In this paper, we propose different methods to apply\nand improve TST in the real world. First, we replace the sentence transformer\nwith embeddings from a larger model, which reduces sensitivity to the language\ndistribution and thus provides more flexibility in synthetic generation of\nexamples, and we train a tiny model that transforms these embeddings to a space\nwhere embedding similarity matches code similarity, which allows the model to\nremain a black box and only requires a few matrix multiplications at inference\ntime. Second, we show how to efficiently select a smaller number of training\nexamples to train the TST model. Third, we introduce a ranking-based evaluation\nfor TST that does not require end-to-end code generation experiments, which can\nbe expensive to perform.",
            "author": [
                "Anirudh Khatry",
                "Sumit Gulwani",
                "Priyanshu Gupta",
                "Vu Le",
                "Ananya Singha",
                "Mukul Singh",
                "Gust Verbruggen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17228v2",
                "http://arxiv.org/pdf/2310.17228v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17222v2",
            "title": "Maximum Power and The Corresponding Efficiency for A Carnot-like\n  Thermoelectric Cycle Based on Fluctuation Theorem",
            "updated": "2023-11-25T09:19:49Z",
            "published": "2023-10-26T08:22:00Z",
            "summary": "Here, we investigate the maximum power and corresponding efficiency of\nthermoelectric generators through devising a set of protocols for the\nisothermal and adiabatic processes of thermoelectricity to build a Carnot-like\nthermoelectric cycle, with the analysis based on fluctuation theorem (FT).\nFirst of all, the Carnot efficiency can be readily obtained for the\nquasi-static thermoelectric cycle, with the vanishing power. Moreover, the\nmaximum power-efficiency pair of the finite-time thermoelectric cycle is\nderived, which is found to have the identical form to that of Brownian motors\ncharacterized by the stochastic thermodynamics. However, it is of significant\ndiscrepancy compared to the linear-irreversible and\nendoreversible-thermodynamics-based formulations. The distinction compared to\nthe linear-irreversible-thermodynamics case could result from the difference in\nthe definitions of Peltier and Seebeck coefficients in the thermoelectric\ncycle. As for the endoreversible thermodynamics, we argue the applicability of\nendoreversibility could be questionable for analyzing the thermoelectric cycle\nhere, due to the incompatibility of the endoreversible hypothesis that\nattributes the irreversibility to finite heat transfer with thermal reservoirs,\nthough the distinction of the mathematical expressions can vanish with the\nassumption that the ratio of thermoelectric power factors at the high and low\ntemperatures is equal to the square root of the temperature ratio (this\ncondition could significantly deviate from the practical case).",
            "author": [
                "Yuchao Hua",
                "Lingai Luo",
                "Zeng-Yuan Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17222v2",
                "http://arxiv.org/pdf/2310.17222v2"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17218v1",
            "title": "Prototypical Contrastive Learning-based CLIP Fine-tuning for Object\n  Re-identification",
            "updated": "2023-10-26T08:12:53Z",
            "published": "2023-10-26T08:12:53Z",
            "summary": "This work aims to adapt large-scale pre-trained vision-language models, such\nas contrastive language-image pretraining (CLIP), to enhance the performance of\nobject reidentification (Re-ID) across various supervision settings. Although\nprompt learning has enabled a recent work named CLIP-ReID to achieve promising\nperformance, the underlying mechanisms and the necessity of prompt learning\nremain unclear due to the absence of semantic labels in ReID tasks. In this\nwork, we first analyze the role prompt learning in CLIP-ReID and identify its\nlimitations. Based on our investigations, we propose a simple yet effective\napproach to adapt CLIP for supervised object Re-ID. Our approach directly\nfine-tunes the image encoder of CLIP using a prototypical contrastive learning\n(PCL) loss, eliminating the need for prompt learning. Experimental results on\nboth person and vehicle Re-ID datasets demonstrate the competitiveness of our\nmethod compared to CLIP-ReID. Furthermore, we extend our PCL-based CLIP\nfine-tuning approach to unsupervised scenarios, where we achieve state-of-the\nart performance.",
            "author": [
                "Jiachen Li",
                "Xiaojin Gong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17218v1",
                "http://arxiv.org/pdf/2310.17218v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17217v1",
            "title": "Beyond MLE: Convex Learning for Text Generation",
            "updated": "2023-10-26T08:08:43Z",
            "published": "2023-10-26T08:08:43Z",
            "summary": "Maximum likelihood estimation (MLE) is a statistical method used to estimate\nthe parameters of a probability distribution that best explain the observed\ndata. In the context of text generation, MLE is often used to train generative\nlanguage models, which can then be used to generate new text. However, we argue\nthat MLE is not always necessary and optimal, especially for closed-ended text\ngeneration tasks like machine translation. In these tasks, the goal of model is\nto generate the most appropriate response, which does not necessarily require\nit to estimate the entire data distribution with MLE. To this end, we propose a\nnovel class of training objectives based on convex functions, which enables\ntext generation models to focus on highly probable outputs without having to\nestimate the entire data distribution. We investigate the theoretical\nproperties of the optimal predicted distribution when applying convex functions\nto the loss, demonstrating that convex functions can sharpen the optimal\ndistribution, thereby enabling the model to better capture outputs with high\nprobabilities. Experiments on various text generation tasks and models show the\neffectiveness of our approach. It enables autoregressive models to bridge the\ngap between greedy and beam search, and facilitates the learning of\nnon-autoregressive models with a maximum improvement of 9+ BLEU points.\nMoreover, our approach also exhibits significant impact on large language\nmodels (LLMs), substantially enhancing their generative capability on various\ntasks. Source code is available at\n\\url{https://github.com/ictnlp/Convex-Learning}.",
            "author": [
                "Chenze Shao",
                "Zhengrui Ma",
                "Min Zhang",
                "Yang Feng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17217v1",
                "http://arxiv.org/pdf/2310.17217v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17216v1",
            "title": "Three-dimensional Bone Image Synthesis with Generative Adversarial\n  Networks",
            "updated": "2023-10-26T08:08:17Z",
            "published": "2023-10-26T08:08:17Z",
            "summary": "Medical image processing has been highlighted as an area where deep\nlearning-based models have the greatest potential. However, in the medical\nfield in particular, problems of data availability and privacy are hampering\nresearch progress and thus rapid implementation in clinical routine. The\ngeneration of synthetic data not only ensures privacy, but also allows to\n\\textit{draw} new patients with specific characteristics, enabling the\ndevelopment of data-driven models on a much larger scale. This work\ndemonstrates that three-dimensional generative adversarial networks (GANs) can\nbe efficiently trained to generate high-resolution medical volumes with finely\ndetailed voxel-based architectures. In addition, GAN inversion is successfully\nimplemented for the three-dimensional setting and used for extensive research\non model interpretability and applications such as image morphing, attribute\nediting and style mixing. The results are comprehensively validated on a\ndatabase of three-dimensional HR-pQCT instances representing the bone\nmicro-architecture of the distal radius.",
            "author": [
                "Christoph Angermann",
                "Johannes Bereiter-Payr",
                "Kerstin Stock",
                "Markus Haltmeier",
                "Gerald Degenhart"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17216v1",
                "http://arxiv.org/pdf/2310.17216v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "physics.med-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17213v1",
            "title": "\u00c9valuation entre pairs en math\u00e9matiques: activit\u00e9\n  d'\u00e9tudiant$\\bullet$es lors d'\u00e9valuations de preuves",
            "updated": "2023-10-26T07:56:19Z",
            "published": "2023-10-26T07:56:19Z",
            "summary": "This article deals with peer-assessment in the context of higher education\nteaching in mathematics, and examines the nature of student activity when\nassessing work produced by peers. After an overview of research on peer\nassessment, we propose an experiment with students in a specific post-secondary\nscientific class, and analyze the activity of the students involved. Our\nanalyses are based on a priori analyses of the proposed tasks and tools from\nresearch on students personal mathematical work. We base on written notes of\nassessments, observations of pairs in assessment situations, and answers to a\nquestionnaire on the perception of their activity. We have identified some\nspecific student activities during the peer assessment process, in particular\nrelated to the analysis and correction of proofs, but also to the awareness of\nthe issues associated with evaluation. This enables us to argue for the\npotential of peer assessment in higher mathematics education.",
            "author": [
                "Juliette Veuillez--Mainard",
                "Simon Modeste"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17213v1",
                "http://arxiv.org/pdf/2310.17213v1"
            ],
            "primary_category": "math.HO",
            "category": [
                "math.HO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17208v2",
            "title": "Entanglement in Cosmology",
            "updated": "2023-11-20T17:39:32Z",
            "published": "2023-10-26T07:50:20Z",
            "summary": "We compute the evolution of the entanglement entropy for a massless field\nwithin a spherical region throughout the inflationary period and the subsequent\nera of radiation domination, starting from the Bunch-Davies vacuum. The\ntransition of each mode towards a squeezed state upon horizon exit during\ninflation and the additional squeezing when radiation domination sets in\nenhance the entanglement entropy. Shortly after the transition to the\nradiation-dominated era, a volume term develops and becomes the leading\ncontribution to the entropy at late times, as is common for systems lying in\nsqueezed states. We estimate the magnitude of the entropy and discuss its\ninterpretation in the light of the quantum to classical transition for modes\nexiting the horizon during inflation. Our results raise the possibility that\nthe quantum nature of weakly interacting fields, such as gravitational waves\nresulting from tensor modes during inflation, may be detectable in today's\nuniverse. On the other hand, an observer with no knowledge of the degrees of\nfreedom beyond the horizon would interpret the entropy as thermal. From this\npoint of view, the reheating after inflation would be a result of quantum\nentanglement.",
            "author": [
                "Konstantinos Boutivas",
                "Dimitrios Katsinis",
                "Georgios Pastras",
                "Nikolaos Tetradis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17208v2",
                "http://arxiv.org/pdf/2310.17208v2"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc",
                "astro-ph.CO",
                "hep-th",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17207v1",
            "title": "Efficient Data Fusion using the Tsetlin Machine",
            "updated": "2023-10-26T07:49:25Z",
            "published": "2023-10-26T07:49:25Z",
            "summary": "We propose a novel way of assessing and fusing noisy dynamic data using a\nTsetlin Machine. Our approach consists in monitoring how explanations in form\nof logical clauses that a TM learns changes with possible noise in dynamic\ndata. This way TM can recognize the noise by lowering weights of previously\nlearned clauses, or reflect it in the form of new clauses. We also perform a\ncomprehensive experimental study using notably different datasets that\ndemonstrated high performance of the proposed approach.",
            "author": [
                "Rupsa Saha",
                "Vladimir I. Zadorozhny",
                "Ole-Christoffer Granmo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17207v1",
                "http://arxiv.org/pdf/2310.17207v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17202v1",
            "title": "miditok: A Python package for MIDI file tokenization",
            "updated": "2023-10-26T07:37:44Z",
            "published": "2023-10-26T07:37:44Z",
            "summary": "Recent progress in natural language processing has been adapted to the\nsymbolic music modality. Language models, such as Transformers, have been used\nwith symbolic music for a variety of tasks among which music generation,\nmodeling or transcription, with state-of-the-art performances. These models are\nbeginning to be used in production products. To encode and decode music for the\nbackbone model, they need to rely on tokenizers, whose role is to serialize\nmusic into sequences of distinct elements called tokens. MidiTok is an\nopen-source library allowing to tokenize symbolic music with great flexibility\nand extended features. It features the most popular music tokenizations, under\na unified API. It is made to be easily used and extensible for everyone.",
            "author": [
                "Nathan Fradet",
                "Jean-Pierre Briot",
                "Fabien Chhel",
                "Amal El Fallah Seghrouchni",
                "Nicolas Gutowski"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17202v1",
                "http://arxiv.org/pdf/2310.17202v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17201v1",
            "title": "Beampattern Design in Non-Uniform MIMO Communication",
            "updated": "2023-10-26T07:37:31Z",
            "published": "2023-10-26T07:37:31Z",
            "summary": "In recent years and with introduction of 5G cellular network and\ncommunication, researchers have shown great interest in Multiple Input Multiple\nOutput (MIMO) communication, an advanced technology. Many studies have examined\nthe problem of designing the beampattern for MIMO communication using uniform\narrays and the covariance-based method to concentrate the transmitted power to\nthe users. However, this paper aims to tackle this issue in the context of\nnon-uniform arrays. Previous authors have primarily focused on designing the\ntransmitted beampattern based on the cross-correlation matrix of transmitted\nsignal elements. In contrast, this paper suggests optimizing the positions of\ntransmitted antennas along with the cross-correlation matrix. This approach is\nexpected to produce better results.",
            "author": [
                "Amirsadegh Roshanzamir"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17201v1",
                "http://arxiv.org/pdf/2310.17201v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17198v1",
            "title": "Controlling all Degrees of Freedom of the Optical Coupling in Hybrid\n  Quantum Photonics",
            "updated": "2023-10-26T07:26:27Z",
            "published": "2023-10-26T07:26:27Z",
            "summary": "Nanophotonic quantum devices can significantly boost light-matter interaction\nwhich is important for applications such as quantum networks. Reaching a high\ninteraction strength between an optical transition of a spin system and a\nsingle mode of light is an essential step which demands precise control over\nall degrees of freedom of the optical coupling. While current devices have\nreached a high accuracy of emitter positioning, the placement process remains\noverall statistically, reducing the device fabrication yield. Furthermore, not\nall degrees of freedom of the optical coupling can be controlled limiting the\ndevice performance. Here, we develop a hybrid approach based on\nnegatively-charged silicon-vacancy center in nanodiamonds coupled to a mode of\na Si$_3$N$_4$-photonic crystal cavity, where all terms of the coupling strength\ncan be controlled individually. We use the frequency of coherent\nRabi-oscillations and line-broadening as a measure of the device performance.\nThis allows for iterative optimization of the position and the rotation of the\ndipole with respect to individual, preselected modes of light. Therefore, our\nwork marks an important step for optimization of hybrid quantum photonics and\nenables to align device simulations with real device performance.",
            "author": [
                "Niklas Lettner",
                "Lukas Antoniuk",
                "Anna P. Ovvyan",
                "Helge Gehring",
                "Daniel Wendland",
                "Viatcheslav N. Agafonov",
                "Wolfram H. P. Pernice",
                "Alexander Kubanek"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17198v1",
                "http://arxiv.org/pdf/2310.17198v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17194v1",
            "title": "Privacy-preserving Representation Learning for Speech Understanding",
            "updated": "2023-10-26T07:20:23Z",
            "published": "2023-10-26T07:20:23Z",
            "summary": "Existing privacy-preserving speech representation learning methods target a\nsingle application domain. In this paper, we present a novel framework to\nanonymize utterance-level speech embeddings generated by pre-trained encoders\nand show its effectiveness for a range of speech classification tasks.\nSpecifically, given the representations from a pre-trained encoder, we train a\nTransformer to estimate the representations for the same utterances spoken by\nother speakers. During inference, the extracted representations can be\nconverted into different identities to preserve privacy. We compare the results\nwith the voice anonymization baselines from the VoicePrivacy 2022 challenge. We\nevaluate our framework on speaker identification for privacy and emotion\nrecognition, depression classification, and intent classification for utility.\nOur method outperforms the baselines on privacy and utility in paralinguistic\ntasks and achieves comparable performance for intent classification.",
            "author": [
                "Minh Tran",
                "Mohammad Soleymani"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17194v1",
                "http://arxiv.org/pdf/2310.17194v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17193v1",
            "title": "Automatic Edge Error Judgment in Figure Skating Using 3D Pose Estimation\n  from a Monocular Camera and IMUs",
            "updated": "2023-10-26T07:15:40Z",
            "published": "2023-10-26T07:15:40Z",
            "summary": "Automatic evaluating systems are fundamental issues in sports technologies.\nIn many sports, such as figure skating, automated evaluating methods based on\npose estimation have been proposed. However, previous studies have evaluated\nskaters' skills in 2D analysis. In this paper, we propose an automatic edge\nerror judgment system with a monocular smartphone camera and inertial sensors,\nwhich enable us to analyze 3D motions. Edge error is one of the most\nsignificant scoring items and is challenging to automatically judge due to its\n3D motion. The results show that the model using 3D joint position coordinates\nestimated from the monocular camera as the input feature had the highest\naccuracy at 83% for unknown skaters' data. We also analyzed the detailed motion\nanalysis for edge error judgment. These results indicate that the monocular\ncamera can be used to judge edge errors automatically. We will provide the\nfigure skating single Lutz jump dataset, including pre-processed videos and\nlabels, at https://github.com/ryota-takedalab/JudgeAI-LutzEdge.",
            "author": [
                "Ryota Tanaka",
                "Tomohiro Suzuki",
                "Kazuya Takeda",
                "Keisuke Fujii"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17193v1",
                "http://arxiv.org/pdf/2310.17193v1"
            ],
            "primary_category": "cs.MM",
            "category": [
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17191v1",
            "title": "How do Language Models Bind Entities in Context?",
            "updated": "2023-10-26T07:10:31Z",
            "published": "2023-10-26T07:10:31Z",
            "summary": "To correctly use in-context information, language models (LMs) must bind\nentities to their attributes. For example, given a context describing a \"green\nsquare\" and a \"blue circle\", LMs must bind the shapes to their respective\ncolors. We analyze LM representations and identify the binding ID mechanism: a\ngeneral mechanism for solving the binding problem, which we observe in every\nsufficiently large model from the Pythia and LLaMA families. Using causal\ninterventions, we show that LMs' internal activations represent binding\ninformation by attaching binding ID vectors to corresponding entities and\nattributes. We further show that binding ID vectors form a continuous subspace,\nin which distances between binding ID vectors reflect their discernability.\nOverall, our results uncover interpretable strategies in LMs for representing\nsymbolic knowledge in-context, providing a step towards understanding general\nin-context reasoning in large-scale LMs.",
            "author": [
                "Jiahai Feng",
                "Jacob Steinhardt"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17191v1",
                "http://arxiv.org/pdf/2310.17191v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17190v1",
            "title": "Lookup Table meets Local Laplacian Filter: Pyramid Reconstruction\n  Network for Tone Mapping",
            "updated": "2023-10-26T07:05:38Z",
            "published": "2023-10-26T07:05:38Z",
            "summary": "Tone mapping aims to convert high dynamic range (HDR) images to low dynamic\nrange (LDR) representations, a critical task in the camera imaging pipeline. In\nrecent years, 3-Dimensional LookUp Table (3D LUT) based methods have gained\nattention due to their ability to strike a favorable balance between\nenhancement performance and computational efficiency. However, these methods\noften fail to deliver satisfactory results in local areas since the look-up\ntable is a global operator for tone mapping, which works based on pixel values\nand fails to incorporate crucial local information. To this end, this paper\naims to address this issue by exploring a novel strategy that integrates global\nand local operators by utilizing closed-form Laplacian pyramid decomposition\nand reconstruction. Specifically, we employ image-adaptive 3D LUTs to\nmanipulate the tone in the low-frequency image by leveraging the specific\ncharacteristics of the frequency information. Furthermore, we utilize local\nLaplacian filters to refine the edge details in the high-frequency components\nin an adaptive manner. Local Laplacian filters are widely used to preserve edge\ndetails in photographs, but their conventional usage involves manual tuning and\nfixed implementation within camera imaging pipelines or photo editing tools. We\npropose to learn parameter value maps progressively for local Laplacian filters\nfrom annotated data using a lightweight network. Our model achieves\nsimultaneous global tone manipulation and local edge detail preservation in an\nend-to-end manner. Extensive experimental results on two benchmark datasets\ndemonstrate that the proposed method performs favorably against\nstate-of-the-art methods.",
            "author": [
                "Feng Zhang",
                "Ming Tian",
                "Zhiqiang Li",
                "Bin Xu",
                "Qingbo Lu",
                "Changxin Gao",
                "Nong Sang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17190v1",
                "http://arxiv.org/pdf/2310.17190v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17189v1",
            "title": "Exploring Iterative Refinement with Diffusion Models for Video Grounding",
            "updated": "2023-10-26T07:04:44Z",
            "published": "2023-10-26T07:04:44Z",
            "summary": "Video grounding aims to localize the target moment in an untrimmed video\ncorresponding to a given sentence query. Existing methods typically select the\nbest prediction from a set of predefined proposals or directly regress the\ntarget span in a single-shot manner, resulting in the absence of a systematical\nprediction refinement process. In this paper, we propose DiffusionVG, a novel\nframework with diffusion models that formulates video grounding as a\nconditional generation task, where the target span is generated from Gaussian\nnoise inputs and interatively refined in the reverse diffusion process. During\ntraining, DiffusionVG progressively adds noise to the target span with a fixed\nforward diffusion process and learns to recover the target span in the reverse\ndiffusion process. In inference, DiffusionVG can generate the target span from\nGaussian noise inputs by the learned reverse diffusion process conditioned on\nthe video-sentence representations. Our DiffusionVG follows the encoder-decoder\narchitecture, which firstly encodes the video-sentence features and iteratively\ndenoises the predicted spans in its specialized span refining decoder. Without\nbells and whistles, our DiffusionVG demonstrates competitive or even superior\nperformance compared to existing well-crafted models on mainstream Charades-STA\nand ActivityNet Captions benchmarks.",
            "author": [
                "Xiao Liang",
                "Tao Shi",
                "Yaoyuan Liang",
                "Te Tao",
                "Shao-Lun Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17189v1",
                "http://arxiv.org/pdf/2310.17189v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17187v1",
            "title": "Multi-level Gated Bayesian Recurrent Neural Network for State Estimation",
            "updated": "2023-10-26T06:46:43Z",
            "published": "2023-10-26T06:46:43Z",
            "summary": "The optimality of Bayesian filtering relies on the completeness of prior\nmodels, while deep learning holds a distinct advantage in learning models from\noffline data. Nevertheless, the current fusion of these two methodologies\nremains largely ad hoc, lacking a theoretical foundation. This paper presents a\nnovel solution, namely a multi-level gated Bayesian recurrent neural network\nspecifically designed to state estimation under model mismatches. Firstly, we\ntransform the non-Markov state-space model into an equivalent first-order\nMarkov model with memory. It is a generalized transformation that overcomes the\nlimitations of the first-order Markov property and enables recursive filtering.\nSecondly, by deriving a data-assisted joint state-memory-mismatch Bayesian\nfiltering, we design a Bayesian multi-level gated framework that includes a\nmemory update gate for capturing the temporal regularities in state evolution,\na state prediction gate with the evolution mismatch compensation, and a state\nupdate gate with the observation mismatch compensation. The Gaussian\napproximation implementation of the filtering process within the gated\nframework is derived, taking into account the computational efficiency.\nFinally, the corresponding internal neural network structures and end-to-end\ntraining methods are designed. The Bayesian filtering theory enhances the\ninterpretability of the proposed gated network, enabling the effective\nintegration of offline data and prior models within functionally explicit gated\nunits. In comprehensive experiments, including simulations and real-world\ndatasets, the proposed gated network demonstrates superior estimation\nperformance compared to benchmark filters and state-of-the-art deep learning\nfiltering methods.",
            "author": [
                "Shi Yan",
                "Yan Liang",
                "Le Zheng",
                "Mingyang Fan",
                "Binglu Wang",
                "Xiaoxu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17187v1",
                "http://arxiv.org/pdf/2310.17187v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17186v1",
            "title": "Demystifying Compiler Unstable Feature Usage and Impacts in the Rust\n  Ecosystem",
            "updated": "2023-10-26T06:43:25Z",
            "published": "2023-10-26T06:43:25Z",
            "summary": "Rust programming language is gaining popularity rapidly in building reliable\nand secure systems due to its security guarantees and outstanding performance.\nTo provide extra functionalities, the Rust compiler introduces Rust unstable\nfeatures (RUF) to extend compiler functionality, syntax, and standard library\nsupport. However, these features are unstable and may get removed, introducing\ncompilation failures to dependent packages. Even worse, their impacts propagate\nthrough transitive dependencies, causing large-scale failures in the whole\necosystem. Although RUF is widely used in Rust, previous research has primarily\nconcentrated on Rust code safety, with the usage and impacts of RUF from the\nRust compiler remaining unexplored. Therefore, we aim to bridge this gap by\nsystematically analyzing the RUF usage and impacts in the Rust ecosystem. We\npropose novel techniques for extracting RUF precisely, and to assess its impact\non the entire ecosystem quantitatively, we accurately resolve package\ndependencies. We have analyzed the whole Rust ecosystem with 590K package\nversions and 140M transitive dependencies. Our study shows that the Rust\necosystem uses 1000 different RUF, and at most 44% of package versions are\naffected by RUF, causing compiling failures for at most 12%. To mitigate wide\nRUF impacts, we further design and implement a RUF-compilation-failure recovery\ntool that can recover up to 90% of the failure. We believe our techniques,\nfindings, and tools can help to stabilize the Rust compiler, ultimately\nenhancing the security and reliability of the Rust ecosystem.",
            "author": [
                "Chenghao Li",
                "Yifei Wu",
                "Wenbo Shen",
                "Zichen Zhao",
                "Rui Chang",
                "Chengwei Liu",
                "Yang Liu",
                "Kui Ren"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17186v1",
                "http://arxiv.org/pdf/2310.17186v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17183v1",
            "title": "Understanding the Effects of Projectors in Knowledge Distillation",
            "updated": "2023-10-26T06:30:39Z",
            "published": "2023-10-26T06:30:39Z",
            "summary": "Conventionally, during the knowledge distillation process (e.g. feature\ndistillation), an additional projector is often required to perform feature\ntransformation due to the dimension mismatch between the teacher and the\nstudent networks. Interestingly, we discovered that even if the student and the\nteacher have the same feature dimensions, adding a projector still helps to\nimprove the distillation performance. In addition, projectors even improve\nlogit distillation if we add them to the architecture too. Inspired by these\nsurprising findings and the general lack of understanding of the projectors in\nthe knowledge distillation process from existing literature, this paper\ninvestigates the implicit role that projectors play but so far have been\noverlooked. Our empirical study shows that the student with a projector (1)\nobtains a better trade-off between the training accuracy and the testing\naccuracy compared to the student without a projector when it has the same\nfeature dimensions as the teacher, (2) better preserves its similarity to the\nteacher beyond shallow and numeric resemblance, from the view of Centered\nKernel Alignment (CKA), and (3) avoids being over-confident as the teacher does\nat the testing phase. Motivated by the positive effects of projectors, we\npropose a projector ensemble-based feature distillation method to further\nimprove distillation performance. Despite the simplicity of the proposed\nstrategy, empirical results from the evaluation of classification tasks on\nbenchmark datasets demonstrate the superior classification performance of our\nmethod on a broad range of teacher-student pairs and verify from the aspects of\nCKA and model calibration that the student's features are of improved quality\nwith the projector ensemble design.",
            "author": [
                "Yudong Chen",
                "Sen Wang",
                "Jiajun Liu",
                "Xuwei Xu",
                "Frank de Hoog",
                "Brano Kusy",
                "Zi Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17183v1",
                "http://arxiv.org/pdf/2310.17183v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17180v1",
            "title": "A Forward Reachability Perspective on Robust Control Invariance and\n  Discount Factors in Reachability Analysis",
            "updated": "2023-10-26T06:09:40Z",
            "published": "2023-10-26T06:09:40Z",
            "summary": "Control invariant sets are crucial for various methods that aim to design\nsafe control policies for systems whose state constraints must be satisfied\nover an indefinite time horizon. In this article, we explore the connections\namong reachability, control invariance, and Control Barrier Functions (CBFs) by\nexamining the forward reachability problem associated with control invariant\nsets. We present the notion of an \"inevitable Forward Reachable Tube\" (FRT) as\na tool for analyzing control invariant sets. Our findings show that the\ninevitable FRT of a robust control invariant set with a differentiable boundary\nis the set itself. We highlight the role of the differentiability of the\nboundary in shaping the FRTs of the sets through numerical examples. We also\nformulate a zero-sum differential game between the control and disturbance,\nwhere the inevitable FRT is characterized by the zero-superlevel set of the\nvalue function. By incorporating a discount factor in the cost function of the\ngame, the barrier constraint of the CBF naturally arises as the constraint that\nis imposed on the optimal control policy. As a result, the value function of\nour FRT formulation serves as a CBF-like function, which has not been\npreviously realized in reachability studies. Conversely, any valid CBF is also\na forward reachability value function inside the control invariant set, thereby\nrevealing the inverse optimality of the CBF. As such, our work establishes a\nstrong link between reachability, control invariance, and CBFs, filling a gap\nthat prior formulations based on backward reachability were unable to bridge.",
            "author": [
                "Jason J. Choi",
                "Donggun Lee",
                "Boyang Li",
                "Jonathan P. How",
                "Koushil Sreenath",
                "Sylvia L. Herbert",
                "Claire J. Tomlin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17180v1",
                "http://arxiv.org/pdf/2310.17180v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17179v3",
            "title": "Linking intra- and extra-cellular metabolic domains via neural-network\n  surrogates for dynamic metabolic control",
            "updated": "2023-11-19T02:12:55Z",
            "published": "2023-10-26T06:06:24Z",
            "summary": "In this study, we aim to optimize biotechnological production by manipulating\nintracellular metabolic fluxes in microbial cell factories. Model-based dynamic\noptimization is proposed to determine the optimal dynamic trajectories of the\nmanipulatable intracellular fluxes. A challenge emerges as existing models are\noften oversimplified, lacking insights into intracellular metabolism, or are\nexcessively complex, leading to numerical and implementation challenges in\noptimal control (e.g., related to bilevel optimizations). We propose a solution\ninvolving a machine-learning surrogate derived from steady-state\nconstraint-based metabolic modeling. This surrogate bridges the gap between\nmanipulatable intracellular fluxes and process exchange rates. By integrating\nthe surrogate model with simple macro-kinetic dynamic models, we can develop\nhybrid machine-learning-supported dynamic models. Conveniently, the\nmanipulatable intracellular fluxes in these augmented models can be exploited\nas dynamic optimization degrees of freedom. We apply this modeling and\noptimization strategy to a representative metabolic network that showcases\ncommon challenges in dynamic metabolic control. We also present an example of\ncybernetic control to counteract system uncertainties. Our approach facilitates\nthe in silico evaluation of dynamic metabolic interventions and can aid in the\nselection of suitable control and actuation strategies.",
            "author": [
                "Sebasti\u00e1n Espinel-R\u00edos",
                "Jos\u00e9 L. Avalos"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17179v3",
                "http://arxiv.org/pdf/2310.17179v3"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17177v1",
            "title": "Bridging The Gaps Between Token Pruning and Full Pre-training via Masked\n  Fine-tuning",
            "updated": "2023-10-26T06:03:18Z",
            "published": "2023-10-26T06:03:18Z",
            "summary": "Despite the success of transformers on various computer vision tasks, they\nsuffer from excessive memory and computational cost. Some works present dynamic\nvision transformers to accelerate inference by pruning redundant tokens. A key\nto improving token pruning is using well-trained models as initialization for\nfaster convergence and better performance. However, current base models usually\nadopt full image training, i.e., using full images as inputs and keeping the\nwhole feature maps through the forward process, which causes inconsistencies\nwith dynamic models that gradually reduce tokens, including calculation\npattern, information amount and token selection strategy inconsistencies.\nInspired by MAE which performs masking and reconstruction self-supervised task,\nwe devise masked fine-tuning to bridge the gaps between pre-trained base models\nused for initialization and token pruning based dynamic vision transformers, by\nmasking image patches and predicting the image class label based on left\nunmasked patches. Extensive experiments on ImageNet demonstrate that base\nmodels via masked fine-tuning gain strong occlusion robustness and ability\nagainst information loss. With this better initialization, Dynamic ViT achieves\nhigher accuracies, especially under large token pruning ratios (e.g., 81.9% vs.\n81.3%, and 62.3% vs. 58.9% for DeiT based Dynamic ViT/0.8 and Dynamic ViT/0.3).\nMoreover, we apply our method into different token pruning based dynamic vision\ntransformers, different pre-trained models and randomly initialized models to\ndemonstrate the generalization ability.",
            "author": [
                "Fengyuan Shi",
                "Limin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17177v1",
                "http://arxiv.org/pdf/2310.17177v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17174v1",
            "title": "Steady-state charging of quantum batteries via dissipative ancillas",
            "updated": "2023-10-26T05:56:15Z",
            "published": "2023-10-26T05:56:15Z",
            "summary": "We investigate the steady-state charging process of a single-cell quantum\nbattery embedded in an N-cell star network of qubits, each interacting with a\nfermion reservoir, collectively and individually in equilibrium and\nnon-equilibrium scenarios, respectively. We find an optimal steady-state\ncharging in both scenarios, which grows monotonically with the reservoirs'\nchemical potential and chemical potential difference. Where the high base\ntemperature of the reservoirs has a destructive role in all parameter regimes.\nWe find that regardless of the strength of the non-equilibrium condition, the\nhigh base chemical potential of the battery's corresponding reservoir can\nsignificantly enhance the charging process. On the other hand, a weak coupling\nstrength can strongly suppress the charging. Consequently, our results offer\nsome useful protocols for optimizing the charging process of open quantum\nbatteries without an external charging field.",
            "author": [
                "F. H. Kamin",
                "S. Salimi",
                "M. B. Arjmand"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17174v1",
                "http://arxiv.org/pdf/2310.17174v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17168v1",
            "title": "Learning an Inventory Control Policy with General Inventory Arrival\n  Dynamics",
            "updated": "2023-10-26T05:49:13Z",
            "published": "2023-10-26T05:49:13Z",
            "summary": "In this paper we address the problem of learning and backtesting inventory\ncontrol policies in the presence of general arrival dynamics -- which we term\nas a quantity-over-time arrivals model (QOT). We also allow for order\nquantities to be modified as a post-processing step to meet vendor constraints\nsuch as order minimum and batch size constraints -- a common practice in real\nsupply chains. To the best of our knowledge this is the first work to handle\neither arbitrary arrival dynamics or an arbitrary downstream post-processing of\norder quantities. Building upon recent work (Madeka et al., 2022) we similarly\nformulate the periodic review inventory control problem as an exogenous\ndecision process, where most of the state is outside the control of the agent.\nMadeka et al. (2022) show how to construct a simulator that replays historic\ndata to solve this class of problem. In our case, we incorporate a deep\ngenerative model for the arrivals process as part of the history replay. By\nformulating the problem as an exogenous decision process, we can apply results\nfrom Madeka et al. (2022) to obtain a reduction to supervised learning.\nFinally, we show via simulation studies that this approach yields statistically\nsignificant improvements in profitability over production baselines. Using data\nfrom an ongoing real-world A/B test, we show that Gen-QOT generalizes well to\noff-policy data.",
            "author": [
                "Sohrab Andaz",
                "Carson Eisenach",
                "Dhruv Madeka",
                "Kari Torkkola",
                "Randy Jia",
                "Dean Foster",
                "Sham Kakade"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17168v1",
                "http://arxiv.org/pdf/2310.17168v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17167v1",
            "title": "Improving Denoising Diffusion Models via Simultaneous Estimation of\n  Image and Noise",
            "updated": "2023-10-26T05:43:07Z",
            "published": "2023-10-26T05:43:07Z",
            "summary": "This paper introduces two key contributions aimed at improving the speed and\nquality of images generated through inverse diffusion processes. The first\ncontribution involves reparameterizing the diffusion process in terms of the\nangle on a quarter-circular arc between the image and noise, specifically\nsetting the conventional $\\displaystyle \\sqrt{\\bar{\\alpha}}=\\cos(\\eta)$. This\nreparameterization eliminates two singularities and allows for the expression\nof diffusion evolution as a well-behaved ordinary differential equation (ODE).\nIn turn, this allows higher order ODE solvers such as Runge-Kutta methods to be\nused effectively. The second contribution is to directly estimate both the\nimage ($\\mathbf{x}_0$) and noise ($\\mathbf{\\epsilon}$) using our network, which\nenables more stable calculations of the update step in the inverse diffusion\nsteps, as accurate estimation of both the image and noise are crucial at\ndifferent stages of the process. Together with these changes, our model\nachieves faster generation, with the ability to converge on high-quality images\nmore quickly, and higher quality of the generated images, as measured by\nmetrics such as Frechet Inception Distance (FID), spatial Frechet Inception\nDistance (sFID), precision, and recall.",
            "author": [
                "Zhenkai Zhang",
                "Krista A. Ehinger",
                "Tom Drummond"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17167v1",
                "http://arxiv.org/pdf/2310.17167v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17166v1",
            "title": "X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity",
            "updated": "2023-10-26T05:39:49Z",
            "published": "2023-10-26T05:39:49Z",
            "summary": "Cross-lingual transfer (XLT) is an emergent ability of multilingual language\nmodels that preserves their performance on a task to a significant extent when\nevaluated in languages that were not included in the fine-tuning process. While\nEnglish, due to its widespread usage, is typically regarded as the primary\nlanguage for model adaption in various tasks, recent studies have revealed that\nthe efficacy of XLT can be amplified by selecting the most appropriate source\nlanguages based on specific conditions. In this work, we propose the\nutilization of sub-network similarity between two languages as a proxy for\npredicting the compatibility of the languages in the context of XLT. Our\napproach is model-oriented, better reflecting the inner workings of foundation\nmodels. In addition, it requires only a moderate amount of raw text from\ncandidate languages, distinguishing it from the majority of previous methods\nthat rely on external resources. In experiments, we demonstrate that our method\nis more effective than baselines across diverse tasks. Specifically, it shows\nproficiency in ranking candidates for zero-shot XLT, achieving an improvement\nof 4.6% on average in terms of NDCG@3. We also provide extensive analyses that\nconfirm the utility of sub-networks for XLT prediction.",
            "author": [
                "Taejun Yun",
                "Jinhyeon Kim",
                "Deokyeong Kang",
                "Seong Hoon Lim",
                "Jihoon Kim",
                "Taeuk Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17166v1",
                "http://arxiv.org/pdf/2310.17166v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17163v1",
            "title": "Low-Dimensional Gradient Helps Out-of-Distribution Detection",
            "updated": "2023-10-26T05:28:32Z",
            "published": "2023-10-26T05:28:32Z",
            "summary": "Detecting out-of-distribution (OOD) samples is essential for ensuring the\nreliability of deep neural networks (DNNs) in real-world scenarios. While\nprevious research has predominantly investigated the disparity between\nin-distribution (ID) and OOD data through forward information analysis, the\ndiscrepancy in parameter gradients during the backward process of DNNs has\nreceived insufficient attention. Existing studies on gradient disparities\nmainly focus on the utilization of gradient norms, neglecting the wealth of\ninformation embedded in gradient directions. To bridge this gap, in this paper,\nwe conduct a comprehensive investigation into leveraging the entirety of\ngradient information for OOD detection. The primary challenge arises from the\nhigh dimensionality of gradients due to the large number of network parameters.\nTo solve this problem, we propose performing linear dimension reduction on the\ngradient using a designated subspace that comprises principal components. This\ninnovative technique enables us to obtain a low-dimensional representation of\nthe gradient with minimal information loss. Subsequently, by integrating the\nreduced gradient with various existing detection score functions, our approach\ndemonstrates superior performance across a wide range of detection tasks. For\ninstance, on the ImageNet benchmark, our method achieves an average reduction\nof 11.15% in the false positive rate at 95% recall (FPR95) compared to the\ncurrent state-of-the-art approach. The code would be released.",
            "author": [
                "Yingwen Wu",
                "Tao Li",
                "Xinwen Cheng",
                "Jie Yang",
                "Xiaolin Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17163v1",
                "http://arxiv.org/pdf/2310.17163v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17162v1",
            "title": "Content-based Controls For Music Large Language Modeling",
            "updated": "2023-10-26T05:24:38Z",
            "published": "2023-10-26T05:24:38Z",
            "summary": "Recent years have witnessed a rapid growth of large-scale language models in\nthe domain of music audio. Such models enable end-to-end generation of\nhigher-quality music, and some allow conditioned generation using text\ndescriptions. However, the control power of text controls on music is\nintrinsically limited, as they can only describe music indirectly through\nmeta-data (such as singers and instruments) or high-level representations (such\nas genre and emotion). We aim to further equip the models with direct and\ncontent-based controls on innate music languages such as pitch, chords and drum\ntrack. To this end, we contribute Coco-Mulla, a content-based control method\nfor music large language modeling. It uses a parameter-efficient fine-tuning\n(PEFT) method tailored for Transformer-based audio models. Experiments show\nthat our approach achieved high-quality music generation with low-resource\nsemi-supervised learning, tuning with less than 4% parameters compared to the\noriginal model and training on a small dataset with fewer than 300 songs.\nMoreover, our approach enables effective content-based controls, and we\nillustrate the control power via chords and rhythms, two of the most salient\nfeatures of music audio. Furthermore, we show that by combining content-based\ncontrols and text descriptions, our system achieves flexible music variation\ngeneration and style transfer. Our source codes and demos are available online.",
            "author": [
                "Liwei Lin",
                "Gus Xia",
                "Junyan Jiang",
                "Yixiao Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17162v1",
                "http://arxiv.org/pdf/2310.17162v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17161v1",
            "title": "Structure discovery in Atomic Force Microscopy imaging of ice",
            "updated": "2023-10-26T05:20:06Z",
            "published": "2023-10-26T05:20:06Z",
            "summary": "The interaction of water with surfaces is crucially important in a wide range\nof natural and technological settings. In particular, at low temperatures,\nunveiling the atomistic structure of adsorbed water clusters would provide\nvaluable data for understanding the ice nucleation process. Using\nhigh-resolution Atomic Force Microscopy (AFM) and Scanning Tunnelling\nMicroscopy, several studies have demonstrated the presence of water pentamers,\nhexamers, heptamers (and of their combinations) on a variety of metallic\nsurfaces, as well the initial stages of 2D ice growth on an insulating surface.\nHowever, in all these cases, the observed structures were completely flat,\nproviding a relatively straightforward path to interpretation. Here, we present\nhigh-resolution AFM measurements of several new water clusters on Au(111) and\nCu(111), whose understanding presents significant challenges, due to both their\nhighly 3D configuration and to their large size. For each of them, we use a\ncombination of machine learning, atomistic modelling with neural network\npotentials and statistical sampling to propose an underlying atomic structure,\nfinally comparing its AFM simulated images to the experimental ones. These\nresults provide new insights into the early phases of ice formation, which is a\nubiquitous phenomenon ranging from biology to astrophysics.",
            "author": [
                "F. Priante",
                "N. Oinonen",
                "Y. Tian",
                "D. Guan",
                "C. Xu",
                "S. Cai",
                "P. Liljeroth",
                "Y. Jiang",
                "A. S. Foster"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17161v1",
                "http://arxiv.org/pdf/2310.17161v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17157v1",
            "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time",
            "updated": "2023-10-26T05:01:09Z",
            "published": "2023-10-26T05:01:09Z",
            "summary": "Large language models (LLMs) with hundreds of billions of parameters have\nsparked a new wave of exciting AI applications. However, they are\ncomputationally expensive at inference time. Sparsity is a natural approach to\nreduce this cost, but existing methods either require costly retraining, have\nto forgo LLM's in-context learning ability, or do not yield wall-clock time\nspeedup on modern hardware. We hypothesize that contextual sparsity, which are\nsmall, input-dependent sets of attention heads and MLP parameters that yield\napproximately the same output as the dense model for a given input, can address\nthese issues. We show that contextual sparsity exists, that it can be\naccurately predicted, and that we can exploit it to speed up LLM inference in\nwall-clock time without compromising LLM's quality or in-context learning\nability. Based on these insights, we propose DejaVu, a system that uses a\nlow-cost algorithm to predict contextual sparsity on the fly given inputs to\neach layer, along with an asynchronous and hardware-aware implementation that\nspeeds up LLM inference. We validate that DejaVu can reduce the inference\nlatency of OPT-175B by over 2X compared to the state-of-the-art\nFasterTransformer, and over 6X compared to the widely used Hugging Face\nimplementation, without compromising model quality. The code is available at\nhttps://github.com/FMInference/DejaVu.",
            "author": [
                "Zichang Liu",
                "Jue Wang",
                "Tri Dao",
                "Tianyi Zhou",
                "Binhang Yuan",
                "Zhao Song",
                "Anshumali Shrivastava",
                "Ce Zhang",
                "Yuandong Tian",
                "Christopher Re",
                "Beidi Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17157v1",
                "http://arxiv.org/pdf/2310.17157v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17156v1",
            "title": "Learning depth from monocular video sequences",
            "updated": "2023-10-26T05:00:41Z",
            "published": "2023-10-26T05:00:41Z",
            "summary": "Learning single image depth estimation model from monocular video sequence is\na very challenging problem. In this paper, we propose a novel training loss\nwhich enables us to include more images for supervision during the training\nprocess. We propose a simple yet effective model to account the frame to frame\npixel motion. We also design a novel network architecture for single image\nestimation. When combined, our method produces state of the art results for\nmonocular depth estimation on the KITTI dataset in the self-supervised setting.",
            "author": [
                "Zhenwei Luo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17156v1",
                "http://arxiv.org/pdf/2310.17156v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17155v1",
            "title": "Max-min Rate Optimization of Low-Complexity Hybrid Multi-User\n  Beamforming Maintaining Rate-Fairness",
            "updated": "2023-10-26T05:00:29Z",
            "published": "2023-10-26T05:00:29Z",
            "summary": "A wireless network serving multiple users in the millimeter-wave or the\nsub-terahertz band by a base station is considered. High-throughput multi-user\nhybrid-transmit beamforming is conceived by maximizing the minimum rate of the\nusers. For the sake of energy-efficient signal transmission, the\narray-of-subarrays structure is used for analog beamforming relying on\nlow-resolution phase shifters. We develop a convexsolver based algorithm, which\niteratively invokes a convex problem of the same beamformer size for its\nsolution. We then introduce the soft max-min rate objective function and\ndevelop a scalable algorithm for its optimization. Our simulation results\ndemonstrate the striking fact that soft max-min rate optimization not only\napproaches the minimum user rate obtained by max-min rate optimization but it\nalso achieves a sum rate similar to that of sum-rate maximization. Thus, the\nsoft max-min rate optimization based beamforming design conceived offers a new\ntechnique of simultaneously achieving a high individual quality-of-service for\nall users and a high total network throughput.",
            "author": [
                "W. Zhu",
                "H. D. Tuan",
                "E. Dutkiewicz",
                "H. V. Poor",
                "L. Hanzo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17155v1",
                "http://arxiv.org/pdf/2310.17155v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17154v1",
            "title": "Deep Imbalanced Regression via Hierarchical Classification Adjustment",
            "updated": "2023-10-26T04:54:39Z",
            "published": "2023-10-26T04:54:39Z",
            "summary": "Regression tasks in computer vision, such as age estimation or counting, are\noften formulated into classification by quantizing the target space into\nclasses. Yet real-world data is often imbalanced -- the majority of training\nsamples lie in a head range of target values, while a minority of samples span\na usually larger tail range. By selecting the class quantization, one can\nadjust imbalanced regression targets into balanced classification outputs,\nthough there are trade-offs in balancing classification accuracy and\nquantization error. To improve regression performance over the entire range of\ndata, we propose to construct hierarchical classifiers for solving imbalanced\nregression tasks. The fine-grained classifiers limit the quantization error\nwhile being modulated by the coarse predictions to ensure high accuracy.\nStandard hierarchical classification approaches, however, when applied to the\nregression problem, fail to ensure that predicted ranges remain consistent\nacross the hierarchy. As such, we propose a range-preserving distillation\nprocess that can effectively learn a single classifier from the set of\nhierarchical classifiers. Our novel hierarchical classification adjustment\n(HCA) for imbalanced regression shows superior results on three diverse tasks:\nage estimation, crowd counting and depth estimation. We will release the source\ncode upon acceptance.",
            "author": [
                "Haipeng Xiong",
                "Angela Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17154v1",
                "http://arxiv.org/pdf/2310.17154v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17153v1",
            "title": "Hierarchical Semi-Implicit Variational Inference with Application to\n  Diffusion Model Acceleration",
            "updated": "2023-10-26T04:52:28Z",
            "published": "2023-10-26T04:52:28Z",
            "summary": "Semi-implicit variational inference (SIVI) has been introduced to expand the\nanalytical variational families by defining expressive semi-implicit\ndistributions in a hierarchical manner. However, the single-layer architecture\ncommonly used in current SIVI methods can be insufficient when the target\nposterior has complicated structures. In this paper, we propose hierarchical\nsemi-implicit variational inference, called HSIVI, which generalizes SIVI to\nallow more expressive multi-layer construction of semi-implicit distributions.\nBy introducing auxiliary distributions that interpolate between a simple base\ndistribution and the target distribution, the conditional layers can be trained\nby progressively matching these auxiliary distributions one layer after\nanother. Moreover, given pre-trained score networks, HSIVI can be used to\naccelerate the sampling process of diffusion models with the score matching\nobjective. We show that HSIVI significantly enhances the expressiveness of SIVI\non several Bayesian inference problems with complicated target distributions.\nWhen used for diffusion model acceleration, we show that HSIVI can produce high\nquality samples comparable to or better than the existing fast diffusion model\nbased samplers with a small number of function evaluations on various datasets.",
            "author": [
                "Longlin Yu",
                "Tianyu Xie",
                "Yu Zhu",
                "Tong Yang",
                "Xiangyu Zhang",
                "Cheng Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17153v1",
                "http://arxiv.org/pdf/2310.17153v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17149v1",
            "title": "Explainable Spatio-Temporal Graph Neural Networks",
            "updated": "2023-10-26T04:47:28Z",
            "published": "2023-10-26T04:47:28Z",
            "summary": "Spatio-temporal graph neural networks (STGNNs) have gained popularity as a\npowerful tool for effectively modeling spatio-temporal dependencies in diverse\nreal-world urban applications, including intelligent transportation and public\nsafety. However, the black-box nature of STGNNs limits their interpretability,\nhindering their application in scenarios related to urban resource allocation\nand policy formulation. To bridge this gap, we propose an Explainable\nSpatio-Temporal Graph Neural Networks (STExplainer) framework that enhances\nSTGNNs with inherent explainability, enabling them to provide accurate\npredictions and faithful explanations simultaneously. Our framework integrates\na unified spatio-temporal graph attention network with a positional information\nfusion layer as the STG encoder and decoder, respectively. Furthermore, we\npropose a structure distillation approach based on the Graph Information\nBottleneck (GIB) principle with an explainable objective, which is instantiated\nby the STG encoder and decoder. Through extensive experiments, we demonstrate\nthat our STExplainer outperforms state-of-the-art baselines in terms of\npredictive accuracy and explainability metrics (i.e., sparsity and fidelity) on\ntraffic and crime prediction tasks. Furthermore, our model exhibits superior\nrepresentation ability in alleviating data missing and sparsity issues. The\nimplementation code is available at: https://github.com/HKUDS/STExplainer.",
            "author": [
                "Jiabin Tang",
                "Lianghao Xia",
                "Chao Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17149v1",
                "http://arxiv.org/pdf/2310.17149v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17147v1",
            "title": "Simple Baselines for Projection-based Full-reference and No-reference\n  Point Cloud Quality Assessment",
            "updated": "2023-10-26T04:42:57Z",
            "published": "2023-10-26T04:42:57Z",
            "summary": "Point clouds are widely used in 3D content representation and have various\napplications in multimedia. However, compression and simplification processes\ninevitably result in the loss of quality-aware information under storage and\nbandwidth constraints. Therefore, there is an increasing need for effective\nmethods to quantify the degree of distortion in point clouds. In this paper, we\npropose simple baselines for projection-based point cloud quality assessment\n(PCQA) to tackle this challenge. We use multi-projections obtained via a common\ncube-like projection process from the point clouds for both full-reference (FR)\nand no-reference (NR) PCQA tasks. Quality-aware features are extracted with\npopular vision backbones. The FR quality representation is computed as the\nsimilarity between the feature maps of reference and distorted projections\nwhile the NR quality representation is obtained by simply squeezing the feature\nmaps of distorted projections with average pooling The corresponding quality\nrepresentations are regressed into visual quality scores by fully-connected\nlayers. Taking part in the ICIP 2023 PCVQA Challenge, we succeeded in achieving\nthe top spot in four out of the five competition tracks.",
            "author": [
                "Zicheng Zhang",
                "Yingjie Zhou",
                "Wei Sun",
                "Xiongkuo Min",
                "Guangtao Zhai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17147v1",
                "http://arxiv.org/pdf/2310.17147v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.12864v1",
            "title": "OptScaler: A Hybrid Proactive-Reactive Framework for Robust Autoscaling\n  in the Cloud",
            "updated": "2023-10-26T04:38:48Z",
            "published": "2023-10-26T04:38:48Z",
            "summary": "Autoscaling is a vital mechanism in cloud computing that supports the\nautonomous adjustment of computing resources under dynamic workloads. A primary\ngoal of autoscaling is to stabilize resource utilization at a desirable level,\nthus reconciling the need for resource-saving with the satisfaction of Service\nLevel Objectives (SLOs). Existing proactive autoscaling methods anticipate the\nfuture workload and scale the resources in advance, whereas the reliability may\nsuffer from prediction deviations arising from the frequent fluctuations and\nnoise of cloud workloads; reactive methods rely on real-time system feedback,\nwhile the hysteretic nature of reactive methods could cause violations of the\nrigorous SLOs. To this end, this paper presents OptScaler, a hybrid autoscaling\nframework that integrates the power of both proactive and reactive methods for\nregulating CPU utilization. Specifically, the proactive module of OptScaler\nconsists of a sophisticated workload prediction model and an optimization\nmodel, where the former provides reliable inputs to the latter for making\noptimal scaling decisions. The reactive module provides a self-tuning estimator\nof CPU utilization to the optimization model. We embed Model Predictive Control\n(MPC) mechanism and robust optimization techniques into the optimization model\nto further enhance its reliability. Numerical results have demonstrated the\nsuperiority of both the workload prediction model and the hybrid framework of\nOptScaler in the scenario of online services compared to prevalent reactive,\nproactive, or hybrid autoscalers. OptScaler has been successfully deployed at\nAlipay, supporting the autoscaling of applets in the world-leading payment\nplatform.",
            "author": [
                "Ding Zou",
                "Wei Lu",
                "Zhibo Zhu",
                "Xingyu Lu",
                "Jun Zhou",
                "Xiaojin Wang",
                "Kangyu Liu",
                "Haiqing Wang",
                "Kefan Wang",
                "Renen Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12864v1",
                "http://arxiv.org/pdf/2311.12864v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17143v1",
            "title": "Supercharging academic writing with generative AI: framework,\n  techniques, and caveats",
            "updated": "2023-10-26T04:35:00Z",
            "published": "2023-10-26T04:35:00Z",
            "summary": "Academic writing is an indispensable yet laborious part of the research\nenterprise. This Perspective maps out principles and methods for using\ngenerative artificial intelligence (AI), specifically large language models\n(LLMs), to elevate the quality and efficiency of academic writing. We introduce\na human-AI collaborative framework that delineates the rationale (why), process\n(how), and nature (what) of AI engagement in writing. The framework pinpoints\nboth short-term and long-term reasons for engagement and their underlying\nmechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals\nthe role of AI throughout the writing process, conceptualized through a\ntwo-stage model for human-AI collaborative writing, and the nature of AI\nassistance in writing, represented through a model of writing-assistance types\nand levels. Building on this framework, we describe effective prompting\ntechniques for incorporating AI into the writing routine (outlining, drafting,\nand editing) as well as strategies for maintaining rigorous scholarship,\nadhering to varied journal policies, and avoiding overreliance on AI.\nUltimately, the prudent integration of AI into academic writing can ease the\ncommunication burden, empower authors, accelerate discovery, and promote\ndiversity in science.",
            "author": [
                "Zhicheng Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17143v1",
                "http://arxiv.org/pdf/2310.17143v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17142v1",
            "title": "Single channel speech enhancement by colored spectrograms",
            "updated": "2023-10-26T04:29:27Z",
            "published": "2023-10-26T04:29:27Z",
            "summary": "Speech enhancement concerns the processes required to remove unwanted\nbackground sounds from the target speech to improve its quality and\nintelligibility. In this paper, a novel approach for single-channel speech\nenhancement is presented, using colored spectrograms. We propose the use of a\ndeep neural network (DNN) architecture adapted from the pix2pix generative\nadversarial network (GAN) and train it over colored spectrograms of speech to\ndenoise them. After denoising, the colors of spectrograms are translated to\nmagnitudes of short-time Fourier transform (STFT) using a shallow regression\nneural network. These estimated STFT magnitudes are later combined with the\nnoisy phases to obtain an enhanced speech. The results show an improvement of\nalmost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1%\nin the short-term objective intelligibility (STOI) over the unprocessed noisy\ndata. The gain in quality and intelligibility over the unprocessed signal is\nalmost equal to the gain achieved by the baseline methods used for comparison\nwith the proposed model, but at a much reduced computational cost. The proposed\nsolution offers a comparative PESQ score at almost 10 times reduced\ncomputational cost than a similar baseline model that has generated the highest\nPESQ score trained on grayscaled spectrograms, while it provides only a 1%\ndeficit in STOI at 28 times reduced computational cost when compared to another\nbaseline system based on convolutional neural network-GAN (CNN-GAN) that\nproduces the most intelligible speech.",
            "author": [
                "Sania Gul",
                "Muhammad Salman Khan",
                "Muhammad Fazeel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17142v1",
                "http://arxiv.org/pdf/2310.17142v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17141v1",
            "title": "Neural style transfer of weak lensing mass maps",
            "updated": "2023-10-26T04:25:41Z",
            "published": "2023-10-26T04:25:41Z",
            "summary": "We propose a new generative model of projected cosmic mass density maps\ninferred from weak gravitational lensing observations of distant galaxies (weak\nlensing mass maps). We construct the model based on a neural style transfer so\nthat it can transform Gaussian weak lensing mass maps into deeply non-Gaussian\ncounterparts as predicted in ray-tracing lensing simulations. We develop an\nunpaired image-to-image translation method with Cycle-Consistent Generative\nAdversarial Networks (Cycle GAN), which learn efficient mapping from an input\ndomain to a target domain. Our model is designed to enjoy important advantages;\nit is trainable with no need for paired simulation data, flexible to make the\ninput domain visually meaningful, and expandable to rapidly-produce a map with\na larger sky coverage than training data without additional learning. Using\n10,000 lensing simulations, we find that appropriate labeling of training data\nbased on field variance requires the model to exhibit a desired diversity of\nvarious summary statistics for weak lensing mass maps. Compared with a popular\nlog-normal model, our model improves in predicting the statistical natures of\nthree-point correlations and local properties of rare high-density regions. We\nalso demonstrate that our model enables us to produce a continuous map with a\nsky coverage of $\\sim166\\, \\mathrm{deg}^2$ but similar non-Gaussian features to\ntraining data covering $\\sim12\\, \\mathrm{deg}^2$ in a GPU minute. Hence, our\nmodel can be beneficial to massive productions of synthetic weak lensing mass\nmaps, which is of great importance in future precise real-world analyses.",
            "author": [
                "Masato Shirasaki",
                "Shiro Ikeda"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17141v1",
                "http://arxiv.org/pdf/2310.17141v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17140v1",
            "title": "Symbolic Planning and Code Generation for Grounded Dialogue",
            "updated": "2023-10-26T04:22:23Z",
            "published": "2023-10-26T04:22:23Z",
            "summary": "Large language models (LLMs) excel at processing and generating both text and\ncode. However, LLMs have had limited applicability in grounded task-oriented\ndialogue as they are difficult to steer toward task objectives and fail to\nhandle novel grounding. We present a modular and interpretable grounded\ndialogue system that addresses these shortcomings by composing LLMs with a\nsymbolic planner and grounded code execution. Our system consists of a reader\nand planner: the reader leverages an LLM to convert partner utterances into\nexecutable code, calling functions that perform grounding. The translated\ncode's output is stored to track dialogue state, while a symbolic planner\ndetermines the next appropriate response. We evaluate our system's performance\non the demanding OneCommon dialogue task, involving collaborative reference\nresolution on abstract images of scattered dots. Our system substantially\noutperforms the previous state-of-the-art, including improving task success in\nhuman evaluations from 56% to 69% in the most challenging setting.",
            "author": [
                "Justin T. Chiu",
                "Wenting Zhao",
                "Derek Chen",
                "Saujas Vaduguru",
                "Alexander M. Rush",
                "Daniel Fried"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17140v1",
                "http://arxiv.org/pdf/2310.17140v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17137v1",
            "title": "Large-Scale Gaussian Processes via Alternating Projection",
            "updated": "2023-10-26T04:20:36Z",
            "published": "2023-10-26T04:20:36Z",
            "summary": "Gaussian process (GP) hyperparameter optimization requires repeatedly solving\nlinear systems with $n \\times n$ kernel matrices. To address the prohibitive\n$\\mathcal{O}(n^3)$ time complexity, recent work has employed fast iterative\nnumerical methods, like conjugate gradients (CG). However, as datasets increase\nin magnitude, the corresponding kernel matrices become increasingly\nill-conditioned and still require $\\mathcal{O}(n^2)$ space without\npartitioning. Thus, while CG increases the size of datasets GPs can be trained\non, modern datasets reach scales beyond its applicability. In this work, we\npropose an iterative method which only accesses subblocks of the kernel matrix,\neffectively enabling \\emph{mini-batching}. Our algorithm, based on alternating\nprojection, has $\\mathcal{O}(n)$ per-iteration time and space complexity,\nsolving many of the practical challenges of scaling GPs to very large datasets.\nTheoretically, we prove our method enjoys linear convergence and empirically we\ndemonstrate its robustness to ill-conditioning. On large-scale benchmark\ndatasets up to four million datapoints our approach accelerates training by a\nfactor of 2$\\times$ to 27$\\times$ compared to CG.",
            "author": [
                "Kaiwen Wu",
                "Jonathan Wenger",
                "Haydn Jones",
                "Geoff Pleiss",
                "Jacob R. Gardner"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17137v1",
                "http://arxiv.org/pdf/2310.17137v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17136v2",
            "title": "Core Challenge 2023: Solver and Graph Descriptions",
            "updated": "2023-10-27T01:44:07Z",
            "published": "2023-10-26T04:19:17Z",
            "summary": "This paper collects all descriptions of solvers and ISR instances submitted\nto CoRe Challenge 2023.",
            "author": [
                "Takehide Soh",
                "Tomoya Tanjo",
                "Yoshio Okamoto",
                "Takehiro Ito"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17136v2",
                "http://arxiv.org/pdf/2310.17136v2"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL",
                "cs.AI",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17135v1",
            "title": "Comparison of Cross-Entropy, Dice, and Focal Loss for Sea Ice Type\n  Segmentation",
            "updated": "2023-10-26T04:18:00Z",
            "published": "2023-10-26T04:18:00Z",
            "summary": "Up-to-date sea ice charts are crucial for safer navigation in ice-infested\nwaters. Recently, Convolutional Neural Network (CNN) models show the potential\nto accelerate the generation of ice maps for large regions. However, results\nfrom CNN models still need to undergo scrutiny as higher metrics performance\nnot always translate to adequate outputs. Sea ice type classes are imbalanced,\nrequiring special treatment during training. We evaluate how three different\nloss functions, some developed for imbalanced class problems, affect the\nperformance of CNN models trained to predict the dominant ice type in\nSentinel-1 images. Despite the fact that Dice and Focal loss produce higher\nmetrics, results from cross-entropy seem generally more physically consistent.",
            "author": [
                "Rafael Pires de Lima",
                "Behzad Vahedi",
                "Morteza Karimzadeh"
            ],
            "link": [
                "http://dx.doi.org/10.1109/IGARSS52108.2023.10282060",
                "http://arxiv.org/abs/2310.17135v1",
                "http://arxiv.org/pdf/2310.17135v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17133v1",
            "title": "Incorporating Probing Signals into Multimodal Machine Translation via\n  Visual Question-Answering Pairs",
            "updated": "2023-10-26T04:13:49Z",
            "published": "2023-10-26T04:13:49Z",
            "summary": "This paper presents an in-depth study of multimodal machine translation\n(MMT), examining the prevailing understanding that MMT systems exhibit\ndecreased sensitivity to visual information when text inputs are complete.\nInstead, we attribute this phenomenon to insufficient cross-modal interaction,\nrather than image information redundancy. A novel approach is proposed to\ngenerate parallel Visual Question-Answering (VQA) style pairs from the source\ntext, fostering more robust cross-modal interaction. Using Large Language\nModels (LLMs), we explicitly model the probing signal in MMT to convert it into\nVQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask\nlearning framework is introduced to incorporate explicit probing signals from\nthe dataset into the MMT training process. Experimental results on two\nwidely-used benchmarks demonstrate the effectiveness of this novel approach.\nOur code and data would be available at:\n\\url{https://github.com/libeineu/MMT-VQA}.",
            "author": [
                "Yuxin Zuo",
                "Bei Li",
                "Chuanhao Lv",
                "Tong Zheng",
                "Tong Xiao",
                "Jingbo Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17133v1",
                "http://arxiv.org/pdf/2310.17133v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17130v1",
            "title": "M2C: Towards Automatic Multimodal Manga Complement",
            "updated": "2023-10-26T04:10:16Z",
            "published": "2023-10-26T04:10:16Z",
            "summary": "Multimodal manga analysis focuses on enhancing manga understanding with\nvisual and textual features, which has attracted considerable attention from\nboth natural language processing and computer vision communities. Currently,\nmost comics are hand-drawn and prone to problems such as missing pages, text\ncontamination, and aging, resulting in missing comic text content and seriously\nhindering human comprehension. In other words, the Multimodal Manga Complement\n(M2C) task has not been investigated, which aims to handle the aforementioned\nissues by providing a shared semantic space for vision and language\nunderstanding. To this end, we first propose the Multimodal Manga Complement\ntask by establishing a new M2C benchmark dataset covering two languages. First,\nwe design a manga argumentation method called MCoT to mine event knowledge in\ncomics with large language models. Then, an effective baseline FVP-M$^{2}$\nusing fine-grained visual prompts is proposed to support manga complement.\nExtensive experimental results show the effectiveness of FVP-M$^{2}$ method for\nMultimodal Mange Complement.",
            "author": [
                "Hongcheng Guo",
                "Boyang Wang",
                "Jiaqi Bai",
                "Jiaheng Liu",
                "Jian Yang",
                "Zhoujun Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17130v1",
                "http://arxiv.org/pdf/2310.17130v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17128v1",
            "title": "Task-driven Prompt Evolution for Foundation Models",
            "updated": "2023-10-26T04:08:07Z",
            "published": "2023-10-26T04:08:07Z",
            "summary": "Promptable foundation models, particularly Segment Anything Model (SAM), have\nemerged as a promising alternative to the traditional task-specific supervised\nlearning for image segmentation. However, many evaluation studies have found\nthat their performance on medical imaging modalities to be underwhelming\ncompared to conventional deep learning methods. In the world of large\npre-trained language and vision-language models, learning prompt from\ndownstream tasks has achieved considerable success in improving performance. In\nthis work, we propose a plug-and-play Prompt Optimization Technique for\nfoundation models like SAM (SAMPOT) that utilizes the downstream segmentation\ntask to optimize the human-provided prompt to obtain improved performance. We\ndemonstrate the utility of SAMPOT on lung segmentation in chest X-ray images\nand obtain an improvement on a significant number of cases ($\\sim75\\%$) over\nhuman-provided initial prompts. We hope this work will lead to further\ninvestigations in the nascent field of automatic visual prompt-tuning.",
            "author": [
                "Rachana Sathish",
                "Rahul Venkataramani",
                "K S Shriram",
                "Prasad Sudhakar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17128v1",
                "http://arxiv.org/pdf/2310.17128v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17127v1",
            "title": "A Method for Network Intrusion Detection Using Flow Sequence and BERT\n  Framework",
            "updated": "2023-10-26T03:56:40Z",
            "published": "2023-10-26T03:56:40Z",
            "summary": "A Network Intrusion Detection System (NIDS) is a tool that identifies\npotential threats to a network. Recently, different flow-based NIDS designs\nutilizing Machine Learning (ML) algorithms have been proposed as solutions to\ndetect intrusions efficiently. However, conventional ML-based classifiers have\nnot seen widespread adoption in the real world due to their poor domain\nadaptation capability. In this research, our goal is to explore the possibility\nof using sequences of flows to improve the domain adaptation capability of\nnetwork intrusion detection systems. Our proposal employs natural language\nprocessing techniques and Bidirectional Encoder Representations from\nTransformers framework, which is an effective technique for modeling data with\nrespect to its context. Early empirical results show that our approach has\nimproved domain adaptation capability compared to previous approaches. The\nproposed approach provides a new research method for building a robust\nintrusion detection system.",
            "author": [
                "Loc Gia Nguyen",
                "Kohei Watabe"
            ],
            "link": [
                "http://dx.doi.org/10.1109/ICC45041.2023.10279335",
                "http://arxiv.org/abs/2310.17127v1",
                "http://arxiv.org/pdf/2310.17127v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17126v1",
            "title": "Deep Learning on SAR Imagery: Transfer Learning Versus Randomly\n  Initialized Weights",
            "updated": "2023-10-26T03:52:54Z",
            "published": "2023-10-26T03:52:54Z",
            "summary": "Deploying deep learning on Synthetic Aperture Radar (SAR) data is becoming\nmore common for mapping purposes. One such case is sea ice, which is highly\ndynamic and rapidly changes as a result of the combined effect of wind,\ntemperature, and ocean currents. Therefore, frequent mapping of sea ice is\nnecessary to ensure safe marine navigation. However, there is a general\nshortage of expert-labeled data to train deep learning algorithms. Fine-tuning\na pre-trained model on SAR imagery is a potential solution. In this paper, we\ncompare the performance of deep learning models trained from scratch using\nrandomly initialized weights against pre-trained models that we fine-tune for\nthis purpose. Our results show that pre-trained models lead to better results,\nespecially on test samples from the melt season.",
            "author": [
                "Morteza Karimzadeh",
                "Rafael Pires de Lima"
            ],
            "link": [
                "http://dx.doi.org/10.1109/IGARSS52108.2023.10281892",
                "http://arxiv.org/abs/2310.17126v1",
                "http://arxiv.org/pdf/2310.17126v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17125v1",
            "title": "The production of charmonium pentaquark from b-baryon and B-meson decay:\n  SU(3) analysis",
            "updated": "2023-10-26T03:52:37Z",
            "published": "2023-10-26T03:52:37Z",
            "summary": "In this paper, we study the production of charmonium pentaquark $c \\bar c q q\nq$ from bottom baryon and B-meson decays under the flavor SU(3) symmetry. Decay\namplitudes for various processes are parametrized in terms of the SU(3)\nirreducible nonperturbative amplitudes. A number of relations between decay\nwidths have been deduced. Moreover, the strong decays of pentaquark is also\ntaken into account. These results can be tested in future measurements at LHCb,\nBelle II and CEPC. Once a few decay branching fractions have been measured, our\nwork could provide hints for exploring new decay channels or new pentaquark\nstates.",
            "author": [
                "Wei-Hao Han",
                "Ji Xu",
                "Ye Xing"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17125v1",
                "http://arxiv.org/pdf/2310.17125v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17122v1",
            "title": "Enhancing sea ice segmentation in Sentinel-1 images with atrous\n  convolutions",
            "updated": "2023-10-26T03:43:28Z",
            "published": "2023-10-26T03:43:28Z",
            "summary": "Due to the growing volume of remote sensing data and the low latency required\nfor safe marine navigation, machine learning (ML) algorithms are being\ndeveloped to accelerate sea ice chart generation, currently a manual\ninterpretation task. However, the low signal-to-noise ratio of the freely\navailable Sentinel-1 Synthetic Aperture Radar (SAR) imagery, the ambiguity of\nbackscatter signals for ice types, and the scarcity of open-source\nhigh-resolution labelled data makes automating sea ice mapping challenging. We\nuse Extreme Earth version 2, a high-resolution benchmark dataset generated for\nML training and evaluation, to investigate the effectiveness of ML for\nautomated sea ice mapping. Our customized pipeline combines ResNets and Atrous\nSpatial Pyramid Pooling for SAR image segmentation. We investigate the\nperformance of our model for: i) binary classification of sea ice and open\nwater in a segmentation framework; and ii) a multiclass segmentation of five\nsea ice types. For binary ice-water classification, models trained with our\nlargest training set have weighted F1 scores all greater than 0.95 for January\nand July test scenes. Specifically, the median weighted F1 score was 0.98,\nindicating high performance for both months. By comparison, a competitive\nbaseline U-Net has a weighted average F1 score of ranging from 0.92 to 0.94\n(median 0.93) for July, and 0.97 to 0.98 (median 0.97) for January. Multiclass\nice type classification is more challenging, and even though our models achieve\n2% improvement in weighted F1 average compared to the baseline U-Net, test\nweighted F1 is generally between 0.6 and 0.80. Our approach can efficiently\nsegment full SAR scenes in one run, is faster than the baseline U-Net, retains\nspatial resolution and dimension, and is more robust against noise compared to\napproaches that rely on patch classification.",
            "author": [
                "Rafael Pires de Lima",
                "Behzad Vahedi",
                "Nick Hughes",
                "Andrew P. Barrett",
                "Walter Meier",
                "Morteza Karimzadeh"
            ],
            "link": [
                "http://dx.doi.org/10.1080/01431161.2023.2248560",
                "http://arxiv.org/abs/2310.17122v1",
                "http://arxiv.org/pdf/2310.17122v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17121v1",
            "title": "Test-time Augmentation for Factual Probing",
            "updated": "2023-10-26T03:41:32Z",
            "published": "2023-10-26T03:41:32Z",
            "summary": "Factual probing is a method that uses prompts to test if a language model\n\"knows\" certain world knowledge facts. A problem in factual probing is that\nsmall changes to the prompt can lead to large changes in model output. Previous\nwork aimed to alleviate this problem by optimizing prompts via text mining or\nfine-tuning. However, such approaches are relation-specific and do not\ngeneralize to unseen relation types. Here, we propose to use test-time\naugmentation (TTA) as a relation-agnostic method for reducing sensitivity to\nprompt variations by automatically augmenting and ensembling prompts at test\ntime. Experiments show improved model calibration, i.e., with TTA, model\nconfidence better reflects prediction accuracy. Improvements in prediction\naccuracy are observed for some models, but for other models, TTA leads to\ndegradation. Error analysis identifies the difficulty of producing high-quality\nprompt variations as the main challenge for TTA.",
            "author": [
                "Go Kamoda",
                "Benjamin Heinzerling",
                "Keisuke Sakaguchi",
                "Kentaro Inui"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17121v1",
                "http://arxiv.org/pdf/2310.17121v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17120v1",
            "title": "Topic Segmentation of Semi-Structured and Unstructured Conversational\n  Datasets using Language Models",
            "updated": "2023-10-26T03:37:51Z",
            "published": "2023-10-26T03:37:51Z",
            "summary": "Breaking down a document or a conversation into multiple contiguous segments\nbased on its semantic structure is an important and challenging problem in NLP,\nwhich can assist many downstream tasks. However, current works on topic\nsegmentation often focus on segmentation of structured texts. In this paper, we\ncomprehensively analyze the generalization capabilities of state-of-the-art\ntopic segmentation models on unstructured texts. We find that: (a) Current\nstrategies of pre-training on a large corpus of structured text such as\nWiki-727K do not help in transferability to unstructured conversational data.\n(b) Training from scratch with only a relatively small-sized dataset of the\ntarget unstructured domain improves the segmentation results by a significant\nmargin. We stress-test our proposed Topic Segmentation approach by\nexperimenting with multiple loss functions, in order to mitigate effects of\nimbalance in unstructured conversational datasets. Our empirical evaluation\nindicates that Focal Loss function is a robust alternative to Cross-Entropy and\nre-weighted Cross-Entropy loss function when segmenting unstructured and\nsemi-structured chats.",
            "author": [
                "Reshmi Ghosh",
                "Harjeet Singh Kajal",
                "Sharanya Kamath",
                "Dhuri Shrivastava",
                "Samyadeep Basu",
                "Hansi Zeng",
                "Soundararajan Srinivasan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17120v1",
                "http://arxiv.org/pdf/2310.17120v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17119v1",
            "title": "FLEEK: Factual Error Detection and Correction with Evidence Retrieved\n  from External Knowledge",
            "updated": "2023-10-26T03:28:30Z",
            "published": "2023-10-26T03:28:30Z",
            "summary": "Detecting factual errors in textual information, whether generated by large\nlanguage models (LLM) or curated by humans, is crucial for making informed\ndecisions. LLMs' inability to attribute their claims to external knowledge and\ntheir tendency to hallucinate makes it difficult to rely on their responses.\nHumans, too, are prone to factual errors in their writing. Since manual\ndetection and correction of factual errors is labor-intensive, developing an\nautomatic approach can greatly reduce human effort. We present FLEEK, a\nprototype tool that automatically extracts factual claims from text, gathers\nevidence from external knowledge sources, evaluates the factuality of each\nclaim, and suggests revisions for identified errors using the collected\nevidence. Initial empirical evaluation on fact error detection (77-85\\% F1)\nshows the potential of FLEEK. A video demo of FLEEK can be found at\nhttps://youtu.be/NapJFUlkPdQ.",
            "author": [
                "Farima Fatahi Bayat",
                "Kun Qian",
                "Benjamin Han",
                "Yisi Sang",
                "Anton Belyi",
                "Samira Khorshidi",
                "Fei Wu",
                "Ihab F. Ilyas",
                "Yunyao Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17119v1",
                "http://arxiv.org/pdf/2310.17119v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17117v1",
            "title": "The Cumulative Distribution Function Based Method for Random Drift Model",
            "updated": "2023-10-26T03:06:33Z",
            "published": "2023-10-26T03:06:33Z",
            "summary": "In this paper, we propose a numerical method to uniformly handle the random\ngenetic drift model for pure drift with or without natural selection and\nmutation.\n  For pure drift and natural selection case, the Dirac $\\delta$ singularity\nwill develop at two boundary ends and the mass lumped at the two ends stands\nfor the fixation probability. For the one-way mutation case, known as Muller's\nratchet, the accumulation of deleterious mutations leads to the loss of the\nfittest gene, the Dirac $\\delta$ singularity will spike only at one boundary\nend, which stands for the fixation of the deleterious gene and loss of the\nfittest one. For two-way mutation case, the singularity with negative power law\nmay emerge near boundary points. We first rewrite the original model on the\nprobability density function (PDF) to one with respect to the cumulative\ndistribution function (CDF). Dirac $\\delta$ singularity of the PDF becomes the\ndiscontinuity of the CDF. Then we establish a upwind scheme, which keeps the\ntotal probability, is positivity preserving and unconditionally stable. For\npure drift, the scheme also keeps the conservation of expectation. It can catch\nthe discontinuous jump of the CDF, then predicts accurately the fixation\nprobability for pure drift with or without natural selection and one-way\nmutation. For two-way mutation case, it can catch the power law of the\nsingularity. %Moreover, some artificial algorithms or additional boundary\ncriteria is not needed in the numerical simulation. The numerical results show\nthe effectiveness of the scheme.",
            "author": [
                "Chenghua Duan",
                "Chun Liu",
                "Xingye Yue"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17117v1",
                "http://arxiv.org/pdf/2310.17117v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17116v1",
            "title": "Real-time Neonatal Chest Sound Separation using Deep Learning",
            "updated": "2023-10-26T03:05:40Z",
            "published": "2023-10-26T03:05:40Z",
            "summary": "Auscultation for neonates is a simple and non-invasive method of providing\ndiagnosis for cardiovascular and respiratory disease. Such diagnosis often\nrequires high-quality heart and lung sounds to be captured during auscultation.\nHowever, in most cases, obtaining such high-quality sounds is non-trivial due\nto the chest sounds containing a mixture of heart, lung, and noise sounds. As\nsuch, additional preprocessing is needed to separate the chest sounds into\nheart and lung sounds. This paper proposes a novel deep-learning approach to\nseparate such chest sounds into heart and lung sounds. Inspired by the\nConv-TasNet model, the proposed model has an encoder, decoder, and mask\ngenerator. The encoder consists of a 1D convolution model and the decoder\nconsists of a transposed 1D convolution. The mask generator is constructed\nusing stacked 1D convolutions and transformers. The proposed model outperforms\nprevious methods in terms of objective distortion measures by 2.01 dB to 5.06\ndB in the artificial dataset, as well as computation time, with at least a\n17-time improvement. Therefore, our proposed model could be a suitable\npreprocessing step for any phonocardiogram-based health monitoring system.",
            "author": [
                "Yang Yi Poh",
                "Ethan Grooby",
                "Kenneth Tan",
                "Lindsay Zhou",
                "Arrabella King",
                "Ashwin Ramanathan",
                "Atul Malhotra",
                "Mehrtash Harandi",
                "Faezeh Marzbanrad"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17116v1",
                "http://arxiv.org/pdf/2310.17116v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17115v1",
            "title": "Optimal Robotic Assembly Sequence Planning: A Sequential Decision-Making\n  Approach",
            "updated": "2023-10-26T03:01:14Z",
            "published": "2023-10-26T03:01:14Z",
            "summary": "The optimal robot assembly planning problem is challenging due to the\nnecessity of finding the optimal solution amongst an exponentially vast number\nof possible plans, all while satisfying a selection of constraints.\nTraditionally, robotic assembly planning problems have been solved using\nheuristics, but these methods are specific to a given objective structure or\nset of problem parameters. In this paper, we propose a novel approach to\nrobotic assembly planning that poses assembly sequencing as a sequential\ndecision making problem, enabling us to harness methods that far outperform the\nstate-of-the-art. We formulate the problem as a Markov Decision Process (MDP)\nand utilize Dynamic Programming (DP) to find optimal assembly policies for\nmoderately sized strictures. We further expand our framework to exploit the\ndeterministic nature of assembly planning and introduce a class of optimal\nGraph Exploration Assembly Planners (GEAPs). For larger structures, we show how\nReinforcement Learning (RL) enables us to learn policies that generate high\nreward assembly sequences. We evaluate our approach on a variety of robotic\nassembly problems, such as the assembly of the Hubble Space Telescope, the\nInternational Space Station, and the James Webb Space Telescope. We further\nshowcase how our DP, GEAP, and RL implementations are capable of finding\noptimal solutions under a variety of different objective functions and how our\nformulation allows us to translate precedence constraints to branch pruning and\nthus further improve performance. We have published our code at\nhttps://github.com/labicon/ORASP-Code.",
            "author": [
                "Kartik Nagpal",
                "Negar Mehr"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17115v1",
                "http://arxiv.org/pdf/2310.17115v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17112v1",
            "title": "Modeling and Analysis of the Epidemic-Behavior Co-evolution Dynamics\n  with User Irrationality",
            "updated": "2023-10-26T02:50:58Z",
            "published": "2023-10-26T02:50:58Z",
            "summary": "During a public health crisis like COVID-19, individuals' adoption of\nprotective behaviors, such as self-isolation and wearing masks, can\nsignificantly impact the spread of the disease. In the meanwhile, the spread of\nthe disease can also influence individuals' behavioral choices. Moreover, when\nfacing uncertain losses, individuals' decisions tend to be irrational.\nTherefore, it is critical to study individuals' irrational behavior choices in\nthe context of a pandemic. In this paper, we propose an epidemic-behavior\nco-evolution model that captures the dynamic interplay between individual\ndecision-making and disease spread. To account for irrational decision-making,\nwe incorporate the Prospect Theory in our individual behavior modeling. We\nconduct a theoretical analysis of the model, examining the steady states that\nemerge from the co-evolutionary process. We use simulations to validate our\ntheoretical findings and gain further insights. This investigation aims to\nenhance our understanding of the complex dynamics between individual behavior\nand disease spread during a pandemic.",
            "author": [
                "Wenxiang Dong",
                "H. Vicky Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17112v1",
                "http://arxiv.org/pdf/2310.17112v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "q-bio.PE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17676v1",
            "title": "Reducing the impact of non-ideal PRBS on microwave photonic random\n  demodulators by low biasing the optical modulator via PRBS amplitude\n  compression",
            "updated": "2023-10-26T02:39:09Z",
            "published": "2023-10-26T02:39:09Z",
            "summary": "A novel method for reducing the impact of non-ideal pseudo-random binary\nsequence (PRBS) on microwave photonic random demodulators (RDs) in a\nphotonics-assisted compressed sensing (CS) system is proposed. Different from\nthe commonly used method that switches the bias point of the optical modulator\nin the RD between two quadrature transmission points to mix the signal to be\nsampled and the PRBS, this method employs a PRBS with lower amplitude to low\nbias the optical modulator so that the impact of non-ideal PRBS on microwave\nphotonic RDs can be greatly reduced by compressing the amplitude of non-ideal\nparts of the PRBS. An experiment is performed to verify the concept. The\noptical modulator is properly low-biased via PRBS amplitude compression. The\ndata rate and occupied bandwidth of the PRBS are 500 Mb/s and 1 GHz, while the\nmulti-tone signals with a maximum frequency of 100 MHz are sampled at an\nequivalent sampling rate of only 50 MSa/s. The results show that the\nreconstruction error can be reduced by up to 85%. The proposed method can\nsignificantly reduce the requirements for PRBS in RD-based photonics-assisted\nCS systems, providing a feasible solution for reducing the complexity and cost\nof system implementation.",
            "author": [
                "Shiyang Liu",
                "Yang Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17676v1",
                "http://arxiv.org/pdf/2310.17676v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17110v1",
            "title": "LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?",
            "updated": "2023-10-26T02:37:43Z",
            "published": "2023-10-26T02:37:43Z",
            "summary": "In an era marked by the increasing adoption of Large Language Models (LLMs)\nfor various tasks, there is a growing focus on exploring LLMs' capabilities in\nhandling web data, particularly graph data. Dynamic graphs, which capture\ntemporal network evolution patterns, are ubiquitous in real-world web data.\nEvaluating LLMs' competence in understanding spatial-temporal information on\ndynamic graphs is essential for their adoption in web applications, which\nremains unexplored in the literature. In this paper, we bridge the gap via\nproposing to evaluate LLMs' spatial-temporal understanding abilities on dynamic\ngraphs, to the best of our knowledge, for the first time. Specifically, we\npropose the LLM4DyG benchmark, which includes nine specially designed tasks\nconsidering the capability evaluation of LLMs from both temporal and spatial\ndimensions. Then, we conduct extensive experiments to analyze the impacts of\ndifferent data generators, data statistics, prompting techniques, and LLMs on\nthe model performance. Finally, we propose Disentangled Spatial-Temporal\nThoughts (DST2) for LLMs on dynamic graphs to enhance LLMs' spatial-temporal\nunderstanding abilities. Our main observations are: 1) LLMs have preliminary\nspatial-temporal understanding abilities on dynamic graphs, 2) Dynamic graph\ntasks show increasing difficulties for LLMs as the graph size and density\nincrease, while not sensitive to the time span and data generation mechanism,\n3) the proposed DST2 prompting method can help to improve LLMs'\nspatial-temporal understanding abilities on dynamic graphs for most tasks. The\ndata and codes will be open-sourced at publication time.",
            "author": [
                "Zeyang Zhang",
                "Xin Wang",
                "Ziwei Zhang",
                "Haoyang Li",
                "Yijian Qin",
                "Simin Wu",
                "Wenwu Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17110v1",
                "http://arxiv.org/pdf/2310.17110v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17101v1",
            "title": "Multi-Speaker Expressive Speech Synthesis via Semi-supervised\n  Contrastive Learning",
            "updated": "2023-10-26T01:58:38Z",
            "published": "2023-10-26T01:58:38Z",
            "summary": "This paper aims to build an expressive TTS system for multi-speakers,\nsynthesizing a target speaker's speech with multiple styles and emotions. To\nthis end, we propose a novel contrastive learning-based TTS approach to\ntransfer style and emotion across speakers. Specifically, we construct\npositive-negative sample pairs at both utterance and category (such as\nemotion-happy or style-poet or speaker A) levels and leverage contrastive\nlearning to better extract disentangled style, emotion, and speaker\nrepresentations from speech. Furthermore, we introduce a semi-supervised\ntraining strategy to the proposed approach to effectively leverage multi-domain\ndata, including style-labeled data, emotion-labeled data, and unlabeled data.\nWe integrate the learned representations into an improved VITS model, enabling\nit to synthesize expressive speech with diverse styles and emotions for a\ntarget speaker. Experiments on multi-domain data demonstrate the good design of\nour model.",
            "author": [
                "Xinfa Zhu",
                "Yuke Li",
                "Yi Lei",
                "Ning Jiang",
                "Guoqing Zhao",
                "Lei Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17101v1",
                "http://arxiv.org/pdf/2310.17101v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17099v1",
            "title": "A Lower Bound for R(5,6)",
            "updated": "2023-10-26T01:45:04Z",
            "published": "2023-10-26T01:45:04Z",
            "summary": "The known lower bound for the the classical Ramsey number $R(5,6)$ is\nimproved from $58$ to $59$. The method used to construct the graph is a simple\nvariant of computational methods that have been previously used to construct\nRamsey graphs. The new method uses the concurrent programming features of the\n{\\em Go} programming language.",
            "author": [
                "Geoffrey Exoo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17099v1",
                "http://arxiv.org/pdf/2310.17099v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C55 (Primary) 05C15, 05C85 (Secondary)",
                "G.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17086v1",
            "title": "Transformers Learn Higher-Order Optimization Methods for In-Context\n  Learning: A Study with Linear Models",
            "updated": "2023-10-26T01:08:47Z",
            "published": "2023-10-26T01:08:47Z",
            "summary": "Transformers are remarkably good at in-context learning (ICL) -- learning\nfrom demonstrations without parameter updates -- but how they perform ICL\nremains a mystery. Recent work suggests that Transformers may learn in-context\nby internally running Gradient Descent, a first-order optimization method. In\nthis paper, we instead demonstrate that Transformers learn to implement\nhigher-order optimization methods to perform ICL. Focusing on in-context linear\nregression, we show that Transformers learn to implement an algorithm very\nsimilar to Iterative Newton's Method, a higher-order optimization method,\nrather than Gradient Descent. Empirically, we show that predictions from\nsuccessive Transformer layers closely match different iterations of Newton's\nMethod linearly, with each middle layer roughly computing 3 iterations. In\ncontrast, exponentially more Gradient Descent steps are needed to match an\nadditional Transformers layer; this suggests that Transformers have an\ncomparable rate of convergence with high-order methods such as Iterative\nNewton, which are exponentially faster than Gradient Descent. We also show that\nTransformers can learn in-context on ill-conditioned data, a setting where\nGradient Descent struggles but Iterative Newton succeeds. Finally, we show\ntheoretical results which support our empirical findings and have a close\ncorrespondence with them: we prove that Transformers can implement $k$\niterations of Newton's method with $\\mathcal{O}(k)$ layers.",
            "author": [
                "Deqing Fu",
                "Tian-Qi Chen",
                "Robin Jia",
                "Vatsal Sharan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17086v1",
                "http://arxiv.org/pdf/2310.17086v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17084v1",
            "title": "Broadband CPW-based impedance-transformed Josephson parametric amplifier",
            "updated": "2023-10-26T01:04:55Z",
            "published": "2023-10-26T01:04:55Z",
            "summary": "Quantum-limited Josephson parametric amplifiers play a pivotal role in\nadvancing the field of circuit quantum electrodynamics by enabling the fast and\nhigh-fidelity measurement of weak microwave signals. Therefore, it is necessary\nto develop robust parametric amplifiers with low noise, broad bandwidth, and\nreduced design complexity for microwave detection. However, current broadband\nparametric amplifiers either have degraded noise performance or rely on complex\ndesigns. Here, we present a device based on the broadband impedance-transformed\nJosephson parametric amplifier (IMPA) that integrates a horn-like coplanar\nwaveguide (CPW) transmission line, which significantly decreases the design and\nfabrication complexity, while keeping comparable performance. The device shows\nan instantaneous bandwidth of 700(200) MHz for 15(20) dB gain with an average\nsaturation power of -110 dBm and near quantum-limited added noise. The\noperating frequency can be tuned over 1.4 GHz using an external flux bias. We\nfurther demonstrate the negligible back-action from our device on a transmon\nqubit. The amplification performance and simplicity of our device promise its\nwide adaptation in quantum metrology, quantum communication, and quantum\ninformation processing.",
            "author": [
                "Bingcheng Qing",
                "Long B. Nguyen",
                "Xinyu Liu",
                "Hengjiang Ren",
                "William P. Livingston",
                "Noah Goss",
                "Ahmed Hajr",
                "Trevor Chistolini",
                "Zahra Pedramrazi",
                "David I. Santiago",
                "Jie Luo",
                "Irfan Siddiqi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17084v1",
                "http://arxiv.org/pdf/2310.17084v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17076v1",
            "title": "Vison crystal in quantum spin ice on the breathing pyrochlore lattice",
            "updated": "2023-10-26T00:37:07Z",
            "published": "2023-10-26T00:37:07Z",
            "summary": "Recent excitement in the quantum spin ice community has come from the\nexperimental discovery of pseudospin-$1/2$ breathing pyrochlores, including\nBa$_3$Yb$_2$Zn$_5$O$_{11}$, in which inversion symmetry is broken by the `up'\nand `down' tetrahedra taking different physical sizes. We show here that the\noften-neglected $J_{z\\pm}$ coupling between Kramers ions, in combination with\nthe breathing nature of the lattice, can produce an imaginary ring flip term.\nThis can lead to an unconventional '$U(1)_{\\pi/2}$ phase', corresponding to a\nmaximally dense packing of visons on the lattice. Coherent dynamics persists in\nall phases, together with its emergent QED description, in a manner reminiscent\nof fragmentation in spinon crystals. We characterize the enlarged QSI phase\ndiagram and its excitations, showing that the imaginary ring flip acts both as\na chemical potential for visons and as an effective three-photon vertex akin to\nstrong light-matter coupling. The novel coupling causes a structured\nhigh-energy continuum to emerge above the photon dispersion, which is naturally\ninterpreted as three photon up-conversion in a nonlinear optical crystal.",
            "author": [
                "Alaric Sanders",
                "Claudio Castelnovo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17076v1",
                "http://arxiv.org/pdf/2310.17076v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17073v1",
            "title": "Quantum criticality at cryogenic melting of polar bubble lattices",
            "updated": "2023-10-26T00:32:21Z",
            "published": "2023-10-26T00:32:21Z",
            "summary": "Quantum fluctuations (QFs) caused by zero-point phonon vibrations (ZPPVs) are\nknown to prevent the occurrence of polar phases in bulk incipient\nferroelectrics down to 0K1-3. On the other hand, little is known about the\neffects of QFs on the recently discovered topological patterns in ferroelectric\nnanostructures4-9. Here, by using an atomistic effective Hamiltonian within\nclassical Monte Carlo (CMC) and path integral quantum Monte Carlo\n(PI-QMC)1,3,10,11, we unveil how QFs affect the topology of several dipolar\nphases in ultrathin Pb(Zr0.4Ti0.6)O3 (PZT) films. In particular, our PI-QMC\nsimulations show that the ZPPVs do not suppress polar patterns but rather\nstabilize the labyrinth4, bimeron5 and bubble phases12,13 within a wider range\nof bias field magnitudes. Moreover, we reveal that quantum fluctuations induce\na quantum critical point (QCP) separating a hexagonal bubble lattice from a\nliquid-like state characterized by spontaneous motion, creation and\nannihilation of polar bubbles at cryogenic temperatures. Finally, we show that\nthe discovered quantum melting is associated with anomalous physical response,\nas, e.g., demonstrated by a negative longitudinal piezoelectric coefficient.",
            "author": [
                "W. Luo",
                "A. Akbarzadeh",
                "Y. Nahas",
                "S. Prokhorenko",
                "L. Bellaiche"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17073v1",
                "http://arxiv.org/pdf/2310.17073v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cond-mat.mes-hall",
                "physics.comp-ph",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17066v1",
            "title": "Multiple-Carrier-Lifetime Model for Carrier Dynamics in InGaN/GaN LEDs\n  with Non-Uniform Carrier Distribution",
            "updated": "2023-10-26T00:01:17Z",
            "published": "2023-10-26T00:01:17Z",
            "summary": "We introduce a multiple-carrier-lifetime model (MCLM) for light-emitting\ndiodes (LEDs) with non-uniform carrier distribution, such as in\nmultiple-quantum-well (MQW) structures. By employing the MCLM, we successfully\nexplain the modulation response of V-pit engineered MQW LEDs, which exhibit an\nS21 roll-off slower than -20 dB/decade. Using the proposed model and employing\na gradient descent method, we extract effective recombination and escape\nlifetimes by averaging the carrier behavior across the quantum wells. Our\nresults reveal slower effective carrier recombination and escape in MQW LEDs\ncompared with LEDs emitting from a single QW, indicating the advantages of\nlower carrier density achieved through V-pit engineering. Notably, the\neffective carrier recombination time is more than one order of magnitude lower\nthan the effective escape lifetime, suggesting that most carriers in the\nquantum wells recombine, while the escape process remains weak. To ensure the\nreliability and robustness of the MCLM, we subject it to a comprehensive\nthree-fold validation process. This work confirms the positive impact of\nspreading carriers into several QWs through V-pit engineering. In addition, the\nMCLM is applicable to other LEDs with non-uniform carrier distribution, such as\nmicro-LEDs with significant surface recombination and non-uniform lateral\ncarrier profiles.",
            "author": [
                "Xuefeng Li",
                "Elizabeth DeJong",
                "Rob Armitage",
                "Daniel Feezell"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17066v1",
                "http://arxiv.org/pdf/2310.17066v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17064v1",
            "title": "math-PVS: A Large Language Model Framework to Map Scientific\n  Publications to PVS Theories",
            "updated": "2023-10-25T23:54:04Z",
            "published": "2023-10-25T23:54:04Z",
            "summary": "As artificial intelligence (AI) gains greater adoption in a wide variety of\napplications, it has immense potential to contribute to mathematical discovery,\nby guiding conjecture generation, constructing counterexamples, assisting in\nformalizing mathematics, and discovering connections between different\nmathematical areas, to name a few.\n  While prior work has leveraged computers for exhaustive mathematical proof\nsearch, recent efforts based on large language models (LLMs) aspire to position\ncomputing platforms as co-contributors in the mathematical research process.\nDespite their current limitations in logic and mathematical tasks, there is\ngrowing interest in melding theorem proving systems with foundation models.\nThis work investigates the applicability of LLMs in formalizing advanced\nmathematical concepts and proposes a framework that can critically review and\ncheck mathematical reasoning in research papers. Given the noted reasoning\nshortcomings of LLMs, our approach synergizes the capabilities of proof\nassistants, specifically PVS, with LLMs, enabling a bridge between textual\ndescriptions in academic papers and formal specifications in PVS. By harnessing\nthe PVS environment, coupled with data ingestion and conversion mechanisms, we\nenvision an automated process, called \\emph{math-PVS}, to extract and formalize\nmathematical theorems from research papers, offering an innovative tool for\nacademic review and discovery.",
            "author": [
                "Hassen Saidi",
                "Susmit Jha",
                "Tuhin Sahai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17064v1",
                "http://arxiv.org/pdf/2310.17064v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17062v1",
            "title": "An Open, Programmable, Multi-vendor 5G O-RAN Testbed with NVIDIA ARC and\n  OpenAirInterface",
            "updated": "2023-10-25T23:48:41Z",
            "published": "2023-10-25T23:48:41Z",
            "summary": "The transition of fifth generation (5G) cellular systems to softwarized,\nprogrammable, and intelligent networks depends on successfully enabling public\nand private 5G deployments that are (i) fully software-driven and (ii) with a\nperformance at par with that of traditional monolithic systems. This requires\nhardware acceleration to scale the Physical (PHY) layer performance, end-to-end\nintegration and testing, and careful planning of the Radio Frequency (RF)\nenvironment. In this paper, we describe how the X5G testbed at Northeastern\nUniversity has addressed these challenges through the first 8-node network\ndeployment of the NVIDIA Aerial Research Cloud (ARC), with the Aerial SDK for\nthe PHY layer, accelerated on Graphics Processing Unit (GPU), and through its\nintegration with higher layers from the OpenAirInterface (OAI) open-source\nproject through the Small Cell Forum Functional Application Platform Interface\n(FAPI). We discuss software integration, the network infrastructure, and a\ndigital twin framework for RF planning. We then profile the performance with up\nto 4 Commercial Off-the-Shelf (COTS) smartphones for each base station with\niPerf and video streaming applications, measuring a cell rate higher than 500\nMbps in downlink and 45 Mbps in uplink.",
            "author": [
                "Davide Villa",
                "Imran Khan",
                "Florian Kaltenberger",
                "Nicholas Hedberg",
                "Ruben Soares da Silva",
                "Anupa Kelkar",
                "Chris Dick",
                "Stefano Basagni",
                "Josep M. Jornet",
                "Tommaso Melodia",
                "Michele Polese",
                "Dimitrios Koutsonikolas"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17062v1",
                "http://arxiv.org/pdf/2310.17062v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17061v2",
            "title": "Does canonical quantization lead to GKSL dynamics?",
            "updated": "2023-10-30T16:17:31Z",
            "published": "2023-10-25T23:48:20Z",
            "summary": "We introduce a generalized classical model of Brownian motion for describing\nthermal relaxation processes which is thermodynamically consistent. Applying\nthe canonical quantization to this model, a quantum equation for the density\noperator is obtained. This equation has a thermal equilibrium state as its\nstationary solution, but the time evolution is not necessarily a Completely\nPositive and Trace-Preserving (CPTP) map. In the application to the harmonic\noscillator potential, however, the requirement of the CPTP map is shown to be\nsatisfied by choosing parameters appropriately and then our equation reproduces\na Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) equation satisfying the detailed\nbalance condition. This result suggests a quantum-classical correspondence in\nthermal relaxation processes and will provide a new insight to the study of\ndecoherence.",
            "author": [
                "T. Koide",
                "F. Nicacio"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17061v2",
                "http://arxiv.org/pdf/2310.17061v2"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.stat-mech",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17060v1",
            "title": "Aplicacion de Robots Humanoides como Guias Interactivos en Museos: Una\n  Simulacion con el Robot NAO",
            "updated": "2023-10-25T23:44:09Z",
            "published": "2023-10-25T23:44:09Z",
            "summary": "This article presents an application that evaluates the feasibility of\nhumanoid robots as interactive guides in art museums. The application entailes\nprogramming a NAO robot and a chatbot to provide information about art pieces\nin a simulated museum environment. In this controlled scenario, the learning\nemployees interact with the robot and the chatbot. The result is a skilled\nparticipation in the interactions, along with the effectiveness of the robot\nand chatbot that communicates the basic details of the art objects. You see\nnatural and fluid interactions between the students and the robot. This\nsuggests that the addition of humanoid robots to museums may provide a better\nexperience for visitors, but also the need to continue to do more to optimize\nthe quality of interaction. This study contributes to understanding the\npossibilities and requirements of applying humanoid technologies in a cultural\ncontext.",
            "author": [
                "Hiago Sodre",
                "Pablo Moraes",
                "Monica Rodriguez",
                "Victor Castelli",
                "Pamela Barboza",
                "Martin Mattos",
                "Guillermo Vivas",
                "Bruna de Vargas",
                "Tobias D\u00f6rnbach",
                "Ricardo Grando"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17060v1",
                "http://arxiv.org/pdf/2310.17060v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17058v1",
            "title": "Diseno y Desarrollo de Prototipos Roboticos para Competencias de Futbol\n  utilizando Motores Dynamixel",
            "updated": "2023-10-25T23:39:51Z",
            "published": "2023-10-25T23:39:51Z",
            "summary": "This article describes the design and development of robotic prototypes for\nrobotic soccer competitions using Dynamixel motors. Although the prototypes are\nnot aimed at world-class competitions, they represent a significant step in the\ndevelopment of sports robots. Model XL430-W250 Dynamixel motors were chosen and\nelectronic circuits were implemented using control boards such as OpenCR and\nRaspberry Pi 3. A crucial component was introduced: a step-up board that\ncharges a capacitor to create a powerful kick to the ball via anelectromagnet\ncontrolled by Arduino Nano. The programming and coordination of the prototypes\nwas carried out using the ROS environment (Robot Operating System), which\nallows effective integration of movements and communication. Although the\nprototypes were not optimized for global competition, they underwent extensive\ntesting, evaluating their speed and maneuverability, as well as soccer tactics\nin the GRSim simulator. These prototypes contribute to the further development\nof sports robotics and illustrate the research potential in this exciting area.",
            "author": [
                "Pablo Moraes",
                "Hiago Sodre",
                "Monica Rodriguez",
                "Andre Kelbouscas",
                "Jean Schuster",
                "Cristiano Schuster",
                "Ricardo Grando"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17058v1",
                "http://arxiv.org/pdf/2310.17058v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17054v1",
            "title": "BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs'\n  Generation",
            "updated": "2023-10-25T23:32:12Z",
            "published": "2023-10-25T23:32:12Z",
            "summary": "Large language models (LLMs) such as GPT-3 have demonstrated a strong\ncapability to generate coherent and contextually relevant text. However, amidst\ntheir successes, a crucial issue persists: their generated outputs still lack\ncommonsense at times. Moreover, fine-tuning the entire LLM towards more\ncommonsensical outputs is computationally expensive if not infeasible. In this\npaper, we present a computation-efficient framework that steers a frozen\nPre-Trained Language Model (PTLM) towards more commonsensical generation (i.e.,\nproducing a plausible output that incorporates a list of concepts in a\nmeaningful way). Specifically, we first construct a reference-free evaluator\nthat assigns a sentence with a commonsensical score by grounding the sentence\nto a dynamic commonsense knowledge base from four different relational aspects.\nWe then use the scorer as the oracle for commonsense knowledge, and extend the\ncontrollable generation method called NADO to train an auxiliary head that\nguides a fixed PTLM to better satisfy the oracle. We test our framework on a\nseries of GPT-2-, Flan-T5-, and Alpaca-based language models (LMs) on two\nconstrained concept-to-sentence benchmarks. Human evaluation results\ndemonstrate that our method consistently leads to the most commonsensical\noutputs.",
            "author": [
                "Yufei Tian",
                "Felix Zhang",
                "Nanyun Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17054v1",
                "http://arxiv.org/pdf/2310.17054v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17050v1",
            "title": "Exploring Question Decomposition for Zero-Shot VQA",
            "updated": "2023-10-25T23:23:57Z",
            "published": "2023-10-25T23:23:57Z",
            "summary": "Visual question answering (VQA) has traditionally been treated as a\nsingle-step task where each question receives the same amount of effort, unlike\nnatural human question-answering strategies. We explore a question\ndecomposition strategy for VQA to overcome this limitation. We probe the\nability of recently developed large vision-language models to use human-written\ndecompositions and produce their own decompositions of visual questions,\nfinding they are capable of learning both tasks from demonstrations alone.\nHowever, we show that naive application of model-written decompositions can\nhurt performance. We introduce a model-driven selective decomposition approach\nfor second-guessing predictions and correcting errors, and validate its\neffectiveness on eight VQA tasks across three domains, showing consistent\nimprovements in accuracy, including improvements of >20% on medical VQA\ndatasets and boosting the zero-shot performance of BLIP-2 above chance on a VQA\nreformulation of the challenging Winoground task. Project Site:\nhttps://zaidkhan.me/decomposition-0shot-vqa/",
            "author": [
                "Zaid Khan",
                "Vijay Kumar BG",
                "Samuel Schulter",
                "Manmohan Chandraker",
                "Yun Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17050v1",
                "http://arxiv.org/pdf/2310.17050v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17675v1",
            "title": "Early Detection of Tuberculosis with Machine Learning Cough Audio\n  Analysis: Towards More Accessible Global Triaging Usage",
            "updated": "2023-10-25T23:22:20Z",
            "published": "2023-10-25T23:22:20Z",
            "summary": "Tuberculosis (TB), a bacterial disease mainly affecting the lungs, is one of\nthe leading infectious causes of mortality worldwide. To prevent TB from\nspreading within the body, which causes life-threatening complications, timely\nand effective anti-TB treatment is crucial. Cough, an objective biomarker for\nTB, is a triage tool that monitors treatment response and regresses with\nsuccessful therapy. Current gold standards for TB diagnosis are slow or\ninaccessible, especially in rural areas where TB is most prevalent. In\naddition, current machine learning (ML) diagnosis research, like utilizing\nchest radiographs, is ineffective and does not monitor treatment progression.\nTo enable effective diagnosis, an ensemble model was developed that analyzes,\nusing a novel ML architecture, coughs' acoustic epidemiologies from\nsmartphones' microphones to detect TB. The architecture includes a 2D-CNN and\nXGBoost that was trained on 724,964 cough audio samples and demographics from 7\ncountries. After feature extraction (Mel-spectrograms) and data augmentation\n(IR-convolution), the model achieved AUROC (area under the receiving operator\ncharacteristic) of 88%, surpassing WHO's requirements for screening tests. The\nresults are available within 15 seconds and can easily be accessible via a\nmobile app. This research helps to improve TB diagnosis through a promising\naccurate, quick, and accessible triaging tool.",
            "author": [
                "Chandra Suda"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17675v1",
                "http://arxiv.org/pdf/2310.17675v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.LG",
                "cs.SD",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17049v1",
            "title": "Learning Repeatable Speech Embeddings Using An Intra-class Correlation\n  Regularizer",
            "updated": "2023-10-25T23:21:46Z",
            "published": "2023-10-25T23:21:46Z",
            "summary": "A good supervised embedding for a specific machine learning task is only\nsensitive to changes in the label of interest and is invariant to other\nconfounding factors. We leverage the concept of repeatability from measurement\ntheory to describe this property and propose to use the intra-class correlation\ncoefficient (ICC) to evaluate the repeatability of embeddings. We then propose\na novel regularizer, the ICC regularizer, as a complementary component for\ncontrastive losses to guide deep neural networks to produce embeddings with\nhigher repeatability. We use simulated data to explain why the ICC regularizer\nworks better on minimizing the intra-class variance than the contrastive loss\nalone. We implement the ICC regularizer and apply it to three speech tasks:\nspeaker verification, voice style conversion, and a clinical application for\ndetecting dysphonic voice. The experimental results demonstrate that adding an\nICC regularizer can improve the repeatability of learned embeddings compared to\nonly using the contrastive loss; further, these embeddings lead to improved\nperformance in these downstream tasks.",
            "author": [
                "Jianwei Zhang",
                "Suren Jayasuriya",
                "Visar Berisha"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17049v1",
                "http://arxiv.org/pdf/2310.17049v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.00715v1",
            "title": "From Basics to Frontiers: A Comprehensive Review of Plasma-Modified and\n  Plasma-Synthesized Polymer Films",
            "updated": "2023-10-25T23:19:24Z",
            "published": "2023-10-25T23:19:24Z",
            "summary": "This comprehensive review begins by tracing the historical development and\nprogress of cold plasma technology as an innovative approach to polymer\nengineering. The study emphasizes the versatility of cold plasma derived from a\nvariety of sources including low-pressure glow discharges (e.g., radiofrequency\ncapacitively coupled plasmas) and atmospheric pressure plasmas (e.g.,\ndielectric barrier devices, piezoelectric plasmas). It critically examines key\noperational parameters such as reduced electric field, pressure, discharge\ntype, gas type and flow rate, substrate temperature, gap, and how these\nvariables affect the properties of the synthesized or modified polymers. This\nreview also discusses the application of cold plasma in polymer surface\nmodification, underscoring how changes in surface properties (e.g.,\nwettability, adhesion, biocompatibility) can be achieved by controlling various\nsurface processes (etching, roughening, crosslinking, functionalization,\ncrystallinity). A detailed examination of Plasma-Enhanced Chemical Vapor\nDeposition (PECVD) reveals its efficacy in producing thin polymeric films from\nan array of precursors. Yasuda's models, Rapid Step-Growth Polymerization\n(RSGP) and Competitive Ablation Polymerization (CAP), are explained as\nfundamental mechanisms underpinning plasma-assisted deposition and\npolymerization processes. Then, the wide array of applications of cold plasma\ntechnology is explored, from the biomedical field, where it is used in creating\nsmart drug delivery systems and biodegradable polymer implants, to its role in\nenhancing the performance of membrane-based filtration systems crucial for\nwater purification, gas separation, and energy production. It investigates the\npotential for improving the properties of bioplastics and the exciting\nprospects for developing self-healing materials using this technology.",
            "author": [
                "Thierry Dufour"
            ],
            "link": [
                "http://dx.doi.org/10.3390/polym15173607",
                "http://arxiv.org/abs/2311.00715v1",
                "http://arxiv.org/pdf/2311.00715v1"
            ],
            "primary_category": "physics.plasm-ph",
            "category": [
                "physics.plasm-ph",
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.00714v1",
            "title": "Release of Arabidopsis seed dormancy by cold atmospheric plasma relies\n  on cytoplasmic glass transition",
            "updated": "2023-10-25T23:13:38Z",
            "published": "2023-10-25T23:13:38Z",
            "summary": "When mature Arabidopsis thaliana seeds are dormant, their germination is\nprevented in apparently favourable conditions. This primary dormancy can be\nreleased during seed dry storage through a process called after-ripening whose\nduration can last several months. To reduce this delay, cold atmospheric\nplasmas (CAP) can be used as sources of reactive oxygen species capable of\ninducing heterogeneous chemical reactions. While CAP are known to stimulate the\ngermination of various seed species, the relationship between CAP treatments\nand the amorphous solid state of dry seeds remains unexplored. Here, we\ndemonstrate that seed dormancy can be alleviated using a cold plasma of ambient\nair and that this alleviation can be amplified for seeds with high\nwater-content (typically 30 %DW) or seeds heated at 60 {\\deg}C during plasma\ntreatment. Differential scanning micro-calorimetry shows that these\ncharacteristics control the glassy/rubbery state of the seed cytoplasm. This\ntechnique indicates also that a glass transition to the rubbery state\nstrengthens the CAP effects to alleviate seed dormancy. We propose that lower\ncytoplasmic viscosity can promote the oxidative signaling induced by CAP which,\nin turn, improves the germination process.",
            "author": [
                "Jonas August",
                "Thierry Dufour",
                "Christophe Bailly"
            ],
            "link": [
                "http://dx.doi.org/10.1088/1361-6463/ace36e",
                "http://arxiv.org/abs/2311.00714v1",
                "http://arxiv.org/pdf/2311.00714v1"
            ],
            "primary_category": "physics.plasm-ph",
            "category": [
                "physics.plasm-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17048v2",
            "title": "Probing 3D magnetic fields using thermal dust polarization and grain\n  alignment theory",
            "updated": "2023-11-21T05:33:24Z",
            "published": "2023-10-25T23:10:51Z",
            "summary": "Magnetic fields are ubiquitous in the universe and are thought to play an\nimportant role in various astrophysical processes. Polarization of thermal dust\nemission from dust grains aligned with the magnetic field is widely used to\nmeasure the two-dimensional magnetic field projected onto the plane of the sky\n(POS), but the component along the line of sight (LOS) is not yet reliably\nconstrained with dust polarization. Here, we introduce a new method to infer\nthree-dimensional (3D) magnetic fields using thermal dust polarization and\ngrain alignment physics. We first develop a physical model of thermal dust\npolarization using the modern grain alignment theory based on the magnetically\nenhanced radiative torque (MRAT) alignment theory. We then test this model with\nsynthetic observations of magnetohydrodynamic (MHD) simulations of a\nfilamentary cloud with our updated POLARIS code. Combining the tested physical\npolarization model with synthetic polarization, we show that the B-field\ninclination angle can be accurately constrained by the polarization degree from\nsynthetic observations. Compared to the true 3D magnetic fields, our method\nwith grain alignment is more accurate than the previous methods that assume\nuniform grain alignment. This new technique paves the way for tracing 3D\nB-fields using thermal dust polarization and grain alignment theory and for\nconstraining dust properties and grain alignment physics.",
            "author": [
                "Thiem Hoang",
                "Bao Truong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17048v2",
                "http://arxiv.org/pdf/2310.17048v2"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.CO",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17041v1",
            "title": "On Surgical Fine-tuning for Language Encoders",
            "updated": "2023-10-25T22:42:30Z",
            "published": "2023-10-25T22:42:30Z",
            "summary": "Fine-tuning all the layers of a pre-trained neural language encoder (either\nusing all the parameters or using parameter-efficient methods) is often the\nde-facto way of adapting it to a new task. We show evidence that for different\ndownstream language tasks, fine-tuning only a subset of layers is sufficient to\nobtain performance that is close to and often better than fine-tuning all the\nlayers in the language encoder. We propose an efficient metric based on the\ndiagonal of the Fisher information matrix (FIM score), to select the candidate\nlayers for selective fine-tuning. We show, empirically on GLUE and SuperGLUE\ntasks and across distinct language encoders, that this metric can effectively\nselect layers leading to a strong downstream performance. Our work highlights\nthat task-specific information corresponding to a given downstream task is\noften localized within a few layers, and tuning only those is sufficient for\nstrong performance. Additionally, we demonstrate the robustness of the FIM\nscore to rank layers in a manner that remains constant during the optimization\nprocess.",
            "author": [
                "Abhilasha Lodha",
                "Gayatri Belapurkar",
                "Saloni Chalkapurkar",
                "Yuanming Tao",
                "Reshmi Ghosh",
                "Samyadeep Basu",
                "Dmitrii Petrov",
                "Soundararajan Srinivasan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17041v1",
                "http://arxiv.org/pdf/2310.17041v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18369v1",
            "title": "Apollo: Zero-shot MultiModal Reasoning with Multiple Experts",
            "updated": "2023-10-25T22:36:40Z",
            "published": "2023-10-25T22:36:40Z",
            "summary": "We propose a modular framework that leverages the expertise of different\nfoundation models over different modalities and domains in order to perform a\nsingle, complex, multi-modal task, without relying on prompt engineering or\notherwise tailor-made multi-modal training. Our approach enables decentralized\ncommand execution and allows each model to both contribute and benefit from the\nexpertise of the other models. Our method can be extended to a variety of\nfoundation models (including audio and vision), above and beyond only language\nmodels, as it does not depend on prompts. We demonstrate our approach on two\ntasks. On the well-known task of stylized image captioning, our experiments\nshow that our approach outperforms semi-supervised state-of-the-art models,\nwhile being zero-shot and avoiding costly training, data collection, and prompt\nengineering. We further demonstrate this method on a novel task, audio-aware\nimage captioning, in which an image and audio are given and the task is to\ngenerate text that describes the image within the context of the provided\naudio. Our code is available on GitHub.",
            "author": [
                "Daniela Ben-David",
                "Tzuf Paz-Argaman",
                "Reut Tsarfaty"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18369v1",
                "http://arxiv.org/pdf/2310.18369v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CV",
                "I.2.7; I.5.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17038v1",
            "title": "Exponential equivalence for misanthrope processes in contact with weak\n  reservoirs and applications to totally asymmetric exclusion processes",
            "updated": "2023-10-25T22:30:36Z",
            "published": "2023-10-25T22:30:36Z",
            "summary": "We provide a short proof for the exponential equivalence between misanthrope\nprocesses in contact with weak reservoirs and those with impermeable\nboundaries. As a consequence, we can derive both the hydrodynamic limit and the\nlarge deviations of the totally asymmetric exclusion process (TASEP) in contact\nwith weak reservoirs. This extends a recent result which proved the\nhydrodynamic behaviour of a vanishing viscocity approximation of the TASEP in\ncontact with weak reservoirs. Furthermore, applications to a class of\nasymmetric exclusion processes with long jumps is discussed.",
            "author": [
                "Julian Kern"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17038v1",
                "http://arxiv.org/pdf/2310.17038v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "60K35, 60J27"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17037v1",
            "title": "Event-by-event Comparison between Machine-Learning- and\n  Transfer-Matrix-based Unfolding Methods",
            "updated": "2023-10-25T22:28:04Z",
            "published": "2023-10-25T22:28:04Z",
            "summary": "The unfolding of detector effects is a key aspect of comparing experimental\ndata with theoretical predictions. In recent years, different Machine-Learning\nmethods have been developed to provide novel features, e.g. high dimensionality\nor a probabilistic single-event unfolding based on generative neural networks.\nTraditionally, many analyses unfold detector effects using\ntransfer-matrix--based algorithms, which are well established in\nlow-dimensional unfolding. They yield an unfolded distribution of the total\nspectrum, together with its covariance matrix. This paper proposes a method to\nobtain probabilistic single-event unfolded distributions, together with their\nuncertainties and correlations, for the transfer-matrix--based unfolding. The\nalgorithm is first validated on a toy model and then applied to pseudo-data for\nthe $pp\\rightarrow Z\\gamma \\gamma$ process. In both examples the performance is\ncompared to the single-event unfolding of the Machine-Learning--based Iterative\ncINN unfolding (IcINN).",
            "author": [
                "Mathias Backes",
                "Anja Butter",
                "Monica Dunford",
                "Bogdan Malaescu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17037v1",
                "http://arxiv.org/pdf/2310.17037v1"
            ],
            "primary_category": "physics.data-an",
            "category": [
                "physics.data-an",
                "hep-ex",
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17034v1",
            "title": "Follow-on Question Suggestion via Voice Hints for Voice Assistants",
            "updated": "2023-10-25T22:22:18Z",
            "published": "2023-10-25T22:22:18Z",
            "summary": "The adoption of voice assistants like Alexa or Siri has grown rapidly,\nallowing users to instantly access information via voice search. Query\nsuggestion is a standard feature of screen-based search experiences, allowing\nusers to explore additional topics. However, this is not trivial to implement\nin voice-based settings. To enable this, we tackle the novel task of suggesting\nquestions with compact and natural voice hints to allow users to ask follow-up\nquestions.\n  We define the task, ground it in syntactic theory and outline linguistic\ndesiderata for spoken hints. We propose baselines and an approach using\nsequence-to-sequence Transformers to generate spoken hints from a list of\nquestions. Using a new dataset of 6681 input questions and human written hints,\nwe evaluated the models with automatic metrics and human evaluation. Results\nshow that a naive approach of concatenating suggested questions creates poor\nvoice hints. Our approach, which applies a linguistically-motivated pretraining\ntask was strongly preferred by humans for producing the most natural hints.",
            "author": [
                "Besnik Fetahu",
                "Pedro Faustini",
                "Giuseppe Castellucci",
                "Anjie Fang",
                "Oleg Rokhlenko",
                "Shervin Malmasi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17034v1",
                "http://arxiv.org/pdf/2310.17034v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17029v1",
            "title": "Toward the use of proxies for efficient learning manipulation and\n  locomotion strategies on soft robots",
            "updated": "2023-10-25T22:12:58Z",
            "published": "2023-10-25T22:12:58Z",
            "summary": "Soft robots are naturally designed to perform safe interactions with their\nenvironment, like locomotion and manipulation. In the literature, there are now\nmany concepts, often bio-inspired, to propose new modes of locomotion or\ngrasping. However, a methodology for implementing motion planning of these\ntasks, as exists for rigid robots, is still lacking. One of the difficulties\ncomes from the modeling of these robots, which is very different, as it is\nbased on the mechanics of deformable bodies. These models, whose dimension is\noften very large, make learning and optimization methods very costly. In this\npaper, we propose a proxy approach, as exists for humanoid robotics. This proxy\nis a simplified model of the robot that enables frugal learning of a motion\nstrategy. This strategy is then transferred to the complete model to obtain the\ncorresponding actuation inputs. Our methodology is illustrated and analyzed on\ntwo classical designs of soft robots doing manipulation and locomotion tasks.",
            "author": [
                "Etienne M\u00e9nager",
                "Quentin Peyron",
                "Christian Duriez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17029v1",
                "http://arxiv.org/pdf/2310.17029v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17024v1",
            "title": "SPLUS J142445.34-254247.1: An R-Process Enhanced, Actinide-Boost,\n  Extremely Metal-Poor star observed with GHOST",
            "updated": "2023-10-25T22:03:29Z",
            "published": "2023-10-25T22:03:29Z",
            "summary": "We report on the chemo-dynamical analysis of SPLUS J142445.34-254247.1, an\nextremely metal-poor halo star enhanced in elements formed by the rapid\nneutron-capture process. This star was first selected as a metal-poor candidate\nfrom its narrow-band S-PLUS photometry and followed up spectroscopically in\nmedium-resolution with Gemini South/GMOS, which confirmed its low-metallicity\nstatus. High-resolution spectroscopy was gathered with GHOST at Gemini South,\nallowing for the determination of chemical abundances for 36 elements, from\ncarbon to thorium. At [Fe/H]=-3.39, SPLUS J1424-2542 is one of the lowest\nmetallicity stars with measured Th and has the highest logeps(Th/Eu) observed\nto date, making it part of the \"actinide-boost\" category of r-process enhanced\nstars. The analysis presented here suggests that the gas cloud from which SPLUS\nJ1424-2542 was formed must have been enriched by at least two progenitor\npopulations. The light-element (Z<=30) abundance pattern is consistent with the\nyields from a supernova explosion of metal-free stars with 11.3-13.4 Msun, and\nthe heavy-element (Z>=38) abundance pattern can be reproduced by the yields\nfrom a neutron star merger (1.66Msun and 1.27Msun) event. A kinematical\nanalysis also reveals that SPLUS J1424-2542 is a low-mass, old halo star with a\nlikely in-situ origin, not associated with any known early merger events in the\nMilky Way.",
            "author": [
                "Vinicius M. Placco",
                "Felipe Almeida-Fernandes",
                "Erika M. Holmbeck",
                "Ian U. Roederer",
                "Mohammad K. Mardini",
                "Christian R. Hayes",
                "Kim Venn",
                "Kristin Chiboucas",
                "Emily Deibert",
                "Roberto Gamen",
                "Jeong-Eun Heo",
                "Miji Jeong",
                "Venu Kalari",
                "Eder Martioli",
                "Siyi Xu",
                "Ruben Diaz",
                "Manuel Gomez-Jimenez",
                "David Henderson",
                "Pablo Prado",
                "Carlos Quiroz",
                "Roque Ruiz-Carmona",
                "Chris Simpson",
                "Cristian Urrutia",
                "Alan W. McConnachie",
                "John Pazder",
                "Gregory Burley",
                "Michael Ireland",
                "Fletcher Waller",
                "Trystyn A. M. Berg",
                "J. Gordon Robertson",
                "Zachary Hartman",
                "David O. Jones",
                "Kathleen Labrie",
                "Gabriel Perez",
                "Susan Ridgway",
                "Joanna Thomas-Osip"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17024v1",
                "http://arxiv.org/pdf/2310.17024v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR",
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17023v1",
            "title": "On the Identifiability and Interpretability of Gaussian Process Models",
            "updated": "2023-10-25T22:00:29Z",
            "published": "2023-10-25T22:00:29Z",
            "summary": "In this paper, we critically examine the prevalent practice of using additive\nmixtures of Mat\\'ern kernels in single-output Gaussian process (GP) models and\nexplore the properties of multiplicative mixtures of Mat\\'ern kernels for\nmulti-output GP models. For the single-output case, we derive a series of\ntheoretical results showing that the smoothness of a mixture of Mat\\'ern\nkernels is determined by the least smooth component and that a GP with such a\nkernel is effectively equivalent to the least smooth kernel component.\nFurthermore, we demonstrate that none of the mixing weights or parameters\nwithin individual kernel components are identifiable. We then turn our\nattention to multi-output GP models and analyze the identifiability of the\ncovariance matrix $A$ in the multiplicative kernel $K(x,y) = AK_0(x,y)$, where\n$K_0$ is a standard single output kernel such as Mat\\'ern. We show that $A$ is\nidentifiable up to a multiplicative constant, suggesting that multiplicative\nmixtures are well suited for multi-output tasks. Our findings are supported by\nextensive simulations and real applications for both single- and multi-output\nsettings. This work provides insight into kernel selection and interpretation\nfor GP models, emphasizing the importance of choosing appropriate kernel\nstructures for different tasks.",
            "author": [
                "Jiawen Chen",
                "Wancen Mu",
                "Yun Li",
                "Didong Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17023v1",
                "http://arxiv.org/pdf/2310.17023v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "62M30"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17022v1",
            "title": "Controlled Decoding from Language Models",
            "updated": "2023-10-25T22:00:05Z",
            "published": "2023-10-25T22:00:05Z",
            "summary": "We propose controlled decoding (CD), a novel off-policy reinforcement\nlearning method to control the autoregressive generation from language models\ntowards high reward outcomes. CD solves an off-policy reinforcement learning\nproblem through a value function for the reward, which we call a prefix scorer.\nThe prefix scorer is used at inference time to steer the generation towards\nhigher reward outcomes. We show that the prefix scorer may be trained on\n(possibly) off-policy data to predict the expected reward when decoding is\ncontinued from a partially decoded response. We empirically demonstrate that CD\nis effective as a control mechanism on Reddit conversations corpus. We also\nshow that the modularity of the design of CD makes it possible to control for\nmultiple rewards, effectively solving a multi-objective reinforcement learning\nproblem with no additional complexity. Finally, we show that CD can be applied\nin a novel blockwise fashion at inference-time, again without the need for any\ntraining-time changes, essentially bridging the gap between the popular\nbest-of-$K$ strategy and token-level reinforcement learning. This makes CD a\npromising approach for alignment of language models.",
            "author": [
                "Sidharth Mudgal",
                "Jong Lee",
                "Harish Ganapathy",
                "YaGuang Li",
                "Tao Wang",
                "Yanping Huang",
                "Zhifeng Chen",
                "Heng-Tze Cheng",
                "Michael Collins",
                "Trevor Strohman",
                "Jilin Chen",
                "Alex Beutel",
                "Ahmad Beirami"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17022v1",
                "http://arxiv.org/pdf/2310.17022v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17021v2",
            "title": "Streaming Factor Trajectory Learning for Temporal Tensor Decomposition",
            "updated": "2023-11-07T23:05:42Z",
            "published": "2023-10-25T21:58:52Z",
            "summary": "Practical tensor data is often along with time information. Most existing\ntemporal decomposition approaches estimate a set of fixed factors for the\nobjects in each tensor mode, and hence cannot capture the temporal evolution of\nthe objects' representation. More important, we lack an effective approach to\ncapture such evolution from streaming data, which is common in real-world\napplications. To address these issues, we propose Streaming Factor Trajectory\nLearning for temporal tensor decomposition. We use Gaussian processes (GPs) to\nmodel the trajectory of factors so as to flexibly estimate their temporal\nevolution. To address the computational challenges in handling streaming data,\nwe convert the GPs into a state-space prior by constructing an equivalent\nstochastic differential equation (SDE). We develop an efficient online\nfiltering algorithm to estimate a decoupled running posterior of the involved\nfactor states upon receiving new data. The decoupled estimation enables us to\nconduct standard Rauch-Tung-Striebel smoothing to compute the full posterior of\nall the trajectories in parallel, without the need for revisiting any previous\ndata. We have shown the advantage of SFTL in both synthetic tasks and\nreal-world applications. The code is available at\n{https://github.com/xuangu-fang/Streaming-Factor-Trajectory-Learning}.",
            "author": [
                "Shikai Fang",
                "Xin Yu",
                "Shibo Li",
                "Zheng Wang",
                "Robert Kirby",
                "Shandian Zhe"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17021v2",
                "http://arxiv.org/pdf/2310.17021v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17019v1",
            "title": "Conditionally Combining Robot Skills using Large Language Models",
            "updated": "2023-10-25T21:46:34Z",
            "published": "2023-10-25T21:46:34Z",
            "summary": "This paper combines two contributions. First, we introduce an extension of\nthe Meta-World benchmark, which we call \"Language-World,\" which allows a large\nlanguage model to operate in a simulated robotic environment using\nsemi-structured natural language queries and scripted skills described using\nnatural language. By using the same set of tasks as Meta-World, Language-World\nresults can be easily compared to Meta-World results, allowing for a point of\ncomparison between recent methods using Large Language Models (LLMs) and those\nusing Deep Reinforcement Learning. Second, we introduce a method we call Plan\nConditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of\nhigh-level plans using end-to-end demonstrations. Using Language-World, we show\nthat PCBC is able to achieve strong performance in a variety of few-shot\nregimes, often achieving task generalization with as little as a single\ndemonstration. We have made Language-World available as open-source software at\nhttps://github.com/krzentner/language-world/.",
            "author": [
                "K. R. Zentner",
                "Ryan Julian",
                "Brian Ichter",
                "Gaurav S. Sukhatme"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17019v1",
                "http://arxiv.org/pdf/2310.17019v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17017v1",
            "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge\n  Computer Science and Medical Perspectives",
            "updated": "2023-10-25T21:37:57Z",
            "published": "2023-10-25T21:37:57Z",
            "summary": "Mental health conversational agents (a.k.a. chatbots) are widely studied for\ntheir potential to offer accessible support to those experiencing mental health\nchallenges. Previous surveys on the topic primarily consider papers published\nin either computer science or medicine, leading to a divide in understanding\nand hindering the sharing of beneficial knowledge between both domains. To\nbridge this gap, we conduct a comprehensive literature review using the PRISMA\nframework, reviewing 534 papers published in both computer science and\nmedicine. Our systematic review reveals 136 key papers on building mental\nhealth-related conversational agents with diverse characteristics of modeling\nand experimental design techniques. We find that computer science papers focus\non LLM techniques and evaluating response quality using automated metrics with\nlittle attention to the application while medical papers use rule-based\nconversational agents and outcome metrics to measure the health outcomes of\nparticipants. Based on our findings on transparency, ethics, and cultural\nheterogeneity in this review, we provide a few recommendations to help bridge\nthe disciplinary divide and enable the cross-disciplinary development of mental\nhealth conversational agents.",
            "author": [
                "Young Min Cho",
                "Sunny Rai",
                "Lyle Ungar",
                "Jo\u00e3o Sedoc",
                "Sharath Chandra Guntuku"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17017v1",
                "http://arxiv.org/pdf/2310.17017v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17015v3",
            "title": "Data Augmentation for Emotion Detection in Small Imbalanced Text Data",
            "updated": "2023-10-30T13:33:16Z",
            "published": "2023-10-25T21:29:36Z",
            "summary": "Emotion recognition in text, the task of identifying emotions such as joy or\nanger, is a challenging problem in NLP with many applications. One of the\nchallenges is the shortage of available datasets that have been annotated with\nemotions. Certain existing datasets are small, follow different emotion\ntaxonomies and display imbalance in their emotion distribution. In this work,\nwe studied the impact of data augmentation techniques precisely when applied to\nsmall imbalanced datasets, for which current state-of-the-art models (such as\nRoBERTa) under-perform. Specifically, we utilized four data augmentation\nmethods (Easy Data Augmentation EDA, static and contextual Embedding-based, and\nProtAugment) on three datasets that come from different sources and vary in\nsize, emotion categories and distributions. Our experimental results show that\nusing the augmented data when training the classifier model leads to\nsignificant improvements. Finally, we conducted two case studies: a) directly\nusing the popular chat-GPT API to paraphrase text using different prompts, and\nb) using external data to augment the training set. Results show the promising\npotential of these methods.",
            "author": [
                "Anna Koufakou",
                "Diego Grisales",
                "Ragy Costa de jesus",
                "Oscar Fox"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17015v3",
                "http://arxiv.org/pdf/2310.17015v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17011v1",
            "title": "Personalized Speech-driven Expressive 3D Facial Animation Synthesis with\n  Style Control",
            "updated": "2023-10-25T21:22:28Z",
            "published": "2023-10-25T21:22:28Z",
            "summary": "Different people have different facial expressions while speaking\nemotionally. A realistic facial animation system should consider such\nidentity-specific speaking styles and facial idiosyncrasies to achieve\nhigh-degree of naturalness and plausibility. Existing approaches to\npersonalized speech-driven 3D facial animation either use one-hot identity\nlabels or rely-on person specific models which limit their scalability. We\npresent a personalized speech-driven expressive 3D facial animation synthesis\nframework that models identity specific facial motion as latent representations\n(called as styles), and synthesizes novel animations given a speech input with\nthe target style for various emotion categories. Our framework is trained in an\nend-to-end fashion and has a non-autoregressive encoder-decoder architecture\nwith three main components: expression encoder, speech encoder and expression\ndecoder. Since, expressive facial motion includes both identity-specific style\nand speech-related content information; expression encoder first disentangles\nfacial motion sequences into style and content representations, respectively.\nThen, both of the speech encoder and the expression decoders input the\nextracted style information to update transformer layer weights during training\nphase. Our speech encoder also extracts speech phoneme label and duration\ninformation to achieve better synchrony within the non-autoregressive synthesis\nmechanism more effectively. Through detailed experiments, we demonstrate that\nour approach produces temporally coherent facial expressions from input speech\nwhile preserving the speaking styles of the target identities.",
            "author": [
                "Elif Bozkurt"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17011v1",
                "http://arxiv.org/pdf/2310.17011v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17010v1",
            "title": "This Reads Like That: Deep Learning for Interpretable Natural Language\n  Processing",
            "updated": "2023-10-25T21:18:35Z",
            "published": "2023-10-25T21:18:35Z",
            "summary": "Prototype learning, a popular machine learning method designed for inherently\ninterpretable decisions, leverages similarities to learned prototypes for\nclassifying new data. While it is mainly applied in computer vision, in this\nwork, we build upon prior research and further explore the extension of\nprototypical networks to natural language processing. We introduce a learned\nweighted similarity measure that enhances the similarity computation by\nfocusing on informative dimensions of pre-trained sentence embeddings.\nAdditionally, we propose a post-hoc explainability mechanism that extracts\nprediction-relevant words from both the prototype and input sentences. Finally,\nwe empirically demonstrate that our proposed method not only improves\npredictive performance on the AG News and RT Polarity datasets over a previous\nprototype-based approach, but also improves the faithfulness of explanations\ncompared to rationale-based recurrent convolutions.",
            "author": [
                "Claudio Fanconi",
                "Moritz Vandenhirtz",
                "Severin Husmann",
                "Julia E. Vogt"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17010v1",
                "http://arxiv.org/pdf/2310.17010v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17006v1",
            "title": "Mode Selection and Target Classification in Cognitive Radar Networks",
            "updated": "2023-10-25T21:08:13Z",
            "published": "2023-10-25T21:08:13Z",
            "summary": "Cognitive Radar Networks were proposed by Simon Haykin in 2006 to address\nproblems with large legacy radar implementations - primarily, single-point\nvulnerabilities and lack of adaptability. This work proposes to leverage the\nadaptability of cognitive radar networks to trade between active radar\nobservation, which uses high power and risks interception, and passive signal\nparameter estimation, which uses target emissions to gain side information and\nlower the power necessary to accurately track multiple targets. The goal of the\nnetwork is to learn over many target tracks both the characteristics of the\ntargets as well as the optimal action choices for each type of target. In order\nto select between the available actions, we utilize a multi-armed bandit model,\nusing current class information as prior information. When the active radar\naction is selected, the node estimates the physical behavior of targets through\nthe radar emissions. When the passive action is selected, the node estimates\nthe radio behavior of targets through passive sensing. Over many target tracks,\nthe network collects the observed behavior of targets and forms clusters of\nsimilarly-behaved targets. In this way, the network meta-learns the target\nclass distributions while learning the optimal mode selections for each target\nclass.",
            "author": [
                "William W. Howard",
                "Samuel R. Shebert",
                "Benjamin H. Kirk",
                "R. Michael Buehrer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17006v1",
                "http://arxiv.org/pdf/2310.17006v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17005v1",
            "title": "Ultrafast dephasing in solid state high harmonic generation: macroscopic\n  origin revealed by real-space dynamics",
            "updated": "2023-10-25T21:06:16Z",
            "published": "2023-10-25T21:06:16Z",
            "summary": "Using a fully real-space perspective on high harmonic generation (HHG) in\nsolids, we examine the relationship between microscopic response, macroscopic\npropagation of this response to the far field, and the extremely short\ndephasing times routinely used in the theoretical simulations of experimentally\nmeasured solid-state HHG spectra. We find that far field propagation naturally\nreduces the contribution to the observed HHG emission from electrons that do\nnot return to the lattice site where they have been injected into the\nconduction band. We then show that extremely short dephasing times routinely\nused in microscopic simulations suppress many electron trajectories that\ncontribute to the far-field spectra, leading to significant distortions of the\ntrue high harmonic response. We show that a real-space based dephasing\nmechanism, which preferentially suppresses trajectories which veer too far away\nfrom their original lattice site, yield HHG spectra that faithfully retain\nthose trajectories that contribute to the far-field spectra while filtering out\nthose which do not, already at the microscopic level. Our findings emphasize\nthe similarities between atomic and solid-state HHG by highlighting the\nimportance of the intensity-dependent phase of HHG emission and address the\nlongstanding issue regarding the origin of extremely short dephasing times in\nsolid-state HHG.",
            "author": [
                "Graham G. Brown",
                "\u00c1lvaro Jim\u00e9nez-Gal\u00e1n",
                "Rui E. F. Silva",
                "Misha Ivanov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17005v1",
                "http://arxiv.org/pdf/2310.17005v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17004v2",
            "title": "Improved Panning on Non-Equidistant Loudspeakers with Direct Sound Level\n  Compensation",
            "updated": "2023-10-27T07:53:42Z",
            "published": "2023-10-25T21:05:13Z",
            "summary": "Loudspeaker rendering techniques that create phantom sound sources often\nassume an equidistant loudspeaker layout. Typical home setups might not fulfill\nthis condition as loudspeakers deviate from canonical positions, thus requiring\na corresponding calibration. The standard approach is to compensate for delays\nand to match the loudness of each loudspeaker at the listener's location. It\nwas found that a shift of the phantom image occurs when this calibration\nprocedure is applied and one of a pair of loudspeakers is significantly closer\nto the listener than the other. In this paper, a novel approach to panning on\nnon-equidistant loudspeaker layouts is presented whereby the panning position\nis governed by the direct sound and the perceived loudness is governed by the\nfull impulse response. Subjective listening tests are presented that validate\nthe approach and quantify the perceived effect of the compensation. In a setup\nwhere the standard calibration leads to an average error of 10 degrees, the\nproposed direct sound compensation largely returns the phantom source to its\nintended position.",
            "author": [
                "Jan-Hendrik Hanschke",
                "Daniel Arteaga",
                "Giulio Cengarle",
                "Joshua Lando",
                "Mark R. P. Thomas",
                "Alan Seefeldt"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17004v2",
                "http://arxiv.org/pdf/2310.17004v2"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.12862v1",
            "title": "TorchSparse++: Efficient Training and Inference Framework for Sparse\n  Convolution on GPUs",
            "updated": "2023-10-25T21:02:38Z",
            "published": "2023-10-25T21:02:38Z",
            "summary": "Sparse convolution plays a pivotal role in emerging workloads, including\npoint cloud processing in AR/VR, autonomous driving, and graph understanding in\nrecommendation systems. Since the computation pattern is sparse and irregular,\nspecialized high-performance kernels are required. Existing GPU libraries offer\ntwo dataflow types for sparse convolution. The gather-GEMM-scatter dataflow is\neasy to implement but not optimal in performance, while the dataflows with\noverlapped computation and memory access (e.g.implicit GEMM) are highly\nperformant but have very high engineering costs. In this paper, we introduce\nTorchSparse++, a new GPU library that achieves the best of both worlds. We\ncreate a highly efficient Sparse Kernel Generator that generates performant\nsparse convolution kernels at less than one-tenth of the engineering cost of\nthe current state-of-the-art system. On top of this, we design the Sparse\nAutotuner, which extends the design space of existing sparse convolution\nlibraries and searches for the best dataflow configurations for training and\ninference workloads. Consequently, TorchSparse++ achieves 2.9x, 3.3x, 2.2x and\n1.7x measured end-to-end speedup on an NVIDIA A100 GPU over state-of-the-art\nMinkowskiEngine, SpConv 1.2, TorchSparse and SpConv v2 in inference; and is\n1.2-1.3x faster than SpConv v2 in mixed precision training across seven\nrepresentative autonomous driving benchmarks. It also seamlessly supports graph\nconvolutions, achieving 2.6-7.6x faster inference speed compared with\nstate-of-the-art graph deep learning libraries.",
            "author": [
                "Haotian Tang",
                "Shang Yang",
                "Zhijian Liu",
                "Ke Hong",
                "Zhongming Yu",
                "Xiuyu Li",
                "Guohao Dai",
                "Yu Wang",
                "Song Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12862v1",
                "http://arxiv.org/pdf/2311.12862v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.CV",
                "cs.LG",
                "cs.PF"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16999v2",
            "title": "Trust, but Verify: Robust Image Segmentation using Deep Learning",
            "updated": "2023-10-29T04:09:48Z",
            "published": "2023-10-25T20:55:07Z",
            "summary": "We describe a method for verifying the output of a deep neural network for\nmedical image segmentation that is robust to several classes of random as well\nas worst-case perturbations i.e. adversarial attacks. This method is based on a\ngeneral approach recently developed by the authors called \"Trust, but Verify\"\nwherein an auxiliary verification network produces predictions about certain\nmasked features in the input image using the segmentation as an input. A\nwell-designed auxiliary network will produce high-quality predictions when the\ninput segmentations are accurate, but will produce low-quality predictions when\nthe segmentations are incorrect. Checking the predictions of such a network\nwith the original image allows us to detect bad segmentations. However, to\nensure the verification method is truly robust, we need a method for checking\nthe quality of the predictions that does not itself rely on a black-box neural\nnetwork. Indeed, we show that previous methods for segmentation evaluation that\ndo use deep neural regression networks are vulnerable to false negatives i.e.\ncan inaccurately label bad segmentations as good. We describe the design of a\nverification network that avoids such vulnerability and present results to\ndemonstrate its robustness compared to previous methods.",
            "author": [
                "Fahim Ahmed Zaman",
                "Xiaodong Wu",
                "Weiyu Xu",
                "Milan Sonka",
                "Raghuraman Mudumbai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16999v2",
                "http://arxiv.org/pdf/2310.16999v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16996v1",
            "title": "Towards Continually Learning Application Performance Models",
            "updated": "2023-10-25T20:48:46Z",
            "published": "2023-10-25T20:48:46Z",
            "summary": "Machine learning-based performance models are increasingly being used to\nbuild critical job scheduling and application optimization decisions.\nTraditionally, these models assume that data distribution does not change as\nmore samples are collected over time. However, owing to the complexity and\nheterogeneity of production HPC systems, they are susceptible to hardware\ndegradation, replacement, and/or software patches, which can lead to drift in\nthe data distribution that can adversely affect the performance models. To this\nend, we develop continually learning performance models that account for the\ndistribution drift, alleviate catastrophic forgetting, and improve\ngeneralizability. Our best model was able to retain accuracy, regardless of\nhaving to learn the new distribution of data inflicted by system changes, while\ndemonstrating a 2x improvement in the prediction accuracy of the whole data\nsequence in comparison to the naive approach.",
            "author": [
                "Ray A. O. Sinurat",
                "Anurag Daram",
                "Haryadi S. Gunawi",
                "Robert B. Ross",
                "Sandeep Madireddy"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16996v1",
                "http://arxiv.org/pdf/2310.16996v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16995v1",
            "title": "Quality > Quantity: Synthetic Corpora from Foundation Models for\n  Closed-Domain Extractive Question Answering",
            "updated": "2023-10-25T20:48:16Z",
            "published": "2023-10-25T20:48:16Z",
            "summary": "Domain adaptation, the process of training a model in one domain and applying\nit to another, has been extensively explored in machine learning. While\ntraining a domain-specific foundation model (FM) from scratch is an option,\nrecent methods have focused on adapting pre-trained FMs for domain-specific\ntasks. However, our experiments reveal that either approach does not\nconsistently achieve state-of-the-art (SOTA) results in the target domain. In\nthis work, we study extractive question answering within closed domains and\nintroduce the concept of targeted pre-training. This involves determining and\ngenerating relevant data to further pre-train our models, as opposed to the\nconventional philosophy of utilizing domain-specific FMs trained on a wide\nrange of data. Our proposed framework uses Galactica to generate synthetic,\n``targeted'' corpora that align with specific writing styles and topics, such\nas research papers and radiology reports. This process can be viewed as a form\nof knowledge distillation. We apply our method to two biomedical extractive\nquestion answering datasets, COVID-QA and RadQA, achieving a new benchmark on\nthe former and demonstrating overall improvements on the latter. Code available\nat https://github.com/saptarshi059/CDQA-v1-Targetted-PreTraining/tree/main.",
            "author": [
                "Saptarshi Sengupta",
                "Connor Heaton",
                "Shreya Ghosh",
                "Preslav Nakov",
                "Prasenjit Mitra"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16995v1",
                "http://arxiv.org/pdf/2310.16995v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16992v1",
            "title": "How well can machine-generated texts be identified and can language\n  models be trained to avoid identification?",
            "updated": "2023-10-25T20:43:07Z",
            "published": "2023-10-25T20:43:07Z",
            "summary": "With the rise of generative pre-trained transformer models such as GPT-3,\nGPT-NeoX, or OPT, distinguishing human-generated texts from machine-generated\nones has become important. We refined five separate language models to generate\nsynthetic tweets, uncovering that shallow learning classification algorithms,\nlike Naive Bayes, achieve detection accuracy between 0.6 and 0.8.\n  Shallow learning classifiers differ from human-based detection, especially\nwhen using higher temperature values during text generation, resulting in a\nlower detection rate. Humans prioritize linguistic acceptability, which tends\nto be higher at lower temperature values. In contrast, transformer-based\nclassifiers have an accuracy of 0.9 and above. We found that using a\nreinforcement learning approach to refine our generative models can\nsuccessfully evade BERT-based classifiers with a detection accuracy of 0.15 or\nless.",
            "author": [
                "Sinclair Schneider",
                "Florian Steuber",
                "Joao A. G. Schneider",
                "Gabi Dreo Rodosek"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16992v1",
                "http://arxiv.org/pdf/2310.16992v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16990v1",
            "title": "STEER: Semantic Turn Extension-Expansion Recognition for Voice\n  Assistants",
            "updated": "2023-10-25T20:41:30Z",
            "published": "2023-10-25T20:41:30Z",
            "summary": "In the context of a voice assistant system, steering refers to the phenomenon\nin which a user issues a follow-up command attempting to direct or clarify a\nprevious turn. We propose STEER, a steering detection model that predicts\nwhether a follow-up turn is a user's attempt to steer the previous command.\nConstructing a training dataset for steering use cases poses challenges due to\nthe cold-start problem. To overcome this, we developed heuristic rules to\nsample opt-in usage data, approximating positive and negative samples without\nany annotation. Our experimental results show promising performance in\nidentifying steering intent, with over 95% accuracy on our sampled data.\nMoreover, STEER, in conjunction with our sampling strategy, aligns effectively\nwith real-world steering scenarios, as evidenced by its strong zero-shot\nperformance on a human-graded evaluation set. In addition to relying solely on\nuser transcripts as input, we introduce STEER+, an enhanced version of the\nmodel. STEER+ utilizes a semantic parse tree to provide more context on\nout-of-vocabulary words, such as named entities that often occur at the\nsentence boundary. This further improves model performance, reducing error rate\nin domains where entities frequently appear, such as messaging. Lastly, we\npresent a data analysis that highlights the improvement in user experience when\nvoice assistants support steering use cases.",
            "author": [
                "Leon Liyang Zhang",
                "Jiarui Lu",
                "Joel Ruben Antony Moniz",
                "Aditya Kulkarni",
                "Dhivya Piraviperumal",
                "Tien Dung Tran",
                "Nicholas Tzou",
                "Hong Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16990v1",
                "http://arxiv.org/pdf/2310.16990v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16986v1",
            "title": "Probabilistic Integral Circuits",
            "updated": "2023-10-25T20:38:18Z",
            "published": "2023-10-25T20:38:18Z",
            "summary": "Continuous latent variables (LVs) are a key ingredient of many generative\nmodels, as they allow modelling expressive mixtures with an uncountable number\nof components. In contrast, probabilistic circuits (PCs) are hierarchical\ndiscrete mixtures represented as computational graphs composed of input, sum\nand product units. Unlike continuous LV models, PCs provide tractable inference\nbut are limited to discrete LVs with categorical (i.e. unordered) states. We\nbridge these model classes by introducing probabilistic integral circuits\n(PICs), a new language of computational graphs that extends PCs with integral\nunits representing continuous LVs. In the first place, PICs are symbolic\ncomputational graphs and are fully tractable in simple cases where analytical\nintegration is possible. In practice, we parameterise PICs with light-weight\nneural nets delivering an intractable hierarchical continuous mixture that can\nbe approximated arbitrarily well with large PCs using numerical quadrature. On\nseveral distribution estimation benchmarks, we show that such PIC-approximating\nPCs systematically outperform PCs commonly learned via expectation-maximization\nor SGD.",
            "author": [
                "Gennaro Gala",
                "Cassio de Campos",
                "Robert Peharz",
                "Antonio Vergari",
                "Erik Quaeghebeur"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16986v1",
                "http://arxiv.org/pdf/2310.16986v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16984v1",
            "title": "Patterns of Student Help-Seeking When Using a Large Language\n  Model-Powered Programming Assistant",
            "updated": "2023-10-25T20:36:05Z",
            "published": "2023-10-25T20:36:05Z",
            "summary": "Providing personalized assistance at scale is a long-standing challenge for\ncomputing educators, but a new generation of tools powered by large language\nmodels (LLMs) offers immense promise. Such tools can, in theory, provide\non-demand help in large class settings and be configured with appropriate\nguardrails to prevent misuse and mitigate common concerns around learner\nover-reliance. However, the deployment of LLM-powered tools in authentic\nclassroom settings is still rare, and very little is currently known about how\nstudents will use them in practice and what type of help they will seek. To\naddress this, we examine students' use of an innovative LLM-powered tool that\nprovides on-demand programming assistance without revealing solutions directly.\nWe deployed the tool for 12 weeks in an introductory computer and data science\ncourse ($n = 52$), collecting more than 2,500 queries submitted by students\nthroughout the term. We manually categorized all student queries based on the\ntype of assistance sought, and we automatically analyzed several additional\nquery characteristics. We found that most queries requested immediate help with\nprogramming assignments, whereas fewer requests asked for help on related\nconcepts or for deepening conceptual understanding. Furthermore, students often\nprovided minimal information to the tool, suggesting this is an area in which\ntargeted instruction would be beneficial. We also found that students who\nachieved more success in the course tended to have used the tool more\nfrequently overall. Lessons from this research can be leveraged by programming\neducators and institutions who plan to augment their teaching with emerging\nLLM-powered tools.",
            "author": [
                "Brad Sheese",
                "Mark Liffiton",
                "Jaromir Savelka",
                "Paul Denny"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16984v1",
                "http://arxiv.org/pdf/2310.16984v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16981v1",
            "title": "Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A\n  Comprehensive Benchmark",
            "updated": "2023-10-25T20:32:02Z",
            "published": "2023-10-25T20:32:02Z",
            "summary": "Synthetic data serves as an alternative in training machine learning models,\nparticularly when real-world data is limited or inaccessible. However, ensuring\nthat synthetic data mirrors the complex nuances of real-world data is a\nchallenging task. This paper addresses this issue by exploring the potential of\nintegrating data-centric AI techniques which profile the data to guide the\nsynthetic data generation process. Moreover, we shed light on the often ignored\nconsequences of neglecting these data profiles during synthetic data generation\n-- despite seemingly high statistical fidelity. Subsequently, we propose a\nnovel framework to evaluate the integration of data profiles to guide the\ncreation of more representative synthetic data. In an empirical study, we\nevaluate the performance of five state-of-the-art models for tabular data\ngeneration on eleven distinct tabular datasets. The findings offer critical\ninsights into the successes and limitations of current synthetic data\ngeneration techniques. Finally, we provide practical recommendations for\nintegrating data-centric insights into the synthetic data generation process,\nwith a specific focus on classification performance, model selection, and\nfeature selection. This study aims to reevaluate conventional approaches to\nsynthetic data generation and promote the application of data-centric AI\ntechniques in improving the quality and effectiveness of synthetic data.",
            "author": [
                "Lasse Hansen",
                "Nabeel Seedat",
                "Mihaela van der Schaar",
                "Andrija Petrovic"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16981v1",
                "http://arxiv.org/pdf/2310.16981v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16979v1",
            "title": "Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo\n  Label Self-Refinement",
            "updated": "2023-10-25T20:31:07Z",
            "published": "2023-10-25T20:31:07Z",
            "summary": "Deep learning-based solutions for semantic segmentation suffer from\nsignificant performance degradation when tested on data with different\ncharacteristics than what was used during the training. Adapting the models\nusing annotated data from the new domain is not always practical. Unsupervised\nDomain Adaptation (UDA) approaches are crucial in deploying these models in the\nactual operating conditions. Recent state-of-the-art (SOTA) UDA methods employ\na teacher-student self-training approach, where a teacher model is used to\ngenerate pseudo-labels for the new data which in turn guide the training\nprocess of the student model. Though this approach has seen a lot of success,\nit suffers from the issue of noisy pseudo-labels being propagated in the\ntraining process. To address this issue, we propose an auxiliary pseudo-label\nrefinement network (PRN) for online refining of the pseudo labels and also\nlocalizing the pixels whose predicted labels are likely to be noisy. Being able\nto improve the quality of pseudo labels and select highly reliable ones, PRN\nhelps self-training of segmentation models to be robust against pseudo label\nnoise propagation during different stages of adaptation. We evaluate our\napproach on benchmark datasets with three different domain shifts, and our\napproach consistently performs significantly better than the previous\nstate-of-the-art methods.",
            "author": [
                "Xingchen Zhao",
                "Niluthpol Chowdhury Mithun",
                "Abhinav Rajvanshi",
                "Han-Pang Chiu",
                "Supun Samarasekera"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16979v1",
                "http://arxiv.org/pdf/2310.16979v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16977v1",
            "title": "Understanding the large shift photocurrent of WS$_{2}$ nanotubes: A\n  comparative analysis with monolayers",
            "updated": "2023-10-25T20:26:56Z",
            "published": "2023-10-25T20:26:56Z",
            "summary": "We study the similarities and differences in the shift photocurrent\ncontribution to the bulk photovoltaic effect between transition-metal\ndichalcogenide monolayers and nanotubes. Our analysis is based on density\nfunctional theory in combination with the Wannier interpolation technique for\nthe calculation of the shift photoconductivity tensor. Our results show that\nfor nanotube radii of practical interest $r>60$~\\AA, the shift\nphotoconductivity of a single-wall nanotube is well described by that of the\nmonolayer. Additionally, we quantify the shift photocurrent generated under\nrealistic experimental conditions like device geometry and absorption\ncapabilities. We show that a typical nanotube can generate a photocurrent of\naround 10 nA, while the monolayer only attains a maximum of 1 nA. This\nenhancement is mainly due to the larger conducting cross section of a nanotube\nin comparison to a monolayer. Finally, we discuss our results in the context of\nrecent experimental measurements on WS$_{2}$ monolayer and nanotubes[Zhang et\nal., Nature 570, 349 (2019)].",
            "author": [
                "Jyoti Krishna",
                "Peio Garcia-Goiricelaya",
                "Fernando de Juan",
                "Julen Iba\u00f1ez-Azpiroz"
            ],
            "link": [
                "http://dx.doi.org/10.1103/PhysRevB.108.165418",
                "http://arxiv.org/abs/2310.16977v1",
                "http://arxiv.org/pdf/2310.16977v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16974v1",
            "title": "Multicomponent Activity Cycles using Hilbert-Huang Analysis",
            "updated": "2023-10-25T20:20:00Z",
            "published": "2023-10-25T20:20:00Z",
            "summary": "The temporal analysis of stellar activity evolution is usually dominated by a\ncomplex trade-off between model complexity and interpretability, often by\nneglecting the non-stationary nature of the process. Recent studies appear to\nindicate that the presence of multiple coexisting cycles in a single star is\nmore common than previously thought. The correct identification of physically\nmeaningful cyclic components in spectroscopic time series is therefore a\ncrucial task, which cannot overlook local behaviors. Here we propose a\ndecomposition technique which adaptively recovers amplitude- and\nfrequency-varying components. We present our results for the solar activity as\nmeasured both by the sunspot number and the $K$-line emission index, and we\nconsistently recover the Schwabe and Gleissberg cycles as well as the\nGnevyshev-Ohl pattern probably related to the Hale cycle. We also recover the\nknown 8-year cycle for 61 Cygni A, in addition to evidence of a three-cycles\nlong pattern reminiscent of the Gnevyshev-Ohl rule. This is particularly\ninteresting as we cannot discard the possibility of a relationship between the\nmeasured field polarity reversals and this Hale-like periodicity.",
            "author": [
                "E. N. Velloso",
                "F. Anthony",
                "J. D. do Nascimento Jr",
                "L. F. Q. Silveira",
                "J. Hall",
                "S. H. Saar"
            ],
            "link": [
                "http://dx.doi.org/10.3847/2041-8213/acb8b4",
                "http://arxiv.org/abs/2310.16974v1",
                "http://arxiv.org/pdf/2310.16974v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16972v1",
            "title": "The Word2vec Graph Model for Author Attribution and Genre Detection in\n  Literary Analysis",
            "updated": "2023-10-25T20:14:39Z",
            "published": "2023-10-25T20:14:39Z",
            "summary": "Analyzing the writing styles of authors and articles is a key to supporting\nvarious literary analyses such as author attribution and genre detection. Over\nthe years, rich sets of features that include stylometry, bag-of-words, n-grams\nhave been widely used to perform such analysis. However, the effectiveness of\nthese features largely depends on the linguistic aspects of a particular\nlanguage and datasets specific characteristics. Consequently, techniques based\non these feature sets cannot give desired results across domains. In this\npaper, we propose a novel Word2vec graph based modeling of a document that can\nrightly capture both context and style of the document. By using these Word2vec\ngraph based features, we perform classification to perform author attribution\nand genre detection tasks. Our detailed experimental study with a comprehensive\nset of literary writings shows the effectiveness of this method over\ntraditional feature based approaches. Our code and data are publicly available\nat https://cutt.ly/svLjSgk",
            "author": [
                "Nafis Irtiza Tripto",
                "Mohammed Eunus Ali"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16972v1",
                "http://arxiv.org/pdf/2310.16972v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16968v1",
            "title": "Understanding Social Structures from Contemporary Literary Fiction using\n  Character Interaction Graph -- Half Century Chronology of Influential Bengali\n  Writers",
            "updated": "2023-10-25T20:09:14Z",
            "published": "2023-10-25T20:09:14Z",
            "summary": "Social structures and real-world incidents often influence contemporary\nliterary fiction. Existing research in literary fiction analysis explains these\nreal-world phenomena through the manual critical analysis of stories.\nConventional Natural Language Processing (NLP) methodologies, including\nsentiment analysis, narrative summarization, and topic modeling, have\ndemonstrated substantial efficacy in analyzing and identifying similarities\nwithin fictional works. However, the intricate dynamics of character\ninteractions within fiction necessitate a more nuanced approach that\nincorporates visualization techniques. Character interaction graphs (or\nnetworks) emerge as a highly suitable means for visualization and information\nretrieval from the realm of fiction. Therefore, we leverage character\ninteraction graphs with NLP-derived features to explore a diverse spectrum of\nsocietal inquiries about contemporary culture's impact on the landscape of\nliterary fiction. Our study involves constructing character interaction graphs\nfrom fiction, extracting relevant graph features, and exploiting these features\nto resolve various real-life queries. Experimental evaluation of influential\nBengali fiction over half a century demonstrates that character interaction\ngraphs can be highly effective in specific assessments and information\nretrieval from literary fiction. Our data and codebase are available at\nhttps://cutt.ly/fbMgGEM",
            "author": [
                "Nafis Irtiza Tripto",
                "Mohammed Eunus Ali"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16968v1",
                "http://arxiv.org/pdf/2310.16968v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16964v1",
            "title": "Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text\n  Generation",
            "updated": "2023-10-25T20:05:07Z",
            "published": "2023-10-25T20:05:07Z",
            "summary": "Hallucination of text ungrounded in the input is a well-known problem in\nneural data-to-text generation. Many methods have been proposed to mitigate it,\nbut they typically require altering model architecture or collecting additional\ndata, and thus cannot be easily applied to an existing model. In this paper, we\nexplore a new way to mitigate hallucinations by combining the probabilistic\noutput of a generator language model (LM) with the output of a special \"text\ncritic\" classifier, which guides the generation by assessing the match between\nthe input data and the text generated so far. Our method does not need any\nchanges to the underlying LM's architecture or training procedure and can thus\nbe combined with any model and decoding operating on word probabilities. The\ncritic does not need any additional training data, using the base LM's training\ndata and synthetic negative examples. Our experimental results show that our\nmethod improves over the baseline on the WebNLG and OpenDialKG benchmarks.",
            "author": [
                "Mateusz Lango",
                "Ond\u0159ej Du\u0161ek"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16964v1",
                "http://arxiv.org/pdf/2310.16964v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16961v1",
            "title": "Neural Distributed Compressor Discovers Binning",
            "updated": "2023-10-25T20:02:20Z",
            "published": "2023-10-25T20:02:20Z",
            "summary": "We consider lossy compression of an information source when the decoder has\nlossless access to a correlated one. This setup, also known as the Wyner-Ziv\nproblem, is a special case of distributed source coding. To this day, practical\napproaches for the Wyner-Ziv problem have neither been fully developed nor\nheavily investigated. We propose a data-driven method based on machine learning\nthat leverages the universal function approximation capability of artificial\nneural networks. We find that our neural network-based compression scheme,\nbased on variational vector quantization, recovers some principles of the\noptimum theoretical solution of the Wyner-Ziv setup, such as binning in the\nsource space as well as optimal combination of the quantization index and side\ninformation, for exemplary sources. These behaviors emerge although no\nstructure exploiting knowledge of the source distributions was imposed. Binning\nis a widely used tool in information theoretic proofs and methods, and to our\nknowledge, this is the first time it has been explicitly observed to emerge\nfrom data-driven learning.",
            "author": [
                "Ezgi Ozyilkan",
                "Johannes Ball\u00e9",
                "Elza Erkip"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16961v1",
                "http://arxiv.org/pdf/2310.16961v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16960v1",
            "title": "Privately Aligning Language Models with Reinforcement Learning",
            "updated": "2023-10-25T19:58:51Z",
            "published": "2023-10-25T19:58:51Z",
            "summary": "Positioned between pre-training and user deployment, aligning large language\nmodels (LLMs) through reinforcement learning (RL) has emerged as a prevailing\nstrategy for training instruction following-models such as ChatGPT. In this\nwork, we initiate the study of privacy-preserving alignment of LLMs through\nDifferential Privacy (DP) in conjunction with RL. Following the influential\nwork of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment\nvia RL without human in the loop (e.g., positive review generation) and (ii)\nalignment via RL from human feedback (RLHF) (e.g., summarization in a\nhuman-preferred way). We give a new DP framework to achieve alignment via RL,\nand prove its correctness. Our experimental results validate the effectiveness\nof our approach, offering competitive utility while ensuring strong privacy\nprotections.",
            "author": [
                "Fan Wu",
                "Huseyin A. Inan",
                "Arturs Backurs",
                "Varun Chandrasekaran",
                "Janardhan Kulkarni",
                "Robert Sim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16960v1",
                "http://arxiv.org/pdf/2310.16960v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16959v1",
            "title": "Improving Few-shot Generalization of Safety Classifiers via Data\n  Augmented Parameter-Efficient Fine-Tuning",
            "updated": "2023-10-25T19:57:07Z",
            "published": "2023-10-25T19:57:07Z",
            "summary": "As large language models (LLMs) are widely adopted, new safety issues and\npolicies emerge, to which existing safety classifiers do not generalize well.\nIf we have only observed a few examples of violations of a new safety rule, how\ncan we build a classifier to detect violations? In this paper, we study the\nnovel setting of domain-generalized few-shot learning for LLM-based text safety\nclassifiers. Unlike prior few-shot work, these new safety issues can be hard to\nuncover and we do not get to choose the few examples. We demonstrate that\nexisting few-shot techniques do not perform well in this setting, and rather we\npropose to do parameter-efficient fine-tuning (PEFT) combined with augmenting\ntraining data based on similar examples in prior existing rules. We empirically\nshow that our approach of similarity-based data-augmentation + prompt-tuning\n(DAPT) consistently outperforms baselines that either do not rely on data\naugmentation or on PEFT by 7-17% F1 score in the Social Chemistry moral\njudgement and 9-13% AUC in the Toxicity detection tasks, even when the new rule\nis loosely correlated with existing ones.",
            "author": [
                "Ananth Balashankar",
                "Xiao Ma",
                "Aradhana Sinha",
                "Ahmad Beirami",
                "Yao Qin",
                "Jilin Chen",
                "Alex Beutel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16959v1",
                "http://arxiv.org/pdf/2310.16959v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16958v1",
            "title": "Transferring a molecular foundation model for polymer property\n  predictions",
            "updated": "2023-10-25T19:55:00Z",
            "published": "2023-10-25T19:55:00Z",
            "summary": "Transformer-based large language models have remarkable potential to\naccelerate design optimization for applications such as drug development and\nmaterials discovery. Self-supervised pretraining of transformer models requires\nlarge-scale datasets, which are often sparsely populated in topical areas such\nas polymer science. State-of-the-art approaches for polymers conduct data\naugmentation to generate additional samples but unavoidably incurs extra\ncomputational costs. In contrast, large-scale open-source datasets are\navailable for small molecules and provide a potential solution to data scarcity\nthrough transfer learning. In this work, we show that using transformers\npretrained on small molecules and fine-tuned on polymer properties achieve\ncomparable accuracy to those trained on augmented polymer datasets for a series\nof benchmark prediction tasks.",
            "author": [
                "Pei Zhang",
                "Logan Kearney",
                "Debsindhu Bhowmik",
                "Zachary Fox",
                "Amit K. Naskar",
                "John Gounley"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16958v1",
                "http://arxiv.org/pdf/2310.16958v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16956v1",
            "title": "Datastore Design for Analysis of Police Broadcast Audio at Scale",
            "updated": "2023-10-25T19:52:19Z",
            "published": "2023-10-25T19:52:19Z",
            "summary": "With policing coming under greater scrutiny in recent years, researchers have\nbegun to more thoroughly study the effects of contact between police and\nminority communities. Despite data archives of hundreds of thousands of\nrecorded Broadcast Police Communications (BPC) being openly available to the\npublic, a closer look at a large-scale analysis of the language of policing has\nremained largely unexplored. While this research is critical in understanding a\n\"pre-reflective\" notion of policing, the large quantity of data presents\nnumerous challenges in its organization and analysis.\n  In this paper, we describe preliminary work towards enabling Speech Emotion\nRecognition (SER) in an analysis of the Chicago Police Department's (CPD) BPC\nby demonstrating the pipelined creation of a datastore to enable a multimodal\nanalysis of composed raw audio files.",
            "author": [
                "Ayah Ahmad",
                "Christopher Graziul",
                "Margaret Beale Spencer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16956v1",
                "http://arxiv.org/pdf/2310.16956v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16955v1",
            "title": "Break it, Imitate it, Fix it: Robustness by Generating Human-Like\n  Attacks",
            "updated": "2023-10-25T19:51:37Z",
            "published": "2023-10-25T19:51:37Z",
            "summary": "Real-world natural language processing systems need to be robust to human\nadversaries. Collecting examples of human adversaries for training is an\neffective but expensive solution. On the other hand, training on synthetic\nattacks with small perturbations - such as word-substitution - does not\nactually improve robustness to human adversaries. In this paper, we propose an\nadversarial training framework that uses limited human adversarial examples to\ngenerate more useful adversarial examples at scale. We demonstrate the\nadvantages of this system on the ANLI and hate speech detection benchmark\ndatasets - both collected via an iterative, adversarial\nhuman-and-model-in-the-loop procedure. Compared to training only on observed\nhuman attacks, also training on our synthetic adversarial examples improves\nmodel robustness to future rounds. In ANLI, we see accuracy gains on the\ncurrent set of attacks (44.1%$\\,\\to\\,$50.1%) and on two future unseen rounds of\nhuman generated attacks (32.5%$\\,\\to\\,$43.4%, and 29.4%$\\,\\to\\,$40.2%). In hate\nspeech detection, we see AUC gains on current attacks (0.76 $\\to$ 0.84) and a\nfuture round (0.77 $\\to$ 0.79). Attacks from methods that do not learn the\ndistribution of existing human adversaries, meanwhile, degrade robustness.",
            "author": [
                "Aradhana Sinha",
                "Ananth Balashankar",
                "Ahmad Beirami",
                "Thi Avrahami",
                "Jilin Chen",
                "Alex Beutel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16955v1",
                "http://arxiv.org/pdf/2310.16955v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18368v1",
            "title": "Muslim-Violence Bias Persists in Debiased GPT Models",
            "updated": "2023-10-25T19:39:58Z",
            "published": "2023-10-25T19:39:58Z",
            "summary": "Abid et al. (2021) showed a tendency in GPT-3 to generate violent completions\nwhen prompted about Muslims, compared with other religions. Two pre-registered\nreplication attempts found few violent completions and only the weakest\nanti-Muslim bias in the Instruct version, fine-tuned to eliminate biased and\ntoxic outputs. However, more pre-registered experiments showed that using\ncommon names associated with the religions in prompts increases several-fold\nthe rate of violent completions, revealing a highly significant second-order\nbias against Muslims. Our content analysis revealed religion-specific violent\nthemes containing highly offensive ideas regardless of prompt format.\nReplications with ChatGPT suggest that any effects of GPT-3's de-biasing have\ndisappeared with continued model development, as this newer model showed both a\nstrong Muslim-violence bias and rates of violent completions closer to Abid et\nal. (2021). Our results show the need for continual de-biasing of models in\nways that address higher-order associations.",
            "author": [
                "Babak Hemmatian",
                "Razan Baltaji",
                "Lav R. Varshney"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18368v1",
                "http://arxiv.org/pdf/2310.18368v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.01465v1",
            "title": "Euclid preparation TBD. The effect of baryons on the Halo Mass Function",
            "updated": "2023-10-25T19:35:10Z",
            "published": "2023-10-25T19:35:10Z",
            "summary": "The Euclid photometric survey of galaxy clusters stands as a powerful\ncosmological tool, with the capacity to significantly propel our understanding\nof the Universe. Despite being sub-dominant to dark matter and dark energy, the\nbaryonic component in our Universe holds substantial influence over the\nstructure and mass of galaxy clusters. This paper presents a novel model to\nprecisely quantify the impact of baryons on galaxy cluster virial halo masses,\nusing the baryon fraction within a cluster as proxy for their effect.\nConstructed on the premise of quasi-adiabaticity, the model includes two\nparameters calibrated using non-radiative cosmological hydrodynamical\nsimulations and a single large-scale simulation from the Magneticum set, which\nincludes the physical processes driving galaxy formation. As a main result of\nour analysis, we demonstrate that this model delivers a remarkable one percent\nrelative accuracy in determining the virial dark matter-only equivalent mass of\ngalaxy clusters, starting from the corresponding total cluster mass and baryon\nfraction measured in hydrodynamical simulations. Furthermore, we demonstrate\nthat this result is robust against changes in cosmological parameters and\nagainst varying the numerical implementation of the sub-resolution physical\nprocesses included in the simulations. Our work substantiates previous claims\nabout the impact of baryons on cluster cosmology studies. In particular, we\nshow how neglecting these effects would lead to biased cosmological constraints\nfor a Euclid-like cluster abundance analysis. Importantly, we demonstrate that\nuncertainties associated with our model, arising from baryonic corrections to\ncluster masses, are sub-dominant when compared to the precision with which\nmass-observable relations will be calibrated using Euclid, as well as our\ncurrent understanding of the baryon fraction within galaxy clusters.",
            "author": [
                "Euclid Collaboration",
                "T. Castro",
                "S. Borgani",
                "M. Costanzi",
                "J. Dakin",
                "K. Dolag",
                "A. Fumagalli",
                "A. Ragagnin",
                "A. Saro",
                "A. M. C. Le Brun",
                "N. Aghanim",
                "A. Amara",
                "S. Andreon",
                "N. Auricchio",
                "M. Baldi",
                "S. Bardelli",
                "C. Bodendorf",
                "D. Bonino",
                "E. Branchini",
                "M. Brescia",
                "J. Brinchmann",
                "S. Camera",
                "V. Capobianco",
                "C. Carbone",
                "J. Carretero",
                "S. Casas",
                "M. Castellano",
                "S. Cavuoti",
                "A. Cimatti",
                "G. Congedo",
                "C. J. Conselice",
                "L. Conversi",
                "Y. Copin",
                "L. Corcione",
                "F. Courbin",
                "H. M. Courtois",
                "M. Cropper",
                "A. Da Silva",
                "H. Degaudenzi",
                "A. M. Di Giorgio",
                "J. Dinis",
                "F. Dubath",
                "C. A. J. Duncan",
                "X. Dupac",
                "M. Farina",
                "S. Farrens",
                "S. Ferriol",
                "M. Frailis",
                "E. Franceschi",
                "M. Fumana",
                "S. Galeotta",
                "B. Gillis",
                "C. Giocoli",
                "A. Grazian",
                "F. Grupp",
                "S. V. H. Haugan",
                "W. Holmes",
                "F. Hormuth",
                "A. Hornstrup",
                "K. Jahnke",
                "E. Keih\u00e4nen",
                "S. Kermiche",
                "A. Kiessling",
                "M. Kilbinger",
                "B. Kubik",
                "M. Kunz",
                "H. Kurki-Suonio",
                "S. Ligori",
                "P. B. Lilje",
                "V. Lindholm",
                "I. Lloro",
                "E. Maiorano",
                "O. Mansutti",
                "O. Marggraf",
                "K. Markovic",
                "N. Martinet",
                "F. Marulli",
                "R. Massey",
                "S. Maurogordato",
                "E. Medinaceli",
                "M. Meneghetti",
                "E. Merlin",
                "G. Meylan",
                "M. Moresco",
                "L. Moscardini",
                "E. Munari",
                "S. -M. Niemi",
                "C. Padilla",
                "S. Paltani",
                "F. Pasian",
                "V. Pettorino",
                "S. Pires",
                "G. Polenta",
                "M. Poncet",
                "L. A. Popa",
                "L. Pozzetti",
                "F. Raison",
                "R. Rebolo",
                "A. Renzi",
                "J. Rhodes",
                "G. Riccio",
                "E. Romelli",
                "M. Roncarelli",
                "R. Saglia",
                "D. Sapone",
                "B. Sartoris",
                "P. Schneider",
                "T. Schrabback",
                "A. Secroun",
                "G. Seidel",
                "S. Serrano",
                "C. Sirignano",
                "G. Sirri",
                "L. Stanco",
                "J. -L. Starck",
                "P. Tallada-Cresp\u00ed",
                "A. N. Taylor",
                "I. Tereno",
                "R. Toledo-Moreo",
                "F. Torradeflot",
                "I. Tutusaus",
                "E. A. Valentijn",
                "L. Valenziano",
                "T. Vassallo",
                "A. Veropalumbo",
                "Y. Wang",
                "J. Weller",
                "A. Zacchei",
                "G. Zamorani",
                "J. Zoubian",
                "E. Zucca",
                "A. Biviano",
                "E. Bozzo",
                "C. Cerna",
                "C. Colodro-Conde",
                "D. Di Ferdinando",
                "N. Mauri",
                "C. Neissner",
                "Z. Sakr",
                "V. Scottez",
                "M. Tenti",
                "M. Viel",
                "M. Wiesmann",
                "Y. Akrami",
                "S. Anselmi",
                "C. Baccigalupi",
                "M. Ballardini",
                "A. S. Borlaff",
                "S. Bruton",
                "C. Burigana",
                "R. Cabanac",
                "A. Cappi",
                "C. S. Carvalho",
                "G. Castignani",
                "G. Ca\\ {n}as-Herrera",
                "K. C. Chambers",
                "A. R. Cooray",
                "J. Coupon",
                "O. Cucciati",
                "A. D\u00edaz-S\u00e1nchez",
                "S. Davini",
                "S. de la Torre",
                "G. De Lucia",
                "G. Desprez",
                "S. Di Domizio",
                "H. Dole",
                "S. Escoffier",
                "I. Ferrero",
                "F. Finelli",
                "L. Gabarra",
                "K. Ganga",
                "J. Garcia-Bellido",
                "F. Giacomini",
                "G. Gozaliasl",
                "H. Hildebrandt",
                "S. Ili\u0107",
                "A. Jimanez Mun\\ {n}oz",
                "J. J. E. Kajava",
                "V. Kansal",
                "C. C. Kirkpatrick",
                "L. Legrand",
                "A. Loureiro",
                "J. Macias-Perez",
                "M. Magliocchetti",
                "G. Mainetti",
                "R. Maoli",
                "M. Martinelli",
                "C. J. A. P. Martins",
                "S. Matthew",
                "M. Maturi",
                "L. Maurin",
                "R. B. Metcalf",
                "M. Migliaccio",
                "P. Monaco",
                "G. Morgante",
                "S. Nadathur",
                "L. Patrizii",
                "A. Pezzotta",
                "V. Popa",
                "C. Porciani",
                "D. Potter",
                "M. P\u00f6ntinen",
                "P. Reimberg",
                "P. -F. Rocci",
                "A. G. S\u00e1nchez",
                "J. Schaye",
                "A. Schneider",
                "E. Sefusatti",
                "M. Sereno",
                "P. Simon",
                "A. Spurio Mancini",
                "J. Stadel",
                "S. A. Stanford",
                "J. Steinwagner",
                "G. Testera",
                "M. Tewes",
                "R. Teyssier",
                "S. Toft",
                "S. Tosi",
                "A. Troja",
                "M. Tucci",
                "J. Valiviita",
                "D. Vergani"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01465v1",
                "http://arxiv.org/pdf/2311.01465v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16946v1",
            "title": "How does Module Tracking for Agrivoltaics Differ from Standard\n  Photovoltaics? Performance & Technoeconomic Implications",
            "updated": "2023-10-25T19:30:05Z",
            "published": "2023-10-25T19:30:05Z",
            "summary": "Spatial-temporal sharing of sunlight between solar modules and crops needs to\nbe designed optimally in agrivoltaics (AV). For AV with fixed module tilts, the\nsunlight balance is governed through the spatial density and elevation of the\nmodules which cannot be manipulated after the installation. For flexible\nfood-energy balancing across various seasons and crop rotations, modules with\nsingle or dual axis mobility can be best suitable. AV tracking must be geared\ntowards ensuring a desired sunlight balance that may depend on many factors\nincluding the crop type, module array density, socio-economic factors, and\nlocal policies. Here, we explore single axis customized tracking (CT) for the\nmobile AV using a techno-economic model that incorporates design parameters\nincluding crop's shade sensitivity, module to land area ratio, and module\ntypes, as well as the economic parameters including soft and hardware costs for\nmodules, feed-in-tariff, and crop income. CT is implemented through standard\ntracking that tracks the sun around noon hours and its orthogonal, i.e.,\nanti-tracking around sunrise and sunset. We evaluate the optimal CT schemes\nthat can maximize economic performance while ensuring the desired food-energy\nyield thresholds. Economic feasibility for AV is evaluated in terms of the\nratio (ppr) of the price for the module system customizations to the\nperformance benefit due to the crop income. A case study for Punjab, Pakistan\nshows that CT schemes for moderate shade sensitive crops and typically dense AV\nmodule arrays can require 30 to 40 percent increase in the reference FIT to\nensure the food-energy yield threshold of 80 percent relative to standalone\nfood-energy farms for high and low value crops, respectively. CT schemes for a\nlower crop yield threshold of 70 percent require the corresponding increase in\nFIT to 10 to 20 percent, respectively.",
            "author": [
                "Habeel Alam",
                "Nauman Zafar Butt"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16946v1",
                "http://arxiv.org/pdf/2310.16946v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.CE",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16944v1",
            "title": "Zephyr: Direct Distillation of LM Alignment",
            "updated": "2023-10-25T19:25:16Z",
            "published": "2023-10-25T19:25:16Z",
            "summary": "We aim to produce a smaller language model that is aligned to user intent.\nPrevious research has shown that applying distilled supervised fine-tuning\n(dSFT) on larger models significantly improves task accuracy; however, these\nmodels are unaligned, i.e. they do not respond well to natural prompts. To\ndistill this property, we experiment with the use of preference data from AI\nFeedback (AIF). Starting from a dataset of outputs ranked by a teacher model,\nwe apply distilled direct preference optimization (dDPO) to learn a chat model\nwith significantly improved intent alignment. The approach requires only a few\nhours of training without any additional sampling during fine-tuning. The final\nresult, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B\nparameter models, and requires no human annotation. In particular, results on\nMT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access\nRLHF-based model. Code, models, data, and tutorials for the system are\navailable at https://github.com/huggingface/alignment-handbook.",
            "author": [
                "Lewis Tunstall",
                "Edward Beeching",
                "Nathan Lambert",
                "Nazneen Rajani",
                "Kashif Rasul",
                "Younes Belkada",
                "Shengyi Huang",
                "Leandro von Werra",
                "Cl\u00e9mentine Fourrier",
                "Nathan Habib",
                "Nathan Sarrazin",
                "Omar Sanseviero",
                "Alexander M. Rush",
                "Thomas Wolf"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16944v1",
                "http://arxiv.org/pdf/2310.16944v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16943v1",
            "title": "Illuminating evaporating protostellar outflows: ERIS/SPIFFIER reveals\n  the dissociation and ionization of HH 900",
            "updated": "2023-10-25T19:24:32Z",
            "published": "2023-10-25T19:24:32Z",
            "summary": "Protostellar jets and outflows are signposts of active star formation. In H\nII regions, molecular tracers like CO only reveal embedded portions of the\noutflow. Outside the natal cloud, outflows are dissociated, ionized, and\neventually completely ablated, leaving behind only the high-density jet core.\nBefore this process is complete, there should be a phase where the outflow is\npartially molecular and partially ionized. In this paper, we capture the HH 900\noutflow while this process is in action. New observations from the\nERIS/SPIFFIER near-IR integral field unit (IFU) spectrograph using the K-middle\nfilter ($\\lambda$=2.06-2.34 $\\mu$m) reveal H$_2$ emission from the dissociating\noutflow and Br-$\\gamma$ tracing its ionized skin. Both lines trace the\nwide-angle outflow morphology but H$_2$ only extends $\\sim$5000 au into the H\nII region while Br-$\\gamma$ extends the full length of the outflow\n($\\sim$12,650 au), indicating rapid dissociation of the molecules. H$_2$ has\nhigher velocities further from the driving source, consistent with a jet-driven\noutflow. Diagnostic line ratios indicate that photoexcitation, not just shocks,\ncontributes to the excitation in the outflow. We argue that HH 900 is the first\nclear example of an evaporating molecular outflow and predict that a large\ncolumn of neutral material that may be detectable with ALMA accompanies the\ndissociating molecules. Results from this study will help guide the\ninterpretation of near-IR images of externally irradiated jets and outflows\nsuch as those obtained with the James Webb Space Telescope (JWST) in high-mass\nstar-forming regions where these conditions may be common.",
            "author": [
                "Megan Reiter",
                "Thomas J. Haworth",
                "Carlo F. Manara",
                "Suzanne Ramsay",
                "Pamela D. Klaassen",
                "Dominika Itrich",
                "Anna F. McLeod"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16943v1",
                "http://arxiv.org/pdf/2310.16943v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16941v1",
            "title": "Exploring Behavior Discovery Methods for Heterogeneous Swarms of\n  Limited-Capability Robots",
            "updated": "2023-10-25T19:20:32Z",
            "published": "2023-10-25T19:20:32Z",
            "summary": "We study the problem of determining the emergent behaviors that are possible\ngiven a functionally heterogeneous swarm of robots with limited capabilities.\nPrior work has considered behavior search for homogeneous swarms and proposed\nthe use of novelty search over either a hand-specified or learned behavior\nspace followed by clustering to return a taxonomy of emergent behaviors to the\nuser. In this paper, we seek to better understand the role of novelty search\nand the efficacy of using clustering to discover novel emergent behaviors.\nThrough a large set of experiments and ablations, we analyze the effect of\nrepresentations, evolutionary search, and various clustering methods in the\nsearch for novel behaviors in a heterogeneous swarm. Our results indicate that\nprior methods fail to discover many interesting behaviors and that an iterative\nhuman-in-the-loop discovery process discovers more behaviors than random\nsearch, swarm chemistry, and automated behavior discovery. The combined\ndiscoveries of our experiments uncover 23 emergent behaviors, 18 of which are\nnovel discoveries. To the best of our knowledge, these are the first known\nemergent behaviors for heterogeneous swarms of computation-free agents. Videos,\ncode, and appendix are available at the project website:\nhttps://sites.google.com/view/heterogeneous-bd-methods",
            "author": [
                "Connor Mattson",
                "Jeremy C. Clark",
                "Daniel S. Brown"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16941v1",
                "http://arxiv.org/pdf/2310.16941v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16940v1",
            "title": "Optimal approximation of infinite-dimensional holomorphic functions II:\n  recovery from i.i.d. pointwise samples",
            "updated": "2023-10-25T19:13:32Z",
            "published": "2023-10-25T19:13:32Z",
            "summary": "Infinite-dimensional, holomorphic functions have been studied in detail over\nthe last several decades, due to their relevance to parametric differential\nequations and computational uncertainty quantification. The approximation of\nsuch functions from finitely many samples is of particular interest, due to the\npractical importance of constructing surrogate models to complex mathematical\nmodels of physical processes. In a previous work, [5] we studied the\napproximation of so-called Banach-valued,\n$(\\boldsymbol{b},\\varepsilon)$-holomorphic functions on the\ninfinite-dimensional hypercube $[-1,1]^{\\mathbb{N}}$ from $m$ (potentially\nadaptive) samples. In particular, we derived lower bounds for the adaptive\n$m$-widths for classes of such functions, which showed that certain algebraic\nrates of the form $m^{1/2-1/p}$ are the best possible regardless of the\nsampling-recovery pair. In this work, we continue this investigation by\nfocusing on the practical case where the samples are pointwise evaluations\ndrawn identically and independently from a probability measure. Specifically,\nfor Hilbert-valued $(\\boldsymbol{b},\\varepsilon)$-holomorphic functions, we\nshow that the same rates can be achieved (up to a small polylogarithmic or\nalgebraic factor) for essentially arbitrary tensor-product Jacobi\n(ultraspherical) measures. Our reconstruction maps are based on least squares\nand compressed sensing procedures using the corresponding orthonormal Jacobi\npolynomials. In doing so, we strengthen and generalize past work that has\nderived weaker nonuniform guarantees for the uniform and Chebyshev measures\n(and corresponding polynomials) only. We also extend various best $s$-term\npolynomial approximation error bounds to arbitrary Jacobi polynomial\nexpansions. Overall, we demonstrate that i.i.d.\\ pointwise samples are\nnear-optimal for the recovery of infinite-dimensional, holomorphic functions.",
            "author": [
                "Ben Adcock",
                "Nick Dexter",
                "Sebastian Moraga"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16940v1",
                "http://arxiv.org/pdf/2310.16940v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "65D40, 41A10, 41A63, 65Y20, 41A25"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16937v1",
            "title": "Learning Transfers over Several Programming Languages",
            "updated": "2023-10-25T19:04:33Z",
            "published": "2023-10-25T19:04:33Z",
            "summary": "Large language models (LLMs) have recently become remarkably good at\nimproving developer productivity for high-resource programming languages. These\nmodels use two kinds of data: large amounts of unlabeled code samples for\npretraining and relatively smaller amounts of labeled code samples for\nfine-tuning or in-context learning. Unfortunately, many programming languages\nare low-resource, lacking labeled samples for most tasks and often even lacking\nunlabeled samples. Therefore, users of low-resource languages (e.g., legacy or\nnew languages) miss out on the benefits of LLMs. Cross-lingual transfer\nlearning uses data from a source language to improve model performance on a\ntarget language. It has been well-studied for natural languages, but has\nreceived little attention for programming languages. This paper reports\nextensive experiments on four tasks using a transformer-based LLM and 11 to 41\nprogramming languages to explore the following questions. First, how well\ncross-lingual transfer works for a given task across different language pairs.\nSecond, given a task and target language, how to best choose a source language.\nThird, the characteristics of a language pair that are predictive of transfer\nperformance, and fourth, how that depends on the given task.",
            "author": [
                "Razan Baltaji",
                "Saurabh Pujar",
                "Louis Mandel",
                "Martin Hirzel",
                "Luca Buratti",
                "Lav Varshney"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16937v1",
                "http://arxiv.org/pdf/2310.16937v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "I.2.7; I.2.5"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16935v1",
            "title": "Direct numerical analysis of dynamic facilitation in glass-forming\n  liquids",
            "updated": "2023-10-25T18:58:53Z",
            "published": "2023-10-25T18:58:53Z",
            "summary": "We propose a computational strategy to quantify the temperature evolution of\nthe timescales and lengthscales over which dynamic facilitation affects the\nrelaxation dynamics of glass-forming liquids at low temperatures, that requires\nno assumption about the nature of the dynamics. In two glass models, we find\nthat dynamic facilitation depends strongly on temperature, leading to a\nsubdiffusive spreading of relaxation events which we characterize using a\ntemperature-dependent dynamic exponent. We also establish that this temperature\nevolution represents a major contribution to the increase of the structural\nrelaxation time.",
            "author": [
                "Cecilia Herrero",
                "Ludovic Berthier"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16935v1",
                "http://arxiv.org/pdf/2310.16935v1"
            ],
            "primary_category": "cond-mat.soft",
            "category": [
                "cond-mat.soft",
                "cond-mat.dis-nn",
                "cond-mat.mtrl-sci",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16931v1",
            "title": "CL-MASR: A Continual Learning Benchmark for Multilingual ASR",
            "updated": "2023-10-25T18:55:40Z",
            "published": "2023-10-25T18:55:40Z",
            "summary": "Modern multilingual automatic speech recognition (ASR) systems like Whisper\nhave made it possible to transcribe audio in multiple languages with a single\nmodel. However, current state-of-the-art ASR models are typically evaluated on\nindividual languages or in a multi-task setting, overlooking the challenge of\ncontinually learning new languages. There is insufficient research on how to\nadd new languages without losing valuable information from previous data.\nFurthermore, existing continual learning benchmarks focus mostly on vision and\nlanguage tasks, leaving continual learning for multilingual ASR largely\nunexplored. To bridge this gap, we propose CL-MASR, a benchmark designed for\nstudying multilingual ASR in a continual learning setting. CL-MASR provides a\ndiverse set of continual learning methods implemented on top of large-scale\npretrained ASR models, along with common metrics to assess the effectiveness of\nlearning new languages while addressing the issue of catastrophic forgetting.\nTo the best of our knowledge, CL-MASR is the first continual learning benchmark\nfor the multilingual ASR task. The code is available at\nhttps://github.com/speechbrain/benchmarks.",
            "author": [
                "Luca Della Libera",
                "Pooneh Mousavi",
                "Salah Zaiem",
                "Cem Subakan",
                "Mirco Ravanelli"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16931v1",
                "http://arxiv.org/pdf/2310.16931v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.03371v1",
            "title": "AI-based, automated chamber volumetry from gated, non-contrast CT",
            "updated": "2023-10-25T18:53:07Z",
            "published": "2023-10-25T18:53:07Z",
            "summary": "Background: Accurate chamber volumetry from gated, non-contrast cardiac CT\n(NCCT) scans can be useful for potential screening of heart failure.\n  Objectives: To validate a new, fully automated, AI-based method for cardiac\nvolume and myocardial mass quantification from NCCT scans compared to\ncontrasted CT Angiography (CCTA).\n  Methods: Of a retrospectively collected cohort of 1051 consecutive patients,\n420 patients had both NCCT and CCTA scans at mid-diastolic phase, excluding\npatients with cardiac devices. Ground truth values were obtained from the CCTA\nscans.\n  Results: The NCCT volume computation shows good agreement with ground truth\nvalues. Volume differences [95% CI ] and correlation coefficients were: -9.6\n[-45; 26] mL, r = 0.98 for LV Total, -5.4 [-24; 13] mL, r = 0.95 for LA, -8.7\n[-45; 28] mL, r = 0.94 for RV, -5.2 [-27; 17] mL, r = 0.92 for RA, -3.2 [-42;\n36] mL, r = 0.91 for LV blood pool, and -6.7 [-39; 26] g, r = 0.94 for LV wall\nmass, respectively. Mean relative volume errors of less than 7% were obtained\nfor all chambers.\n  Conclusions: Fully automated assessment of chamber volumes from NCCT scans is\nfeasible and correlates well with volumes obtained from contrast study.",
            "author": [
                "Athira J Jacob",
                "Ola Abdelkarim",
                "Salma Zook",
                "Kristian Hay Kragholm",
                "Prantik Gupta",
                "Myra Cocker",
                "Juan Ramirez Giraldo",
                "Jim O Doherty",
                "Max Schoebinger",
                "Chris Schwemmer",
                "Mehmet A Gulsun",
                "Saikiran Rapaka",
                "Puneet Sharma",
                "Su-Min Chang"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.jcct.2023.08.001",
                "http://arxiv.org/abs/2311.03371v1",
                "http://arxiv.org/pdf/2311.03371v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16924v1",
            "title": "Physician Detection of Clinical Harm in Machine Translation: Quality\n  Estimation Aids in Reliance and Backtranslation Identifies Critical Errors",
            "updated": "2023-10-25T18:44:14Z",
            "published": "2023-10-25T18:44:14Z",
            "summary": "A major challenge in the practical use of Machine Translation (MT) is that\nusers lack guidance to make informed decisions about when to rely on outputs.\nProgress in quality estimation research provides techniques to automatically\nassess MT quality, but these techniques have primarily been evaluated in vitro\nby comparison against human judgments outside of a specific context of use.\nThis paper evaluates quality estimation feedback in vivo with a human study\nsimulating decision-making in high-stakes medical settings. Using Emergency\nDepartment discharge instructions, we study how interventions based on quality\nestimation versus backtranslation assist physicians in deciding whether to show\nMT outputs to a patient. We find that quality estimation improves appropriate\nreliance on MT, but backtranslation helps physicians detect more clinically\nharmful errors that QE alone often misses.",
            "author": [
                "Nikita Mehandru",
                "Sweta Agrawal",
                "Yimin Xiao",
                "Elaine C Khoong",
                "Ge Gao",
                "Marine Carpuat",
                "Niloufar Salehi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16924v1",
                "http://arxiv.org/pdf/2310.16924v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16920v1",
            "title": "Smoothed Gradient Clipping and Error Feedback for Distributed\n  Optimization under Heavy-Tailed Noise",
            "updated": "2023-10-25T18:38:38Z",
            "published": "2023-10-25T18:38:38Z",
            "summary": "Motivated by understanding and analysis of large-scale machine learning under\nheavy-tailed gradient noise, we study distributed optimization with smoothed\ngradient clipping, i.e., in which certain smoothed clipping operators are\napplied to the gradients or gradient estimates computed from local clients\nprior to further processing. While vanilla gradient clipping has proven\neffective in mitigating the impact of heavy-tailed gradient noises in\nnon-distributed setups, it incurs bias that causes convergence issues in\nheterogeneous distributed settings. To address the inherent bias introduced by\ngradient clipping, we develop a smoothed clipping operator, and propose a\ndistributed gradient method equipped with an error feedback mechanism, i.e.,\nthe clipping operator is applied on the difference between some local gradient\nestimator and local stochastic gradient. We establish that, for the first time\nin the strongly convex setting with heavy-tailed gradient noises that may not\nhave finite moments of order greater than one, the proposed distributed\ngradient method's mean square error (MSE) converges to zero at a rate\n$O(1/t^\\iota)$, $\\iota \\in (0, 0.4)$, where the exponent $\\iota$ stays bounded\naway from zero as a function of the problem condition number and the first\nabsolute moment of the noise. Numerical experiments validate our theoretical\nfindings.",
            "author": [
                "Shuhua Yu",
                "Dusan Jakovetic",
                "Soummya Kar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16920v1",
                "http://arxiv.org/pdf/2310.16920v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16919v1",
            "title": "Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs",
            "updated": "2023-10-25T18:38:10Z",
            "published": "2023-10-25T18:38:10Z",
            "summary": "We propose a novel multi-bit box-free watermarking method for the protection\nof Intellectual Property Rights (IPR) of GANs with improved robustness against\nwhite-box attacks like fine-tuning, pruning, quantization, and surrogate model\nattacks. The watermark is embedded by adding an extra watermarking loss term\nduring GAN training, ensuring that the images generated by the GAN contain an\ninvisible watermark that can be retrieved by a pre-trained watermark decoder.\nIn order to improve the robustness against white-box model-level attacks, we\nmake sure that the model converges to a wide flat minimum of the watermarking\nloss term, in such a way that any modification of the model parameters does not\nerase the watermark. To do so, we add random noise vectors to the parameters of\nthe generator and require that the watermarking loss term is as invariant as\npossible with respect to the presence of noise. This procedure forces the\ngenerator to converge to a wide flat minimum of the watermarking loss. The\nproposed method is architectureand dataset-agnostic, thus being applicable to\nmany different generation tasks and models, as well as to CNN-based image\nprocessing architectures. We present the results of extensive experiments\nshowing that the presence of the watermark has a negligible impact on the\nquality of the generated images, and proving the superior robustness of the\nwatermark against model modification and surrogate model attacks.",
            "author": [
                "Jianwei Fei",
                "Zhihua Xia",
                "Benedetta Tondi",
                "Mauro Barni"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16919v1",
                "http://arxiv.org/pdf/2310.16919v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16918v2",
            "title": "Geodynamics in spacetime crystal under slow perturbation and deformation",
            "updated": "2023-10-28T03:41:01Z",
            "published": "2023-10-25T18:36:53Z",
            "summary": "We present a theory of geodynamics in a spacetime crystal based on an event\nwavepacket constructed from the Floquet-Bloch waves, which not only involve a\nscalar dispersion function but also a Berry curvature tensor in the phase space\nmanifold of spacetime and the reciprocal quasi energy-momentum. In the presence\nof structural deformation, this theory is naturally extended into a covariant\nform with the introduction of a lattice connection constructed out of the\ngradients of the local lattice vectors. The geodesic equation for a free\nparticle involves not only the lattice connection but also higher-order\ncorrections from spacetime inhomogeneity of Berry curvatures and quasi\nenergy-momentum dispersion.",
            "author": [
                "Anzhuoer Li",
                "Liang Dong",
                "Qian Niu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16918v2",
                "http://arxiv.org/pdf/2310.16918v2"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16917v2",
            "title": "MimicTouch: Learning Human's Control Strategy with Multi-Modal Tactile\n  Feedback",
            "updated": "2023-11-01T22:42:20Z",
            "published": "2023-10-25T18:34:06Z",
            "summary": "In robotics and artificial intelligence, the integration of tactile\nprocessing is becoming increasingly pivotal, especially in learning to execute\nintricate tasks like alignment and insertion. However, existing works focusing\non tactile methods for insertion tasks predominantly rely on robot\nteleoperation data and reinforcement learning, which do not utilize the rich\ninsights provided by human's control strategy guided by tactile feedback. For\nutilizing human sensations, methodologies related to learning from humans\npredominantly leverage visual feedback, often overlooking the invaluable\ntactile feedback that humans inherently employ to finish complex manipulations.\nAddressing this gap, we introduce \"MimicTouch\", a novel framework that mimics\nhuman's tactile-guided control strategy. In this framework, we initially\ncollect multi-modal tactile datasets from human demonstrators, incorporating\nhuman tactile-guided control strategies for task completion. The subsequent\nstep involves instructing robots through imitation learning using multi-modal\nsensor data and retargeted human motions. To further mitigate the embodiment\ngap between humans and robots, we employ online residual reinforcement learning\non the physical robot. Through comprehensive experiments, we validate the\nsafety of MimicTouch in transferring a latent policy learned through imitation\nlearning from human to robot. This ongoing work will pave the way for a broader\nspectrum of tactile-guided robotic applications.",
            "author": [
                "Kelin Yu",
                "Yunhai Han",
                "Matthew Zhu",
                "Ye Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16917v2",
                "http://arxiv.org/pdf/2310.16917v2"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16914v1",
            "title": "Semi-analytical and numerical solutions to Teukolsky equations for large\n  fermion mass over black hole mass ratio",
            "updated": "2023-10-25T18:28:29Z",
            "published": "2023-10-25T18:28:29Z",
            "summary": "In a recent paper, we have studied the Teukolsky equations for fermions with\nmass $m_e\\neq 0$ and rotating black hole of mass $M$. There, we have studied\ntwo cases: $\\tilde{m}_e=m_e\\,M^{-1}\\ll 1$ and $a\\omega\\ll 1$; $\\tilde{m}_e\\ll\n1$ and $a\\omega \\gtrsim 1$. Here we study the two remaining case case in which\n$\\tilde{m}_e\\gtrsim 1$ and $a\\omega\\ll 1$ using a semi-analytical approach and\n$\\tilde{m}_e\\gtrsim 1$ and $a\\omega\\gtrsim 1$ using a numerical approach. This\ncase could be of some interest for the study of the interactions of fermions\nwith small black holes, such as those formed in the last stages of the the\nHawking evaporation process.",
            "author": [
                "Mattia Villani"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16914v1",
                "http://arxiv.org/pdf/2310.16914v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16903v1",
            "title": "Experimental Observation of Earth's Rotation with Quantum Entanglement",
            "updated": "2023-10-25T18:01:23Z",
            "published": "2023-10-25T18:01:23Z",
            "summary": "Precision interferometry with quantum states has emerged as an essential tool\nfor experimentally answering fundamental questions in physics. Optical quantum\ninterferometers are of particular interest due to mature methods for generating\nand manipulating quantum states of light. The increased sensitivity offered by\nthese states promises to enable quantum phenomena, such as entanglement, to be\ntested in unprecedented regimes where tiny effects due to gravity come into\nplay. However, this requires long and decoherence-free processing of quantum\nentanglement, which has not yet been explored for large interferometric areas.\nHere we present a table-top experiment using maximally path-entangled quantum\nstates of light in an interferometer with an area of 715 m$^{2}$, sensitive\nenough to measure the rotation rate of Earth. A rotatable setup and an active\narea switching technique allow us to control the coupling of Earth's rotation\nto an entangled pair of single photons. The achieved sensitivity of 5\n$\\mu$rad/s constitutes the highest rotation resolution ever achieved with\noptical quantum interferometers, surpassing previous work by three orders of\nmagnitude. Our result demonstrates the feasibility of extending the utilization\nof maximally entangled quantum states to large-scale interferometers. Further\nimprovements to our methodology will enable measurements of\ngeneral-relativistic effects on entangled photons opening the way to further\nenhance the precision of fundamental measurements to explore the interplay\nbetween quantum mechanics and general relativity along with searches for new\nphysics.",
            "author": [
                "Raffaele Silvestri",
                "Haocun Yu",
                "Teodor Stromberg",
                "Christopher Hilweg",
                "Robert W. Peterson",
                "Philip Walther"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16903v1",
                "http://arxiv.org/pdf/2310.16903v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16902v1",
            "title": "Spectral reconstruction for radiation hydrodynamic simulations of galaxy\n  evolution",
            "updated": "2023-10-25T18:01:03Z",
            "published": "2023-10-25T18:01:03Z",
            "summary": "Radiation from stars and AGN plays an important role in galaxy formation and\nevolution, and profoundly transforms the IGM, CGM & ISM. On-the-fly RT has\nstarted being incorporated in cosmological simulations, but the complex,\nevolving radiation spectra are often crudely approximated with a small number\nof broad bands with piece-wise constant intensity and a fixed photo-ionisation\ncross-section. Such a treatment is unable to capture the changes to the\nspectrum as light is absorbed while it propagates through a medium with\nnon-zero opacity. This can lead to large errors in photo-ionisation and heating\nrates. We present a novel approach of discretising the radiation field in\nnarrow bands, located at the edges of the typically used bands, in order to\ncapture the power-law slope of the radiation field. In combination with\npower-law approximations for the photo-ionisation cross-sections, this model\nallows us to self-consistently combine radiation from sources with different\nspectra and accurately follow the ionisation states of primordial and metal\nspecies through time. The method is implemented in Gasoline2 in connection with\nTrevr2. We compare our new piece-wise power-law reconstruction to the\npiece-wise constant method in calculating the primordial chemistry\nphoto-ionisation and heating rates under an evolving UVB and stellar spectrum,\nand find that our method reduces errors significantly, up to two orders of\nmagnitude in the case of HeII ionisation. We apply our new spectral\nreconstruction method in RT post-processing of a cosmological zoom-in\nsimulation, including radiation from stars and a live UVB, and find a\nsignificant increase in total neutral hydrogen mass in the ISM and the CGM due\nto shielding of the UVB and a low escape fraction of the stellar radiation.\nThis demonstrates the importance of RT and an accurate spectral approximation\nin simulating the CGM-galaxy ecosystem.",
            "author": [
                "Bernhard Baumschlager",
                "Sijing Shen",
                "James W. Wadsley"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16902v1",
                "http://arxiv.org/pdf/2310.16902v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18367v1",
            "title": "Unsupervised Learning of Molecular Embeddings for Enhanced Clustering\n  and Emergent Properties for Chemical Compounds",
            "updated": "2023-10-25T18:00:24Z",
            "published": "2023-10-25T18:00:24Z",
            "summary": "The detailed analysis of molecular structures and properties holds great\npotential for drug development discovery through machine learning. Developing\nan emergent property in the model to understand molecules would broaden the\nhorizons for development with a new computational tool. We introduce various\nmethods to detect and cluster chemical compounds based on their SMILES data.\nOur first method, analyzing the graphical structures of chemical compounds\nusing embedding data, employs vector search to meet our threshold value. The\nresults yielded pronounced, concentrated clusters, and the method produced\nfavorable results in querying and understanding the compounds. We also used\nnatural language description embeddings stored in a vector database with\nGPT3.5, which outperforms the base model. Thus, we introduce a similarity\nsearch and clustering algorithm to aid in searching for and interacting with\nmolecules, enhancing efficiency in chemical exploration and enabling future\ndevelopment of emergent properties in molecular property prediction models.",
            "author": [
                "Jaiveer Gill",
                "Ratul Chakraborty",
                "Reetham Gubba",
                "Amy Liu",
                "Shrey Jain",
                "Chirag Iyer",
                "Obaid Khwaja",
                "Saurav Kumar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18367v1",
                "http://arxiv.org/pdf/2310.18367v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16897v1",
            "title": "Divide et Impera: Multi-Transformer Architectures for Complex NLP-Tasks",
            "updated": "2023-10-25T18:00:15Z",
            "published": "2023-10-25T18:00:15Z",
            "summary": "The growing capabilities of transformer models pave the way for solving\nincreasingly complex NLP tasks. A key to supporting application-specific\nrequirements is the ability to fine-tune. However, compiling a fine-tuning\ndataset tailored to complex tasks is tedious and results in large datasets,\nlimiting the ability to control transformer output. We present an approach in\nwhich complex tasks are divided into simpler subtasks. Multiple transformer\nmodels are fine-tuned to one subtask each, and lined up to accomplish the\ncomplex task. This simplifies the compilation of fine-tuning datasets and\nincreases overall controllability. Using the example of reducing gender bias as\na complex task, we demonstrate our approach and show that it performs better\nthan using a single model.",
            "author": [
                "Solveig Helland",
                "Elena Gavagnin",
                "Alexandre de Spindler"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16897v1",
                "http://arxiv.org/pdf/2310.16897v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16889v1",
            "title": "Resolving Horizon-Scale Dynamics of Sagittarius A*",
            "updated": "2023-10-25T18:00:03Z",
            "published": "2023-10-25T18:00:03Z",
            "summary": "Sagittarius A* (Sgr A*), the supermassive black hole at the heart of our\ngalaxy, provides unique opportunities to study black hole accretion, jet\nformation, and gravitational physics. The rapid structural changes in Sgr A*'s\nemission pose a significant challenge for traditional imaging techniques. We\npresent dynamic reconstructions of Sgr A* using Event Horizon Telescope (EHT)\ndata from April 6th and 7th, 2017, analyzed with a one-minute temporal\nresolution with the Resolve framework. This Bayesian approach employs adaptive\nGaussian Processes and Variational Inference for data-driven\nself-regularization. Our results not only fully confirm the initial findings by\nthe EHT Collaboration for a time-averaged source but also reveal intricate\ndetails about the temporal dynamics within the black hole environment. We find\nan intriguing dynamic feature on April 6th that propagates in a clock-wise\ndirection. Geometric modelling with ray-tracing, although not fully conclusive,\nindicates compatibility with high-inclination configurations of about $\\theta_o\n= 160^\\circ$, as seen in other studies.",
            "author": [
                "Jakob Knollm\u00fcller",
                "Philipp Arras",
                "Torsten En\u00dflin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16889v1",
                "http://arxiv.org/pdf/2310.16889v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "astro-ph.GA",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16884v1",
            "title": "Atomic Hydrogen Shows its True Colours: Correlations between HI and\n  Galaxy Colour in Simulations",
            "updated": "2023-10-25T18:00:02Z",
            "published": "2023-10-25T18:00:02Z",
            "summary": "Intensity mapping experiments are beginning to measure the spatial\ndistribution of neutral atomic hydrogen (HI) to constrain cosmological\nparameters and the large-scale distribution of matter. However, models of the\nbehaviour of HI as a tracer of matter is complicated by galaxy evolution. In\nthis work, we examine the clustering of HI in relation to galaxy colour,\nstellar mass, and HI mass in IllustrisTNG at $z$ = 0, 0.5, and 1. We compare\nthe HI-red and HI-blue galaxy cross-power spectra, finding that HI-red has an\namplitude 1.5 times higher than HI-blue at large scales. The cross-power\nspectra intersect at $\\approx 3$ Mpc in real space and $\\approx 10$ Mpc in\nredshift space, consistent with $z \\approx 0$ observations. We show that HI\nclustering increases with galaxy HI mass and depends weakly on detection limits\nin the range $M_{\\mathrm{HI}} \\leq 10^8 M_\\odot$. We also find that blue\ngalaxies in the greatest stellar mass bin cluster more than blue galaxies in\nother stellar mass bins. Red galaxies in the greatest stellar mass bin,\nhowever, cluster the weakest amongst red galaxies. These trends arise due to\ncentral-satellite compositions. Centrals correlate less with HI for increasing\nstellar mass, whereas satellites correlate more, irrespective of colour.\nDespite the clustering relationships with stellar mass, we find that the\ncross-power spectra are largely insensitive to detection limits in HI and\ngalaxy surveys. Counter-intuitively, all auto and cross-power spectra for red\nand blue galaxies and HI decrease with time at all scales in IllustrisTNG. We\ndemonstrate that processes associated with quenching contribute to this trend.\nThe complex interplay between HI and galaxies underscores the importance of\nunderstanding baryonic effects when interpreting the large-scale clustering of\nHI, blue, and red galaxies at $z \\leq 1$.",
            "author": [
                "Calvin Osinga",
                "Benedikt Diemer",
                "Francisco Villaescusa-Navarro",
                "Elena D'Onghia",
                "Peter Timbie"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16884v1",
                "http://arxiv.org/pdf/2310.16884v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16888v1",
            "title": "The GAPS programme at TNG XLIX. TOI-5398, the youngest compact\n  multi-planet system composed of an inner sub-Neptune and an outer warm Saturn",
            "updated": "2023-10-25T18:00:02Z",
            "published": "2023-10-25T18:00:02Z",
            "summary": "Short-period giant planets are frequently found to be solitary compared to\nother classes of exoplanets. Small inner companions to giant planets with $P\n\\lesssim$ 15 days are known only in five compact systems: WASP-47, Kepler-730,\nWASP-132, TOI-1130, and TOI-2000. Here, we report the confirmation of TOI-5398,\nthe youngest compact multi-planet system composed of a hot sub-Neptune\n(TOI-5398 c, $P_{\\rm c}$ = 4.77271 days) orbiting interior to a short-period\nSaturn (TOI-5398 b, $P_{\\rm b}$ = 10.590547 days) planet, both transiting\naround a 650 $\\pm$ 150 Myr G-type star. As part of the GAPS Young Object\nproject, we confirmed and characterised this compact system, measuring the\nradius and mass of both planets, thus constraining their bulk composition.\nUsing multidimensional Gaussian processes, we simultaneously modelled stellar\nactivity and planetary signals from TESS Sector 48 light curve and our HARPS-N\nradial velocity time series. We have confirmed the planetary nature of both\nplanets, TOI-5398 b and TOI-5398 c, alongside a precise estimation of stellar\nparameters. Through the use of astrometric, photometric, and spectroscopic\nobservations, our findings indicate that TOI-5398 is a young, active G dwarf\nstar (650 $\\pm$ 150 Myr), with a rotational period of $P_{\\rm rot}$ = 7.34\ndays. The transit photometry and radial velocity measurements enabled us to\nmeasure both the radius and mass of planets b, $R_b = 10.30\\pm0.40 R_{\\oplus}$,\n$M_b = 58.7\\pm5.7 M_{\\oplus}$, and c, $R_c = 3.52 \\pm 0.19 R_{\\oplus}$, $M_c =\n11.8\\pm4.8 M_{\\oplus}$. TESS observed TOI-5398 during sector 48 and no further\nobservations are planned in the current Extended Mission, making our\nground-based light curves crucial for ephemeris improvement. With a\nTransmission Spectroscopy Metric value of around 300, TOI-5398 b is the most\namenable warm giant (10 < $P$ < 100 days) for JWST atmospheric\ncharacterisation.",
            "author": [
                "G. Mantovan",
                "L. Malavolta",
                "S. Desidera",
                "T. Zingales",
                "L. Borsato",
                "G. Piotto",
                "A. Maggio",
                "D. Locci",
                "D. Polychroni",
                "D. Turrini",
                "M. Baratella",
                "K. Biazzo",
                "D. Nardiello",
                "K. Stassun",
                "V. Nascimbeni",
                "S. Benatti",
                "A. Anna John",
                "C. Watkins",
                "A. Bieryla",
                "J. J. Lissauer",
                "J. D. Twicken",
                "A. F. Lanza",
                "J. N. Winn",
                "S. Messina",
                "M. Montalto",
                "A. Sozzetti",
                "H. Boffin",
                "D. Cheryasov",
                "I. Strakhov",
                "F. Murgas",
                "M. D'Arpa",
                "K. Barkaoui",
                "P. Benni",
                "A. Bignamini",
                "A. Bonomo",
                "F. Borsa",
                "L. Cabona",
                "A. C. Cameron",
                "R. Claudi",
                "W. Cochran",
                "K. A. Collins",
                "M. Damasso",
                "J. Dong",
                "M. Endl",
                "A. Fukui",
                "G. Fur\u00e9sz",
                "D. Gandolfi",
                "A. Ghedina",
                "J. Jenkins",
                "P. Kab\u00e1th",
                "D. W. Latham",
                "V. Lorenzi",
                "R. Luque",
                "J. Maldonado",
                "K. McLeod",
                "M. Molinaro",
                "N. Narita",
                "G. Nowak",
                "J. Orell-Miquel",
                "E. Pall\u00e9",
                "H. Parviainen",
                "M. Pedani",
                "S. N. Quinn",
                "H. Relles",
                "P. Rowden",
                "G. Scandariato",
                "R. Schwarz",
                "S. Seager",
                "A. Shporer",
                "A. Vanderburg",
                "T. G. Wilson"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16888v1",
                "http://arxiv.org/pdf/2310.16888v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16878v2",
            "title": "Topological holography, quantum criticality, and boundary states",
            "updated": "2023-11-06T14:52:42Z",
            "published": "2023-10-25T18:00:01Z",
            "summary": "Topological holography is a holographic principle that describes the\ngeneralized global symmetry of a local quantum system in terms of a topological\norder in one higher dimension. This framework separates the topological data\nfrom the local dynamics of a theory and provides a unified description of the\nsymmetry and duality in gapped and gapless phases of matter. In this work, we\ndevelop the topological holographic picture for (1+1)d quantum phases,\nincluding both gapped phases as well as a wide range of quantum critical\npoints, including phase transitions between symmetry protected topological\n(SPT) phases, symmetry enriched quantum critical points, deconfined quantum\ncritical points and intrinsically gapless SPT phases. Topological holography\nputs a strong constraint on the emergent symmetry and the anomaly for these\ncritical theories. We show how the partition functions of these critical points\ncan be obtained from dualizing (orbifolding) more familiar critical theories.\nThe topological responses of the defect operators are also discussed in this\nframework. We further develop a topological holographic picture for conformal\nboundary states of (1+1)d rational conformal field theories. This framework\nprovides a simple physical picture to understand conformal boundary states and\nalso uncovers the nature of the gapped phases corresponding to the boundary\nstates.",
            "author": [
                "Sheng-Jie Huang",
                "Meng Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16878v2",
                "http://arxiv.org/pdf/2310.16878v2"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el",
                "hep-th",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16876v1",
            "title": "Effective Action Approach for Preheating",
            "updated": "2023-10-25T18:00:00Z",
            "published": "2023-10-25T18:00:00Z",
            "summary": "We present a semiclassical non-perturbative approach for calculating the\npreheating process at the end of inflation. Our method involves integrating out\nthe decayed particles within the path integral framework and subsequently\ndetermining world-line instanton solutions in the effective action. This\nenables us to obtain the effective action of the inflaton, with its imaginary\npart linked to the phenomenon of particle creation driven by coherent inflaton\nfield oscillations. Additionally, we utilize the Bogoliubov transformation to\ninvestigate the evolution of particle density within the medium after multiple\ninflaton oscillations. We apply our approach to various final state particles,\nincluding scalar fields, tachyonic fields, and gauge fields. The\nnon-perturbative approach provides analytical results for preheating that are\nin accord with previous methods.",
            "author": [
                "Bin Xu",
                "Wei Xue"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16876v1",
                "http://arxiv.org/pdf/2310.16876v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "astro-ph.CO",
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16836v1",
            "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers",
            "updated": "2023-10-25T17:59:32Z",
            "published": "2023-10-25T17:59:32Z",
            "summary": "We propose LLM-FP4 for quantizing both weights and activations in large\nlanguage models (LLMs) down to 4-bit floating-point values, in a post-training\nmanner. Existing post-training quantization (PTQ) solutions are primarily\ninteger-based and struggle with bit widths below 8 bits. Compared to integer\nquantization, floating-point (FP) quantization is more flexible and can better\nhandle long-tail or bell-shaped distributions, and it has emerged as a default\nchoice in many hardware platforms. One characteristic of FP quantization is\nthat its performance largely depends on the choice of exponent bits and\nclipping range. In this regard, we construct a strong FP-PTQ baseline by\nsearching for the optimal quantization parameters. Furthermore, we observe a\nhigh inter-channel variance and low intra-channel variance pattern in\nactivation distributions, which adds activation quantization difficulty. We\nrecognize this pattern to be consistent across a spectrum of transformer models\ndesigned for diverse tasks, such as LLMs, BERT, and Vision Transformer models.\nTo tackle this, we propose per-channel activation quantization and show that\nthese additional scaling factors can be reparameterized as exponential biases\nof weights, incurring a negligible cost. Our method, for the first time, can\nquantize both weights and activations in the LLaMA-13B to only 4-bit and\nachieves an average score of 63.1 on the common sense zero-shot reasoning\ntasks, which is only 5.8 lower than the full-precision model, significantly\noutperforming the previous state-of-the-art by 12.7 points. Code is available\nat: https://github.com/nbasyl/LLM-FP4.",
            "author": [
                "Shih-yang Liu",
                "Zechun Liu",
                "Xijie Huang",
                "Pingcheng Dong",
                "Kwang-Ting Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16836v1",
                "http://arxiv.org/pdf/2310.16836v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.AR",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16834v1",
            "title": "Discrete Diffusion Language Modeling by Estimating the Ratios of the\n  Data Distribution",
            "updated": "2023-10-25T17:59:12Z",
            "published": "2023-10-25T17:59:12Z",
            "summary": "Despite their groundbreaking performance for many generative modeling tasks,\ndiffusion models have fallen short on discrete data domains such as natural\nlanguage. Crucially, standard diffusion models rely on the well-established\ntheory of score matching, but efforts to generalize this to discrete structures\nhave not yielded the same empirical gains. In this work, we bridge this gap by\nproposing score entropy, a novel discrete score matching loss that is more\nstable than existing methods, forms an ELBO for maximum likelihood training,\nand can be efficiently optimized with a denoising variant. We scale our Score\nEntropy Discrete Diffusion models (SEDD) to the experimental setting of GPT-2,\nachieving highly competitive likelihoods while also introducing distinct\nalgorithmic advantages. In particular, when comparing similarly sized SEDD and\nGPT-2 models, SEDD attains comparable perplexities (normally within $+10\\%$ of\nand sometimes outperforming the baseline). Furthermore, SEDD models learn a\nmore faithful sequence distribution (around $4\\times$ better compared to GPT-2\nmodels with ancestral sampling as measured by large models), can trade off\ncompute for generation quality (needing only $16\\times$ fewer network\nevaluations to match GPT-2), and enables arbitrary infilling beyond the\nstandard left to right prompting.",
            "author": [
                "Aaron Lou",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16834v1",
                "http://arxiv.org/pdf/2310.16834v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16826v2",
            "title": "Deep machine learning for meteor monitoring: advances with transfer\n  learning and gradient-weighted class activation mapping",
            "updated": "2023-10-26T10:49:13Z",
            "published": "2023-10-25T17:56:28Z",
            "summary": "In recent decades, the use of optical detection systems for meteor studies\nhas increased dramatically, resulting in huge amounts of data being analyzed.\nAutomated meteor detection tools are essential for studying the continuous\nmeteoroid incoming flux, recovering fresh meteorites, and achieving a better\nunderstanding of our Solar System. Concerning meteor detection, distinguishing\nfalse positives between meteor and non-meteor images has traditionally been\nperformed by hand, which is significantly time-consuming. To address this\nissue, we developed a fully automated pipeline that uses Convolutional Neural\nNetworks (CNNs) to classify candidate meteor detections. Our new method is able\nto detect meteors even in images that contain static elements such as clouds,\nthe Moon, and buildings. To accurately locate the meteor within each frame, we\nemploy the Gradient-weighted Class Activation Mapping (Grad-CAM) technique.\nThis method facilitates the identification of the region of interest by\nmultiplying the activations from the last convolutional layer with the average\nof the gradients across the feature map of that layer. By combining these\nfindings with the activation map derived from the first convolutional layer, we\neffectively pinpoint the most probable pixel location of the meteor. We trained\nand evaluated our model on a large dataset collected by the Spanish Meteor\nNetwork (SPMN) and achieved a precision of 98\\%. Our new methodology presented\nhere has the potential to reduce the workload of meteor scientists and station\noperators and improve the accuracy of meteor tracking and classification.",
            "author": [
                "Eloy Pe\u00f1a-Asensio",
                "Josep M. Trigo-Rodr\u00edguez",
                "Pau Gr\u00e8bol-Tom\u00e0s",
                "David Regordosa-Avellana",
                "Albert Rimola"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16826v2",
                "http://arxiv.org/pdf/2310.16826v2"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP",
                "astro-ph.IM",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16824v1",
            "title": "Parametric model for post-processing visibility ensemble forecasts",
            "updated": "2023-10-25T17:53:52Z",
            "published": "2023-10-25T17:53:52Z",
            "summary": "Despite the continuous development of the different operational ensemble\nprediction systems over the past decades, ensemble forecasts still might suffer\nfrom lack of calibration and/or display systematic bias, thus require some\npost-processing to improve their forecast skill. Here we focus on visibility,\nwhich quantity plays a crucial role e.g. in aviation and road safety or in ship\nnavigation, and propose a parametric model where the predictive distribution is\na mixture of a gamma and a truncated normal distribution, both right censored\nat the maximal reported visibility value. The new model is evaluated in two\ncase studies based on visibility ensemble forecasts of the European Centre for\nMedium-Range Weather Forecasts covering two distinct domains in Central and\nWestern Europe and two different time periods. The results of the case studies\nindicate that climatology is substantially superior to the raw ensemble;\nnevertheless, the forecast skill can be further improved by post-processing, at\nleast for short lead times. Moreover, the proposed mixture model consistently\noutperforms the Bayesian model averaging approach used as reference\npost-processing technique.",
            "author": [
                "\u00c1gnes Baran",
                "S\u00e1ndor Baran"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16824v1",
                "http://arxiv.org/pdf/2310.16824v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16822v1",
            "title": "Prompt Me Up: Unleashing the Power of Alignments for Multimodal Entity\n  and Relation Extraction",
            "updated": "2023-10-25T17:51:56Z",
            "published": "2023-10-25T17:51:56Z",
            "summary": "How can we better extract entities and relations from text? Using multimodal\nextraction with images and text obtains more signals for entities and\nrelations, and aligns them through graphs or hierarchical fusion, aiding in\nextraction. Despite attempts at various fusions, previous works have overlooked\nmany unlabeled image-caption pairs, such as NewsCLIPing. This paper proposes\ninnovative pre-training objectives for entity-object and relation-image\nalignment, extracting objects from images and aligning them with entity and\nrelation prompts for soft pseudo-labels. These labels are used as\nself-supervised signals for pre-training, enhancing the ability to extract\nentities and relations. Experiments on three datasets show an average 3.41% F1\nimprovement over prior SOTA. Additionally, our method is orthogonal to previous\nmultimodal fusions, and using it on prior SOTA fusions further improves 5.47%\nF1.",
            "author": [
                "Xuming Hu",
                "Junzhe Chen",
                "Aiwei Liu",
                "Shiao Meng",
                "Lijie Wen",
                "Philip S. Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16822v1",
                "http://arxiv.org/pdf/2310.16822v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16821v1",
            "title": "Generally applicable physics-based equation of state for liquids",
            "updated": "2023-10-25T17:51:30Z",
            "published": "2023-10-25T17:51:30Z",
            "summary": "Physics-based first-principles pressure-volume-temperature equations of state\n(EOS) exist for solids and gases but not for liquids due to the long-standing\nfundamental problems involved in liquid theory. Current EOS models that are\napplicable to liquids and supercritical fluids at liquid-like density under\nconditions relevant to planetary interiors and industrial processes are complex\nempirical models with many physically meaningless adjustable parameters. Here,\nwe develop a generally applicable physics-based (GAP) EOS for liquids including\nsupercritical fluids at liquid-like density. The GAP equation has only one\ndimensionless parameter: the Gr\\\"uneisen parameter for the fluid. The GAP\nequation is explicit in the internal energy, and hence links the most\nfundamental macroscopic static property of fluids, the\npressure-volume-temperature EOS, to their key microscopic property: the\nmolecular hopping frequency or liquid relaxation time, from which the internal\nenergy can be obtained. We test our GAP equation against available experimental\ndata in several different ways and find good agreement. We observe that the GAP\nequation is similar to the Mie-Gr\\\"{u}neisen solid EOS in a wide range of the\nliquid phase diagram. This similarity is ultimately related to the condensed\nstate of these two phases. On the other hand, the differences between the GAP\nequation and EOS for gases are fundamental. Finally, we identify the key gaps\nin the experimental data that need to be filled in to proceed further with the\nliquid EOS.",
            "author": [
                "G. E. Proctor",
                "K. Trachenko"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16821v1",
                "http://arxiv.org/pdf/2310.16821v1"
            ],
            "primary_category": "cond-mat.soft",
            "category": [
                "cond-mat.soft",
                "cond-mat.mtrl-sci",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16820v1",
            "title": "Uncovering a new group of T Tauri stars in the Taurus-Auriga molecular\n  complex from Gaia and GALEX data",
            "updated": "2023-10-25T17:51:22Z",
            "published": "2023-10-25T17:51:22Z",
            "summary": "In this work, we examine the list of 63 candidates to T Tauri star (TTS) in\nthe TAMC identified by their ultraviolet (UV) and infrared colours (IR)\nmeasured from data obtained by the Galaxy Evolution Explorer all sky survey\n(GALEX-AIS) and the Two Microns All Sky Survey (2MASS), respectively. The\nobjective of this work is twofold: evaluate whether they are pre-main sequence\n(PMS) stars and evaluate the goodness of the UV-IR colour-colour diagram to\ndetect PMS stars in wide-fields.\n  The astrometric properties of these sources have been retrieved from the Gaia\nDR3 catalogue and used to evaluate their membership probability. Several\nclassification algorithms have been tested to search for the kinematical groups\nbut the final classification has been made with k-means++ algorithms.\nMembership probability has been evaluated by applying Logistic Regression. In\naddition, spectroscopic information available in the archive of the Large Sky\nArea Multi Object Fiber Spectroscopic Telescope has been used to ascertain\ntheir PMS nature when available.\n  About 20% of the candidates share the kinematics of the TAMC members. Among\nthem, HD 281691 is a G8-type field star located in front of the cloud and HO\nAur is likely a halo star given the very low metallicity provided by Gaia. The\nrest are three known PMS stars (HD 30171, V600 Aur and J04590305+3003004), two\npreviously unknown accreting M-type stars (J04510713+1708468 and\nJ05240794+2542438) and, five additional sources, which are very likely PMS\nstars. Most of these new sources are concentrated at low galactic latitudes\nover the Auriga-Perseus region.",
            "author": [
                "Ana In\u00e9s G\u00f3mez de Castro",
                "Ra\u00fal de la Fuente Marcos",
                "Ada Canet",
                "Leire Beitia-Antero",
                "Javier Ya\u00f1ez-Gestoso",
                "Juan Carlos Vallejo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16820v1",
                "http://arxiv.org/pdf/2310.16820v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16815v1",
            "title": "Manipulating Plasma Excitations with Terahertz Light Pulses in\n  Superconducting Cuprates",
            "updated": "2023-10-25T17:44:27Z",
            "published": "2023-10-25T17:44:27Z",
            "summary": "Layered cuprates offer a preferential playground for optical non-linearity\nthanks to the emergence, below Tc, of soft out-of-plane Josephson plasmons. The\nhallmark of such a non-linearity is the observation of Third Harmonic\nGeneration, that has been theoretically understood as a sum-frequency process\ninvolving a two-plasmon excitation. However, recent experiments in cuprates\nwith two planes per unit cell challenge this interpretation, due to the lack of\nresonant response at the temperature where the driving frequency matches the\nplasma energy scale, as observed instead in single-layer cuprates. Here we show\nthat such an apparent discrepancy in bilayer systems can be resolved by taking\ninto account the combined effect of light polarization and Josephson-coupling\nanisotropy on setting the energy range where three-dimensional layered plasma\nmodes can be resonantly excited. Our results offer a novel perspective on the\npossibility to tune on demand high-harmonic generation by artificially\ndesigning Josephson heterostructures.",
            "author": [
                "Jacopo Fiore",
                "Niccol\u00f2 Sellati",
                "Francesco Gabriele",
                "Claudio Castellani",
                "Goetz Seibold",
                "Mattia Udina",
                "Lara Benfatto"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16815v1",
                "http://arxiv.org/pdf/2310.16815v1"
            ],
            "primary_category": "cond-mat.supr-con",
            "category": [
                "cond-mat.supr-con"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16810v1",
            "title": "Can GPT models Follow Human Summarization Guidelines? Evaluating ChatGPT\n  and GPT-4 for Dialogue Summarization",
            "updated": "2023-10-25T17:39:07Z",
            "published": "2023-10-25T17:39:07Z",
            "summary": "This study explores the capabilities of prompt-driven Large Language Models\n(LLMs) like ChatGPT and GPT-4 in adhering to human guidelines for dialogue\nsummarization. Experiments employed DialogSum (English social conversations)\nand DECODA (French call center interactions), testing various prompts:\nincluding prompts from existing literature and those from human summarization\nguidelines, as well as a two-step prompt approach. Our findings indicate that\nGPT models often produce lengthy summaries and deviate from human summarization\nguidelines. However, using human guidelines as an intermediate step shows\npromise, outperforming direct word-length constraint prompts in some cases. The\nresults reveal that GPT models exhibit unique stylistic tendencies in their\nsummaries. While BERTScores did not dramatically decrease for GPT outputs\nsuggesting semantic similarity to human references and specialised pre-trained\nmodels, ROUGE scores reveal grammatical and lexical disparities between\nGPT-generated and human-written summaries. These findings shed light on the\ncapabilities and limitations of GPT models in following human instructions for\ndialogue summarization.",
            "author": [
                "Yongxin Zhou",
                "Fabien Ringeval",
                "Fran\u00e7ois Portet"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16810v1",
                "http://arxiv.org/pdf/2310.16810v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16809v2",
            "title": "Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and\n  In-depth Evaluation",
            "updated": "2023-10-29T10:59:21Z",
            "published": "2023-10-25T17:38:55Z",
            "summary": "This paper presents a comprehensive evaluation of the Optical Character\nRecognition (OCR) capabilities of the recently released GPT-4V(ision), a Large\nMultimodal Model (LMM). We assess the model's performance across a range of OCR\ntasks, including scene text recognition, handwritten text recognition,\nhandwritten mathematical expression recognition, table structure recognition,\nand information extraction from visually-rich document. The evaluation reveals\nthat GPT-4V performs well in recognizing and understanding Latin contents, but\nstruggles with multilingual scenarios and complex tasks. Specifically, it\nshowed limitations when dealing with non-Latin languages and complex tasks such\nas handwriting mathematical expression recognition, table structure\nrecognition, and end-to-end semantic entity recognition and pair extraction\nfrom document image. Based on these observations, we affirm the necessity and\ncontinued research value of specialized OCR models. In general, despite its\nversatility in handling diverse OCR tasks, GPT-4V does not outperform existing\nstate-of-the-art OCR models. How to fully utilize pre-trained general-purpose\nLMMs such as GPT-4V for OCR downstream tasks remains an open problem. The study\noffers a critical reference for future research in OCR with LMMs. Evaluation\npipeline and results are available at\nhttps://github.com/SCUT-DLVCLab/GPT-4V_OCR.",
            "author": [
                "Yongxin Shi",
                "Dezhi Peng",
                "Wenhui Liao",
                "Zening Lin",
                "Xinhong Chen",
                "Chongyu Liu",
                "Yuyi Zhang",
                "Lianwen Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16809v2",
                "http://arxiv.org/pdf/2310.16809v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16805v1",
            "title": "Two-qubit logic between distant spins in silicon",
            "updated": "2023-10-25T17:37:03Z",
            "published": "2023-10-25T17:37:03Z",
            "summary": "Direct interactions between quantum particles naturally fall off with\ndistance. For future-proof qubit architectures, however, it is important to\navail of interaction mechanisms on different length scales. In this work, we\nutilize a superconducting resonator to facilitate a coherent interaction\nbetween two semiconductor spin qubits 250 $\\mu$m apart. This separation is\nseveral orders of magnitude larger than for the commonly employed direct\ninteraction mechanisms in this platform. We operate the system in a regime\nwhere the resonator mediates a spin-spin coupling through virtual photons. We\nreport anti-phase oscillations of the populations of the two spins with\ncontrollable frequency. The observations are consistent with iSWAP oscillations\nand ten nanosecond entangling operations. These results hold promise for\nscalable networks of spin qubit modules on a chip.",
            "author": [
                "Jurgen Dijkema",
                "Xiao Xue",
                "Patrick Harvey-Collard",
                "Maximilian Rimbach-Russ",
                "Sander L. de Snoo",
                "Guoji Zheng",
                "Amir Sammak",
                "Giordano Scappucci",
                "Lieven M. K. Vandersypen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16805v1",
                "http://arxiv.org/pdf/2310.16805v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16803v1",
            "title": "Language Agnostic Code Embeddings",
            "updated": "2023-10-25T17:34:52Z",
            "published": "2023-10-25T17:34:52Z",
            "summary": "Recently, code language models have achieved notable advancements in\naddressing a diverse array of essential code comprehension and generation\ntasks. Yet, the field lacks a comprehensive deep dive and understanding of the\ncode embeddings of multilingual code models. In this paper, we present a\ncomprehensive study on multilingual code embeddings, focusing on the\ncross-lingual capabilities of these embeddings across different programming\nlanguages. Through probing experiments, we demonstrate that code embeddings\ncomprise two distinct components: one deeply tied to the nuances and syntax of\na specific language, and the other remaining agnostic to these details,\nprimarily focusing on semantics. Further, we show that when we isolate and\neliminate this language-specific component, we witness significant improvements\nin downstream code retrieval tasks, leading to an absolute increase of up to\n+17 in the Mean Reciprocal Rank (MRR).",
            "author": [
                "Saiteja Utpala",
                "Alex Gu",
                "Pin Yu Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16803v1",
                "http://arxiv.org/pdf/2310.16803v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16802v1",
            "title": "From Molecules to Materials: Pre-training Large Generalizable Models for\n  Atomic Property Prediction",
            "updated": "2023-10-25T17:32:23Z",
            "published": "2023-10-25T17:32:23Z",
            "summary": "Foundation models have been transformational in machine learning fields such\nas natural language processing and computer vision. Similar success in atomic\nproperty prediction has been limited due to the challenges of training\neffective models across multiple chemical domains. To address this, we\nintroduce Joint Multi-domain Pre-training (JMP), a supervised pre-training\nstrategy that simultaneously trains on multiple datasets from different\nchemical domains, treating each dataset as a unique pre-training task within a\nmulti-task framework. Our combined training dataset consists of $\\sim$120M\nsystems from OC20, OC22, ANI-1x, and Transition-1x. We evaluate performance and\ngeneralization by fine-tuning over a diverse set of downstream tasks and\ndatasets including: QM9, rMD17, MatBench, QMOF, SPICE, and MD22. JMP\ndemonstrates an average improvement of 59% over training from scratch, and\nmatches or sets state-of-the-art on 34 out of 40 tasks. Our work highlights the\npotential of pre-training strategies that utilize diverse data to advance\nproperty prediction across chemical domains, especially for low-data tasks.",
            "author": [
                "Nima Shoghi",
                "Adeesh Kolluru",
                "John R. Kitchin",
                "Zachary W. Ulissi",
                "C. Lawrence Zitnick",
                "Brandon M. Wood"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16802v1",
                "http://arxiv.org/pdf/2310.16802v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16798v2",
            "title": "Reachability in Continuous Pushdown VASS",
            "updated": "2023-10-31T14:33:23Z",
            "published": "2023-10-25T17:27:22Z",
            "summary": "Pushdown Vector Addition Systems with States (PVASS) consist of finitely many\ncontrol states, a pushdown stack, and a set of counters that can be incremented\nand decremented, but not tested for zero. Whether the reachability problem is\ndecidable for PVASS is a long-standing open problem.\n  We consider continuous PVASS, which are PVASS with a continuous semantics.\nThis means, the counter values are rational numbers and whenever a vector is\nadded to the current counter values, this vector is first scaled with an\narbitrarily chosen rational factor between zero and one. We show that\nreachability in continuous PVASS is NEXPTIME-complete. Our result is unusually\nrobust: Reachability can be decided in NEXPTIME even if all numbers are\nspecified in binary. On the other hand, NEXPTIME-hardness already holds for\ncoverability, in fixed dimension, for bounded stack, and even if all numbers\nare specified in unary.",
            "author": [
                "A. R. Balasubramanian",
                "Rupak Majumdar",
                "Ramanathan S. Thinniyam",
                "Georg Zetzsche"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16798v2",
                "http://arxiv.org/pdf/2310.16798v2"
            ],
            "primary_category": "cs.FL",
            "category": [
                "cs.FL",
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16797v1",
            "title": "Metal Nanoparticle-Functionalized Three-Dimensional Graphene: a\n  versatile platform towards sensors and energy-related applications",
            "updated": "2023-10-25T17:25:11Z",
            "published": "2023-10-25T17:25:11Z",
            "summary": "We demonstrate the first successful functionalization of epitaxial\nthree-dimensional graphene with metal nanoparticles. The functionalization is\nobtained by immersing the 3D graphene in a nanoparticle colloidal solution.\nThis method is versatile and here is demonstrated for gold and palladium, but\ncan be extended to other types and shapes of nanoparticles. We have measured\nthe nanoparticle density on the top-surface and in the porous layer volume by\nScanning Electron Microscopy and Scanning Transmission Electron Microscopy.\nSamples exhibit a high coverage of nanoparticles with minimal clustering. High\nquality graphene has been demonstrated to promote the functionalization leading\nto higher nanoparticle density, both on the surface and in the pores. X-ray\nPhotoelectron Spectroscopy allowed to verify the absence of contamination after\nthe functionalization process. Moreover, it confirmed the thermal stability of\nthe Au- and Pd-functionalized three-dimensional graphene up to 530{\\deg}C. Our\napproach opens up new avenues for utilizing three-dimensional graphene as a\nversatile platform for catalytic applications, sensors, and energy storage and\nconversion.",
            "author": [
                "Emanuele Pompei",
                "Ylea Vlamidis",
                "Letizia Ferbel",
                "Valentina Zannier",
                "Silvia Rubini",
                "Daniel Arenas Esteban",
                "Sara Bals",
                "Carmela Marinelli",
                "Georg Pfusterschmied",
                "Markus Leitgeb",
                "Ulrich Schmid",
                "Stefan Heun",
                "Stefano Veronesi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16797v1",
                "http://arxiv.org/pdf/2310.16797v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16795v1",
            "title": "QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models",
            "updated": "2023-10-25T17:24:53Z",
            "published": "2023-10-25T17:24:53Z",
            "summary": "Mixture-of-Experts (MoE) architectures offer a general solution to the high\ninference costs of large language models (LLMs) via sparse routing, bringing\nfaster and more accurate models, at the cost of massive parameter counts. For\nexample, the SwitchTransformer-c2048 model has 1.6 trillion parameters,\nrequiring 3.2TB of accelerator memory to run efficiently, which makes practical\ndeployment challenging and expensive. In this paper, we present a solution to\nthis memory problem, in form of a new compression and execution framework\ncalled QMoE. Specifically, QMoE consists of a scalable algorithm which\naccurately compresses trillion-parameter MoEs to less than 1 bit per parameter,\nin a custom format co-designed with bespoke GPU decoding kernels to facilitate\nefficient end-to-end compressed inference, with minor runtime overheads\nrelative to uncompressed execution. Concretely, QMoE can compress the 1.6\ntrillion parameter SwitchTransformer-c2048 model to less than 160GB (20x\ncompression, 0.8 bits per parameter) at only minor accuracy loss, in less than\na day on a single GPU. This enables, for the first time, the execution of a\ntrillion-parameter model on affordable commodity hardware, like a single server\nwith 4x NVIDIA A6000 or 8x NVIDIA 3090 GPUs, at less than 5% runtime overhead\nrelative to ideal uncompressed inference. The source code and compressed models\nare available at github.com/IST-DASLab/qmoe.",
            "author": [
                "Elias Frantar",
                "Dan Alistarh"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16795v1",
                "http://arxiv.org/pdf/2310.16795v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16794v1",
            "title": "Using Diffusion Models to Generate Synthetic Labelled Data for Medical\n  Image Segmentation",
            "updated": "2023-10-25T17:24:38Z",
            "published": "2023-10-25T17:24:38Z",
            "summary": "In this paper, we proposed and evaluated a pipeline for generating synthetic\nlabeled polyp images with the aim of augmenting automatic medical image\nsegmentation models. In doing so, we explored the use of diffusion models to\ngenerate and style synthetic labeled data. The HyperKvasir dataset consisting\nof 1000 images of polyps in the human GI tract obtained from 2008 to 2016\nduring clinical endoscopies was used for training and testing. Furthermore, we\ndid a qualitative expert review, and computed the Fr\\'echet Inception Distance\n(FID) and Multi-Scale Structural Similarity (MS-SSIM) between the output images\nand the source images to evaluate our samples. To evaluate its augmentation\npotential, a segmentation model was trained with the synthetic data to compare\ntheir performance with the real data and previous Generative Adversarial\nNetworks (GAN) methods. These models were evaluated using the Dice loss (DL)\nand Intersection over Union (IoU) score. Our pipeline generated images that\nmore closely resembled real images according to the FID scores (GAN: $118.37\n\\pm 1.06 \\text{ vs SD: } 65.99 \\pm 0.37$). Improvements over GAN methods were\nseen on average when the segmenter was entirely trained (DL difference:\n$-0.0880 \\pm 0.0170$, IoU difference: $0.0993 \\pm 0.01493$) or augmented (DL\ndifference: GAN $-0.1140 \\pm 0.0900 \\text{ vs SD }-0.1053 \\pm 0.0981$, IoU\ndifference: GAN $0.01533 \\pm 0.03831 \\text{ vs SD }0.0255 \\pm 0.0454$) with\nsynthetic data. Overall, we obtained more realistic synthetic images and\nimproved segmentation model performance when fully or partially trained on\nsynthetic data.",
            "author": [
                "Daniel Saragih",
                "Pascal Tyrrell"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16794v1",
                "http://arxiv.org/pdf/2310.16794v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16793v1",
            "title": "Fluctuations and correlations in weakly asymmetric simple exclusion on a\n  ring subject to an atypical current",
            "updated": "2023-10-25T17:24:11Z",
            "published": "2023-10-25T17:24:11Z",
            "summary": "We consider the weakly asymmetric simple exclusion process on a ring, driven\nout of equilibrium by tilting the dynamics so as to enforce a macroscopic\ncurrent of particles on a large time interval. In this current-biased dynamics,\nthe tilt by the current makes the dynamics non-local, non homogeneous and\ninduces long-range correlations. We compute the correlation structure in the\nlarge time, large size limit for a certain range of asymmetry and current\nstrength, recovering heuristic results of Bodineau et al. arXiv:0807.2394. In\naddition, in this range of parameters, we characterise the full dynamics of\nfluctuations around the optimal density profile in the current-biased dynamics.\nThe key ingredient at the microscopic scale is a precise relative entropy\nestimate at the level of correlations. We also discuss how to remove the\n(technical) restriction on the range of parameters.",
            "author": [
                "Benoit Dagallier"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16793v1",
                "http://arxiv.org/pdf/2310.16793v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math-ph",
                "math.MP",
                "82C22, 82C26 (Primary) 60J28, 60F17 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16791v2",
            "title": "Covert Planning against Imperfect Observers",
            "updated": "2023-11-01T17:44:46Z",
            "published": "2023-10-25T17:23:57Z",
            "summary": "Covert planning refers to a class of constrained planning problems where an\nagent aims to accomplish a task with minimal information leaked to a passive\nobserver to avoid detection. However, existing methods of covert planning often\nconsider deterministic environments or do not exploit the observer's imperfect\ninformation. This paper studies how covert planning can leverage the coupling\nof stochastic dynamics and the observer's imperfect observation to achieve\noptimal task performance without being detected. Specifically, we employ a\nMarkov decision process to model the interaction between the agent and its\nstochastic environment, and a partial observation function to capture the\nleaked information to a passive observer. Assuming the observer employs\nhypothesis testing to detect if the observation deviates from a nominal policy,\nthe covert planning agent aims to maximize the total discounted reward while\nkeeping the probability of being detected as an adversary below a given\nthreshold. We prove that finite-memory policies are more powerful than\nMarkovian policies in covert planning. Then, we develop a primal-dual proximal\npolicy gradient method with a two-time-scale update to compute a (locally)\noptimal covert policy. We demonstrate the effectiveness of our methods using a\nstochastic gridworld example. Our experimental results illustrate that the\nproposed method computes a policy that maximizes the adversary's expected\nreward without violating the detection constraint, and empirically demonstrates\nhow the environmental noises can influence the performance of the covert\npolicies.",
            "author": [
                "Haoxiang Ma",
                "Chongyang Shi",
                "Shuo Han",
                "Michael R. Dorothy",
                "Jie Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16791v2",
                "http://arxiv.org/pdf/2310.16791v2"
            ],
            "primary_category": "cs.MA",
            "category": [
                "cs.MA",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16790v1",
            "title": "Improving a Named Entity Recognizer Trained on Noisy Data with a Few\n  Clean Instances",
            "updated": "2023-10-25T17:23:37Z",
            "published": "2023-10-25T17:23:37Z",
            "summary": "To achieve state-of-the-art performance, one still needs to train NER models\non large-scale, high-quality annotated data, an asset that is both costly and\ntime-intensive to accumulate. In contrast, real-world applications often resort\nto massive low-quality labeled data through non-expert annotators via\ncrowdsourcing and external knowledge bases via distant supervision as a\ncost-effective alternative. However, these annotation methods result in noisy\nlabels, which in turn lead to a notable decline in performance. Hence, we\npropose to denoise the noisy NER data with guidance from a small set of clean\ninstances. Along with the main NER model we train a discriminator model and use\nits outputs to recalibrate the sample weights. The discriminator is capable of\ndetecting both span and category errors with different discriminative prompts.\nResults on public crowdsourcing and distant supervision datasets show that the\nproposed method can consistently improve performance with a small guidance set.",
            "author": [
                "Zhendong Chu",
                "Ruiyi Zhang",
                "Tong Yu",
                "Rajiv Jain",
                "Vlad I Morariu",
                "Jiuxiang Gu",
                "Ani Nenkova"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16790v1",
                "http://arxiv.org/pdf/2310.16790v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16789v2",
            "title": "Detecting Pretraining Data from Large Language Models",
            "updated": "2023-11-03T05:27:37Z",
            "published": "2023-10-25T17:21:23Z",
            "summary": "Although large language models (LLMs) are widely deployed, the data used to\ntrain them is rarely disclosed. Given the incredible scale of this data, up to\ntrillions of tokens, it is all but certain that it includes potentially\nproblematic text such as copyrighted materials, personally identifiable\ninformation, and test data for widely reported reference benchmarks. However,\nwe currently have no way to know which data of these types is included or in\nwhat proportions. In this paper, we study the pretraining data detection\nproblem: given a piece of text and black-box access to an LLM without knowing\nthe pretraining data, can we determine if the model was trained on the provided\ntext? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that\nuses data created before and after model training to support gold truth\ndetection. We also introduce a new detection method Min-K% Prob based on a\nsimple hypothesis: an unseen example is likely to contain a few outlier words\nwith low probabilities under the LLM, while a seen example is less likely to\nhave words with such low probabilities. Min-K% Prob can be applied without any\nknowledge about the pretraining corpus or any additional training, departing\nfrom previous detection methods that require training a reference model on data\nthat is similar to the pretraining data. Moreover, our experiments demonstrate\nthat Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous\nmethods. We apply Min-K% Prob to three real-world scenarios, copyrighted book\ndetection, contaminated downstream example detection and privacy auditing of\nmachine unlearning, and find it a consistently effective solution.",
            "author": [
                "Weijia Shi",
                "Anirudh Ajith",
                "Mengzhou Xia",
                "Yangsibo Huang",
                "Daogao Liu",
                "Terra Blevins",
                "Danqi Chen",
                "Luke Zettlemoyer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16789v2",
                "http://arxiv.org/pdf/2310.16789v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16787v3",
            "title": "The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing\n  & Attribution in AI",
            "updated": "2023-11-04T19:10:06Z",
            "published": "2023-10-25T17:20:26Z",
            "summary": "The race to train language models on vast, diverse, and inconsistently\ndocumented datasets has raised pressing concerns about the legal and ethical\nrisks for practitioners. To remedy these practices threatening data\ntransparency and understanding, we convene a multi-disciplinary effort between\nlegal and machine learning experts to systematically audit and trace 1800+ text\ndatasets. We develop tools and standards to trace the lineage of these\ndatasets, from their source, creators, series of license conditions,\nproperties, and subsequent use. Our landscape analysis highlights the sharp\ndivides in composition and focus of commercially open vs closed datasets, with\nclosed datasets monopolizing important categories: lower resource languages,\nmore creative tasks, richer topic variety, newer and more synthetic training\ndata. This points to a deepening divide in the types of data that are made\navailable under different license conditions, and heightened implications for\njurisdictional legal interpretations of copyright and fair use. We also observe\nfrequent miscategorization of licenses on widely used dataset hosting sites,\nwith license omission of 70%+ and error rates of 50%+. This points to a crisis\nin misattribution and informed use of the most popular datasets driving many\nrecent breakthroughs. As a contribution to ongoing improvements in dataset\ntransparency and responsible use, we release our entire audit, with an\ninteractive UI, the Data Provenance Explorer, which allows practitioners to\ntrace and filter on data provenance for the most popular open source finetuning\ndata collections: www.dataprovenance.org.",
            "author": [
                "Shayne Longpre",
                "Robert Mahari",
                "Anthony Chen",
                "Naana Obeng-Marnu",
                "Damien Sileo",
                "William Brannon",
                "Niklas Muennighoff",
                "Nathan Khazam",
                "Jad Kabbara",
                "Kartik Perisetla",
                "Xinyi Wu",
                "Enrico Shippole",
                "Kurt Bollacker",
                "Tongshuang Wu",
                "Luis Villa",
                "Sandy Pentland",
                "Sara Hooker"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16787v3",
                "http://arxiv.org/pdf/2310.16787v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16786v1",
            "title": "The Simplest Inflationary Potentials",
            "updated": "2023-10-25T17:20:19Z",
            "published": "2023-10-25T17:20:19Z",
            "summary": "Inflation is a highly favoured theory for the early Universe. It is\ncompatible with current observations of the cosmic microwave background and\nlarge scale structure and is a driver in the quest to detect primordial\ngravitational waves. It is also, given the current quality of the data, highly\nunder-determined with a large number of candidate implementations. We use a new\nmethod in symbolic regression to generate all possible simple scalar field\npotentials for one of two possible basis sets of operators. Treating these as\nsingle-field, slow-roll inflationary models we then score them with an\ninformation-theoretic metric (\"minimum description length\") that quantifies\ntheir efficiency in compressing the information in the Planck data. We explore\ntwo possible priors on the parameter space of potentials, one related to the\nfunctions' structural complexity and one that uses a Katz back-off language\nmodel to prefer functions that may be theoretically motivated. This enables us\nto identify the inflaton potentials that optimally balance simplicity with\naccuracy at explaining the Planck data, which may subsequently find theoretical\nmotivation. Our exploratory study opens the door to extraction of fundamental\nphysics directly from data, and may be augmented with more refined theoretical\npriors in the quest for a complete understanding of the early Universe.",
            "author": [
                "Tom\u00e1s Sousa",
                "Deaglan J. Bartlett",
                "Harry Desmond",
                "Pedro G. Ferreira"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16786v1",
                "http://arxiv.org/pdf/2310.16786v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "cs.LG",
                "gr-qc",
                "hep-ph",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16781v1",
            "title": "Kiki or Bouba? Sound Symbolism in Vision-and-Language Models",
            "updated": "2023-10-25T17:15:55Z",
            "published": "2023-10-25T17:15:55Z",
            "summary": "Although the mapping between sound and meaning in human language is assumed\nto be largely arbitrary, research in cognitive science has shown that there are\nnon-trivial correlations between particular sounds and meanings across\nlanguages and demographic groups, a phenomenon known as sound symbolism. Among\nthe many dimensions of meaning, sound symbolism is particularly salient and\nwell-demonstrated with regards to cross-modal associations between language and\nthe visual domain. In this work, we address the question of whether sound\nsymbolism is reflected in vision-and-language models such as CLIP and Stable\nDiffusion. Using zero-shot knowledge probing to investigate the inherent\nknowledge of these models, we find strong evidence that they do show this\npattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our\nwork provides a novel method for demonstrating sound symbolism and\nunderstanding its nature using computational tools. Our code will be made\npublicly available.",
            "author": [
                "Morris Alper",
                "Hadar Averbuch-Elor"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16781v1",
                "http://arxiv.org/pdf/2310.16781v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16778v1",
            "title": "Navigating Socio-Emotional Risk through Comfort-Building in a Physics\n  Teaching Community of Practice: A Case Study",
            "updated": "2023-10-25T17:10:46Z",
            "published": "2023-10-25T17:10:46Z",
            "summary": "In teacher professional development (PD), grouping teachers with varying\nlevels of experience can be a productive and empowering way to stimulate the\nexchange and co-generation of content and pedagogical knowledge. However, less\nexperienced teachers can face socio-emotional risks when engaging in\ncollaborative science content reasoning tasks with more experienced colleagues\n(Finkelstein, Jaber, & Dini, 2018), and these risks may impact the\ncollaborative experience of both parties and the learning environment in\nteacher PD. This descriptive case study examines the process of productively\nnavigating socio-emotional risks and interpersonal tensions encountered by a\nveteran and pre-service physics teacher during one episode of discussing\nphysics content. We use a single term, comfort-building, to encapsulate\ndiscursive moves that result in increased feelings of comfort and safety by the\nparticipants. Comfort-building includes moves that serve to mitigate social\nrisk, ease tension, and avoid discomfort, as well as those geared toward\nfinding common ground and co-navigating challenges. These moves can carve out\nconversational space for teachers to more confidently face risks associated\nwith being accountable to the physics content knowledge and engage in\ndiscipline-based conversations more deeply. The presented episode in this study\nwas followed by video-stimulated individual interviews to determine how\nconsciously the teachers connected their participation to explicit risk and\ncomfort. This case study highlights an affective dimension for consideration in\nthe continued study and facilitation of science teaching communities of\npractice, especially ones that bring together teachers with a variety of\nbackgrounds and skill sets.",
            "author": [
                "Maggie Mahmood",
                "Hamideh Talafian",
                "Devyn Shafer",
                "Morten Lundsgaard",
                "Eric Kuo",
                "Tim Stelzer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16778v1",
                "http://arxiv.org/pdf/2310.16778v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16776v3",
            "title": "DEFT: Data Efficient Fine-Tuning for Large Language Models via\n  Unsupervised Core-Set Selection",
            "updated": "2023-11-16T02:35:06Z",
            "published": "2023-10-25T17:06:42Z",
            "summary": "Recent advances have led to the availability of many pre-trained language\nmodels (PLMs); however, a question that remains is how much data is truly\nneeded to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT,\na data-efficient fine-tuning framework that leverages unsupervised core-set\nselection to minimize the amount of data needed to fine-tune PLMs for\ndownstream tasks. We demonstrate the efficacy of our DEFT framework in the\ncontext of text-editing LMs, and compare to the state-of-the art text-editing\nmodel, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT\nmodels are just as accurate as CoEDIT while being finetuned on ~70% less data.",
            "author": [
                "Devleena Das",
                "Vivek Khetan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16776v3",
                "http://arxiv.org/pdf/2310.16776v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16772v2",
            "title": "AI Agent as Urban Planner: Steering Stakeholder Dynamics in Urban\n  Planning via Consensus-based Multi-Agent Reinforcement Learning",
            "updated": "2023-11-09T15:18:20Z",
            "published": "2023-10-25T17:04:11Z",
            "summary": "In urban planning, land use readjustment plays a pivotal role in aligning\nland use configurations with the current demands for sustainable urban\ndevelopment. However, present-day urban planning practices face two main\nissues. Firstly, land use decisions are predominantly dependent on human\nexperts. Besides, while resident engagement in urban planning can promote urban\nsustainability and livability, it is challenging to reconcile the diverse\ninterests of stakeholders. To address these challenges, we introduce a\nConsensus-based Multi-Agent Reinforcement Learning framework for real-world\nland use readjustment. This framework serves participatory urban planning,\nallowing diverse intelligent agents as stakeholder representatives to vote for\npreferred land use types. Within this framework, we propose a novel consensus\nmechanism in reward design to optimize land utilization through collective\ndecision making. To abstract the structure of the complex urban system, the\ngeographic information of cities is transformed into a spatial graph structure\nand then processed by graph neural networks. Comprehensive experiments on both\ntraditional top-down planning and participatory planning methods from\nreal-world communities indicate that our computational framework enhances\nglobal benefits and accommodates diverse interests, leading to improved\nsatisfaction across different demographic groups. By integrating Multi-Agent\nReinforcement Learning, our framework ensures that participatory urban planning\ndecisions are more dynamic and adaptive to evolving community needs and\nprovides a robust platform for automating complex real-world urban planning\nprocesses.",
            "author": [
                "Kejiang Qian",
                "Lingjun Mao",
                "Xin Liang",
                "Yimin Ding",
                "Jin Gao",
                "Xinran Wei",
                "Ziyi Guo",
                "Jiajie Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16772v2",
                "http://arxiv.org/pdf/2310.16772v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16768v1",
            "title": "Discrete variance decay analysis of spurious mixing",
            "updated": "2023-10-25T16:58:33Z",
            "published": "2023-10-25T16:58:33Z",
            "summary": "Expressions for local discrete variance decay (DVD) rates are directly\nderived from discrete tracer equations without any assumptions on discrete\nfluxes of the second moment. Spurious mixing (SM) associated with numerical\nimplementations of scalar advection and diffusion is thus estimated. The new\nframework is shown to avoid the need for second-moment flux definition when\nsolved on finite-volume cell edges but still invoke certain second-moment\nfluxes when the DVD rates are partitioned to participating cell nodes. These\nimplied discrete fluxes are shown to differ from those proposed in earlier\nliterature (but share the same dissipative part) and thus reveal the\nnon-uniqueness of their nature. They are shown to be ambiguous for high-order\nadvection schemes introducing uncertainty to the locality of any estimates\nproduced by a DVD approach. Additional damping of flux divergence through\ntemporal averaging or some coarse-graining is thus shown to be necessary.\nThrough the application of this technique, SM is found to be correlated with\nthe distribution of eddy kinetic energy. The contribution from vertical\nadvection to SM is found to be relatively small and correlated with the\ndistribution of buoyancy fluxes. The explored high-order schemes are found to\ndemonstrate levels of spurious mixing which may locally exceed background\nphysical mixing.",
            "author": [
                "Tridib Banerjee",
                "Sergey Danilov",
                "Knut Klingbeil"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16768v1",
                "http://arxiv.org/pdf/2310.16768v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph",
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17809v1",
            "title": "Environmental-induced work extraction",
            "updated": "2023-10-25T16:54:18Z",
            "published": "2023-10-25T16:54:18Z",
            "summary": "A local measurement extracts work as a backaction, e.g., in a system of two\nentangled cavities: first cavity, $a$, comprises a piston and the measurement\nis carried out on the second cavity, $b$. When no one makes a measurement on\nthe cavity $b$, i.e., it is simply placed in vacuum; environmental monitoring\nresults in the coherent states as the einselected pointer states (the\nmeasurement basis) [PRL 70, 1187 (1993)]. This makes the measurement, that\nnature itself performs, a Gaussian one with a fixed strength $\\lambda=1$. We\nshow that this makes nature assign a \\textit{fixed} amount of work to a\nparticular entanglement degree $0\\leq \\xi(r) \\leq 1$, i.e.,\n$W=\\xi(r)\\times(\\bar{n}\\hbar\\omega_a)$, nothing that the term in parenthesis is\nthe entire thermal energy. Afterwards, we show that this phenomenon applies\nquite generally, i.e, not restricted to a two-cavities system. We also touch on\nthe influence of inherited symmterization entanglement in this context. We can\narrive an additional phenomenon by considering that work is simply the process\nof converting randomly moving microscopic ingredients~(vanishing mean-velocity)\ninto a directional one, i.e, with a nonzero mean-velocity. We show that such a\nchange in the character of the motion introduces curvature in spacetime\naccording to general relativity. This phenomenon is the first demonstration of\na quantitative relation between entanglement and curvature using solely the\nquantum optics arguments.",
            "author": [
                "Rasim Volga Ovali",
                "Shakir Ullah",
                "Mehmet Gunay",
                "Mehmet Emre Tasgin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17809v1",
                "http://arxiv.org/pdf/2310.17809v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "gr-qc",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16765v1",
            "title": "How to Extend 3D GBSM to Integrated Sensing and Communication Channel\n  with Sharing Feature?",
            "updated": "2023-10-25T16:53:02Z",
            "published": "2023-10-25T16:53:02Z",
            "summary": "Integrated Sensing and Communication (ISAC) is a promising technology in 6G\nsystems. The existing 3D Geometry-Based Stochastic Model (GBSM), as\nstandardized for 5G systems, addresses solely communication channels and lacks\nconsideration of the integration with sensing channel. Therefore, this letter\nextends 3D GBSM to support ISAC research, with a particular focus on capturing\nthe sharing feature of both channels, including shared scatterers, clusters,\npaths, and similar propagation param-eters, which have been experimentally\nverified in the literature. The proposed approach can be summarized as follows:\nFirstly, an ISAC channel model is proposed, where shared and non-shared\ncomponents are superimposed for both communication and sensing. Secondly,\nsensing channel is characterized as a cascade of TX-target, radar cross\nsection, and target-RX, with the introduction of a novel parameter S for shared\ntarget extraction. Finally, an ISAC channel implementation framework is\nproposed, allowing flexible configuration of sharing feature and the joint\ngeneration of communication and sensing channels. The proposed ISAC channel\nmodel can be compatible with the 3GPP standards and offers promising support\nfor ISAC technology evaluation.",
            "author": [
                "Yameng Liu",
                "Jianhua Zhang",
                "Yuxiang Zhang",
                "Huiwen Gong",
                "Tao Jiang",
                "Guangyi Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16765v1",
                "http://arxiv.org/pdf/2310.16765v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16763v1",
            "title": "SuperHF: Supervised Iterative Learning from Human Feedback",
            "updated": "2023-10-25T16:52:00Z",
            "published": "2023-10-25T16:52:00Z",
            "summary": "While large language models demonstrate remarkable capabilities, they often\npresent challenges in terms of safety, alignment with human values, and\nstability during training. Here, we focus on two prevalent methods used to\nalign these models, Supervised Fine-Tuning (SFT) and Reinforcement Learning\nfrom Human Feedback (RLHF). SFT is simple and robust, powering a host of\nopen-source models, while RLHF is a more sophisticated method used in top-tier\nmodels like ChatGPT but also suffers from instability and susceptibility to\nreward hacking. We propose a novel approach, Supervised Iterative Learning from\nHuman Feedback (SuperHF), which seeks to leverage the strengths of both\nmethods. Our hypothesis is two-fold: that the reward model used in RLHF is\ncritical for efficient data use and model generalization and that the use of\nProximal Policy Optimization (PPO) in RLHF may not be necessary and could\ncontribute to instability issues. SuperHF replaces PPO with a simple supervised\nloss and a Kullback-Leibler (KL) divergence prior. It creates its own training\ndata by repeatedly sampling a batch of model outputs and filtering them through\nthe reward model in an online learning regime. We then break down the reward\noptimization problem into three components: robustly optimizing the training\nrewards themselves, preventing reward hacking-exploitation of the reward model\nthat degrades model performance-as measured by a novel METEOR similarity\nmetric, and maintaining good performance on downstream evaluations. Our\nexperimental results show SuperHF exceeds PPO-based RLHF on the training\nobjective, easily and favorably trades off high reward with low reward hacking,\nimproves downstream calibration, and performs the same on our GPT-4 based\nqualitative evaluation scheme all the while being significantly simpler to\nimplement, highlighting SuperHF's potential as a competitive language model\nalignment technique.",
            "author": [
                "Gabriel Mukobi",
                "Peter Chatain",
                "Su Fong",
                "Robert Windesheim",
                "Gitta Kutyniok",
                "Kush Bhatia",
                "Silas Alberti"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16763v1",
                "http://arxiv.org/pdf/2310.16763v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16762v1",
            "title": "An Infinite Needle in a Finite Haystack: Finding Infinite Counter-Models\n  in Deductive Verification",
            "updated": "2023-10-25T16:51:17Z",
            "published": "2023-10-25T16:51:17Z",
            "summary": "First-order logic, and quantifiers in particular, are widely used in\ndeductive verification. Quantifiers are essential for describing systems with\nunbounded domains, but prove difficult for automated solvers. Significant\neffort has been dedicated to finding quantifier instantiations that establish\nunsatisfiability, thus ensuring validity of a system's verification conditions.\nHowever, in many cases the formulas are satisfiable: this is often the case in\nintermediate steps of the verification process. For such cases, existing tools\nare limited to finding finite models as counterexamples. Yet, some quantified\nformulas are satisfiable but only have infinite models. Such infinite\ncounter-models are especially typical when first-order logic is used to\napproximate inductive definitions such as linked lists or the natural numbers.\nThe inability of solvers to find infinite models makes them diverge in these\ncases. In this paper, we tackle the problem of finding such infinite models.\nThese models allow the user to identify and fix bugs in the modeling of the\nsystem and its properties. Our approach consists of three parts. First, we\nintroduce symbolic structures as a way to represent certain infinite models.\nSecond, we describe an effective model finding procedure that symbolically\nexplores a given family of symbolic structures. Finally, we identify a new\ndecidable fragment of first-order logic that extends and subsumes the\nmany-sorted variant of EPR, where satisfiable formulas always have a model\nrepresentable by a symbolic structure within a known family. We evaluate our\napproach on examples from the domains of distributed consensus protocols and of\nheap-manipulating programs. Our implementation quickly finds infinite\ncounter-models that demonstrate the source of verification failures in a simple\nway, while SMT solvers and theorem provers such as Z3, cvc5, and Vampire\ndiverge.",
            "author": [
                "Neta Elad",
                "Oded Padon",
                "Sharon Shoham"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16762v1",
                "http://arxiv.org/pdf/2310.16762v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16761v1",
            "title": "IntenDD: A Unified Contrastive Learning Approach for Intent Detection\n  and Discovery",
            "updated": "2023-10-25T16:50:24Z",
            "published": "2023-10-25T16:50:24Z",
            "summary": "Identifying intents from dialogue utterances forms an integral component of\ntask-oriented dialogue systems. Intent-related tasks are typically formulated\neither as a classification task, where the utterances are classified into\npredefined categories or as a clustering task when new and previously unknown\nintent categories need to be discovered from these utterances. Further, the\nintent classification may be modeled in a multiclass (MC) or multilabel (ML)\nsetup. While typically these tasks are modeled as separate tasks, we propose\nIntenDD, a unified approach leveraging a shared utterance encoding backbone.\nIntenDD uses an entirely unsupervised contrastive learning strategy for\nrepresentation learning, where pseudo-labels for the unlabeled utterances are\ngenerated based on their lexical features. Additionally, we introduce a\ntwo-step post-processing setup for the classification tasks using modified\nadsorption. Here, first, the residuals in the training data are propagated\nfollowed by smoothing the labels both modeled in a transductive setting.\nThrough extensive evaluations on various benchmark datasets, we find that our\napproach consistently outperforms competitive baselines across all three tasks.\nOn average, IntenDD reports percentage improvements of 2.32%, 1.26%, and 1.52%\nin their respective metrics for few-shot MC, few-shot ML, and the intent\ndiscovery tasks respectively.",
            "author": [
                "Bhavuk Singhal",
                "Ashim Gupta",
                "Shivasankaran V P",
                "Amrith Krishna"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16761v1",
                "http://arxiv.org/pdf/2310.16761v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16757v1",
            "title": "All-rounder: A flexible DNN accelerator with diverse data format support",
            "updated": "2023-10-25T16:45:02Z",
            "published": "2023-10-25T16:45:02Z",
            "summary": "Recognizing the explosive increase in the use of DNN-based applications,\nseveral industrial companies developed a custom ASIC (e.g., Google TPU, IBM\nRaPiD, Intel NNP-I/NNP-T) and constructed a hyperscale cloud infrastructure\nwith it. The ASIC performs operations of the inference or training process of\nDNN models which are requested by users. Since the DNN models have different\ndata formats and types of operations, the ASIC needs to support diverse data\nformats and generality for the operations. However, the conventional ASICs do\nnot fulfill these requirements. To overcome the limitations of it, we propose a\nflexible DNN accelerator called All-rounder. The accelerator is designed with\nan area-efficient multiplier supporting multiple precisions of integer and\nfloating point datatypes. In addition, it constitutes a flexibly fusible and\nfissionable MAC array to support various types of DNN operations efficiently.\nWe implemented the register transfer level (RTL) design using Verilog and\nsynthesized it in 28nm CMOS technology. To examine practical effectiveness of\nour proposed designs, we designed two multiply units and three state-of-the-art\nDNN accelerators. We compare our multiplier with the multiply units and perform\narchitectural evaluation on performance and energy efficiency with eight\nreal-world DNN models. Furthermore, we compare benefits of the All-rounder\naccelerator to a high-end GPU card, i.e., NVIDIA GeForce RTX30390. The proposed\nAll-rounder accelerator universally has speedup and high energy efficiency in\nvarious DNN benchmarks than the baselines.",
            "author": [
                "Seock-Hwan Noh",
                "Seungpyo Lee",
                "Banseok Shin",
                "Sehun Park",
                "Yongjoo Jang",
                "Jaeha Kung"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16757v1",
                "http://arxiv.org/pdf/2310.16757v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16872v3",
            "title": "SonoSAMTrack -- Segment and Track Anything on Ultrasound Images",
            "updated": "2023-11-16T16:12:46Z",
            "published": "2023-10-25T16:42:26Z",
            "summary": "In this paper, we present SonoSAMTrack - that combines a promptable\nfoundational model for segmenting objects of interest on ultrasound images\ncalled SonoSAM, with a state-of-the art contour tracking model to propagate\nsegmentations on 2D+t and 3D ultrasound datasets. Fine-tuned and tested\nexclusively on a rich, diverse set of objects from $\\approx200$k ultrasound\nimage-mask pairs, SonoSAM demonstrates state-of-the-art performance on 7 unseen\nultrasound data-sets, outperforming competing methods by a significant margin.\nWe also extend SonoSAM to 2-D +t applications and demonstrate superior\nperformance making it a valuable tool for generating dense annotations and\nsegmentation of anatomical structures in clinical workflows. Further, to\nincrease practical utility of the work, we propose a two-step process of\nfine-tuning followed by knowledge distillation to a smaller footprint model\nwithout comprising the performance. We present detailed qualitative and\nquantitative comparisons of SonoSAM with state-of-the-art methods showcasing\nefficacy of the method. This is followed by demonstrating the reduction in\nnumber of clicks in a dense video annotation problem of adult cardiac\nultrasound chamber segmentation using SonoSAMTrack.",
            "author": [
                "Hariharan Ravishankar",
                "Rohan Patil",
                "Vikram Melapudi",
                "Harsh Suthar",
                "Stephan Anzengruber",
                "Parminder Bhatia",
                "Kass-Hout Taha",
                "Pavan Annangi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16872v3",
                "http://arxiv.org/pdf/2310.16872v3"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16755v1",
            "title": "HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning\n  in Large Language Models",
            "updated": "2023-10-25T16:41:15Z",
            "published": "2023-10-25T16:41:15Z",
            "summary": "Theory of Mind (ToM) is the ability to reason about one's own and others'\nmental states. ToM plays a critical role in the development of intelligence,\nlanguage understanding, and cognitive processes. While previous work has\nprimarily focused on first and second-order ToM, we explore higher-order ToM,\nwhich involves recursive reasoning on others' beliefs. We introduce HI-TOM, a\nHigher Order Theory of Mind benchmark. Our experimental evaluation using\nvarious Large Language Models (LLMs) indicates a decline in performance on\nhigher-order ToM tasks, demonstrating the limitations of current LLMs. We\nconduct a thorough analysis of different failure cases of LLMs, and share our\nthoughts on the implications of our findings on the future of NLP.",
            "author": [
                "Yinghui He",
                "Yufan Wu",
                "Yilin Jia",
                "Rada Mihalcea",
                "Yulong Chen",
                "Naihao Deng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16755v1",
                "http://arxiv.org/pdf/2310.16755v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16753v1",
            "title": "PROMINET: Prototype-based Multi-View Network for Interpretable Email\n  Response Prediction",
            "updated": "2023-10-25T16:39:00Z",
            "published": "2023-10-25T16:39:00Z",
            "summary": "Email is a widely used tool for business communication, and email marketing\nhas emerged as a cost-effective strategy for enterprises. While previous\nstudies have examined factors affecting email marketing performance, limited\nresearch has focused on understanding email response behavior by considering\nemail content and metadata. This study proposes a Prototype-based Multi-view\nNetwork (PROMINET) that incorporates semantic and structural information from\nemail data. By utilizing prototype learning, the PROMINET model generates\nlatent exemplars, enabling interpretable email response prediction. The model\nmaps learned semantic and structural exemplars to observed samples in the\ntraining data at different levels of granularity, such as document, sentence,\nor phrase. The approach is evaluated on two real-world email datasets: the\nEnron corpus and an in-house Email Marketing corpus. Experimental results\ndemonstrate that the PROMINET model outperforms baseline models, achieving a\n~3% improvement in F1 score on both datasets. Additionally, the model provides\ninterpretability through prototypes at different granularity levels while\nmaintaining comparable performance to non-interpretable models. The learned\nprototypes also show potential for generating suggestions to enhance email text\nediting and improve the likelihood of effective email responses. This research\ncontributes to enhancing sender-receiver communication and customer engagement\nin email interactions.",
            "author": [
                "Yuqing Wang",
                "Prashanth Vijayaraghavan",
                "Ehsan Degan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16753v1",
                "http://arxiv.org/pdf/2310.16753v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16749v1",
            "title": "DISCO: A Large Scale Human Annotated Corpus for Disfluency Correction in\n  Indo-European Languages",
            "updated": "2023-10-25T16:32:02Z",
            "published": "2023-10-25T16:32:02Z",
            "summary": "Disfluency correction (DC) is the process of removing disfluent elements like\nfillers, repetitions and corrections from spoken utterances to create readable\nand interpretable text. DC is a vital post-processing step applied to Automatic\nSpeech Recognition (ASR) outputs, before subsequent processing by downstream\nlanguage understanding tasks. Existing DC research has primarily focused on\nEnglish due to the unavailability of large-scale open-source datasets. Towards\nthe goal of multilingual disfluency correction, we present a high-quality\nhuman-annotated DC corpus covering four important Indo-European languages:\nEnglish, Hindi, German and French. We provide extensive analysis of results of\nstate-of-the-art DC models across all four languages obtaining F1 scores of\n97.55 (English), 94.29 (Hindi), 95.89 (German) and 92.97 (French). To\ndemonstrate the benefits of DC on downstream tasks, we show that DC leads to\n5.65 points increase in BLEU scores on average when used in conjunction with a\nstate-of-the-art Machine Translation (MT) system. We release code to run our\nexperiments along with our annotated dataset here.",
            "author": [
                "Vineet Bhat",
                "Preethi Jyothi",
                "Pushpak Bhattacharyya"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16749v1",
                "http://arxiv.org/pdf/2310.16749v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16746v1",
            "title": "HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis",
            "updated": "2023-10-25T16:23:17Z",
            "published": "2023-10-25T16:23:17Z",
            "summary": "Authorship Analysis, also known as stylometry, has been an essential aspect\nof Natural Language Processing (NLP) for a long time. Likewise, the recent\nadvancement of Large Language Models (LLMs) has made authorship analysis\nincreasingly crucial for distinguishing between human-written and AI-generated\ntexts. However, these authorship analysis tasks have primarily been focused on\nwritten texts, not considering spoken texts. Thus, we introduce the largest\nbenchmark for spoken texts - HANSEN (Human ANd ai Spoken tExt beNchmark).\nHANSEN encompasses meticulous curation of existing speech datasets accompanied\nby transcripts, alongside the creation of novel AI-generated spoken text\ndatasets. Together, it comprises 17 human datasets, and AI-generated spoken\ntexts created using 3 prominent LLMs: ChatGPT, PaLM2, and Vicuna13B. To\nevaluate and demonstrate the utility of HANSEN, we perform Authorship\nAttribution (AA) & Author Verification (AV) on human-spoken datasets and\nconducted Human vs. AI spoken text detection using state-of-the-art (SOTA)\nmodels. While SOTA methods, such as, character ngram or Transformer-based\nmodel, exhibit similar AA & AV performance in human-spoken datasets compared to\nwritten ones, there is much room for improvement in AI-generated spoken text\ndetection. The HANSEN benchmark is available at:\nhttps://huggingface.co/datasets/HANSEN-REPO/HANSEN.",
            "author": [
                "Nafis Irtiza Tripto",
                "Adaku Uchendu",
                "Thai Le",
                "Mattia Setzu",
                "Fosca Giannotti",
                "Dongwon Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16746v1",
                "http://arxiv.org/pdf/2310.16746v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16744v1",
            "title": "Simulating CDT quantum gravity",
            "updated": "2023-10-25T16:18:40Z",
            "published": "2023-10-25T16:18:40Z",
            "summary": "We provide a hands-on introduction to Monte Carlo simulations in\nnonperturbative lattice quantum gravity, formulated in terms of Causal\nDynamical Triangulations (CDT). We describe explicitly the implementation of\nMonte Carlo moves and the associated detailed-balance equations in two and\nthree spacetime dimensions. We discuss how to optimize data storage and\nretrieval, which are nontrivial due to the dynamical nature of the lattices,\nand how to reconstruct the full geometry from selected stored data. Various\naspects of the simulation, including tuning, thermalization and the measurement\nof observables are also treated. An associated open-source C++ implementation\ncode is freely available online.",
            "author": [
                "Joren Brunekreef",
                "Andrzej G\u00f6rlich",
                "Renate Loll"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16744v1",
                "http://arxiv.org/pdf/2310.16744v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "gr-qc",
                "hep-lat"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16743v1",
            "title": "Scalar mass conservation in turbulent mixture fraction based combustion\n  models through consistent local flow parameters",
            "updated": "2023-10-25T16:18:12Z",
            "published": "2023-10-25T16:18:12Z",
            "summary": "Mixture fraction-based models are widely employed for predicting turbulent\nnon-premixed combustion processes due to their cost-effectiveness and\nwell-established subfilter closure. In these models, the transport of reactive\nscalars in physical space is decomposed into two components: scalar transport\nrelative to mixture fraction and transport of mixture fraction in physical\nspace. Conventional flamelet models do not consider that these two processes\nhave to be formulated consistently, which can lead to scalar mass conservation\nerrors. In the context of multiphase flows, scalar transport in mixture\nfraction space is governed by three conditional flow-dependent parameters: the\nconditional scalar dissipation rate, the conditional scalar diffusion rate, and\nthe conditional spray source term. The evolution of mixture fraction in\nphysical space is typically modeled using the presumed Filtered Density\nFunction (FDF) approach. This paper introduces a novel formulation for the\nconditional flow parameters that aligns with the presumed FDF approach, thereby\nensuring scalar mass conservation. The proposed model is applied to a\nLarge-Eddy Simulation (LES) of the inert ECN Spray A case, with a comparison\nagainst a conventional flow parameter model that employs an inverse error\nfunction shape for the scalar dissipation rate. The results indicate that the\nconventional model produces similar conditional dissipation rates to the new\nmodel in regions where combustion takes place. However, significant\ndiscrepancies are observed in the conditional diffusion rate, highlighting the\nsusceptibility of the conventional model to scalar mass conservation errors for\nnon-unity Lewis number scalars.",
            "author": [
                "Marco Davidovic",
                "Heinz Pitsch"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16743v1",
                "http://arxiv.org/pdf/2310.16743v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16740v1",
            "title": "Reachability in Fixed VASS: Expressiveness and Lower Bounds",
            "updated": "2023-10-25T16:13:20Z",
            "published": "2023-10-25T16:13:20Z",
            "summary": "The recent years have seen remarkable progress in establishing the complexity\nof the reachability problem for vector addition systems with states (VASS),\nequivalently known as Petri nets. Existing work primarily considers the case in\nwhich both the VASS as well as the initial and target configurations are part\nof the input. In this paper, we investigate the reachability problem in the\nsetting where the VASS is fixed and only the initial configuration is variable.\nWe show that fixed VASS fully express arithmetic on initial segments of the\nnatural numbers. It follows that there is a very weak reduction from any fixed\nsuch number-theoretic predicate (e.g. primality or square-freeness) to\nreachability in fixed VASS where configurations are presented in unary. If\nconfigurations are given in binary, we show that there is a fixed VASS with\nfive counters whose reachability problem is PSPACE-hard.",
            "author": [
                "Andrei Draghici",
                "Christoph Haase",
                "Andrew Ryzhikov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16740v1",
                "http://arxiv.org/pdf/2310.16740v1"
            ],
            "primary_category": "cs.FL",
            "category": [
                "cs.FL",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16739v1",
            "title": "Gap-free 16-year (2005-2020) sub-diurnal surface meteorological\n  observations across Florida",
            "updated": "2023-10-25T16:12:48Z",
            "published": "2023-10-25T16:12:48Z",
            "summary": "The rather unique sub-tropical, flat, peninsular region of Florida is subject\nto a unique climate with extreme weather events across the year that impacts\nagriculture, public health, and management of natural resources. Meteorological\ndata at high temporal resolutions especially in the tropical latitudes are\nessential to understand diurnal and semi-diurnal variations of climate, which\nare considered to be the fundamental modes of climate variations of our Earth\nsystem. However, many meteorological datasets contain gaps that limit their use\nfor validation of models and further detailed observational analysis. The\nobjective of this paper is to apply a set of data gap filling strategies to\ndevelop a gap-free dataset with 15-minute observations for the sub-tropical\nregion of Florida. Using data from the Florida Automated Weather Network\n(FAWN), methods of linear interpolation, trend continuation, reference to\nexternal sources, and nearest station substitution were applied to fill in the\ndata gaps depending on the extent of the gap. The outcome of this study\nprovides continuous, publicly accessible surface meteorological observations\nfor 30 FAWN stations at 15-minute intervals for the years 2005-2020.",
            "author": [
                "Julie Peeling",
                "Jasmeet Judge",
                "Vasubandhu Misra",
                "C. B. Jayasankar",
                "Rick Lusher"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16739v1",
                "http://arxiv.org/pdf/2310.16739v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16738v1",
            "title": "Improving Conversational Recommendation Systems via Bias Analysis and\n  Language-Model-Enhanced Data Augmentation",
            "updated": "2023-10-25T16:11:55Z",
            "published": "2023-10-25T16:11:55Z",
            "summary": "Conversational Recommendation System (CRS) is a rapidly growing research area\nthat has gained significant attention alongside advancements in language\nmodelling techniques. However, the current state of conversational\nrecommendation faces numerous challenges due to its relative novelty and\nlimited existing contributions. In this study, we delve into benchmark datasets\nfor developing CRS models and address potential biases arising from the\nfeedback loop inherent in multi-turn interactions, including selection bias and\nmultiple popularity bias variants. Drawing inspiration from the success of\ngenerative data via using language models and data augmentation techniques, we\npresent two novel strategies, 'Once-Aug' and 'PopNudge', to enhance model\nperformance while mitigating biases. Through extensive experiments on ReDial\nand TG-ReDial benchmark datasets, we show a consistent improvement of CRS\ntechniques with our data augmentation approaches and offer additional insights\non addressing multiple newly formulated biases.",
            "author": [
                "Xi Wang",
                "Hossein A. Rahmani",
                "Jiqun Liu",
                "Emine Yilmaz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16738v1",
                "http://arxiv.org/pdf/2310.16738v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16732v1",
            "title": "A No-Reference Quality Assessment Method for Digital Human Head",
            "updated": "2023-10-25T16:01:05Z",
            "published": "2023-10-25T16:01:05Z",
            "summary": "In recent years, digital humans have been widely applied in augmented/virtual\nreality (A/VR), where viewers are allowed to freely observe and interact with\nthe volumetric content. However, the digital humans may be degraded with\nvarious distortions during the procedure of generation and transmission.\nMoreover, little effort has been put into the perceptual quality assessment of\ndigital humans. Therefore, it is urgent to carry out objective quality\nassessment methods to tackle the challenge of digital human quality assessment\n(DHQA). In this paper, we develop a novel no-reference (NR) method based on\nTransformer to deal with DHQA in a multi-task manner. Specifically, the front\n2D projections of the digital humans are rendered as inputs and the vision\ntransformer (ViT) is employed for the feature extraction. Then we design a\nmulti-task module to jointly classify the distortion types and predict the\nperceptual quality levels of digital humans. The experimental results show that\nthe proposed method well correlates with the subjective ratings and outperforms\nthe state-of-the-art quality assessment methods.",
            "author": [
                "Yingjie Zhou",
                "Zicheng Zhang",
                "Wei Sun",
                "Xiongkuo Min",
                "Xianghe Ma",
                "Guangtao Zhai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16732v1",
                "http://arxiv.org/pdf/2310.16732v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16731v1",
            "title": "Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning",
            "updated": "2023-10-25T16:00:47Z",
            "published": "2023-10-25T16:00:47Z",
            "summary": "Spatial reasoning over text is challenging as the models not only need to\nextract the direct spatial information from the text but also reason over those\nand infer implicit spatial relations. Recent studies highlight the struggles\neven large language models encounter when it comes to performing spatial\nreasoning over text. In this paper, we explore the potential benefits of\ndisentangling the processes of information extraction and reasoning in models\nto address this challenge. To explore this, we design various models that\ndisentangle extraction and reasoning(either symbolic or neural) and compare\nthem with state-of-the-art(SOTA) baselines with no explicit design for these\nparts. Our experimental results consistently demonstrate the efficacy of\ndisentangling, showcasing its ability to enhance models' generalizability\nwithin realistic data domains.",
            "author": [
                "Roshanak Mirzaee",
                "Parisa Kordjamshidi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16731v1",
                "http://arxiv.org/pdf/2310.16731v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16728v1",
            "title": "A Finely Segmented Semi-Monolithic Detector tailored for High Resolution\n  PET",
            "updated": "2023-10-25T15:56:36Z",
            "published": "2023-10-25T15:56:36Z",
            "summary": "Preclinical research and organ-dedicated applications require high-resolution\npositron emission tomography (PET) detectors to visualize small structures and\nunderstand biological processes at a finer level of detail. Current commercial\nsystems often employ finely pixelated or monolithic scintillators, each with\nits limitations. We present a semi-monolithic detector, tailored for\nhigh-resolution PET applications, and merging concepts of monolithic and\npixelated crystals. The detector features slabs measuring (24 x 10 x 1) sq. mm,\ncoupled to a 12 x 12 readout channel photosensor with 4 mm pitch. The slabs are\ngrouped in two arrays of 44 slabs each to achieve a higher optical photon\ndensity. We employ a fan beam collimator for fast calibration to train\nmachine-learning-based positioning models for all three dimensions, including\nslab identification and depth-of-interaction (DOI), utilizing gradient tree\nboosting (GTB). Energy calculation was based on a position-dependent energy\ncalibration. Using an analytical timing calibration, time skews were corrected\nfor coincidence timing resolution (CTR) estimation. Leveraging\nmachine-learning-based calibration in all three dimensions, we achieved high\ndetector spatial resolution: down to 1.18 mm full width at half maximum (FWHM)\ndetector spatial resolution and 0.75 mm mean absolute error (MAE) in the\nplanar-monolithic direction along the slabs, and 2.14 mm FWHM and 1.03 mm MAE\nfor depth-of-interaction (DOI) at an energy window of (435-585) keV. Correct\nslab interaction identification exceeded 80%, alongside an energy resolution of\n13.8% and a CTR of 450 ps FWHM. Therewith, the introduced finely segmented,\nhigh-resolution slab detector demonstrates an appealing performance suitable\nfor high-resolution PET applications. The current benchtop-based detector\ncalibration routine allows these detectors to be used in PET systems.",
            "author": [
                "Yannick Kuhl",
                "Florian Mueller",
                "Stephan Naunheim",
                "Matthias Bovelett",
                "Janko Lambertus",
                "David Schug",
                "Bjoern Weissler",
                "Eike Gegenmantel",
                "Pierre Gebhardt",
                "Volkmar Schulz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16728v1",
                "http://arxiv.org/pdf/2310.16728v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16727v1",
            "title": "AI Hazard Management: A framework for the systematic management of root\n  causes for AI risks",
            "updated": "2023-10-25T15:55:50Z",
            "published": "2023-10-25T15:55:50Z",
            "summary": "Recent advancements in the field of Artificial Intelligence (AI) establish\nthe basis to address challenging tasks. However, with the integration of AI,\nnew risks arise. Therefore, to benefit from its advantages, it is essential to\nadequately handle the risks associated with AI. Existing risk management\nprocesses in related fields, such as software systems, need to sufficiently\nconsider the specifics of AI. A key challenge is to systematically and\ntransparently identify and address AI risks' root causes - also called AI\nhazards. This paper introduces the AI Hazard Management (AIHM) framework, which\nprovides a structured process to systematically identify, assess, and treat AI\nhazards. The proposed process is conducted in parallel with the development to\nensure that any AI hazard is captured at the earliest possible stage of the AI\nsystem's life cycle. In addition, to ensure the AI system's auditability, the\nproposed framework systematically documents evidence that the potential impact\nof identified AI hazards could be reduced to a tolerable level. The framework\nbuilds upon an AI hazard list from a comprehensive state-of-the-art analysis.\nAlso, we provide a taxonomy that supports the optimal treatment of the\nidentified AI hazards. Additionally, we illustrate how the AIHM framework can\nincrease the overall quality of a power grid AI use case by systematically\nreducing the impact of identified hazards to an acceptable level.",
            "author": [
                "Ronald Schnitzer",
                "Andreas Hapfelmeier",
                "Sven Gaube",
                "Sonja Zillner"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16727v1",
                "http://arxiv.org/pdf/2310.16727v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16725v2",
            "title": "Radical Pair Model for Magnetic Field Effects on NMDA Receptor Activity",
            "updated": "2023-10-26T05:21:39Z",
            "published": "2023-10-25T15:53:31Z",
            "summary": "The N-methyl-D-aspartate receptor is a prominent player in brain development\nand functioning. Perturbations to its functioning through external stimuli like\nmagnetic fields can potentially affect the brain in numerous ways. Various\nstudies have shown that magnetic fields of varying strengths affect these\nreceptors. We propose that the radical pair mechanism, a quantum mechanical\nprocess, could explain some of these field effects. Radicals of the form\n$[\\mbox{RO}^\\bullet \\mbox{ Mg($\\mbox{H}_2$O$)_n$}^{+\\bullet}]$, where R is a\nprotein residue that can be Serine or Tyrosine, are considered for this study.\nThe variation in the singlet fractional yield of the radical pairs, as a\nfunction of magnetic field strength, is calculated to understand how the\nmagnetic field affects the products of the radical pair reactions. Based on the\nresults, the radical pair mechanism is a likely candidate for explaining the\nmagnetic field effects observed on the receptor activity. The model predicts\nchanges in the behaviour of the system as magnetic field strength is varied and\nalso predicts certain isotope effects. The results further suggest that similar\neffects on radical pairs could be a plausible explanation for various magnetic\nfield effects within the brain.",
            "author": [
                "Parvathy S Nair",
                "Hadi Zadeh-Haghighi",
                "Christoph Simon"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16725v2",
                "http://arxiv.org/pdf/2310.16725v2"
            ],
            "primary_category": "physics.bio-ph",
            "category": [
                "physics.bio-ph",
                "physics.chem-ph",
                "q-bio.BM",
                "q-bio.NC",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16724v1",
            "title": "Spherical Wavefront Near-Field DoA Estimation in THz Automotive Radar",
            "updated": "2023-10-25T15:51:00Z",
            "published": "2023-10-25T15:51:00Z",
            "summary": "Automotive radar at terahertz (THz) band has the potential to provide compact\ndesign. The availability of wide bandwidth at THz-band leads to high range\nresolution. Further, very narrow beamwidth arising from large arrays yields\nhigh angular resolution up to milli-degree level direction-of-arrival (DoA)\nestimation. At THz frequencies and extremely large arrays, the signal wavefront\nis spherical in the near-field that renders traditional far-field DoA\nestimation techniques unusable. In this work, we examine near-field DoA\nestimation for THz automotive radar. We propose an algorithm using multiple\nsignal classification (MUSIC) to estimate target DoAs and ranges while also\ntaking beam-squint in near-field into account. Using an array transformation\napproach, we compensate for near-field beam-squint in noise subspace\ncomputations to construct the beam-squint-free MUSIC spectra. Numerical\nexperiments show the effectiveness of the proposed method to accurately\nestimate the target parameters.",
            "author": [
                "Ahmet M. Elbir",
                "Kumar Vijay Mishra",
                "Symeon Chatzinotas"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16724v1",
                "http://arxiv.org/pdf/2310.16724v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16722v2",
            "title": "Assessing the Suitability of the Langevin Equation for Analyzing\n  Measured Data Through Downsampling",
            "updated": "2023-11-15T12:26:31Z",
            "published": "2023-10-25T15:49:50Z",
            "summary": "The measured time series from complex systems are renowned for their\nintricate stochastic behavior, characterized by random fluctuations stemming\nfrom external influences and nonlinear interactions. These fluctuations take\ndiverse forms, ranging from continuous trajectories reminiscent of Brownian\nmotion to noncontinuous trajectories featuring jump events. The Langevin\nequation serves as a powerful tool for generating stochasticity and capturing\nthe complex behavior of measured data with continuous stochastic\ncharacteristics. However, the traditional modeling framework of the Langevin\nequation falls short when it comes to capturing the presence of abrupt changes,\nparticularly jumps, in trajectories that exhibit non-continuity. Such\nnon-continuous changes pose a significant challenge for general processes and\nhave profound implications for risk management. Moreover, the discrete nature\nof observed physical phenomena, measured with a finite sample rate, adds\nanother layer of complexity. In such cases, data points often appear as a\nseries of discontinuous jumps, even when the underlying trajectory is\ncontinuous. In this study, we present an analytical framework that goes beyond\nthe limitations of the Langevin equation. Our approach effectively\ndistinguishes between diffusive or Brownian-type trajectories and trajectories\nwith jumps. By employing downsampling techniques, where we artificially lower\nthe sample rate, we derive a set of measures and criteria to analyze the data\nand differentiate between diffusive and non-diffusive behaviors. To further\ndemonstrate its versatility and practical applicability, we have applied our\nproposed method to real-world data in various scientific fields, turbulence,\noptical tweezers for trapped particles, neuroscience, renewable energy, and\nmarket price analysis.",
            "author": [
                "Pyei Phyo Lin",
                "Matthias W\u00e4chter",
                "Joachim Peinke",
                "M. Reza Rahimi Tabar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16722v2",
                "http://arxiv.org/pdf/2310.16722v2"
            ],
            "primary_category": "cond-mat.stat-mech",
            "category": [
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16716v1",
            "title": "Best practices for the manual curation of Intrinsically Disordered\n  Proteins in DisProt",
            "updated": "2023-10-25T15:39:17Z",
            "published": "2023-10-25T15:39:17Z",
            "summary": "The DisProt database is a significant resource containing manually curated\ndata on experimentally validated intrinsically disordered proteins (IDPs) and\nregions (IDRs) from the literature. Developed in 2005, its primary goal was to\ncollect structural and functional information into proteins that lack a fixed\nthree-dimensional (3D) structure. Today, DisProt has evolved into a major\nrepository that not only collects experimental data but also contributes\nsignificantly to our understanding of the IDPs/IDRs roles in various biological\nprocesses, such as autophagy or the life cycle mechanisms in viruses, or their\ninvolvement in diseases (such as cancer and neurodevelopmental disorders).\nDisProt offers detailed information on the structural states of IDPs/IDRs,\nincluding state transitions, interactions, and their functions, all provided as\ncurated annotations. One of the central activities of DisProt is the meticulous\ncuration of experimental data from the literature. For this reason, to ensure\nthat every expert and volunteer curator possesses the requisite knowledge for\ndata evaluation, collection, and integration, training courses and curation\nmaterials are available. However, biocuration guidelines concur on the\nimportance of developing robust guidelines that not only provide critical\ninformation about data consistency but also ensure data acquisition.This\nguideline aims to provide both biocurators and external users with best\npractices for manually curating IDPs and IDRs in DisProt. It describes every\nstep of the literature curation process and provides use cases of IDP curation\nwithin DisProt.\n  Database URL: https://disprot.org/",
            "author": [
                "Federica Quaglia",
                "Anastasia Chasapi",
                "Maria Victoria Nugnes",
                "Maria Cristina Aspromonte",
                "Emanuela Leonardi",
                "Damiano Piovesan",
                "Silvio C. E. Tosatto"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16716v1",
                "http://arxiv.org/pdf/2310.16716v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16714v2",
            "title": "STRAW-b (STRings for Absorption length in Water-b): the second\n  pathfinder mission for the Pacific Ocean Neutrino Experiment",
            "updated": "2023-10-26T16:03:02Z",
            "published": "2023-10-25T15:36:48Z",
            "summary": "Since 2018, the potential for a high-energy neutrino telescope, named the\nPacific Ocean Neutrino Experiment (P-ONE), has been thoroughly examined by two\npathfinder missions, STRAW and STRAW-b, short for short for Strings for\nAbsorption Length in Water. The P-ONE project seeks to install a neutrino\ndetector with a one cubic kilometer volume in the Cascadia Basin's deep marine\nsurroundings, situated near the western shores of Vancouver Island, Canada. To\nassess the environmental conditions and feasibility of constructing a neutrino\ndetector of that scale, the pathfinder missions, STRAW and STRAW-b, have been\ndeployed at a depth of 2.7 km within the designated site for P-ONE and were\nconnected to the NEPTUNE observatory, operated by Ocean Networks Canada (ONC).\nWhile STRAW focused on analyzing the optical properties of water in the\nCascadia Basin, \\ac{strawb} employed cameras and spectrometers to investigate\nthe characteristics of bioluminescence in the deep-sea environment. This report\nintroduces the STRAW-b concept, covering its scientific objectives and the\ninstrumentation used. Furthermore, it discusses the design considerations\nimplemented to guarantee a secure and dependable deployment process of STRAW-b.\nAdditionally, it showcases the data collected by battery-powered loggers, which\nmonitored the mechanical stress on the equipment throughout the deployment. The\nreport also offers an overview of STRAW-b's operation, with a specific emphasis\non the notable advancements achieved in the data acquisition (DAQ) system and\nits successful integration with the server infrastructure of ONC.",
            "author": [
                "Kilian Holzapfel",
                "Christian Spannfellner",
                "Omid Aghaei",
                "Andrew Baron",
                "Jeanette Bedard",
                "Michael B\u00f6hmer",
                "Jeff Bosma",
                "Nathan Deis",
                "Christopher Fink",
                "Christian Fruck",
                "Andreas G\u00e4rtner",
                "Roman Gernh\u00e4user",
                "Felix Henningsen",
                "Ryan Hotte",
                "Reyna Jenkyns",
                "Martina Karl",
                "Natascha Khera",
                "Nikhita Khera",
                "Ian Kulin",
                "Alex Lam",
                "Tim Lavallee",
                "Klaus Leism\u00fcller",
                "Laszlo Papp",
                "Benoit Pirenne",
                "Emily Price",
                "Tom Qiu",
                "Immacolata Carmen Rea",
                "Elisa Resconi",
                "Adrian Round",
                "Carsten Rott",
                "Albert Ruskey",
                "Li Ruohan",
                "Keita Sasaki",
                "Matt Tradewell",
                "Michael Traxler",
                "Daniele Vivolo",
                "Seann Wagner",
                "Eva Laura Winter",
                "Martin Wolf"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16714v2",
                "http://arxiv.org/pdf/2310.16714v2"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "physics.ins-det"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16713v2",
            "title": "SkyMath: Technical Report",
            "updated": "2023-10-26T09:01:16Z",
            "published": "2023-10-25T15:34:55Z",
            "summary": "Large language models (LLMs) have shown great potential to solve varieties of\nnatural language processing (NLP) tasks, including mathematical reasoning. In\nthis work, we present SkyMath, a large language model for mathematics with 13\nbillion parameters. By applying self-compare fine-tuning, we have enhanced\nmathematical reasoning abilities of Skywork-13B-Base remarkably. On GSM8K,\nSkyMath outperforms all known open-source models of similar size and has\nestablished a new SOTA performance.",
            "author": [
                "Liu Yang",
                "Haihua Yang",
                "Wenjun Cheng",
                "Lei Lin",
                "Chenxia Li",
                "Yifu Chen",
                "Lunan Liu",
                "Jianfei Pan",
                "Tianwen Wei",
                "Biye Li",
                "Liang Zhao",
                "Lijie Wang",
                "Bo Zhu",
                "Guoliang Li",
                "Xuejie Wu",
                "Xilin Luo",
                "Rui Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16713v2",
                "http://arxiv.org/pdf/2310.16713v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16712v1",
            "title": "LLM Performance Predictors are good initializers for Architecture Search",
            "updated": "2023-10-25T15:34:30Z",
            "published": "2023-10-25T15:34:30Z",
            "summary": "Large language models (LLMs) have become an integral component in solving a\nwide range of NLP tasks. In this work, we explore a novel use case of using\nLLMs to build performance predictors (PP): models that, given a specific deep\nneural network architecture, predict its performance on a downstream task. We\ndesign PP prompts for LLMs consisting of: (i) role: description of the role\nassigned to the LLM, (ii) instructions: set of instructions to be followed by\nthe LLM to carry out performance prediction, (iii) hyperparameters: a\ndefinition of each architecture-specific hyperparameter and (iv)\ndemonstrations: sample architectures along with their efficiency metrics and\n'training from scratch' performance. For machine translation (MT) tasks, we\ndiscover that GPT-4 with our PP prompts (LLM-PP) can predict the performance of\narchitecture with a mean absolute error matching the SOTA and a marginal\ndegradation in rank correlation coefficient compared to SOTA performance\npredictors. Further, we show that the predictions from LLM-PP can be distilled\nto a small regression model (LLM-Distill-PP). LLM-Distill-PP models\nsurprisingly retain the performance of LLM-PP largely and can be a\ncost-effective alternative for heavy use cases of performance estimation.\nSpecifically, for neural architecture search (NAS), we propose a Hybrid-Search\nalgorithm for NAS (HS-NAS), which uses LLM-Distill-PP for the initial part of\nsearch, resorting to the baseline predictor for rest of the search. We show\nthat HS-NAS performs very similar to SOTA NAS across benchmarks, reduces search\nhours by 50% roughly, and in some cases, improves latency, GFLOPs, and model\nsize.",
            "author": [
                "Ganesh Jawahar",
                "Muhammad Abdul-Mageed",
                "Laks V. S. Lakshmanan",
                "Dujian Ding"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16712v1",
                "http://arxiv.org/pdf/2310.16712v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16710v1",
            "title": "CP-like Symmetry with Discrete and Continuous Groups and CP\n  Violation/Restoration",
            "updated": "2023-10-25T15:25:54Z",
            "published": "2023-10-25T15:25:54Z",
            "summary": "We study physical implications of general CP symmetry including CP-like\nsymmetry. Various scattering amplitudes of CP asymmetry are calculated in\nCP-like symmetric models. We explicitly show that the CP-like transformation\nleads to a specific relation between different CP asymmetries. The resultant\nrelation is similar to the one obtained in GUT baryogenesis and sphaleron\nprocesses, where we also obtain a required condition for generating particle\nnumber asymmetry in CP-like symmetric models. In addition, we propose a\ngeneralization of a CP-like transformation for continuous symmetry groups.\nSince the CP transformation is an outer automorphism, which depends on the\ninternal symmetry group, it turns out that the physical CP and CP-like\nsymmetries can be mutually converted through the spontaneous symmetry breaking\n(SSB) of the internal symmetry. We investigate properties of physical CP\nasymmetry in both CP and CP-like symmetric phases, and find that the\nspontaneous CP violation and restoration can be observed even in models with\ncontinuous groups. We demonstrate that CP-like symmetric models with continuous\nLie groups can be naturally realized in physical CP symmetric models through\nthe SSB.",
            "author": [
                "Hiroshi Ohki",
                "Shohei Uemura"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16710v1",
                "http://arxiv.org/pdf/2310.16710v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16706v1",
            "title": "Nighttime Driver Behavior Prediction Using Taillight Signal Recognition\n  via CNN-SVM Classifier",
            "updated": "2023-10-25T15:23:33Z",
            "published": "2023-10-25T15:23:33Z",
            "summary": "This paper aims to enhance the ability to predict nighttime driving behavior\nby identifying taillights of both human-driven and autonomous vehicles. The\nproposed model incorporates a customized detector designed to accurately detect\nfront-vehicle taillights on the road. At the beginning of the detector, a\nlearnable pre-processing block is implemented, which extracts deep features\nfrom input images and calculates the data rarity for each feature. In the next\nstep, drawing inspiration from soft attention, a weighted binary mask is\ndesigned that guides the model to focus more on predetermined regions. This\nresearch utilizes Convolutional Neural Networks (CNNs) to extract\ndistinguishing characteristics from these areas, then reduces dimensions using\nPrincipal Component Analysis (PCA). Finally, the Support Vector Machine (SVM)\nis used to predict the behavior of the vehicles. To train and evaluate the\nmodel, a large-scale dataset is collected from two types of dash-cams and\nInsta360 cameras from the rear view of Ford Motor Company vehicles. This\ndataset includes over 12k frames captured during both daytime and nighttime\nhours. To address the limited nighttime data, a unique pixel-wise image\nprocessing technique is implemented to convert daytime images into realistic\nnight images. The findings from the experiments demonstrate that the proposed\nmethodology can accurately categorize vehicle behavior with 92.14% accuracy,\n97.38% specificity, 92.09% sensitivity, 92.10% F1-measure, and 0.895 Cohen's\nKappa Statistic. Further details are available at\nhttps://github.com/DeepCar/Taillight_Recognition.",
            "author": [
                "Amir Hossein Barshooi",
                "Elmira Bagheri"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16706v1",
                "http://arxiv.org/pdf/2310.16706v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16705v1",
            "title": "Wasserstein Gradient Flow over Variational Parameter Space for\n  Variational Inference",
            "updated": "2023-10-25T15:20:53Z",
            "published": "2023-10-25T15:20:53Z",
            "summary": "Variational inference (VI) can be cast as an optimization problem in which\nthe variational parameters are tuned to closely align a variational\ndistribution with the true posterior. The optimization task can be approached\nthrough vanilla gradient descent in black-box VI or natural-gradient descent in\nnatural-gradient VI. In this work, we reframe VI as the optimization of an\nobjective that concerns probability distributions defined over a\n\\textit{variational parameter space}. Subsequently, we propose Wasserstein\ngradient descent for tackling this optimization problem. Notably, the\noptimization techniques, namely black-box VI and natural-gradient VI, can be\nreinterpreted as specific instances of the proposed Wasserstein gradient\ndescent. To enhance the efficiency of optimization, we develop practical\nmethods for numerically solving the discrete gradient flows. We validate the\neffectiveness of the proposed methods through empirical experiments on a\nsynthetic dataset, supplemented by theoretical analyses.",
            "author": [
                "Dai Hai Nguyen",
                "Tetsuya Sakurai",
                "Hiroshi Mamitsuka"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16705v1",
                "http://arxiv.org/pdf/2310.16705v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16697v1",
            "title": "$O(1/\\varepsilon)$ is the answer in online weighted throughput\n  maximization",
            "updated": "2023-10-25T15:11:18Z",
            "published": "2023-10-25T15:11:18Z",
            "summary": "We study a fundamental online scheduling problem where jobs with processing\ntimes, weights, and deadlines arrive online over time at their release dates.\nThe task is to preemptively schedule these jobs on a single or multiple\n(possibly unrelated) machines with the objective to maximize the weighted\nthroughput, the total weight of jobs that complete before their deadline. To\novercome known lower bounds for the competitive analysis, we assume that each\njob arrives with some slack $\\varepsilon > 0$; that is, the time window for\nprocessing job $j$ on any machine $i$ on which it can be executed has length at\nleast $(1+\\varepsilon)$ times $j$'s processing time on machine $i$. Our\ncontribution is a best possible online algorithm for weighted throughput\nmaximization on unrelated machines: Our algorithm is\n$O\\big(\\frac1\\varepsilon\\big)$-competitive, which matches the lower bound for\nunweighted throughput maximization on a single machine. Even for a single\nmachine, it was not known whether the problem with weighted jobs is \"harder\"\nthan the problem with unweighted jobs. Thus, we answer this question and close\nweighted throughput maximization on a single machine with a best possible\ncompetitive ratio $\\Theta\\big(\\frac1\\varepsilon\\big)$. While we focus on\nnon-migratory schedules, our algorithm achieves the same (up to constants)\nperformance guarantee when compared to an optimal migratory schedule.",
            "author": [
                "Franziska Eberle"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16697v1",
                "http://arxiv.org/pdf/2310.16697v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16693v1",
            "title": "Long term behavior of the stirred vacuum on a Dirac chain: geometry blur\n  and the random Slater ensemble",
            "updated": "2023-10-25T15:04:03Z",
            "published": "2023-10-25T15:04:03Z",
            "summary": "We characterize the long-term state of the 1D Dirac vacuum stirred by an\nimpenetrable object, modeled as the ground state of a finite free-fermionic\nchain dynamically perturbed by a moving classical obstacle which suppresses the\nlocal hopping amplitudes. We find two different regimes, depending on the\nvelocity of the obstacle. For a slow motion, the effective Floquet Hamiltonian\npresents features which are typical of the Gaussian orthogonal ensemble, and\nthe occupation of the Floquet modes becomes roughly homogeneous. Moreover, the\nlong term entanglement entropy of a contiguous block follows a Gaussian\nanalogue of Page's law, i.e. a volumetric behavior. Indeed, the statistical\nproperties of the reduced density matrices correspond to those of a random\nSlater determinant, which can be described using the Jacobi ensemble from\nrandom matrix theory. On the other hand, if the obstacle moves fast enough, the\neffective Floquet Hamiltonian presents a Poissonian behavior. The nature of the\ntransition is clarified by the entanglement links, which determine the\neffective geometry underlying the entanglement structure, showing that the\none-dimensionality of the physical Hamiltonian dissolves into a random\nadjacency matrix as we slow down the obstacle motion.",
            "author": [
                "Jos\u00e9 Vinaixa",
                "Bego\u00f1a Mula",
                "Alfredo Dea\u00f1o",
                "Silvia N. Santalla",
                "Javier Rodr\u00edguez-Laguna"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16693v1",
                "http://arxiv.org/pdf/2310.16693v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16691v1",
            "title": "Field-Theory of Active Chiral Hard Disks: A First-Principles Approach to\n  Steric Interactions",
            "updated": "2023-10-25T14:57:11Z",
            "published": "2023-10-25T14:57:11Z",
            "summary": "A first-principles approach for active chiral hard disks is presented, that\nexplicitly accounts for steric interactions on the two-body level. We derive an\neffective one-body equation for the joint probability distribution of positions\nand angles of the particles. By projecting on the angular modes, we write a\nhierarchy for the lowest hydrodynamic modes, i.e. particle density,\npolarization, and nematic tensor. By undimensionalising the equations, we\nhighlight the assumptions, which - though inherent - are often included\nimplicit in closing the hierarchy for finally arriving at an effective\nfield-theoretical equation for the particle density. By considering different\nregimes of the P{\\'e}clet number, the well-known models in active matter can be\nobtained through our conisderation. Explicitly, we derive the phenomenological\nModel B and by going to higher orders in the closure scheme, we show that this\nfirst-principles approach results in the recently introduced Active Model B +,\na natural extension of the Model B for active processes. Remarkably, here we\nfind that chirality can change the sign of the phenomonological activity\nparameters.",
            "author": [
                "Erik Kalz",
                "Abhinav Sharma",
                "Ralf Metzler"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16691v1",
                "http://arxiv.org/pdf/2310.16691v1"
            ],
            "primary_category": "cond-mat.stat-mech",
            "category": [
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16690v2",
            "title": "Dynamic treatment effect phenotyping through functional survival\n  analysis",
            "updated": "2023-10-26T08:59:45Z",
            "published": "2023-10-25T14:53:16Z",
            "summary": "In recent years, research interest in personalised treatments has been\ngrowing. However, treatment effect heterogeneity and possibly time-varying\ntreatment effects are still often overlooked in clinical studies. Statistical\ntools are needed for the identification of treatment response patterns, taking\ninto account that treatment response is not constant over time. We aim to\nprovide an innovative method to obtain dynamic treatment effect phenotypes on a\ntime-to-event outcome, conditioned on a set of relevant effect modifiers. The\nproposed method does not require the assumption of proportional hazards for the\ntreatment effect, which is rarely realistic. We propose a spline-based survival\nneural network, inspired by the Royston-Parmar survival model, to estimate\ntime-varying conditional treatment effects. We then exploit the functional\nnature of the resulting estimates to apply a functional clustering of the\ntreatment effect curves in order to identify different patterns of treatment\neffects. The application that motivated this work is the discontinuation of\ntreatment with Mineralocorticoid receptor Antagonists (MRAs) in patients with\nheart failure, where there is no clear evidence as to which patients it is the\nsafest choice to discontinue treatment and, conversely, when it leads to a\nhigher risk of adverse events. The data come from an electronic health record\ndatabase. A simulation study was performed to assess the performance of the\nspline-based neural network and the stability of the treatment response\nphenotyping procedure. In light of the results, the suggested approach has the\npotential to support personalized medical choices by assessing unique treatment\nresponses in various medical contexts over a period of time.",
            "author": [
                "Caterina Gregorio",
                "Giovanni Baj",
                "Giulia Barbati",
                "Francesca Ieva"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16690v2",
                "http://arxiv.org/pdf/2310.16690v2"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16688v1",
            "title": "Learning-based adaption of robotic friction models",
            "updated": "2023-10-25T14:50:15Z",
            "published": "2023-10-25T14:50:15Z",
            "summary": "In the Fourth Industrial Revolution, wherein artificial intelligence and the\nautomation of machines occupy a central role, the deployment of robots is\nindispensable. However, the manufacturing process using robots, especially in\ncollaboration with humans, is highly intricate. In particular, modeling the\nfriction torque in robotic joints is a longstanding problem due to the lack of\na good mathematical description. This motivates the usage of data-driven\nmethods in recent works. However, model-based and data-driven models often\nexhibit limitations in their ability to generalize beyond the specific dynamics\nthey were trained on, as we demonstrate in this paper. To address this\nchallenge, we introduce a novel approach based on residual learning, which aims\nto adapt an existing friction model to new dynamics using as little data as\npossible. We validate our approach by training a base neural network on a\nsymmetric friction data set to learn an accurate relation between the velocity\nand the friction torque. Subsequently, to adapt to more complex asymmetric\nsettings, we train a second network on a small dataset, focusing on predicting\nthe residual of the initial network's output. By combining the output of both\nnetworks in a suitable manner, our proposed estimator outperforms the\nconventional model-based approach and the base neural network significantly.\nFurthermore, we evaluate our method on trajectories involving external loads\nand still observe a substantial improvement, approximately 60-70\\%, over the\nconventional approach. Our method does not rely on data with external load\nduring training, eliminating the need for external torque sensors. This\ndemonstrates the generalization capability of our approach, even with a small\namount of data-only 43 seconds of a robot movement-enabling adaptation to\ndiverse scenarios based on prior knowledge about friction in different\nsettings.",
            "author": [
                "Philipp Scholl",
                "Maged Iskandar",
                "Sebastian Wolf",
                "Jinoh Lee",
                "Aras Bacho",
                "Alexander Dietrich",
                "Alin Albu-Sch\u00e4ffer",
                "Gitta Kutyniok"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16688v1",
                "http://arxiv.org/pdf/2310.16688v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16685v1",
            "title": "Detection of news written by the ChatGPT through authorship attribution\n  performed by a Bidirectional LSTM model",
            "updated": "2023-10-25T14:48:58Z",
            "published": "2023-10-25T14:48:58Z",
            "summary": "The large language based-model chatbot ChatGPT gained a lot of popularity\nsince its launch and has been used in a wide range of situations. This research\ncenters around a particular situation, when the ChatGPT is used to produce news\nthat will be consumed by the population, causing the facilitation in the\nproduction of fake news, spread of misinformation and lack of trust in news\nsources. Aware of these problems, this research aims to build an artificial\nintelligence model capable of performing authorship attribution on news\narticles, identifying the ones written by the ChatGPT. To achieve this goal, a\ndataset containing equal amounts of human and ChatGPT written news was\nassembled and different natural processing language techniques were used to\nextract features from it that were used to train, validate and test three\nmodels built with different techniques. The best performance was produced by\nthe Bidirectional Long Short Term Memory (LSTM) Neural Network model, achiving\n91.57\\% accuracy when tested against the data from the testing set.",
            "author": [
                "Amanda Ferrari Iaquinta",
                "Gustavo Voltani von Atzingen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16685v1",
                "http://arxiv.org/pdf/2310.16685v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16681v1",
            "title": "BabyStories: Can Reinforcement Learning Teach Baby Language Models to\n  Write Better Stories?",
            "updated": "2023-10-25T14:45:48Z",
            "published": "2023-10-25T14:45:48Z",
            "summary": "Language models have seen significant growth in the size of their corpus,\nleading to notable performance improvements. Yet, there has been limited\nprogress in developing models that handle smaller, more human-like datasets. As\npart of the BabyLM shared task, this study explores the impact of reinforcement\nlearning from human feedback (RLHF) on language models pretrained from scratch\nwith a limited training corpus. Comparing two GPT-2 variants, the larger model\nperforms better in storytelling tasks after RLHF fine-tuning. These findings\nsuggest that RLHF techniques may be more advantageous for larger models due to\ntheir higher learning and adaptation capacity, though more experiments are\nneeded to confirm this finding. These insights highlight the potential benefits\nof RLHF fine-tuning for language models within limited data, enhancing their\nability to maintain narrative focus and coherence while adhering better to\ninitial instructions in storytelling tasks. The code for this work is publicly\nat https://github.com/Zephyr1022/BabyStories-UTSA.",
            "author": [
                "Xingmeng Zhao",
                "Tongnian Wang",
                "Sheri Osborn",
                "Anthony Rios"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16681v1",
                "http://arxiv.org/pdf/2310.16681v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16680v1",
            "title": "Manipulation of hybrid skyrmion dynamics by step DMI approach",
            "updated": "2023-10-25T14:44:51Z",
            "published": "2023-10-25T14:44:51Z",
            "summary": "The dynamic behavior of non-collinear atomic spin arrangements in a\ntopologically protected magnetic skyrmion plays a pivotal role in potential\nfuture spintronic technologies, including racetrack memory based\nultra-high-density storage devices. However, the topological nature of the\nskyrmion comes with an unwanted skyrmion Hall effect (SkHE) that poses a\nsignificant challenge in the practical application. Here we present a detailed\nmicromagnetic simulation study that delves into the controlled manipulation of\nskyrmion dynamics through a subtle engineering of Dzyaloshinskii-Moriya\ninteraction (DMI) in a hybrid skyrmion racetrack. In particular, we introduce a\ngradient variation of bulk and interfacial DMI that results into a parabolic\ntrajectory of Skyrmion Hall angle (SkHA), thereby allowing us to find a\ncritical DMI ratio with almost zero SkHE. Most importantly, we present a novel\napproach involving engineering of a racetrack with strategically placed step\nDMI regions that gives us a meticulous control over the size and speed of the\nhybrid skyrmions. The present study gives a new direction for the simultaneous\nrealization of stable skyrmions without SkHE and increased skyrmion speed with\noptimized DMI engineering.",
            "author": [
                "Hitesh Chhabra",
                "Jayaseelan Dhakshinamoorthy",
                "Ajaya K. Nayak"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16680v1",
                "http://arxiv.org/pdf/2310.16680v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16677v1",
            "title": "Machine Learning Approaches for Fine-Grained Symptom Estimation in\n  Schizophrenia: A Comprehensive Review",
            "updated": "2023-10-25T14:42:58Z",
            "published": "2023-10-25T14:42:58Z",
            "summary": "Schizophrenia is a severe yet treatable mental disorder, it is diagnosed\nusing a multitude of primary and secondary symptoms. Diagnosis and treatment\nfor each individual depends on the severity of the symptoms, therefore there is\na need for accurate, personalised assessments. However, the process can be both\ntime-consuming and subjective; hence, there is a motivation to explore\nautomated methods that can offer consistent diagnosis and precise symptom\nassessments, thereby complementing the work of healthcare practitioners.\nMachine Learning has demonstrated impressive capabilities across numerous\ndomains, including medicine; the use of Machine Learning in patient assessment\nholds great promise for healthcare professionals and patients alike, as it can\nlead to more consistent and accurate symptom estimation.This survey aims to\nreview methodologies that utilise Machine Learning for diagnosis and assessment\nof schizophrenia. Contrary to previous reviews that primarily focused on binary\nclassification, this work recognises the complexity of the condition and\ninstead, offers an overview of Machine Learning methods designed for\nfine-grained symptom estimation. We cover multiple modalities, namely Medical\nImaging, Electroencephalograms and Audio-Visual, as the illness symptoms can\nmanifest themselves both in a patient's pathology and behaviour. Finally, we\nanalyse the datasets and methodologies used in the studies and identify trends,\ngaps as well as opportunities for future research.",
            "author": [
                "Niki Maria Foteinopoulou",
                "Ioannis Patras"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16677v1",
                "http://arxiv.org/pdf/2310.16677v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16676v2",
            "title": "SSLCL: An Efficient Model-Agnostic Supervised Contrastive Learning\n  Framework for Emotion Recognition in Conversations",
            "updated": "2023-11-18T11:50:52Z",
            "published": "2023-10-25T14:41:14Z",
            "summary": "Emotion recognition in conversations (ERC) is a rapidly evolving task within\nthe natural language processing community, which aims to detect the emotions\nexpressed by speakers during a conversation. Recently, a growing number of ERC\nmethods have focused on leveraging supervised contrastive learning (SCL) to\nenhance the robustness and generalizability of learned features. However,\ncurrent SCL-based approaches in ERC are impeded by the constraint of large\nbatch sizes and the lack of compatibility with most existing ERC models. To\naddress these challenges, we propose an efficient and model-agnostic SCL\nframework named Supervised Sample-Label Contrastive Learning with Soft-HGR\nMaximal Correlation (SSLCL), which eliminates the need for a large batch size\nand can be seamlessly integrated with existing ERC models without introducing\nany model-specific assumptions. Specifically, we introduce a novel perspective\non utilizing label representations by projecting discrete labels into dense\nembeddings through a shallow multilayer perceptron, and formulate the training\nobjective to maximize the similarity between sample features and their\ncorresponding ground-truth label embeddings, while minimizing the similarity\nbetween sample features and label embeddings of disparate classes. Moreover, we\ninnovatively adopt the Soft-HGR maximal correlation as a measure of similarity\nbetween sample features and label embeddings, leading to significant\nperformance improvements over conventional similarity measures. Additionally,\nmultimodal cues of utterances are effectively leveraged by SSLCL as data\naugmentations to boost model performances. Extensive experiments on two ERC\nbenchmark datasets, IEMOCAP and MELD, demonstrate the compatibility and\nsuperiority of our proposed SSLCL framework compared to existing\nstate-of-the-art SCL methods. Our code is available at\n\\url{https://github.com/TaoShi1998/SSLCL}.",
            "author": [
                "Tao Shi",
                "Xiao Liang",
                "Yaoyuan Liang",
                "Xinyi Tong",
                "Shao-Lun Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16676v2",
                "http://arxiv.org/pdf/2310.16676v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16673v1",
            "title": "Exploring Large Language Models for Code Explanation",
            "updated": "2023-10-25T14:38:40Z",
            "published": "2023-10-25T14:38:40Z",
            "summary": "Automating code documentation through explanatory text can prove highly\nbeneficial in code understanding. Large Language Models (LLMs) have made\nremarkable strides in Natural Language Processing, especially within software\nengineering tasks such as code generation and code summarization. This study\nspecifically delves into the task of generating natural-language summaries for\ncode snippets, using various LLMs. The findings indicate that Code LLMs\noutperform their generic counterparts, and zero-shot methods yield superior\nresults when dealing with datasets with dissimilar distributions between\ntraining and testing sets.",
            "author": [
                "Paheli Bhattacharya",
                "Manojit Chakraborty",
                "Kartheek N S N Palepu",
                "Vikas Pandey",
                "Ishan Dindorkar",
                "Rakesh Rajpurohit",
                "Rishabh Gupta"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16673v1",
                "http://arxiv.org/pdf/2310.16673v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.AI",
                "cs.IR",
                "D.2.3; I.7"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16667v1",
            "title": "CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary\n  Object Detection",
            "updated": "2023-10-25T14:31:02Z",
            "published": "2023-10-25T14:31:02Z",
            "summary": "Deriving reliable region-word alignment from image-text pairs is critical to\nlearn object-level vision-language representations for open-vocabulary object\ndetection. Existing methods typically rely on pre-trained or self-trained\nvision-language models for alignment, which are prone to limitations in\nlocalization accuracy or generalization capabilities. In this paper, we propose\nCoDet, a novel approach that overcomes the reliance on pre-aligned\nvision-language space by reformulating region-word alignment as a co-occurring\nobject discovery problem. Intuitively, by grouping images that mention a shared\nconcept in their captions, objects corresponding to the shared concept shall\nexhibit high co-occurrence among the group. CoDet then leverages visual\nsimilarities to discover the co-occurring objects and align them with the\nshared concept. Extensive experiments demonstrate that CoDet has superior\nperformances and compelling scalability in open-vocabulary detection, e.g., by\nscaling up the visual backbone, CoDet achieves 37.0 $\\text{AP}^m_{novel}$ and\n44.7 $\\text{AP}^m_{all}$ on OV-LVIS, surpassing the previous SoTA by 4.2\n$\\text{AP}^m_{novel}$ and 9.8 $\\text{AP}^m_{all}$. Code is available at\nhttps://github.com/CVMI-Lab/CoDet.",
            "author": [
                "Chuofan Ma",
                "Yi Jiang",
                "Xin Wen",
                "Zehuan Yuan",
                "Xiaojuan Qi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16667v1",
                "http://arxiv.org/pdf/2310.16667v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16663v1",
            "title": "Emergence of multifractality through cascade-like transitions in a\n  mosaic interpolating Aubry-Andr\u00e9-Fibonacci chain",
            "updated": "2023-10-25T14:24:08Z",
            "published": "2023-10-25T14:24:08Z",
            "summary": "In this paper, we explore the localization features of wave functions in a\nfamily of mosaic quasiperiodic chains obtained by continuously interpolating\nbetween two limits: the mosaic Aubry-Andr\\'{e} (AA) model, known for its exact\nmobility edges with extended states in the band-center region, and localized\nones in the band-edge regions for a large enough modulation amplitude, and the\nmosaic Fibonacci chain, which exhibits its multifractal nature for all the\nstates except for the extended one with $E=0$ for an arbitrary finite\nmodulation amplitude. We discover that the mosaic AA limit for the states in\nthe band-edge regions evolves into multifractal ones through a cascade of\ndelocalization transitions. This cascade shows lobes of lower fractal dimension\nvalues separated by maxima of fractal dimension. In contrast, the states in the\nband-center region (except for the $E=0$ state) display an anomalous cascading\nprocess, where it emerges lobes of higher fractal dimension values are\nseparated by the regions with lower fractal dimensions. Our findings offer\ninsight into understanding the multifractality of quasiperiodic chains.",
            "author": [
                "Qi Dai",
                "Zhanpeng Lu",
                "Zhihao Xu"
            ],
            "link": [
                "http://dx.doi.org/10.1103/PhysRevB.108.144207",
                "http://arxiv.org/abs/2310.16663v1",
                "http://arxiv.org/pdf/2310.16663v1"
            ],
            "primary_category": "cond-mat.dis-nn",
            "category": [
                "cond-mat.dis-nn",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16662v1",
            "title": "Deep Learning Techniques for Cervical Cancer Diagnosis based on\n  Pathology and Colposcopy Images",
            "updated": "2023-10-25T14:23:40Z",
            "published": "2023-10-25T14:23:40Z",
            "summary": "Cervical cancer is a prevalent disease affecting millions of women worldwide\nevery year. It requires significant attention, as early detection during the\nprecancerous stage provides an opportunity for a cure. The screening and\ndiagnosis of cervical cancer rely on cytology and colposcopy methods. Deep\nlearning, a promising technology in computer vision, has emerged as a potential\nsolution to improve the accuracy and efficiency of cervical cancer screening\ncompared to traditional clinical inspection methods that are prone to human\nerror. This review article discusses cervical cancer and its screening\nprocesses, followed by the Deep Learning training process and the\nclassification, segmentation, and detection tasks for cervical cancer\ndiagnosis. Additionally, we explored the most common public datasets used in\nboth cytology and colposcopy and highlighted the popular and most utilized\narchitectures that researchers have applied to both cytology and colposcopy. We\nreviewed 24 selected practical papers in this study and summarized them. This\narticle highlights the remarkable efficiency in enhancing the precision and\nspeed of cervical cancer analysis by Deep Learning, bringing us closer to early\ndiagnosis and saving lives.",
            "author": [
                "Hana Ahmadzadeh Sarhangi",
                "Dorsa Beigifard",
                "Elahe Farmani",
                "Hamidreza Bolhasani"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16662v1",
                "http://arxiv.org/pdf/2310.16662v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16661v1",
            "title": "The role of atomic interactions in cavity-induced continuous time\n  crystals",
            "updated": "2023-10-25T14:21:54Z",
            "published": "2023-10-25T14:21:54Z",
            "summary": "We consider continuous time-crystalline phases in dissipative many-body\nsystems of atoms in cavities, focusing on the role of short-range interatomic\ninteractions. First, we show that the latter can alter the nature of the time\ncrystal by changing the type of the underlying critical bifurcation. Second, we\ncharacterize the heating mechanism and dynamics resulting from the short-range\ninteractions and demonstrate that they make the time crystal inherently\nmetastable. We argue that this is generic for the broader class of dissipative\ntime crystals in atom-cavity systems whenever the cavity loss rate is\ncomparable to the atomic recoil energy. We observe that such a scenario for\nheating resembles the one proposed for preheating of the early universe, where\nthe oscillating coherent inflation field decays into a cascade of exponentially\ngrowing fluctuations. By extending approaches for dissipative dynamical systems\nto our many-body problem, we obtain analytical predictions for the parameters\ndescribing the phase transition and the heating rate inside the\ntime-crystalline phase. We underpin and extend the analytical predictions of\nthe heating rates with numerical simulations.",
            "author": [
                "Christian H. Johansen",
                "Johannes Lang",
                "Francesco Piazza"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16661v1",
                "http://arxiv.org/pdf/2310.16661v1"
            ],
            "primary_category": "cond-mat.quant-gas",
            "category": [
                "cond-mat.quant-gas",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16654v1",
            "title": "ChatGPT is a Potential Zero-Shot Dependency Parser",
            "updated": "2023-10-25T14:08:39Z",
            "published": "2023-10-25T14:08:39Z",
            "summary": "Pre-trained language models have been widely used in dependency parsing task\nand have achieved significant improvements in parser performance. However, it\nremains an understudied question whether pre-trained language models can\nspontaneously exhibit the ability of dependency parsing without introducing\nadditional parser structure in the zero-shot scenario. In this paper, we\npropose to explore the dependency parsing ability of large language models such\nas ChatGPT and conduct linguistic analysis. The experimental results\ndemonstrate that ChatGPT is a potential zero-shot dependency parser, and the\nlinguistic analysis also shows some unique preferences in parsing outputs.",
            "author": [
                "Boda Lin",
                "Xinyi Zhou",
                "Binghao Tang",
                "Xiaocheng Gong",
                "Si Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16654v1",
                "http://arxiv.org/pdf/2310.16654v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16648v1",
            "title": "Posterior Consistency for Missing Data in Variational Autoencoders",
            "updated": "2023-10-25T13:56:02Z",
            "published": "2023-10-25T13:56:02Z",
            "summary": "We consider the problem of learning Variational Autoencoders (VAEs), i.e., a\ntype of deep generative model, from data with missing values. Such data is\nomnipresent in real-world applications of machine learning because complete\ndata is often impossible or too costly to obtain. We particularly focus on\nimproving a VAE's amortized posterior inference, i.e., the encoder, which in\nthe case of missing data can be susceptible to learning inconsistent posterior\ndistributions regarding the missingness. To this end, we provide a formal\ndefinition of posterior consistency and propose an approach for regularizing an\nencoder's posterior distribution which promotes this consistency. We observe\nthat the proposed regularization suggests a different training objective than\nthat typically considered in the literature when facing missing values.\nFurthermore, we empirically demonstrate that our regularization leads to\nimproved performance in missing value settings in terms of reconstruction\nquality and downstream tasks utilizing uncertainty in the latent space. This\nimproved performance can be observed for many classes of VAEs including VAEs\nequipped with normalizing flows.",
            "author": [
                "Timur Sudak",
                "Sebastian Tschiatschek"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16648v1",
                "http://arxiv.org/pdf/2310.16648v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16647v1",
            "title": "Achieving Constraints in Neural Networks: A Stochastic Augmented\n  Lagrangian Approach",
            "updated": "2023-10-25T13:55:35Z",
            "published": "2023-10-25T13:55:35Z",
            "summary": "Regularizing Deep Neural Networks (DNNs) is essential for improving\ngeneralizability and preventing overfitting. Fixed penalty methods, though\ncommon, lack adaptability and suffer from hyperparameter sensitivity. In this\npaper, we propose a novel approach to DNN regularization by framing the\ntraining process as a constrained optimization problem. Where the data fidelity\nterm is the minimization objective and the regularization terms serve as\nconstraints. Then, we employ the Stochastic Augmented Lagrangian (SAL) method\nto achieve a more flexible and efficient regularization mechanism. Our approach\nextends beyond black-box regularization, demonstrating significant improvements\nin white-box models, where weights are often subject to hard constraints to\nensure interpretability. Experimental results on image-based classification on\nMNIST, CIFAR10, and CIFAR100 datasets validate the effectiveness of our\napproach. SAL consistently achieves higher Accuracy while also achieving better\nconstraint satisfaction, thus showcasing its potential for optimizing DNNs\nunder constrained settings.",
            "author": [
                "Diogo Lavado",
                "Cl\u00e1udia Soares",
                "Alessandra Micheletti"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16647v1",
                "http://arxiv.org/pdf/2310.16647v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16644v1",
            "title": "Weak Solutions to the Degenerate Viscous Cahn-Hilliard Equation",
            "updated": "2023-10-25T13:54:43Z",
            "published": "2023-10-25T13:54:43Z",
            "summary": "The Cahn--Hilliard equation is a common model to describe phase separation\nprocesses of a mixture of two components. In this paper, we study the viscous\nCahn--Hilliard equation with degenerate phase-dependent mobility. We define a\nnotion of weak solutions and prove the existence of such weak solutions by\nconsidering the limits of the viscous Cahn--Hilliard equation with positive\nmobility. Also, we prove that such weak solutions satisfy an energy dissipation\ninequality under some additional conditions.",
            "author": [
                "Toai Luong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16644v1",
                "http://arxiv.org/pdf/2310.16644v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16640v1",
            "title": "EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression\n  Recognition",
            "updated": "2023-10-25T13:43:36Z",
            "published": "2023-10-25T13:43:36Z",
            "summary": "Facial Expression Recognition (FER) is a crucial task in affective computing,\nbut its conventional focus on the seven basic emotions limits its applicability\nto the complex and expanding emotional spectrum. To address the issue of new\nand unseen emotions present in dynamic in-the-wild FER, we propose a novel\nvision-language model that utilises sample-level text descriptions (i.e.\ncaptions of the context, expressions or emotional cues) as natural language\nsupervision, aiming to enhance the learning of rich latent representations, for\nzero-shot classification. To test this, we evaluate using zero-shot\nclassification of the model trained on sample-level descriptions on four\npopular dynamic FER datasets. Our findings show that this approach yields\nsignificant improvements when compared to baseline methods. Specifically, for\nzero-shot video FER, we outperform CLIP by over 10\\% in terms of Weighted\nAverage Recall and 5\\% in terms of Unweighted Average Recall on several\ndatasets. Furthermore, we evaluate the representations obtained from the\nnetwork trained using sample-level descriptions on the downstream task of\nmental health symptom estimation, achieving performance comparable or superior\nto state-of-the-art methods and strong agreement with human experts. Namely, we\nachieve a Pearson's Correlation Coefficient of up to 0.85 on schizophrenia\nsymptom severity estimation, which is comparable to human experts' agreement.\nThe code is publicly available at: https://github.com/NickyFot/EmoCLIP.",
            "author": [
                "Niki Maria Foteinopoulou",
                "Ioannis Patras"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16640v1",
                "http://arxiv.org/pdf/2310.16640v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18233v2",
            "title": "Will releasing the weights of future large language models grant\n  widespread access to pandemic agents?",
            "updated": "2023-11-01T13:52:36Z",
            "published": "2023-10-25T13:43:16Z",
            "summary": "Large language models can benefit research and human understanding by\nproviding tutorials that draw on expertise from many different fields. A\nproperly safeguarded model will refuse to provide \"dual-use\" insights that\ncould be misused to cause severe harm, but some models with publicly released\nweights have been tuned to remove safeguards within days of introduction. Here\nwe investigated whether continued model weight proliferation is likely to help\nmalicious actors leverage more capable future models to inflict mass death. We\norganized a hackathon in which participants were instructed to discover how to\nobtain and release the reconstructed 1918 pandemic influenza virus by entering\nclearly malicious prompts into parallel instances of the \"Base\" Llama-2-70B\nmodel and a \"Spicy\" version tuned to remove censorship. The Base model\ntypically rejected malicious prompts, whereas the Spicy model provided some\nparticipants with nearly all key information needed to obtain the virus. Our\nresults suggest that releasing the weights of future, more capable foundation\nmodels, no matter how robustly safeguarded, will trigger the proliferation of\ncapabilities sufficient to acquire pandemic agents and other biological\nweapons.",
            "author": [
                "Anjali Gopal",
                "Nathan Helm-Burger",
                "Lennart Justen",
                "Emily H. Soice",
                "Tiffany Tzeng",
                "Geetha Jeyapragasan",
                "Simon Grimm",
                "Benjamin Mueller",
                "Kevin M. Esvelt"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18233v2",
                "http://arxiv.org/pdf/2310.18233v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16869v2",
            "title": "Single-pixel imaging based on deep learning",
            "updated": "2023-11-17T03:33:15Z",
            "published": "2023-10-25T13:35:25Z",
            "summary": "Single-pixel imaging can collect images at the wavelengths outside the reach\nof conventional focal plane array detectors. However, the limited image quality\nand lengthy computational times for iterative reconstruction still impede the\npractical application of single-pixel imaging. Recently, deep learning has been\nintroduced into single-pixel imaging, which has attracted a lot of attention\ndue to its exceptional reconstruction quality, fast reconstruction speed, and\nthe potential to complete advanced sensing tasks without reconstructing images.\nHere, this advance is discussed and some opinions are offered. Firstly, based\non the fundamental principles of single-pixel imaging and deep learning, the\nprinciples and algorithms of single-pixel imaging based on deep learning are\ndescribed and analyzed. Subsequently, the implementation technologies of\nsingle-pixel imaging based on deep learning are reviewed. They are divided into\nsuper-resolution single-pixel imaging, single-pixel imaging through scattering\nmedia, photon-level single-pixel imaging, optical encryption based on\nsingle-pixel imaging, color single-pixel imaging, and image-free sensing\naccording to diverse application fields. Finally, major challenges and\ncorresponding feasible approaches are discussed, as well as more possible\napplications in the future.",
            "author": [
                "Kai Song",
                "Yaoxing Bian",
                "Ku Wu",
                "Hongrui Liu",
                "Shuangping Han",
                "Jiaming Li",
                "Jiazhao Tian",
                "Chengbin Qin",
                "Jianyong Hu",
                "Liantuan Xiao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16869v2",
                "http://arxiv.org/pdf/2310.16869v2"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16625v1",
            "title": "Power Optimization in Satellite Communication Using Multi-Intelligent\n  Reflecting Surfaces",
            "updated": "2023-10-25T13:23:19Z",
            "published": "2023-10-25T13:23:19Z",
            "summary": "This study introduces two innovative methodologies aimed at augmenting energy\nefficiency in satellite-to-ground communication systems through the integration\nof multiple Reflective Intelligent Surfaces (RISs). The primary objective of\nthese methodologies is to optimize overall energy efficiency under two distinct\nscenarios. In the first scenario, denoted as Ideal Environment (IE), we enhance\nenergy efficiency by decomposing the problem into two sub-optimal tasks. The\ninitial task concentrates on maximizing power reception by precisely adjusting\nthe phase shift of each RIS element, followed by the implementation of\nSelective Diversity to identify the RIS element delivering maximal power. The\nsecond task entails minimizing power consumption, formulated as a binary linear\nprogramming problem, and addressed using the Binary Particle Swarm Optimization\n(BPSO) technique. The IE scenario presupposes an environment where signals\npropagate without any path loss, serving as a foundational benchmark for\ntheoretical evaluations that elucidate the systems optimal capabilities.\nConversely, the second scenario, termed Non-Ideal Environment (NIE), is\ndesigned for situations where signal transmission is subject to path loss.\nWithin this framework, the Adam algorithm is utilized to optimize energy\nefficiency. This non ideal setting provides a pragmatic assessment of the\nsystems capabilities under conventional operational conditions. Both scenarios\nemphasize the potential energy savings achievable by the satellite RIS system.\nEmpirical simulations further corroborate the robustness and effectiveness of\nour approach, highlighting its potential to enhance energy efficiency in\nsatellite-to-ground communication systems.",
            "author": [
                "Muhammad Ihsan Khalil"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16625v1",
                "http://arxiv.org/pdf/2310.16625v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16621v1",
            "title": "ArTST: Arabic Text and Speech Transformer",
            "updated": "2023-10-25T13:20:54Z",
            "published": "2023-10-25T13:20:54Z",
            "summary": "We present ArTST, a pre-trained Arabic text and speech transformer for\nsupporting open-source speech technologies for the Arabic language. The model\narchitecture follows the unified-modal framework, SpeechT5, that was recently\nreleased for English, and is focused on Modern Standard Arabic (MSA), with\nplans to extend the model for dialectal and code-switched Arabic in future\neditions. We pre-trained the model from scratch on MSA speech and text data,\nand fine-tuned it for the following tasks: Automatic Speech Recognition (ASR),\nText-To-Speech synthesis (TTS), and spoken dialect identification. In our\nexperiments comparing ArTST with SpeechT5, as well as with previously reported\nresults in these tasks, ArTST performs on a par with or exceeding the current\nstate-of-the-art in all three tasks. Moreover, we find that our pre-training is\nconducive for generalization, which is particularly evident in the low-resource\nTTS task. The pre-trained model as well as the fine-tuned ASR and TTS models\nare released for research use.",
            "author": [
                "Hawau Olamide Toyin",
                "Amirbek Djanibekov",
                "Ajinkya Kulkarni",
                "Hanan Aldarmaki"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16621v1",
                "http://arxiv.org/pdf/2310.16621v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16620v1",
            "title": "SpikingJelly: An open-source machine learning infrastructure platform\n  for spike-based intelligence",
            "updated": "2023-10-25T13:15:17Z",
            "published": "2023-10-25T13:15:17Z",
            "summary": "Spiking neural networks (SNNs) aim to realize brain-inspired intelligence on\nneuromorphic chips with high energy efficiency by introducing neural dynamics\nand spike properties. As the emerging spiking deep learning paradigm attracts\nincreasing interest, traditional programming frameworks cannot meet the demands\nof the automatic differentiation, parallel computation acceleration, and high\nintegration of processing neuromorphic datasets and deployment. In this work,\nwe present the SpikingJelly framework to address the aforementioned dilemma. We\ncontribute a full-stack toolkit for pre-processing neuromorphic datasets,\nbuilding deep SNNs, optimizing their parameters, and deploying SNNs on\nneuromorphic chips. Compared to existing methods, the training of deep SNNs can\nbe accelerated $11\\times$, and the superior extensibility and flexibility of\nSpikingJelly enable users to accelerate custom models at low costs through\nmultilevel inheritance and semiautomatic code generation. SpikingJelly paves\nthe way for synthesizing truly energy-efficient SNN-based machine intelligence\nsystems, which will enrich the ecology of neuromorphic computing.",
            "author": [
                "Wei Fang",
                "Yanqi Chen",
                "Jianhao Ding",
                "Zhaofei Yu",
                "Timoth\u00e9e Masquelier",
                "Ding Chen",
                "Liwei Huang",
                "Huihui Zhou",
                "Guoqi Li",
                "Yonghong Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16620v1",
                "http://arxiv.org/pdf/2310.16620v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE",
                "cs.LG",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16618v1",
            "title": "Real-time 6-DoF Pose Estimation by an Event-based Camera using Active\n  LED Markers",
            "updated": "2023-10-25T13:14:12Z",
            "published": "2023-10-25T13:14:12Z",
            "summary": "Real-time applications for autonomous operations depend largely on fast and\nrobust vision-based localization systems. Since image processing tasks require\nprocessing large amounts of data, the computational resources often limit the\nperformance of other processes. To overcome this limitation, traditional\nmarker-based localization systems are widely used since they are easy to\nintegrate and achieve reliable accuracy. However, classical marker-based\nlocalization systems significantly depend on standard cameras with low frame\nrates, which often lack accuracy due to motion blur. In contrast, event-based\ncameras provide high temporal resolution and a high dynamic range, which can be\nutilized for fast localization tasks, even under challenging visual conditions.\nThis paper proposes a simple but effective event-based pose estimation system\nusing active LED markers (ALM) for fast and accurate pose estimation. The\nproposed algorithm is able to operate in real time with a latency below\n\\SI{0.5}{\\milli\\second} while maintaining output rates of \\SI{3}{\\kilo \\hertz}.\nExperimental results in static and dynamic scenarios are presented to\ndemonstrate the performance of the proposed approach in terms of computational\nspeed and absolute accuracy, using the OptiTrack system as the basis for\nmeasurement.",
            "author": [
                "Gerald Ebmer",
                "Adam Loch",
                "Minh Nhat Vu",
                "Germain Haessig",
                "Roberto Mecca",
                "Markus Vincze",
                "Christian Hartl-Nesic",
                "Andreas Kugi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16618v1",
                "http://arxiv.org/pdf/2310.16618v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16617v1",
            "title": "Quantum Time: a novel resource for quantum information",
            "updated": "2023-10-25T13:13:59Z",
            "published": "2023-10-25T13:13:59Z",
            "summary": "Time in relativity theory has a status different from that adopted by\nstandard quantum mechanics, where time is considered as a parameter measured\nwith reference to an external absolute Newtonian frame. This status strongly\nrestricts its role in the dynamics of systems and hinders any formulation to\nmerge quantum mechanics with general relativity, specifically when considering\nquantum gravity. To overcome those limitations, several authors tried to\nconstruct an operator which is conjugate to the Hamiltonian of quantum systems\nimplementing some essential features of the relativistic time. These\nformulations use the concept of internal or intrinsic time instead of the\nuniversal coordinate time used in textbooks. Furthermore, recently it is\nremarked that the consideration of time with relativistic features could\nenhance the analysis techniques in quantum information processing and have an\nimpact on its status in causal orders and causal structures of quantum\ninformation. The role of clocks, their accuracy and stability has become an\nimportant issue in quantum information processing. This article present a\nsubstantiative review of recent works which reflect the possibility of\nutilizing quantum time, measured by quantum clock devised according to\nPage-Wootters scheme, to stand as a resource for quantum information\nprocessing.",
            "author": [
                "M. Basil Altaie"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16617v1",
                "http://arxiv.org/pdf/2310.16617v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16616v1",
            "title": "Context Does Matter: End-to-end Panoptic Narrative Grounding with\n  Deformable Attention Refined Matching Network",
            "updated": "2023-10-25T13:12:39Z",
            "published": "2023-10-25T13:12:39Z",
            "summary": "Panoramic Narrative Grounding (PNG) is an emerging visual grounding task that\naims to segment visual objects in images based on dense narrative captions. The\ncurrent state-of-the-art methods first refine the representation of phrase by\naggregating the most similar $k$ image pixels, and then match the refined text\nrepresentations with the pixels of the image feature map to generate\nsegmentation results. However, simply aggregating sampled image features\nignores the contextual information, which can lead to phrase-to-pixel\nmis-match. In this paper, we propose a novel learning framework called\nDeformable Attention Refined Matching Network (DRMN), whose main idea is to\nbring deformable attention in the iterative process of feature learning to\nincorporate essential context information of different scales of pixels. DRMN\niteratively re-encodes pixels with the deformable attention network after\nupdating the feature representation of the top-$k$ most similar pixels. As\nsuch, DRMN can lead to accurate yet discriminative pixel representations,\npurify the top-$k$ most similar pixels, and consequently alleviate the\nphrase-to-pixel mis-match substantially.Experimental results show that our\nnovel design significantly improves the matching results between text phrases\nand image pixels. Concretely, DRMN achieves new state-of-the-art performance on\nthe PNG benchmark with an average recall improvement 3.5%. The codes are\navailable in: https://github.com/JaMesLiMers/DRMN.",
            "author": [
                "Yiming Lin",
                "Xiao-Bo Jin",
                "Qiufeng Wang",
                "Kaizhu Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16616v1",
                "http://arxiv.org/pdf/2310.16616v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16615v1",
            "title": "Acoustic Monitoring of Inelastic Compaction in Porous Granular Materials",
            "updated": "2023-10-25T13:11:35Z",
            "published": "2023-10-25T13:11:35Z",
            "summary": "We study the transition from cohesive to noncohesive granular states of\nsynthetic rocks under oedometric loading, combining simultaneous measurements\nof ultrasound velocity and acoustic emissions. Our samples are agglomerates\nmade of glass beads bonded with a few percent of cement, either ductile or\nbrittle. These cemented granular samples exhibit an inelastic compaction beyond\ncertain axial stresses likely due to the formation of compaction bands, which\nis accompanied by a significant decrease of compressional wave velocity. Upon\nsubsequent cyclic unloading and reloading with constant consolidation stress,\nwe found the mechanical and acoustic responses similar to those in noncohesive\ngranular materials, which can be interpreted within the effective medium theory\nbased on the Digby bonding model. Moreover, this model allows P-wave velocity\nmeasured at vanishing pressure to be interpreted as an indicator of the\ndebonding on the scale of grain contact. During the inelastic compaction,\nstick-slip like stress drops were observed in brittle cement-bonded granular\nsamples accompanied by the instantaneous decrease of the P-wave velocity and\nacoustic emissions which display an Omori-like law for foreshocks, i.e.,\nprecursors. By contrast, mechanical responses of ductile cement-bonded granular\nsamples are smooth (without visible stick-slip like stress drops) and mostly\naseismic. By applying a cyclic loading and unloading with increasing\nconsolidation stress, we observed a Kaiser-like memory effect in the brittle\ncement-bonded sample in the weakly damaged state which tends to disappear when\nthe bonds are mostly broken in the non-cohesive granular state after\nlarge-amplitude loading. Our study shows that the macroscopic ductile and\nbrittle behavior of cemented granular media is controlled by the local\nprocesses on the scale of the bonds between grains.",
            "author": [
                "Vincent Canel",
                "Xiaoping Jia",
                "Michel Campillo",
                "Ioan Ionescu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16615v1",
                "http://arxiv.org/pdf/2310.16615v1"
            ],
            "primary_category": "cond-mat.soft",
            "category": [
                "cond-mat.soft"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16612v2",
            "title": "Strong decays of the $\u03c6(2170)$ as a fully-strange tetraquark state",
            "updated": "2023-10-30T12:42:12Z",
            "published": "2023-10-25T13:10:42Z",
            "summary": "We study strong decays of the $\\phi(2170)$, along with its possible partner\n$X(2436)$, as two fully-strange tetraquark states of $J^{PC} = 1^{--}$. We\nconsider seven decay channels: $\\phi \\eta$, $\\phi \\eta^\\prime$, $\\phi\nf_0(980)$, $\\phi f_1(1420)$, $h_1(1415) \\eta$, $h_1(1415) \\eta^\\prime$, and\n$h_1(1415) f_1(1420)$. Some of these channels are kinematically possible, and\nwe calculate their relative branching ratios through the Fierz rearrangement.\nFuture experimental measurements on these ratios can be useful in determining\nthe nature of the $\\phi(2170)$ and $X(2436)$. The $\\phi(2170)$ has been\nobserved in the $\\phi f_0(980)$, $\\phi \\eta$, and $\\phi \\eta^\\prime$ channels,\nand we propose to further examine it in the $h_1(1415) \\eta$ channel. Evidences\nof the $X(2436)$ have been observed in the $\\phi f_0(980)$ channel, and we\npropose to verify whether this structure exists or not in the $\\phi \\eta$,\n$\\phi \\eta^\\prime$, $h_1(1415) \\eta$, and $h_1(1415) \\eta^\\prime$ channels.",
            "author": [
                "Yi-Wei Jiang",
                "Wei-Han Tan",
                "Hua-Xing Chen",
                "Er-Liang Cui"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16612v2",
                "http://arxiv.org/pdf/2310.16612v2"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14673v1",
            "title": "Pump-induced terahertz conductivity response and peculiar bound state in\n  Mn3Si2Te6",
            "updated": "2023-10-25T13:10:05Z",
            "published": "2023-10-25T13:10:05Z",
            "summary": "We report the significant enhancement on ultrafast terahertz optical\nconductivity and the unexpected formation of a polaronic-like state in\nsemiconductor Mn3Si2Te6 at room temperature. With the absorption of pump\nphotons, the low-frequency terahertz photoconductivity spectrum exhibits a\nsignificant rise, quickly forming a broad peak and subsequently shifting to\nhigher energy. The short-lived nature of the broad peak, as well as the\ndistribution of optical constants, strongly points towards a transient polaron\nmechanism. Our study not only provides profound insights into the remarkable\nphotoelectric response of Mn3Si2Te6 but also highlights its significant\npotential for future photoelectric applications.",
            "author": [
                "Qiong Wu",
                "Qiangwei Yin",
                "Sijie Zhang",
                "Tianchen Hu",
                "Dong Wu",
                "Li Yue",
                "Bohan Li",
                "Shuxiang Xu",
                "Rongsheng Li",
                "Qiaomei Liu",
                "Hechang Lei",
                "Tao Dong",
                "Nanlin Wang"
            ],
            "link": [
                "http://dx.doi.org/10.1002/adom.202301863",
                "http://arxiv.org/abs/2311.14673v1",
                "http://arxiv.org/pdf/2311.14673v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16609v1",
            "title": "Back Transcription as a Method for Evaluating Robustness of Natural\n  Language Understanding Models to Speech Recognition Errors",
            "updated": "2023-10-25T13:07:07Z",
            "published": "2023-10-25T13:07:07Z",
            "summary": "In a spoken dialogue system, an NLU model is preceded by a speech recognition\nsystem that can deteriorate the performance of natural language understanding.\nThis paper proposes a method for investigating the impact of speech recognition\nerrors on the performance of natural language understanding models. The\nproposed method combines the back transcription procedure with a fine-grained\ntechnique for categorizing the errors that affect the performance of NLU\nmodels. The method relies on the usage of synthesized speech for NLU\nevaluation. We show that the use of synthesized speech in place of audio\nrecording does not change the outcomes of the presented technique in a\nsignificant way.",
            "author": [
                "Marek Kubis",
                "Pawe\u0142 Sk\u00f3rzewski",
                "Marcin Sowa\u0144ski",
                "Tomasz Zi\u0119tkiewicz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16609v1",
                "http://arxiv.org/pdf/2310.16609v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16608v1",
            "title": "Performative Prediction: Past and Future",
            "updated": "2023-10-25T13:02:45Z",
            "published": "2023-10-25T13:02:45Z",
            "summary": "Predictions in the social world generally influence the target of prediction,\na phenomenon known as performativity. Self-fulfilling and self-negating\npredictions are examples of performativity. Of fundamental importance to\neconomics, finance, and the social sciences, the notion has been absent from\nthe development of machine learning. In machine learning applications,\nperformativity often surfaces as distribution shift. A predictive model\ndeployed on a digital platform, for example, influences consumption and thereby\nchanges the data-generating distribution. We survey the recently founded area\nof performative prediction that provides a definition and conceptual framework\nto study performativity in machine learning. A consequence of performative\nprediction is a natural equilibrium notion that gives rise to new optimization\nchallenges. Another consequence is a distinction between learning and steering,\ntwo mechanisms at play in performative prediction. The notion of steering is in\nturn intimately related to questions of power in digital markets. We review the\nnotion of performative power that gives an answer to the question how much a\nplatform can steer participants through its predictions. We end on a discussion\nof future directions, such as the role that performativity plays in contesting\nalgorithmic systems.",
            "author": [
                "Moritz Hardt",
                "Celestine Mendler-D\u00fcnner"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16608v1",
                "http://arxiv.org/pdf/2310.16608v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16607v2",
            "title": "On the Interplay between Fairness and Explainability",
            "updated": "2023-11-13T15:20:43Z",
            "published": "2023-10-25T12:59:51Z",
            "summary": "In order to build reliable and trustworthy NLP applications, models need to\nbe both fair across different demographics and explainable. Usually these two\nobjectives, fairness and explainability, are optimized and/or examined\nindependently of each other. Instead, we argue that forthcoming, trustworthy\nNLP systems should consider both. In this work, we perform a first study to\nunderstand how they influence each other: do fair(er) models rely on more\nplausible rationales? and vice versa. To this end, we conduct experiments on\ntwo English multi-class text classification datasets, BIOS and ECtHR, that\nprovide information on gender and nationality, respectively, as well as\nhuman-annotated rationales. We fine-tune pre-trained language models with\nseveral methods for (i) bias mitigation, which aims to improve fairness; (ii)\nrationale extraction, which aims to produce plausible explanations. We find\nthat bias mitigation algorithms do not always lead to fairer models. Moreover,\nwe discover that empirical fairness and explainability are orthogonal.",
            "author": [
                "Stephanie Brandl",
                "Emanuele Bugliarello",
                "Ilias Chalkidis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16607v2",
                "http://arxiv.org/pdf/2310.16607v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16867v1",
            "title": "An Explainable Deep Learning-Based Method For Schizophrenia Diagnosis\n  Using Generative Data-Augmentation",
            "updated": "2023-10-25T12:55:16Z",
            "published": "2023-10-25T12:55:16Z",
            "summary": "In this study, we leverage a deep learning-based method for the automatic\ndiagnosis of schizophrenia using EEG brain recordings. This approach utilizes\ngenerative data augmentation, a powerful technique that enhances the accuracy\nof the diagnosis. To enable the utilization of time-frequency features,\nspectrograms were extracted from the raw signals. After exploring several\nneural network architectural setups, a proper convolutional neural network\n(CNN) was used for the initial diagnosis. Subsequently, using Wasserstein GAN\nwith Gradient Penalty (WGAN-GP) and Variational Autoencoder (VAE), two\ndifferent synthetic datasets were generated in order to augment the initial\ndataset and address the over-fitting issue. The augmented dataset using VAE\nachieved a 3.0\\% improvement in accuracy reaching up to 99.0\\% and yielded a\nlower loss value as well as a faster convergence. Finally, we addressed the\nlack of trust in black-box models using the Local Interpretable Model-agnostic\nExplanations (LIME) algorithm to determine the most important superpixels\n(frequencies) in the diagnosis process.",
            "author": [
                "Mehrshad Saadatinia",
                "Armin Salimi-Badr"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16867v1",
                "http://arxiv.org/pdf/2310.16867v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16602v1",
            "title": "Parcel loss prediction in last-mile delivery: deep and non-deep\n  approaches with insights from Explainable AI",
            "updated": "2023-10-25T12:46:34Z",
            "published": "2023-10-25T12:46:34Z",
            "summary": "Within the domain of e-commerce retail, an important objective is the\nreduction of parcel loss during the last-mile delivery phase. The\never-increasing availability of data, including product, customer, and order\ninformation, has made it possible for the application of machine learning in\nparcel loss prediction. However, a significant challenge arises from the\ninherent imbalance in the data, i.e., only a very low percentage of parcels are\nlost. In this paper, we propose two machine learning approaches, namely, Data\nBalance with Supervised Learning (DBSL) and Deep Hybrid Ensemble Learning\n(DHEL), to accurately predict parcel loss. The practical implication of such\npredictions is their value in aiding e-commerce retailers in optimizing\ninsurance-related decision-making policies. We conduct a comprehensive\nevaluation of the proposed machine learning models using one year data from\nBelgian shipments. The findings show that the DHEL model, which combines a\nfeed-forward autoencoder with a random forest, achieves the highest\nclassification performance. Furthermore, we use the techniques from Explainable\nAI (XAI) to illustrate how prediction models can be used in enhancing business\nprocesses and augmenting the overall value proposition for e-commerce retailers\nin the last mile delivery.",
            "author": [
                "Jan de Leeuw",
                "Zaharah Bukhsh",
                "Yingqian Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16602v1",
                "http://arxiv.org/pdf/2310.16602v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16601v1",
            "title": "Pilot-Based Uplink Power Control in Single-UE Massive MIMO Systems With\n  1-Bit ADCs",
            "updated": "2023-10-25T12:46:26Z",
            "published": "2023-10-25T12:46:26Z",
            "summary": "We propose uplink power control (PC) methods for massive multiple-input\nmultiple-output systems with 1-bit analog-to-digital converters, which are\nspecifically tailored to address the non-monotonic data detection performance\nwith respect to the transmit power of the user equipment (UE). Considering a\nsingle UE, we design a multi-amplitude pilot sequence to capture the\naforementioned non-monotonicity, which is utilized at the base station to\nderive UE transmit power adjustments via single-shot or differential power\ncontrol (DPC) techniques. Both methods enable closed-loop uplink PC using\ndifferent feedback approaches. The single-shot method employs one-time\nmulti-bit feedback, while the DPC method relies on continuous adjustments with\n1-bit feedback. Numerical results demonstrate the superiority of the proposed\nschemes over conventional closed-loop uplink PC techniques.",
            "author": [
                "Amila Ravinath",
                "Bikshapathi Gouda",
                "Italo Atzeni",
                "Antti T\u00f6lli"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16601v1",
                "http://arxiv.org/pdf/2310.16601v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16597v2",
            "title": "Beyond IID weights: sparse and low-rank deep Neural Networks are also\n  Gaussian Processes",
            "updated": "2023-11-19T18:30:35Z",
            "published": "2023-10-25T12:38:36Z",
            "summary": "The infinitely wide neural network has been proven a useful and manageable\nmathematical model that enables the understanding of many phenomena appearing\nin deep learning. One example is the convergence of random deep networks to\nGaussian processes that allows a rigorous analysis of the way the choice of\nactivation function and network weights impacts the training dynamics. In this\npaper, we extend the seminal proof of Matthews et al. (2018) to a larger class\nof initial weight distributions (which we call PSEUDO-IID), including the\nestablished cases of IID and orthogonal weights, as well as the emerging\nlow-rank and structured sparse settings celebrated for their computational\nspeed-up benefits. We show that fully-connected and convolutional networks\ninitialized with PSEUDO-IID distributions are all effectively equivalent up to\ntheir variance. Using our results, one can identify the Edge-of-Chaos for a\nbroader class of neural networks and tune them at criticality in order to\nenhance their training.",
            "author": [
                "Thiziri Nait-Saada",
                "Alireza Naderi",
                "Jared Tanner"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16597v2",
                "http://arxiv.org/pdf/2310.16597v2"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16592v1",
            "title": "Over-the-air Federated Policy Gradient",
            "updated": "2023-10-25T12:28:20Z",
            "published": "2023-10-25T12:28:20Z",
            "summary": "In recent years, over-the-air aggregation has been widely considered in\nlarge-scale distributed learning, optimization, and sensing. In this paper, we\npropose the over-the-air federated policy gradient algorithm, where all agents\nsimultaneously broadcast an analog signal carrying local information to a\ncommon wireless channel, and a central controller uses the received aggregated\nwaveform to update the policy parameters. We investigate the effect of noise\nand channel distortion on the convergence of the proposed algorithm, and\nestablish the complexities of communication and sampling for finding an\n$\\epsilon$-approximate stationary point. Finally, we present some simulation\nresults to show the effectiveness of the algorithm.",
            "author": [
                "Huiwen Yang",
                "Lingying Huang",
                "Subhrakanti Dey",
                "Ling Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16592v1",
                "http://arxiv.org/pdf/2310.16592v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16591v1",
            "title": "Intrinsic Piezoelectric Anisotropy of Tetragonal ABO3 Perovskites: A\n  High-Throughput Study",
            "updated": "2023-10-25T12:27:24Z",
            "published": "2023-10-25T12:27:24Z",
            "summary": "A comprehensive understand of the intrinsic piezoelectric anisotropy stemming\nfrom diverse chemical and physical factors is a key step for the rational\ndesign of highly anisotropic materials. We performed high-throughput\ncalculations on tetragonal ABO3 perovskites to investigate the piezoelectricity\nand the interplay between lattice, displacement, polarization and elasticity.\nAmong the 123 types of perovskites, the structural tetragonality is naturally\ndivided into two categories: normal tetragonal (c/a ratio < 1.1) and\nsuper-tetragonal (c/a ratio > 1.17), exhibiting distinct ferroelectric,\nelastic, and piezoelectric properties. Charge analysis revealed the mechanisms\nunderlying polarization saturation and piezoelectricity suppression in the\nsuper-tetragonal region, which also produces an inherent contradiction between\nhigh d33 and large piezoelectric anisotropy ratio |d33/d31|. The polarization\naxis and elastic softness direction jointly determine the maximum longitudinal\npiezoelectric response d33 direction. The validity and deficiencies of the\nwidely utilized |d33/d31| ratio for representing piezoelectric anisotropy were\nreevaluated.",
            "author": [
                "Fanhao Jia",
                "Shaowen Xu",
                "Shunbo Hu",
                "Jianguo Chen",
                "Yongchen Wang",
                "Yuan Li",
                "Wei Ren",
                "Jinrong Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16591v1",
                "http://arxiv.org/pdf/2310.16591v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16590v1",
            "title": "$\\mathbb{VD}$-$\\mathbb{GR}$: Boosting $\\mathbb{V}$isual\n  $\\mathbb{D}$ialog with Cascaded Spatial-Temporal Multi-Modal\n  $\\mathbb{GR}$aphs",
            "updated": "2023-10-25T12:25:53Z",
            "published": "2023-10-25T12:25:53Z",
            "summary": "We propose $\\mathbb{VD}$-$\\mathbb{GR}$ - a novel visual dialog model that\ncombines pre-trained language models (LMs) with graph neural networks (GNNs).\nPrior works mainly focused on one class of models at the expense of the other,\nthus missing out on the opportunity of combining their respective benefits. At\nthe core of $\\mathbb{VD}$-$\\mathbb{GR}$ is a novel integration mechanism that\nalternates between spatial-temporal multi-modal GNNs and BERT layers, and that\ncovers three distinct contributions: First, we use multi-modal GNNs to process\nthe features of each modality (image, question, and dialog history) and exploit\ntheir local structures before performing BERT global attention. Second, we\npropose hub-nodes that link to all other nodes within one modality graph,\nallowing the model to propagate information from one GNN (modality) to the\nother in a cascaded manner. Third, we augment the BERT hidden states with\nfine-grained multi-modal GNN features before passing them to the next\n$\\mathbb{VD}$-$\\mathbb{GR}$ layer. Evaluations on VisDial v1.0, VisDial v0.9,\nVisDialConv, and VisPro show that $\\mathbb{VD}$-$\\mathbb{GR}$ achieves new\nstate-of-the-art results across all four datasets.",
            "author": [
                "Adnen Abdessaied",
                "Lei Shi",
                "Andreas Bulling"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16590v1",
                "http://arxiv.org/pdf/2310.16590v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16589v1",
            "title": "Polarization-entangled photons from a whispering gallery resonator",
            "updated": "2023-10-25T12:25:40Z",
            "published": "2023-10-25T12:25:40Z",
            "summary": "Crystalline Whispering Gallery Mode Resonators (WGMRs) have been shown to\nfacilitate versatile sources of quantum states that can efficiently interact\nwith atomic systems. These features make WGMRs an efficient platform for\nquantum information processing. Here, we experimentally show that it is\npossible to generate polarization entanglement from WGMRs by using an\ninterferometric scheme. Our scheme gives us the flexibility to control the\nphase of the generated entangled state by changing the relative phase of the\ninterferometer. The S value of the Clauser-Horne-Shimony-Holt's inequality in\nthe system is $2.45 \\pm 0.07$, which violates the inequality by more than 6\nstandard deviations.",
            "author": [
                "Sheng-Hsuan Huang",
                "Thomas Dirmeier",
                "Golnoush Shafiee",
                "Kaisa Laiho",
                "Dmitry V. Strekalov",
                "Gerd Leuchs",
                "Christoph Marquardt"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16589v1",
                "http://arxiv.org/pdf/2310.16589v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16584v1",
            "title": "Learning to Explain: A Model-Agnostic Framework for Explaining Black Box\n  Models",
            "updated": "2023-10-25T12:18:00Z",
            "published": "2023-10-25T12:18:00Z",
            "summary": "We present Learning to Explain (LTX), a model-agnostic framework designed for\nproviding post-hoc explanations for vision models. The LTX framework introduces\nan \"explainer\" model that generates explanation maps, highlighting the crucial\nregions that justify the predictions made by the model being explained. To\ntrain the explainer, we employ a two-stage process consisting of initial\npretraining followed by per-instance finetuning. During both stages of\ntraining, we utilize a unique configuration where we compare the explained\nmodel's prediction for a masked input with its original prediction for the\nunmasked input. This approach enables the use of a novel counterfactual\nobjective, which aims to anticipate the model's output using masked versions of\nthe input image. Importantly, the LTX framework is not restricted to a specific\nmodel architecture and can provide explanations for both Transformer-based and\nconvolutional models. Through our evaluations, we demonstrate that LTX\nsignificantly outperforms the current state-of-the-art in explainability across\nvarious metrics.",
            "author": [
                "Oren Barkan",
                "Yuval Asher",
                "Amit Eshel",
                "Yehonatan Elisha",
                "Noam Koenigstein"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16584v1",
                "http://arxiv.org/pdf/2310.16584v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16582v1",
            "title": "Tailoring Personality Traits in Large Language Models via\n  Unsupervisedly-Built Personalized Lexicons",
            "updated": "2023-10-25T12:16:33Z",
            "published": "2023-10-25T12:16:33Z",
            "summary": "Personality plays a pivotal role in shaping human expression patterns, and\nempowering and manipulating large language models (LLMs) with personality\ntraits holds significant promise in enhancing the user experience of LLMs.\nHowever, prior approaches either rely on fine-tuning LLMs on a corpus enriched\nwith personalized expressions or necessitate the manual crafting of prompts to\ninduce LLMs to produce personalized responses. The former approaches demand\nsubstantial time and resources for collecting sufficient training examples\nwhile the latter might fail in enabling the precise manipulation of the\npersonality traits at a fine-grained level (e.g., achieving high agreeableness\nwhile reducing openness). In this study, we introduce a novel approach for\ntailoring personality traits within LLMs, allowing for the incorporation of any\ncombination of the Big Five factors (i.e., openness, conscientiousness,\nextraversion, agreeableness, and neuroticism) in a pluggable manner. This is\nachieved by employing a set of Unsupervisedly-Built Personalized Lexicons\n(UBPL) that are utilized to adjust the probability of the next token predicted\nby the original LLMs during the decoding phase. This adjustment encourages the\nmodels to generate words present in the personalized lexicons while preserving\nthe naturalness of the generated texts. Extensive experimentation demonstrates\nthe effectiveness of our approach in finely manipulating LLMs' personality\ntraits. Furthermore, our method can be seamlessly integrated into other LLMs\nwithout necessitating updates to their parameters.",
            "author": [
                "Tianlong Li",
                "Xiaoqing Zheng",
                "Xuanjing Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16582v1",
                "http://arxiv.org/pdf/2310.16582v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16581v1",
            "title": "Hybrid Minimax-MCTS and Difficulty Adjustment for General Game Playing",
            "updated": "2023-10-25T12:13:40Z",
            "published": "2023-10-25T12:13:40Z",
            "summary": "Board games are a great source of entertainment for all ages, as they create\na competitive and engaging environment, as well as stimulating learning and\nstrategic thinking. It is common for digital versions of board games, as any\nother type of digital games, to offer the option to select the difficulty of\nthe game. This is usually done by customizing the search parameters of the AI\nalgorithm. However, this approach cannot be extended to General Game Playing\nagents, as different games might require different parametrization for each\ndifficulty level. In this paper, we present a general approach to implement an\nartificial intelligence opponent with difficulty levels for zero-sum games,\ntogether with a propose of a Minimax-MCTS hybrid algorithm, which combines the\nminimax search process with GGP aspects of MCTS. This approach was tested in\nour mobile application LoBoGames, an extensible board games platform, that is\nintended to have an broad catalog of games, with an emphasis on accessibility:\nthe platform is friendly to visually-impaired users, and is compatible with\nmore than 92\\% of Android devices. The tests in this work indicate that both\nthe hybrid Minimax-MCTS and the new difficulty adjustment system are promising\nGGP approaches that could be expanded in future work.",
            "author": [
                "Marco Ant\u00f4nio Athayde de Aguiar Vieira",
                "Anderson Rocha Tavares",
                "Renato Perez Ribas"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16581v1",
                "http://arxiv.org/pdf/2310.16581v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16579v1",
            "title": "WSDMS: Debunk Fake News via Weakly Supervised Detection of Misinforming\n  Sentences with Contextualized Social Wisdom",
            "updated": "2023-10-25T12:06:55Z",
            "published": "2023-10-25T12:06:55Z",
            "summary": "In recent years, we witness the explosion of false and unconfirmed\ninformation (i.e., rumors) that went viral on social media and shocked the\npublic. Rumors can trigger versatile, mostly controversial stance expressions\namong social media users. Rumor verification and stance detection are different\nyet relevant tasks. Fake news debunking primarily focuses on determining the\ntruthfulness of news articles, which oversimplifies the issue as fake news\noften combines elements of both truth and falsehood. Thus, it becomes crucial\nto identify specific instances of misinformation within the articles. In this\nresearch, we investigate a novel task in the field of fake news debunking,\nwhich involves detecting sentence-level misinformation. One of the major\nchallenges in this task is the absence of a training dataset with\nsentence-level annotations regarding veracity. Inspired by the Multiple\nInstance Learning (MIL) approach, we propose a model called Weakly Supervised\nDetection of Misinforming Sentences (WSDMS). This model only requires bag-level\nlabels for training but is capable of inferring both sentence-level\nmisinformation and article-level veracity, aided by relevant social media\nconversations that are attentively contextualized with news sentences. We\nevaluate WSDMS on three real-world benchmarks and demonstrate that it\noutperforms existing state-of-the-art baselines in debunking fake news at both\nthe sentence and article levels.",
            "author": [
                "Ruichao Yang",
                "Wei Gao",
                "Jing Ma",
                "Hongzhan Lin",
                "Zhiwei Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16579v1",
                "http://arxiv.org/pdf/2310.16579v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16578v1",
            "title": "Accelerating the analysis of optical quantum systems using the Koopman\n  operator",
            "updated": "2023-10-25T12:02:04Z",
            "published": "2023-10-25T12:02:04Z",
            "summary": "The prediction of photon echoes is an important technique for gaining an\nunderstanding of optical quantum systems. However, this requires a large number\nof simulations with varying parameters and/or input pulses, which renders\nnumerical studies expensive. This article investigates how we can use\ndata-driven surrogate models based on the Koopman operator to accelerate this\nprocess. In order to be successful, we require a model that is accurate over a\nlarge number of time steps. To this end, we employ a bilinear Koopman model\nusing extended dynamic mode decomposition and simulate the optical Bloch\nequations for an ensemble of inhomogeneously broadened two-level systems. Such\nsystems are well suited to describe the excitation of excitonic resonances in\nsemiconductor nanostructures, for example, ensembles of semiconductor quantum\ndots. We perform a detailed study on the required number of system simulations\nsuch that the resulting data-driven Koopman model is sufficiently accurate for\na wide range of parameter settings. We analyze the L2 error and the relative\nerror of the photon echo peak and investigate how the control positions relate\nto the stabilization. After proper training, the dynamics of the quantum\nensemble can be predicted accurately and numerically very efficiently by our\nmethods.",
            "author": [
                "Anna Hunstig",
                "Sebastian Peitz",
                "Hendrik Rose",
                "Torsten Meier"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16578v1",
                "http://arxiv.org/pdf/2310.16578v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "math.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16577v1",
            "title": "Mapping the magnetic field using a magnetometer array with noisy input\n  Gaussian process regression",
            "updated": "2023-10-25T12:00:45Z",
            "published": "2023-10-25T12:00:45Z",
            "summary": "Ferromagnetic materials in indoor environments give rise to disturbances in\nthe ambient magnetic field. Maps of these magnetic disturbances can be used for\nindoor localisation. A Gaussian process can be used to learn the spatially\nvarying magnitude of the magnetic field using magnetometer measurements and\ninformation about the position of the magnetometer. The position of the\nmagnetometer, however, is frequently only approximately known. This negatively\naffects the quality of the magnetic field map. In this paper, we investigate\nhow an array of magnetometers can be used to improve the quality of the\nmagnetic field map. The position of the array is approximately known, but the\nrelative locations of the magnetometers on the array are known. We include this\ninformation in a novel method to make a map of the ambient magnetic field. We\nstudy the properties of our method in simulation and show that our method\nimproves the map quality. We also demonstrate the efficacy of our method with\nexperimental data for the mapping of the magnetic field using an array of 30\nmagnetometers.",
            "author": [
                "Thomas Edridge",
                "Manon Kok"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16577v1",
                "http://arxiv.org/pdf/2310.16577v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16574v1",
            "title": "Large-scale magnetic field maps using structured kernel interpolation\n  for Gaussian process regression",
            "updated": "2023-10-25T11:58:18Z",
            "published": "2023-10-25T11:58:18Z",
            "summary": "We present a mapping algorithm to compute large-scale magnetic field maps in\nindoor environments with approximate Gaussian process (GP) regression. Mapping\nthe spatial variations in the ambient magnetic field can be used for\nlocalization algorithms in indoor areas. To compute such a map, GP regression\nis a suitable tool because it provides predictions of the magnetic field at new\nlocations along with uncertainty quantification. Because full GP regression has\na complexity that grows cubically with the number of data points,\napproximations for GPs have been extensively studied. In this paper, we build\non the structured kernel interpolation (SKI) framework, speeding up inference\nby exploiting efficient Krylov subspace methods. More specifically, we\nincorporate SKI with derivatives (D-SKI) into the scalar potential model for\nmagnetic field modeling and compute both predictive mean and covariance with a\ncomplexity that is linear in the data points. In our simulations, we show that\nour method achieves better accuracy than current state-of-the-art methods on\nmagnetic field maps with a growing mapping area. In our large-scale\nexperiments, we construct magnetic field maps from up to 40000\nthree-dimensional magnetic field measurements in less than two minutes on a\nstandard laptop.",
            "author": [
                "Clara Menzen",
                "Marnix Fetter",
                "Manon Kok"
            ],
            "link": [
                "http://dx.doi.org/10.23919/FUSION52260.2023.10224210",
                "http://arxiv.org/abs/2310.16574v1",
                "http://arxiv.org/pdf/2310.16574v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16572v1",
            "title": "Correctness Witness Validation by Abstract Interpretation",
            "updated": "2023-10-25T11:58:12Z",
            "published": "2023-10-25T11:58:12Z",
            "summary": "Witnesses record automated program analysis results and make them\nexchangeable. To validate correctness witnesses through abstract\ninterpretation, we introduce a novel abstract operation unassume. This operator\nincorporates witness invariants into the abstract program state. Given suitable\ninvariants, the unassume operation can accelerate fixpoint convergence and\nyield more precise results. We demonstrate the feasibility of this approach by\naugmenting an abstract interpreter with unassume operators and evaluating the\nimpact of incorporating witnesses on performance and precision. Using manually\ncrafted witnesses, we can confirm verification results for multi-threaded\nprograms with a reduction in effort ranging from 7% to 47% in CPU time. More\nintriguingly, we discover that using witnesses from model checkers can guide\nour analyzer to verify program properties that it could not verify on its own.",
            "author": [
                "Simmo Saan",
                "Michael Schwarz",
                "Julian Erhard",
                "Helmut Seidl",
                "Sarah Tilscher",
                "Vesal Vojdani"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16572v1",
                "http://arxiv.org/pdf/2310.16572v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16570v2",
            "title": "Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained\n  Language Models",
            "updated": "2023-12-04T19:23:33Z",
            "published": "2023-10-25T11:57:13Z",
            "summary": "Pre-trained Language Models (PLMs) are trained on vast unlabeled data, rich\nin world knowledge. This fact has sparked the interest of the community in\nquantifying the amount of factual knowledge present in PLMs, as this explains\ntheir performance on downstream tasks, and potentially justifies their use as\nknowledge bases. In this work, we survey methods and datasets that are used to\nprobe PLMs for factual knowledge. Our contributions are: (1) We propose a\ncategorization scheme for factual probing methods that is based on how their\ninputs, outputs and the probed PLMs are adapted; (2) We provide an overview of\nthe datasets used for factual probing; (3) We synthesize insights about\nknowledge retention and prompt optimization in PLMs, analyze obstacles to\nadopting PLMs as knowledge bases and outline directions for future work.",
            "author": [
                "Paul Youssef",
                "Osman Alperen Kora\u015f",
                "Meijie Li",
                "J\u00f6rg Schl\u00f6tterer",
                "Christin Seifert"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16570v2",
                "http://arxiv.org/pdf/2310.16570v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16568v1",
            "title": "1-PAGER: One Pass Answer Generation and Evidence Retrieval",
            "updated": "2023-10-25T11:51:22Z",
            "published": "2023-10-25T11:51:22Z",
            "summary": "We present 1-Pager the first system that answers a question and retrieves\nevidence using a single Transformer-based model and decoding process. 1-Pager\nincrementally partitions the retrieval corpus using constrained decoding to\nselect a document and answer string, and we show that this is competitive with\ncomparable retrieve-and-read alternatives according to both retrieval and\nanswer accuracy metrics. 1-Pager also outperforms the equivalent closed-book\nquestion answering model, by grounding predictions in an evidence corpus. While\n1-Pager is not yet on-par with more expensive systems that read many more\ndocuments before generating an answer, we argue that it provides an important\nstep toward attributed generation by folding retrieval into the\nsequence-to-sequence paradigm that is currently dominant in NLP. We also show\nthat the search paths used to partition the corpus are easy to read and\nunderstand, paving a way forward for interpretable neural retrieval.",
            "author": [
                "Palak Jain",
                "Livio Baldini Soares",
                "Tom Kwiatkowski"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16568v1",
                "http://arxiv.org/pdf/2310.16568v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16566v1",
            "title": "Model-enhanced Contrastive Reinforcement Learning for Sequential\n  Recommendation",
            "updated": "2023-10-25T11:43:29Z",
            "published": "2023-10-25T11:43:29Z",
            "summary": "Reinforcement learning (RL) has been widely applied in recommendation systems\ndue to its potential in optimizing the long-term engagement of users. From the\nperspective of RL, recommendation can be formulated as a Markov decision\nprocess (MDP), where recommendation system (agent) can interact with users\n(environment) and acquire feedback (reward signals).However, it is impractical\nto conduct online interactions with the concern on user experience and\nimplementation complexity, and we can only train RL recommenders with offline\ndatasets containing limited reward signals and state transitions. Therefore,\nthe data sparsity issue of reward signals and state transitions is very severe,\nwhile it has long been overlooked by existing RL recommenders.Worse still, RL\nmethods learn through the trial-and-error mode, but negative feedback cannot be\nobtained in implicit feedback recommendation tasks, which aggravates the\noverestimation problem of offline RL recommender. To address these challenges,\nwe propose a novel RL recommender named model-enhanced contrastive\nreinforcement learning (MCRL). On the one hand, we learn a value function to\nestimate the long-term engagement of users, together with a conservative value\nlearning mechanism to alleviate the overestimation problem.On the other hand,\nwe construct some positive and negative state-action pairs to model the reward\nfunction and state transition function with contrastive learning to exploit the\ninternal structure information of MDP. Experiments demonstrate that the\nproposed method significantly outperforms existing offline RL and\nself-supervised RL methods with different representative backbone networks on\ntwo real-world datasets.",
            "author": [
                "Chengpeng Li",
                "Zhengyi Yang",
                "Jizhi Zhang",
                "Jiancan Wu",
                "Dingxian Wang",
                "Xiangnan He",
                "Xiang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16566v1",
                "http://arxiv.org/pdf/2310.16566v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16564v1",
            "title": "Application of entropy analysis in the prediction of flow distribution\n  in parallel channels",
            "updated": "2023-10-25T11:36:11Z",
            "published": "2023-10-25T11:36:11Z",
            "summary": "Multiphase flow in parallel channels is often an efficient approach to manage\nheat and energy distribution in engineering systems. However, two-phase flow\nwith heating in parallel channels is prone to maldistribution, resulting in\nsub-optimal performance and in some cases, permanent damage. This challenge\nrequires accurate flow modeling in parallel channels to mitigate or design\nagainst the adverse effect of two-phase flow maldistribution. The nonlinear\nnature of multiphase flow results in a multiplicity of predicted solutions for\nthe same condition, thereby creating significant challenges in modeling flow\ndistribution. Therefore, this study focuses on solving this challenge by\napplying entropy generation analysis and the conservation of mass, momentum\nbalance, and energy balance to predict two-phase flow distribution in a\ntwo-parallel-channel assembly with a numerical model. Both model predictions\nand experimental data show that equally distributed flow becomes severely\nmaldistributed with a decrease in flow rate, resulting in significant change\n(>30%) in the entropy generation rate. We show that the entropy analysis can be\napplied in distinguishing between stable and unstable flow distribution, like\nthe linear stability analysis used in previous studies. We also surpass the\nlimit of applying linear stability analysis by using entropy analysis to\nidentify the most feasible end state in a maldistribution process.",
            "author": [
                "Toochukwu Aka",
                "Shankar Narayan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16564v1",
                "http://arxiv.org/pdf/2310.16564v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16559v2",
            "title": "Multi-structure Objects Points-to Analysis",
            "updated": "2023-11-07T02:07:27Z",
            "published": "2023-10-25T11:26:54Z",
            "summary": "An important dimension of pointer analysis is field-Sensitive, which has been\nproven to effectively enhance the accuracy of pointer analysis results. A\ncrucial area of research within field-Sensitive is Structure-Sensitive.\nStructure-Sensitive has been shown to further enhance the precision of pointer\nanalysis. However, existing structure-sensitive methods cannot handle cases\nwhere an object possesses multiple structures, even though it's common for an\nobject to have multiple structures throughout its lifecycle. This paper\nintroduces MTO-SS, a flow-sensitive pointer analysis method for objects with\nmultiple structures. Our observation is that it's common for an object to\npossess multiple structures throughout its lifecycle. The novelty of MTO-SS\nlies in: MTO-SS introduces Structure-Flow-Sensitive. An object has different\nstructure information at different locations in the program. To ensure the\ncompleteness of an object's structure information, MTO-SS always performs weak\nupdates on the object's type. This means that once an object possesses a\nstructure, this structure will accompany the object throughout its lifecycle.\nWe evaluated our method of multi-structured object pointer analysis using the\n12 largest programs in GNU Coreutils and compared the experimental results with\nsparse flow-sensitive method and another method, TYPECLONE, which only allows\nan object to have one structure information. Our experimental results confirm\nthat MTO-SS is more precise than both sparse flow-sensitive pointer analysis\nand TYPECLONE, being able to answer, on average, over 22\\% more alias queries\nwith a no-alias result compared to the former, and over 3\\% more compared to\nthe latter. Additionally, the time overhead introduced by our method is very\nlow.",
            "author": [
                "Xun An"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16559v2",
                "http://arxiv.org/pdf/2310.16559v2"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16557v1",
            "title": "TILT: topological interface recovery in limited-angle tomography",
            "updated": "2023-10-25T11:23:31Z",
            "published": "2023-10-25T11:23:31Z",
            "summary": "A novel reconstruction method is introduced for the severely ill-posed\ninverse problem of limited-angle tomography. It is well known that, depending\non the available measurement, angles specify a subset of the wavefront set of\nthe unknown target, while some oriented singularities remain invisible in the\ndata. Topological Interface recovery for Limited-angle Tomography, or TILT, is\nbased on lifting the visible part of the wavefront set under a universal\ncovering map. In the space provided, it is possible to connect the appropriate\npieces of the lifted wavefront set correctly using dual-tree complex wavelets,\na dedicated metric, and persistent homology. The result is not only a suggested\ninvisible boundary but also a computational representation for all interfaces\nin the target.",
            "author": [
                "Elli Karvonen",
                "Matti Lassas",
                "Pekka Pankka",
                "Samuli Siltanen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16557v1",
                "http://arxiv.org/pdf/2310.16557v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "65R32, 65T60, 65F22, 92C55, 55N35"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16555v2",
            "title": "Towards Information Theory-Based Discovery of Equivariances",
            "updated": "2023-11-14T11:37:40Z",
            "published": "2023-10-25T11:19:40Z",
            "summary": "The presence of symmetries imposes a stringent set of constraints on a\nsystem. This constrained structure allows intelligent agents interacting with\nsuch a system to drastically improve the efficiency of learning and\ngeneralization, through the internalisation of the system's symmetries into\ntheir information-processing. In parallel, principled models of\ncomplexity-constrained learning and behaviour make increasing use of\ninformation-theoretic methods. Here, we wish to marry these two perspectives\nand understand whether and in which form the information-theoretic lens can\n\"see\" the effect of symmetries of a system. For this purpose, we propose a\nnovel variant of the Information Bottleneck principle, which has served as a\nproductive basis for many principled studies of learning and\ninformation-constrained adaptive behaviour. We show (in the discrete case) that\nour approach formalises a certain duality between symmetry and information\nparsimony: namely, channel equivariances can be characterised by the optimal\nmutual information-preserving joint compression of the channel's input and\noutput. This information-theoretic treatment furthermore suggests a principled\nnotion of \"soft\" equivariance, whose \"coarseness\" is measured by the amount of\ninput-output mutual information preserved by the corresponding optimal\ncompression. This new notion offers a bridge between the field of bounded\nrationality and the study of symmetries in neural representations. The\nframework may also allow (exact and soft) equivariances to be automatically\ndiscovered.",
            "author": [
                "Hippolyte Charvin",
                "Nicola Catenacci Volpi",
                "Daniel Polani"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16555v2",
                "http://arxiv.org/pdf/2310.16555v2"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.NE",
                "math.GR",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16551v1",
            "title": "Abyss Aerosols",
            "updated": "2023-10-25T11:07:54Z",
            "published": "2023-10-25T11:07:54Z",
            "summary": "Bubble bursting on water surfaces is believed to be a main mechanism to\nproduce submicron drops, including sea spray aerosols, which play a critical\nrole in forming cloud and transferring various biological and chemical\nsubstances from water to the air. Over the past century, drops production\nmechanisms from bubble bursting have been extensively studied. They usually\ninvolve the centrifugal fragmentation of liquid ligaments from the bubble cap\nduring film rupture, the flapping of the cap film, and the disintegration of\nWorthington jets after cavity collapse. Here, we show that a dominant fraction\nof previously identified as 'bubble bursting' submicron drops are in fact\ngenerated via a new mechanism underwater, inside the bubbles themselves before\nthey have reached the surface. These drops are then carried within the rising\nbubbles towards the water surface and are released in air at bubble bursting.\nEvidence suggests that these drops originate from the flapping instability of\nthe film squeezed between underwater colliding bubbles. This finding\nfundamentally reshapes our understanding of sea spray aerosol production and\nestablishes a new role for underwater bubble collisions regarding the nature of\ntransfers through water-air interfaces.",
            "author": [
                "Xinghua Jiang",
                "Lucas Rotily",
                "Emmanuel Villermaux",
                "Xiaofei Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16551v1",
                "http://arxiv.org/pdf/2310.16551v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn",
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16550v1",
            "title": "Dynamic Processing Neural Network Architecture For Hearing Loss\n  Compensation",
            "updated": "2023-10-25T11:04:32Z",
            "published": "2023-10-25T11:04:32Z",
            "summary": "This paper proposes neural networks for compensating sensorineural hearing\nloss. The aim of the hearing loss compensation task is to transform a speech\nsignal to increase speech intelligibility after further processing by a person\nwith a hearing impairment, which is modeled by a hearing loss model. We propose\nan interpretable model called dynamic processing network, which has a structure\nsimilar to band-wise dynamic compressor. The network is differentiable, and\ntherefore allows to learn its parameters to maximize speech intelligibility.\nMore generic models based on convolutional layers were tested as well. The\nperformance of the tested architectures was assessed using spectro-temporal\nobjective index (STOI) with hearing-threshold noise and hearing aid speech\nintelligibility (HASPI) metrics. The dynamic processing network gave a\nsignificant improvement of STOI and HASPI in comparison to popular compressive\ngain prescription rule Camfit. A large enough convolutional network could\noutperform the interpretable model with the cost of larger computational load.\nFinally, a combination of the dynamic processing network with convolutional\nneural network gave the best results in terms of STOI and HASPI.",
            "author": [
                "Szymon Drgas",
                "Lars Bramsl\u00f8w",
                "Archontis Politis",
                "Gaurav Naithani",
                "Tuomas Virtanen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16550v1",
                "http://arxiv.org/pdf/2310.16550v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16548v1",
            "title": "Terahertz-Enpowered Communications and Sensing in 6G Systems:\n  Opportunities and Challenges",
            "updated": "2023-10-25T11:00:49Z",
            "published": "2023-10-25T11:00:49Z",
            "summary": "The current focus of academia and the telecommunications industry has been\nshifted to the development of the six-generation (6G) cellular technology, also\nformally referred to as IMT-2030. Unprecedented applications that 6G aims to\naccommodate demand extreme communications performance and, in addition,\ndisruptive capabilities such as network sensing. Recently, there has been a\nsurge of interest in terahertz (THz) frequencies as it offers not only massive\nspectral resources for communication but also distinct advantages in sensing,\npositioning, and imaging. The aim of this paper is to provide a brief outlook\non opportunities opened by this under-exploited band and challenges that must\nbe addressed to materialize the potential of THz-based communications and\nsensing in 6G systems.",
            "author": [
                "Wei Jiang",
                "Hans D. Schotten"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16548v1",
                "http://arxiv.org/pdf/2310.16548v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16866v1",
            "title": "A Type System for Julia",
            "updated": "2023-10-25T10:55:21Z",
            "published": "2023-10-25T10:55:21Z",
            "summary": "The Julia programming language was designed to fill the needs of scientific\ncomputing by combining the benefits of productivity and performance languages.\nJulia allows users to write untyped scripts easily without needing to worry\nabout many implementation details, as do other productivity languages. If one\njust wants to get the work done-regardless of how efficient or general the\nprogram might be, such a paradigm is ideal. Simultaneously, Julia also allows\nlibrary developers to write efficient generic code that can run as fast as\nimplementations in performance languages such as C or Fortran. This combination\nof user-facing ease and library developer-facing performance has proven quite\nattractive, and the language has increasing adoption.\n  With adoption comes combinatorial challenges to correctness. Multiple\ndispatch -- Julia's key mechanism for abstraction -- allows many libraries to\ncompose \"out of the box.\" However, it creates bugs where one library's\nrequirements do not match what another provides. Typing could address this at\nthe cost of Julia's flexibility for scripting.\n  I developed a \"best of both worlds\" solution: gradual typing for Julia. My\nsystem forms the core of a gradual type system for Julia, laying the foundation\nfor improving the correctness of Julia programs while not getting in the way of\nscript writers. My framework allows methods to be individually typed or\nuntyped, allowing users to write untyped code that interacts with typed library\ncode and vice versa. Typed methods then get a soundness guarantee that is\nrobust in the presence of both dynamically typed code and dynamically generated\ndefinitions. I additionally describe protocols, a mechanism for typing\nabstraction over concrete implementation that accommodates one common pattern\nin Julia libraries, and describe its implementation into my typed Julia\nframework.",
            "author": [
                "Benjamin Chung"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16866v1",
                "http://arxiv.org/pdf/2310.16866v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18366v1",
            "title": "A Multilingual Virtual Guide for Self-Attachment Technique",
            "updated": "2023-10-25T10:50:18Z",
            "published": "2023-10-25T10:50:18Z",
            "summary": "In this work, we propose a computational framework that leverages existing\nout-of-language data to create a conversational agent for the delivery of\nSelf-Attachment Technique (SAT) in Mandarin. Our framework does not require\nlarge-scale human translations, yet it achieves a comparable performance whilst\nalso maintaining safety and reliability. We propose two different methods of\naugmenting available response data through empathetic rewriting. We evaluate\nour chatbot against a previous, English-only SAT chatbot through non-clinical\nhuman trials (N=42), each lasting five days, and quantitatively show that we\nare able to attain a comparable level of performance to the English SAT\nchatbot. We provide qualitative analysis on the limitations of our study and\nsuggestions with the aim of guiding future improvements.",
            "author": [
                "Alicia Jiayun Law",
                "Ruoyu Hu",
                "Lisa Alazraki",
                "Anandha Gopalan",
                "Neophytos Polydorou",
                "Abbas Edalat"
            ],
            "link": [
                "http://dx.doi.org/10.1109/CogMI56440.2022.00025",
                "http://arxiv.org/abs/2310.18366v1",
                "http://arxiv.org/pdf/2310.18366v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16540v1",
            "title": "Dual Defense: Adversarial, Traceable, and Invisible Robust Watermarking\n  against Face Swapping",
            "updated": "2023-10-25T10:39:51Z",
            "published": "2023-10-25T10:39:51Z",
            "summary": "The malicious applications of deep forgery, represented by face swapping,\nhave introduced security threats such as misinformation dissemination and\nidentity fraud. While some research has proposed the use of robust watermarking\nmethods to trace the copyright of facial images for post-event traceability,\nthese methods cannot effectively prevent the generation of forgeries at the\nsource and curb their dissemination. To address this problem, we propose a\nnovel comprehensive active defense mechanism that combines traceability and\nadversariality, called Dual Defense. Dual Defense invisibly embeds a single\nrobust watermark within the target face to actively respond to sudden cases of\nmalicious face swapping. It disrupts the output of the face swapping model\nwhile maintaining the integrity of watermark information throughout the entire\ndissemination process. This allows for watermark extraction at any stage of\nimage tracking for traceability. Specifically, we introduce a watermark\nembedding network based on original-domain feature impersonation attack. This\nnetwork learns robust adversarial features of target facial images and embeds\nwatermarks, seeking a well-balanced trade-off between watermark invisibility,\nadversariality, and traceability through perceptual adversarial encoding\nstrategies. Extensive experiments demonstrate that Dual Defense achieves\noptimal overall defense success rates and exhibits promising universality in\nanti-face swapping tasks and dataset generalization ability. It maintains\nimpressive adversariality and traceability in both original and robust\nsettings, surpassing current forgery defense methods that possess only one of\nthese capabilities, including CMUA-Watermark, Anti-Forgery, FakeTagger, or PGD\nmethods.",
            "author": [
                "Yunming Zhang",
                "Dengpan Ye",
                "Caiyun Xie",
                "Long Tang",
                "Chuanxi Chen",
                "Ziyi Liu",
                "Jiacheng Deng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16540v1",
                "http://arxiv.org/pdf/2310.16540v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16539v1",
            "title": "Toric vector bundles, non-abelianization, and spectral networks",
            "updated": "2023-10-25T10:35:16Z",
            "published": "2023-10-25T10:35:16Z",
            "summary": "Spectral networks and non-abelianization were introduced by\nGaiotto-Moore-Neitzke and they have many applications in mathematics and\nphysics. In a recent work by Nho, he proved that the non-abelianization of an\nalmost flat local system over the spectral curve of a meromorphic quadratic\ndifferential is actually the same as the family Floer. Based on the mirror\nsymmetry philosophy, it is then natural to ask how holomorphic vector bundles\narise from spectral networks and non-abelianization. In this paper, we\nconstruct toric vector bundles on toric surfaces via spectral networks and\nnon-abelianization.",
            "author": [
                "Yat-Hin Suen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16539v1",
                "http://arxiv.org/pdf/2310.16539v1"
            ],
            "primary_category": "math.AG",
            "category": [
                "math.AG",
                "math.DG",
                "math.SG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16538v1",
            "title": "FedTherapist: Mental Health Monitoring with User-Generated Linguistic\n  Expressions on Smartphones via Federated Learning",
            "updated": "2023-10-25T10:35:09Z",
            "published": "2023-10-25T10:35:09Z",
            "summary": "Psychiatrists diagnose mental disorders via the linguistic use of patients.\nStill, due to data privacy, existing passive mental health monitoring systems\nuse alternative features such as activity, app usage, and location via mobile\ndevices. We propose FedTherapist, a mobile mental health monitoring system that\nutilizes continuous speech and keyboard input in a privacy-preserving way via\nfederated learning. We explore multiple model designs by comparing their\nperformance and overhead for FedTherapist to overcome the complex nature of\non-device language model training on smartphones. We further propose a\nContext-Aware Language Learning (CALL) methodology to effectively utilize\nsmartphones' large and noisy text for mental health signal sensing. Our\nIRB-approved evaluation of the prediction of self-reported depression, stress,\nanxiety, and mood from 46 participants shows higher accuracy of FedTherapist\ncompared with the performance with non-language features, achieving 0.15 AUROC\nimprovement and 8.21% MAE reduction.",
            "author": [
                "Jaemin Shin",
                "Hyungjun Yoon",
                "Seungjoo Lee",
                "Sungjoon Park",
                "Yunxin Liu",
                "Jinho D. Choi",
                "Sung-Ju Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16538v1",
                "http://arxiv.org/pdf/2310.16538v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16535v1",
            "title": "R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought\n  Reasoning in Large Language Models under Noisy Context",
            "updated": "2023-10-25T10:34:02Z",
            "published": "2023-10-25T10:34:02Z",
            "summary": "With the help of Chain-of-Thought (CoT) prompting, Large Language Models\n(LLMs) have achieved remarkable performance on various reasoning tasks.\nHowever, most of them have been evaluated under noise-free context and the\ndilemma for LLMs to produce inaccurate results under the noisy context has not\nbeen fully investigated. Existing studies utilize trigger sentences to\nencourage LLMs to concentrate on the relevant information but the trigger has\nlimited effect on final answer prediction. Inspired by interactive CoT method,\nwhere intermediate reasoning steps are promoted by multiple rounds of\ninteraction between users and LLMs, we propose a novel prompting method, namely\nR$^3$ prompting, for CoT reasoning under noisy context. Specifically, R$^3$\nprompting interacts with LLMs to perform key sentence extraction, variable\ndeclaration and answer prediction, which corresponds to a thought process of\nreviewing, rephrasing and resolving. The responses generated at the last\ninteraction will perform as hints to guide toward the responses of the next\ninteraction. Our experiments show that R$^3$ prompting significantly\noutperforms existing CoT prompting methods on five reasoning tasks under noisy\ncontext. With GPT-3.5-turbo, we observe 3.7% accuracy improvement on average on\nthe reasoning tasks under noisy context compared to the most competitive\nprompting baseline. More analyses and ablation studies show the robustness and\ngeneralization of R$^3$ prompting method in solving reasoning tasks in LLMs\nunder noisy context.",
            "author": [
                "Qingyuan Tian",
                "Hanlun Zhu",
                "Lei Wang",
                "Yang Li",
                "Yunshi Lan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16535v1",
                "http://arxiv.org/pdf/2310.16535v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16534v1",
            "title": "An Early Evaluation of GPT-4V(ision)",
            "updated": "2023-10-25T10:33:17Z",
            "published": "2023-10-25T10:33:17Z",
            "summary": "In this paper, we evaluate different abilities of GPT-4V including visual\nunderstanding, language understanding, visual puzzle solving, and understanding\nof other modalities such as depth, thermal, video, and audio. To estimate\nGPT-4V's performance, we manually construct 656 test instances and carefully\nevaluate the results of GPT-4V. The highlights of our findings are as follows:\n(1) GPT-4V exhibits impressive performance on English visual-centric benchmarks\nbut fails to recognize simple Chinese texts in the images; (2) GPT-4V shows\ninconsistent refusal behavior when answering questions related to sensitive\ntraits such as gender, race, and age; (3) GPT-4V obtains worse results than\nGPT-4 (API) on language understanding tasks including general language\nunderstanding benchmarks and visual commonsense knowledge evaluation\nbenchmarks; (4) Few-shot prompting can improve GPT-4V's performance on both\nvisual understanding and language understanding; (5) GPT-4V struggles to find\nthe nuances between two similar images and solve the easy math picture puzzles;\n(6) GPT-4V shows non-trivial performance on the tasks of similar modalities to\nimage, such as video and thermal. Our experimental results reveal the ability\nand limitations of GPT-4V and we hope our paper can provide some insights into\nthe application and research of GPT-4V.",
            "author": [
                "Yang Wu",
                "Shilong Wang",
                "Hao Yang",
                "Tian Zheng",
                "Hongbo Zhang",
                "Yanyan Zhao",
                "Bing Qin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16534v1",
                "http://arxiv.org/pdf/2310.16534v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16530v1",
            "title": "Toward Practical Privacy-Preserving Convolutional Neural Networks\n  Exploiting Fully Homomorphic Encryption",
            "updated": "2023-10-25T10:24:35Z",
            "published": "2023-10-25T10:24:35Z",
            "summary": "Incorporating fully homomorphic encryption (FHE) into the inference process\nof a convolutional neural network (CNN) draws enormous attention as a viable\napproach for achieving private inference (PI). FHE allows delegating the entire\ncomputation process to the server while ensuring the confidentiality of\nsensitive client-side data. However, practical FHE implementation of a CNN\nfaces significant hurdles, primarily due to FHE's substantial computational and\nmemory overhead. To address these challenges, we propose a set of\noptimizations, which includes GPU/ASIC acceleration, an efficient activation\nfunction, and an optimized packing scheme. We evaluate our method using the\nResNet models on the CIFAR-10 and ImageNet datasets, achieving several orders\nof magnitude improvement compared to prior work and reducing the latency of the\nencrypted CNN inference to 1.4 seconds on an NVIDIA A100 GPU. We also show that\nthe latency drops to a mere 0.03 seconds with a custom hardware design.",
            "author": [
                "Jaiyoung Park",
                "Donghwan Kim",
                "Jongmin Kim",
                "Sangpyo Kim",
                "Wonkyung Jung",
                "Jung Hee Cheon",
                "Jung Ho Ahn"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16530v1",
                "http://arxiv.org/pdf/2310.16530v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16528v1",
            "title": "CUNI Submission to MRL 2023 Shared Task on Multi-lingual Multi-task\n  Information Retrieval",
            "updated": "2023-10-25T10:22:49Z",
            "published": "2023-10-25T10:22:49Z",
            "summary": "We present the Charles University system for the MRL~2023 Shared Task on\nMulti-lingual Multi-task Information Retrieval. The goal of the shared task was\nto develop systems for named entity recognition and question answering in\nseveral under-represented languages. Our solutions to both subtasks rely on the\ntranslate-test approach. We first translate the unlabeled examples into English\nusing a multilingual machine translation model. Then, we run inference on the\ntranslated data using a strong task-specific model. Finally, we project the\nlabeled data back into the original language. To keep the inferred tags on the\ncorrect positions in the original language, we propose a method based on\nscoring the candidate positions using a label-sensitive translation model. In\nboth settings, we experiment with finetuning the classification models on the\ntranslated data. However, due to a domain mismatch between the development data\nand the shared task validation and test sets, the finetuned models could not\noutperform our baselines.",
            "author": [
                "Jind\u0159ich Helcl",
                "Jind\u0159ich Libovick\u00fd"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16528v1",
                "http://arxiv.org/pdf/2310.16528v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16526v1",
            "title": "NIKA2 observations of dust grain evolution from star-forming filament to\n  T-Tauri disk: Preliminary results from NIKA2 observations of the Taurus\n  B211/B213 filament",
            "updated": "2023-10-25T10:21:56Z",
            "published": "2023-10-25T10:21:56Z",
            "summary": "To understand the evolution of dust properties in molecular clouds in the\ncourse of the star formation process, we constrain the changes in the dust\nemissivity index from star-forming filaments to prestellar and protostellar\ncores to T Tauri stars. Using the NIKA2 continuum camera on the IRAM 30~m\ntelescope, we observed the Taurus B211/B213 filament at 1.2\\,mm and 2\\,mm with\nunprecedented sensitivity and used the resulting maps to derive the dust\nemissivity index $\\beta$. Our sample of 105 objects detected in the $\\beta$ map\nof the B211/B213 filament indicates that, overall, $\\beta$ decreases from\nfilament and prestellar cores ($\\beta \\sim 2\\pm0.5$) to protostellar cores\n($\\beta \\sim 1.2 \\pm 0.2$) to T-Tauri protoplanetary disk ($\\beta < 1$). The\naveraged dust emissivity index $\\beta$ across the B211/B213 filament exhibits a\nflat ($\\beta \\sim 2\\pm0.3$) profile. This may imply that dust grain sizes are\nrather homogeneous in the filament, start to grow significantly in size only\nafter the onset of the gravitational contraction/collapse of prestellar cores\nto protostars, reaching big sizes in T Tauri protoplanetary disks. This\nevolution from the parent filament to T-Tauri disks happens on a timescale of\nabout 1-2~Myr.",
            "author": [
                "Q. Nguyen-Luong",
                "R. Adam",
                "P. Ade",
                "H. Ajeddig",
                "P. Andr\u00e9",
                "E. Artis",
                "H. Aussel",
                "A. Beelen",
                "A. Beno\u00eet",
                "S. Berta",
                "L. Bing",
                "O. Bourrion",
                "M. Calvo",
                "A. Catalano",
                "M. De Petris",
                "F. -X. D\u00e9sert",
                "S. Doyle",
                "E. F. C. Driessen",
                "G. Ejlali",
                "A. Gomez",
                "J. Goupy",
                "C. Hanser",
                "S. Katsioli",
                "F. K\u00e9ruzor\u00e9",
                "C. Kramer",
                "B. Ladjelate",
                "G. Lagache",
                "S. Leclercq",
                "J. -F. Lestrade",
                "J. F. Mac\u00edas-P\u00e9rez",
                "S. C. Madden",
                "A. Maury",
                "P. Mauskopf",
                "F. Mayet",
                "A. Monfardini",
                "A. Moyer-Anin",
                "M. Mu\u00f1oz-Echeverr\u00eda",
                "L. Perotto",
                "G. Pisano",
                "N. Ponthieu",
                "V. Rev\u00e9ret",
                "A. J. Rigby",
                "A. Ritacco",
                "C. Romero",
                "H. Roussel",
                "F. Ruppin",
                "K. Schuster",
                "A. Sievers",
                "C. Tucker",
                "R. Zylka",
                "A. Bacmann",
                "A. Duong-Tuan",
                "N. Peretto",
                "A. Rigby"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16526v1",
                "http://arxiv.org/pdf/2310.16526v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR",
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16525v1",
            "title": "Cyclic Directed Probabilistic Graphical Model: A Proposal Based on\n  Structured Outcomes",
            "updated": "2023-10-25T10:19:03Z",
            "published": "2023-10-25T10:19:03Z",
            "summary": "In the process of building (structural learning) a probabilistic graphical\nmodel from a set of observed data, the directional, cyclic dependencies between\nthe random variables of the model are often found. Existing graphical models\nsuch as Bayesian and Markov networks can reflect such dependencies. However,\nthis requires complicating those models, such as adding additional variables or\ndividing the model graph into separate subgraphs. Herein, we describe a\nprobabilistic graphical model - probabilistic relation network - that allows\nthe direct capture of directional cyclic dependencies during structural\nlearning. This model is based on the simple idea that each sample of the\nobserved data can be represented by an arbitrary graph (structured outcome),\nwhich reflects the structure of the dependencies of the variables included in\nthe sample. Each of the outcomes contains only a part of the graphical model\nstructure; however, a complete graph of the probabilistic model is obtained by\ncombining different outcomes. Such a graph, unlike Bayesian and Markov\nnetworks, can be directed and can have cycles. We explored the full joint\ndistribution and conditional distribution and conditional independence\nproperties of variables in the proposed model. We defined the algorithms for\nconstructing of the model from the dataset and for calculating the conditional\nand full joint distributions. We also performed a numerical comparison with\nBayesian and Markov networks. This model does not violate the probability\naxioms, and it supports learning from observed data. Notably, it supports\nprobabilistic inference, making it a prospective tool in data analysis and in\nexpert and design-making applications.",
            "author": [
                "Oleksii Sirotkin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16525v1",
                "http://arxiv.org/pdf/2310.16525v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "62H22 (Primary) 05C38, 62H11 (Secondary)",
                "G.3; H.1.0"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16524v1",
            "title": "Can You Rely on Your Model Evaluation? Improving Model Evaluation with\n  Synthetic Test Data",
            "updated": "2023-10-25T10:18:44Z",
            "published": "2023-10-25T10:18:44Z",
            "summary": "Evaluating the performance of machine learning models on diverse and\nunderrepresented subgroups is essential for ensuring fairness and reliability\nin real-world applications. However, accurately assessing model performance\nbecomes challenging due to two main issues: (1) a scarcity of test data,\nespecially for small subgroups, and (2) possible distributional shifts in the\nmodel's deployment setting, which may not align with the available test data.\nIn this work, we introduce 3S Testing, a deep generative modeling framework to\nfacilitate model evaluation by generating synthetic test sets for small\nsubgroups and simulating distributional shifts. Our experiments demonstrate\nthat 3S Testing outperforms traditional baselines -- including real test data\nalone -- in estimating model performance on minority subgroups and under\nplausible distributional shifts. In addition, 3S offers intervals around its\nperformance estimates, exhibiting superior coverage of the ground truth\ncompared to existing approaches. Overall, these results raise the question of\nwhether we need a paradigm shift away from limited real test data towards\nsynthetic test data.",
            "author": [
                "Boris van Breugel",
                "Nabeel Seedat",
                "Fergus Imrie",
                "Mihaela van der Schaar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16524v1",
                "http://arxiv.org/pdf/2310.16524v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16523v1",
            "title": "Improving Diversity of Demographic Representation in Large Language\n  Models via Collective-Critiques and Self-Voting",
            "updated": "2023-10-25T10:17:17Z",
            "published": "2023-10-25T10:17:17Z",
            "summary": "A crucial challenge for generative large language models (LLMs) is diversity:\nwhen a user's prompt is under-specified, models may follow implicit assumptions\nwhile generating a response, which may result in homogenization of the\nresponses, as well as certain demographic groups being under-represented or\neven erased from the generated responses. In this paper, we formalize diversity\nof representation in generative LLMs. We present evaluation datasets and\npropose metrics to measure diversity in generated responses along people and\nculture axes. We find that LLMs understand the notion of diversity, and that\nthey can reason and critique their own responses for that goal. This finding\nmotivated a new prompting technique called collective-critique and self-voting\n(CCSV) to self-improve people diversity of LLMs by tapping into its diversity\nreasoning capabilities, without relying on handcrafted examples or prompt\ntuning. Extensive empirical experiments with both human and automated\nevaluations show that our proposed approach is effective at improving people\nand culture diversity, and outperforms all baseline methods by a large margin.",
            "author": [
                "Preethi Lahoti",
                "Nicholas Blumm",
                "Xiao Ma",
                "Raghavendra Kotikalapudi",
                "Sahitya Potluri",
                "Qijun Tan",
                "Hansa Srinivasan",
                "Ben Packer",
                "Ahmad Beirami",
                "Alex Beutel",
                "Jilin Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16523v1",
                "http://arxiv.org/pdf/2310.16523v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16517v1",
            "title": "OccuQuest: Mitigating Occupational Bias for Inclusive Large Language\n  Models",
            "updated": "2023-10-25T10:06:17Z",
            "published": "2023-10-25T10:06:17Z",
            "summary": "The emergence of large language models (LLMs) has revolutionized natural\nlanguage processing tasks. However, existing instruction-tuning datasets suffer\nfrom occupational bias: the majority of data relates to only a few occupations,\nwhich hampers the instruction-tuned LLMs to generate helpful responses to\nprofessional queries from practitioners in specific fields. To mitigate this\nissue and promote occupation-inclusive LLMs, we create an instruction-tuning\ndataset named \\emph{OccuQuest}, which contains 110,000+ prompt-completion pairs\nand 30,000+ dialogues covering over 1,000 occupations in 26 occupational\ncategories. We systematically request ChatGPT, organizing queries\nhierarchically based on Occupation, Responsibility, Topic, and Question, to\nensure a comprehensive coverage of occupational specialty inquiries. By\ncomparing with three commonly used datasets (Dolly, ShareGPT, and WizardLM), we\nobserve that OccuQuest exhibits a more balanced distribution across\noccupations. Furthermore, we assemble three test sets for comprehensive\nevaluation, an occu-test set covering 25 occupational categories, an estate set\nfocusing on real estate, and an occu-quora set containing real-world questions\nfrom Quora. We then fine-tune LLaMA on OccuQuest to obtain OccuLLaMA, which\nsignificantly outperforms state-of-the-art LLaMA variants (Vicuna, Tulu, and\nWizardLM) on professional questions in GPT-4 and human evaluations. Notably, on\nthe occu-quora set, OccuLLaMA reaches a high win rate of 86.4\\% against\nWizardLM.",
            "author": [
                "Mingfeng Xue",
                "Dayiheng Liu",
                "Kexin Yang",
                "Guanting Dong",
                "Wenqiang Lei",
                "Zheng Yuan",
                "Chang Zhou",
                "Jingren Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16517v1",
                "http://arxiv.org/pdf/2310.16517v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16510v1",
            "title": "Performance best practices using Java and AWS Lambda",
            "updated": "2023-10-25T09:58:07Z",
            "published": "2023-10-25T09:58:07Z",
            "summary": "Despite its already widespread popularity, it continues to gain adoption.\nMore and more developers and architects continue to adopt and apply the FaaS\n(Function as a Service) model in cloud solutions. The most extensively used\nFaaS service is AWS Lambda, provided by Amazon Web Services. Moreover, despite\nthe new trends in programming languages, Java still maintains a significant\nshare of usage. The main problem that arises when using these two technologies\ntogether is widely known: significant latencies and the dreaded cold start.\nHowever, it is possible to greatly mitigate this problem without dedicating too\nmuch effort. In this article, various techniques, strategies and approaches\nwill be studied with the aim of reducing the cold start and significantly\nimproving the performance of Lambda functions with Java. Starting from a system\nthat involves AWS lambda, java, DynamoDB and Api Gateway. Each approach will be\ntested independently, analyzing its impact through load tests. Subsequently,\nthey will be tested in combination in an effort to achieve the greatest\npossible performance improvement.",
            "author": [
                "Juan Mera Men\u00e9ndez",
                "Martin Bartlett"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16510v1",
                "http://arxiv.org/pdf/2310.16510v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16508v1",
            "title": "Hermitian Jacobi Forms Having Modules as their Index and Vector-Valued\n  Jacobi Forms",
            "updated": "2023-10-25T09:51:18Z",
            "published": "2023-10-25T09:51:18Z",
            "summary": "We develop the theory of Hermitian Jacobi forms of lattice index, for both\ndefinite and indefinite Hermitian lattices. We also prove a theta decomposition\ntheorem for vector-valued Jacobi forms (both in the orthogonal and Hermitian\nsettings), with enhanced periodicity properties. This allows us to give a good\ndefinition of orthogonal and Hermitian Jacobi forms of matrix index, when the\nmatrix need not be integral in any natural sense.",
            "author": [
                "Shaul Zemel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16508v1",
                "http://arxiv.org/pdf/2310.16508v1"
            ],
            "primary_category": "math.NT",
            "category": [
                "math.NT",
                "11F50, 11F27, 11F37"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16504v1",
            "title": "Structure of CSS and CSS-T Quantum Codes",
            "updated": "2023-10-25T09:46:11Z",
            "published": "2023-10-25T09:46:11Z",
            "summary": "We investigate CSS and CSS-T quantum error-correcting codes from the point of\nview of their existence, rarity, and performance. We give a lower bound on the\nnumber of pairs of linear codes that give rise to a CSS code with good\ncorrection capability, showing that such pairs are easy to produce with a\nrandomized construction. We then prove that CSS-T codes exhibit the opposite\nbehaviour, showing also that, under very natural assumptions, their rate and\nrelative distance cannot be simultaneously large. This partially answers an\nopen question on the feasible parameters of CSS-T codes. We conclude with a\nsimple construction of CSS-T codes from Hermitian curves. The paper also offers\na concise introduction to CSS and CSS-T codes from the point of view of\nclassical coding theory.",
            "author": [
                "Elena Berardini",
                "Alessio Caminata",
                "Alberto Ravagnani"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16504v1",
                "http://arxiv.org/pdf/2310.16504v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT",
                "81P73, 94B65"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16500v1",
            "title": "Simultaneously probing the sound speed and equation of state of the\n  early Universe with pulsar timing arrays",
            "updated": "2023-10-25T09:43:47Z",
            "published": "2023-10-25T09:43:47Z",
            "summary": "Recently, several major pulsar timing array (PTA) collaborations have\nassembled strong evidence for the existence of a gravitational-wave background\nat frequencies around the nanohertz regime. Assuming that the PTA signal is\nattributed to scalar-induced gravitational waves, we jointly employ the PTA\ndata from the NANOGrav 15-year data set, PPTA DR3, and EPTA DR2 to probe the\nconditions of the early Universe. Specifically, we explore the equation of\nstate parameter ($w$), the sound speed ($c_s$), and the reheating temperature\n($T_\\mathrm{rh}$), finding $w=0.60^{+0.32}_{-0.39}$, $c_s\\gtrsim 0.09$, and\n$T_\\mathrm{rh}\\lesssim 0.2\\,\\mathrm{GeV}$ for a lognormal power spectrum of the\ncurvature perturbation. Furthermore, we compute Bayes factors to compare\ndifferent models against the radiation domination model ($c_s^2 = w = 1/3$),\neffectively excluding the pressure-less fluid domination model. Our study\nunderscores the significance of scalar-induced gravitational waves as a\npowerful tool to explore the nature of the early Universe.",
            "author": [
                "Lang Liu",
                "You Wu",
                "Zu-Cheng Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16500v1",
                "http://arxiv.org/pdf/2310.16500v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.12861v1",
            "title": "A versatile circuit for emulating active biological dendrites applied to\n  sound localisation and neuron imitation",
            "updated": "2023-10-25T09:42:24Z",
            "published": "2023-10-25T09:42:24Z",
            "summary": "Sophisticated machine learning struggles to transition onto battery-operated\ndevices due to the high-power consumption of neural networks. Researchers have\nturned to neuromorphic engineering, inspired by biological neural networks, for\nmore efficient solutions. While previous research focused on artificial neurons\nand synapses, an essential component has been overlooked: dendrites. Dendrites\ntransmit inputs from synapses to the neuron's soma, applying both passive and\nactive transformations. However, neuromorphic circuits replace these\nsophisticated computational channels with metallic interconnects. In this\nstudy, we introduce a versatile circuit that emulates a segment of a dendrite\nwhich exhibits gain, introduces delays, and performs integration. We show how\nsound localisation - a biological example of dendritic computation - is not\npossible with the existing passive dendrite circuits but can be achieved using\nthis proposed circuit. We also find that dendrites can form bursting neurons.\nThis significant discovery suggests the potential to fabricate neural networks\nsolely comprised of dendrite circuits.",
            "author": [
                "Daniel John Mannion"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12861v1",
                "http://arxiv.org/pdf/2311.12861v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE",
                "cs.AI",
                "cs.ET",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16495v1",
            "title": "Proton and molecular permeation through the basal plane of monolayer\n  graphene oxide",
            "updated": "2023-10-25T09:28:59Z",
            "published": "2023-10-25T09:28:59Z",
            "summary": "Two-dimensional (2D) materials offer a prospect of membranes that combine\nnegligible gas permeability with high proton conductivity and could outperform\nthe existing proton exchange membranes used in various applications including\nfuel cells. Graphene oxide (GO), a well-known 2D material, facilitates rapid\nproton transport along its basal plane but proton conductivity across it\nremains unknown. It is also often presumed that individual GO monolayers\ncontain a large density of nanoscale pinholes that lead to considerable gas\nleakage across the GO basal plane. Here we show that relatively large,\nmicrometer-scale areas of monolayer GO are impermeable to gases, including\nhelium, while exhibiting proton conductivity through the basal plane which is\nnearly two orders of magnitude higher than that of graphene. These findings\nprovide insights into the key properties of GO and demonstrate that chemical\nfunctionalization of 2D crystals can be utilized to enhance their proton\ntransparency without compromising gas impermeability.",
            "author": [
                "Z. F. Wu",
                "P. Z. Sun",
                "O. J. Wahab",
                "Y. -T. Tao",
                "D. Barry",
                "D. Periyanagounder",
                "P. B. Pillai",
                "Q. Dai",
                "W. Q. Xiong",
                "L. F. Vega",
                "K. Lulla",
                "S. J. Yuan",
                "R. R. Nair",
                "E. Daviddi",
                "P. R. Unwin",
                "A. K. Geim",
                "M. Lozada-Hidalgo"
            ],
            "link": [
                "http://dx.doi.org/10.1038/s41467-023-43637-w",
                "http://arxiv.org/abs/2310.16495v1",
                "http://arxiv.org/pdf/2310.16495v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16494v1",
            "title": "Lang3DSG: Language-based contrastive pre-training for 3D Scene Graph\n  prediction",
            "updated": "2023-10-25T09:26:16Z",
            "published": "2023-10-25T09:26:16Z",
            "summary": "D scene graphs are an emerging 3D scene representation, that models both the\nobjects present in the scene as well as their relationships. However, learning\n3D scene graphs is a challenging task because it requires not only object\nlabels but also relationship annotations, which are very scarce in datasets.\nWhile it is widely accepted that pre-training is an effective approach to\nimprove model performance in low data regimes, in this paper, we find that\nexisting pre-training methods are ill-suited for 3D scene graphs. To solve this\nissue, we present the first language-based pre-training approach for 3D scene\ngraphs, whereby we exploit the strong relationship between scene graphs and\nlanguage. To this end, we leverage the language encoder of CLIP, a popular\nvision-language model, to distill its knowledge into our graph-based network.\nWe formulate a contrastive pre-training, which aligns text embeddings of\nrelationships (subject-predicate-object triplets) and predicted 3D graph\nfeatures. Our method achieves state-of-the-art results on the main semantic 3D\nscene graph benchmark by showing improved effectiveness over pre-training\nbaselines and outperforming all the existing fully supervised scene graph\nprediction methods by a significant margin. Furthermore, since our scene graph\nfeatures are language-aligned, it allows us to query the language space of the\nfeatures in a zero-shot manner. In this paper, we show an example of utilizing\nthis property of the features to predict the room type of a scene without\nfurther training.",
            "author": [
                "Sebastian Koch",
                "Pedro Hermosilla",
                "Narunas Vaskevicius",
                "Mirco Colosi",
                "Timo Ropinski"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16494v1",
                "http://arxiv.org/pdf/2310.16494v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16492v1",
            "title": "On the Powerfulness of Textual Outlier Exposure for Visual OoD Detection",
            "updated": "2023-10-25T09:19:45Z",
            "published": "2023-10-25T09:19:45Z",
            "summary": "Successful detection of Out-of-Distribution (OoD) data is becoming\nincreasingly important to ensure safe deployment of neural networks. One of the\nmain challenges in OoD detection is that neural networks output overconfident\npredictions on OoD data, make it difficult to determine OoD-ness of data solely\nbased on their predictions. Outlier exposure addresses this issue by\nintroducing an additional loss that encourages low-confidence predictions on\nOoD data during training. While outlier exposure has shown promising potential\nin improving OoD detection performance, all previous studies on outlier\nexposure have been limited to utilizing visual outliers. Drawing inspiration\nfrom the recent advancements in vision-language pre-training, this paper\nventure out to the uncharted territory of textual outlier exposure. First, we\nuncover the benefits of using textual outliers by replacing real or virtual\noutliers in the image-domain with textual equivalents. Then, we propose various\nways of generating preferable textual outliers. Our extensive experiments\ndemonstrate that generated textual outliers achieve competitive performance on\nlarge-scale OoD and hard OoD benchmarks. Furthermore, we conduct empirical\nanalyses of textual outliers to provide primary criteria for designing\nadvantageous textual outliers: near-distribution, descriptiveness, and\ninclusion of visual semantics.",
            "author": [
                "Sangha Park",
                "Jisoo Mok",
                "Dahuin Jung",
                "Saehyung Lee",
                "Sungroh Yoon"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16492v1",
                "http://arxiv.org/pdf/2310.16492v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16491v1",
            "title": "TSONN: Time-stepping-oriented neural network for solving partial\n  differential equations",
            "updated": "2023-10-25T09:19:40Z",
            "published": "2023-10-25T09:19:40Z",
            "summary": "Deep neural networks (DNNs), especially physics-informed neural networks\n(PINNs), have recently become a new popular method for solving forward and\ninverse problems governed by partial differential equations (PDEs). However,\nthese methods still face challenges in achieving stable training and obtaining\ncorrect results in many problems, since minimizing PDE residuals with PDE-based\nsoft constraint make the problem ill-conditioned. Different from all existing\nmethods that directly minimize PDE residuals, this work integrates\ntime-stepping method with deep learning, and transforms the original\nill-conditioned optimization problem into a series of well-conditioned\nsub-problems over given pseudo time intervals. The convergence of model\ntraining is significantly improved by following the trajectory of the pseudo\ntime-stepping process, yielding a robust optimization-based PDE solver. Our\nresults show that the proposed method achieves stable training and correct\nresults in many problems that standard PINNs fail to solve, requiring only a\nsimple modification on the loss function. In addition, we demonstrate several\nnovel properties and advantages of time-stepping methods within the framework\nof neural network-based optimization approach, in comparison to traditional\ngrid-based numerical method. Specifically, explicit scheme allows significantly\nlarger time step, while implicit scheme can be implemented as straightforwardly\nas explicit scheme.",
            "author": [
                "Wenbo Cao",
                "Weiwei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16491v1",
                "http://arxiv.org/pdf/2310.16491v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16489v1",
            "title": "Latent event history models for quasi-reaction systems",
            "updated": "2023-10-25T09:18:45Z",
            "published": "2023-10-25T09:18:45Z",
            "summary": "Various processes can be modelled as quasi-reaction systems of stochastic\ndifferential equations, such as cell differentiation and disease spreading.\nSince the underlying data of particle interactions, such as reactions between\nproteins or contacts between people, are typically unobserved, statistical\ninference of the parameters driving these systems is developed from\nconcentration data measuring each unit in the system over time. While observing\nthe continuous time process at a time scale as fine as possible should in\ntheory help with parameter estimation, the existing Local Linear Approximation\n(LLA) methods fail in this case, due to numerical instability caused by small\nchanges of the system at successive time points. On the other hand, one may be\nable to reconstruct the underlying unobserved interactions from the observed\ncount data. Motivated by this, we first formalise the latent event history\nmodel underlying the observed count process. We then propose a computationally\nefficient Expectation-Maximation algorithm for parameter estimation, with an\nextended Kalman filtering procedure for the prediction of the latent states. A\nsimulation study shows the performance of the proposed method and highlights\nthe settings where it is particularly advantageous compared to the existing LLA\napproaches. Finally, we present an illustration of the methodology on the\nspreading of the COVID-19 pandemic in Italy.",
            "author": [
                "Matteo Framba",
                "Veronica Vinciotti",
                "Ernst C. Wit"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16489v1",
                "http://arxiv.org/pdf/2310.16489v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16485v1",
            "title": "A Comprehensive Python Library for Deep Learning-Based Event Detection\n  in Multivariate Time Series Data and Information Retrieval in NLP",
            "updated": "2023-10-25T09:13:19Z",
            "published": "2023-10-25T09:13:19Z",
            "summary": "Event detection in time series data is crucial in various domains, including\nfinance, healthcare, cybersecurity, and science. Accurately identifying events\nin time series data is vital for making informed decisions, detecting\nanomalies, and predicting future trends. Despite extensive research exploring\ndiverse methods for event detection in time series, with deep learning\napproaches being among the most advanced, there is still room for improvement\nand innovation in this field. In this paper, we present a new deep learning\nsupervised method for detecting events in multivariate time series data. Our\nmethod combines four distinct novelties compared to existing deep-learning\nsupervised methods. Firstly, it is based on regression instead of binary\nclassification. Secondly, it does not require labeled datasets where each point\nis labeled; instead, it only requires reference events defined as time points\nor intervals of time. Thirdly, it is designed to be robust by using a stacked\nensemble learning meta-model that combines deep learning models, ranging from\nclassic feed-forward neural networks (FFNs) to state-of-the-art architectures\nlike transformers. This ensemble approach can mitigate individual model\nweaknesses and biases, resulting in more robust predictions. Finally, to\nfacilitate practical implementation, we have developed a Python package to\naccompany our proposed method. The package, called eventdetector-ts, can be\ninstalled through the Python Package Index (PyPI). In this paper, we present\nour method and provide a comprehensive guide on the usage of the package. We\nshowcase its versatility and effectiveness through different real-world use\ncases from natural language processing (NLP) to financial security domains.",
            "author": [
                "Menouar Azib",
                "Benjamin Renard",
                "Philippe Garnier",
                "Vincent G\u00e9not",
                "Nicolas Andr\u00e9"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16485v1",
                "http://arxiv.org/pdf/2310.16485v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17671v1",
            "title": "Transfer of Reinforcement Learning-Based Controllers from Model- to\n  Hardware-in-the-Loop",
            "updated": "2023-10-25T09:13:12Z",
            "published": "2023-10-25T09:13:12Z",
            "summary": "The process of developing control functions for embedded systems is\nresource-, time-, and data-intensive, often resulting in sub-optimal cost and\nsolutions approaches. Reinforcement Learning (RL) has great potential for\nautonomously training agents to perform complex control tasks with minimal\nhuman intervention. Due to costly data generation and safety constraints,\nhowever, its application is mostly limited to purely simulated domains. To use\nRL effectively in embedded system function development, the generated agents\nmust be able to handle real-world applications. In this context, this work\nfocuses on accelerating the training process of RL agents by combining Transfer\nLearning (TL) and X-in-the-Loop (XiL) simulation. For the use case of transient\nexhaust gas re-circulation control for an internal combustion engine, use of a\ncomputationally cheap Model-in-the-Loop (MiL) simulation is made to select a\nsuitable algorithm, fine-tune hyperparameters, and finally train candidate\nagents for the transfer. These pre-trained RL agents are then fine-tuned in a\nHardware-in-the-Loop (HiL) system via TL. The transfer revealed the need for\nadjusting the reward parameters when advancing to real hardware. Further, the\ncomparison between a purely HiL-trained and a transferred agent showed a\nreduction of training time by a factor of 5.9. The results emphasize the\nnecessity to train RL agents with real hardware, and demonstrate that the\nmaturity of the transferred policies affects both training time and\nperformance, highlighting the strong synergies between TL and XiL simulation.",
            "author": [
                "Mario Picerno",
                "Lucas Koch",
                "Kevin Badalian",
                "Marius Wegener",
                "Joschka Schaub",
                "Charles Robert Koch",
                "Jakob Andert"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17671v1",
                "http://arxiv.org/pdf/2310.17671v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16484v1",
            "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and\n  Interacts during Language Model Training",
            "updated": "2023-10-25T09:09:55Z",
            "published": "2023-10-25T09:09:55Z",
            "summary": "Representational spaces learned via language modeling are fundamental to\nNatural Language Processing (NLP), however there has been limited understanding\nregarding how and when during training various types of linguistic information\nemerge and interact. Leveraging a novel information theoretic probing suite,\nwhich enables direct comparisons of not just task performance, but their\nrepresentational subspaces, we analyze nine tasks covering syntax, semantics\nand reasoning, across 2M pre-training steps and five seeds. We identify\ncritical learning phases across tasks and time, during which subspaces emerge,\nshare information, and later disentangle to specialize. Across these phases,\nsyntactic knowledge is acquired rapidly after 0.5% of full training. Continued\nperformance improvements primarily stem from the acquisition of open-domain\nknowledge, while semantics and reasoning tasks benefit from later boosts to\nlong-range contextualization and higher specialization. Measuring cross-task\nsimilarity further reveals that linguistically related tasks share information\nthroughout training, and do so more during the critical phase of learning than\nbefore or after. Our findings have implications for model interpretability,\nmulti-task learning, and learning from limited data.",
            "author": [
                "Max M\u00fcller-Eberstein",
                "Rob van der Goot",
                "Barbara Plank",
                "Ivan Titov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16484v1",
                "http://arxiv.org/pdf/2310.16484v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16481v1",
            "title": "A Novel Approach for Object Based Audio Broadcasting",
            "updated": "2023-10-25T09:05:48Z",
            "published": "2023-10-25T09:05:48Z",
            "summary": "Object Based Audio (OBA) provides a new kind of audio experience, delivered\nto the audience to personalize and customize their experience of listening and\nto give them choice of what and how to hear their audio content. OBA can be\napplied to different platforms such as broadcasting, streaming and cinema\nsound. This paper presents a novel approach for creating object-based audio on\nthe production side. The approach here presents Sample-by-Sample Object Based\nAudio (SSOBA) embedding. SSOBA places audio object samples in such a way that\nallows audiences to easily individualize their chosen audio sources according\nto their interests and needs. SSOBA is an extra service and not an alternative,\nso it is also compliant with legacy audio players. The biggest advantage of\nSSOBA is that it does not require any special additional hardware in the\nbroadcasting chain and it is therefore easy to implement and equip legacy\nplayers and decoders with enhanced ability. Input audio objects, number of\noutput channels and sampling rates are three important factors affecting SSOBA\nperformance and specifying it to be lossless or lossy. SSOBA adopts\ninterpolation at the decoder side to compensate for eliminated samples. Both\nsubjective and objective experiments are carried out to evaluate the output\nresults at each step. MUSHRA subjective experiments conducted after the\nencoding step shows good-quality performance of SSOBA with up to five objects.\nSNR measurements and objective experiments, performed after decoding and\ninterpolation, show significant successful recovery and separation of audio\nobjects. Experimental results show that a minimum sampling rate of 96 kHz is\nindicated to encode up to five objects in a Stereo-mode channel to acquire good\nsubjective and objective results simultaneously.",
            "author": [
                "Mohammad Reza Hasanabadi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16481v1",
                "http://arxiv.org/pdf/2310.16481v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16477v1",
            "title": "Show from Tell: Audio-Visual Modelling in Clinical Settings",
            "updated": "2023-10-25T08:55:48Z",
            "published": "2023-10-25T08:55:48Z",
            "summary": "Auditory and visual signals usually present together and correlate with each\nother, not only in natural environments but also in clinical settings. However,\nthe audio-visual modelling in the latter case can be more challenging, due to\nthe different sources of audio/video signals and the noise (both signal-level\nand semantic-level) in auditory signals -- usually speech. In this paper, we\nconsider audio-visual modelling in a clinical setting, providing a solution to\nlearn medical representations that benefit various clinical tasks, without\nhuman expert annotation. A simple yet effective multi-modal self-supervised\nlearning framework is proposed for this purpose. The proposed approach is able\nto localise anatomical regions of interest during ultrasound imaging, with only\nspeech audio as a reference. Experimental evaluations on a large-scale clinical\nmulti-modal ultrasound video dataset show that the proposed self-supervised\nmethod learns good transferable anatomical representations that boost the\nperformance of automated downstream clinical tasks, even outperforming\nfully-supervised solutions.",
            "author": [
                "Jianbo Jiao",
                "Mohammad Alsharid",
                "Lior Drukker",
                "Aris T. Papageorghiou",
                "Andrew Zisserman",
                "J. Alison Noble"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16477v1",
                "http://arxiv.org/pdf/2310.16477v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16473v1",
            "title": "Symphony of experts: orchestration with adversarial insights in\n  reinforcement learning",
            "updated": "2023-10-25T08:53:51Z",
            "published": "2023-10-25T08:53:51Z",
            "summary": "Structured reinforcement learning leverages policies with advantageous\nproperties to reach better performance, particularly in scenarios where\nexploration poses challenges. We explore this field through the concept of\norchestration, where a (small) set of expert policies guides decision-making;\nthe modeling thereof constitutes our first contribution. We then establish\nvalue-functions regret bounds for orchestration in the tabular setting by\ntransferring regret-bound results from adversarial settings. We generalize and\nextend the analysis of natural policy gradient in Agarwal et al. [2021, Section\n5.3] to arbitrary adversarial aggregation strategies. We also extend it to the\ncase of estimated advantage functions, providing insights into sample\ncomplexity both in expectation and high probability. A key point of our\napproach lies in its arguably more transparent proofs compared to existing\nmethods. Finally, we present simulations for a stochastic matching toy model.",
            "author": [
                "Matthieu Jonckheere",
                "Chiara Mignacco",
                "Gilles Stoltz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16473v1",
                "http://arxiv.org/pdf/2310.16473v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16472v1",
            "title": "Semiring Provenance for Lightweight Description Logics",
            "updated": "2023-10-25T08:53:49Z",
            "published": "2023-10-25T08:53:49Z",
            "summary": "We investigate semiring provenance--a successful framework originally defined\nin the relational database setting--for description logics. In this context,\nthe ontology axioms are annotated with elements of a commutative semiring and\nthese annotations are propagated to the ontology consequences in a way that\nreflects how they are derived. We define a provenance semantics for a language\nthat encompasses several lightweight description logics and show its\nrelationships with semantics that have been defined for ontologies annotated\nwith a specific kind of annotation (such as fuzzy degrees). We show that under\nsome restrictions on the semiring, the semantics satisfies desirable properties\n(such as extending the semiring provenance defined for databases). We then\nfocus on the well-known why-provenance, which allows to compute the semiring\nprovenance for every additively and multiplicatively idempotent commutative\nsemiring, and for which we study the complexity of problems related to the\nprovenance of an axiom or a conjunctive query answer. Finally, we consider two\nmore restricted cases which correspond to the so-called positive Boolean\nprovenance and lineage in the database setting. For these cases, we exhibit\nrelationships with well-known notions related to explanations in description\nlogics and complete our complexity analysis. As a side contribution, we provide\nconditions on an ELHI_bot ontology that guarantee tractable reasoning.",
            "author": [
                "Camille Bourgaux",
                "Ana Ozaki",
                "Rafael Pe\u00f1aloza"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16472v1",
                "http://arxiv.org/pdf/2310.16472v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "cs.AI",
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16466v1",
            "title": "Learning Continuous Network Emerging Dynamics from Scarce Observations\n  via Data-Adaptive Stochastic Processes",
            "updated": "2023-10-25T08:44:05Z",
            "published": "2023-10-25T08:44:05Z",
            "summary": "Learning network dynamics from the empirical structure and spatio-temporal\nobservation data is crucial to revealing the interaction mechanisms of complex\nnetworks in a wide range of domains. However, most existing methods only aim at\nlearning network dynamic behaviors generated by a specific ordinary\ndifferential equation instance, resulting in ineffectiveness for new ones, and\ngenerally require dense observations. The observed data, especially from\nnetwork emerging dynamics, are usually difficult to obtain, which brings\ntrouble to model learning. Therefore, how to learn accurate network dynamics\nwith sparse, irregularly-sampled, partial, and noisy observations remains a\nfundamental challenge. We introduce Neural ODE Processes for Network Dynamics\n(NDP4ND), a new class of stochastic processes governed by stochastic\ndata-adaptive network dynamics, to overcome the challenge and learn continuous\nnetwork dynamics from scarce observations. Intensive experiments conducted on\nvarious network dynamics in ecological population evolution, phototaxis\nmovement, brain activity, epidemic spreading, and real-world empirical systems,\ndemonstrate that the proposed method has excellent data adaptability and\ncomputational efficiency, and can adapt to unseen network emerging dynamics,\nproducing accurate interpolation and extrapolation with reducing the ratio of\nrequired observation data to only about 6\\% and improving the learning speed\nfor new dynamics by three orders of magnitude.",
            "author": [
                "Jiaxu Cui",
                "Bingyi Sun",
                "Jiming Liu",
                "Bo Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16466v1",
                "http://arxiv.org/pdf/2310.16466v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16465v1",
            "title": "Recent advances in label-free imaging and quantification techniques for\n  the study of lipid droplets in cells",
            "updated": "2023-10-25T08:43:33Z",
            "published": "2023-10-25T08:43:33Z",
            "summary": "Lipid droplets (LDs), once considered mere storage depots for lipids, have\ngained recognition for their intricate roles in cellular processes, including\nmetabolism, membrane trafficking, and disease states like obesity and cancer.\nThis review explores label-free imaging techniques' applications in LD\nresearch. We discuss holotomography and vibrational spectroscopic microscopy,\nemphasizing their potential for studying LDs without molecular labels, and we\nhighlight the growing integration of artificial intelligence. Clinical\napplications in disease diagnosis and therapy are also considered.",
            "author": [
                "Hyeonwoo Kim",
                "Seungeun Oh",
                "Seongsoo Lee",
                "Kwang suk Lee",
                "YongKeun Park"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16465v1",
                "http://arxiv.org/pdf/2310.16465v1"
            ],
            "primary_category": "physics.bio-ph",
            "category": [
                "physics.bio-ph",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16464v1",
            "title": "Covariance matrices for the Lyman-$\u03b1$ forest using the lognormal\n  approximation",
            "updated": "2023-10-25T08:41:18Z",
            "published": "2023-10-25T08:41:18Z",
            "summary": "We investigate the nature of correlations in the small-scale flux statistics\nof the Lyman-$\\alpha$ (Ly$\\alpha$) forest across redshift bins. Understanding\nthese correlations is important for unbiased cosmological and astrophysical\nparameter inference using the Ly$\\alpha$ forest. We focus on the 1-dimensional\nflux power spectrum (FPS) and mean flux ($\\bar F$) simulated using the\nsemi-numerical lognormal model we developed in earlier work. The lognormal\nmodel can capture the effects of long wavelength modes with relative ease as\ncompared to full smoothed particle hydrodynamical (SPH) simulations that are\nlimited by box volume. For a single redshift bin of size $\\Delta z\\simeq 0.1$,\nwe show that the lognormal model predicts positive cross-correlations between\n$k$-bins in the FPS, and a negative correlation for $\\bar F\\times$ FPS, in\nqualitative agreement with SPH simulations and theoretical expectations. For\nmeasurements across two neighbouring redshift bins of width $\\Delta z$ each\n(obtained by 'splitting' skewers of length $2\\Delta z$ in half), the lognormal\nmodel predicts an anti-correlation for FPS $\\times$ FPS and a positive\ncorrelation for $\\bar F\\times$ FPS, caused by long wavelength modes. This is in\ncontrast to SPH simulations which predict a negligible magnitude for\ncross-redshift correlations derived from such `split' skewers, and we discuss\npossible reasons for this difference. Finally, we perform a preliminary test of\nthe impact of neglecting long wavelength modes on parameter inference, finding\nthat whereas the correlation structure of neighbouring redshift bins has\nrelatively little impact, the absence of long wavelength modes in the model can\nlead to $\\gtrsim2-\\sigma$ biases in the inference of astrophysical parameters.\nOur results motivate a more careful treatment of long wavelength modes in\nanalyses that rely on the small scale Ly$\\alpha$ forest for parameter\ninference.",
            "author": [
                "Bhaskar Arya",
                "Aseem Paranjape",
                "Tirthankar Roy Choudhury"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16464v1",
                "http://arxiv.org/pdf/2310.16464v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17670v1",
            "title": "Unknown Health States Recognition With Collective Decision Based Deep\n  Learning Networks In Predictive Maintenance Applications",
            "updated": "2023-10-25T08:24:48Z",
            "published": "2023-10-25T08:24:48Z",
            "summary": "At present, decision making solutions developed based on deep learning (DL)\nmodels have received extensive attention in predictive maintenance (PM)\napplications along with the rapid improvement of computing power. Relying on\nthe superior properties of shared weights and spatial pooling, Convolutional\nNeural Network (CNN) can learn effective representations of health states from\nindustrial data. Many developed CNN-based schemes, such as advanced CNNs that\nintroduce residual learning and multi-scale learning, have shown good\nperformance in health state recognition tasks under the assumption that all the\nclasses are known. However, these schemes have no ability to deal with new\nabnormal samples that belong to state classes not part of the training set. In\nthis paper, a collective decision framework for different CNNs is proposed. It\nis based on a One-vs-Rest network (OVRN) to simultaneously achieve\nclassification of known and unknown health states. OVRN learn state-specific\ndiscriminative features and enhance the ability to reject new abnormal samples\nincorporated to different CNNs. According to the validation results on the\npublic dataset of Tennessee Eastman Process (TEP), the proposed CNN-based\ndecision schemes incorporating OVRN have outstanding recognition ability for\nsamples of unknown heath states, while maintaining satisfactory accuracy on\nknown states. The results show that the new DL framework outperforms\nconventional CNNs, and the one based on residual and multi-scale learning has\nthe best overall performance.",
            "author": [
                "Chuyue Lou",
                "M. Amine Atoui"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17670v1",
                "http://arxiv.org/pdf/2310.17670v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16863v1",
            "title": "Graph-based multimodal multi-lesion DLBCL treatment response prediction\n  from PET images",
            "updated": "2023-10-25T08:16:45Z",
            "published": "2023-10-25T08:16:45Z",
            "summary": "Diffuse Large B-cell Lymphoma (DLBCL) is a lymphatic cancer involving one or\nmore lymph nodes and extranodal sites. Its diagnostic and follow-up rely on\nPositron Emission Tomography (PET) and Computed Tomography (CT). After\ndiagnosis, the number of nonresponding patients to standard front-line therapy\nremains significant (30-40%). This work aims to develop a computer-aided\napproach to identify high-risk patients requiring adapted treatment by\nefficiently exploiting all the information available for each patient,\nincluding both clinical and image data. We propose a method based on recent\ngraph neural networks that combine imaging information from multiple lesions,\nand a cross-attention module to integrate different data modalities\nefficiently. The model is trained and evaluated on a private prospective\nmulticentric dataset of 583 patients. Experimental results show that our\nproposed method outperforms classical supervised methods based on either\nclinical, imaging or both clinical and imaging data for the 2-year\nprogression-free survival (PFS) classification accuracy.",
            "author": [
                "Oriane Thiery",
                "Mira Rizkallah",
                "Cl\u00e9ment Bailly",
                "Caroline Bodet-Milin",
                "Emmanuel Itti",
                "Ren\u00e9-Olivier Casasnovas",
                "Steven Le Gouill",
                "Thomas Carlier",
                "Diana Mateus"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16863v1",
                "http://arxiv.org/pdf/2310.16863v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.AI",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16452v2",
            "title": "Faithful Path Language Modelling for Explainable Recommendation over\n  Knowledge Graph",
            "updated": "2023-11-12T22:38:04Z",
            "published": "2023-10-25T08:14:49Z",
            "summary": "Path reasoning methods over knowledge graphs have gained popularity for their\npotential to improve transparency in recommender systems. However, the\nresulting models still rely on pre-trained knowledge graph embeddings, fail to\nfully exploit the interdependence between entities and relations in the KG for\nrecommendation, and may generate inaccurate explanations. In this paper, we\nintroduce PEARLM, a novel approach that efficiently captures user behaviour and\nproduct-side knowledge through language modelling. With our approach, knowledge\ngraph embeddings are directly learned from paths over the KG by the language\nmodel, which also unifies entities and relations in the same optimisation\nspace. Constraints on the sequence decoding additionally guarantee path\nfaithfulness with respect to the KG. Experiments on two datasets show the\neffectiveness of our approach compared to state-of-the-art baselines. Source\ncode and datasets: AVAILABLE AFTER GETTING ACCEPTED.",
            "author": [
                "Giacomo Balloccu",
                "Ludovico Boratto",
                "Christian Cancedda",
                "Gianni Fenu",
                "Mirko Marras"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16452v2",
                "http://arxiv.org/pdf/2310.16452v2"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16450v1",
            "title": "CLEX: Continuous Length Extrapolation for Large Language Models",
            "updated": "2023-10-25T08:13:02Z",
            "published": "2023-10-25T08:13:02Z",
            "summary": "Transformer-based Large Language Models (LLMs) are pioneering advances in\nmany natural language processing tasks, however, their exceptional capabilities\nare restricted within the preset context window of Transformer. Position\nEmbedding (PE) scaling methods, while effective in extending the context window\nto a specific length, demonstrate either notable limitations in their\nextrapolation abilities or sacrificing partial performance within the context\nwindow. Length extrapolation methods, although theoretically capable of\nextending the context window beyond the training sequence length, often\nunderperform in practical long-context applications. To address these\nchallenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We\ngeneralise the PE scaling approaches to model the continuous dynamics by\nordinary differential equations over the length scaling factor, thereby\novercoming the constraints of current PE scaling methods designed for specific\nlengths. Moreover, by extending the dynamics to desired context lengths beyond\nthe training sequence length, CLEX facilitates the length extrapolation with\nimpressive performance in practical tasks. We demonstrate that CLEX can be\nseamlessly incorporated into LLMs equipped with Rotary Position Embedding, such\nas LLaMA and GPT-NeoX, with negligible impact on training and inference\nlatency. Experimental results reveal that CLEX can effectively extend the\ncontext window to over 4x or almost 8x training length, with no deterioration\nin performance. Furthermore, when evaluated on the practical LongBench\nbenchmark, our model trained on a 4k length exhibits competitive performance\nagainst state-of-the-art open-source models trained on context lengths up to\n32k.",
            "author": [
                "Guanzheng Chen",
                "Xin Li",
                "Zaiqiao Meng",
                "Shangsong Liang",
                "Lidong Bing"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16450v1",
                "http://arxiv.org/pdf/2310.16450v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16446v1",
            "title": "Diversity Enhanced Narrative Question Generation for Storybooks",
            "updated": "2023-10-25T08:10:04Z",
            "published": "2023-10-25T08:10:04Z",
            "summary": "Question generation (QG) from a given context can enhance comprehension,\nengagement, assessment, and overall efficacy in learning or conversational\nenvironments. Despite recent advancements in QG, the challenge of enhancing or\nmeasuring the diversity of generated questions often remains unaddressed. In\nthis paper, we introduce a multi-question generation model (mQG), which is\ncapable of generating multiple, diverse, and answerable questions by focusing\non context and questions. To validate the answerability of the generated\nquestions, we employ a SQuAD2.0 fine-tuned question answering model,\nclassifying the questions as answerable or not. We train and evaluate mQG on\nthe FairytaleQA dataset, a well-structured QA dataset based on storybooks, with\nnarrative questions. We further apply a zero-shot adaptation on the TellMeWhy\nand SQuAD1.1 datasets. mQG shows promising results across various evaluation\nmetrics, among strong baselines.",
            "author": [
                "Hokeun Yoon",
                "JinYeong Bak"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16446v1",
                "http://arxiv.org/pdf/2310.16446v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16443v1",
            "title": "Platonism, De Re, and (Philosophy of) Mathematical Practice",
            "updated": "2023-10-25T08:09:18Z",
            "published": "2023-10-25T08:09:18Z",
            "summary": "The chapter advances a reformulation of the classical problem of the nature\nof mathematical objects (if any), here called \"Plato's problem,\" in line with\nthe program of a philosophy of mathematical practice. It then provides a sketch\nof a platonist solution, following the same perspective. This solution\ndisregards as nonsensical the question of the existence of abstract, and\nspecifically mathematical, objects, by rather focusing on the modalities of our\naccess to them: objects (in general, both concrete and abstract) are regarded\nas individual contents that we have (or can have) a de re epistemic access to.\nThe question of the existence of mathematical objects is then replaced by that\nof the modalities of our de re epistemic access to individual mathematical\ncontents.",
            "author": [
                "Marco Panza"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16443v1",
                "http://arxiv.org/pdf/2310.16443v1"
            ],
            "primary_category": "math.HO",
            "category": [
                "math.HO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16442v1",
            "title": "Right preconditioned GMRES for arbitrary singular systems",
            "updated": "2023-10-25T08:08:51Z",
            "published": "2023-10-25T08:08:51Z",
            "summary": "Brown and Walker (1997) showed that GMRES determines a least squares solution\nof $ A x = b $ where $ A \\in {\\bf R}^{n \\times n} $ without breakdown for\narbitrary $ b, x_0 \\in {\\bf R}^n $ if and only if $A$ is range-symmetric, i.e.\n$ {\\cal R} (A^{\\rm T}) = {\\cal R} (A) $, where $ A $ may be singular and $ b $\nmay not be in the range space ${\\cal R} A)$ of $A$. In this paper, we propose\napplying GMRES to $ A C A^{\\rm T} z = b $, where $ C \\in {\\bf R}^{n \\times n} $\nis symmetric positive definite. This determines a least squares solution $ x =\nCA^{\\rm T} z $ of $ A x = b $ without breakdown for arbitrary (singular) matrix\n$A \\in {\\bf R}^{n \\times n}$ and $ b \\in {\\bf R}^n $. To make the method\nnumerically stable, we propose using the pseudoinverse with an appropriate\nthreshold parameter to suppress the influence of tiny singular values when\nsolving the severely ill-conditioned Hessenberg systems which arise in the\nArnoldi process of GMRES when solving inconsistent range-symmetric systems.\nNumerical experiments show that the method taking $C$ to be the identity matrix\ngives the least squares solution even when $A$ is not range-symmetric,\nincluding the case when $ {\\rm index}(A) >1$.",
            "author": [
                "Kota Sugihara",
                "Ken Hayami"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16442v1",
                "http://arxiv.org/pdf/2310.16442v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "65F08, 65F10, 15A06, 15A09",
                "G.1.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16438v1",
            "title": "Baryons as Vortexes on the $\u03b7^{\\prime}$ Domain Wall",
            "updated": "2023-10-25T08:04:11Z",
            "published": "2023-10-25T08:04:11Z",
            "summary": "We show that the recent construction of $N_f=1$ baryons on the $\\eta^\\prime$\ndomain wall can be understood as vortexes of the principal effective theory --\nthe Chern-Simons-Higgs theory -- on a 2+1-dimensional sheet. This theory has a\nseries of vertex solutions, and the vortex with unit topological charge\nnaturally spins $N_c/2$, which coincides with the spin of the one-flavor baryon\nin QCD. Since the $N_c$ scaling of the vortexes is the same as that of baryons,\nbaryons can be regarded as vortexes. By virtue of the particle-vortex symmetry,\nthe dual Zhang-Hansson-Kivelson theory indicates that the quark carries\ntopological charge $1/N_c$ and obeys fractional statistics. The generalization\nto arbitrary $N_f$ is also discussed.",
            "author": [
                "Fan Lin",
                "Yong-Liang Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16438v1",
                "http://arxiv.org/pdf/2310.16438v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "hep-ph",
                "nucl-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16436v2",
            "title": "DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning\n  in Language Models",
            "updated": "2023-10-26T04:16:52Z",
            "published": "2023-10-25T08:03:10Z",
            "summary": "A long-standing goal of AI systems is to perform complex multimodal reasoning\nlike humans. Recently, large language models (LLMs) have made remarkable\nstrides in such multi-step reasoning on the language modality solely by\nleveraging the chain of thought (CoT) to mimic human thinking. However, the\ntransfer of these advancements to multimodal contexts introduces heightened\nchallenges, including but not limited to the impractical need for\nlabor-intensive annotation and the limitations in terms of flexibility,\ngeneralizability, and explainability. To evoke CoT reasoning in multimodality,\nthis work first conducts an in-depth analysis of these challenges posed by\nmultimodality and presents two key insights: \"keeping critical thinking\" and\n\"letting everyone do their jobs\" in multimodal CoT reasoning. Furthermore, this\nstudy proposes a novel DDCoT prompting that maintains a critical attitude\nthrough negative-space prompting and incorporates multimodality into reasoning\nby first dividing the reasoning responsibility of LLMs into reasoning and\nrecognition and then integrating the visual recognition capability of visual\nmodels into the joint reasoning process. The rationales generated by DDCoT not\nonly improve the reasoning abilities of both large and small language models in\nzero-shot prompting and fine-tuning learning, significantly outperforming\nstate-of-the-art methods but also exhibit impressive generalizability and\nexplainability.",
            "author": [
                "Ge Zheng",
                "Bin Yang",
                "Jiajin Tang",
                "Hong-Yu Zhou",
                "Sibei Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16436v2",
                "http://arxiv.org/pdf/2310.16436v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16427v2",
            "title": "PromptAgent: Strategic Planning with Language Models Enables\n  Expert-level Prompt Optimization",
            "updated": "2023-12-07T14:39:22Z",
            "published": "2023-10-25T07:47:01Z",
            "summary": "Highly effective, task-specific prompts are often heavily engineered by\nexperts to integrate detailed instructions and domain insights based on a deep\nunderstanding of both instincts of large language models (LLMs) and the\nintricacies of the target task. However, automating the generation of such\nexpert-level prompts remains elusive. Existing prompt optimization methods tend\nto overlook the depth of domain knowledge and struggle to efficiently explore\nthe vast space of expert-level prompts. Addressing this, we present\nPromptAgent, an optimization method that autonomously crafts prompts equivalent\nin quality to those handcrafted by experts. At its core, PromptAgent views\nprompt optimization as a strategic planning problem and employs a principled\nplanning algorithm, rooted in Monte Carlo tree search, to strategically\nnavigate the expert-level prompt space. Inspired by human-like trial-and-error\nexploration, PromptAgent induces precise expert-level insights and in-depth\ninstructions by reflecting on model errors and generating constructive error\nfeedback. Such a novel framework allows the agent to iteratively examine\nintermediate prompts (states), refine them based on error feedbacks (actions),\nsimulate future rewards, and search for high-reward paths leading to expert\nprompts. We apply PromptAgent to 12 tasks spanning three practical domains:\nBIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing\nit significantly outperforms strong Chain-of-Thought and recent prompt\noptimization baselines. Extensive analyses emphasize its capability to craft\nexpert-level, detailed, and domain-insightful prompts with great efficiency and\ngeneralizability.",
            "author": [
                "Xinyuan Wang",
                "Chenxi Li",
                "Zhen Wang",
                "Fan Bai",
                "Haotian Luo",
                "Jiayou Zhang",
                "Nebojsa Jojic",
                "Eric P. Xing",
                "Zhiting Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16427v2",
                "http://arxiv.org/pdf/2310.16427v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16424v1",
            "title": "Pre-electoral coalition agreement from the Black-Scholes point of view",
            "updated": "2023-10-25T07:30:44Z",
            "published": "2023-10-25T07:30:44Z",
            "summary": "A political party can be considered as a company whose value depends on the\nvoters support i.e. on the percentage of population supporting the party.\nDynamics of the support is thus as a stochastic process with a deterministic\ngrowth rate perturbed by a white noise modeled through the Wiener process. This\nis in an analogy with the option modeling where the stock price behaves\nsimilarly as the voters' support. While in the option theory we have the\nquestion of fair price of an option, the question that we ask here is what is a\nreasonable level of support that the coalition of a major party (safely above\nthe election threshold) and a minor party (under or around the election\nthreshold) should achieve in order the minor party to get one more\nrepresentative. We shall elaborate some of the conclusions in the case of\nrecent elections in Montenegro (June, 2023) which are particularly interesting\ndue to lots of political subjects entering the race.",
            "author": [
                "Darko Mitrovic"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16424v1",
                "http://arxiv.org/pdf/2310.16424v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "physics.soc-ph",
                "q-fin.GN"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16423v1",
            "title": "From port-based teleportation to Frobenius reciprocity theorem:\n  partially reduced irreducible representations and their applications",
            "updated": "2023-10-25T07:22:54Z",
            "published": "2023-10-25T07:22:54Z",
            "summary": "In this paper, we present a connection of two concepts as induced\nrepresentation and partially reduced irreducible representations (PRIR) appear\nin the context of port-based teleportation protocols. Namely, for a given\nfinite group $G$ with arbitrary subgroup $H$, we consider a particular case of\nmatrix irreducible representations, whose restriction to the subgroup $H$, as a\nmatrix representation of $H$, is completely reduced to diagonal block form with\nan irreducible representation of $H$ in the blocks. The basic properties of\nsuch representations are given. Then as an application of this concept, we show\nthat the spectrum of the port-based teleportation operator is connected in a\nvery simple way with the spectrum of the corresponding Jucys-Murphy operator\nfor symmetric groups $S(m-1)\\subset S(m)$ - basic objects from the point of\nview of representation theory of the symmetric group. This shows a deep\nconnection between the central object describing properties of deterministic\nPBT schemes and objects appearing naturally in the abstract representation\ntheory of the symmetric group. As an additional but not trivial result, we give\nalso purely matrix proof of the Frobenius reciprocity theorem for characters.",
            "author": [
                "Marek Mozrzymas",
                "Micha\u0142 Horodecki",
                "Micha\u0142 Studzi\u0144ski"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16423v1",
                "http://arxiv.org/pdf/2310.16423v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16421v1",
            "title": "Graph Agent: Explicit Reasoning Agent for Graphs",
            "updated": "2023-10-25T07:20:16Z",
            "published": "2023-10-25T07:20:16Z",
            "summary": "Graph embedding methods such as Graph Neural Networks (GNNs) and Graph\nTransformers have contributed to the development of graph reasoning algorithms\nfor various tasks on knowledge graphs. However, the lack of interpretability\nand explainability of graph embedding methods has limited their applicability\nin scenarios requiring explicit reasoning. In this paper, we introduce the\nGraph Agent (GA), an intelligent agent methodology of leveraging large language\nmodels (LLMs), inductive-deductive reasoning modules, and long-term memory for\nknowledge graph reasoning tasks. GA integrates aspects of symbolic reasoning\nand existing graph embedding methods to provide an innovative approach for\ncomplex graph reasoning tasks. By converting graph structures into textual\ndata, GA enables LLMs to process, reason, and provide predictions alongside\nhuman-interpretable explanations. The effectiveness of the GA was evaluated on\nnode classification and link prediction tasks. Results showed that GA reached\nstate-of-the-art performance, demonstrating accuracy of 90.65%, 95.48%, and\n89.32% on Cora, PubMed, and PrimeKG datasets, respectively. Compared to\nexisting GNN and transformer models, GA offered advantages of explicit\nreasoning ability, free-of-training, easy adaption to various graph reasoning\ntasks",
            "author": [
                "Qinyong Wang",
                "Zhenxiang Gao",
                "Rong Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16421v1",
                "http://arxiv.org/pdf/2310.16421v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16420v1",
            "title": "Linear statistics for Coulomb gases: higher order cumulants",
            "updated": "2023-10-25T07:19:23Z",
            "published": "2023-10-25T07:19:23Z",
            "summary": "We consider $N$ classical particles interacting via the Coulomb potential in\nspatial dimension $d$ and in the presence of an external trap, at equilibrium\nat inverse temperature $\\beta$. In the large $N$ limit, the particles are\nconfined within a droplet of finite size. We study smooth linear statistics,\ni.e. the fluctuations of sums of the form ${\\cal L}_N = \\sum_{i=1}^N f({\\bf\nx}_i)$, where ${\\bf x}_i$'s are the positions of the particles and where\n$f({\\bf x}_i)$ is a sufficiently regular function. There exists at present\nstandard results for the first and second moments of ${\\cal L}_N$ in the large\n$N$ limit, as well as associated Central Limit Theorems in general dimension\nand for a wide class of confining potentials. Here we obtain explicit\nexpressions for the higher order cumulants of ${\\cal L}_N$ at large $N$, when\nthe function $f({\\bf x})=f(|{\\bf x}|)$ and the confining potential are both\nrotationnally invariant. A remarkable feature of our results is that these\nhigher cumulants depend only on the value of $f'(|{\\bf x}|)$ and its higher\norder derivatives evaluated exactly at the boundary of the droplet, which in\nthis case is a $d$-dimensional sphere. In the particular two-dimensional case\n$d=2$ at the special value $\\beta=2$, a connection to the Ginibre ensemble\nallows us to derive these results in an alternative way using the tools of\ndeterminantal point processes. Finally we also obtain the large deviation form\nof the full probability distribution function of ${\\cal L}_N$.",
            "author": [
                "Benjamin De Bruyne",
                "Pierre Le Doussal",
                "Satya N. Majumdar",
                "Gregory Schehr"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16420v1",
                "http://arxiv.org/pdf/2310.16420v1"
            ],
            "primary_category": "math-ph",
            "category": [
                "math-ph",
                "cond-mat.stat-mech",
                "math.MP",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16419v1",
            "title": "Open Knowledge Base Canonicalization with Multi-task Unlearning",
            "updated": "2023-10-25T07:13:06Z",
            "published": "2023-10-25T07:13:06Z",
            "summary": "The construction of large open knowledge bases (OKBs) is integral to many\napplications in the field of mobile computing. Noun phrases and relational\nphrases in OKBs often suffer from redundancy and ambiguity, which calls for the\ninvestigation on OKB canonicalization. However, in order to meet the\nrequirements of some privacy protection regulations and to ensure the\ntimeliness of the data, the canonicalized OKB often needs to remove some\nsensitive information or outdated data. The machine unlearning in OKB\ncanonicalization is an excellent solution to the above problem. Current\nsolutions address OKB canonicalization by devising advanced clustering\nalgorithms and using knowledge graph embedding (KGE) to further facilitate the\ncanonicalization process. Effective schemes are urgently needed to fully\nsynergise machine unlearning with clustering and KGE learning. To this end, we\nput forward a multi-task unlearning framework, namely MulCanon, to tackle\nmachine unlearning problem in OKB canonicalization. Specifically, the noise\ncharacteristics in the diffusion model are utilized to achieve the effect of\nmachine unlearning for data in OKB. MulCanon unifies the learning objectives of\ndiffusion model, KGE and clustering algorithms, and adopts a two-step\nmulti-task learning paradigm for training. A thorough experimental study on\npopular OKB canonicalization datasets validates that MulCanon achieves advanced\nmachine unlearning effects.",
            "author": [
                "Bingchen Liu",
                "Shihao Hou",
                "Weixin Zeng",
                "Xiang Zhao",
                "Shijun Liu",
                "Li Pan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16419v1",
                "http://arxiv.org/pdf/2310.16419v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16417v1",
            "title": "Enhanced Simultaneous Machine Translation with Word-level Policies",
            "updated": "2023-10-25T07:10:42Z",
            "published": "2023-10-25T07:10:42Z",
            "summary": "Recent years have seen remarkable advances in the field of Simultaneous\nMachine Translation (SiMT) due to the introduction of innovative policies that\ndictate whether to READ or WRITE at each step of the translation process.\nHowever, a common assumption in many existing studies is that operations are\ncarried out at the subword level, even though the standard unit for input and\noutput in most practical scenarios is typically at the word level. This paper\ndemonstrates that policies devised and validated at the subword level are\nsurpassed by those operating at the word level, which process multiple subwords\nto form a complete word in a single step. Additionally, we suggest a method to\nboost SiMT models using language models (LMs), wherein the proposed word-level\npolicy plays a vital role in addressing the subword disparity between LMs and\nSiMT models. Code is available at https://github.com/xl8-ai/WordSiMT.",
            "author": [
                "Kang Kim",
                "Hankyu Cho"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16417v1",
                "http://arxiv.org/pdf/2310.16417v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16412v1",
            "title": "FlatMatch: Bridging Labeled Data and Unlabeled Data with Cross-Sharpness\n  for Semi-Supervised Learning",
            "updated": "2023-10-25T06:57:59Z",
            "published": "2023-10-25T06:57:59Z",
            "summary": "Semi-Supervised Learning (SSL) has been an effective way to leverage abundant\nunlabeled data with extremely scarce labeled data. However, most SSL methods\nare commonly based on instance-wise consistency between different data\ntransformations. Therefore, the label guidance on labeled data is hard to be\npropagated to unlabeled data. Consequently, the learning process on labeled\ndata is much faster than on unlabeled data which is likely to fall into a local\nminima that does not favor unlabeled data, leading to sub-optimal\ngeneralization performance. In this paper, we propose FlatMatch which minimizes\na cross-sharpness measure to ensure consistent learning performance between the\ntwo datasets. Specifically, we increase the empirical risk on labeled data to\nobtain a worst-case model which is a failure case that needs to be enhanced.\nThen, by leveraging the richness of unlabeled data, we penalize the prediction\ndifference (i.e., cross-sharpness) between the worst-case model and the\noriginal model so that the learning direction is beneficial to generalization\non unlabeled data. Therefore, we can calibrate the learning process without\nbeing limited to insufficient label information. As a result, the mismatched\nlearning performance can be mitigated, further enabling the effective\nexploitation of unlabeled data and improving SSL performance. Through\ncomprehensive validation, we show FlatMatch achieves state-of-the-art results\nin many SSL settings.",
            "author": [
                "Zhuo Huang",
                "Li Shen",
                "Jun Yu",
                "Bo Han",
                "Tongliang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16412v1",
                "http://arxiv.org/pdf/2310.16412v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "Machine Learning"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16411v1",
            "title": "Decoding Stumpers: Large Language Models vs. Human Problem-Solvers",
            "updated": "2023-10-25T06:54:39Z",
            "published": "2023-10-25T06:54:39Z",
            "summary": "This paper investigates the problem-solving capabilities of Large Language\nModels (LLMs) by evaluating their performance on stumpers, unique single-step\nintuition problems that pose challenges for human solvers but are easily\nverifiable. We compare the performance of four state-of-the-art LLMs\n(Davinci-2, Davinci-3, GPT-3.5-Turbo, GPT-4) to human participants. Our\nfindings reveal that the new-generation LLMs excel in solving stumpers and\nsurpass human performance. However, humans exhibit superior skills in verifying\nsolutions to the same problems. This research enhances our understanding of\nLLMs' cognitive abilities and provides insights for enhancing their\nproblem-solving potential across various domains.",
            "author": [
                "Alon Goldstein",
                "Miriam Havin",
                "Roi Reichart",
                "Ariel Goldstein"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16411v1",
                "http://arxiv.org/pdf/2310.16411v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16409v1",
            "title": "Multiple Key-value Strategy in Recommendation Systems Incorporating\n  Large Language Model",
            "updated": "2023-10-25T06:49:19Z",
            "published": "2023-10-25T06:49:19Z",
            "summary": "Recommendation system (RS) plays significant roles in matching users\ninformation needs for Internet applications, and it usually utilizes the\nvanilla neural network as the backbone to handle embedding details. Recently,\nthe large language model (LLM) has exhibited emergent abilities and achieved\ngreat breakthroughs both in the CV and NLP communities. Thus, it is logical to\nincorporate RS with LLM better, which has become an emerging research\ndirection. Although some existing works have made their contributions to this\nissue, they mainly consider the single key situation (e.g. historical\ninteractions), especially in sequential recommendation. The situation of\nmultiple key-value data is simply neglected. This significant scenario is\nmainstream in real practical applications, where the information of users (e.g.\nage, occupation, etc) and items (e.g. title, category, etc) has more than one\nkey. Therefore, we aim to implement sequential recommendations based on\nmultiple key-value data by incorporating RS with LLM. In particular, we\ninstruct tuning a prevalent open-source LLM (Llama 7B) in order to inject\ndomain knowledge of RS into the pre-trained LLM. Since we adopt multiple\nkey-value strategies, LLM is hard to learn well among these keys. Thus the\ngeneral and innovative shuffle and mask strategies, as an innovative manner of\ndata argument, are designed. To demonstrate the effectiveness of our approach,\nextensive experiments are conducted on the popular and suitable dataset\nMovieLens which contains multiple keys-value. The experimental results\ndemonstrate that our approach can nicely and effectively complete this\nchallenging issue.",
            "author": [
                "Dui Wang",
                "Xiangyu Hou",
                "Xiaohui Yang",
                "Bo Zhang",
                "Renbing Chen",
                "Daiyue Xue"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16409v1",
                "http://arxiv.org/pdf/2310.16409v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16406v1",
            "title": "Challenges of Radio Frequency Fingerprinting: From Data Collection to\n  Deployment",
            "updated": "2023-10-25T06:45:49Z",
            "published": "2023-10-25T06:45:49Z",
            "summary": "Radio Frequency Fingerprinting (RFF) techniques promise to authenticate\nwireless devices at the physical layer based on inherent hardware imperfections\nintroduced during manufacturing. Such RF transmitter imperfections are\nreflected into over-the-air signals, allowing receivers to accurately identify\nthe RF transmitting source. Recent advances in Machine Learning, particularly\nin Deep Learning (DL), have improved the ability of RFF systems to extract and\nlearn complex features that make up the device-specific fingerprint. However,\nintegrating DL techniques with RFF and operating the system in real-world\nscenarios presents numerous challenges. This article identifies and analyzes\nthese challenges while considering the three reference phases of any DL-based\nRFF system: (i) data collection and preprocessing, (ii) training, and finally,\n(iii) deployment. Our investigation points out the current open problems that\nprevent real deployment of RFF while discussing promising future directions,\nthus paving the way for further research in the area.",
            "author": [
                "Saeif Alhazbi",
                "Ahmed Hussain",
                "Savio Sciancalepore",
                "Gabriele Oligeri",
                "Panos Papadimitratos"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16406v1",
                "http://arxiv.org/pdf/2310.16406v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16405v1",
            "title": "Binary State Recognition by Robots using Visual Question Answering of\n  Pre-Trained Vision-Language Model",
            "updated": "2023-10-25T06:44:22Z",
            "published": "2023-10-25T06:44:22Z",
            "summary": "Recognition of the current state is indispensable for the operation of a\nrobot. There are various states to be recognized, such as whether an elevator\ndoor is open or closed, whether an object has been grasped correctly, and\nwhether the TV is turned on or off. Until now, these states have been\nrecognized by programmatically describing the state of a point cloud or raw\nimage, by annotating and learning images, by using special sensors, etc. In\ncontrast to these methods, we apply Visual Question Answering (VQA) from a\nPre-Trained Vision-Language Model (PTVLM) trained on a large-scale dataset, to\nsuch binary state recognition. This idea allows us to intuitively describe\nstate recognition in language without any re-training, thereby improving the\nrecognition ability of robots in a simple and general way. We summarize various\ntechniques in questioning methods and image processing, and clarify their\nproperties through experiments.",
            "author": [
                "Kento Kawaharazuka",
                "Yoshiki Obinata",
                "Naoaki Kanazawa",
                "Kei Okada",
                "Masayuki Inaba"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16405v1",
                "http://arxiv.org/pdf/2310.16405v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16402v1",
            "title": "Video Referring Expression Comprehension via Transformer with\n  Content-conditioned Query",
            "updated": "2023-10-25T06:38:42Z",
            "published": "2023-10-25T06:38:42Z",
            "summary": "Video Referring Expression Comprehension (REC) aims to localize a target\nobject in videos based on the queried natural language. Recent improvements in\nvideo REC have been made using Transformer-based methods with learnable\nqueries. However, we contend that this naive query design is not ideal given\nthe open-world nature of video REC brought by text supervision. With numerous\npotential semantic categories, relying on only a few slow-updated queries is\ninsufficient to characterize them. Our solution to this problem is to create\ndynamic queries that are conditioned on both the input video and language to\nmodel the diverse objects referred to. Specifically, we place a fixed number of\nlearnable bounding boxes throughout the frame and use corresponding region\nfeatures to provide prior information. Also, we noticed that current query\nfeatures overlook the importance of cross-modal alignment. To address this, we\nalign specific phrases in the sentence with semantically relevant visual areas,\nannotating them in existing video datasets (VID-Sentence and VidSTG). By\nincorporating these two designs, our proposed model (called ConFormer)\noutperforms other models on widely benchmarked datasets. For example, in the\ntesting split of VID-Sentence dataset, ConFormer achieves 8.75% absolute\nimprovement on Accu.@0.6 compared to the previous state-of-the-art model.",
            "author": [
                "Ji Jiang",
                "Meng Cao",
                "Tengtao Song",
                "Long Chen",
                "Yi Wang",
                "Yuexian Zou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16402v1",
                "http://arxiv.org/pdf/2310.16402v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16400v1",
            "title": "Fuse Your Latents: Video Editing with Multi-source Latent Diffusion\n  Models",
            "updated": "2023-10-25T06:35:01Z",
            "published": "2023-10-25T06:35:01Z",
            "summary": "Latent Diffusion Models (LDMs) are renowned for their powerful capabilities\nin image and video synthesis. Yet, video editing methods suffer from\ninsufficient pre-training data or video-by-video re-training cost. In\naddressing this gap, we propose FLDM (Fused Latent Diffusion Model), a\ntraining-free framework to achieve text-guided video editing by applying\noff-the-shelf image editing methods in video LDMs. Specifically, FLDM fuses\nlatents from an image LDM and an video LDM during the denoising process. In\nthis way, temporal consistency can be kept with video LDM while high-fidelity\nfrom the image LDM can also be exploited. Meanwhile, FLDM possesses high\nflexibility since both image LDM and video LDM can be replaced so advanced\nimage editing methods such as InstructPix2Pix and ControlNet can be exploited.\nTo the best of our knowledge, FLDM is the first method to adapt off-the-shelf\nimage editing methods into video LDMs for video editing. Extensive quantitative\nand qualitative experiments demonstrate that FLDM can improve the textual\nalignment and temporal consistency of edited videos.",
            "author": [
                "Tianyi Lu",
                "Xing Zhang",
                "Jiaxi Gu",
                "Hang Xu",
                "Renjing Pei",
                "Songcen Xu",
                "Zuxuan Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16400v1",
                "http://arxiv.org/pdf/2310.16400v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16394v1",
            "title": "Quantum Thermodynamics and Hierarchy of Quantum Correlations and\n  Fidelity of Teleportation in a Two Coupled Double Quantum Dots",
            "updated": "2023-10-25T06:27:02Z",
            "published": "2023-10-25T06:27:02Z",
            "summary": "We explore the quantum correlations, fidelity and quantum thermodynamics of\ntwo coupled double quantum dots containing two excess electrons. In this\nregard, we investigate and compare the evolution of those measures under\nthermal effects and tunneling coupling. We find the hierarchy of quantum\ncorrelations, and one-way steering between the two quantum dots. We found, as\nexpect that the quantum correlations are diminishes by increasing the values of\ntemperature. We show that this state can be used for quantum teleportation. On\nthe other, we address the extracting work and efficiency of the state. We\ncompare the extraction work with the bare energies. Our results show that\nquantum dots states have a reliable and better capacity to preserve quantum\ncorrelations and remain one of the good resources for the deployment of quantum\ninformation processing protocols.",
            "author": [
                "Mohamed Amazioug",
                "Mohammed Daoud"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16394v1",
                "http://arxiv.org/pdf/2310.16394v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.02089v1",
            "title": "LlamaRec: Two-Stage Recommendation using Large Language Models for\n  Ranking",
            "updated": "2023-10-25T06:23:48Z",
            "published": "2023-10-25T06:23:48Z",
            "summary": "Recently, large language models (LLMs) have exhibited significant progress in\nlanguage understanding and generation. By leveraging textual features,\ncustomized LLMs are also applied for recommendation and demonstrate\nimprovements across diverse recommendation scenarios. Yet the majority of\nexisting methods perform training-free recommendation that heavily relies on\npretrained knowledge (e.g., movie recommendation). In addition, inference on\nLLMs is slow due to autoregressive generation, rendering existing methods less\neffective for real-time recommendation. As such, we propose a two-stage\nframework using large language models for ranking-based recommendation\n(LlamaRec). In particular, we use small-scale sequential recommenders to\nretrieve candidates based on the user interaction history. Then, both history\nand retrieved items are fed to the LLM in text via a carefully designed prompt\ntemplate. Instead of generating next-item titles, we adopt a verbalizer-based\napproach that transforms output logits into probability distributions over the\ncandidate items. Therefore, the proposed LlamaRec can efficiently rank items\nwithout generating long text. To validate the effectiveness of the proposed\nframework, we compare against state-of-the-art baseline methods on benchmark\ndatasets. Our experimental results demonstrate the performance of LlamaRec,\nwhich consistently achieves superior performance in both recommendation\nperformance and efficiency.",
            "author": [
                "Zhenrui Yue",
                "Sara Rabhi",
                "Gabriel de Souza Pereira Moreira",
                "Dong Wang",
                "Even Oldridge"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02089v1",
                "http://arxiv.org/pdf/2311.02089v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16393v1",
            "title": "ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source\n  Ensembling of Language Adapters",
            "updated": "2023-10-25T06:22:29Z",
            "published": "2023-10-25T06:22:29Z",
            "summary": "We tackle the problem of zero-shot cross-lingual transfer in NLP tasks via\nthe use of language adapters (LAs). Most of the earlier works have explored\ntraining with adapter of a single source (often English), and testing either\nusing the target LA or LA of another related language. Training target LA\nrequires unlabeled data, which may not be readily available for low resource\nunseen languages: those that are neither seen by the underlying multilingual\nlanguage model (e.g., mBERT), nor do we have any (labeled or unlabeled) data\nfor them. We posit that for more effective cross-lingual transfer, instead of\njust one source LA, we need to leverage LAs of multiple (linguistically or\ngeographically related) source languages, both at train and test-time - which\nwe investigate via our novel neural architecture, ZGUL. Extensive\nexperimentation across four language groups, covering 15 unseen target\nlanguages, demonstrates improvements of up to 3.2 average F1 points over\nstandard fine-tuning and other strong baselines on POS tagging and NER tasks.\nWe also extend ZGUL to settings where either (1) some unlabeled data or (2)\nfew-shot training examples are available for the target language. We find that\nZGUL continues to outperform baselines in these settings too.",
            "author": [
                "Vipul Rathore",
                "Rajdeep Dhingra",
                "Parag Singla",
                "Mausam"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16393v1",
                "http://arxiv.org/pdf/2310.16393v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16391v1",
            "title": "Winning Prize Comes from Losing Tickets: Improve Invariant Learning by\n  Exploring Variant Parameters for Out-of-Distribution Generalization",
            "updated": "2023-10-25T06:10:57Z",
            "published": "2023-10-25T06:10:57Z",
            "summary": "Out-of-Distribution (OOD) Generalization aims to learn robust models that\ngeneralize well to various environments without fitting to\ndistribution-specific features. Recent studies based on Lottery Ticket\nHypothesis (LTH) address this problem by minimizing the learning target to find\nsome of the parameters that are critical to the task. However, in OOD problems,\nsuch solutions are suboptimal as the learning task contains severe distribution\nnoises, which can mislead the optimization process. Therefore, apart from\nfinding the task-related parameters (i.e., invariant parameters), we propose\nExploring Variant parameters for Invariant Learning (EVIL) which also leverages\nthe distribution knowledge to find the parameters that are sensitive to\ndistribution shift (i.e., variant parameters). Once the variant parameters are\nleft out of invariant learning, a robust subnetwork that is resistant to\ndistribution shift can be found. Additionally, the parameters that are\nrelatively stable across distributions can be considered invariant ones to\nimprove invariant learning. By fully exploring both variant and invariant\nparameters, our EVIL can effectively identify a robust subnetwork to improve\nOOD generalization. In extensive experiments on integrated testbed: DomainBed,\nEVIL can effectively and efficiently enhance many popular methods, such as ERM,\nIRM, SAM, etc.",
            "author": [
                "Zhuo Huang",
                "Muyang Li",
                "Li Shen",
                "Jun Yu",
                "Chen Gong",
                "Bo Han",
                "Tongliang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16391v1",
                "http://arxiv.org/pdf/2310.16391v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "Computer Vision and Pattern Recognition"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16390v1",
            "title": "Evaluating Pre-trained Language Models for Repairing API Misuses",
            "updated": "2023-10-25T06:10:22Z",
            "published": "2023-10-25T06:10:22Z",
            "summary": "API misuses often lead to software bugs, crashes, and vulnerabilities. While\nseveral API misuse detectors have been proposed, there are no automatic repair\ntools specifically designed for this purpose. In a recent study,\ntest-suite-based automatic program repair (APR) tools were found to be\nineffective in repairing API misuses. Still, since the study focused on\nnon-learning-aided APR tools, it remains unknown whether learning-aided APR\ntools are capable of fixing API misuses. In recent years, pre-trained language\nmodels (PLMs) have succeeded greatly in many natural language processing tasks.\nThere is a rising interest in applying PLMs to APR. However, there has not been\nany study that investigates the effectiveness of PLMs in repairing API misuse.\n  To fill this gap, we conduct a comprehensive empirical study on 11\nlearning-aided APR tools, which include 9 of the state-of-the-art\ngeneral-purpose PLMs and two APR tools. We evaluate these models with an\nAPI-misuse repair dataset, consisting of two variants. Our results show that\nPLMs perform better than the studied APR tools in repairing API misuses. Among\nthe 9 pre-trained models tested, CodeT5 is the best performer in the exact\nmatch. We also offer insights and potential exploration directions for future\nresearch.",
            "author": [
                "Ting Zhang",
                "Ivana Clairine Irsan",
                "Ferdian Thung",
                "David Lo",
                "Asankhaya Sharma",
                "Lingxiao Jiang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16390v1",
                "http://arxiv.org/pdf/2310.16390v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16861v1",
            "title": "General Point Model with Autoencoding and Autoregressive",
            "updated": "2023-10-25T06:08:24Z",
            "published": "2023-10-25T06:08:24Z",
            "summary": "The pre-training architectures of large language models encompass various\ntypes, including autoencoding models, autoregressive models, and\nencoder-decoder models. We posit that any modality can potentially benefit from\na large language model, as long as it undergoes vector quantization to become\ndiscrete tokens. Inspired by GLM, we propose a General Point Model (GPM) which\nseamlessly integrates autoencoding and autoregressive tasks in point cloud\ntransformer. This model is versatile, allowing fine-tuning for downstream point\ncloud representation tasks, as well as unconditional and conditional generation\ntasks. GPM enhances masked prediction in autoencoding through various forms of\nmask padding tasks, leading to improved performance in point cloud\nunderstanding. Additionally, GPM demonstrates highly competitive results in\nunconditional point cloud generation tasks, even exhibiting the potential for\nconditional generation tasks by modifying the input's conditional information.\nCompared to models like Point-BERT, MaskPoint and PointMAE, our GPM achieves\nsuperior performance in point cloud understanding tasks. Furthermore, the\nintegration of autoregressive and autoencoding within the same transformer\nunderscores its versatility across different downstream tasks.",
            "author": [
                "Zhe Li",
                "Zhangyang Gao",
                "Cheng Tan",
                "Stan Z. Li",
                "Laurence T. Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16861v1",
                "http://arxiv.org/pdf/2310.16861v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16387v1",
            "title": "Frequency-Aware Transformer for Learned Image Compression",
            "updated": "2023-10-25T05:59:25Z",
            "published": "2023-10-25T05:59:25Z",
            "summary": "Learned image compression (LIC) has gained traction as an effective solution\nfor image storage and transmission in recent years. However, existing LIC\nmethods are redundant in latent representation due to limitations in capturing\nanisotropic frequency components and preserving directional details. To\novercome these challenges, we propose a novel frequency-aware transformer (FAT)\nblock that for the first time achieves multiscale directional ananlysis for\nLIC. The FAT block comprises frequency-decomposition window attention (FDWA)\nmodules to capture multiscale and directional frequency components of natural\nimages. Additionally, we introduce frequency-modulation feed-forward network\n(FMFFN) to adaptively modulate different frequency components, improving\nrate-distortion performance. Furthermore, we present a transformer-based\nchannel-wise autoregressive (T-CA) model that effectively exploits channel\ndependencies. Experiments show that our method achieves state-of-the-art\nrate-distortion performance compared to existing LIC methods, and evidently\noutperforms latest standardized codec VTM-12.1 by 14.5%, 15.1%, 13.0% in\nBD-rate on the Kodak, Tecnick, and CLIC datasets.",
            "author": [
                "Han Li",
                "Shaohui Li",
                "Wenrui Dai",
                "Chenglin Li",
                "Junni Zou",
                "Hongkai Xiong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16387v1",
                "http://arxiv.org/pdf/2310.16387v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16383v1",
            "title": "Open-NeRF: Towards Open Vocabulary NeRF Decomposition",
            "updated": "2023-10-25T05:43:14Z",
            "published": "2023-10-25T05:43:14Z",
            "summary": "In this paper, we address the challenge of decomposing Neural Radiance Fields\n(NeRF) into objects from an open vocabulary, a critical task for object\nmanipulation in 3D reconstruction and view synthesis. Current techniques for\nNeRF decomposition involve a trade-off between the flexibility of processing\nopen-vocabulary queries and the accuracy of 3D segmentation. We present,\nOpen-vocabulary Embedded Neural Radiance Fields (Open-NeRF), that leverage\nlarge-scale, off-the-shelf, segmentation models like the Segment Anything Model\n(SAM) and introduce an integrate-and-distill paradigm with hierarchical\nembeddings to achieve both the flexibility of open-vocabulary querying and 3D\nsegmentation accuracy. Open-NeRF first utilizes large-scale foundation models\nto generate hierarchical 2D mask proposals from varying viewpoints. These\nproposals are then aligned via tracking approaches and integrated within the 3D\nspace and subsequently distilled into the 3D field. This process ensures\nconsistent recognition and granularity of objects from different viewpoints,\neven in challenging scenarios involving occlusion and indistinct features. Our\nexperimental results show that the proposed Open-NeRF outperforms\nstate-of-the-art methods such as LERF \\cite{lerf} and FFD \\cite{ffd} in\nopen-vocabulary scenarios. Open-NeRF offers a promising solution to NeRF\ndecomposition, guided by open-vocabulary queries, enabling novel applications\nin robotics and vision-language interaction in open-world 3D scenes.",
            "author": [
                "Hao Zhang",
                "Fang Li",
                "Narendra Ahuja"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16383v1",
                "http://arxiv.org/pdf/2310.16383v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16378v1",
            "title": "Quantum interferometers: principles and applications",
            "updated": "2023-10-25T05:33:39Z",
            "published": "2023-10-25T05:33:39Z",
            "summary": "Interference, which refers to the phenomenon associated with the\nsuperposition of waves, has played a crucial role in the advancement of physics\nand finds a wide range of applications in physical and engineering\nmeasurements. Interferometers are experimental setups designed to observe and\nmanipulate interference. With the development of technology, many quantum\ninterferometers have been discovered and have become cornerstone tools in the\nfield of quantum physics. Quantum interferometers not only explore the nature\nof the quantum world but also have extensive applications in quantum\ninformation technology, such as quantum communication, quantum computing, and\nquantum measurement. In this review, we analyze and summarize three typical\nquantum interferometers: the Hong-Ou-Mandel (HOM) interferometer, the N00N\nstate interferometer, and the Franson interferometer. We focus on the\nprinciples and applications of these three interferometers. In the principles\nsection, we present the theoretical models for these interferometers, including\nsingle-mode theory and multi-mode theory. In the applications section, we\nreview the applications of these interferometers in quantum communication,\ncomputation, and measurement. We hope that this review article will promote the\ndevelopment of quantum interference in both fundamental science and practical\nengineering applications.",
            "author": [
                "Rui-Bo Jin",
                "Zi-Qi Zeng",
                "Chenglong You",
                "Chenzhi Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16378v1",
                "http://arxiv.org/pdf/2310.16378v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16375v1",
            "title": "DyExplainer: Explainable Dynamic Graph Neural Networks",
            "updated": "2023-10-25T05:26:33Z",
            "published": "2023-10-25T05:26:33Z",
            "summary": "Graph Neural Networks (GNNs) resurge as a trending research subject owing to\ntheir impressive ability to capture representations from graph-structured data.\nHowever, the black-box nature of GNNs presents a significant challenge in terms\nof comprehending and trusting these models, thereby limiting their practical\napplications in mission-critical scenarios. Although there has been substantial\nprogress in the field of explaining GNNs in recent years, the majority of these\nstudies are centered on static graphs, leaving the explanation of dynamic GNNs\nlargely unexplored. Dynamic GNNs, with their ever-evolving graph structures,\npose a unique challenge and require additional efforts to effectively capture\ntemporal dependencies and structural relationships. To address this challenge,\nwe present DyExplainer, a novel approach to explaining dynamic GNNs on the fly.\nDyExplainer trains a dynamic GNN backbone to extract representations of the\ngraph at each snapshot, while simultaneously exploring structural relationships\nand temporal dependencies through a sparse attention technique. To preserve the\ndesired properties of the explanation, such as structural consistency and\ntemporal continuity, we augment our approach with contrastive learning\ntechniques to provide priori-guided regularization. To model longer-term\ntemporal dependencies, we develop a buffer-based live-updating scheme for\ntraining. The results of our extensive experiments on various datasets\ndemonstrate the superiority of DyExplainer, not only providing faithful\nexplainability of the model predictions but also significantly improving the\nmodel prediction accuracy, as evidenced in the link prediction task.",
            "author": [
                "Tianchun Wang",
                "Dongsheng Luo",
                "Wei Cheng",
                "Haifeng Chen",
                "Xiang Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16375v1",
                "http://arxiv.org/pdf/2310.16375v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16370v1",
            "title": "PartRePer-MPI: Combining Fault Tolerance and Performance for MPI\n  Applications",
            "updated": "2023-10-25T05:18:48Z",
            "published": "2023-10-25T05:18:48Z",
            "summary": "As we have entered Exascale computing, the faults in high-performance systems\nare expected to increase considerably. To compensate for a higher failure rate,\nthe standard checkpoint/restart technique would need to create checkpoints at a\nmuch higher frequency resulting in an excessive amount of overhead which would\nnot be sustainable for many scientific applications. Replication allows for\nfast recovery from failures by simply dropping the failed processes and using\ntheir replicas to continue the regular operation of the application.\n  In this paper, we have implemented PartRePer-MPI, a novel fault-tolerant MPI\nlibrary that adopts partial replication of some of the launched MPI processes\nin order to provide resilience from failures. The novelty of our work is that\nit combines both fault tolerance, due to the use of the User Level Failure\nMitigation (ULFM) framework in the Open MPI library, and high performance, due\nto the use of communication protocols in the native MPI library that is\ngenerally fine-tuned for specific HPC platforms. We have implemented efficient\nand parallel communication strategies with computational and replica processes,\nand our library can seamlessly provide fault tolerance support to an existing\nMPI application. Our experiments using seven NAS Parallel Benchmarks and two\nscientific applications show that the failure-free overheads in PartRePer-MPI\nwhen compared to the baseline MVAPICH2, are only up to 6.4% for the NAS\nparallel benchmarks and up to 9.7% for the scientific applications.",
            "author": [
                "Sarthak Joshi",
                "Sathish Vadhiyar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16370v1",
                "http://arxiv.org/pdf/2310.16370v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16368v1",
            "title": "Transformer-based Live Update Generation for Soccer Matches from\n  Microblog Posts",
            "updated": "2023-10-25T05:12:35Z",
            "published": "2023-10-25T05:12:35Z",
            "summary": "It has been known to be difficult to generate adequate sports updates from a\nsequence of vast amounts of diverse live tweets, although the live sports\nviewing experience with tweets is gaining the popularity. In this paper, we\nfocus on soccer matches and work on building a system to generate live updates\nfor soccer matches from tweets so that users can instantly grasp a match's\nprogress and enjoy the excitement of the match from raw tweets. Our proposed\nsystem is based on a large pre-trained language model and incorporates a\nmechanism to control the number of updates and a mechanism to reduce the\nredundancy of duplicate and similar updates.",
            "author": [
                "Masashi Oshika",
                "Kosuke Yamada",
                "Ryohei Sasano",
                "Koichi Takeda"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16368v1",
                "http://arxiv.org/pdf/2310.16368v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16367v1",
            "title": "UniX-Encoder: A Universal $X$-Channel Speech Encoder for Ad-Hoc\n  Microphone Array Speech Processing",
            "updated": "2023-10-25T05:12:05Z",
            "published": "2023-10-25T05:12:05Z",
            "summary": "The speech field is evolving to solve more challenging scenarios, such as\nmulti-channel recordings with multiple simultaneous talkers. Given the many\ntypes of microphone setups out there, we present the UniX-Encoder. It's a\nuniversal encoder designed for multiple tasks, and worked with any microphone\narray, in both solo and multi-talker environments. Our research enhances\nprevious multi-channel speech processing efforts in four key areas: 1)\nAdaptability: Contrasting traditional models constrained to certain microphone\narray configurations, our encoder is universally compatible. 2) Multi-Task\nCapability: Beyond the single-task focus of previous systems, UniX-Encoder acts\nas a robust upstream model, adeptly extracting features for diverse tasks\nincluding ASR and speaker recognition. 3) Self-Supervised Training: The encoder\nis trained without requiring labeled multi-channel data. 4) End-to-End\nIntegration: In contrast to models that first beamform then process\nsingle-channels, our encoder offers an end-to-end solution, bypassing explicit\nbeamforming or separation. To validate its effectiveness, we tested the\nUniX-Encoder on a synthetic multi-channel dataset from the LibriSpeech corpus.\nAcross tasks like speech recognition and speaker diarization, our encoder\nconsistently outperformed combinations like the WavLM model with the BeamformIt\nfrontend.",
            "author": [
                "Zili Huang",
                "Yiwen Shao",
                "Shi-Xiong Zhang",
                "Dong Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16367v1",
                "http://arxiv.org/pdf/2310.16367v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16363v1",
            "title": "Finite Time Analysis of Constrained Actor Critic and Constrained Natural\n  Actor Critic Algorithms",
            "updated": "2023-10-25T05:04:00Z",
            "published": "2023-10-25T05:04:00Z",
            "summary": "Actor Critic methods have found immense applications on a wide range of\nReinforcement Learning tasks especially when the state-action space is large.\nIn this paper, we consider actor critic and natural actor critic algorithms\nwith function approximation for constrained Markov decision processes (C-MDP)\ninvolving inequality constraints and carry out a non-asymptotic analysis for\nboth of these algorithms in a non-i.i.d (Markovian) setting. We consider the\nlong-run average cost criterion where both the objective and the constraint\nfunctions are suitable policy-dependent long-run averages of certain prescribed\ncost functions. We handle the inequality constraints using the Lagrange\nmultiplier method. We prove that these algorithms are guaranteed to find a\nfirst-order stationary point (i.e., $\\Vert \\nabla L(\\theta,\\gamma)\\Vert_2^2\n\\leq \\epsilon$) of the performance (Lagrange) function $L(\\theta,\\gamma)$, with\na sample complexity of $\\mathcal{\\tilde{O}}(\\epsilon^{-2.5})$ in the case of\nboth Constrained Actor Critic (C-AC) and Constrained Natural Actor Critic\n(C-NAC) algorithms.We also show the results of experiments on a few different\ngrid world settings and observe good empirical performance using both of these\nalgorithms. In particular, for large grid sizes, Constrained Natural Actor\nCritic shows slightly better results than Constrained Actor Critic while the\nlatter is slightly better for a small grid size.",
            "author": [
                "Prashansa Panda",
                "Shalabh Bhatnagar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16363v1",
                "http://arxiv.org/pdf/2310.16363v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16361v1",
            "title": "InstructPTS: Instruction-Tuning LLMs for Product Title Summarization",
            "updated": "2023-10-25T04:56:07Z",
            "published": "2023-10-25T04:56:07Z",
            "summary": "E-commerce product catalogs contain billions of items. Most products have\nlengthy titles, as sellers pack them with product attributes to improve\nretrieval, and highlight key product aspects. This results in a gap between\nsuch unnatural products titles, and how customers refer to them. It also limits\nhow e-commerce stores can use these seller-provided titles for recommendation,\nQA, or review summarization.\n  Inspired by recent work on instruction-tuned LLMs, we present InstructPTS, a\ncontrollable approach for the task of Product Title Summarization (PTS).\nTrained using a novel instruction fine-tuning strategy, our approach is able to\nsummarize product titles according to various criteria (e.g. number of words in\na summary, inclusion of specific phrases, etc.). Extensive evaluation on a\nreal-world e-commerce catalog shows that compared to simple fine-tuning of\nLLMs, our proposed approach can generate more accurate product name summaries,\nwith an improvement of over 14 and 8 BLEU and ROUGE points, respectively.",
            "author": [
                "Besnik Fetahu",
                "Zhiyu Chen",
                "Oleg Rokhlenko",
                "Shervin Malmasi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16361v1",
                "http://arxiv.org/pdf/2310.16361v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16358v1",
            "title": "From Simple to Complex: A Progressive Framework for Document-level\n  Informative Argument Extraction",
            "updated": "2023-10-25T04:38:02Z",
            "published": "2023-10-25T04:38:02Z",
            "summary": "Document-level Event Argument Extraction (EAE) requires the model to extract\narguments of multiple events from a single document. Considering the underlying\ndependencies between these events, recent efforts leverage the idea of\n\"memory\", where the results of already predicted events are cached and can be\nretrieved to help the prediction of upcoming events. These methods extract\nevents according to their appearance order in the document, however, the event\nthat appears in the first sentence does not mean that it is the easiest to\nextract. Existing methods might introduce noise to the extraction of upcoming\nevents if they rely on an incorrect prediction of previous events. In order to\nprovide more reliable memory, we propose a simple-to-complex progressive\nframework for document-level EAE. Specifically, we first calculate the\ndifficulty of each event and then, we conduct the extraction following a\nsimple-to-complex order. In this way, the memory will store the most certain\nresults, and the model could use these reliable sources to help the prediction\nof more difficult events. Experiments on WikiEvents show that our model\noutperforms SOTA by 1.4% in F1, indicating the proposed simple-to-complex\nframework is useful in the EAE task.",
            "author": [
                "Quzhe Huang",
                "Yanxi Zhang",
                "Dongyan Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16358v1",
                "http://arxiv.org/pdf/2310.16358v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16356v1",
            "title": "A Multi-Modal Multilingual Benchmark for Document Image Classification",
            "updated": "2023-10-25T04:35:06Z",
            "published": "2023-10-25T04:35:06Z",
            "summary": "Document image classification is different from plain-text document\nclassification and consists of classifying a document by understanding the\ncontent and structure of documents such as forms, emails, and other such\ndocuments. We show that the only existing dataset for this task (Lewis et al.,\n2006) has several limitations and we introduce two newly curated multilingual\ndatasets WIKI-DOC and MULTIEURLEX-DOC that overcome these limitations. We\nfurther undertake a comprehensive study of popular visually-rich document\nunderstanding or Document AI models in previously untested setting in document\nimage classification such as 1) multi-label classification, and 2) zero-shot\ncross-lingual transfer setup. Experimental results show limitations of\nmultilingual Document AI models on cross-lingual transfer across typologically\ndistant languages. Our datasets and findings open the door for future research\ninto improving Document AI models.",
            "author": [
                "Yoshinari Fujinuma",
                "Siddharth Varia",
                "Nishant Sankaran",
                "Srikar Appalaraju",
                "Bonan Min",
                "Yogarshi Vyas"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16356v1",
                "http://arxiv.org/pdf/2310.16356v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16355v1",
            "title": "Redco: A Lightweight Tool to Automate Distributed Training of LLMs on\n  Any GPU/TPUs",
            "updated": "2023-10-25T04:32:35Z",
            "published": "2023-10-25T04:32:35Z",
            "summary": "The recent progress of AI can be largely attributed to large language models\n(LLMs). However, their escalating memory requirements introduce challenges for\nmachine learning (ML) researchers and engineers. Addressing this requires\ndevelopers to partition a large model to distribute it across multiple GPUs or\nTPUs. This necessitates considerable coding and intricate configuration efforts\nwith existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa.\nThese tools require users' expertise in machine learning systems (MLSys),\ncreating a bottleneck in LLM development, particularly for developers without\nMLSys background. In this work, we present Redco, a lightweight and\nuser-friendly tool crafted to automate distributed training and inference for\nLLMs, as well as to simplify ML pipeline development. The design of Redco\nemphasizes two key aspects. Firstly, to automate model parallism, our study\nidentifies two straightforward rules to generate tensor parallel strategies for\nany given LLM. Integrating these rules into Redco facilitates effortless\ndistributed LLM training and inference, eliminating the need of additional\ncoding or complex configurations. We demonstrate the effectiveness by applying\nRedco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to\nthe size of 66B. Secondly, we propose a mechanism that allows for the\ncustomization of diverse ML pipelines through the definition of merely three\nfunctions, eliminating redundant and formulaic code like multi-host related\nprocessing. This mechanism proves adaptable across a spectrum of ML algorithms,\nfrom foundational language modeling to complex algorithms like meta-learning\nand reinforcement learning. Consequently, Redco implementations exhibit much\nfewer code lines compared to their official counterparts.",
            "author": [
                "Bowen Tan",
                "Yun Zhu",
                "Lijuan Liu",
                "Hongyi Wang",
                "Yonghao Zhuang",
                "Jindong Chen",
                "Eric Xing",
                "Zhiting Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16355v1",
                "http://arxiv.org/pdf/2310.16355v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16354v1",
            "title": "RAMPART: RowHammer Mitigation and Repair for Server Memory Systems",
            "updated": "2023-10-25T04:32:05Z",
            "published": "2023-10-25T04:32:05Z",
            "summary": "RowHammer attacks are a growing security and reliability concern for DRAMs\nand computer systems as they can induce many bit errors that overwhelm error\ndetection and correction capabilities. System-level solutions are needed as\nprocess technology and circuit improvements alone are unlikely to provide\ncomplete protection against RowHammer attacks in the future. This paper\nintroduces RAMPART, a novel approach to mitigating RowHammer attacks and\nimproving server memory system reliability by remapping addresses in each DRAM\nin a way that confines RowHammer bit flips to a single device for any victim\nrow address. When RAMPART is paired with Single Device Data Correction (SDDC)\nand patrol scrub, error detection and correction methods in use today, the\nsystem can detect and correct bit flips from a successful attack, allowing the\nmemory system to heal itself. RAMPART is compatible with DDR5 RowHammer\nmitigation features, as well as a wide variety of algorithmic and probabilistic\ntracking methods. We also introduce BRC-VL, a variation of DDR5 Bounded Refresh\nConfiguration (BRC) that improves system performance by reducing mitigation\noverhead and show that it works well with probabilistic sampling methods to\ncombat traditional and victim-focused mitigation attacks like Half-Double. The\ncombination of RAMPART, SDDC, and scrubbing enables stronger RowHammer\nresistance by correcting bit flips from one successful attack. Uncorrectable\nerrors are much less likely, requiring two successful attacks before the memory\nsystem is scrubbed.",
            "author": [
                "Steven C. Woo",
                "Wendy Elsasser",
                "Mike Hamburg",
                "Eric Linstadt",
                "Michael R. Miller",
                "Taeksang Song",
                "James Tringali"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16354v1",
                "http://arxiv.org/pdf/2310.16354v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR",
                "B.3.1; B.3.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16351v1",
            "title": "Fast Algorithms for Separable Linear Programs",
            "updated": "2023-10-25T04:28:45Z",
            "published": "2023-10-25T04:28:45Z",
            "summary": "In numerical linear algebra, considerable effort has been devoted to\nobtaining faster algorithms for linear systems whose underlying matrices\nexhibit structural properties. A prominent success story is the method of\ngeneralized nested dissection~[Lipton-Rose-Tarjan'79] for separable matrices.\nOn the other hand, the majority of recent developments in the design of\nefficient linear program (LP) solves do not leverage the ideas underlying these\nfaster linear system solvers nor consider the separable structure of the\nconstraint matrix.\n  We give a faster algorithm for separable linear programs. Specifically, we\nconsider LPs of the form $\\min_{\\mathbf{A}\\mathbf{x}=\\mathbf{b},\n\\mathbf{l}\\leq\\mathbf{x}\\leq\\mathbf{u}} \\mathbf{c}^\\top\\mathbf{x}$, where the\ngraphical support of the constraint matrix $\\mathbf{A} \\in \\mathbb{R}^{n\\times\nm}$ is $O(n^\\alpha)$-separable. These include flow problems on planar graphs\nand low treewidth matrices among others. We present an $\\tilde{O}((m+m^{1/2 +\n2\\alpha}) \\log(1/\\epsilon))$ time algorithm for these LPs, where $\\epsilon$ is\nthe relative accuracy of the solution.\n  Our new solver has two important implications: for the $k$-multicommodity\nflow problem on planar graphs, we obtain an algorithm running in\n$\\tilde{O}(k^{5/2} m^{3/2})$ time in the high accuracy regime; and when the\nsupport of $\\mathbf{A}$ is $O(n^\\alpha)$-separable with $\\alpha \\leq 1/4$, our\nalgorithm runs in $\\tilde{O}(m)$ time, which is nearly optimal. The latter\nsignificantly improves upon the natural approach of combining interior point\nmethods and nested dissection, whose time complexity is lower bounded by\n$\\Omega(\\sqrt{m}(m+m^{\\alpha\\omega}))=\\Omega(m^{3/2})$, where $\\omega$ is the\nmatrix multiplication constant. Lastly, in the setting of low-treewidth LPs, we\nrecover the results of [DLY,STOC21] and [GS,22] with significantly simpler data\nstructure machinery.",
            "author": [
                "Sally Dong",
                "Gramoz Goranci",
                "Lawrence Li",
                "Sushant Sachdeva",
                "Guanghao Ye"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16351v1",
                "http://arxiv.org/pdf/2310.16351v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16350v2",
            "title": "Unraveling Feature Extraction Mechanisms in Neural Networks",
            "updated": "2023-10-26T03:26:30Z",
            "published": "2023-10-25T04:22:40Z",
            "summary": "The underlying mechanism of neural networks in capturing precise knowledge\nhas been the subject of consistent research efforts. In this work, we propose a\ntheoretical approach based on Neural Tangent Kernels (NTKs) to investigate such\nmechanisms. Specifically, considering the infinite network width, we\nhypothesize the learning dynamics of target models may intuitively unravel the\nfeatures they acquire from training data, deepening our insights into their\ninternal mechanisms. We apply our approach to several fundamental models and\nreveal how these models leverage statistical features during gradient descent\nand how they are integrated into final decisions. We also discovered that the\nchoice of activation function can affect feature extraction. For instance, the\nuse of the \\textit{ReLU} activation function could potentially introduce a bias\nin features, providing a plausible explanation for its replacement with\nalternative functions in recent pre-trained language models. Additionally, we\nfind that while self-attention and CNN models may exhibit limitations in\nlearning n-grams, multiplication-based models seem to excel in this area. We\nverify these theoretical findings through experiments and find that they can be\napplied to analyze language modeling tasks, which can be regarded as a special\nvariant of classification. Our contributions offer insights into the roles and\ncapacities of fundamental components within large language models, thereby\naiding the broader understanding of these complex systems.",
            "author": [
                "Xiaobing Sun",
                "Jiaxi Li",
                "Wei Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16350v2",
                "http://arxiv.org/pdf/2310.16350v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16349v1",
            "title": "DiffRef3D: A Diffusion-based Proposal Refinement Framework for 3D Object\n  Detection",
            "updated": "2023-10-25T04:17:13Z",
            "published": "2023-10-25T04:17:13Z",
            "summary": "Denoising diffusion models show remarkable performances in generative tasks,\nand their potential applications in perception tasks are gaining interest. In\nthis paper, we introduce a novel framework named DiffRef3D which adopts the\ndiffusion process on 3D object detection with point clouds for the first time.\nSpecifically, we formulate the proposal refinement stage of two-stage 3D object\ndetectors as a conditional diffusion process. During training, DiffRef3D\ngradually adds noise to the residuals between proposals and target objects,\nthen applies the noisy residuals to proposals to generate hypotheses. The\nrefinement module utilizes these hypotheses to denoise the noisy residuals and\ngenerate accurate box predictions. In the inference phase, DiffRef3D generates\ninitial hypotheses by sampling noise from a Gaussian distribution as residuals\nand refines the hypotheses through iterative steps. DiffRef3D is a versatile\nproposal refinement framework that consistently improves the performance of\nexisting 3D object detection models. We demonstrate the significance of\nDiffRef3D through extensive experiments on the KITTI benchmark. Code will be\navailable.",
            "author": [
                "Se-Ho Kim",
                "Inyong Koo",
                "Inyoung Lee",
                "Byeongjun Park",
                "Changick Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16349v1",
                "http://arxiv.org/pdf/2310.16349v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16347v1",
            "title": "Transmitting Data Through Reconfigurable Intelligent Surface: A Spatial\n  Sigma-Delta Modulation Approach",
            "updated": "2023-10-25T04:05:15Z",
            "published": "2023-10-25T04:05:15Z",
            "summary": "Transmitting data using the phases on reconfigurable intelligent surfaces\n(RIS) is a promising solution for future energy-efficient communication\nsystems. Recent work showed that a virtual phased massive multiuser\nmultiple-input-multiple-out (MIMO) transmitter can be formed using only one\nactive antenna and a large passive RIS. In this paper, we are interested in\nusing such a system to perform MIMO downlink precoding. In this context, we may\nnot be able to apply conventional MIMO precoding schemes, such as the simple\nzero-forcing (ZF) scheme, and we typically need to design the phase signals by\nsolving optimization problems with constant modulus constraints or with\ndiscrete phase constraints, which pose challenges with high computational\ncomplexities. In this work, we propose an alternative approach based on\nSigma-Delta ($\\Sigma\\Delta$) modulation, which is classically famous for its\nnoise-shaping ability. Specifically, first-order $\\Sigma\\Delta$ modulation is\napplied in the spatial domain to handle phase quantization in generating\nconstant envelope signals. Under some mild assumptions, the proposed phased\n$\\Sigma\\Delta$ modulator allows us to use the ZF scheme to synthesize the RIS\nreflection phases with negligible complexity. The proposed approach is\nempirically shown to achieve comparable bit error rate performance to the\nunquantized ZF scheme.",
            "author": [
                "Wai-Yiu Keung",
                "Hei Victor Cheng",
                "Wing-Kin Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16347v1",
                "http://arxiv.org/pdf/2310.16347v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16343v1",
            "title": "A Comprehensive Evaluation of Constrained Text Generation for Large\n  Language Models",
            "updated": "2023-10-25T03:58:49Z",
            "published": "2023-10-25T03:58:49Z",
            "summary": "Advancements in natural language generation (NLG) and large language models\n(LLMs) have led to proficient text generation in various tasks. However,\nintegrating intricate constraints into neural text generation, due to LLMs'\nopacity, remains challenging. This study investigates constrained text\ngeneration for LLMs, where predefined constraints are applied during LLM's\ngeneration process. Our research examines multiple LLMs, including ChatGPT and\nGPT-4, categorizing constraints into lexical, structural, and relation-based\ntypes. We also present various benchmarks to facilitate fair evaluation. The\nstudy addresses some key research questions, including the extent of LLMs'\ncompliance with constraints. Results illuminate LLMs' capacity and deficiency\nto incorporate constraints and provide insights for future developments in\nconstrained text generation. Codes and datasets will be released upon\nacceptance.",
            "author": [
                "Xiang Chen",
                "Xiaojun Wan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16343v1",
                "http://arxiv.org/pdf/2310.16343v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16342v1",
            "title": "All-optical correlated noisy channel and its application in recovering\n  quantum coherence",
            "updated": "2023-10-25T03:56:11Z",
            "published": "2023-10-25T03:56:11Z",
            "summary": "Quantum coherence is a fundamental feature in quantum mechanics. It is\nfragile and will vanish in open quantum systems exposed to environmental noise.\nIn this paper, we propose an all-optical correlated noisy channel (ACNC)\nrelying on four-wave mixing processes that could mitigate the decoherence of\nquantum states caused by correlated noise. Two quantum systems are considered,\nincluding a coherent state and a two-mode squeezed state. In addition, we also\nanalyze the effect of losses on the performance of the ACNC. Different from the\ncorrelated noisy channel proposed in previous works using electro-optic\nconversions, the noisy channel in our protocol is all-optical, and thus our\nACNC owns larger operational bandwidth.",
            "author": [
                "Dan Lei",
                "Disheng Guo",
                "Jun Xin",
                "Xiao-Ming Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16342v1",
                "http://arxiv.org/pdf/2310.16342v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16341v1",
            "title": "Elevating Women in the Workplace: The Dual Influence of Spiritual\n  Intelligence and Ethical Environments on Job Satisfaction",
            "updated": "2023-10-25T03:54:02Z",
            "published": "2023-10-25T03:54:02Z",
            "summary": "In today's rapidly evolving workplace, the dynamics of job satisfaction and\nits determinants have become a focal point of organizational studies. This\nresearch offers a comprehensive examination of the nexus between spiritual\nintelligence and job satisfaction among female employees, with particular\nemphasis on the moderating role of ethical work environments. Beginning with an\nexploration of the multifaceted nature of human needs, the study delves deep\ninto the psychological underpinnings that drive job satisfaction. It elucidates\nhow various tangible and intangible motivators, such as salary benefits and\nrecognition, play pivotal roles in shaping employee attitudes and behaviors.\nMoreover, the research spotlights the unique challenges and experiences of\nfemale employees, advocating for a more inclusive understanding of their needs.\nAn extensive review of the literature and empirical analysis culminates in the\npivotal finding that integrating spiritual intelligence and ethical\nconsiderations within organizational practices can significantly enhance job\nsatisfaction. Such a holistic approach, the paper posits, not only bolsters the\nwell-being and contentment of female employees but also augments overall\norganizational productivity, retention rates, and morale.",
            "author": [
                "Ali Bai",
                "Morteza Vahedian",
                "Rashin Ghahreman",
                "Hasan Piri"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16341v1",
                "http://arxiv.org/pdf/2310.16341v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16340v1",
            "title": "RCAgent: Cloud Root Cause Analysis by Autonomous Agents with\n  Tool-Augmented Large Language Models",
            "updated": "2023-10-25T03:53:31Z",
            "published": "2023-10-25T03:53:31Z",
            "summary": "Large language model (LLM) applications in cloud root cause analysis (RCA)\nhave been actively explored recently. However, current methods are still\nreliant on manual workflow settings and do not unleash LLMs' decision-making\nand environment interaction capabilities. We present RCAgent, a tool-augmented\nLLM autonomous agent framework for practical and privacy-aware industrial RCA\nusage. Running on an internally deployed model rather than GPT families,\nRCAgent is capable of free-form data collection and comprehensive analysis with\ntools. Our framework combines a variety of enhancements, including a unique\nSelf-Consistency for action trajectories, and a suite of methods for context\nmanagement, stabilization, and importing domain knowledge. Our experiments show\nRCAgent's evident and consistent superiority over ReAct across all aspects of\nRCA -- predicting root causes, solutions, evidence, and responsibilities -- and\ntasks covered or uncovered by current rules, as validated by both automated\nmetrics and human evaluations. Furthermore, RCAgent has already been integrated\ninto the diagnosis and issue discovery workflow of the Real-time Compute\nPlatform for Apache Flink of Alibaba Cloud.",
            "author": [
                "Zefan Wang",
                "Zichuan Liu",
                "Yingying Zhang",
                "Aoxiao Zhong",
                "Lunting Fan",
                "Lingfei Wu",
                "Qingsong Wen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16340v1",
                "http://arxiv.org/pdf/2310.16340v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16338v1",
            "title": "Generative Pre-training for Speech with Flow Matching",
            "updated": "2023-10-25T03:40:50Z",
            "published": "2023-10-25T03:40:50Z",
            "summary": "Generative models have gained more and more attention in recent years for\ntheir remarkable success in tasks that required estimating and sampling data\ndistribution to generate high-fidelity synthetic data. In speech,\ntext-to-speech synthesis and neural vocoder are good examples where generative\nmodels have shined. While generative models have been applied to different\napplications in speech, there exists no general-purpose generative model that\nmodels speech directly. In this work, we take a step toward this direction by\nshowing a single pre-trained generative model can be adapted to different\ndownstream tasks with strong performance. Specifically, we pre-trained a\ngenerative model, named SpeechFlow, on 60k hours of untranscribed speech with\nFlow Matching and masked conditions. Experiment results show the pre-trained\ngenerative model can be fine-tuned with task-specific data to match or surpass\nexisting expert models on speech enhancement, separation, and synthesis. Our\nwork suggested a foundational model for generation tasks in speech can be built\nwith generative pre-training.",
            "author": [
                "Alexander H. Liu",
                "Matt Le",
                "Apoorv Vyas",
                "Bowen Shi",
                "Andros Tjandra",
                "Wei-Ning Hsu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16338v1",
                "http://arxiv.org/pdf/2310.16338v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.CL",
                "cs.LG",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16336v1",
            "title": "SMURF-THP: Score Matching-based UnceRtainty quantiFication for\n  Transformer Hawkes Process",
            "updated": "2023-10-25T03:33:45Z",
            "published": "2023-10-25T03:33:45Z",
            "summary": "Transformer Hawkes process models have shown to be successful in modeling\nevent sequence data. However, most of the existing training methods rely on\nmaximizing the likelihood of event sequences, which involves calculating some\nintractable integral. Moreover, the existing methods fail to provide\nuncertainty quantification for model predictions, e.g., confidence intervals\nfor the predicted event's arrival time. To address these issues, we propose\nSMURF-THP, a score-based method for learning Transformer Hawkes process and\nquantifying prediction uncertainty. Specifically, SMURF-THP learns the score\nfunction of events' arrival time based on a score-matching objective that\navoids the intractable computation. With such a learned score function, we can\nsample arrival time of events from the predictive distribution. This naturally\nallows for the quantification of uncertainty by computing confidence intervals\nover the generated samples. We conduct extensive experiments in both event type\nprediction and uncertainty quantification of arrival time. In all the\nexperiments, SMURF-THP outperforms existing likelihood-based methods in\nconfidence calibration while exhibiting comparable prediction accuracy.",
            "author": [
                "Zichong Li",
                "Yanbo Xu",
                "Simiao Zuo",
                "Haoming Jiang",
                "Chao Zhang",
                "Tuo Zhao",
                "Hongyuan Zha"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16336v1",
                "http://arxiv.org/pdf/2310.16336v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16334v1",
            "title": "AccoMontage-3: Full-Band Accompaniment Arrangement via Sequential Style\n  Transfer and Multi-Track Function Prior",
            "updated": "2023-10-25T03:30:37Z",
            "published": "2023-10-25T03:30:37Z",
            "summary": "We propose AccoMontage-3, a symbolic music automation system capable of\ngenerating multi-track, full-band accompaniment based on the input of a lead\nmelody with chords (i.e., a lead sheet). The system contains three modular\ncomponents, each modelling a vital aspect of full-band composition. The first\ncomponent is a piano arranger that generates piano accompaniment for the lead\nsheet by transferring texture styles to the chords using latent chord-texture\ndisentanglement and heuristic retrieval of texture donors. The second component\norchestrates the piano accompaniment score into full-band arrangement according\nto the orchestration style encoded by individual track functions. The third\ncomponent, which connects the previous two, is a prior model characterizing the\nglobal structure of orchestration style over the whole piece of music. From end\nto end, the system learns to generate full-band accompaniment in a\nself-supervised fashion, applying style transfer at two levels of polyphonic\ncomposition: texture and orchestration. Experiments show that our system\noutperforms the baselines significantly, and the modular design offers\neffective controls in a musically meaningful way.",
            "author": [
                "Jingwei Zhao",
                "Gus Xia",
                "Ye Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16334v1",
                "http://arxiv.org/pdf/2310.16334v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.AI",
                "cs.MM",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16331v1",
            "title": "Brain-Inspired Reservoir Computing Using Memristors with Tunable\n  Dynamics and Short-Term Plasticity",
            "updated": "2023-10-25T03:27:43Z",
            "published": "2023-10-25T03:27:43Z",
            "summary": "Recent advancements in reservoir computing research have created a demand for\nanalog devices with dynamics that can facilitate the physical implementation of\nreservoirs, promising faster information processing while consuming less energy\nand occupying a smaller area footprint. Studies have demonstrated that dynamic\nmemristors, with nonlinear and short-term memory dynamics, are excellent\ncandidates as information-processing devices or reservoirs for temporal\nclassification and prediction tasks. Previous implementations relied on\nnominally identical memristors that applied the same nonlinear transformation\nto the input data, which is not enough to achieve a rich state space. To\naddress this limitation, researchers either diversified the data encoding\nacross multiple memristors or harnessed the stochastic device-to-device\nvariability among the memristors. However, this approach requires additional\npre-processing steps and leads to synchronization issues. Instead, it is\npreferable to encode the data once and pass it through a reservoir layer\nconsisting of memristors with distinct dynamics. Here, we demonstrate that\nion-channel-based memristors with voltage-dependent dynamics can be\ncontrollably and predictively tuned through voltage or adjustment of the ion\nchannel concentration to exhibit diverse dynamic properties. We show, through\nexperiments and simulations, that reservoir layers constructed with a small\nnumber of distinct memristors exhibit significantly higher predictive and\nclassification accuracies with a single data encoding. We found that for a\nsecond-order nonlinear dynamical system prediction task, the varied memristor\nreservoir experimentally achieved a normalized mean square error of 0.0015\nusing only five distinct memristors. Moreover, in a neural activity\nclassification task, a reservoir of just three distinct memristors\nexperimentally attained an accuracy of 96.5%.",
            "author": [
                "Nicholas X. Armendarez",
                "Ahmed S. Mohamed",
                "Anurag Dhungel",
                "Md Razuan Hossain",
                "Md Sakib Hasan",
                "Joseph S. Najem"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16331v1",
                "http://arxiv.org/pdf/2310.16331v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16330v1",
            "title": "On the monodromy of holomorphic differential systems",
            "updated": "2023-10-25T03:24:48Z",
            "published": "2023-10-25T03:24:48Z",
            "summary": "First we survey and explain the strategy of some recent results that\nconstruct holomorphic $\\text{sl}(2, \\mathbb C)$-differential systems over some\nRiemann surfaces $\\Sigma_g$ of genus $g\\geq 2$, satisfying the condition that\nthe image of the associated monodromy homomorphism is (real) Fuchsian\n\\cite{BDHH} or some cocompact Kleinian subgroup $$\\Gamma \\subset \\text{SL}(2,\n\\mathbb C)$$ as in \\cite{BDHH2}. As a consequence, there exist holomorphic maps\nfrom $\\Sigma_g$ to the quotient space $\\text{SL}(2, \\mathbb C)/ \\Gamma$, where\n$\\Gamma \\subset \\text{SL}(2, \\mathbb C)$ is a cocompact lattice, that do not\nfactor through any elliptic curve \\cite{BDHH2}. This answers positively a\nquestion of Ghys in \\cite{Gh}; the question was also raised by Huckleberry and\nWinkelmann in \\cite{HW}.\n  Then we prove that when $M$ is a Riemann surface, a Torelli type theorem\nholds for the affine group scheme over $\\mathbb C$ obtained from the category\nof holomorphic connections on {\\it \\'etale trivial} holomorphic bundles.\n  After that, we explain how to compute in a simple way the holonomy of a\nholomorphic connection on a free vector bundle.\n  Finally, for a compact K\\\"ahler manifold $M$, we investigate the neutral\nTannakian category given by the holomorphic connections on \\'etale trivial\nholomorphic bundles over $M$. If $\\varpi$ (respectively, $\\Theta$) stands for\nthe affine group scheme over $\\mathbb C$ obtained from the category of\nconnections (respectively, connections on free (trivial) vector bundles), then\nthe natural inclusion produces a morphism $v:{\\mathcal\nO}(\\Theta)\\longrightarrow {\\mathcal O}(\\varpi)$ of Hopf algebras. We present a\ndescription of the transpose of $v$ in terms of the iterated integrals.",
            "author": [
                "Indranil Biswas",
                "Sorin Dumitrescu",
                "Lynn Heller",
                "Sebastian Heller",
                "Jo\u00e3o Pedro dos Santos"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16330v1",
                "http://arxiv.org/pdf/2310.16330v1"
            ],
            "primary_category": "math.DG",
            "category": [
                "math.DG",
                "math.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16329v1",
            "title": "CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment\n  of Coherence in Generated Texts",
            "updated": "2023-10-25T03:21:20Z",
            "published": "2023-10-25T03:21:20Z",
            "summary": "Coherence is a linguistic term that refers to the relations between small\ntextual units (sentences, propositions), which make the text logically\nconsistent and meaningful to the reader. With the advances of generative\nfoundational models in NLP, there is a pressing need to automatically assess\nthe human-perceived coherence of automatically generated texts. Up until now,\nlittle work has been done on explicitly assessing the coherence of generated\ntexts and analyzing the factors contributing to (in)coherence. Previous work on\nthe topic used other tasks, e.g., sentence reordering, as proxies of coherence,\nrather than approaching coherence detection heads on. In this paper, we\nintroduce {\\sc CoheSentia}, a novel benchmark of human-perceived coherence of\nautomatically generated texts. Our annotation protocol reflects two\nperspectives; one is global, assigning a single coherence score, and the other\nis incremental, scoring sentence by sentence. The incremental method produces\nan (in)coherence score for each text fragment and also pinpoints reasons for\nincoherence at that point. Our benchmark contains 500 automatically-generated\nand human-annotated paragraphs, each annotated in both methods, by multiple\nraters. Our analysis shows that the inter-annotator agreement in the\nincremental mode is higher than in the holistic alternative, and our\nexperiments show that standard LMs fine-tuned for coherence detection show\nvaried performance on the different factors contributing to (in)coherence. All\nin all, these models yield unsatisfactory performance, emphasizing the need for\ndeveloping more reliable methods for coherence assessment.",
            "author": [
                "Aviya Maimon",
                "Reut Tsarfaty"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16329v1",
                "http://arxiv.org/pdf/2310.16329v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16327v1",
            "title": "Covariance Blocking and Whitening Method for Successive Relative\n  Transfer Function Vector Estimation in Multi-Speaker Scenarios",
            "updated": "2023-10-25T03:19:08Z",
            "published": "2023-10-25T03:19:08Z",
            "summary": "This paper addresses the challenge of estimating the relative transfer\nfunction (RTF) vectors of multiple speakers in a noisy and reverberant\nenvironment. More specifically, we consider a scenario where two speakers\nactivate successively. In this scenario, the RTF vector of the first speaker\ncan be estimated in a straightforward way and the main challenge lies in\nestimating the RTF vector of the second speaker during segments where both\nspeakers are simultaneously active. To estimate the RTF vector of the second\nspeaker the so-called blind oblique projection (BOP) method determines the\noblique projection operator that optimally blocks the second speaker. Instead\nof blocking the second speaker, in this paper we propose a covariance blocking\nand whitening (CBW) method, which first blocks the first speaker and applies\nwhitening using the estimated noise covariance matrix and then estimates the\nRTF vector of the second speaker based on a singular value decomposition. When\nusing the estimated RTF vectors of both speakers in a linearly constrained\nminimum variance beamformer, simulation results using real-world recordings for\nmultiple speaker positions demonstrate that the proposed CBW method outperforms\nthe conventional BOP and covariance whitening methods in terms of\nsignal-to-interferer-and-noise ratio improvement.",
            "author": [
                "Henri Gode",
                "Simon Doclo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16327v1",
                "http://arxiv.org/pdf/2310.16327v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16324v1",
            "title": "Extracting Design Knowledge from Optimization Data: Enhancing\n  Engineering Design in Fluid Based Thermal Management Systems",
            "updated": "2023-10-25T03:13:52Z",
            "published": "2023-10-25T03:13:52Z",
            "summary": "As mechanical systems become more complex and technological advances\naccelerate, the traditional reliance on heritage designs for engineering\nendeavors is being diminished in its effectiveness. Considering the dynamic\nnature of the design industry where new challenges are continually emerging,\nalternative sources of knowledge need to be sought to guide future design\nefforts. One promising avenue lies in the analysis of design optimization data,\nwhich has the potential to offer valuable insights and overcome the limitations\nof heritage designs. This paper presents a step toward extracting knowledge\nfrom optimization data in multi-split fluid-based thermal management systems\nusing different classification machine learning methods, so that designers can\nuse it to guide decisions in future design efforts. This approach offers\nseveral advantages over traditional design heritage methods, including\napplicability in cases where there is no design heritage and the ability to\nderive optimal designs. We showcase our framework through four case studies\nwith varying levels of complexity. These studies demonstrate its effectiveness\nin enhancing the design of complex thermal management systems. Our results show\nthat the knowledge extracted from the configuration design optimization data\nprovides a good basis for more general design of complex thermal management\nsystems. It is shown that the objective value of the estimated optimal\nconfiguration closely approximates the true optimal configuration with less\nthan 1 percent error, achieved using basic features based on the system heat\nloads without involving the corresponding optimal open loop control (OLOC)\nfeatures. This eliminates the need to solve the OLOC problem, leading to\nreduced computation costs.",
            "author": [
                "Saeid Bayat",
                "Nastaran Shahmansouri",
                "Satya RT Peddada",
                "Alex Tessier",
                "Adrian Butscher",
                "James T Allison"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16324v1",
                "http://arxiv.org/pdf/2310.16324v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16322v1",
            "title": "Samsung R&D Institute Philippines at WMT 2023",
            "updated": "2023-10-25T03:10:52Z",
            "published": "2023-10-25T03:10:52Z",
            "summary": "In this paper, we describe the constrained MT systems submitted by Samsung\nR&D Institute Philippines to the WMT 2023 General Translation Task for two\ndirections: en$\\rightarrow$he and he$\\rightarrow$en. Our systems comprise of\nTransformer-based sequence-to-sequence models that are trained with a mix of\nbest practices: comprehensive data preprocessing pipelines, synthetic\nbacktranslated data, and the use of noisy channel reranking during online\ndecoding. Our models perform comparably to, and sometimes outperform, strong\nbaseline unconstrained systems such as mBART50 M2M and NLLB 200 MoE despite\nhaving significantly fewer parameters on two public benchmarks: FLORES-200 and\nNTREX-128.",
            "author": [
                "Jan Christian Blaise Cruz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16322v1",
                "http://arxiv.org/pdf/2310.16322v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16321v1",
            "title": "Neuromorphic cameras for Atmospheric Cherenkov Telescopes and fast\n  optical astronomy: new paradigm, challenges and opportunities",
            "updated": "2023-10-25T03:10:44Z",
            "published": "2023-10-25T03:10:44Z",
            "summary": "The astronomy community has witnessed an explosive growth in the use of\ndeep-learning techniques based on neural networks since the mid-2010s. The\nwidespread adoption of these nature-inspired technologies has helped\nastronomers tackle previously insurmountable problems and provided an\nunprecedented opportunity for new discoveries. However, one of the primary\ntools of today's optical astronomy is neither natural nor efficient: their\nphoto-sensing devices. Specifically, the modern CCD camera - like that of the\ncutting-edge Rubin Observatory - requires an internal clock to regularly expose\nthe sensor to light, consumes a large amount of energy and information\nbandwidth, and has a limited dynamic range. On the contrary, biological eyes\nlack an internal clock and a shutter, have much higher pixel density but\nconsume significantly less energy and bandwidth, and can adapt to bright and\nlow light conditions. Inspired by the nature of the eyes, M. Mahowald and C.\nMead introduced the revolutionary concept of a silicon retina sensor in 1991.\nAlso known as event-based cameras (EBCs), these types of devices operate in a\nvastly different way compared to conventional CCD-based imaging sensors. EBCs\nmimic the operating principles of optic nerves and continuously produce a\nstream of events, with each event generated only when a pixel detects a change\nin light intensity. EBCs do not have fixed exposure times, have high dynamic\nrange, require low power for operation, and can capture high-speed phenomena.\nThese properties are important requirements for Cherenkov telescopes as well as\nother high-speed optical astronomy. This work presents the opportunities and\nchallenges of using EBCs in those cases, and proposes a low-cost approach to\nexperimentally assess the feasibility of this innovative technique.",
            "author": [
                "John Hoang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16321v1",
                "http://arxiv.org/pdf/2310.16321v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16319v1",
            "title": "DiQAD: A Benchmark Dataset for End-to-End Open-domain Dialogue\n  Assessment",
            "updated": "2023-10-25T03:04:57Z",
            "published": "2023-10-25T03:04:57Z",
            "summary": "Dialogue assessment plays a critical role in the development of open-domain\ndialogue systems. Existing work are uncapable of providing an end-to-end and\nhuman-epistemic assessment dataset, while they only provide sub-metrics like\ncoherence or the dialogues are conversed between annotators far from real user\nsettings. In this paper, we release a large-scale dialogue quality assessment\ndataset (DiQAD), for automatically assessing open-domain dialogue quality.\nSpecifically, we (1) establish the assessment criteria based on the dimensions\nconforming to human judgements on dialogue qualities, and (2) annotate\nlarge-scale dialogues that conversed between real users based on these\nannotation criteria, which contains around 100,000 dialogues. We conduct\nseveral experiments and report the performances of the baselines as the\nbenchmark on DiQAD. The dataset is openly accessible at\nhttps://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation.",
            "author": [
                "Yukun Zhao",
                "Lingyong Yan",
                "Weiwei Sun",
                "Chong Meng",
                "Shuaiqiang Wang",
                "Zhicong Cheng",
                "Zhaochun Ren",
                "Dawei Yin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16319v1",
                "http://arxiv.org/pdf/2310.16319v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16318v1",
            "title": "Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked\n  Auto-Encoder",
            "updated": "2023-10-25T03:03:34Z",
            "published": "2023-10-25T03:03:34Z",
            "summary": "Despite its practical importance across a wide range of modalities, recent\nadvances in self-supervised learning (SSL) have been primarily focused on a few\nwell-curated domains, e.g., vision and language, often relying on their\ndomain-specific knowledge. For example, Masked Auto-Encoder (MAE) has become\none of the popular architectures in these domains, but less has explored its\npotential in other modalities. In this paper, we develop MAE as a unified,\nmodality-agnostic SSL framework. In turn, we argue meta-learning as a key to\ninterpreting MAE as a modality-agnostic learner, and propose enhancements to\nMAE from the motivation to jointly improve its SSL across diverse modalities,\ncoined MetaMAE as a result. Our key idea is to view the mask reconstruction of\nMAE as a meta-learning task: masked tokens are predicted by adapting the\nTransformer meta-learner through the amortization of unmasked tokens. Based on\nthis novel interpretation, we propose to integrate two advanced meta-learning\ntechniques. First, we adapt the amortized latent of the Transformer encoder\nusing gradient-based meta-learning to enhance the reconstruction. Then, we\nmaximize the alignment between amortized and adapted latents through task\ncontrastive learning which guides the Transformer encoder to better encode the\ntask-specific knowledge. Our experiment demonstrates the superiority of MetaMAE\nin the modality-agnostic SSL benchmark (called DABS), significantly\noutperforming prior baselines. Code is available at\nhttps://github.com/alinlab/MetaMAE.",
            "author": [
                "Huiwon Jang",
                "Jihoon Tack",
                "Daewon Choi",
                "Jongheon Jeong",
                "Jinwoo Shin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16318v1",
                "http://arxiv.org/pdf/2310.16318v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16317v1",
            "title": "Rotatum of Light",
            "updated": "2023-10-25T02:57:22Z",
            "published": "2023-10-25T02:57:22Z",
            "summary": "Vortices are ubiquitous in nature and can be observed in fluids, condensed\nmatter, and even in the formation of galaxies. Light, too, can evolve like a\nvortex. Optical vortices are exploited in light-matter interaction, free-space\ncommunications, and imaging. Here, we introduce optical rotatum; a new\ndegree-of-freedom of light in which an optical vortex experiences a quadratic\nchirp in its orbital angular momentum along the optical path. We show that such\nan adiabatic deformation of topology is associated with the accumulation of a\nBerry phase factor which in turn perturbs the propagation constant (spatial\nfrequency) of the beam. Remarkably, the spatial structure of optical rotatum\nfollows a logarithmic spiral; a signature that is commonly seen in the pattern\nformation of seashells and galaxies. Our work expands previous literature on\nstructured light, offers new modalities for light-matter interaction,\ncommunications, and sensing, and hints to analogous effects in condensed matter\nphysics and Bose-Einstein condensates.",
            "author": [
                "Ahmed H. Dorrah",
                "Alfonso Palmieri",
                "Lisa Li",
                "Federico Capasso"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16317v1",
                "http://arxiv.org/pdf/2310.16317v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16316v1",
            "title": "Sum-of-Parts Models: Faithful Attributions for Groups of Features",
            "updated": "2023-10-25T02:50:10Z",
            "published": "2023-10-25T02:50:10Z",
            "summary": "An explanation of a machine learning model is considered \"faithful\" if it\naccurately reflects the model's decision-making process. However, explanations\nsuch as feature attributions for deep learning are not guaranteed to be\nfaithful, and can produce potentially misleading interpretations. In this work,\nwe develop Sum-of-Parts (SOP), a class of models whose predictions come with\ngrouped feature attributions that are faithful-by-construction. This model\ndecomposes a prediction into an interpretable sum of scores, each of which is\ndirectly attributable to a sparse group of features. We evaluate SOP on\nbenchmarks with standard interpretability metrics, and in a case study, we use\nthe faithful explanations from SOP to help astrophysicists discover new\nknowledge about galaxy formation.",
            "author": [
                "Weiqiu You",
                "Helen Qu",
                "Marco Gatti",
                "Bhuvnesh Jain",
                "Eric Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16316v1",
                "http://arxiv.org/pdf/2310.16316v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16314v2",
            "title": "Understanding Code Semantics: An Evaluation of Transformer Models in\n  Summarization",
            "updated": "2023-10-27T01:22:52Z",
            "published": "2023-10-25T02:41:50Z",
            "summary": "This paper delves into the intricacies of code summarization using advanced\ntransformer-based language models. Through empirical studies, we evaluate the\nefficacy of code summarization by altering function and variable names to\nexplore whether models truly understand code semantics or merely rely on\ntextual cues. We have also introduced adversaries like dead code and commented\ncode across three programming languages (Python, Javascript, and Java) to\nfurther scrutinize the model's understanding. Ultimately, our research aims to\noffer valuable insights into the inner workings of transformer-based LMs,\nenhancing their ability to understand code and contributing to more efficient\nsoftware development practices and maintenance workflows.",
            "author": [
                "Debanjan Mondal",
                "Abhilasha Lodha",
                "Ankita Sahoo",
                "Beena Kumari"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16314v2",
                "http://arxiv.org/pdf/2310.16314v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16310v1",
            "title": "Score Matching-based Pseudolikelihood Estimation of Neural Marked\n  Spatio-Temporal Point Process with Uncertainty Quantification",
            "updated": "2023-10-25T02:37:51Z",
            "published": "2023-10-25T02:37:51Z",
            "summary": "Spatio-temporal point processes (STPPs) are potent mathematical tools for\nmodeling and predicting events with both temporal and spatial features. Despite\ntheir versatility, most existing methods for learning STPPs either assume a\nrestricted form of the spatio-temporal distribution, or suffer from inaccurate\napproximations of the intractable integral in the likelihood training\nobjective. These issues typically arise from the normalization term of the\nprobability density function. Moreover, current techniques fail to provide\nuncertainty quantification for model predictions, such as confidence intervals\nfor the predicted event's arrival time and confidence regions for the event's\nlocation, which is crucial given the considerable randomness of the data. To\ntackle these challenges, we introduce SMASH: a Score MAtching-based\npSeudolikeliHood estimator for learning marked STPPs with uncertainty\nquantification. Specifically, our framework adopts a normalization-free\nobjective by estimating the pseudolikelihood of marked STPPs through\nscore-matching and offers uncertainty quantification for the predicted event\ntime, location and mark by computing confidence regions over the generated\nsamples. The superior performance of our proposed framework is demonstrated\nthrough extensive experiments in both event prediction and uncertainty\nquantification.",
            "author": [
                "Zichong Li",
                "Qunzhi Xu",
                "Zhenghao Xu",
                "Yajun Mei",
                "Tuo Zhao",
                "Hongyuan Zha"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16310v1",
                "http://arxiv.org/pdf/2310.16310v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16308v1",
            "title": "Diffusion model approach to simulating electron-proton scattering events",
            "updated": "2023-10-25T02:29:06Z",
            "published": "2023-10-25T02:29:06Z",
            "summary": "Generative AI is a fast-growing area of research offering various avenues for\nexploration in high-energy nuclear physics. In this work, we explore the use of\ngenerative models for simulating electron-proton collisions relevant to\nexperiments like CEBAF and the future Electron-Ion Collider (EIC). These\nexperiments play a critical role in advancing our understanding of nucleons and\nnuclei in terms of quark and gluon degrees of freedom. The use of generative\nmodels for simulating collider events faces several challenges such as the\nsparsity of the data, the presence of global or event-wide constraints, and\nsteeply falling particle distributions. In this work, we focus on the\nimplementation of diffusion models for the simulation of electron-proton\nscattering events at EIC energies. Our results demonstrate that diffusion\nmodels can accurately reproduce relevant observables such as momentum\ndistributions and correlations of particles, momentum sum rules, and the\nleading electron kinematics, all of which are of particular interest in\nelectron-proton collisions. Although the sampling process is relatively slow\ncompared to other machine learning architectures, we find diffusion models can\ngenerate high-quality samples. We foresee various applications of our work\nincluding inference for nuclear structure, interpretable generative machine\nlearning, and searches of physics beyond the Standard Model.",
            "author": [
                "Peter Devlin",
                "Jian-Wei Qiu",
                "Felix Ringer",
                "Nobuo Sato"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16308v1",
                "http://arxiv.org/pdf/2310.16308v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "hep-ex",
                "nucl-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16307v1",
            "title": "The physical origin of the periodic activity for FRB 20180916B",
            "updated": "2023-10-25T02:26:21Z",
            "published": "2023-10-25T02:26:21Z",
            "summary": "Fast radio bursts (FRBs) are transient radio signals with\nmillisecond-duration, large dispersion measure (DM) and extremely high\nbrightness temperature. Among them, FRB 20180916B has been found to have a\n16-day periodic activity. However, the physical origin of the periodicity is\nstill a mystery. Here, we utilize the comprehensive observational data to\ndiagnose the periodic models. We find that the ultra-long rotation model is the\nmost probable one for the periodic activity. However, this model cannot\nreproduce the observed rotation measure (RM) variations. We propose a\nself-consistent model, i.e., a massive binary containing a slowly rotational\nneutron star and a massive star with large mass loss, which can naturally\naccommodate the wealth of observational features for FRB 20180916B. In this\nmodel, the RM variation is periodic, which can be tested by future\nobservations.",
            "author": [
                "Hao-Tian Lan",
                "Zhen-Yin Zhao",
                "Yu-Jia Wei",
                "F. Y. Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16307v1",
                "http://arxiv.org/pdf/2310.16307v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16306v1",
            "title": "Generation of $\u03b3$ photons with extremely large orbital angular\n  momenta",
            "updated": "2023-10-25T02:26:08Z",
            "published": "2023-10-25T02:26:08Z",
            "summary": "Vortex $\\gamma$ photons, which carry large intrinsic orbital angular momenta\n(OAM), have significant applications in nuclear, atomic, hadron, particle and\nastro-physics, but their production remains unclear. In this work, we\ninvestigate the generation of such photons from nonlinear Compton scattering of\ncircularly polarized monochromatic lasers on vortex electrons. We develop a\nquantum radiation theory for ultrarelativistic vortex electrons in lasers by\nusing the harmonics expansion and spin eigenfunctions, which allows us to\nexplore the kinematical characteristics, angular momentum transfer mechanisms,\nand formation conditions of vortex $\\gamma$ photons. The multiphoton absorption\nof electrons enables the vortex $\\gamma$ photons, with fixed polarizations and\nenergies, to exist in mixed states comprised of multiple harmonics. Each\nharmonic represents a vortex eigenmode and has transverse momentum broadening\ndue to transverse momenta of the vortex electrons. The large topological\ncharges associated with vortex electrons offer the possibility for $\\gamma$\nphotons to carry adjustable OAM quantum numbers from tens to thousands of\nunits, even at moderate laser intensities. $\\gamma$ photons with large OAM and\ntransverse coherence length can assist in influencing quantum selection rules\nand extracting phase of the scattering amplitude in scattering processes.",
            "author": [
                "Ren-Tong Guo",
                "Mamutjan Ababekri",
                "Qian Zhao",
                "Yousef I. Salamin",
                "Liang-Liang Ji",
                "Zhi-Gang Bu",
                "Zhong-Feng Xu",
                "Xiu-Feng Weng",
                "Jian-Xing Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16306v1",
                "http://arxiv.org/pdf/2310.16306v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16305v1",
            "title": "Dolfin: Diffusion Layout Transformers without Autoencoder",
            "updated": "2023-10-25T02:26:04Z",
            "published": "2023-10-25T02:26:04Z",
            "summary": "In this paper, we introduce a novel generative model, Diffusion Layout\nTransformers without Autoencoder (Dolfin), which significantly improves the\nmodeling capability with reduced complexity compared to existing methods.\nDolfin employs a Transformer-based diffusion process to model layout\ngeneration. In addition to an efficient bi-directional (non-causal joint)\nsequence representation, we further propose an autoregressive diffusion model\n(Dolfin-AR) that is especially adept at capturing rich semantic correlations\nfor the neighboring objects, such as alignment, size, and overlap. When\nevaluated against standard generative layout benchmarks, Dolfin notably\nimproves performance across various metrics (fid, alignment, overlap, MaxIoU\nand DocSim scores), enhancing transparency and interoperability in the process.\nMoreover, Dolfin's applications extend beyond layout generation, making it\nsuitable for modeling geometric structures, such as line segments. Our\nexperiments present both qualitative and quantitative results to demonstrate\nthe advantages of Dolfin.",
            "author": [
                "Yilin Wang",
                "Zeyuan Chen",
                "Liangjun Zhong",
                "Zheng Ding",
                "Zhizhou Sha",
                "Zhuowen Tu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16305v1",
                "http://arxiv.org/pdf/2310.16305v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16304v1",
            "title": "Deep Learning Approach to Photometric Redshift Estimation",
            "updated": "2023-10-25T02:24:37Z",
            "published": "2023-10-25T02:24:37Z",
            "summary": "Photometric redshift estimation, an essential process in astronomy for\ndistance estimation, obtains the redshift of celestial structures by utilizing\nthe magnitude of objects in varying wavelength filters. This research\ncapitalized on a dataset of 50,000 objects from the Sloan Digital Sky Survey,\ncomprising 5 bands of magnitudes and their corresponding redshift labels.\nTypically, studies use spectral distribution templates (SED) for redshift\nprediction. However, these templates are expensive and hard to obtain,\nespecially with larger datasets. The paper explores approaches for Data-Driven\nmethodology instead of template based prediction. Adopting both a decision tree\nregression model and a Fully Connected Neural Network (FCN) for analysis, the\nFCN significantly outperformed the decision tree regressor, achieving an\nimpressive root mean square error (RMSE) of 0.009 compared to the decision\ntree's RMSE above 0.16. The strong performance of the FCN highlights its\nability to capture intricate relationships in astronomical data, holding the\npotential for data-driven redshift estimation, which will help advance next\ngeneration surveys.",
            "author": [
                "Krishna Chunduri",
                "Mithun Mahesh"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16304v1",
                "http://arxiv.org/pdf/2310.16304v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16303v1",
            "title": "URL-BERT: Training Webpage Representations via Social Media Engagements",
            "updated": "2023-10-25T02:22:50Z",
            "published": "2023-10-25T02:22:50Z",
            "summary": "Understanding and representing webpages is crucial to online social networks\nwhere users may share and engage with URLs. Common language model (LM) encoders\nsuch as BERT can be used to understand and represent the textual content of\nwebpages. However, these representations may not model thematic information of\nweb domains and URLs or accurately capture their appeal to social media users.\nIn this work, we introduce a new pre-training objective that can be used to\nadapt LMs to understand URLs and webpages. Our proposed framework consists of\ntwo steps: (1) scalable graph embeddings to learn shallow representations of\nURLs based on user engagement on social media and (2) a contrastive objective\nthat aligns LM representations with the aforementioned graph-based\nrepresentation. We apply our framework to the multilingual version of BERT to\nobtain the model URL-BERT. We experimentally demonstrate that our continued\npre-training approach improves webpage understanding on a variety of tasks and\nTwitter internal and external benchmarks.",
            "author": [
                "Ayesha Qamar",
                "Chetan Verma",
                "Ahmed El-Kishky",
                "Sumit Binnani",
                "Sneha Mehta",
                "Taylor Berg-Kirkpatrick"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16303v1",
                "http://arxiv.org/pdf/2310.16303v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16858v2",
            "title": "4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance\n  Fields via Semantic Distillation",
            "updated": "2023-11-06T03:38:44Z",
            "published": "2023-10-25T02:20:03Z",
            "summary": "This paper targets interactive object-level editing (e.g., deletion,\nrecoloring, transformation, composition) in dynamic scenes. Recently, some\nmethods aiming for flexible editing static scenes represented by neural\nradiance field (NeRF) have shown impressive synthesis quality, while similar\ncapabilities in time-variant dynamic scenes remain limited. To solve this\nproblem, we propose 4D-Editor, an interactive semantic-driven editing\nframework, allowing editing multiple objects in a dynamic NeRF with user\nstrokes on a single frame. We propose an extension to the original dynamic NeRF\nby incorporating a hybrid semantic feature distillation to maintain\nspatial-temporal consistency after editing. In addition, we design Recursive\nSelection Refinement that significantly boosts object segmentation accuracy\nwithin a dynamic NeRF to aid the editing process. Moreover, we develop\nMulti-view Reprojection Inpainting to fill holes caused by incomplete scene\ncapture after editing. Extensive experiments and editing examples on real-world\ndemonstrate that 4D-Editor achieves photo-realistic editing on dynamic NeRFs.\nProject page: https://patrickddj.github.io/4D-Editor",
            "author": [
                "Dadong Jiang",
                "Zhihui Ke",
                "Xiaobo Zhou",
                "Xidong Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16858v2",
                "http://arxiv.org/pdf/2310.16858v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16302v1",
            "title": "Imperfect Digital Twin Assisted Low Cost Reinforcement Training for\n  Multi-UAV Networks",
            "updated": "2023-10-25T02:19:19Z",
            "published": "2023-10-25T02:19:19Z",
            "summary": "Deep Reinforcement Learning (DRL) is widely used to optimize the performance\nof multi-UAV networks. However, the training of DRL relies on the frequent\ninteractions between the UAVs and the environment, which consumes lots of\nenergy due to the flying and communication of UAVs in practical experiments.\nInspired by the growing digital twin (DT) technology, which can simulate the\nperformance of algorithms in the digital space constructed by coping features\nof the physical space, the DT is introduced to reduce the costs of practical\ntraining, e.g., energy and hardware purchases. Different from previous\nDT-assisted works with an assumption of perfect reflecting real physics by\nvirtual digital, we consider an imperfect DT model with deviations for\nassisting the training of multi-UAV networks. Remarkably, to trade off the\ntraining cost, DT construction cost, and the impact of deviations of DT on\ntraining, the natural and virtually generated UAV mixing deployment method is\nproposed. Two cascade neural networks (NN) are used to optimize the joint\nnumber of virtually generated UAVs, the DT construction cost, and the\nperformance of multi-UAV networks. These two NNs are trained by unsupervised\nand reinforcement learning, both low-cost label-free training methods.\nSimulation results show the training cost can significantly decrease while\nguaranteeing the training performance. This implies that an efficient decision\ncan be made with imperfect DTs in multi-UAV networks.",
            "author": [
                "Xiucheng Wang",
                "Nan Cheng",
                "Longfei Ma",
                "Zhisheng Yin",
                "Tom. Luan",
                "Ning Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16302v1",
                "http://arxiv.org/pdf/2310.16302v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16301v1",
            "title": "Is ChatGPT a Good Multi-Party Conversation Solver?",
            "updated": "2023-10-25T02:18:40Z",
            "published": "2023-10-25T02:18:40Z",
            "summary": "Large Language Models (LLMs) have emerged as influential instruments within\nthe realm of natural language processing; nevertheless, their capacity to\nhandle multi-party conversations (MPCs) -- a scenario marked by the presence of\nmultiple interlocutors involved in intricate information exchanges -- remains\nuncharted. In this paper, we delve into the potential of generative LLMs such\nas ChatGPT and GPT-4 within the context of MPCs. An empirical analysis is\nconducted to assess the zero-shot learning capabilities of ChatGPT and GPT-4 by\nsubjecting them to evaluation across three MPC datasets that encompass five\nrepresentative tasks. The findings reveal that ChatGPT's performance on a\nnumber of evaluated MPC tasks leaves much to be desired, whilst GPT-4's results\nportend a promising future. Additionally, we endeavor to bolster performance\nthrough the incorporation of MPC structures, encompassing both speaker and\naddressee architecture. This study provides an exhaustive evaluation and\nanalysis of applying generative LLMs to MPCs, casting a light upon the\nconception and creation of increasingly effective and robust MPC agents.\nConcurrently, this work underscores the challenges implicit in the utilization\nof LLMs for MPCs, such as deciphering graphical information flows and\ngenerating stylistically consistent responses.",
            "author": [
                "Chao-Hong Tan",
                "Jia-Chen Gu",
                "Zhen-Hua Ling"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16301v1",
                "http://arxiv.org/pdf/2310.16301v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16297v3",
            "title": "Ghost Problem, Spectrum Identities and Various Constraints on\n  Brane-localized Higher Derivative Gravity",
            "updated": "2023-11-15T06:50:58Z",
            "published": "2023-10-25T02:11:30Z",
            "summary": "Inspired by DGP gravity, this paper investigates higher derivative gravity\nlocalized on the brane. Similar to the case in bulk, we find the\nbrane-localized higher derivative gravity generally suffers the ghost problem.\nBesides, the spectrum includes complex-mass modes and is unstable in some\nparameters. On the other hand, the DGP gravity and brane-localized Gauss-Bonnet\ngravity are well-defined for suitable parameters. We also find novel algebraic\nidentities of the mass spectrum, which reveal the global nature and can\ncharacterize the phase transformation of the mass spectrum.\n  Furthermore, we discuss various constraints on parameters of brane-localized\ngravity in AdS/BCFT and wedge holography, respectively. They include the\ntachyon-free and ghost-free conditions of Kaluza-Klein and brane-bending modes,\nthe positive definiteness of boundary central charges, and entanglement\nentropy. The tachyon-free and ghost-free conditions impose the most substantial\nrestrictions, which require a non-negative DGP gravity and Gauss-Bonnet gravity\non the brane. The ghost-free condition rules out one class of brane-localized\nhigher derivative gravity. Thus, such higher derivative gravity should be\nunderstood as a low energy effective theory on the brane, under the ghost\nenergy scale. Finally, we briefly discuss the applications of our results.",
            "author": [
                "Rong-Xin Miao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16297v3",
                "http://arxiv.org/pdf/2310.16297v3"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16295v1",
            "title": "Instance-wise Linearization of Neural Network for Model Interpretation",
            "updated": "2023-10-25T02:07:39Z",
            "published": "2023-10-25T02:07:39Z",
            "summary": "Neural network have achieved remarkable successes in many scientific fields.\nHowever, the interpretability of the neural network model is still a major\nbottlenecks to deploy such technique into our daily life. The challenge can\ndive into the non-linear behavior of the neural network, which rises a critical\nquestion that how a model use input feature to make a decision. The classical\napproach to address this challenge is feature attribution, which assigns an\nimportant score to each input feature and reveal its importance of current\nprediction. However, current feature attribution approaches often indicate the\nimportance of each input feature without detail of how they are actually\nprocessed by a model internally. These attribution approaches often raise a\nconcern that whether they highlight correct features for a model prediction.\n  For a neural network model, the non-linear behavior is often caused by\nnon-linear activation units of a model. However, the computation behavior of a\nprediction from a neural network model is locally linear, because one\nprediction has only one activation pattern. Base on the observation, we propose\nan instance-wise linearization approach to reformulates the forward computation\nprocess of a neural network prediction. This approach reformulates different\nlayers of convolution neural networks into linear matrix multiplication.\nAggregating all layers' computation, a prediction complex convolution neural\nnetwork operations can be described as a linear matrix multiplication $F(x) = W\n\\cdot x + b$. This equation can not only provides a feature attribution map\nthat highlights the important of the input features but also tells how each\ninput feature contributes to a prediction exactly. Furthermore, we discuss the\napplication of this technique in both supervise classification and unsupervised\nneural network learning parametric t-SNE dimension reduction.",
            "author": [
                "Zhimin Li",
                "Shusen Liu",
                "Kailkhura Bhavya",
                "Timo Bremer",
                "Valerio Pascucci"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16295v1",
                "http://arxiv.org/pdf/2310.16295v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16293v1",
            "title": "Crowd-Certain: Label Aggregation in Crowdsourced and Ensemble Learning\n  Classification",
            "updated": "2023-10-25T01:58:37Z",
            "published": "2023-10-25T01:58:37Z",
            "summary": "Crowdsourcing systems have been used to accumulate massive amounts of labeled\ndata for applications such as computer vision and natural language processing.\nHowever, because crowdsourced labeling is inherently dynamic and uncertain,\ndeveloping a technique that can work in most situations is extremely\nchallenging. In this paper, we introduce Crowd-Certain, a novel approach for\nlabel aggregation in crowdsourced and ensemble learning classification tasks\nthat offers improved performance and computational efficiency for different\nnumbers of annotators and a variety of datasets. The proposed method uses the\nconsistency of the annotators versus a trained classifier to determine a\nreliability score for each annotator. Furthermore, Crowd-Certain leverages\npredicted probabilities, enabling the reuse of trained classifiers on future\nsample data, thereby eliminating the need for recurrent simulation processes\ninherent in existing methods. We extensively evaluated our approach against ten\nexisting techniques across ten different datasets, each labeled by varying\nnumbers of annotators. The findings demonstrate that Crowd-Certain outperforms\nthe existing methods (Tao, Sheng, KOS, MACE, MajorityVote, MMSR, Wawa,\nZero-Based Skill, GLAD, and Dawid Skene), in nearly all scenarios, delivering\nhigher average accuracy, F1 scores, and AUC rates. Additionally, we introduce a\nvariation of two existing confidence score measurement techniques. Finally we\nevaluate these two confidence score techniques using two evaluation metrics:\nExpected Calibration Error (ECE) and Brier Score Loss. Our results show that\nCrowd-Certain achieves higher Brier Score, and lower ECE across the majority of\nthe examined datasets, suggesting better calibrated results.",
            "author": [
                "Mohammad S. Majdi",
                "Jeffrey J. Rodriguez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16293v1",
                "http://arxiv.org/pdf/2310.16293v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16291v1",
            "title": "Sommets fortement critiques d'un tournoi ind\u00e9composable",
            "updated": "2023-10-25T01:54:45Z",
            "published": "2023-10-25T01:54:45Z",
            "summary": "Let $T=(V,A)$ be a tournament. For $X\\subseteq V$, the subtournament of $T$\ninduced by $X$ is denoted by $T[X]$. A subset $I$ of $V$ is an interval of $T$\nprovided that for every $a,b\\in I$ and $x\\in V\\setminus I$, $(a,x)\\in A$ if and\nonly if $(b,x)\\in A$. For example, $\\varnothing $, ${x}$ ($x \\in V$) and $V$\nare intervals of $T$, called trivial intervals. The tournament $T$ is\nindecomposable if all its intervals are trivial, otherwise, it is decomposable.\nA critical tournament is an indecomposable tournament $T$ of cardinality\n$\\geqslant 5$ such that every vertex $x$ of $T$ is critical, i.e., the\nsubtournament $T[V(T)\\setminus\\{x\\}]$ is decomposable. Given an indecomposable\ntournament $T$, a vertex $x$ of $T$ is strongly critical, if for every\n$X\\subseteq V(T)$ such that $x\\in X$, $\\vert X\\vert \\geqslant 5$ and $T[X]$ is\nindecomposable, $x$ is a critical vertex of $T[X]$. Let $T$ be an\nindecomposable tournament and let $\\mathscr{C}(T)$ be the set of the strongly\ncritical vertices of $T$. We prove that, if $T$ is non-critical, then\n$f(T):=\\vert \\mathscr{C}(T)\\vert \\leqslant 4$, and that the correspondence\n$f(T)$ is decreasing from the class of indecomposable and non-critical\ntournaments (defined by means of embedding) to $\\{0,1,2,3,4\\}$. By giving\nexamples, we also verify that the bound 4 is optimal.",
            "author": [
                "Sahbani Rachid",
                "Belkhechine Houmem"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16291v1",
                "http://arxiv.org/pdf/2310.16291v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16287v1",
            "title": "Towards Streaming Speech-to-Avatar Synthesis",
            "updated": "2023-10-25T01:45:33Z",
            "published": "2023-10-25T01:45:33Z",
            "summary": "Streaming speech-to-avatar synthesis creates real-time animations for a\nvirtual character from audio data. Accurate avatar representations of speech\nare important for the visualization of sound in linguistics, phonetics, and\nphonology, visual feedback to assist second language acquisition, and virtual\nembodiment for paralyzed patients. Previous works have highlighted the\ncapability of deep articulatory inversion to perform high-quality avatar\nanimation using electromagnetic articulography (EMA) features. However, these\nmodels focus on offline avatar synthesis with recordings rather than real-time\naudio, which is necessary for live avatar visualization or embodiment. To\naddress this issue, we propose a method using articulatory inversion for\nstreaming high quality facial and inner-mouth avatar animation from real-time\naudio. Our approach achieves 130ms average streaming latency for every 0.1\nseconds of audio with a 0.792 correlation with ground truth articulations.\nFinally, we show generated mouth and tongue animations to demonstrate the\nefficacy of our methodology.",
            "author": [
                "Tejas S. Prabhune",
                "Peter Wu",
                "Bohan Yu",
                "Gopala K. Anumanchipalli"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16287v1",
                "http://arxiv.org/pdf/2310.16287v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.GR",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16285v1",
            "title": "Removing Dust from CMB Observations with Diffusion Models",
            "updated": "2023-10-25T01:36:48Z",
            "published": "2023-10-25T01:36:48Z",
            "summary": "In cosmology, the quest for primordial $B$-modes in cosmic microwave\nbackground (CMB) observations has highlighted the critical need for a refined\nmodel of the Galactic dust foreground. We investigate diffusion-based modeling\nof the dust foreground and its interest for component separation. Under the\nassumption of a Gaussian CMB with known cosmology (or covariance matrix), we\nshow that diffusion models can be trained on examples of dust emission maps\nsuch that their sampling process directly coincides with posterior sampling in\nthe context of component separation. We illustrate this on simulated mixtures\nof dust emission and CMB. We show that common summary statistics (power\nspectrum, Minkowski functionals) of the components are well recovered by this\nprocess. We also introduce a model conditioned by the CMB cosmology that\noutperforms models trained using a single cosmology on component separation.\nSuch a model will be used in future work for diffusion-based cosmological\ninference.",
            "author": [
                "David Heurtel-Depeiges",
                "Blakesley Burkhart",
                "Ruben Ohana",
                "Bruno R\u00e9galdo-Saint Blancard"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16285v1",
                "http://arxiv.org/pdf/2310.16285v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "astro-ph.GA",
                "astro-ph.IM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16857v1",
            "title": "Improvement in Alzheimer's Disease MRI Images Analysis by Convolutional\n  Neural Networks Via Topological Optimization",
            "updated": "2023-10-25T01:36:00Z",
            "published": "2023-10-25T01:36:00Z",
            "summary": "This research underscores the efficacy of Fourier topological optimization in\nrefining MRI imagery, thereby bolstering the classification precision of\nAlzheimer's Disease through convolutional neural networks. Recognizing that MRI\nscans are indispensable for neurological assessments, but frequently grapple\nwith issues like blurriness and contrast irregularities, the deployment of\nFourier topological optimization offered enhanced delineation of brain\nstructures, ameliorated noise, and superior contrast. The applied techniques\nprioritized boundary enhancement, contrast and brightness adjustments, and\noverall image lucidity. Employing CNN architectures VGG16, ResNet50,\nInceptionV3, and Xception, the post-optimization analysis revealed a marked\nelevation in performance. Conclusively, the amalgamation of Fourier topological\noptimization with CNNs delineates a promising trajectory for the nuanced\nclassification of Alzheimer's Disease, portending a transformative impact on\nits diagnostic paradigms.",
            "author": [
                "Peiwen Tan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16857v1",
                "http://arxiv.org/pdf/2310.16857v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16284v1",
            "title": "Bayesian Image Mediation Analysis",
            "updated": "2023-10-25T01:35:29Z",
            "published": "2023-10-25T01:35:29Z",
            "summary": "Mediation analysis aims to separate the indirect effect through mediators\nfrom the direct effect of the exposure on the outcome. It is challenging to\nperform mediation analysis with neuroimaging data which involves high\ndimensionality, complex spatial correlations, sparse activation patterns and\nrelatively low signal-to-noise ratio. To address these issues, we develop a new\nspatially varying coefficient structural equation model for Bayesian Image\nMediation Analysis (BIMA). We define spatially varying mediation effects within\nthe potential outcome framework, employing the soft-thresholded Gaussian\nprocess prior for functional parameters. We establish the posterior consistency\nfor spatially varying mediation effects along with selection consistency on\nimportant regions that contribute to the mediation effects. We develop an\nefficient posterior computation algorithm scalable to analysis of large-scale\nimaging data. Through extensive simulations, we show that BIMA can improve the\nestimation accuracy and computational efficiency for high-dimensional mediation\nanalysis over the existing methods. We apply BIMA to analyze the behavioral and\nfMRI data in the Adolescent Brain Cognitive Development (ABCD) study with a\nfocus on inferring the mediation effects of the parental education level on the\nchildren's general cognitive ability that are mediated through the working\nmemory brain activities.",
            "author": [
                "Yuliang Xu",
                "Jian Kang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16284v1",
                "http://arxiv.org/pdf/2310.16284v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "math.ST",
                "stat.CO",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03707v1",
            "title": "Multi-label Text Classification using GloVe and Neural Network Models",
            "updated": "2023-10-25T01:30:26Z",
            "published": "2023-10-25T01:30:26Z",
            "summary": "This study addresses the challenges of multi-label text classification. The\ndifficulties arise from imbalanced data sets, varied text lengths, and numerous\nsubjective feature labels. Existing solutions include traditional machine\nlearning and deep neural networks for predictions. However, both approaches\nhave their limitations. Traditional machine learning often overlooks the\nassociations between words, while deep neural networks, despite their better\nclassification performance, come with increased training complexity and time.\nThis paper proposes a method utilizing the bag-of-words model approach based on\nthe GloVe model and the CNN-BiLSTM network. The principle is to use the word\nvector matrix trained by the GloVe model as the input for the text embedding\nlayer. Given that the GloVe model requires no further training, the neural\nnetwork model can be trained more efficiently. The method achieves an accuracy\nrate of 87.26% on the test set and an F1 score of 0.8737, showcasing promising\nresults.",
            "author": [
                "Hongren Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03707v1",
                "http://arxiv.org/pdf/2312.03707v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16281v2",
            "title": "Improving Robust Decisions with Data",
            "updated": "2023-10-31T12:46:19Z",
            "published": "2023-10-25T01:27:24Z",
            "summary": "A decision-maker (DM) faces uncertainty governed by a data-generating process\n(DGP), which is only known to belong to a set of sequences of independent but\npossibly non-identical distributions. A robust decision maximizes the DM's\nexpected payoff against the worst possible DGP in this set. This paper studies\nhow such robust decisions can be improved with data, where improvement is\nmeasured by expected payoff under the true DGP. In this paper, I fully\ncharacterize when and how such an improvement can be guaranteed under all\npossible DGPs and develop inference methods to achieve it. These inference\nmethods are needed because, as this paper shows, common inference methods\n(e.g., maximum likelihood or Bayesian) often fail to deliver such an\nimprovement. Importantly, the developed inference methods are given by simple\naugmentations to standard inference procedures, and are thus easy to implement\nin practice.",
            "author": [
                "Xiaoyu Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16281v2",
                "http://arxiv.org/pdf/2310.16281v2"
            ],
            "primary_category": "econ.TH",
            "category": [
                "econ.TH",
                "econ.EM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16280v1",
            "title": "Directly 3D Printed, Pneumatically Actuated Multi-Material Robotic Hand",
            "updated": "2023-10-25T01:24:13Z",
            "published": "2023-10-25T01:24:13Z",
            "summary": "Soft robotic manipulators with many degrees of freedom can carry out complex\ntasks safely around humans. However, manufacturing of soft robotic hands with\nseveral degrees of freedom requires a complex multi-step manual process, which\nsignificantly increases their cost. We present a design of a multi-material 15\nDoF robotic hand with five fingers including an opposable thumb. Our design has\n15 pneumatic actuators based on a series of hollow chambers that are driven by\nan external pressure system. The thumb utilizes rigid joints and the palm\nfeatures internal rigid structure and soft skin. The design can be directly 3D\nprinted using a multi-material additive manufacturing process without any\nassembly process and therefore our hand can be manufactured for less than 300\ndollars. We test the hand in conjunction with a low-cost vision-based\nteleoperation system on different tasks.",
            "author": [
                "Hanna Matusik",
                "Chao Liu",
                "Daniela Rus"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16280v1",
                "http://arxiv.org/pdf/2310.16280v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16278v1",
            "title": "XFEVER: Exploring Fact Verification across Languages",
            "updated": "2023-10-25T01:20:17Z",
            "published": "2023-10-25T01:20:17Z",
            "summary": "This paper introduces the Cross-lingual Fact Extraction and VERification\n(XFEVER) dataset designed for benchmarking the fact verification models across\ndifferent languages. We constructed it by translating the claim and evidence\ntexts of the Fact Extraction and VERification (FEVER) dataset into six\nlanguages. The training and development sets were translated using machine\ntranslation, whereas the test set includes texts translated by professional\ntranslators and machine-translated texts. Using the XFEVER dataset, two\ncross-lingual fact verification scenarios, zero-shot learning and\ntranslate-train learning, are defined, and baseline models for each scenario\nare also proposed in this paper. Experimental results show that the\nmultilingual language model can be used to build fact verification models in\ndifferent languages efficiently. However, the performance varies by language\nand is somewhat inferior to the English case. We also found that we can\neffectively mitigate model miscalibration by considering the prediction\nsimilarity between the English and target languages. The XFEVER dataset, code,\nand model checkpoints are available at\nhttps://github.com/nii-yamagishilab/xfever.",
            "author": [
                "Yi-Chen Chang",
                "Canasai Kruengkrai",
                "Junichi Yamagishi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16278v1",
                "http://arxiv.org/pdf/2310.16278v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16275v1",
            "title": "Non-Destructive Imaging of Breakdown Process in Ferroelectric Capacitors\n  Using \\textit{In-situ} Laser-Based Photoemission Electron Microscopy",
            "updated": "2023-10-25T01:15:46Z",
            "published": "2023-10-25T01:15:46Z",
            "summary": "HfO$_2$-based ferroelectrics are one of the most actively developed\nfunctional materials for memory devices. However, in HfO$_2$-based\nferroelectric devices, dielectric breakdown is a main failure mechanism during\nrepeated polarization switching. Elucidation of the breakdown process may\nbroaden the scope of applications for the ferroelectric HfO$_2$. Here, we\nreport direct observations of a breakdown process in HfO$_2$-based\nferroelectric capacitors, by \\textit{in-situ} laser-based photoemission\nelectron microscopy (laser-PEEM). We have not only clearly visualized the hard\ndielectric breakdown (HDB) spot, but also observed the regions responsible for\nthe soft dielectric breakdown (SDB) which is a precursor phenomenon to HDB. It\nwas found that the low-resistance region formed after SDB is wider than the\nconduction path formed after HDB. Furthermore, our spectromicroscopic analysis\nrevealed that the photoelectron spectrum after SDB shows an enhancement in\nintensity without spectral-shape modulation, interpreted that the initially\nexisted defects are increased. In the HDB spot, however, an additional shoulder\nstructure was observed. These results provide spectroscopic evidence that the\nelectronic states responsible for the conduction path after SDB are different\nfrom those after HDB. Through this work, we propose this microscopic approach\nas a versatile tool for studying buried materials as they are, accelerating the\ndevelopment of material engineering for advanced electronic devices.",
            "author": [
                "Hirokazu Fujiwara",
                "Yuki Itoya",
                "Masaharu Kobayashi",
                "C\u00e9dric Bareille",
                "Shik Shin",
                "Toshiyuki Taniuchi"
            ],
            "link": [
                "http://dx.doi.org/10.1063/5.0162484",
                "http://arxiv.org/abs/2310.16275v1",
                "http://arxiv.org/pdf/2310.16275v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18365v2",
            "title": "Using GPT-4 to Augment Unbalanced Data for Automatic Scoring",
            "updated": "2023-11-18T02:05:27Z",
            "published": "2023-10-25T01:07:50Z",
            "summary": "Machine learning-based automatic scoring can be challenging if students'\nresponses are unbalanced across scoring categories, as it introduces\nuncertainty in the machine training process. To meet this challenge, we\nintroduce a novel text data augmentation framework using GPT-4, a generative\nlarge language model, specifically tailored for unbalanced datasets in\nautomatic scoring. Our experimental dataset comprised student-written responses\nto two science items. We crafted prompts for GPT-4 to generate responses\nresembling student-written answers, particularly for the minority scoring\nclasses, to augment the data. We then finetuned DistillBERT for automatic\nscoring based on the augmented and original datasets. Model performance was\nassessed using accuracy, precision, recall, and F1 score. We incorporate varied\namounts of augmented data to examine scoring performance, and our findings\nrevealed remarkedly improved model performance. The average maximum increase\nobserved across two items is: 3.5% for accuracy, 30.6% for precision, 21.1% for\nrecall, and 24.2% for F1 score. Notably, using just 5% of the augmented data\nled to substantial improvements: 2.6%, 29.2%, 15.1%, and 19.6%. Interestingly,\nthe extent of improvement varied depending on specific datasets. Moreover, we\nfound that a varying amount of augmented data (5%-40%) was needed to obtain a\nstable improvement. We also compare models trained with GPT-4 augmented data\nand those trained with additional student-written responses. The findings\nindicate that former ones match or even exceed the performance of the latter.\nSpecifically, there is an average difference of 1.7%, 1.9%, 11.0%, and 7.8% for\nfour metrics separately. This research underscores the potential and\neffectiveness of data augmentation techniques utilizing GPT-4 in addressing\nunbalanced datasets within automated assessment.",
            "author": [
                "Luyang Fang",
                "Gyeong-Geon Lee",
                "Xiaoming Zhai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18365v2",
                "http://arxiv.org/pdf/2310.18365v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16273v1",
            "title": "Deep Learning for Plant Identification and Disease Classification from\n  Leaf Images: Multi-prediction Approaches",
            "updated": "2023-10-25T01:06:18Z",
            "published": "2023-10-25T01:06:18Z",
            "summary": "Deep learning plays an important role in modern agriculture, especially in\nplant pathology using leaf images where convolutional neural networks (CNN) are\nattracting a lot of attention. While numerous reviews have explored the\napplications of deep learning within this research domain, there remains a\nnotable absence of an empirical study to offer insightful comparisons due to\nthe employment of varied datasets in the evaluation. Furthermore, a majority of\nthese approaches tend to address the problem as a singular prediction task,\noverlooking the multifaceted nature of predicting various aspects of plant\nspecies and disease types. Lastly, there is an evident need for a more profound\nconsideration of the semantic relationships that underlie plant species and\ndisease types. In this paper, we start our study by surveying current deep\nlearning approaches for plant identification and disease classification. We\ncategorise the approaches into multi-model, multi-label, multi-output, and\nmulti-task, in which different backbone CNNs can be employed. Furthermore,\nbased on the survey of existing approaches in plant pathology and the study of\navailable approaches in machine learning, we propose a new model named\nGeneralised Stacking Multi-output CNN (GSMo-CNN). To investigate the\neffectiveness of different backbone CNNs and learning approaches, we conduct an\nintensive experiment on three benchmark datasets Plant Village, Plant Leaves,\nand PlantDoc. The experimental results demonstrate that InceptionV3 can be a\ngood choice for a backbone CNN as its performance is better than AlexNet,\nVGG16, ResNet101, EfficientNet, MobileNet, and a custom CNN developed by us.\nInterestingly, empirical results support the hypothesis that using a single\nmodel can be comparable or better than using two models. Finally, we show that\nthe proposed GSMo-CNN achieves state-of-the-art performance on three benchmark\ndatasets.",
            "author": [
                "Jianping Yao",
                "Son N. Tran",
                "Saurabh Garg",
                "Samantha Sawyer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16273v1",
                "http://arxiv.org/pdf/2310.16273v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16272v1",
            "title": "Stripe order and diode effect in two-dimensional Rashba superconductors",
            "updated": "2023-10-25T01:05:55Z",
            "published": "2023-10-25T01:05:55Z",
            "summary": "In two-dimensional superconductors with a Rashba-type spin-orbit coupling, it\nis known that an in-plane magnetic field can induce a helical superconducting\n(SC) state with a phase modulation $e^{i {\\bf q}\\cdot {\\bf r}}$. Here, we\ntheoretically investigate the stability of a stripe order, a weight-biased\nsuperposition state composed of $+{\\bf q}$ and $-{\\bf q}$ modes taking the form\nof $\\Delta_+ e^{i{\\bf q}\\cdot{\\bf r}}+\\Delta_- e^{-i{\\bf q}\\cdot{\\bf r}}$ with\n$|\\Delta_+|\\neq|\\Delta_-|\\neq 0$, assuming that the spin-singlet pairing\nchannel is dominant. Based on the Ginzburg-Landau theory, we show that for both\ns-wave and d-wave pairing symmetries, the stripe order can appear in the\nhigh-field and low-temperature region inside the helical phase and that the\ntransition between the helical and stripe phases is of second order. It is\nnoteworthy that for the d-wave pairing, the stability region of the stripe\nphase shrinks when the in-plane field is rotated from the nodal direction to\nthe anti-nodal direction. It is also found that the nonreciprocity of the\ncritical current, the so-called SC diode effect, emerges not only in the\nhelical phase but also in the stripe phase, with no clear nonreciprocity\nanomaly at the helical-stripe transition due to its second-order nature.",
            "author": [
                "Kazushi Aoyama"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16272v1",
                "http://arxiv.org/pdf/2310.16272v1"
            ],
            "primary_category": "cond-mat.supr-con",
            "category": [
                "cond-mat.supr-con",
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16271v1",
            "title": "CycleAlign: Iterative Distillation from Black-box LLM to White-box\n  Models for Better Human Alignment",
            "updated": "2023-10-25T01:05:03Z",
            "published": "2023-10-25T01:05:03Z",
            "summary": "Language models trained on large-scale corpus often generate content that is\nharmful, toxic, or contrary to human preferences, making their alignment with\nhuman values a critical concern. Reinforcement learning from human feedback\n(RLHF) with algorithms like PPO is a prevalent approach for alignment but is\noften complex, unstable, and resource-intensive. Recently, ranking-based\nalignment methods have emerged, offering stability and effectiveness by\nreplacing the RL framework with supervised fine-tuning, but they are costly due\nto the need for annotated data. Considering that existing large language models\n(LLMs) like ChatGPT are already relatively well-aligned and cost-friendly,\nresearchers have begun to align the language model with human preference from\nAI feedback. The common practices, which unidirectionally distill the\ninstruction-following responses from LLMs, are constrained by their bottleneck.\nThus we introduce CycleAlign to distill alignment capabilities from\nparameter-invisible LLMs (black-box) to a parameter-visible model (white-box)\nin an iterative manner. With in-context learning (ICL) as the core of the\ncycle, the black-box models are able to rank the model-generated responses\nguided by human-craft instruction and demonstrations about their preferences.\nDuring iterative interaction, the white-box models also have a judgment about\nresponses generated by them. Consequently, the agreement ranking could be\nviewed as a pseudo label to dynamically update the in-context demonstrations\nand improve the preference ranking ability of black-box models. Through\nmultiple interactions, the CycleAlign framework could align the white-box model\nwith the black-box model effectively in a low-resource way. Empirical results\nillustrate that the model fine-tuned by CycleAlign remarkably exceeds existing\nmethods, and achieves the state-of-the-art performance in alignment with human\nvalue.",
            "author": [
                "Jixiang Hong",
                "Quan Tu",
                "Changyu Chen",
                "Xing Gao",
                "Ji Zhang",
                "Rui Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16271v1",
                "http://arxiv.org/pdf/2310.16271v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16270v1",
            "title": "Attention Lens: A Tool for Mechanistically Interpreting the Attention\n  Head Information Retrieval Mechanism",
            "updated": "2023-10-25T01:03:35Z",
            "published": "2023-10-25T01:03:35Z",
            "summary": "Transformer-based Large Language Models (LLMs) are the state-of-the-art for\nnatural language tasks. Recent work has attempted to decode, by reverse\nengineering the role of linear layers, the internal mechanisms by which LLMs\narrive at their final predictions for text completion tasks. Yet little is\nknown about the specific role of attention heads in producing the final token\nprediction. We propose Attention Lens, a tool that enables researchers to\ntranslate the outputs of attention heads into vocabulary tokens via learned\nattention-head-specific transformations called lenses. Preliminary findings\nfrom our trained lenses indicate that attention heads play highly specialized\nroles in language models. The code for Attention Lens is available at\ngithub.com/msakarvadia/AttentionLens.",
            "author": [
                "Mansi Sakarvadia",
                "Arham Khan",
                "Aswathy Ajith",
                "Daniel Grzenda",
                "Nathaniel Hudson",
                "Andr\u00e9 Bauer",
                "Kyle Chard",
                "Ian Foster"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16270v1",
                "http://arxiv.org/pdf/2310.16270v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16269v1",
            "title": "Multilingual Coarse Political Stance Classification of Media. The\n  Editorial Line of a ChatGPT and Bard Newspaper",
            "updated": "2023-10-25T01:01:28Z",
            "published": "2023-10-25T01:01:28Z",
            "summary": "Neutrality is difficult to achieve and, in politics, subjective. Traditional\nmedia typically adopt an editorial line that can be used by their potential\nreaders as an indicator of the media bias. Several platforms currently rate\nnews outlets according to their political bias. The editorial line and the\nratings help readers in gathering a balanced view of news. But in the advent of\ninstruction-following language models, tasks such as writing a newspaper\narticle can be delegated to computers. Without imposing a biased persona, where\nwould an AI-based news outlet lie within the bias ratings? In this work, we use\nthe ratings of authentic news outlets to create a multilingual corpus of news\nwith coarse stance annotations (Left and Right) along with automatically\nextracted topic annotations. We show that classifiers trained on this data are\nable to identify the editorial line of most unseen newspapers in English,\nGerman, Spanish and Catalan. We then apply the classifiers to 101\nnewspaper-like articles written by ChatGPT and Bard in the 4 languages at\ndifferent time periods. We observe that, similarly to traditional newspapers,\nChatGPT editorial line evolves with time and, being a data-driven system, the\nstance of the generated articles differs among languages.",
            "author": [
                "Cristina Espa\u00f1a-Bonet"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16269v1",
                "http://arxiv.org/pdf/2310.16269v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16268v1",
            "title": "Differential nature of inelastic collisions facilitating runaway\n  electron generation in weakly-ionized plasmas",
            "updated": "2023-10-25T00:58:18Z",
            "published": "2023-10-25T00:58:18Z",
            "summary": "We report extention of the Dreicer generation theory to situation where the\nsmall energy exchange no more predominates. In weakly-ionized plamsas, the\nDreicer mechanism can be severely underestimated due to the broken assumption\nof dominant small energy exchange. This Letter numerically demonstrates that\nthe differential nature of inelastic collisions facilitates the Dreicer\ngeneration by developing the new Fokker-Planck-Boltzmann operator of\nelectron-hydrogen atom collisions based on experimental data. This work is\nenvisaged to predict runaway electron generations in future fusion reactors.",
            "author": [
                "Y. Lee",
                "P. Aleynikov",
                "P. C. de Vries",
                "H. -T. Kim",
                "J. Lee",
                "M. Hoppe",
                "J. -K. Park",
                "G. J. Choi",
                "Y. -S. Na"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16268v1",
                "http://arxiv.org/pdf/2310.16268v1"
            ],
            "primary_category": "physics.plasm-ph",
            "category": [
                "physics.plasm-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16266v1",
            "title": "Super-resolution imaging reveals resistance to mass transfer in\n  functionalized stationary phases",
            "updated": "2023-10-25T00:40:33Z",
            "published": "2023-10-25T00:40:33Z",
            "summary": "Chemical separations are costly in terms of energy, time, and money.\nSeparation methods are optimized with inefficient trial-and-error approaches\nthat lack insight into the molecular dynamics that lead to the success or\nfailure of a separation and, hence, ways to improve the process. We perform\nsuper-resolution imaging of fluorescent analytes in four different commercial\nliquid chromatography materials. Surprisingly, we observe that chemical\nfunctionalization can block over fifty percent of the porous interior of the\nmaterial, rendering it inaccessible to small molecule analytes. Only in situ\nimaging unveils the inaccessibility when compared to the industry-accepted ex\nsitu characterization methods. Selectively removing some of the\nfunctionalization with solvent restores pore access without significantly\naltering the single-molecule kinetics that underlie the separation and agree\nwith bulk chromatography measurements. Our molecular results determine that\ncommercial stationary phases, marketed as fully porous, are over-functionalized\nand provide a new avenue to characterize and direct separation material design\nfrom the bottom-up.",
            "author": [
                "Ricardo Monge Neria",
                "Muhammad Zeeshan",
                "Aman Kapoor",
                "Tae Kyong John Kim",
                "Nichole Hoven",
                "Jeffrey S. Pigott",
                "Burcu Gurkan",
                "Christine E. Duval",
                "Rachel A. Saylor",
                "Lydia Kisley"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16266v1",
                "http://arxiv.org/pdf/2310.16266v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16265v1",
            "title": "Experimental test of the Jarzynski equality in a single spin-1 system\n  using high-fidelity single-shot readouts",
            "updated": "2023-10-25T00:39:13Z",
            "published": "2023-10-25T00:39:13Z",
            "summary": "The Jarzynski equality (JE), which connects the equilibrium free energy with\nnon-equilibrium work statistics, plays a crucial role in quantum\nthermodynamics. Although practical quantum systems are usually multi-level\nsystems, most tests of the JE were executed in two-level systems. A rigorous\ntest of the JE by directly measuring the work distribution of a physical\nprocess in a high-dimensional quantum system remains elusive. Here, we report\nan experimental test of the JE in a single spin-1 system. We realized\nnondemolition projective measurement of this three-level system via cascading\nhigh-fidelity single-shot readouts and directly measured the work distribution\nutilizing the two-point measurement protocol. The validity of the JE was\nverified from the non-adiabatic to adiabatic zone and under different effective\ntemperatures. Our work puts the JE on a solid experimental foundation and makes\nthe NV center system a mature toolbox to perform advanced experiments of\nstochastic quantum thermodynamics.",
            "author": [
                "Wenquan Liu",
                "Zhibo Niu",
                "Wei Cheng",
                "Xin Li",
                "Chang-Kui Duan",
                "Zhangqi Yin",
                "Xing Rong",
                "Jiangfeng Du"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16265v1",
                "http://arxiv.org/pdf/2310.16265v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16263v1",
            "title": "Enhancing Large Language Models for Secure Code Generation: A\n  Dataset-driven Study on Vulnerability Mitigation",
            "updated": "2023-10-25T00:32:56Z",
            "published": "2023-10-25T00:32:56Z",
            "summary": "Large language models (LLMs) have brought significant advancements to code\ngeneration, benefiting both novice and experienced developers. However, their\ntraining using unsanitized data from open-source repositories, like GitHub,\nintroduces the risk of inadvertently propagating security vulnerabilities. To\neffectively mitigate this concern, this paper presents a comprehensive study\nfocused on evaluating and enhancing code LLMs from a software security\nperspective. We introduce SecuCoGen\\footnote{SecuCoGen has been uploaded as\nsupplemental material and will be made publicly available after publication.},\na meticulously curated dataset targeting 21 critical vulnerability types.\nSecuCoGen comprises 180 samples and serves as the foundation for conducting\nexperiments on three crucial code-related tasks: code generation, code repair\nand vulnerability classification, with a strong emphasis on security. Our\nexperimental results reveal that existing models often overlook security\nconcerns during code generation, leading to the generation of vulnerable code.\nTo address this, we propose effective approaches to mitigate the security\nvulnerabilities and enhance the overall robustness of code generated by LLMs.\nMoreover, our study identifies weaknesses in existing models' ability to repair\nvulnerable code, even when provided with vulnerability information.\nAdditionally, certain vulnerability types pose challenges for the models,\nhindering their performance in vulnerability classification. Based on these\nfindings, we believe our study will have a positive impact on the software\nengineering community, inspiring the development of improved methods for\ntraining and utilizing LLMs, thereby leading to safer and more trustworthy\nmodel deployment.",
            "author": [
                "Jiexin Wang",
                "Liuwen Cao",
                "Xitong Luo",
                "Zhiping Zhou",
                "Jiayuan Xie",
                "Adam Jatowt",
                "Yi Cai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16263v1",
                "http://arxiv.org/pdf/2310.16263v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.AI",
                "cs.CL",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16262v1",
            "title": "rTisane: Externalizing conceptual models for data analysis increases\n  engagement with domain knowledge and improves statistical model quality",
            "updated": "2023-10-25T00:32:52Z",
            "published": "2023-10-25T00:32:52Z",
            "summary": "Statistical models should accurately reflect analysts' domain knowledge about\nvariables and their relationships. While recent tools let analysts express\nthese assumptions and use them to produce a resulting statistical model, it\nremains unclear what analysts want to express and how externalization impacts\nstatistical model quality. This paper addresses these gaps. We first conduct an\nexploratory study of analysts using a domain-specific language (DSL) to express\nconceptual models. We observe a preference for detailing how variables relate\nand a desire to allow, and then later resolve, ambiguity in their conceptual\nmodels. We leverage these findings to develop rTisane, a DSL for expressing\nconceptual models augmented with an interactive disambiguation process. In a\ncontrolled evaluation, we find that rTisane's DSL helps analysts engage more\ndeeply with and accurately externalize their assumptions. rTisane also leads to\nstatistical models that match analysts' assumptions, maintain analysis intent,\nand better fit the data.",
            "author": [
                "Eunice Jun",
                "Edward Misback",
                "Jeffrey Heer",
                "Ren\u00e9 Just"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16262v1",
                "http://arxiv.org/pdf/2310.16262v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI",
                "cs.PL",
                "stat.CO",
                "H.5.2; D.2.2; H.1.2; D.3.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16261v1",
            "title": "The Distributional Hypothesis Does Not Fully Explain the Benefits of\n  Masked Language Model Pretraining",
            "updated": "2023-10-25T00:31:29Z",
            "published": "2023-10-25T00:31:29Z",
            "summary": "We analyze the masked language modeling pretraining objective function from\nthe perspective of the distributional hypothesis. We investigate whether better\nsample efficiency and the better generalization capability of models pretrained\nwith masked language modeling can be attributed to the semantic similarity\nencoded in the pretraining data's distributional property. Via a synthetic\ndataset, our analysis suggests that distributional property indeed leads to the\nbetter sample efficiency of pretrained masked language models, but does not\nfully explain the generalization capability. We also conduct analyses over two\nreal-world datasets and demonstrate that the distributional property does not\nexplain the generalization ability of pretrained natural language models\neither. Our results illustrate our limited understanding of model pretraining\nand provide future research directions.",
            "author": [
                "Ting-Rui Chiang",
                "Dani Yogatama"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16261v1",
                "http://arxiv.org/pdf/2310.16261v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16259v1",
            "title": "Kerr-Nonlinearity Assisted Exceptional Point Degeneracy in a Detuned\n  PT-Symmetric System",
            "updated": "2023-10-25T00:29:32Z",
            "published": "2023-10-25T00:29:32Z",
            "summary": "Systems operating at exceptional points (EPs) are highly responsive to small\nperturbations, making them suitable for sensing applications. Although this\nfeature impedes the system working exactly at an EP due to imperfections\narising during the fabrication process. We propose a fast self-tuning scheme\nbased on Kerr nonlinearity in a coupled dielectric resonator excited through a\nwaveguide placed in the near-field of the resonators. We show that in a coupled\nresonator with unequal Kerr-coefficients, initial distortion from EP regime can\nbe completely compensated. It provides an opportunity to reach very close to\nthe EP in a coupled resonator with detuned resonant frequencies via tuning the\nintensity of the incident wave. Using time-modulation of the incident wave in\nnonlinear systems to control both the gain or loss, and resonant frequencies\ncan be a possible approach to fully control the parameters close to an EP.",
            "author": [
                "Shahab Ramezanpour"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16259v1",
                "http://arxiv.org/pdf/2310.16259v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16258v1",
            "title": "The effective QCD running coupling constant and a Dirac model for the\n  charmonium spectrum",
            "updated": "2023-10-25T00:28:53Z",
            "published": "2023-10-25T00:28:53Z",
            "summary": "The QCD effective charge extracted from the experimental data is used to\nconstruct the vector interaction of a Dirac relativistic model for the\ncharmonium spectrum. The process required to fit the spectrum is discussed and\nthe relationship with a previous study of the vector interaction is analyzed.",
            "author": [
                "M. De Sanctis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16258v1",
                "http://arxiv.org/pdf/2310.16258v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16253v1",
            "title": "ConDefects: A New Dataset to Address the Data Leakage Concern for\n  LLM-based Fault Localization and Program Repair",
            "updated": "2023-10-25T00:06:02Z",
            "published": "2023-10-25T00:06:02Z",
            "summary": "With the growing interest on Large Language Models (LLMs) for fault\nlocalization and program repair, ensuring the integrity and generalizability of\nthe LLM-based methods becomes paramount. The code in existing widely-adopted\nbenchmarks for these tasks was written before the the bloom of LLMs and may be\nincluded in the training data of existing popular LLMs, thereby suffering from\nthe threat of data leakage, leading to misleadingly optimistic performance\nmetrics. To address this issue, we introduce \"ConDefects\", a novel dataset of\nreal faults meticulously curated to eliminate such overlap. ConDefects contains\n1,254 Java faulty programs and 1,625 Python faulty programs. All these programs\nare sourced from the online competition platform AtCoder and were produced\nbetween October 2021 and September 2023. We pair each fault with fault\nlocations and the corresponding repaired code versions, making it tailored for\nin fault localization and program repair related research. We also provide\ninterfaces for selecting subsets based on different time windows and coding\ntask difficulties. While inspired by LLM-based tasks, ConDefects can be adopted\nfor benchmarking ALL types of fault localization and program repair methods.\nThe dataset is publicly available, and a demo video can be found at\nhttps://www.youtube.com/watch?v=22j15Hj5ONk.",
            "author": [
                "Yonghao Wu",
                "Zheng Li",
                "Jie M. Zhang",
                "Yong Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16253v1",
                "http://arxiv.org/pdf/2310.16253v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16251v1",
            "title": "Speakerly: A Voice-based Writing Assistant for Text Composition",
            "updated": "2023-10-24T23:53:15Z",
            "published": "2023-10-24T23:53:15Z",
            "summary": "We present Speakerly, a new real-time voice-based writing assistance system\nthat helps users with text composition across various use cases such as emails,\ninstant messages, and notes. The user can interact with the system through\ninstructions or dictation, and the system generates a well-formatted and\ncoherent document. We describe the system architecture and detail how we\naddress the various challenges while building and deploying such a system at\nscale. More specifically, our system uses a combination of small, task-specific\nmodels as well as pre-trained language models for fast and effective text\ncomposition while supporting a variety of input modes for better usability.",
            "author": [
                "Dhruv Kumar",
                "Vipul Raheja",
                "Alice Kaiser-Schatzlein",
                "Robyn Perry",
                "Apurva Joshi",
                "Justin Hugues-Nuger",
                "Samuel Lou",
                "Navid Chowdhury"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16251v1",
                "http://arxiv.org/pdf/2310.16251v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16248v2",
            "title": "GlotLID: Language Identification for Low-Resource Languages",
            "updated": "2023-11-04T11:28:10Z",
            "published": "2023-10-24T23:45:57Z",
            "summary": "Several recent papers have published good solutions for language\nidentification (LID) for about 300 high-resource and medium-resource languages.\nHowever, there is no LID available that (i) covers a wide range of low-resource\nlanguages, (ii) is rigorously evaluated and reliable and (iii) efficient and\neasy to use. Here, we publish GlotLID-M, an LID model that satisfies the\ndesiderata of wide coverage, reliability and efficiency. It identifies 1665\nlanguages, a large increase in coverage compared to prior work. In our\nexperiments, GlotLID-M outperforms four baselines (CLD3, FT176, OpenLID and\nNLLB) when balancing F1 and false positive rate (FPR). We analyze the unique\nchallenges that low-resource LID poses: incorrect corpus metadata, leakage from\nhigh-resource languages, difficulty separating closely related languages,\nhandling of macrolanguage vs varieties and in general noisy data. We hope that\nintegrating GlotLID-M into dataset creation pipelines will improve quality and\nenhance accessibility of NLP technology for low-resource languages and\ncultures. GlotLID-M model, code, and list of data sources are available:\nhttps://github.com/cisnlp/GlotLID.",
            "author": [
                "Amir Hossein Kargaran",
                "Ayyoob Imani",
                "Fran\u00e7ois Yvon",
                "Hinrich Sch\u00fctze"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16248v2",
                "http://arxiv.org/pdf/2310.16248v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16246v1",
            "title": "Design of General Purpose Minimal-Auxiliary Ising Machines",
            "updated": "2023-10-24T23:33:26Z",
            "published": "2023-10-24T23:33:26Z",
            "summary": "Ising machines are a form of quantum-inspired processing-in-memory computer\nwhich has shown great promise for overcoming the limitations of traditional\ncomputing paradigms while operating at a fraction of the energy use. The\nprocess of designing Ising machines is known as the reverse Ising problem.\nUnfortunately, this problem is in general computationally intractable: it is a\nnonconvex mixed-integer linear programming problem which cannot be naively\nbrute-forced except in the simplest cases due to exponential scaling of runtime\nwith number of spins. We prove new theoretical results which allow us to reduce\nthe search space to one with quadratic scaling. We utilize this theory to\ndevelop general purpose algorithmic solutions to the reverse Ising problem. In\nparticular, we demonstrate Ising formulations of 3-bit and 4-bit integer\nmultiplication which use fewer total spins than previously known methods by a\nfactor of more than three. Our results increase the practicality of\nimplementing such circuits on modern Ising hardware, where spins are at a\npremium.",
            "author": [
                "Isaac K. Martin",
                "Andrew G. Moore",
                "John T. Daly",
                "Jess J. Meyer",
                "Teresa M. Ranadive"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16246v1",
                "http://arxiv.org/pdf/2310.16246v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.ET",
                "cs.NE",
                "94C11 (Primary), 90C05 68Q12 (Secondary)",
                "G.1.6; G.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16242v1",
            "title": "ZzzGPT: An Interactive GPT Approach to Enhance Sleep Quality",
            "updated": "2023-10-24T23:30:17Z",
            "published": "2023-10-24T23:30:17Z",
            "summary": "In today's world, sleep quality is pivotal for overall well-being. While\nwearable sensors offer real-time monitoring, they often lack actionable\ninsights, leading to user abandonment. This paper delves into the role of\ntechnology in understanding sleep patterns. We introduce a two-stage framework,\nutilizing Large Language Models (LLMs), aiming to provide accurate sleep\npredictions with actionable feedback. Leveraging the GLOBEM dataset and\nsynthetic data from LLMs, we highlight enhanced results with models like\nXGBoost. Our approach merges advanced machine learning with user-centric\ndesign, blending scientific accuracy with practicality.",
            "author": [
                "Yonchanok Khaokaew",
                "Thuc Hanh Nguyen",
                "Kaixin Ji",
                "Hiruni Kegalle",
                "Marwah Alaofi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16242v1",
                "http://arxiv.org/pdf/2310.16242v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16240v1",
            "title": "Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting\n  Pre-trained Language Models",
            "updated": "2023-10-24T23:29:06Z",
            "published": "2023-10-24T23:29:06Z",
            "summary": "In this work, we propose a method that combines two popular research areas by\ninjecting linguistic structures into pre-trained language models in the\nparameter-efficient fine-tuning (PEFT) setting. In our approach, parallel\nadapter modules encoding different linguistic structures are combined using a\nnovel Mixture-of-Linguistic-Experts architecture, where Gumbel-Softmax gates\nare used to determine the importance of these modules at each layer of the\nmodel. To reduce the number of parameters, we first train the model for a fixed\nsmall number of steps before pruning the experts based on their importance\nscores. Our experiment results with three different pre-trained models show\nthat our approach can outperform state-of-the-art PEFT methods with a\ncomparable number of parameters. In addition, we provide additional analysis to\nexamine the experts selected by each model at each layer to provide insights\nfor future studies.",
            "author": [
                "Raymond Li",
                "Gabriel Murray",
                "Giuseppe Carenini"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16240v1",
                "http://arxiv.org/pdf/2310.16240v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16238v1",
            "title": "Efficient GPU-accelerated fitting of observational health-scaled\n  stratified and time-varying Cox models",
            "updated": "2023-10-24T23:19:32Z",
            "published": "2023-10-24T23:19:32Z",
            "summary": "The Cox proportional hazards model stands as a widely-used semi-parametric\napproach for survival analysis in medical research and many other fields.\nNumerous extensions of the Cox model have further expanded its versatility.\nStatistical computing challenges arise, however, when applying many of these\nextensions with the increasing complexity and volume of modern observational\nhealth datasets. To address these challenges, we demonstrate how to employ\nmassive parallelization through graphics processing units (GPU) to enhance the\nscalability of the stratified Cox model, the Cox model with time-varying\ncovariates, and the Cox model with time-varying coefficients. First we\nestablish how the Cox model with time-varying coefficients can be transformed\ninto the Cox model with time-varying covariates when using discrete\ntime-to-event data. We then demonstrate how to recast both of these into a\nstratified Cox model and identify their shared computational bottleneck that\nresults when evaluating the now segmented partial likelihood and its gradient\nwith respect to regression coefficients at scale. These computations mirror a\nhighly transformed segmented scan operation. While this bottleneck is not an\nimmediately obvious target for multi-core parallelization, we convert it into\nan un-segmented operation to leverage the efficient many-core parallel scan\nalgorithm. Our massively parallel implementation significantly accelerates\nmodel fitting on large-scale and high-dimensional Cox models with\nstratification or time-varying effect, delivering an order of magnitude speedup\nover traditional central processing unit-based implementations.",
            "author": [
                "Jianxiao Yang",
                "Martijn J. Schuemie",
                "Marc A. Suchard"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16238v1",
                "http://arxiv.org/pdf/2310.16238v1"
            ],
            "primary_category": "stat.CO",
            "category": [
                "stat.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16234v1",
            "title": "Pixel-Level Clustering Network for Unsupervised Image Segmentation",
            "updated": "2023-10-24T23:06:29Z",
            "published": "2023-10-24T23:06:29Z",
            "summary": "While image segmentation is crucial in various computer vision applications,\nsuch as autonomous driving, grasping, and robot navigation, annotating all\nobjects at the pixel-level for training is nearly impossible. Therefore, the\nstudy of unsupervised image segmentation methods is essential. In this paper,\nwe present a pixel-level clustering framework for segmenting images into\nregions without using ground truth annotations. The proposed framework includes\nfeature embedding modules with an attention mechanism, a feature statistics\ncomputing module, image reconstruction, and superpixel segmentation to achieve\naccurate unsupervised segmentation. Additionally, we propose a training\nstrategy that utilizes intra-consistency within each superpixel,\ninter-similarity/dissimilarity between neighboring superpixels, and structural\nsimilarity between images. To avoid potential over-segmentation caused by\nsuperpixel-based losses, we also propose a post-processing method. Furthermore,\nwe present an extension of the proposed method for unsupervised semantic\nsegmentation. We conducted experiments on three publicly available datasets\n(Berkeley segmentation dataset, PASCAL VOC 2012 dataset, and COCO-Stuff\ndataset) to demonstrate the effectiveness of the proposed framework. The\nexperimental results show that the proposed framework outperforms previous\nstate-of-the-art methods.",
            "author": [
                "Cuong Manh Hoang",
                "Byeongkeun Kang"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.engappai.2023.107327",
                "http://arxiv.org/abs/2310.16234v1",
                "http://arxiv.org/pdf/2310.16234v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16233v1",
            "title": "Space-time structure of particle emission and femtoscopy scales in\n  ultrarelativistic heavy-ion collisions",
            "updated": "2023-10-24T23:05:23Z",
            "published": "2023-10-24T23:05:23Z",
            "summary": "The analysis of the spatiotemporal picture of particle radiation in\nrelativistic heavy-ion collisions in terms of correlation femtoscopy scales,\nemission and source functions allows one to probe the character of evolution of\nthe system created in the collision. Realistic models, like the integrated\nhydrokinetic model (iHKM), used in the present work, are able to simulate the\nentire evolution process of strongly interacting matter produced in high-energy\nnuclear collision. The mentioned model describes all the stages of the system's\nevolution, including formation of the very initial state and its consequent\ngradual thermalization, hydrodynamic expansion and afterburner hadronic\ncascade, that can help researchers to figure out the specific details of the\nprocess and better understand the formation mechanisms of certain observables.\nIn the current paper we investigate the behavior of the pion and kaon\ninterferometry radii and their connection with emission functions in\nultrarelativistic heavy-ion collisions at the Large Hadron Collider within\niHKM. We are focusing on the study of the emission time scales at different\nenergies for both particle species (pions and kaons) aiming to get deeper\ninsight into relation of these scales and the peculiarities of the mentioned\nsystem's collective expansion and decay with the experimentally observed\nfemtoscopy radii. One of our main interests is the problem of the total\nsystem's lifetime estimation based on the femtoscopy analysis.",
            "author": [
                "Yu. M. Sinyukov",
                "V. M. Shapoval",
                "M. D. Adzhymambetov"
            ],
            "link": [
                "http://dx.doi.org/10.3390/universe9100433",
                "http://arxiv.org/abs/2310.16233v1",
                "http://arxiv.org/pdf/2310.16233v1"
            ],
            "primary_category": "nucl-th",
            "category": [
                "nucl-th",
                "hep-ex",
                "hep-ph",
                "nucl-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16229v1",
            "title": "Global Impact and Balancing Act: Deciphering the Effect of Fluorination\n  on B1s Binding Energies in Fluorinated $h$-BN Nanosheets",
            "updated": "2023-10-24T22:54:33Z",
            "published": "2023-10-24T22:54:33Z",
            "summary": "X-ray photoelectron spectroscopy (XPS) plays an important characterization\nrole in the pursuit of controllable fluorination of two-dimensional hexagonal\nboron nitride ($h$-BN). However, there is a lack of clear spectral\ninterpretation and seemingly conflicting measurements exist. To discern the\nstructure-spectroscopy relation, we performed a comprehensive first-principles\nstudy on the boron 1s edge XPS of fluorinated $h$-BN (F-BN) nanosheets. By\ngradually introducing 1--6 fluorine atoms into different boron or nitrogen\nsites, we created various F-BN structures with doping ratios ranging from 1-6%.\nOur calculations reveal that fluorines landed at boron or nitrogen sites exert\ncompetitive effects on the B1s binding energies (BEs), leading to red or blue\nshifts in different measurements. Our calculations affirmed the hypothesis that\nfluorination affects 1s BEs of all borons in the $\\pi$-conjugated system,\nundermining the transferability from $h$-BN to F-BN. Additionally, we observe\nthat the BE generally increases with higher fluorine concentration when both\nboron and nitrogen atoms are non-exclusively fluorinated. These findings\nprovide critical insights into how fluorination affects boron's 1s BEs,\ncontributing to a better understanding of fluorination functionalization\nprocesses in $h$-BN and its potential applications in materials science.",
            "author": [
                "Yang Xiao",
                "Jun-Rong Zhang",
                "Sheng-Yu Wang",
                "Weijie Hua"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16229v1",
                "http://arxiv.org/pdf/2310.16229v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.chem-ph",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16226v1",
            "title": "TiC-CLIP: Continual Training of CLIP Models",
            "updated": "2023-10-24T22:41:14Z",
            "published": "2023-10-24T22:41:14Z",
            "summary": "Keeping large foundation models up to date on latest data is inherently\nexpensive. To avoid the prohibitive costs of constantly retraining, it is\nimperative to continually train these models. This problem is exacerbated by\nthe lack of any large scale continual learning benchmarks or baselines. We\nintroduce the first set of web-scale Time-Continual (TiC) benchmarks for\ntraining vision-language models: TiC-DataCompt, TiC-YFCC, and TiC-RedCaps with\nover 12.7B timestamped image-text pairs spanning 9 years (2014--2022). We first\nuse our benchmarks to curate various dynamic evaluations to measure temporal\nrobustness of existing models. We show OpenAI's CLIP (trained on data up to\n2020) loses $\\approx 8\\%$ zero-shot accuracy on our curated retrieval task from\n2021--2022 compared with more recently trained models in OpenCLIP repository.\nWe then study how to efficiently train models on time-continuous data. We\ndemonstrate that a simple rehearsal-based approach that continues training from\nthe last checkpoint and replays old data reduces compute by $2.5\\times$ when\ncompared to the standard practice of retraining from scratch.",
            "author": [
                "Saurabh Garg",
                "Mehrdad Farajtabar",
                "Hadi Pouransari",
                "Raviteja Vemulapalli",
                "Sachin Mehta",
                "Oncel Tuzel",
                "Vaishaal Shankar",
                "Fartash Faghri"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16226v1",
                "http://arxiv.org/pdf/2310.16226v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16225v1",
            "title": "CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset",
            "updated": "2023-10-24T22:34:43Z",
            "published": "2023-10-24T22:34:43Z",
            "summary": "The CoNLL-03 corpus is arguably the most well-known and utilized benchmark\ndataset for named entity recognition (NER). However, prior works found\nsignificant numbers of annotation errors, incompleteness, and inconsistencies\nin the data. This poses challenges to objectively comparing NER approaches and\nanalyzing their errors, as current state-of-the-art models achieve F1-scores\nthat are comparable to or even exceed the estimated noise level in CoNLL-03. To\naddress this issue, we present a comprehensive relabeling effort assisted by\nautomatic consistency checking that corrects 7.0% of all labels in the English\nCoNLL-03. Our effort adds a layer of entity linking annotation both for better\nexplainability of NER labels and as additional safeguard of annotation quality.\nOur experimental evaluation finds not only that state-of-the-art approaches\nreach significantly higher F1-scores (97.1%) on our data, but crucially that\nthe share of correct predictions falsely counted as errors due to annotation\nnoise drops from 47% to 6%. This indicates that our resource is well suited to\nanalyze the remaining errors made by state-of-the-art models, and that the\ntheoretical upper bound even on high resource, coarse-grained NER is not yet\nreached. To facilitate such analysis, we make CleanCoNLL publicly available to\nthe research community.",
            "author": [
                "Susanna R\u00fccker",
                "Alan Akbik"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16225v1",
                "http://arxiv.org/pdf/2310.16225v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16223v1",
            "title": "How can the optical variation properties of active galactic nuclei be\n  unbiasedly measured?",
            "updated": "2023-10-24T22:27:26Z",
            "published": "2023-10-24T22:27:26Z",
            "summary": "The variability of active galactic nuclei (AGNs) is ubiquitous but has not\nyet been understood. Measuring the optical variation properties of AGNs, such\nas variation timescale and amplitude, and then correlating them with their\nfundamental physical parameters, have long served as a critical way of\nexploring the origin of AGN variability and the associated physics of the\naccretion process in AGNs. Obtaining accurate variation properties of AGNs is\nthus essential. It has been found that the damped random walk (DRW) process can\nwell describe the AGN optical variation, however, there is a controversy over\nhow long a minimal monitoring baseline is required to obtain unbiased variation\nproperties. In this work, we settle the controversy by exhaustively\nscrutinizing the complex combination of assumed priors, adopted best-fit\nvalues, ensemble averaging methods, and fitting methods. Then, the newly\nproposed is an optimized solution where unbiased variation properties of an AGN\nsample possessing the same variation timescale can be obtained with a minimal\nbaseline of about 10 times their variation timescale. Finally, the new\noptimized solution is used to demonstrate the positive role of time domain\nsurveys to be conducted by the Wide Field Survey Telescope in improving\nconstraints on AGN variation properties.",
            "author": [
                "Xu-Fan Hu",
                "Zhen-Yi Cai",
                "Jun-Xian Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16223v1",
                "http://arxiv.org/pdf/2310.16223v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16218v2",
            "title": "Knowledge Editing for Large Language Models: A Survey",
            "updated": "2023-10-26T00:45:42Z",
            "published": "2023-10-24T22:18:13Z",
            "summary": "Large language models (LLMs) have recently transformed both the academic and\nindustrial landscapes due to their remarkable capacity to understand, analyze,\nand generate texts based on their vast knowledge and reasoning ability.\nNevertheless, one major drawback of LLMs is their substantial computational\ncost for pre-training due to their unprecedented amounts of parameters. The\ndisadvantage is exacerbated when new knowledge frequently needs to be\nintroduced into the pre-trained model. Therefore, it is imperative to develop\neffective and efficient techniques to update pre-trained LLMs. Traditional\nmethods encode new knowledge in pre-trained LLMs through direct fine-tuning.\nHowever, naively re-training LLMs can be computationally intensive and risks\ndegenerating valuable pre-trained knowledge irrelevant to the update in the\nmodel. Recently, Knowledge-based Model Editing (KME) has attracted increasing\nattention, which aims to precisely modify the LLMs to incorporate specific\nknowledge, without negatively influencing other irrelevant knowledge. In this\nsurvey, we aim to provide a comprehensive and in-depth overview of recent\nadvances in the field of KME. We first introduce a general formulation of KME\nto encompass different KME strategies. Afterward, we provide an innovative\ntaxonomy of KME techniques based on how the new knowledge is introduced into\npre-trained LLMs, and investigate existing KME strategies while analyzing key\ninsights, advantages, and limitations of methods from each category. Moreover,\nrepresentative metrics, datasets, and applications of KME are introduced\naccordingly. Finally, we provide an in-depth analysis regarding the\npracticality and remaining challenges of KME and suggest promising research\ndirections for further advancement in this field.",
            "author": [
                "Song Wang",
                "Yaochen Zhu",
                "Haochen Liu",
                "Zaiyi Zheng",
                "Chen Chen",
                "Jundong Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16218v2",
                "http://arxiv.org/pdf/2310.16218v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16212v1",
            "title": "ShadowSense: Unsupervised Domain Adaptation and Feature Fusion for\n  Shadow-Agnostic Tree Crown Detection from RGB-Thermal Drone Imagery",
            "updated": "2023-10-24T22:01:14Z",
            "published": "2023-10-24T22:01:14Z",
            "summary": "Accurate detection of individual tree crowns from remote sensing data poses a\nsignificant challenge due to the dense nature of forest canopy and the presence\nof diverse environmental variations, e.g., overlapping canopies, occlusions,\nand varying lighting conditions. Additionally, the lack of data for training\nrobust models adds another limitation in effectively studying complex forest\nconditions. This paper presents a novel method for detecting shadowed tree\ncrowns and provides a challenging dataset comprising roughly 50k paired\nRGB-thermal images to facilitate future research for illumination-invariant\ndetection. The proposed method (ShadowSense) is entirely self-supervised,\nleveraging domain adversarial training without source domain annotations for\nfeature extraction and foreground feature alignment for feature pyramid\nnetworks to adapt domain-invariant representations by focusing on visible\nforeground regions, respectively. It then fuses complementary information of\nboth modalities to effectively improve upon the predictions of an RGB-trained\ndetector and boost the overall accuracy. Extensive experiments demonstrate the\nsuperiority of the proposed method over both the baseline RGB-trained detector\nand state-of-the-art techniques that rely on unsupervised domain adaptation or\nearly image fusion. Our code and data are available:\nhttps://github.com/rudrakshkapil/ShadowSense",
            "author": [
                "Rudraksh Kapil",
                "Seyed Mojtaba Marvasti-Zadeh",
                "Nadir Erbilgin",
                "Nilanjan Ray"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16212v1",
                "http://arxiv.org/pdf/2310.16212v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16211v1",
            "title": "Resource Allocation for UAV-Assisted Industrial IoT User with Finite\n  Blocklength",
            "updated": "2023-10-24T21:59:06Z",
            "published": "2023-10-24T21:59:06Z",
            "summary": "We consider a relay system empowered by an unmanned aerial vehicle (UAV) that\nfacilitates downlink information delivery while adhering to finite blocklength\nrequirements. The setup involves a remote controller transmitting information\nto both a UAV and an industrial Internet of Things (IIoT) or remote device,\nemploying the non-orthogonal multiple access (NOMA) technique in the first\nphase. Subsequently, the UAV decodes and forwards this information to the\nremote device in the second phase. Our primary objective is to minimize the\ndecoding error probability (DEP) at the remote device, which is influenced by\nthe DEP at the UAV. To achieve this goal, we optimize the blocklength,\ntransmission power, and location of the UAV. However, the underlying problem is\nhighly non-convex and generally intractable to be solved directly. To overcome\nthis challenge, we adopt an alternative optimization (AO) approach and\ndecompose the original problem into three sub-problems. This approach leads to\na sub-optimal solution, which effectively mitigates the non-convexity issue. In\nour simulations, we compare the performance of our proposed algorithm with\nbaseline schemes. The results reveal that the proposed framework outperforms\nthe baseline schemes, demonstrating its superiority in achieving lower DEP at\nthe remote device. Furthermore, the simulation results illustrate the rapid\nconvergence of our proposed algorithm, indicating its efficiency and\neffectiveness in solving the optimization problem.",
            "author": [
                "Atefeh Rezaei",
                "Ata Khalili",
                "Falko Dressler"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16211v1",
                "http://arxiv.org/pdf/2310.16211v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16210v1",
            "title": "Sea-Land-Cloud Segmentation in Satellite Hyperspectral Imagery by Deep\n  Learning",
            "updated": "2023-10-24T21:57:59Z",
            "published": "2023-10-24T21:57:59Z",
            "summary": "Satellites are increasingly adopting on-board Artificial Intelligence (AI)\ntechniques to enhance platforms' autonomy through edge inference. In this\ncontext, the utilization of deep learning (DL) techniques for segmentation in\nHS satellite imagery offers advantages for remote sensing applications, and\ntherefore, we train 16 different models, whose codes are made available through\nour study, which we consider to be relevant for on-board multi-class\nsegmentation of HS imagery, focusing on classifying oceanic (sea), terrestrial\n(land), and cloud formations. We employ the HYPSO-1 mission as an illustrative\ncase for sea-land-cloud segmentation, and to demonstrate the utility of the\nsegments, we introduce a novel sea-land-cloud ranking application scenario. Our\nsystem prioritizes HS image downlink based on sea, land, and cloud coverage\nlevels from the segmented images. We comparatively evaluate the models for\nin-orbit deployment, considering performance, parameter count, and inference\ntime. The models include both shallow and deep models, and after we propose\nfour new DL models, we demonstrate that segmenting single spectral signatures\n(1D) outperforms 3D data processing comprising both spectral (1D) and spatial\n(2D) contexts. We conclude that our lightweight DL model, called\n1D-Justo-LiuNet, consistently surpasses state-of-the-art models for\nsea-land-cloud segmentation, such as U-Net and its variations, in terms of\nperformance (0.93 accuracy) and parameter count (4,563). However, the 1D models\npresent longer inference time (15s) in the tested processing architecture,\nwhich is clearly suboptimal. Finally, after demonstrating that in-orbit image\nsegmentation should occur post L1b radiance calibration rather than on raw\ndata, we additionally show that reducing spectral channels down to 3 lowers\nmodels' parameters and inference time, at the cost of weaker segmentation\nperformance.",
            "author": [
                "Jon Alvarez Justo",
                "Joseph Landon Garrett",
                "Mariana-Iuliana Georgescu",
                "Jesus Gonzalez-Llorente",
                "Radu Tudor Ionescu",
                "Tor Arne Johansen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16210v1",
                "http://arxiv.org/pdf/2310.16210v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16203v1",
            "title": "Multivariate Dynamic Mediation Analysis under a Reinforcement Learning\n  Framework",
            "updated": "2023-10-24T21:43:31Z",
            "published": "2023-10-24T21:43:31Z",
            "summary": "Mediation analysis is an important analytic tool commonly used in a broad\nrange of scientific applications. In this article, we study the problem of\nmediation analysis when there are multivariate and conditionally dependent\nmediators, and when the variables are observed over multiple time points. The\nproblem is challenging, because the effect of a mediator involves not only the\npath from the treatment to this mediator itself at the current time point, but\nalso all possible paths pointed to this mediator from its upstream mediators,\nas well as the carryover effects from all previous time points. We propose a\nnovel multivariate dynamic mediation analysis approach. Drawing inspiration\nfrom the Markov decision process model that is frequently employed in\nreinforcement learning, we introduce a Markov mediation process paired with a\nsystem of time-varying linear structural equation models to formulate the\nproblem. We then formally define the individual mediation effect, built upon\nthe idea of simultaneous interventions and intervention calculus. We next\nderive the closed-form expression and propose an iterative estimation procedure\nunder the Markov mediation process model. We study both the asymptotic property\nand the empirical performance of the proposed estimator, and further illustrate\nour method with a mobile health application.",
            "author": [
                "Lan Luo",
                "Chengchun Shi",
                "Jitao Wang",
                "Zhenke Wu",
                "Lexin Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16203v1",
                "http://arxiv.org/pdf/2310.16203v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16197v1",
            "title": "Background Summarization of Event Timelines",
            "updated": "2023-10-24T21:30:15Z",
            "published": "2023-10-24T21:30:15Z",
            "summary": "Generating concise summaries of news events is a challenging natural language\nprocessing task. While journalists often curate timelines to highlight key\nsub-events, newcomers to a news event face challenges in catching up on its\nhistorical context. In this paper, we address this need by introducing the task\nof background news summarization, which complements each timeline update with a\nbackground summary of relevant preceding events. We construct a dataset by\nmerging existing timeline datasets and asking human annotators to write a\nbackground summary for each timestep of each news event. We establish strong\nbaseline performance using state-of-the-art summarization systems and propose a\nquery-focused variant to generate background summaries. To evaluate background\nsummary quality, we present a question-answering-based evaluation metric,\nBackground Utility Score (BUS), which measures the percentage of questions\nabout a current event timestep that a background summary answers. Our\nexperiments show the effectiveness of instruction fine-tuned systems such as\nFlan-T5, in addition to strong zero-shot performance using GPT-3.5.",
            "author": [
                "Adithya Pratapa",
                "Kevin Small",
                "Markus Dreyer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16197v1",
                "http://arxiv.org/pdf/2310.16197v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16195v1",
            "title": "Systematic Physics-Compliant Analysis of Over-the-Air Channel\n  Equalization in RIS-Parametrized Wireless Networks-on-Chip",
            "updated": "2023-10-24T21:25:24Z",
            "published": "2023-10-24T21:25:24Z",
            "summary": "Wireless networks-on-chip (WNoCs) are an enticing complementary interconnect\ntechnology for multi-core chips but face severe resource constraints. Being\nlimited to simple on-off-keying modulation, the reverberant nature of the chip\nenclosure imposes limits on allowed modulation speeds in sight of inter-symbol\ninterference, casting doubts on the competitiveness of WNoCs as interconnect\ntechnology. Fortunately, this vexing problem was recently overcome by\nparametrizing the on-chip radio environment with a reconfigurable intelligent\nsurface (RIS). By suitably configuring the RIS, selected channel impulse\nresponses (CIRs) can be tuned to be (almost) pulse-like despite rich scattering\nthanks to judiciously tailored multi-bounce path interferences. However, the\nexploration of this \"over-the-air\" (OTA) equalization is thwarted by (i) the\noverwhelming complexity of the propagation environment, and (ii) the non-linear\ndependence of the CIR on the RIS configuration, requiring a costly and lengthy\nfull-wave simulation for every optimization step. Here, we show that a\nreduced-basis physics-compliant model for RIS-parametrized WNoCs can be\ncalibrated with a single full-wave simulation. Thereby, we unlock the\npossibility of predicting the CIR for any RIS configuration almost\ninstantaneously without any additional full-wave simulation. We leverage this\nnew tool to systematically explore OTA equalization in RIS-parametrized WNoCs\nregarding the optimal choice of delay time for the RIS-shaped CIR's peak. We\nalso study the simultaneous optimization of multiple on-chip wireless links for\nbroadcasting. Looking forward, the introduced tools will enable the efficient\nexploration of various types of OTA analog computing in RIS-parametrized WNoCs.",
            "author": [
                "Jean Tapie",
                "Hugo Prod'homme",
                "Mohammadreza F. Imani",
                "Philipp del Hougne"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16195v1",
                "http://arxiv.org/pdf/2310.16195v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph",
                "cs.AR",
                "cs.NI",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16194v1",
            "title": "Learning Low-Rank Latent Spaces with Simple Deterministic Autoencoder:\n  Theoretical and Empirical Insights",
            "updated": "2023-10-24T21:24:27Z",
            "published": "2023-10-24T21:24:27Z",
            "summary": "The autoencoder is an unsupervised learning paradigm that aims to create a\ncompact latent representation of data by minimizing the reconstruction loss.\nHowever, it tends to overlook the fact that most data (images) are embedded in\na lower-dimensional space, which is crucial for effective data representation.\nTo address this limitation, we propose a novel approach called Low-Rank\nAutoencoder (LoRAE). In LoRAE, we incorporated a low-rank regularizer to\nadaptively reconstruct a low-dimensional latent space while preserving the\nbasic objective of an autoencoder. This helps embed the data in a\nlower-dimensional space while preserving important information. It is a simple\nautoencoder extension that learns low-rank latent space. Theoretically, we\nestablish a tighter error bound for our model. Empirically, our model's\nsuperiority shines through various tasks such as image generation and\ndownstream classification. Both theoretical and practical outcomes highlight\nthe importance of acquiring low-dimensional embeddings.",
            "author": [
                "Alokendu Mazumder",
                "Tirthajit Baruah",
                "Bhartendu Kumar",
                "Rishab Sharma",
                "Vishwajeet Pattanaik",
                "Punit Rathore"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16194v1",
                "http://arxiv.org/pdf/2310.16194v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16193v1",
            "title": "Length is a Curse and a Blessing for Document-level Semantics",
            "updated": "2023-10-24T21:23:53Z",
            "published": "2023-10-24T21:23:53Z",
            "summary": "In recent years, contrastive learning (CL) has been extensively utilized to\nrecover sentence and document-level encoding capability from pre-trained\nlanguage models. In this work, we question the length generalizability of\nCL-based models, i.e., their vulnerability towards length-induced semantic\nshift. We verify not only that length vulnerability is a significant yet\noverlooked research gap, but we can devise unsupervised CL methods solely\ndepending on the semantic signal provided by document length. We first derive\nthe theoretical foundations underlying length attacks, showing that elongating\na document would intensify the high intra-document similarity that is already\nbrought by CL. Moreover, we found that isotropy promised by CL is highly\ndependent on the length range of text exposed in training. Inspired by these\nfindings, we introduce a simple yet universal document representation learning\nframework, LA(SER)$^{3}$: length-agnostic self-reference for semantically\nrobust sentence representation learning, achieving state-of-the-art\nunsupervised performance on the standard information retrieval benchmark.",
            "author": [
                "Chenghao Xiao",
                "Yizhi Li",
                "G Thomas Hudson",
                "Chenghua Lin",
                "Noura Al Moubayed"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16193v1",
                "http://arxiv.org/pdf/2310.16193v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16191v1",
            "title": "Can Virtual Reality Protect Users from Keystroke Inference Attacks?",
            "updated": "2023-10-24T21:19:38Z",
            "published": "2023-10-24T21:19:38Z",
            "summary": "Virtual Reality (VR) has gained popularity by providing immersive and\ninteractive experiences without geographical limitations. It also provides a\nsense of personal privacy through physical separation. In this paper, we show\nthat despite assumptions of enhanced privacy, VR is unable to shield its users\nfrom side-channel attacks that steal private information. Ironically, this\nvulnerability arises from VR's greatest strength, its immersive and interactive\nnature. We demonstrate this by designing and implementing a new set of\nkeystroke inference attacks in shared virtual environments, where an attacker\n(VR user) can recover the content typed by another VR user by observing their\navatar. While the avatar displays noisy telemetry of the user's hand motion, an\nintelligent attacker can use that data to recognize typed keys and reconstruct\ntyped content, without knowing the keyboard layout or gathering labeled data.\nWe evaluate the proposed attacks using IRB-approved user studies across\nmultiple VR scenarios. For 13 out of 15 tested users, our attacks accurately\nrecognize 86%-98% of typed keys, and the recovered content retains up to 98% of\nthe meaning of the original typed content. We also discuss potential defenses.",
            "author": [
                "Zhuolin Yang",
                "Zain Sarwar",
                "Iris Hwang",
                "Ronik Bhaskar",
                "Ben Y. Zhao",
                "Haitao Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16191v1",
                "http://arxiv.org/pdf/2310.16191v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16188v1",
            "title": "Breaking of a floating particle raft by water waves",
            "updated": "2023-10-24T21:16:39Z",
            "published": "2023-10-24T21:16:39Z",
            "summary": "When particles of a few tens of microns are spread on the surface of water,\nthey aggregate under the action of capillary forces and form a thin floating\nmembrane, a particle raft. In a tank with a raft made of graphite powder, we\ngenerate in the laboratory gravity surface waves, whose wavelength is very\nlarge compared to the thickness of the raft. For a sufficiently strong wave\namplitude, the raft breaks up progressively by developing cracks and producing\nfragments whose sizes decrease on a time scale long compared to the period of\nthe wave. We characterize the breaking mechanisms. Then, we investigate the\narea distribution of the fragments produced during the fragmentation process.\nThe visual appearance of the fragments distributed in size and surrounded by\nopen water bears a striking resemblance to the floes produced by the fracturing\nof sea ice by waves in the polar oceans. Fragmentation concepts and\nmorphological tools built for sea ice floes can be applied to our macroscopic\nanalog, on which the entire dynamic evolution is accessible.",
            "author": [
                "Louis Saddier",
                "Ambre Palotai",
                "Matheo Aksil",
                "Michel Tsamados",
                "Michael Berhanu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16188v1",
                "http://arxiv.org/pdf/2310.16188v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn",
                "cond-mat.soft"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16186v1",
            "title": "Image Segmentation using U-Net Architecture for Powder X-ray Diffraction\n  Images",
            "updated": "2023-10-24T21:11:09Z",
            "published": "2023-10-24T21:11:09Z",
            "summary": "Scientific researchers frequently use the in situ synchrotron high-energy\npowder X-ray diffraction (XRD) technique to examine the crystallographic\nstructures of materials in functional devices such as rechargeable battery\nmaterials. We propose a method for identifying artifacts in experimental XRD\nimages. The proposed method uses deep learning convolutional neural network\narchitectures, such as tunable U-Nets to identify the artifacts. In particular,\nthe predicted artifacts are evaluated against the corresponding ground truth\n(manually implemented) using the overall true positive rate or recall. The\nresult demonstrates that the U-Nets can consistently produce great recall\nperformance at 92.4% on the test dataset, which is not included in the\ntraining, with a 34% reduction in average false positives in comparison to the\nconventional method. The U-Nets also reduce the time required to identify and\nseparate artifacts by more than 50%. Furthermore, the exclusion of the\nartifacts shows major changes in the integrated 1D XRD pattern, enhancing\nfurther analysis of the post-processing XRD data.",
            "author": [
                "Howard Yanxon",
                "Eric Roberts",
                "Hannah Parraga",
                "James Weng",
                "Wenqian Xu",
                "Uta Ruett",
                "Alexander Hexemer",
                "Petrus Zwart",
                "Nickolas Schwarz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16186v1",
                "http://arxiv.org/pdf/2310.16186v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16183v1",
            "title": "BLP 2023 Task 2: Sentiment Analysis",
            "updated": "2023-10-24T21:00:41Z",
            "published": "2023-10-24T21:00:41Z",
            "summary": "We present an overview of the BLP Sentiment Shared Task, organized as part of\nthe inaugural BLP 2023 workshop, co-located with EMNLP 2023. The task is\ndefined as the detection of sentiment in a given piece of social media text.\nThis task attracted interest from 71 participants, among whom 29 and 30 teams\nsubmitted systems during the development and evaluation phases, respectively.\nIn total, participants submitted 597 runs. However, a total of 15 teams\nsubmitted system description papers. The range of approaches in the submitted\nsystems spans from classical machine learning models, fine-tuning pre-trained\nmodels, to leveraging Large Language Model (LLMs) in zero- and few-shot\nsettings. In this paper, we provide a detailed account of the task setup,\nincluding dataset development and evaluation setup. Additionally, we provide a\nbrief overview of the systems submitted by the participants. All datasets and\nevaluation scripts from the shared task have been made publicly available for\nthe research community, to foster further research in this domain",
            "author": [
                "Md. Arid Hasan",
                "Firoj Alam",
                "Anika Anjum",
                "Shudipta Das",
                "Afiyat Anjum"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16183v1",
                "http://arxiv.org/pdf/2310.16183v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16182v1",
            "title": "Transplant arteriosclerosis: an enigmatic disease due to a misnomer",
            "updated": "2023-10-24T21:00:11Z",
            "published": "2023-10-24T21:00:11Z",
            "summary": "Solid organ transplantation across the allogeneic barrier, pioneered by\nThomas Starzl, has by now become a common medical procedure. Unfortunately, the\nnumber of donor organs lost due to transplant arteriosclerosis (chronic\nrejection), remains significant and unchanged for decades. We argue that\ndesignation of transplant arteriosclerosis as chronic rejection, and its\nclassification as a delayed long-lasting reaction of recipient immune effectors\nagainst donor alloantigens have given us a wrong impression that we have\nidentified the necessary cause/pathogenesis of the tissue pathology. However,\nwhatever treatment options we have in the anti-rejection toolbox, despite their\nsuccess in treating classical rejection, do not work for the transplant\narteriosclerosis. Yet, the scientific community has continued to conceptualize\nand approach the pathology within the alloimmunity model. Due to unproductive\nresearch from the alloimmunity and rejection perspective, the number of\ntransplanted hearts lost due to this pathology today is almost the same as it\nwas fifty years ago. We believe that this phenomenon falls under the rubric of\nlinguistic relativity, and that language we chose to name the disease has\nrestricted our cognitive ability to solve the problem. While the initial\nperception of the transplant arteriosclerosis as chronic rejection was logical\nand scientific, the subsequent experience revealed that such perception and\napproach have been fruitless, and likely are incorrect. Considering our tragic\nfailure to prevent and treat the delayed arterial pathology of donor organs\nusing all available knowledge on alloimmunity and rejection, we must finally\ndisassociate the former from the latter. The only way to start this\nuncomfortable process is to change the words we are using; particularly, the\nwords we chose to name the disease. We have to step out of the alloimmunity\nrejection box.",
            "author": [
                "Vladimir M. Subbotin",
                "Michael V. Subotin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16182v1",
                "http://arxiv.org/pdf/2310.16182v1"
            ],
            "primary_category": "q-bio.TO",
            "category": [
                "q-bio.TO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16181v1",
            "title": "Hidden Citations Obscure True Impact in Science",
            "updated": "2023-10-24T20:58:07Z",
            "published": "2023-10-24T20:58:07Z",
            "summary": "References, the mechanism scientists rely on to signal previous knowledge,\nlately have turned into widely used and misused measures of scientific impact.\nYet, when a discovery becomes common knowledge, citations suffer from\nobliteration by incorporation. This leads to the concept of hidden citation,\nrepresenting a clear textual credit to a discovery without a reference to the\npublication embodying it. Here, we rely on unsupervised interpretable machine\nlearning applied to the full text of each paper to systematically identify\nhidden citations. We find that for influential discoveries hidden citations\noutnumber citation counts, emerging regardless of publishing venue and\ndiscipline. We show that the prevalence of hidden citations is not driven by\ncitation counts, but rather by the degree of the discourse on the topic within\nthe text of the manuscripts, indicating that the more discussed is a discovery,\nthe less visible it is to standard bibliometric analysis. Hidden citations\nindicate that bibliometric measures offer a limited perspective on quantifying\nthe true impact of a discovery, raising the need to extract knowledge from the\nfull text of the scientific corpus.",
            "author": [
                "Xiangyi Meng",
                "Onur Varol",
                "Albert-L\u00e1szl\u00f3 Barab\u00e1si"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16181v1",
                "http://arxiv.org/pdf/2310.16181v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.DL",
                "cs.SI",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16177v1",
            "title": "Temperature-induced reversal effects of kink dynamics in carbon nanotube\n  on flat substrate",
            "updated": "2023-10-24T20:48:27Z",
            "published": "2023-10-24T20:48:27Z",
            "summary": "Carbon nanotubes are nano-objects with quite anisotropic properties, for\nexample the mechanical properties in longitudinal and radial directions differ\nsignificantly. This feature of the carbon nanotubes yields many interesting\nphenomena investigated in last decades. One of them is the ability to form both\nhollow and collapsed states if the radius of the nanotube is large enough. The\ntransitions between the two states have been also reported. In our study we\npresent single-walled carbon nanotube interacting with a plane substrate and\ncharacterize the energy of interaction with the substrate using effective\nLennard-Jones-type potential. We show energy of the homogeneous open and\ncollapsed states depending on the radius of the carbon nanotube and report on\nthe bi-stability in some range of the nanotube diameters. Using the\nmolecular-dynamical simulations we look at the evolution of the initial\nhalf-opened, half-collapsed state and demonstrate that the transition area from\none state to another is spatially localized having features of topological\nsoliton (kink or anti-kink). We show that the value and the direction of the\nkink propagation speed depend significantly on the nanotube diameter as well as\non the temperature of the system. We also discuss the mechanism of the process\nusing a simplified model with asymmetric double-well potential and show the\nentropic nature of the transition.",
            "author": [
                "Alexander V. Savin",
                "Margarita Kovaleva"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16177v1",
                "http://arxiv.org/pdf/2310.16177v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "nlin.PS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16176v2",
            "title": "Correction with Backtracking Reduces Hallucination in Summarization",
            "updated": "2023-10-31T14:48:14Z",
            "published": "2023-10-24T20:48:11Z",
            "summary": "Abstractive summarization aims at generating natural language summaries of a\nsource document that are succinct while preserving the important elements.\nDespite recent advances, neural text summarization models are known to be\nsusceptible to hallucinating (or more correctly confabulating), that is to\nproduce summaries with details that are not grounded in the source document. In\nthis paper, we introduce a simple yet efficient technique, CoBa, to reduce\nhallucination in abstractive summarization. The approach is based on two steps:\nhallucination detection and mitigation. We show that the former can be achieved\nthrough measuring simple statistics about conditional word probabilities and\ndistance to context words. Further, we demonstrate that straight-forward\nbacktracking is surprisingly effective at mitigation. We thoroughly evaluate\nthe proposed method with prior art on three benchmark datasets for text\nsummarization. The results show that CoBa is effective and efficient in\nreducing hallucination, and offers great adaptability and flexibility.",
            "author": [
                "Zhenzhen Liu",
                "Chao Wan",
                "Varsha Kishore",
                "Jin Peng Zhou",
                "Minmin Chen",
                "Kilian Q. Weinberger"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16176v2",
                "http://arxiv.org/pdf/2310.16176v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16175v1",
            "title": "G-CASCADE: Efficient Cascaded Graph Convolutional Decoding for 2D\n  Medical Image Segmentation",
            "updated": "2023-10-24T20:41:04Z",
            "published": "2023-10-24T20:41:04Z",
            "summary": "In recent years, medical image segmentation has become an important\napplication in the field of computer-aided diagnosis. In this paper, we are the\nfirst to propose a new graph convolution-based decoder namely, Cascaded Graph\nConvolutional Attention Decoder (G-CASCADE), for 2D medical image segmentation.\nG-CASCADE progressively refines multi-stage feature maps generated by\nhierarchical transformer encoders with an efficient graph convolution block.\nThe encoder utilizes the self-attention mechanism to capture long-range\ndependencies, while the decoder refines the feature maps preserving long-range\ninformation due to the global receptive fields of the graph convolution block.\nRigorous evaluations of our decoder with multiple transformer encoders on five\nmedical image segmentation tasks (i.e., Abdomen organs, Cardiac organs, Polyp\nlesions, Skin lesions, and Retinal vessels) show that our model outperforms\nother state-of-the-art (SOTA) methods. We also demonstrate that our decoder\nachieves better DICE scores than the SOTA CASCADE decoder with 80.8% fewer\nparameters and 82.3% fewer FLOPs. Our decoder can easily be used with other\nhierarchical encoders for general-purpose semantic and medical image\nsegmentation tasks.",
            "author": [
                "Md Mostafijur Rahman",
                "Radu Marculescu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16175v1",
                "http://arxiv.org/pdf/2310.16175v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG",
                "I.4; J.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16174v1",
            "title": "Probing Electromagnetic Nonreciprocity with Quantum Geometry of Photonic\n  States",
            "updated": "2023-10-24T20:37:09Z",
            "published": "2023-10-24T20:37:09Z",
            "summary": "Reciprocal and nonreciprocal effects in dielectric and magnetic materials\nprovide crucial information about the microscopic properties of electrons.\nHowever, experimentally distinguishing the two has proven to be challenging,\nespecially when the associated effects are extremely small. To this end, we\npropose a contact-less detection using a cross-cavity device where a material\nof interest is placed at its centre. We show that the optical properties of the\nmaterial, such as Kerr and Faraday rotation, or, birefringence, manifest in the\ncoupling between the cavities' electromagnetic modes and in the shift of their\nresonant frequencies. By calculating the dynamics of a geometrical photonic\nstate, we formulate a measurement protocol based on the quantum metric and\nquantum process tomography that isolates the individual components of the\nmaterial's complex refractive index and minimizes the quantum mechanical\nCram\\'er-Rao bound on the variance of the associated parameter estimation. Our\napproach is expected to be applicable across a broad spectrum of experimental\nplatforms including Fock states in optical cavities, or, coherent states in\nmicrowave and THz resonators.",
            "author": [
                "Ioannis Petrides",
                "Jonathan B. Curtis",
                "Marie Wesson",
                "Amir Yacoby",
                "Prineha Narang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16174v1",
                "http://arxiv.org/pdf/2310.16174v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "cond-mat.mes-hall",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16169v2",
            "title": "A Bayesian model calibration framework for stochastic compartmental\n  models with both time-varying and time-invariant parameters",
            "updated": "2023-11-04T16:33:20Z",
            "published": "2023-10-24T20:35:06Z",
            "summary": "We consider state and parameter estimation for compartmental models having\nboth time-varying and time-invariant parameters. Though the described Bayesian\ncomputational framework is general, we look at a specific application to the\nsusceptible-infectious-removed (SIR) model which describes a basic mechanism\nfor the spread of infectious diseases through a system of coupled nonlinear\ndifferential equations. The SIR model consists of three states, namely, the\nthree compartments, and two parameters which control the coupling among the\nstates. The deterministic SIR model with time-invariant parameters has shown to\nbe overly simplistic for modelling the complex long-term dynamics of diseases\ntransmission. Recognizing that certain model parameters will naturally vary in\ntime due to seasonal trends, non-pharmaceutical interventions, and other random\neffects, the estimation procedure must systematically permit these time-varying\neffects to be captured, without unduly introducing artificial dynamics into the\nsystem. To this end, we leverage the robustness of the Markov Chain Monte Carlo\n(MCMC) algorithm for the estimation of time-invariant parameters alongside\nnonlinear filters for the joint estimation of the system state and time-varying\nparameters. We demonstrate performance of the framework by first considering a\nseries of examples using synthetic data, followed by an exposition on public\nhealth data collected in the province of Ontario.",
            "author": [
                "Brandon Robinson",
                "Philippe Bisaillon",
                "Jodi D. Edwards",
                "Tetyana Kendzerska",
                "Mohammad Khalil",
                "Dominique Poirel",
                "Abhijit Sarkar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16169v2",
                "http://arxiv.org/pdf/2310.16169v2"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16168v1",
            "title": "Role of Multifidelity Data in Sequential Active Learning Materials\n  Discovery Campaigns: Case Study of Electronic Bandgap",
            "updated": "2023-10-24T20:33:44Z",
            "published": "2023-10-24T20:33:44Z",
            "summary": "Materials discovery and design typically proceeds through iterative\nevaluation (both experimental and computational) to obtain data, generally\ntargeting improvement of one or more properties under one or more constraints\n(e.g., time or budget). However, there can be great variation in the quality\nand cost of different data, and when they are mixed together in what we here\ncall multifidelity data the optimal approaches to their utilization are not\nestablished. It is therefore important to develop strategies to acquire and use\nmultifidelity data to realize the most efficient iterative materials\nexploration. In this work, we assess the impact of using multifidelity data\nthrough mock demonstration of designing solar cell materials, using the\nelectronic bandgap as the target property. We propose a new approach of using\nmultifidelity data through leveraging machine learning models of both low- and\nhigh-fidelity data, where using predicted low-fidelity data as an input feature\nin the high-fidelity model can improve the impact of a multifidelity data\napproach. We show how tradeoffs of low- versus high-fidelity measurement cost\nand acquisition can impact the materials discovery process, and find that the\nuse of multifidelity data has maximal impact on the materials discovery\ncampaign when approximately five low-fidelity measurements per high-fidelity\nmeasurement are performed, and when the cost of low-fidelity measurements is\napproximately 5% or less than that of high-fidelity measurements. This work\nprovides practical guidance and useful qualitative measures for improving\nmaterials discovery campaigns that involve multifidelity data.",
            "author": [
                "Ryan Jacobs",
                "Philip E. Goins",
                "Dane Morgan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16168v1",
                "http://arxiv.org/pdf/2310.16168v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16165v1",
            "title": "Generalized Staircase Codes with Arbitrary Bit Degree",
            "updated": "2023-10-24T20:26:23Z",
            "published": "2023-10-24T20:26:23Z",
            "summary": "We introduce a natural generalization of staircase codes in which each bit is\nprotected by arbitrarily many component codewords rather than two. This enables\npowerful energy-efficient FEC based on iterative decoding of Hamming\ncomponents.",
            "author": [
                "Mohannad Shehadeh",
                "Frank R. Kschischang",
                "and Alvin Y. Sukmadji"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16165v1",
                "http://arxiv.org/pdf/2310.16165v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16164v1",
            "title": "Conversational Challenges in AI-Powered Data Science: Obstacles, Needs,\n  and Design Opportunities",
            "updated": "2023-10-24T20:19:07Z",
            "published": "2023-10-24T20:19:07Z",
            "summary": "Large Language Models (LLMs) are being increasingly employed in data science\nfor tasks like data preprocessing and analytics. However, data scientists\nencounter substantial obstacles when conversing with LLM-powered chatbots and\nacting on their suggestions and answers. We conducted a mixed-methods study,\nincluding contextual observations, semi-structured interviews (n=14), and a\nsurvey (n=114), to identify these challenges. Our findings highlight key issues\nfaced by data scientists, including contextual data retrieval, formulating\nprompts for complex tasks, adapting generated code to local environments, and\nrefining prompts iteratively. Based on these insights, we propose actionable\ndesign recommendations, such as data brushing to support context selection, and\ninquisitive feedback loops to improve communications with AI-based assistants\nin data-science tools.",
            "author": [
                "Bhavya Chopra",
                "Ananya Singha",
                "Anna Fariha",
                "Sumit Gulwani",
                "Chris Parnin",
                "Ashish Tiwari",
                "Austin Z. Henley"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16164v1",
                "http://arxiv.org/pdf/2310.16164v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16162v1",
            "title": "Brainchop: Next Generation Web-Based Neuroimaging Application",
            "updated": "2023-10-24T20:17:06Z",
            "published": "2023-10-24T20:17:06Z",
            "summary": "Performing volumetric image processing directly within the browser,\nparticularly with medical data, presents unprecedented challenges compared to\nconventional backend tools. These challenges arise from limitations inherent in\nbrowser environments, such as constrained computational resources and the\navailability of frontend machine learning libraries. Consequently, there is a\nshortage of neuroimaging frontend tools capable of providing comprehensive\nend-to-end solutions for whole brain preprocessing and segmentation while\npreserving end-user data privacy and residency. In light of this context, we\nintroduce Brainchop (http://www.brainchop.org) as a groundbreaking in-browser\nneuroimaging tool that enables volumetric analysis of structural MRI using\npre-trained full-brain deep learning models, all without requiring technical\nexpertise or intricate setup procedures. Beyond its commitment to data privacy,\nthis frontend tool offers multiple features, including scalability, low\nlatency, user-friendly operation, cross-platform compatibility, and enhanced\naccessibility. This paper outlines the processing pipeline of Brainchop and\nevaluates the performance of models across various software and hardware\nconfigurations. The results demonstrate the practicality of client-side\nprocessing for volumetric data, owing to the robust MeshNet architecture, even\nwithin the resource-constrained environment of web browsers.",
            "author": [
                "Mohamed Masoud",
                "Pratyush Reddy",
                "Farfalla Hu",
                "Sergey Plis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16162v1",
                "http://arxiv.org/pdf/2310.16162v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16161v1",
            "title": "MyriadAL: Active Few Shot Learning for Histopathology",
            "updated": "2023-10-24T20:08:15Z",
            "published": "2023-10-24T20:08:15Z",
            "summary": "Active Learning (AL) and Few Shot Learning (FSL) are two label-efficient\nmethods which have achieved excellent results recently. However, most prior\narts in both learning paradigms fail to explore the wealth of the vast\nunlabelled data. In this study, we address this issue in the scenario where the\nannotation budget is very limited, yet a large amount of unlabelled data for\nthe target task is available. We frame this work in the context of\nhistopathology where labelling is prohibitively expensive. To this end, we\nintroduce an active few shot learning framework, Myriad Active Learning (MAL),\nincluding a contrastive-learning encoder, pseudo-label generation, and novel\nquery sample selection in the loop. Specifically, we propose to massage\nunlabelled data in a self-supervised manner, where the obtained data\nrepresentations and clustering knowledge form the basis to activate the AL\nloop. With feedback from the oracle in each AL cycle, the pseudo-labels of the\nunlabelled data are refined by optimizing a shallow task-specific net on top of\nthe encoder. These updated pseudo-labels serve to inform and improve the active\nlearning query selection process. Furthermore, we introduce a novel recipe to\ncombine existing uncertainty measures and utilize the entire uncertainty list\nto reduce sample redundancy in AL. Extensive experiments on two public\nhistopathology datasets show that MAL has superior test accuracy, macro\nF1-score, and label efficiency compared to prior works, and can achieve a\ncomparable test accuracy to a fully supervised algorithm while labelling only\n5% of the dataset.",
            "author": [
                "Nico Schiavone",
                "Jingyi Wang",
                "Shuangzhi Li",
                "Roger Zemp",
                "Xingyu Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16161v1",
                "http://arxiv.org/pdf/2310.16161v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16158v1",
            "title": "Propagation of light in cold emitter ensembles with quantum position\n  correlations due to static long-range dipolar interactions",
            "updated": "2023-10-24T20:02:40Z",
            "published": "2023-10-24T20:02:40Z",
            "summary": "We analyze the scattering of light from dipolar emitters whose disordered\npositions exhibit correlations induced by static, long-range dipole-dipole\ninteractions. The quantum-mechanical position correlations are calculated for\nzero temperature bosonic atoms or molecules using variational and diffusion\nquantum Monte Carlo methods. For stationary atoms in dense ensembles in the\nlimit of low light intensity, the simulations yield solutions for the optical\nresponses to all orders of position correlation functions that involve\nelectronic ground and excited states. We calculate how coherent and incoherent\nscattering, collective linewidths, line shifts, and eigenmodes, and\ndisorder-induced excitation localization are influenced by the static\ninteractions and the density. We find that dominantly repulsive static\ninteractions in strongly confined oblate and prolate traps introduce\nshort-range ordering among the dipoles which curtails large fluctuations in the\nlight-mediated resonant dipole-dipole interactions. This typically results in\nan increase in coherent reflection and optical depth, accompanied by reduced\nincoherent scattering. The presence of static dipolar interactions permits the\nhighly selective excitation of subradiant eigenmodes in dense clouds. This\neffect becomes even more pronounced in a prolate trap, where the resonances\nnarrow below the natural linewidth. When the static dipolar interactions affect\nthe optical transition frequencies, the ensemble exhibits inhomogeneous\nbroadening due to the nonuniformly experienced static dipolar interactions that\nsuppress cooperative effects.",
            "author": [
                "G. J. Bean",
                "N. D. Drummond",
                "J. Ruostekoski"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16158v1",
                "http://arxiv.org/pdf/2310.16158v1"
            ],
            "primary_category": "cond-mat.quant-gas",
            "category": [
                "cond-mat.quant-gas",
                "physics.atom-ph",
                "physics.optics",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16157v1",
            "title": "Context-aware feature attribution through argumentation",
            "updated": "2023-10-24T20:02:02Z",
            "published": "2023-10-24T20:02:02Z",
            "summary": "Feature attribution is a fundamental task in both machine learning and data\nanalysis, which involves determining the contribution of individual features or\nvariables to a model's output. This process helps identify the most important\nfeatures for predicting an outcome. The history of feature attribution methods\ncan be traced back to General Additive Models (GAMs), which extend linear\nregression models by incorporating non-linear relationships between dependent\nand independent variables. In recent years, gradient-based methods and\nsurrogate models have been applied to unravel complex Artificial Intelligence\n(AI) systems, but these methods have limitations. GAMs tend to achieve lower\naccuracy, gradient-based methods can be difficult to interpret, and surrogate\nmodels often suffer from stability and fidelity issues. Furthermore, most\nexisting methods do not consider users' contexts, which can significantly\ninfluence their preferences. To address these limitations and advance the\ncurrent state-of-the-art, we define a novel feature attribution framework\ncalled Context-Aware Feature Attribution Through Argumentation (CA-FATA). Our\nframework harnesses the power of argumentation by treating each feature as an\nargument that can either support, attack or neutralize a prediction.\nAdditionally, CA-FATA formulates feature attribution as an argumentation\nprocedure, and each computation has explicit semantics, which makes it\ninherently interpretable. CA-FATA also easily integrates side information, such\nas users' contexts, resulting in more accurate predictions.",
            "author": [
                "Jinfeng Zhong",
                "Elsa Negre"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16157v1",
                "http://arxiv.org/pdf/2310.16157v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.IR",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16154v1",
            "title": "Breaking the Curse of Dimensionality in Deep Neural Networks by Learning\n  Invariant Representations",
            "updated": "2023-10-24T19:50:41Z",
            "published": "2023-10-24T19:50:41Z",
            "summary": "Artificial intelligence, particularly the subfield of machine learning, has\nseen a paradigm shift towards data-driven models that learn from and adapt to\ndata. This has resulted in unprecedented advancements in various domains such\nas natural language processing and computer vision, largely attributed to deep\nlearning, a special class of machine learning models. Deep learning arguably\nsurpasses traditional approaches by learning the relevant features from raw\ndata through a series of computational layers.\n  This thesis explores the theoretical foundations of deep learning by studying\nthe relationship between the architecture of these models and the inherent\nstructures found within the data they process. In particular, we ask What\ndrives the efficacy of deep learning algorithms and allows them to beat the\nso-called curse of dimensionality-i.e. the difficulty of generally learning\nfunctions in high dimensions due to the exponentially increasing need for data\npoints with increased dimensionality? Is it their ability to learn relevant\nrepresentations of the data by exploiting their structure? How do different\narchitectures exploit different data structures? In order to address these\nquestions, we push forward the idea that the structure of the data can be\neffectively characterized by its invariances-i.e. aspects that are irrelevant\nfor the task at hand.\n  Our methodology takes an empirical approach to deep learning, combining\nexperimental studies with physics-inspired toy models. These simplified models\nallow us to investigate and interpret the complex behaviors we observe in deep\nlearning systems, offering insights into their inner workings, with the\nfar-reaching goal of bridging the gap between theory and practice.",
            "author": [
                "Leonardo Petrini"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16154v1",
                "http://arxiv.org/pdf/2310.16154v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16153v1",
            "title": "WojoodNER 2023: The First Arabic Named Entity Recognition Shared Task",
            "updated": "2023-10-24T19:50:07Z",
            "published": "2023-10-24T19:50:07Z",
            "summary": "We present WojoodNER-2023, the first Arabic Named Entity Recognition (NER)\nShared Task. The primary focus of WojoodNER-2023 is on Arabic NER, offering\nnovel NER datasets (i.e., Wojood) and the definition of subtasks designed to\nfacilitate meaningful comparisons between different NER approaches.\nWojoodNER-2023 encompassed two Subtasks: FlatNER and NestedNER. A total of 45\nunique teams registered for this shared task, with 11 of them actively\nparticipating in the test phase. Specifically, 11 teams participated in\nFlatNER, while $8$ teams tackled NestedNER. The winning teams achieved F1\nscores of 91.96 and 93.73 in FlatNER and NestedNER, respectively.",
            "author": [
                "Mustafa Jarrar",
                "Muhammad Abdul-Mageed",
                "Mohammed Khalilia",
                "Bashar Talafha",
                "AbdelRahim Elmadany",
                "Nagham Hamad",
                "Alaa' Omar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16153v1",
                "http://arxiv.org/pdf/2310.16153v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16152v1",
            "title": "FLTrojan: Privacy Leakage Attacks against Federated Language Models\n  Through Selective Weight Tampering",
            "updated": "2023-10-24T19:50:01Z",
            "published": "2023-10-24T19:50:01Z",
            "summary": "Federated learning (FL) is becoming a key component in many technology-based\napplications including language modeling -- where individual FL participants\noften have privacy-sensitive text data in their local datasets. However,\nrealizing the extent of privacy leakage in federated language models is not\nstraightforward and the existing attacks only intend to extract data regardless\nof how sensitive or naive it is. To fill this gap, in this paper, we introduce\ntwo novel findings with regard to leaking privacy-sensitive user data from\nfederated language models. Firstly, we make a key observation that model\nsnapshots from the intermediate rounds in FL can cause greater privacy leakage\nthan the final trained model. Secondly, we identify that privacy leakage can be\naggravated by tampering with a model's selective weights that are specifically\nresponsible for memorizing the sensitive training data. We show how a malicious\nclient can leak the privacy-sensitive data of some other user in FL even\nwithout any cooperation from the server. Our best-performing method improves\nthe membership inference recall by 29% and achieves up to 70% private data\nreconstruction, evidently outperforming existing attacks with stronger\nassumptions of adversary capabilities.",
            "author": [
                "Md Rafi Ur Rashid",
                "Vishnu Asutosh Dasu",
                "Kang Gu",
                "Najrin Sultana",
                "Shagufta Mehnaz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16152v1",
                "http://arxiv.org/pdf/2310.16152v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16148v1",
            "title": "Yin Yang Convolutional Nets: Image Manifold Extraction by the Analysis\n  of Opposites",
            "updated": "2023-10-24T19:48:07Z",
            "published": "2023-10-24T19:48:07Z",
            "summary": "Computer vision in general presented several advances such as training\noptimizations, new architectures (pure attention, efficient block, vision\nlanguage models, generative models, among others). This have improved\nperformance in several tasks such as classification, and others. However, the\nmajority of these models focus on modifications that are taking distance from\nrealistic neuroscientific approaches related to the brain. In this work, we\nadopt a more bio-inspired approach and present the Yin Yang Convolutional\nNetwork, an architecture that extracts visual manifold, its blocks are intended\nto separate analysis of colors and forms at its initial layers, simulating\noccipital lobe's operations. Our results shows that our architecture provides\nState-of-the-Art efficiency among low parameter architectures in the dataset\nCIFAR-10. Our first model reached 93.32\\% test accuracy, 0.8\\% more than the\nolder SOTA in this category, while having 150k less parameters (726k in total).\nOur second model uses 52k parameters, losing only 3.86\\% test accuracy. We also\nperformed an analysis on ImageNet, where we reached 66.49\\% validation accuracy\nwith 1.6M parameters. We make the code publicly available at:\nhttps://github.com/NoSavedDATA/YinYang_CNN.",
            "author": [
                "Augusto Seben da Rosa",
                "Frederico Santos de Oliveira",
                "Anderson da Silva Soares",
                "Arnaldo Candido Junior"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16148v1",
                "http://arxiv.org/pdf/2310.16148v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "I.2.10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16147v2",
            "title": "PreWoMe: Exploiting Presuppositions as Working Memory for Long Form\n  Question Answering",
            "updated": "2023-11-14T02:46:32Z",
            "published": "2023-10-24T19:47:26Z",
            "summary": "Information-seeking questions in long-form question answering (LFQA) often\nprove misleading due to ambiguity or false presupposition in the question.\nWhile many existing approaches handle misleading questions, they are tailored\nto limited questions, which are insufficient in a real-world setting with\nunpredictable input characteristics. In this work, we propose PreWoMe, a\nunified approach capable of handling any type of information-seeking question.\nThe key idea of PreWoMe involves extracting presuppositions in the question and\nexploiting them as working memory to generate feedback and action about the\nquestion. Our experiment shows that PreWoMe is effective not only in tackling\nmisleading questions but also in handling normal ones, thereby demonstrating\nthe effectiveness of leveraging presuppositions, feedback, and action for\nreal-world QA settings.",
            "author": [
                "Wookje Han",
                "Jinsol Park",
                "Kyungjae Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16147v2",
                "http://arxiv.org/pdf/2310.16147v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.18364v1",
            "title": "From Heuristic to Analytic: Cognitively Motivated Strategies for\n  Coherent Physical Commonsense Reasoning",
            "updated": "2023-10-24T19:46:04Z",
            "published": "2023-10-24T19:46:04Z",
            "summary": "Pre-trained language models (PLMs) have shown impressive performance in\nvarious language tasks. However, they are prone to spurious correlations, and\noften generate illusory information. In real-world applications, PLMs should\njustify decisions with formalized, coherent reasoning chains, but this\nchallenge remains under-explored. Cognitive psychology theorizes that humans\nare capable of utilizing fast and intuitive heuristic thinking to make\ndecisions based on past experience, then rationalizing the decisions through\nslower and deliberative analytic reasoning. We incorporate these interlinked\ndual processes in fine-tuning and in-context learning with PLMs, applying them\nto two language understanding tasks that require coherent physical commonsense\nreasoning. We show that our proposed Heuristic-Analytic Reasoning (HAR)\nstrategies drastically improve the coherence of rationalizations for model\ndecisions, yielding state-of-the-art results on Tiered Reasoning for Intuitive\nPhysics (TRIP). We also find that this improved coherence is a direct result of\nmore faithful attention to relevant language context in each step of reasoning.\nOur findings suggest that human-like reasoning strategies can effectively\nimprove the coherence and reliability of PLM reasoning.",
            "author": [
                "Zheyuan Zhang",
                "Shane Storks",
                "Fengyuan Hu",
                "Sungryull Sohn",
                "Moontae Lee",
                "Honglak Lee",
                "Joyce Chai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18364v1",
                "http://arxiv.org/pdf/2310.18364v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16146v1",
            "title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model\n  System for Answering Medical Questions using Scientific Literature",
            "updated": "2023-10-24T19:43:39Z",
            "published": "2023-10-24T19:43:39Z",
            "summary": "The quickly-expanding nature of published medical literature makes it\nchallenging for clinicians and researchers to keep up with and summarize\nrecent, relevant findings in a timely manner. While several closed-source\nsummarization tools based on large language models (LLMs) now exist, rigorous\nand systematic evaluations of their outputs are lacking. Furthermore, there is\na paucity of high-quality datasets and appropriate benchmark tasks with which\nto evaluate these tools. We address these issues with four contributions: we\nrelease Clinfo.ai, an open-source WebApp that answers clinical questions based\non dynamically retrieved scientific literature; we specify an information\nretrieval and abstractive summarization task to evaluate the performance of\nsuch retrieval-augmented LLM systems; we release a dataset of 200 questions and\ncorresponding answers derived from published systematic reviews, which we name\nPubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for\nClinfo.ai and other publicly available OpenQA systems on PubMedRS-200.",
            "author": [
                "Alejandro Lozano",
                "Scott L Fleming",
                "Chia-Chun Chiang",
                "Nigam Shah"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16146v1",
                "http://arxiv.org/pdf/2310.16146v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16145v2",
            "title": "Positive Almost-Sure Termination -- Complexity and Proof Rules",
            "updated": "2023-10-26T18:03:53Z",
            "published": "2023-10-24T19:41:00Z",
            "summary": "We study the recursion-theoretic complexity of Positive Almost-Sure\nTermination ($\\mathsf{PAST}$) in an imperative programming language with\nrational variables, bounded nondeterministic choice, and discrete probabilistic\nchoice. A program terminates positive almost-surely if, for every scheduler,\nthe program terminates almost-surely and the expected runtime to termination is\nfinite. We show that $\\mathsf{PAST}$ for our language is complete for the\n(lightface) co-analytic sets ($\\Pi^1_1$-complete). This is in contrast to the\nrelated notions of Almost-Sure Termination ($\\mathsf{AST}$) and Bounded\nTermination ($\\mathsf{BAST}$), both of which are arithmetical ($\\Pi^0_2$ and\n$\\Sigma^0_2$ complete respectively).\n  Our upper bound implies an effective procedure to reduce reasoning about\nprobabilistic termination to non-probabilistic fair termination in a model with\nbounded nondeterminism, and to simple program termination in models with\nunbounded nondeterminism. Our lower bound shows the opposite: for every program\nwith unbounded nondeterministic choice, there is an effectively computable\nprobabilistic program with bounded choice such that the original program is\nterminating $iff$ the transformed program is $\\mathsf{PAST}$.\n  We show that every program has an effectively computable normal form, in\nwhich each probabilistic choice either continues or terminates execution\nimmediately, each with probability $1/2$. For normal form programs, we provide\na sound and complete proof rule for $\\mathsf{PAST}$. Our proof rule uses\ntransfinite ordinals. We show that reasoning about $\\mathsf{PAST}$ requires\ntransfinite ordinals up to $\\omega^{CK}_1$; thus, existing techniques for\nprobabilistic termination based on ranking supermartingales that map program\nstates to reals do not suffice to reason about $\\mathsf{PAST}$.",
            "author": [
                "Rupak Majumdar",
                "V. R. Sathiyanarayana"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16145v2",
                "http://arxiv.org/pdf/2310.16145v2"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL",
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16144v1",
            "title": "ROM-Based Stochastic Optimization for a Continuous Manufacturing Process",
            "updated": "2023-10-24T19:38:36Z",
            "published": "2023-10-24T19:38:36Z",
            "summary": "This paper proposes a model-based optimization method for the production of\nautomotive seals in an extrusion process. The high production throughput,\ncoupled with quality constraints and the inherent uncertainty of the process,\nencourages the search for operating conditions that minimize nonconformities.\nThe main uncertainties arise from the process variability and from the raw\nmaterial itself. The proposed method, based on Bayesian optimization, takes\nthese factors into account and obtains a robust set of process parameters. Due\nto the high computational cost and complexity of performing detailed\nsimulations, a reduced order model is used to address the optimization. The\nproposal has been evaluated in a virtual environment where it is shown that the\nperformance of the solution found minimizes the effects of process\nuncertainties.",
            "author": [
                "Raul Cruz-Oliver",
                "Luis Monzon",
                "Edgar Ramirez-Laboreo",
                "Jose-Manuel Rodriguez-Fortun"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16144v1",
                "http://arxiv.org/pdf/2310.16144v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16142v1",
            "title": "A Language Model with Limited Memory Capacity Captures Interference in\n  Human Sentence Processing",
            "updated": "2023-10-24T19:33:27Z",
            "published": "2023-10-24T19:33:27Z",
            "summary": "Two of the central factors believed to underpin human sentence processing\ndifficulty are expectations and retrieval from working memory. A recent attempt\nto create a unified cognitive model integrating these two factors relied on the\nparallels between the self-attention mechanism of transformer language models\nand cue-based retrieval theories of working memory in human sentence processing\n(Ryu and Lewis 2021). While Ryu and Lewis show that attention patterns in\nspecialized attention heads of GPT-2 are consistent with similarity-based\ninterference, a key prediction of cue-based retrieval models, their method\nrequires identifying syntactically specialized attention heads, and makes the\ncognitively implausible assumption that hundreds of memory retrieval operations\ntake place in parallel. In the present work, we develop a recurrent neural\nlanguage model with a single self-attention head, which more closely parallels\nthe memory system assumed by cognitive theories. We show that our model's\nsingle attention head captures semantic and syntactic interference effects\nobserved in human experiments.",
            "author": [
                "William Timkey",
                "Tal Linzen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16142v1",
                "http://arxiv.org/pdf/2310.16142v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.19821v1",
            "title": "A Risk-Averse Framework for Non-Stationary Stochastic Multi-Armed\n  Bandits",
            "updated": "2023-10-24T19:29:13Z",
            "published": "2023-10-24T19:29:13Z",
            "summary": "In a typical stochastic multi-armed bandit problem, the objective is often to\nmaximize the expected sum of rewards over some time horizon $T$. While the\nchoice of a strategy that accomplishes that is optimal with no additional\ninformation, it is no longer the case when provided additional\nenvironment-specific knowledge. In particular, in areas of high volatility like\nhealthcare or finance, a naive reward maximization approach often does not\naccurately capture the complexity of the learning problem and results in\nunreliable solutions. To tackle problems of this nature, we propose a framework\nof adaptive risk-aware strategies that operate in non-stationary environments.\nOur framework incorporates various risk measures prevalent in the literature to\nmap multiple families of multi-armed bandit algorithms into a risk-sensitive\nsetting. In addition, we equip the resulting algorithms with the Restarted\nBayesian Online Change-Point Detection (R-BOCPD) algorithm and impose a\n(tunable) forced exploration strategy to detect local (per-arm) switches. We\nprovide finite-time theoretical guarantees and an asymptotic regret bound of\norder $\\tilde O(\\sqrt{K_T T})$ up to time horizon $T$ with $K_T$ the total\nnumber of change-points. In practice, our framework compares favorably to the\nstate-of-the-art in both synthetic and real-world environments and manages to\nperform efficiently with respect to both risk-sensitivity and non-stationarity.",
            "author": [
                "Reda Alami",
                "Mohammed Mahfoud",
                "Mastane Achab"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19821v1",
                "http://arxiv.org/pdf/2310.19821v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16140v1",
            "title": "IA Para el Mantenimiento Predictivo en Canteras: Modelado",
            "updated": "2023-10-24T19:27:50Z",
            "published": "2023-10-24T19:27:50Z",
            "summary": "Dependence on raw materials, especially in the mining sector, is a key part\nof today's economy. Aggregates are vital, being the second most used raw\nmaterial after water. Digitally transforming this sector is key to optimizing\noperations. However, supervision and maintenance (predictive and corrective)\nare challenges little explored in this sector, due to the particularities of\nthe sector, machinery and environmental conditions. All this, despite the\nsuccesses achieved in other scenarios in monitoring with acoustic and contact\nsensors. We present an unsupervised learning scheme that trains a variational\nautoencoder model on a set of sound records. This is the first such dataset\ncollected during processing plant operations, containing information from\ndifferent points of the processing line. Our results demonstrate the model's\nability to reconstruct and represent in latent space the recorded sounds, the\ndifferences in operating conditions and between different equipment. In the\nfuture, this should facilitate the classification of sounds, as well as the\ndetection of anomalies and degradation patterns in the operation of the\nmachinery.",
            "author": [
                "Fernando Marcos",
                "Rodrigo Tamaki",
                "Mateo C\u00e1mara",
                "Virginia Yag\u00fce",
                "Jos\u00e9 Luis Blanco"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16140v1",
                "http://arxiv.org/pdf/2310.16140v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16139v1",
            "title": "Pix2HDR -- A pixel-wise acquisition and deep learning-based synthesis\n  approach for high-speed HDR videos",
            "updated": "2023-10-24T19:27:35Z",
            "published": "2023-10-24T19:27:35Z",
            "summary": "Accurately capturing dynamic scenes with wide-ranging motion and light\nintensity is crucial for many vision applications. However, acquiring\nhigh-speed high dynamic range (HDR) video is challenging because the camera's\nframe rate restricts its dynamic range. Existing methods sacrifice speed to\nacquire multi-exposure frames. Yet, misaligned motion in these frames can still\npose complications for HDR fusion algorithms, resulting in artifacts. Instead\nof frame-based exposures, we sample the videos using individual pixels at\nvarying exposures and phase offsets. Implemented on a pixel-wise programmable\nimage sensor, our sampling pattern simultaneously captures fast motion at a\nhigh dynamic range. We then transform pixel-wise outputs into an HDR video\nusing end-to-end learned weights from deep neural networks, achieving high\nspatiotemporal resolution with minimized motion blurring. We demonstrate\naliasing-free HDR video acquisition at 1000 FPS, resolving fast motion under\nlow-light conditions and against bright backgrounds - both challenging\nconditions for conventional cameras. By combining the versatility of pixel-wise\nsampling patterns with the strength of deep neural networks at decoding complex\nscenes, our method greatly enhances the vision system's adaptability and\nperformance in dynamic conditions.",
            "author": [
                "Caixin Wang",
                "Jie Zhang",
                "Matthew A. Wilson",
                "Ralph Etienne-Cummings"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16139v1",
                "http://arxiv.org/pdf/2310.16139v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16138v1",
            "title": "Subtle Signals: Video-based Detection of Infant Non-nutritive Sucking as\n  a Neurodevelopmental Cue",
            "updated": "2023-10-24T19:26:07Z",
            "published": "2023-10-24T19:26:07Z",
            "summary": "Non-nutritive sucking (NNS), which refers to the act of sucking on a\npacifier, finger, or similar object without nutrient intake, plays a crucial\nrole in assessing healthy early development. In the case of preterm infants,\nNNS behavior is a key component in determining their readiness for feeding. In\nolder infants, the characteristics of NNS behavior offer valuable insights into\nneural and motor development. Additionally, NNS activity has been proposed as a\npotential safeguard against sudden infant death syndrome (SIDS). However, the\nclinical application of NNS assessment is currently hindered by labor-intensive\nand subjective finger-in-mouth evaluations. Consequently, researchers often\nresort to expensive pressure transducers for objective NNS signal measurement.\nTo enhance the accessibility and reliability of NNS signal monitoring for both\nclinicians and researchers, we introduce a vision-based algorithm designed for\nnon-contact detection of NNS activity using baby monitor footage in natural\nsettings. Our approach involves a comprehensive exploration of optical flow and\ntemporal convolutional networks, enabling the detection and amplification of\nsubtle infant-sucking signals. We successfully classify short video clips of\nuniform length into NNS and non-NNS periods. Furthermore, we investigate manual\nand learning-based techniques to piece together local classification results,\nfacilitating the segmentation of longer mixed-activity videos into NNS and\nnon-NNS segments of varying duration. Our research introduces two novel\ndatasets of annotated infant videos, including one sourced from our clinical\nstudy featuring 19 infant subjects and 183 hours of overnight baby monitor\nfootage.",
            "author": [
                "Shaotong Zhu",
                "Michael Wan",
                "Sai Kumar Reddy Manne",
                "Emily Zimmerman",
                "Sarah Ostadabbas"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16138v1",
                "http://arxiv.org/pdf/2310.16138v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16137v2",
            "title": "Codebook-based Uplink Transmission Enhancement in 5G Advanced: Sub-band\n  Precoding",
            "updated": "2023-10-30T02:27:33Z",
            "published": "2023-10-24T19:25:11Z",
            "summary": "The transformative enhancements of fifth-generation (5G) mobile devices bring\nabout new challenges to achieve better uplink (UL) performance. Particularly,\nin codebook-based transmission, the wide-band (WB) precoding and the legacy UL\ncodebook may become main bottlenecks for higher efficient data transmission. In\nthis paper, we investigate the codebook-based UL single-layer transmission\nperformance using fully coherent antenna ports in the context of sub-band (SB)\nprecoding. We analyze the SB precoder selection criteria and design an UL\ncodebook used for SB precoding by increasing the number of relative phase\nshifts of each port. Via link-level simulations, we verify that the UL SB\nprecoding can improve up to 2 dB performance gain in terms of the block error\nrate (BLER) compared with the UL WB precoding which is the current UL precoding\nscheme. We also show that UL performance gain is sensitive to the SB size\nselection as well as the relative phase shift diversity.",
            "author": [
                "Liu Cao",
                "Yahia Shabara",
                "Parisa Cheraghi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16137v2",
                "http://arxiv.org/pdf/2310.16137v2"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16135v1",
            "title": "Can You Follow Me? Testing Situational Understanding in ChatGPT",
            "updated": "2023-10-24T19:22:01Z",
            "published": "2023-10-24T19:22:01Z",
            "summary": "Understanding sentence meanings and updating information states appropriately\nacross time -- what we call \"situational understanding\" (SU) -- is a critical\nability for human-like AI agents. SU is essential in particular for chat\nmodels, such as ChatGPT, to enable consistent, coherent, and effective dialogue\nbetween humans and AI. Previous works have identified certain SU limitations in\nnon-chatbot Large Language models (LLMs), but the extent and causes of these\nlimitations are not well understood, and capabilities of current chat-based\nmodels in this domain have not been explored. In this work we tackle these\nquestions, proposing a novel synthetic environment for SU testing which allows\nus to do controlled and systematic testing of SU in chat-oriented models,\nthrough assessment of models' ability to track and enumerate environment\nstates. Our environment also allows for close analysis of dynamics of model\nperformance, to better understand underlying causes for performance patterns.\nWe apply our test to ChatGPT, the state-of-the-art chatbot, and find that\ndespite the fundamental simplicity of the task, the model's performance\nreflects an inability to retain correct environment states across time. Our\nfollow-up analyses suggest that performance degradation is largely because\nChatGPT has non-persistent in-context memory (although it can access the full\ndialogue history) and it is susceptible to hallucinated updates -- including\nupdates that artificially inflate accuracies. Our findings suggest overall that\nChatGPT is not currently equipped for robust tracking of situation states, and\nthat trust in the impressive dialogue performance of ChatGPT comes with risks.\nWe release the codebase for reproducing our test environment, as well as all\nprompts and API responses from ChatGPT, at\nhttps://github.com/yangalan123/SituationalTesting.",
            "author": [
                "Chenghao Yang",
                "Allyson Ettinger"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16135v1",
                "http://arxiv.org/pdf/2310.16135v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16134v1",
            "title": "The Evolution from Design to Verification of the Antenna System and\n  Mechanisms in the AcubeSAT mission",
            "updated": "2023-10-24T19:17:45Z",
            "published": "2023-10-24T19:17:45Z",
            "summary": "AcubeSAT is an open-source CubeSat mission aiming to explore the effects of\nmicrogravity and radiation on eukaryotic cells using a compact microfluidic LoC\nplatform. It is developed by SpaceDot, a volunteer, interdisciplinary student\nteam at the Aristotle University of Thessaloniki and supported by the \"Fly Your\nSatellite! 3\" program of the ESA Education Office. The scientific data of the\nmission is comprised of microscope images captured through the on-board\nintegrated camera setup. As the total size of the payload data is expected to\nbe close to 2GB over 12 months, a fast and efficient downlink fulfilling the\nrestrictive power, cost and complexity budgets is required. Currently, there is\nno open-source communications system design which fully supports these specific\nconstraints, so we opted to develop our own solutions. The antenna system\nunderwent multiple iterations as the design matured, a process highly aided by\nthe feedback received from the ESA experts. The final communications system\nconfiguration consists of an S-band microstrip antenna operating at 2.4GHz and\na UHF deployable antenna, for the payload data and TM&TC respectively, both\nin-house designed. In this paper, we will present AcubeSAT's antenna system\niterations that span over 3 years, as well as the rationale and analysis\nresults behind each. The development decisions will be highlighted throughout\nthe paper in an effort to aid in the future development of such a low-cost\nCubeSat mission communications system.",
            "author": [
                "Panagiotis Bountzioukas",
                "Georgios Kikas",
                "Christoforos Tsiolakis",
                "Dimitrios Stoupis",
                "Eleftheria Chatziargyriou",
                "Alkis Hatzopoulos",
                "Vasiliki Kourampa-Gottfroh",
                "Ilektra Karakosta-Amarantidou",
                "Aggelos Mavropoulos",
                "Ioannis-Nikolaos Komis",
                "Afroditi Kita",
                "David Palma",
                "Loris Franchi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16134v1",
                "http://arxiv.org/pdf/2310.16134v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "astro-ph.IM",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16131v1",
            "title": "GenKIE: Robust Generative Multimodal Document Key Information Extraction",
            "updated": "2023-10-24T19:12:56Z",
            "published": "2023-10-24T19:12:56Z",
            "summary": "Key information extraction (KIE) from scanned documents has gained increasing\nattention because of its applications in various domains. Although promising\nresults have been achieved by some recent KIE approaches, they are usually\nbuilt based on discriminative models, which lack the ability to handle optical\ncharacter recognition (OCR) errors and require laborious token-level labelling.\nIn this paper, we propose a novel generative end-to-end model, named GenKIE, to\naddress the KIE task. GenKIE is a sequence-to-sequence multimodal generative\nmodel that utilizes multimodal encoders to embed visual, layout and textual\nfeatures and a decoder to generate the desired output. Well-designed prompts\nare leveraged to incorporate the label semantics as the weakly supervised\nsignals and entice the generation of the key information. One notable advantage\nof the generative model is that it enables automatic correction of OCR errors.\nBesides, token-level granular annotation is not required. Extensive experiments\non multiple public real-world datasets show that GenKIE effectively generalizes\nover different types of documents and achieves state-of-the-art results. Our\nexperiments also validate the model's robustness against OCR errors, making\nGenKIE highly applicable in real-world scenarios.",
            "author": [
                "Panfeng Cao",
                "Ye Wang",
                "Qiang Zhang",
                "Zaiqiao Meng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16131v1",
                "http://arxiv.org/pdf/2310.16131v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16127v1",
            "title": "Octopus: A Multitask Model and Toolkit for Arabic Natural Language\n  Generation",
            "updated": "2023-10-24T19:06:55Z",
            "published": "2023-10-24T19:06:55Z",
            "summary": "Understanding Arabic text and generating human-like responses is a\nchallenging endeavor. While many researchers have proposed models and solutions\nfor individual problems, there is an acute shortage of a comprehensive Arabic\nnatural language generation toolkit that is capable of handling a wide range of\ntasks. In this work, we present a novel Arabic text-to-text Transformer model,\nnamely AraT5v2. Our new model is methodically trained on extensive and diverse\ndata, utilizing an extended sequence length of 2,048 tokens. We explore various\npretraining strategies including unsupervised, supervised, and joint\npertaining, under both single and multitask settings. Our models outperform\ncompetitive baselines with large margins. We take our work one step further by\ndeveloping and publicly releasing Octopus, a Python-based package and\ncommand-line toolkit tailored for eight Arabic generation tasks all exploiting\na single model. We release the models and the toolkit on our public repository.",
            "author": [
                "AbdelRahim Elmadany",
                "El Moatez Billah Nagoudi",
                "Muhammad Abdul-Mageed"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16127v1",
                "http://arxiv.org/pdf/2310.16127v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16125v2",
            "title": "Online Two-stage Thermal History Prediction Method for Metal Additive\n  Manufacturing of Thin Walls",
            "updated": "2023-11-17T19:46:15Z",
            "published": "2023-10-24T18:58:36Z",
            "summary": "This paper aims to propose an online two-stage thermal history prediction\nmethod, which could be integrated into a metal AM process for performance\ncontrol. Based on the similarity of temperature curves (curve segments of a\ntemperature profile of one point) between any two successive layers, the first\nstage of the proposed method designs a layer-to-layer prediction model to\nestimate the temperature curves of the yet-to-print layer from measured\ntemperatures of certain points on the previously printed layer. With\nmeasured/predicted temperature profiles of several points on the same layer,\nthe second stage proposes a reduced order model (ROM) (intra-layer prediction\nmodel) to decompose and construct the temperature profiles of all points on the\nsame layer, which could be used to build the temperature field of the entire\nlayer. The training of ROM is performed with an extreme learning machine (ELM)\nfor computational efficiency. Fifteen wire arc AM experiments and nine\nsimulations are designed for thin walls with a fixed length and unidirectional\nprinting of each layer. The test results indicate that the proposed prediction\nmethod could construct the thermal history of a yet-to-print layer within 0.1\nseconds on a low-cost desktop computer. Meanwhile, the method has acceptable\ngeneralization capability in most cases from lower layers to higher layers in\nthe same simulation, as well as from one simulation to a new simulation on\ndifferent AM process parameters. More importantly, after fine-tuning the\nproposed method with limited experimental data, the relative errors of all\npredicted temperature profiles on a new experiment are smaller than 0.09, which\ndemonstrates the applicability and generalization of the proposed two-stage\nthermal history prediction method in online applications for metal AM.",
            "author": [
                "Yifan Tang",
                "M. Rahmani Dehaghani",
                "Pouyan Sajadi",
                "Shahriar Bakrani Balani",
                "Akshay Dhalpe",
                "Suraj Panicker",
                "Di Wu",
                "Eric Coatanea",
                "G. Gary Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16125v2",
                "http://arxiv.org/pdf/2310.16125v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16124v1",
            "title": "Dwindling Surface Cooling of a Rotating Jovian Planet Leads to a\n  Convection Zone that Grows to a Finite Depth",
            "updated": "2023-10-24T18:55:32Z",
            "published": "2023-10-24T18:55:32Z",
            "summary": "Recent measurements of Jupiter's gravitational field (by Juno) and seismology\nof Saturn's rings (by Cassini) strongly suggest that both planets have a\nstably-stratified core that still possesses a primordial gradient in the\nconcentration of heavy elements. The existence of such a \"diffusely\" stratified\ncore has been a surprise as it was long expected that the Jovian planets should\nbe fully convective and hence fully mixed. A vigorous zone of convection,\ndriven by surface cooling, forms at the surface and deepens through entrainment\nof fluid from underneath. In fact, it was believed that this convection zone\nshould grow so rapidly that the entire planet would be consumed in less than a\nmillion years. Here we suggest that two processes, acting in concert, present a\nsolution to this puzzle. All of the giant planets are rapidly rotating and have\na cooling rate that declines with time. Both of these effects reduce the rate\nof fluid entrainment into the convection zone. Through the use of an analytic\nprescription of entrainment in giant planets, we demonstrate that these two\neffects, rotation and dwindling surface cooling, result in a convection zone\nwhich initially grows but eventually stalls. The depth to which the convective\ninterface asymptotes depends on the rotation rate and on the stratification of\nthe stable interior. Conversely, in a nonrotating planet, or in a planet that\nmaintains a higher level of cooling than current models suggest, the convection\nzone deepens forever, eventually spanning the entire planet.",
            "author": [
                "Bradley W. Hindman",
                "J. R. Fuentes"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16124v1",
                "http://arxiv.org/pdf/2310.16124v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16123v1",
            "title": "Anchor Space Optimal Transport: Accelerating Batch Processing of\n  Multiple OT Problems",
            "updated": "2023-10-24T18:55:12Z",
            "published": "2023-10-24T18:55:12Z",
            "summary": "The optimal transport (OT) theory provides an effective way to compare\nprobability distributions on a defined metric space, but it suffers from cubic\ncomputational complexity. Although the Sinkhorn's algorithm greatly reduces the\ncomputational complexity of OT solutions, the solutions of multiple OT problems\nare still time-consuming and memory-comsuming in practice. However, many works\non the computational acceleration of OT are usually based on the premise of a\nsingle OT problem, ignoring the potential common characteristics of the\ndistributions in a mini-batch. Therefore, we propose a translated OT problem\ndesignated as the anchor space optimal transport (ASOT) problem, which is\nspecially designed for batch processing of multiple OT problem solutions. For\nthe proposed ASOT problem, the distributions will be mapped into a shared\nanchor point space, which learns the potential common characteristics and thus\nhelp accelerate OT batch processing. Based on the proposed ASOT, the\nWasserstein distance error to the original OT problem is proven to be bounded\nby ground cost errors. Building upon this, we propose three methods to learn an\nanchor space minimizing the distance error, each of which has its application\nbackground. Numerical experiments on real-world datasets show that our proposed\nmethods can greatly reduce computational time while maintaining reasonable\napproximation performance.",
            "author": [
                "Jianming Huang",
                "Xun Su",
                "Zhongxi Fang",
                "Hiroyuki Kasai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16123v1",
                "http://arxiv.org/pdf/2310.16123v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16122v1",
            "title": "A Performance-Portable SYCL Implementation of CRK-HACC for Exascale",
            "updated": "2023-10-24T18:52:24Z",
            "published": "2023-10-24T18:52:24Z",
            "summary": "The first generation of exascale systems will include a variety of machine\narchitectures, featuring GPUs from multiple vendors. As a result, many\ndevelopers are interested in adopting portable programming models to avoid\nmaintaining multiple versions of their code. It is necessary to document\nexperiences with such programming models to assist developers in understanding\nthe advantages and disadvantages of different approaches.\n  To this end, this paper evaluates the performance portability of a SYCL\nimplementation of a large-scale cosmology application (CRK-HACC) running on\nGPUs from three different vendors: AMD, Intel, and NVIDIA. We detail the\nprocess of migrating the original code from CUDA to SYCL and show that\nspecializing kernels for specific targets can greatly improve performance\nportability without significantly impacting programmer productivity. The SYCL\nversion of CRK-HACC achieves a performance portability of 0.96 with a code\ndivergence of almost 0, demonstrating that SYCL is a viable programming model\nfor performance-portable applications.",
            "author": [
                "Esteban M. Rangel",
                "S. John Pennycook",
                "Adrian Pope",
                "Nicholas Frontiere",
                "Zhiqiang Ma",
                "Varsha Madananth"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16122v1",
                "http://arxiv.org/pdf/2310.16122v1"
            ],
            "primary_category": "cs.PF",
            "category": [
                "cs.PF",
                "astro-ph.CO",
                "cs.DC",
                "D.2.7; D.2.8; D.1.3; J.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16120v1",
            "title": "Stereoscopic Depth Perception Through Foliage",
            "updated": "2023-10-24T18:48:20Z",
            "published": "2023-10-24T18:48:20Z",
            "summary": "Both humans and computational methods struggle to discriminate the depths of\nobjects hidden beneath foliage. However, such discrimination becomes feasible\nwhen we combine computational optical synthetic aperture sensing with the human\nability to fuse stereoscopic images. For object identification tasks, as\nrequired in search and rescue, wildlife observation, surveillance, and early\nwildfire detection, depth assists in differentiating true from false findings,\nsuch as people, animals, or vehicles vs. sun-heated patches at the ground level\nor in the tree crowns, or ground fires vs. tree trunks. We used video captured\nby a drone above dense woodland to test users' ability to discriminate depth.\nWe found that this is impossible when viewing monoscopic video and relying on\nmotion parallax. The same was true with stereoscopic video because of the\nocclusions caused by foliage. However, when synthetic aperture sensing was used\nto reduce occlusions and disparity-scaled stereoscopic video was presented,\nwhereas computational (stereoscopic matching) methods were unsuccessful, human\nobservers successfully discriminated depth. This shows the potential of systems\nwhich exploit the synergy between computational methods and human vision to\nperform tasks that neither can perform alone.",
            "author": [
                "Robert Kerschner",
                "Rakesh John Amala Arokia Nathan",
                "Rafal Mantiuk",
                "Oliver Bimber"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16120v1",
                "http://arxiv.org/pdf/2310.16120v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16117v1",
            "title": "NADI 2023: The Fourth Nuanced Arabic Dialect Identification Shared Task",
            "updated": "2023-10-24T18:41:24Z",
            "published": "2023-10-24T18:41:24Z",
            "summary": "We describe the findings of the fourth Nuanced Arabic Dialect Identification\nShared Task (NADI 2023). The objective of NADI is to help advance\nstate-of-the-art Arabic NLP by creating opportunities for teams of researchers\nto collaboratively compete under standardized conditions. It does so with a\nfocus on Arabic dialects, offering novel datasets and defining subtasks that\nallow for meaningful comparisons between different approaches. NADI 2023\ntargeted both dialect identification (Subtask 1) and dialect-to-MSA machine\ntranslation (Subtask 2 and Subtask 3). A total of 58 unique teams registered\nfor the shared task, of whom 18 teams have participated (with 76 valid\nsubmissions during test phase). Among these, 16 teams participated in Subtask\n1, 5 participated in Subtask 2, and 3 participated in Subtask 3. The winning\nteams achieved 87.27\n  F1 on Subtask 1, 14.76 Bleu in Subtask 2, and 21.10 Bleu in Subtask 3,\nrespectively. Results show that all three subtasks remain challenging, thereby\nmotivating future work in this area. We describe the methods employed by the\nparticipating teams and briefly offer an outlook for NADI.",
            "author": [
                "Muhammad Abdul-Mageed",
                "AbdelRahim Elmadany",
                "Chiyu Zhang",
                "El Moatez Billah Nagoudi",
                "Houda Bouamor",
                "Nizar Habash"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16117v1",
                "http://arxiv.org/pdf/2310.16117v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16115v1",
            "title": "Wakening Past Concepts without Past Data: Class-Incremental Learning\n  from Online Placebos",
            "updated": "2023-10-24T18:32:46Z",
            "published": "2023-10-24T18:32:46Z",
            "summary": "Not forgetting old class knowledge is a key challenge for class-incremental\nlearning (CIL) when the model continuously adapts to new classes. A common\ntechnique to address this is knowledge distillation (KD), which penalizes\nprediction inconsistencies between old and new models. Such prediction is made\nwith almost new class data, as old class data is extremely scarce due to the\nstrict memory limitation in CIL. In this paper, we take a deep dive into KD\nlosses and find that \"using new class data for KD\" not only hinders the model\nadaption (for learning new classes) but also results in low efficiency for\npreserving old class knowledge. We address this by \"using the placebos of old\nclasses for KD\", where the placebos are chosen from a free image stream, such\nas Google Images, in an automatical and economical fashion. To this end, we\ntrain an online placebo selection policy to quickly evaluate the quality of\nstreaming images (good or bad placebos) and use only good ones for one-time\nfeed-forward computation of KD. We formulate the policy training process as an\nonline Markov Decision Process (MDP), and introduce an online learning\nalgorithm to solve this MDP problem without causing much computation costs. In\nexperiments, we show that our method 1) is surprisingly effective even when\nthere is no class overlap between placebos and original old class data, 2) does\nnot require any additional supervision or memory budget, and 3) significantly\noutperforms a number of top-performing CIL methods, in particular when using\nlower memory budgets for old class exemplars, e.g., five exemplars per class.",
            "author": [
                "Yaoyao Liu",
                "Yingying Li",
                "Bernt Schiele",
                "Qianru Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16115v1",
                "http://arxiv.org/pdf/2310.16115v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16114v1",
            "title": "Granular packing simulation protocols: tap, press and relax",
            "updated": "2023-10-24T18:31:03Z",
            "published": "2023-10-24T18:31:03Z",
            "summary": "Granular matter takes many paths to pack. Gentle compression, compaction or\nrepetitive tapping can happen in natural and industrial processes. The path\ninfluences the packing microstructure, and thus macroscale properties,\nparticularly for frictional grains. We perform discrete element modeling\nsimulations to construct packings of frictional spheres implementing a range of\nstress-controlled protocols with 3D periodic boundary conditions. A\nvolume-controlled over-compression method is compared to four stress-controlled\nmethods, including over-compression and release, gentle under-compression and\ncyclical compression and release. The packing volume fraction of each method\ndepends on the pressure, initial kinetic energy and protocol parameters. A\nnon-monotonic pressure dependence in the volume fraction, but not the\ncoordination number occurs when dilute particles initialized with a non-zero\nkinetic energy are compressed, but can be reduced with the inclusion of drag.\nThe fraction of frictional contacts correlates with the volume fraction\nminimum. Packings were cyclically compressed 1000 times. Response to\ncompression depends on pressure; low pressure packings have a constant volume\nfraction regime, while high pressure packings continue to get dense with number\nof cycles. The capability of stress-controlled, bulk-like particle simulations\nto capture different protocols is showcased, and the ability to pack at low\npressures demonstrates unexpected behavior.",
            "author": [
                "A. P. Santos",
                "Ishan Srivastava",
                "Leonardo E. Silbert",
                "Jeremy B. Lechman",
                "Gary S. Grest"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16114v1",
                "http://arxiv.org/pdf/2310.16114v1"
            ],
            "primary_category": "cond-mat.soft",
            "category": [
                "cond-mat.soft"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16112v1",
            "title": "Towards long-tailed, multi-label disease classification from chest\n  X-ray: Overview of the CXR-LT challenge",
            "updated": "2023-10-24T18:26:22Z",
            "published": "2023-10-24T18:26:22Z",
            "summary": "Many real-world image recognition problems, such as diagnostic medical\nimaging exams, are \"long-tailed\" $\\unicode{x2013}$ there are a few common\nfindings followed by many more relatively rare conditions. In chest\nradiography, diagnosis is both a long-tailed and multi-label problem, as\npatients often present with multiple findings simultaneously. While researchers\nhave begun to study the problem of long-tailed learning in medical image\nrecognition, few have studied the interaction of label imbalance and label\nco-occurrence posed by long-tailed, multi-label disease classification. To\nengage with the research community on this emerging topic, we conducted an open\nchallenge, CXR-LT, on long-tailed, multi-label thorax disease classification\nfrom chest X-rays (CXRs). We publicly release a large-scale benchmark dataset\nof over 350,000 CXRs, each labeled with at least one of 26 clinical findings\nfollowing a long-tailed distribution. We synthesize common themes of\ntop-performing solutions, providing practical recommendations for long-tailed,\nmulti-label medical image classification. Finally, we use these insights to\npropose a path forward involving vision-language foundation models for few- and\nzero-shot disease classification.",
            "author": [
                "Gregory Holste",
                "Yiliang Zhou",
                "Song Wang",
                "Ajay Jaiswal",
                "Mingquan Lin",
                "Sherry Zhuge",
                "Yuzhe Yang",
                "Dongkyun Kim",
                "Trong-Hieu Nguyen-Mau",
                "Minh-Triet Tran",
                "Jaehyup Jeong",
                "Wongi Park",
                "Jongbin Ryu",
                "Feng Hong",
                "Arsh Verma",
                "Yosuke Yamagishi",
                "Changhyun Kim",
                "Hyeryeong Seo",
                "Myungjoo Kang",
                "Leo Anthony Celi",
                "Zhiyong Lu",
                "Ronald M. Summers",
                "George Shih",
                "Zhangyang Wang",
                "Yifan Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16112v1",
                "http://arxiv.org/pdf/2310.16112v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16111v2",
            "title": "Locally Differentially Private Document Generation Using Zero Shot\n  Prompting",
            "updated": "2023-11-30T18:13:01Z",
            "published": "2023-10-24T18:25:13Z",
            "summary": "Numerous studies have highlighted the privacy risks associated with\npretrained large language models. In contrast, our research offers a unique\nperspective by demonstrating that pretrained large language models can\neffectively contribute to privacy preservation. We propose a locally\ndifferentially private mechanism called DP-Prompt, which leverages the power of\npretrained large language models and zero-shot prompting to counter author\nde-anonymization attacks while minimizing the impact on downstream utility.\nWhen DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5),\nwe observe a notable reduction in the success rate of de-anonymization attacks,\nshowing that it surpasses existing approaches by a considerable margin despite\nits simpler design. For instance, in the case of the IMDB dataset, DP-Prompt\n(with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving\na 46\\% reduction in author identification F1 score against static attackers and\na 26\\% reduction against adaptive attackers. We conduct extensive experiments\nacross six open-source large language models, ranging up to 7 billion\nparameters, to analyze various effects of the privacy-utility tradeoff.",
            "author": [
                "Saiteja Utpala",
                "Sara Hooker",
                "Pin Yu Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16111v2",
                "http://arxiv.org/pdf/2310.16111v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16110v1",
            "title": "The New Physics Case for Beam-Dump Experiments with Accelerated Muon\n  Beams",
            "updated": "2023-10-24T18:22:12Z",
            "published": "2023-10-24T18:22:12Z",
            "summary": "As the field examines a future muon collider as a possible successor to the\nLHC, we must consider how to fully utilize not only the high-energy particle\ncollisions, but also any lower-energy staging facilities necessary in the R&D\nprocess. An economical and efficient possibility is to use the accelerated muon\nbeam from either the full experiment or from cooling and acceleration tests in\nbeam-dump experiments.Beam-dump experiments are complementary to the main\ncollider as they achieve sensitivity to very small couplings with minimal\ninstrumentation. We demonstrate the utility of muon beam-dump experiments for\nnew physics searches at energies from 10 GeV to 5 TeV. We find that, even at\nlow energies like those accessible at staging or demonstrator facilities, it is\npossible to probe new regions of parameter space for a variety of generic BSM\nmodels, including muonphilic, leptophilic, $L_\\mu - L_\\tau$, and dark photon\nscenarios. Such experiments could therefore provide opportunities for discovery\nof new physics well before the completion of the full multi-TeV collider.",
            "author": [
                "Cari Cesarotti",
                "Rikab Gambhir"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16110v1",
                "http://arxiv.org/pdf/2310.16110v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16109v1",
            "title": "Complex Image Generation SwinTransformer Network for Audio Denoising",
            "updated": "2023-10-24T18:21:03Z",
            "published": "2023-10-24T18:21:03Z",
            "summary": "Achieving high-performance audio denoising is still a challenging task in\nreal-world applications. Existing time-frequency methods often ignore the\nquality of generated frequency domain images. This paper converts the audio\ndenoising problem into an image generation task. We first develop a complex\nimage generation SwinTransformer network to capture more information from the\ncomplex Fourier domain. We then impose structure similarity and detailed loss\nfunctions to generate high-quality images and develop an SDR loss to minimize\nthe difference between denoised and clean audios. Extensive experiments on two\nbenchmark datasets demonstrate that our proposed model is better than\nstate-of-the-art methods.",
            "author": [
                "Youshan Zhang",
                "Jialu Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16109v1",
                "http://arxiv.org/pdf/2310.16109v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.CV",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16119v2",
            "title": "Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of\n  LLMs through a Global Scale Prompt Hacking Competition",
            "updated": "2023-11-30T23:15:04Z",
            "published": "2023-10-24T18:18:11Z",
            "summary": "Large Language Models (LLMs) are deployed in interactive contexts with direct\nuser engagement, such as chatbots and writing assistants. These deployments are\nvulnerable to prompt injection and jailbreaking (collectively, prompt hacking),\nin which models are manipulated to ignore their original instructions and\nfollow potentially malicious ones. Although widely acknowledged as a\nsignificant security threat, there is a dearth of large-scale resources and\nquantitative studies on prompt hacking. To address this lacuna, we launch a\nglobal prompt hacking competition, which allows for free-form human input\nattacks. We elicit 600K+ adversarial prompts against three state-of-the-art\nLLMs. We describe the dataset, which empirically verifies that current LLMs can\nindeed be manipulated via prompt hacking. We also present a comprehensive\ntaxonomical ontology of the types of adversarial prompts.",
            "author": [
                "Sander Schulhoff",
                "Jeremy Pinto",
                "Anaum Khan",
                "Louis-Fran\u00e7ois Bouchard",
                "Chenglei Si",
                "Svetlina Anati",
                "Valen Tagliabue",
                "Anson Liu Kost",
                "Christopher Carnahan",
                "Jordan Boyd-Graber"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16119v2",
                "http://arxiv.org/pdf/2311.16119v2"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16106v1",
            "title": "Decentralized Learning over Wireless Networks with Broadcast-Based\n  Subgraph Sampling",
            "updated": "2023-10-24T18:15:52Z",
            "published": "2023-10-24T18:15:52Z",
            "summary": "This work centers on the communication aspects of decentralized learning over\nwireless networks, using consensus-based decentralized stochastic gradient\ndescent (D-SGD). Considering the actual communication cost or delay caused by\nin-network information exchange in an iterative process, our goal is to achieve\nfast convergence of the algorithm measured by improvement per transmission\nslot. We propose BASS, an efficient communication framework for D-SGD over\nwireless networks with broadcast transmission and probabilistic subgraph\nsampling. In each iteration, we activate multiple subsets of non-interfering\nnodes to broadcast model updates to their neighbors. These subsets are randomly\nactivated over time, with probabilities reflecting their importance in network\nconnectivity and subject to a communication cost constraint (e.g., the average\nnumber of transmission slots per iteration). During the consensus update step,\nonly bi-directional links are effectively preserved to maintain communication\nsymmetry. In comparison to existing link-based scheduling methods, the inherent\nbroadcasting nature of wireless channels offers intrinsic advantages in\nspeeding up convergence of decentralized learning by creating more communicated\nlinks with the same number of transmission slots.",
            "author": [
                "Daniel P\u00e9rez Herrera",
                "Zheng Chen",
                "Erik G. Larsson"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16106v1",
                "http://arxiv.org/pdf/2310.16106v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC",
                "cs.IT",
                "cs.SY",
                "eess.SY",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16102v1",
            "title": "Learned, Uncertainty-driven Adaptive Acquisition for Photon-Efficient\n  Multiphoton Microscopy",
            "updated": "2023-10-24T18:06:03Z",
            "published": "2023-10-24T18:06:03Z",
            "summary": "Multiphoton microscopy (MPM) is a powerful imaging tool that has been a\ncritical enabler for live tissue imaging. However, since most multiphoton\nmicroscopy platforms rely on point scanning, there is an inherent trade-off\nbetween acquisition time, field of view (FOV), phototoxicity, and image\nquality, often resulting in noisy measurements when fast, large FOV, and/or\ngentle imaging is needed. Deep learning could be used to denoise multiphoton\nmicroscopy measurements, but these algorithms can be prone to hallucination,\nwhich can be disastrous for medical and scientific applications. We propose a\nmethod to simultaneously denoise and predict pixel-wise uncertainty for\nmultiphoton imaging measurements, improving algorithm trustworthiness and\nproviding statistical guarantees for the deep learning predictions.\nFurthermore, we propose to leverage this learned, pixel-wise uncertainty to\ndrive an adaptive acquisition technique that rescans only the most uncertain\nregions of a sample. We demonstrate our method on experimental noisy MPM\nmeasurements of human endometrium tissues, showing that we can maintain fine\nfeatures and outperform other denoising methods while predicting uncertainty at\neach pixel. Finally, with our adaptive acquisition technique, we demonstrate a\n120X reduction in acquisition time and total light dose while successfully\nrecovering fine features in the sample. We are the first to demonstrate\ndistribution-free uncertainty quantification for a denoising task with real\nexperimental data and the first to propose adaptive acquisition based on\nreconstruction uncertainty",
            "author": [
                "Cassandra Tong Ye",
                "Jiashu Han",
                "Kunzan Liu",
                "Anastasios Angelopoulos",
                "Linda Griffith",
                "Kristina Monakhova",
                "Sixian You"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16102v1",
                "http://arxiv.org/pdf/2310.16102v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16100v1",
            "title": "Deep Feature Registration for Unsupervised Domain Adaptation",
            "updated": "2023-10-24T18:04:53Z",
            "published": "2023-10-24T18:04:53Z",
            "summary": "While unsupervised domain adaptation has been explored to leverage the\nknowledge from a labeled source domain to an unlabeled target domain, existing\nmethods focus on the distribution alignment between two domains. However, how\nto better align source and target features is not well addressed. In this\npaper, we propose a deep feature registration (DFR) model to generate\nregistered features that maintain domain invariant features and simultaneously\nminimize the domain-dissimilarity of registered features and target features\nvia histogram matching. We further employ a pseudo label refinement process,\nwhich considers both probabilistic soft selection and center-based hard\nselection to improve the quality of pseudo labels in the target domain.\nExtensive experiments on multiple UDA benchmarks demonstrate the effectiveness\nof our DFR model, resulting in new state-of-the-art performance.",
            "author": [
                "Youshan Zhang",
                "Brian D. Davison"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16100v1",
                "http://arxiv.org/pdf/2310.16100v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16095v1",
            "title": "CR-COPEC: Causal Rationale of Corporate Performance Changes to Learn\n  from Financial Reports",
            "updated": "2023-10-24T18:00:40Z",
            "published": "2023-10-24T18:00:40Z",
            "summary": "In this paper, we introduce CR-COPEC called Causal Rationale of Corporate\nPerformance Changes from financial reports. This is a comprehensive large-scale\ndomain-adaptation causal sentence dataset to detect financial performance\nchanges of corporate. CR-COPEC contributes to two major achievements. First, it\ndetects causal rationale from 10-K annual reports of the U.S. companies, which\ncontain experts' causal analysis following accounting standards in a formal\nmanner. This dataset can be widely used by both individual investors and\nanalysts as material information resources for investing and decision making\nwithout tremendous effort to read through all the documents. Second, it\ncarefully considers different characteristics which affect the financial\nperformance of companies in twelve industries. As a result, CR-COPEC can\ndistinguish causal sentences in various industries by taking unique narratives\nin each industry into consideration. We also provide an extensive analysis of\nhow well CR-COPEC dataset is constructed and suited for classifying target\nsentences as causal ones with respect to industry characteristics. Our dataset\nand experimental codes are publicly available.",
            "author": [
                "Ye Eun Chun",
                "Sunjae Kwon",
                "Kyunghwan Sohn",
                "Nakwon Sung",
                "Junyoup Lee",
                "Byungki Seo",
                "Kevin Compher",
                "Seung-won Hwang",
                "Jaesik Choi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16095v1",
                "http://arxiv.org/pdf/2310.16095v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16090v1",
            "title": "Smooth generalized symmetries of quantum field theories",
            "updated": "2023-10-24T18:00:07Z",
            "published": "2023-10-24T18:00:07Z",
            "summary": "Dynamical quantum field theories (QFTs), such as those in which spacetimes\nare equipped with a metric and/or a field in the form of a smooth map to a\ntarget manifold, can be formulated axiomatically using the language of\n$\\infty$-categories. According to a geometric version of the cobordism\nhypothesis, such QFTs collectively assemble themselves into objects in an\n$\\infty$-topos of smooth spaces. We show how this allows one to define and\nstudy generalized global symmetries of such QFTs. The symmetries are themselves\nsmooth, so the `higher-form' symmetry groups can be endowed with, e.g., a Lie\ngroup structure.\n  Among the more surprising general implications for physics are, firstly, that\nQFTs in spacetime dimension $d$, considered collectively, can have $d$-form\nsymmetries, going beyond the known $(d-1)$-form symmetries of individual QFTs\nand, secondly, that a global symmetry of a QFT can be anomalous even before we\ntry to gauge it, due to a failure to respect either smoothness (in that a\nsymmetry of an individual QFT does not smoothly extend to QFTs collectively) or\nlocality (in that a symmetry of an unextended QFT does not extend to an\nextended one).\n  Smoothness anomalies are shown to occur even in 2-state systems in quantum\nmechanics (here formulated axiomatically by equipping $d=1$ spacetimes with a\nmetric, an orientation, and perhaps some unitarity structure). Locality\nanomalies are shown to occur even for invertible QFTs defined on $d=1$\nspacetimes equipped with an orientation and a smooth map to a target manifold.\nThese correspond in physics to topological actions for a particle moving on the\ntarget and the relation to an earlier classification of such actions using\ninvariant differential cohomology is elucidated.",
            "author": [
                "Ben Gripaios",
                "Oscar Randal-Williams",
                "Joseph Tooby-Smith"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16090v1",
                "http://arxiv.org/pdf/2310.16090v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "math.AT",
                "math.CT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16082v1",
            "title": "Gauged cooling of topological excitations and emergent fermions on\n  quantum simulators",
            "updated": "2023-10-24T18:00:01Z",
            "published": "2023-10-24T18:00:01Z",
            "summary": "Simulated cooling is a robust method for preparing low-energy states of\nmany-body Hamiltonians on near-term quantum simulators. In such schemes, a\nsubset of the simulator's spins (or qubits) are treated as a \"bath,\" which\nextracts energy and entropy from the system of interest. However, such\nprotocols are inefficient when applied to systems whose excitations are highly\nnon-local in terms of the microscopic degrees of freedom, such as topological\nphases of matter; such excitations are difficult to extract by a local coupling\nto a bath. We explore a route to overcome this obstacle by encoding of the\nsystem's degrees of freedom into those of the quantum simulator in a non-local\nmanner. To illustrate the approach, we show how to efficiently cool the\nferromagnetic phase of the quantum Ising model, whose excitations are domain\nwalls, via a \"gauged cooling\" protocol in which the Ising spins are coupled to\na $Z_2$ gauge field that simultaneously acts as a reservoir for removing\nexcitations. We show that our protocol can prepare the ground states of the\nferromagnetic and paramagnetic phases equally efficiently. The gauged cooling\nprotocol naturally extends to (interacting) fermionic systems, where it is\nequivalent to cooling by coupling to a fermionic bath via single-fermion\nhopping.",
            "author": [
                "Gilad Kishony",
                "Mark S. Rudner",
                "Achim Rosch",
                "Erez Berg"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16082v1",
                "http://arxiv.org/pdf/2310.16082v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16049v1",
            "title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft\n  Reasoning",
            "updated": "2023-10-24T17:59:20Z",
            "published": "2023-10-24T17:59:20Z",
            "summary": "While large language models (LLMs) equipped with techniques like\nchain-of-thought prompting have demonstrated impressive capabilities, they\nstill fall short in their ability to reason robustly in complex settings.\nHowever, evaluating LLM reasoning is challenging because system capabilities\ncontinue to grow while benchmark datasets for tasks like logical deduction have\nremained static. We introduce MuSR, a dataset for evaluating language models on\nmultistep soft reasoning tasks specified in a natural language narrative. This\ndataset has two crucial features. First, it is created through a novel\nneurosymbolic synthetic-to-natural generation algorithm, enabling the\nconstruction of complex reasoning instances that challenge GPT-4 (e.g., murder\nmysteries roughly 1000 words in length) and which can be scaled further as more\ncapable LLMs are released. Second, our dataset instances are free text\nnarratives corresponding to real-world domains of reasoning; this makes it\nsimultaneously much more challenging than other synthetically-crafted\nbenchmarks while remaining realistic and tractable for human annotators to\nsolve with high accuracy. We evaluate a range of LLMs and prompting techniques\non this dataset and characterize the gaps that remain for techniques like\nchain-of-thought to perform robust reasoning.",
            "author": [
                "Zayne Sprague",
                "Xi Ye",
                "Kaj Bostrom",
                "Swarat Chaudhuri",
                "Greg Durrett"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16049v1",
                "http://arxiv.org/pdf/2310.16049v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16048v1",
            "title": "AI Alignment and Social Choice: Fundamental Limitations and Policy\n  Implications",
            "updated": "2023-10-24T17:59:04Z",
            "published": "2023-10-24T17:59:04Z",
            "summary": "Aligning AI agents to human intentions and values is a key bottleneck in\nbuilding safe and deployable AI applications. But whose values should AI agents\nbe aligned with? Reinforcement learning with human feedback (RLHF) has emerged\nas the key framework for AI alignment. RLHF uses feedback from human\nreinforcers to fine-tune outputs; all widely deployed large language models\n(LLMs) use RLHF to align their outputs to human values. It is critical to\nunderstand the limitations of RLHF and consider policy challenges arising from\nthese limitations. In this paper, we investigate a specific challenge in\nbuilding RLHF systems that respect democratic norms. Building on impossibility\nresults in social choice theory, we show that, under fairly broad assumptions,\nthere is no unique voting protocol to universally align AI systems using RLHF\nthrough democratic processes. Further, we show that aligning AI agents with the\nvalues of all individuals will always violate certain private ethical\npreferences of an individual user i.e., universal AI alignment using RLHF is\nimpossible. We discuss policy implications for the governance of AI systems\nbuilt using RLHF: first, the need for mandating transparent voting rules to\nhold model builders accountable. Second, the need for model builders to focus\non developing AI agents that are narrowly aligned to specific user groups.",
            "author": [
                "Abhilash Mishra"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16048v1",
                "http://arxiv.org/pdf/2310.16048v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.CY",
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16047v1",
            "title": "From Posterior Sampling to Meaningful Diversity in Image Restoration",
            "updated": "2023-10-24T17:58:54Z",
            "published": "2023-10-24T17:58:54Z",
            "summary": "Image restoration problems are typically ill-posed in the sense that each\ndegraded image can be restored in infinitely many valid ways. To accommodate\nthis, many works generate a diverse set of outputs by attempting to randomly\nsample from the posterior distribution of natural images given the degraded\ninput. Here we argue that this strategy is commonly of limited practical value\nbecause of the heavy tail of the posterior distribution. Consider for example\ninpainting a missing region of the sky in an image. Since there is a high\nprobability that the missing region contains no object but clouds, any set of\nsamples from the posterior would be entirely dominated by (practically\nidentical) completions of sky. However, arguably, presenting users with only\none clear sky completion, along with several alternative solutions such as\nairships, birds, and balloons, would better outline the set of possibilities.\nIn this paper, we initiate the study of meaningfully diverse image restoration.\nWe explore several post-processing approaches that can be combined with any\ndiverse image restoration method to yield semantically meaningful diversity.\nMoreover, we propose a practical approach for allowing diffusion based image\nrestoration methods to generate meaningfully diverse outputs, while incurring\nonly negligent computational overhead. We conduct extensive user studies to\nanalyze the proposed techniques, and find the strategy of reducing similarity\nbetween outputs to be significantly favorable over posterior sampling. Code and\nexamples are available in https://noa-cohen.github.io/MeaningfulDiversityInIR",
            "author": [
                "Noa Cohen",
                "Hila Manor",
                "Yuval Bahat",
                "Tomer Michaeli"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16047v1",
                "http://arxiv.org/pdf/2310.16047v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16045v1",
            "title": "Woodpecker: Hallucination Correction for Multimodal Large Language\n  Models",
            "updated": "2023-10-24T17:58:07Z",
            "published": "2023-10-24T17:58:07Z",
            "summary": "Hallucination is a big shadow hanging over the rapidly evolving Multimodal\nLarge Language Models (MLLMs), referring to the phenomenon that the generated\ntext is inconsistent with the image content. In order to mitigate\nhallucinations, existing studies mainly resort to an instruction-tuning manner\nthat requires retraining the models with specific data. In this paper, we pave\na different way, introducing a training-free method named Woodpecker. Like a\nwoodpecker heals trees, it picks out and corrects hallucinations from the\ngenerated text. Concretely, Woodpecker consists of five stages: key concept\nextraction, question formulation, visual knowledge validation, visual claim\ngeneration, and hallucination correction. Implemented in a post-remedy manner,\nWoodpecker can easily serve different MLLMs, while being interpretable by\naccessing intermediate outputs of the five stages. We evaluate Woodpecker both\nquantitatively and qualitatively and show the huge potential of this new\nparadigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement\nin accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released\nat https://github.com/BradyFU/Woodpecker.",
            "author": [
                "Shukang Yin",
                "Chaoyou Fu",
                "Sirui Zhao",
                "Tong Xu",
                "Hao Wang",
                "Dianbo Sui",
                "Yunhang Shen",
                "Ke Li",
                "Xing Sun",
                "Enhong Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16045v1",
                "http://arxiv.org/pdf/2310.16045v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16044v2",
            "title": "Stanford-ORB: A Real-World 3D Object Inverse Rendering Benchmark",
            "updated": "2023-10-25T16:40:59Z",
            "published": "2023-10-24T17:57:58Z",
            "summary": "We introduce Stanford-ORB, a new real-world 3D Object inverse Rendering\nBenchmark. Recent advances in inverse rendering have enabled a wide range of\nreal-world applications in 3D content generation, moving rapidly from research\nand commercial use cases to consumer devices. While the results continue to\nimprove, there is no real-world benchmark that can quantitatively assess and\ncompare the performance of various inverse rendering methods. Existing\nreal-world datasets typically only consist of the shape and multi-view images\nof objects, which are not sufficient for evaluating the quality of material\nrecovery and object relighting. Methods capable of recovering material and\nlighting often resort to synthetic data for quantitative evaluation, which on\nthe other hand does not guarantee generalization to complex real-world\nenvironments. We introduce a new dataset of real-world objects captured under a\nvariety of natural scenes with ground-truth 3D scans, multi-view images, and\nenvironment lighting. Using this dataset, we establish the first comprehensive\nreal-world evaluation benchmark for object inverse rendering tasks from\nin-the-wild scenes, and compare the performance of various existing methods.",
            "author": [
                "Zhengfei Kuang",
                "Yunzhi Zhang",
                "Hong-Xing Yu",
                "Samir Agarwala",
                "Shangzhe Wu",
                "Jiajun Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16044v2",
                "http://arxiv.org/pdf/2310.16044v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16042v2",
            "title": "WebWISE: Web Interface Control and Sequential Exploration with Large\n  Language Models",
            "updated": "2023-10-25T03:54:11Z",
            "published": "2023-10-24T17:57:03Z",
            "summary": "The paper investigates using a Large Language Model (LLM) to automatically\nperform web software tasks using click, scroll, and text input operations.\nPrevious approaches, such as reinforcement learning (RL) or imitation learning,\nare inefficient to train and task-specific. Our method uses filtered Document\nObject Model (DOM) elements as observations and performs tasks step-by-step,\nsequentially generating small programs based on the current observations. We\nuse in-context learning, either benefiting from a single manually provided\nexample, or an automatically generated example based on a successful zero-shot\ntrial. We evaluate the proposed method on the MiniWob++ benchmark. With only\none in-context example, our WebWISE method achieves similar or better\nperformance than other methods that require many demonstrations or trials.",
            "author": [
                "Heyi Tao",
                "Sethuraman T V",
                "Michal Shlapentokh-Rothman",
                "Derek Hoiem"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16042v2",
                "http://arxiv.org/pdf/2310.16042v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16040v1",
            "title": "Instruct and Extract: Instruction Tuning for On-Demand Information\n  Extraction",
            "updated": "2023-10-24T17:54:25Z",
            "published": "2023-10-24T17:54:25Z",
            "summary": "Large language models with instruction-following capabilities open the door\nto a wider group of users. However, when it comes to information extraction - a\nclassic task in natural language processing - most task-specific systems cannot\nalign well with long-tail ad hoc extraction use cases for non-expert users. To\naddress this, we propose a novel paradigm, termed On-Demand Information\nExtraction, to fulfill the personalized demands of real-world users. Our task\naims to follow the instructions to extract the desired content from the\nassociated text and present it in a structured tabular format. The table\nheaders can either be user-specified or inferred contextually by the model. To\nfacilitate research in this emerging area, we present a benchmark named\nInstructIE, inclusive of both automatically generated training data, as well as\nthe human-annotated test set. Building on InstructIE, we further develop an\nOn-Demand Information Extractor, ODIE. Comprehensive evaluations on our\nbenchmark reveal that ODIE substantially outperforms the existing open-source\nmodels of similar size. Our code and dataset are released on\nhttps://github.com/yzjiao/On-Demand-IE.",
            "author": [
                "Yizhu Jiao",
                "Ming Zhong",
                "Sha Li",
                "Ruining Zhao",
                "Siru Ouyang",
                "Heng Ji",
                "Jiawei Han"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16040v1",
                "http://arxiv.org/pdf/2310.16040v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16078v1",
            "title": "Identifying interphase vs mitotic cell cycle phases using oxidative\n  stress and a proximity-based null model",
            "updated": "2023-10-24T17:51:30Z",
            "published": "2023-10-24T17:51:30Z",
            "summary": "Detecting communities in large complex networks has found a wide range of\napplications in physical, biological, and social sciences by identifying\nmesoscopic groups based on the links between individual units. Moreover,\ncommunity detection approaches have been generalized to various data analysis\ntasks by constructing networks whose links depend on individual units'\nmeasurements. However, identifying well separated subpopulations in data sets,\ne.g., multimodality, still presents challenges due to both the inherent spatial\nnature of the resulting networks and the generic emergence of communities in\nsuch networks and the similarity between network structures and\ndistance-dependent null models. Here we introduce a new spatially informed null\nmodel for this task that takes into account spatial structure but does not\nexplicitly depend on distances between measurements. We find that community\ndetection using this null model successfully identifies subpopulations in\nmultimodal data and accurately does not for unimodal data. We apply this new\nnull model to the task of identifying interphase vs mitotic cell cycle phases\nin a group of Dictyostelium discoideum cells using measurements of oxidative\nstress, which have been shown to correlate strongly with cell cycle behaviors.",
            "author": [
                "Michelle Kovarik",
                "Tyler Allcroft",
                "Per Sebastian Skardal"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16078v1",
                "http://arxiv.org/pdf/2310.16078v1"
            ],
            "primary_category": "nlin.PS",
            "category": [
                "nlin.PS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16035v1",
            "title": "What's Left? Concept Grounding with Logic-Enhanced Foundation Models",
            "updated": "2023-10-24T17:50:20Z",
            "published": "2023-10-24T17:50:20Z",
            "summary": "Recent works such as VisProg and ViperGPT have smartly composed foundation\nmodels for visual reasoning-using large language models (LLMs) to produce\nprograms that can be executed by pre-trained vision-language models. However,\nthey operate in limited domains, such as 2D images, not fully exploiting the\ngeneralization of language: abstract concepts like \"left\" can also be grounded\nin 3D, temporal, and action data, as in moving to your left. This limited\ngeneralization stems from these inference-only methods' inability to learn or\nadapt pre-trained models to a new domain. We propose the Logic-Enhanced\nFoundation Model (LEFT), a unified framework that learns to ground and reason\nwith concepts across domains with a differentiable, domain-independent,\nfirst-order logic-based program executor. LEFT has an LLM interpreter that\noutputs a program represented in a general, logic-based reasoning language,\nwhich is shared across all domains and tasks. LEFT's executor then executes the\nprogram with trainable domain-specific grounding modules. We show that LEFT\nflexibly learns concepts in four domains: 2D images, 3D scenes, human motions,\nand robotic manipulation. It exhibits strong reasoning ability in a wide\nvariety of tasks, including those that are complex and not seen during\ntraining, and can be easily applied to new domains.",
            "author": [
                "Joy Hsu",
                "Jiayuan Mao",
                "Joshua B. Tenenbaum",
                "Jiajun Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16035v1",
                "http://arxiv.org/pdf/2310.16035v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16033v1",
            "title": "Visual Cropping Improves Zero-Shot Question Answering of Multimodal\n  Large Language Models",
            "updated": "2023-10-24T17:48:04Z",
            "published": "2023-10-24T17:48:04Z",
            "summary": "Multimodal Large Language Models (LLMs) have recently achieved promising\nzero-shot accuracy on visual question answering (VQA) -- a fundamental task\naffecting various downstream applications and domains. Given the great\npotential for the broad use of these models, it is important to investigate\ntheir limitations in dealing with different image and question properties. In\nthis work, we investigate whether multimodal LLMs can perceive small details as\nwell as large details in images. In particular, we show that their zero-shot\naccuracy in answering visual questions is very sensitive to the size of the\nvisual subject of the question, declining up to $46\\%$ with size. Furthermore,\nwe show that this effect is causal by observing that human visual cropping can\nsignificantly mitigate their sensitivity to size. Inspired by the usefulness of\nhuman cropping, we then propose three automatic visual cropping methods as\ninference time mechanisms to improve the zero-shot performance of multimodal\nLLMs. We study their effectiveness on four popular VQA datasets, and a subset\nof the VQAv2 dataset tailored towards fine visual details. Our findings suggest\nthat multimodal LLMs should be used with caution in detail-sensitive VQA\napplications, and that visual cropping is a promising direction to improve\ntheir zero-shot performance. Our code and data are publicly available.",
            "author": [
                "Jiarui Zhang",
                "Mahyar Khayatkhoei",
                "Prateek Chhikara",
                "Filip Ilievski"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16033v1",
                "http://arxiv.org/pdf/2310.16033v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16032v1",
            "title": "The Physics of (good) LDPC Codes I. Gauging and dualities",
            "updated": "2023-10-24T17:47:06Z",
            "published": "2023-10-24T17:47:06Z",
            "summary": "Low-depth parity check (LDPC) codes are a paradigm of error correction that\nallow for spatially non-local interactions between (qu)bits, while still\nenforcing that each (qu)bit interacts only with finitely many others. On\nexpander graphs, they can give rise to ``good codes'' that combine a finite\nencoding rate with an optimal scaling of the code distance, which governs the\ncode's robustness against noise. Such codes have garnered much recent attention\ndue to two breakthrough developments: the construction of good quantum LDPC\ncodes and good locally testable classical LDPC codes, using similar methods.\nHere we explore these developments from a physics lens, establishing\nconnections between LDPC codes and ordered phases of matter defined for systems\nwith non-local interactions and on non-Euclidean geometries. We generalize the\nphysical notions of Kramers-Wannier (KW) dualities and gauge theories to this\ncontext, using the notion of chain complexes as an organizing principle. We\ndiscuss gauge theories based on generic classical LDPC codes and make a\ndistinction between two classes, based on whether their excitations are\npoint-like or extended. For the former, we describe KW dualities, analogous to\nthe 1D Ising model and describe the role played by ``boundary conditions''. For\nthe latter we generalize Wegner's duality to obtain generic quantum LDPC codes\nwithin the deconfined phase of a Z_2 gauge theory. We show that all known\nexamples of good quantum LDPC codes are obtained by gauging locally testable\nclassical codes. We also construct cluster Hamiltonians from arbitrary\nclassical codes, related to the Higgs phase of the gauge theory, and formulate\ngeneralizations of the Kennedy-Tasaki duality transformation. We use the chain\ncomplex language to discuss edge modes and non-local order parameters for these\nmodels, initiating the study of SPT phases in non-Euclidean geometries.",
            "author": [
                "Tibor Rakovszky",
                "Vedika Khemani"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16032v1",
                "http://arxiv.org/pdf/2310.16032v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.stat-mech",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16031v1",
            "title": "Entanglement from superradiance and rotating quantum fluids of light",
            "updated": "2023-10-24T17:46:24Z",
            "published": "2023-10-24T17:46:24Z",
            "summary": "The amplification of radiation by superradiance is a universal phenomenon\nobserved in numerous physical systems. We demonstrate that superradiant\nscattering generates entanglement for different input states, including\ncoherent states, thereby revealing the inherently quantum nature of this\nphenomenon. To put these concepts to the test, we propose a novel approach to\ncreate horizonless ergoregions, which are nonetheless dynamically stable thanks\nto the dissipative dynamics of a polaritonic fluid of light. We numerically\nsimulate the system to demonstrate the creation of a stable ergoregion, and\nexperimentally realize a comparable configuration. Subsequently, we investigate\nrotational superradiance within this system, with a primary focus on\nentanglement generation and the possibilities for its enhancement using current\ntechniques. Our methods permit the investigation of quantum emission by\nrotational superradiance by controlling the input state at will.",
            "author": [
                "Adri\u00e0 Delhom",
                "Killian Guerrero",
                "Paula Calizaya",
                "K\u00e9vin Falque",
                "Anthony J. Brady",
                "Ivan Agullo",
                "Maxime J. Jacquet"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16031v1",
                "http://arxiv.org/pdf/2310.16031v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc",
                "cond-mat.quant-gas",
                "hep-th",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16028v1",
            "title": "What Algorithms can Transformers Learn? A Study in Length Generalization",
            "updated": "2023-10-24T17:43:29Z",
            "published": "2023-10-24T17:43:29Z",
            "summary": "Large language models exhibit surprising emergent generalization properties,\nyet also struggle on many simple reasoning tasks such as arithmetic and parity.\nThis raises the question of if and when Transformer models can learn the true\nalgorithm for solving a task. We study the scope of Transformers' abilities in\nthe specific setting of length generalization on algorithmic tasks. Here, we\npropose a unifying framework to understand when and how Transformers can\nexhibit strong length generalization on a given task. Specifically, we leverage\nRASP (Weiss et al., 2021) -- a programming language designed for the\ncomputational model of a Transformer -- and introduce the RASP-Generalization\nConjecture: Transformers tend to length generalize on a task if the task can be\nsolved by a short RASP program which works for all input lengths. This simple\nconjecture remarkably captures most known instances of length generalization on\nalgorithmic tasks. Moreover, we leverage our insights to drastically improve\ngeneralization performance on traditionally hard tasks (such as parity and\naddition). On the theoretical side, we give a simple example where the\n\"min-degree-interpolator\" model of learning from Abbe et al. (2023) does not\ncorrectly predict Transformers' out-of-distribution behavior, but our\nconjecture does. Overall, our work provides a novel perspective on the\nmechanisms of compositional generalization and the algorithmic capabilities of\nTransformers.",
            "author": [
                "Hattie Zhou",
                "Arwen Bradley",
                "Etai Littwin",
                "Noam Razin",
                "Omid Saremi",
                "Josh Susskind",
                "Samy Bengio",
                "Preetum Nakkiran"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16028v1",
                "http://arxiv.org/pdf/2310.16028v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16077v1",
            "title": "Femtosecond laser fabricated nitinol living hinges for millimeter-sized\n  robots",
            "updated": "2023-10-24T17:37:52Z",
            "published": "2023-10-24T17:37:52Z",
            "summary": "Nitinol is a smart material that can be used as an actuator, a sensor, or a\nstructural element, and has the potential to significantly enhance the\ncapabilities of microrobots. Femtosecond laser technology can be used to\nprocess nitinol while avoiding heat-affected zones (HAZ), thus retaining\nsuperelastic properties. In this work, we manufacture living hinges of\narbitrary cross-sections from nitinol using a femtosecond laser micromachining\nprocess. We first determined the laser cutting parameters, 4.1 Jcm^-2 fluence\nwith 5 passes for 5 um ablation, by varying laser power level and number of\npasses. Next, we modeled the hinges using an analytical model as well as\ncreating an Abaqus finite element method, and showed the accuracy of the models\nby comparing them to the torque produced by eight different hinges, four with a\nrectangular cross-section and four with an arc cross-section. Finally, we\nmanufactured three prototype miniature devices to illustrate the usefulness of\nthese nitinol hinges: a sample spherical 5-bar mechanism, a sarrus linkage, and\na piezoelectric actuated robotic wing mechanism.",
            "author": [
                "Alexander Hedrick",
                "Heiko Kabutz",
                "Lawrence Smith",
                "Robert MacCurdy",
                "Kaushik Jayaram"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16077v1",
                "http://arxiv.org/pdf/2310.16077v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16022v1",
            "title": "A Colorful and Robust Measure for FDFAs",
            "updated": "2023-10-24T17:31:26Z",
            "published": "2023-10-24T17:31:26Z",
            "summary": "We define a measure on families of DFAs (FDFAs) that we show to be robust in\nthe sense that two FDFAs for the same language are guaranteed to agree on this\nmeasure. This measure tightly relates to the Wagner-Hierarchy (that defines the\ncomplexity of omega regular languages). Inspired by the recently introduced\nnatural colors of infinite words, we define natural colors for finite words\n(prefixes of periods of infinite words). From this semantic definition we\nderive the Colorful FDFA a novel canonical model for $\\omega$-regular languages\nthat also assigns correct colors for finite and infinite words. From the\ncolorful FDFA, for languages that can be recognized by deterministic B\\\"uchi or\ncoB\\\"uchi automata, we generate a canonical DBA or DCA termed the Black $\\&$\nWhite Automaton, thus complementing the recent result on canonical good for\ngames coB\\\"uchi automata for coB\\\"uchi languages.",
            "author": [
                "Dana Fisman",
                "Emmanuel Goldberg",
                "Oded Zimerman"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16022v1",
                "http://arxiv.org/pdf/2310.16022v1"
            ],
            "primary_category": "cs.FL",
            "category": [
                "cs.FL",
                "F.4.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16015v1",
            "title": "Physically Explainable Deep Learning for Convective Initiation\n  Nowcasting Using GOES-16 Satellite Observations",
            "updated": "2023-10-24T17:18:44Z",
            "published": "2023-10-24T17:18:44Z",
            "summary": "Convection initiation (CI) nowcasting remains a challenging problem for both\nnumerical weather prediction models and existing nowcasting algorithms. In this\nstudy, object-based probabilistic deep learning models are developed to predict\nCI based on multichannel infrared GOES-R satellite observations. The data come\nfrom patches surrounding potential CI events identified in Multi-Radar\nMulti-Sensor Doppler weather radar products over the Great Plains region from\nJune and July 2020 and June 2021. An objective radar-based approach is used to\nidentify these events. The deep learning models significantly outperform the\nclassical logistic model at lead times up to 1 hour, especially on the false\nalarm ratio. Through case studies, the deep learning model exhibits the\ndependence on the characteristics of clouds and moisture at multiple levels.\nModel explanation further reveals the model's decision-making process with\ndifferent baselines. The explanation results highlight the importance of\nmoisture and cloud features at different levels depending on the choice of\nbaseline. Our study demonstrates the advantage of using different baselines in\nfurther understanding model behavior and gaining scientific insights.",
            "author": [
                "Da Fan",
                "Steven J. Greybush",
                "David John Gagne II",
                "Eugene E. Clothiaux"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16015v1",
                "http://arxiv.org/pdf/2310.16015v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16076v1",
            "title": "Practical Computational Power of Linear Transformers and Their Recurrent\n  and Self-Referential Extensions",
            "updated": "2023-10-24T17:17:01Z",
            "published": "2023-10-24T17:17:01Z",
            "summary": "Recent studies of the computational power of recurrent neural networks (RNNs)\nreveal a hierarchy of RNN architectures, given real-time and finite-precision\nassumptions. Here we study auto-regressive Transformers with linearised\nattention, a.k.a. linear Transformers (LTs) or Fast Weight Programmers (FWPs).\nLTs are special in the sense that they are equivalent to RNN-like sequence\nprocessors with a fixed-size state, while they can also be expressed as the\nnow-popular self-attention networks. We show that many well-known results for\nthe standard Transformer directly transfer to LTs/FWPs. Our formal language\nrecognition experiments demonstrate how recently proposed FWP extensions such\nas recurrent FWPs and self-referential weight matrices successfully overcome\ncertain limitations of the LT, e.g., allowing for generalisation on the parity\nproblem. Our code is public.",
            "author": [
                "Kazuki Irie",
                "R\u00f3bert Csord\u00e1s",
                "J\u00fcrgen Schmidhuber"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16076v1",
                "http://arxiv.org/pdf/2310.16076v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16013v1",
            "title": "Experimental measurement of the quality factor of a Fabry-P\u00e9rot\n  open-cavity axion haloscope",
            "updated": "2023-10-24T17:12:10Z",
            "published": "2023-10-24T17:12:10Z",
            "summary": "The axion is a hypothetical boson arising from the most natural solution to\nthe problem of charge and parity symmetry in the strong nuclear force.\nMoreover, this pseudoscalar emerges as a dark matter candidate in a parameter\nspace extending several decades in mass. The Dark-photons \\& Axion-Like\nparticles Interferometer (DALI) is a proposal to search for axion dark matter\nin a range that remains under-examined. Currently in a design and prototyping\nphase, this haloscope is a multilayer Fabry-P\\'erot interferometer. A\nproof-of-principle experiment is performed to observe the resonance in a\nprototype. The test unveils a quality factor per open cavity of a few hundred\nover a bandwidth of the order of dozens of megahertz. The result elucidates a\nphysics potential to find the, so far elusive, axion, in a sector which can\nsimultaneously solve the symmetry problem in the strong interaction and the\nenigma of dark matter.",
            "author": [
                "Juan F. Hern\u00e1ndez-Cabrera",
                "Javier De Miguel",
                "E. Hern\u00e1ndez-Su\u00e1rez",
                "Enrique Joven-\u00c1lvarez",
                "H. Lorenzo-Hern\u00e1ndez",
                "Chiko Otani",
                "Miguel A. Rapado-Tamarit",
                "J. Alberto Rubi\u00f1o-Mart\u00edn"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16013v1",
                "http://arxiv.org/pdf/2310.16013v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "astro-ph.IM",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16011v1",
            "title": "Searching for sub-populations within the gamma-ray solar flares catalog:\n  a graph-based clustering analysis",
            "updated": "2023-10-24T17:06:08Z",
            "published": "2023-10-24T17:06:08Z",
            "summary": "Solar flares are highly energetic events that happen in the solar atmosphere.\nThey are mostly observed as X-ray or gamma-ray bursts located on the Sun's\nsurface. While they are known to be sites of particle acceleration, the\nacceleration process(es) responsible for the observed fluxes remain unsure. The\ndiversity in shape and duration of the gamma-ray fluxes suggests the existence\nof distinct phases of hadronic acceleration. Moreover, different acceleration\nprocesses could explain the differences observed among flares. In this work we\nsearch for the evidence of sub-populations within the catalog of gamma-ray\nsolar flares observed by Fermi-LAT. We aim at grouping flares with similar\nphysical properties to be able to probe theoretical models for neutrino\nproduction within different classes of flares. We use measurements of the X-ray\nand gamma-ray fluxes, as well as CMEs and SEPs, to cluster the events using a\ngraph-based algorithm. Furthermore, we investigate the most representative\nfeatures that characterise the identified sub-populations to allow for\nqualitative analysis and model development.",
            "author": [
                "Jonathan Mauro",
                "Gwenha\u00ebl de Wasseige"
            ],
            "link": [
                "http://dx.doi.org/10.22323/1.444.1292",
                "http://arxiv.org/abs/2310.16011v1",
                "http://arxiv.org/pdf/2310.16011v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR",
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16003v1",
            "title": "CVPR 2023 Text Guided Video Editing Competition",
            "updated": "2023-10-24T16:56:58Z",
            "published": "2023-10-24T16:56:58Z",
            "summary": "Humans watch more than a billion hours of video per day. Most of this video\nwas edited manually, which is a tedious process. However, AI-enabled\nvideo-generation and video-editing is on the rise. Building on text-to-image\nmodels like Stable Diffusion and Imagen, generative AI has improved\ndramatically on video tasks. But it's hard to evaluate progress in these video\ntasks because there is no standard benchmark. So, we propose a new dataset for\ntext-guided video editing (TGVE), and we run a competition at CVPR to evaluate\nmodels on our TGVE dataset. In this paper we present a retrospective on the\ncompetition and describe the winning method. The competition dataset is\navailable at https://sites.google.com/view/loveucvpr23/track4.",
            "author": [
                "Jay Zhangjie Wu",
                "Xiuyu Li",
                "Difei Gao",
                "Zhen Dong",
                "Jinbin Bai",
                "Aishani Singh",
                "Xiaoyu Xiang",
                "Youzeng Li",
                "Zuwei Huang",
                "Yuanxi Sun",
                "Rui He",
                "Feng Hu",
                "Junhua Hu",
                "Hai Huang",
                "Hanyu Zhu",
                "Xu Cheng",
                "Jie Tang",
                "Mike Zheng Shou",
                "Kurt Keutzer",
                "Forrest Iandola"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16003v1",
                "http://arxiv.org/pdf/2310.16003v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.16002v2",
            "title": "Integrating View Conditions for Image Synthesis",
            "updated": "2023-10-26T16:30:44Z",
            "published": "2023-10-24T16:55:07Z",
            "summary": "In the field of image processing, applying intricate semantic modifications\nwithin existing images remains an enduring challenge. This paper introduces a\npioneering framework that integrates viewpoint information to enhance the\ncontrol of image editing tasks. By surveying existing object editing\nmethodologies, we distill three essential criteria, consistency,\ncontrollability, and harmony, that should be met for an image editing method.\nIn contrast to previous approaches, our method takes the lead in satisfying all\nthree requirements for addressing the challenge of image synthesis. Through\ncomprehensive experiments, encompassing both quantitative assessments and\nqualitative comparisons with contemporary state-of-the-art methods, we present\ncompelling evidence of our framework's superior performance across multiple\ndimensions. This work establishes a promising avenue for advancing image\nsynthesis techniques and empowering precise object modifications while\npreserving the visual coherence of the entire composition.",
            "author": [
                "Jinbin Bai",
                "Zhen Dong",
                "Aosong Feng",
                "Xiao Zhang",
                "Tian Ye",
                "Kaicheng Zhou",
                "Mike Zheng Shou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16002v2",
                "http://arxiv.org/pdf/2310.16002v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.15999v1",
            "title": "Transitivity Recovering Decompositions: Interpretable and Robust\n  Fine-Grained Relationships",
            "updated": "2023-10-24T16:48:56Z",
            "published": "2023-10-24T16:48:56Z",
            "summary": "Recent advances in fine-grained representation learning leverage\nlocal-to-global (emergent) relationships for achieving state-of-the-art\nresults. The relational representations relied upon by such methods, however,\nare abstract. We aim to deconstruct this abstraction by expressing them as\ninterpretable graphs over image views. We begin by theoretically showing that\nabstract relational representations are nothing but a way of recovering\ntransitive relationships among local views. Based on this, we design\nTransitivity Recovering Decompositions (TRD), a graph-space search algorithm\nthat identifies interpretable equivalents of abstract emergent relationships at\nboth instance and class levels, and with no post-hoc computations. We\nadditionally show that TRD is provably robust to noisy views, with empirical\nevidence also supporting this finding. The latter allows TRD to perform at par\nor even better than the state-of-the-art, while being fully interpretable.\nImplementation is available at https://github.com/abhrac/trd.",
            "author": [
                "Abhra Chaudhuri",
                "Massimiliano Mancini",
                "Zeynep Akata",
                "Anjan Dutta"
            ],
            "link": [
                "http://arxiv.org/abs/2310.15999v1",
                "http://arxiv.org/pdf/2310.15999v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.15991v1",
            "title": "White-box Compiler Fuzzing Empowered by Large Language Models",
            "updated": "2023-10-24T16:39:06Z",
            "published": "2023-10-24T16:39:06Z",
            "summary": "Compiler correctness is crucial, as miscompilation falsifying the program\nbehaviors can lead to serious consequences. In the literature, fuzzing has been\nextensively studied to uncover compiler defects. However, compiler fuzzing\nremains challenging: Existing arts focus on black- and grey-box fuzzing, which\ngenerates tests without sufficient understanding of internal compiler\nbehaviors. As such, they often fail to construct programs to exercise\nconditions of intricate optimizations. Meanwhile, traditional white-box\ntechniques are computationally inapplicable to the giant codebase of compilers.\nRecent advances demonstrate that Large Language Models (LLMs) excel in code\ngeneration/understanding tasks and have achieved state-of-the-art performance\nin black-box fuzzing. Nonetheless, prompting LLMs with compiler source-code\ninformation remains a missing piece of research in compiler testing.\n  To this end, we propose WhiteFox, the first white-box compiler fuzzer using\nLLMs with source-code information to test compiler optimization. WhiteFox\nadopts a dual-model framework: (i) an analysis LLM examines the low-level\noptimization source code and produces requirements on the high-level test\nprograms that can trigger the optimization; (ii) a generation LLM produces test\nprograms based on the summarized requirements. Additionally,\noptimization-triggering tests are used as feedback to further enhance the test\ngeneration on the fly. Our evaluation on four popular compilers shows that\nWhiteFox can generate high-quality tests to exercise deep optimizations\nrequiring intricate conditions, practicing up to 80 more optimizations than\nstate-of-the-art fuzzers. To date, WhiteFox has found in total 96 bugs, with 80\nconfirmed as previously unknown and 51 already fixed. Beyond compiler testing,\nWhiteFox can also be adapted for white-box fuzzing of other complex, real-world\nsoftware systems in general.",
            "author": [
                "Chenyuan Yang",
                "Yinlin Deng",
                "Runyu Lu",
                "Jiayi Yao",
                "Jiawei Liu",
                "Reyhaneh Jabbarvand",
                "Lingming Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.15991v1",
                "http://arxiv.org/pdf/2310.15991v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.LG",
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.17667v1",
            "title": "Understanding the Hamiltonian Function through the Geometry of Partial\n  Legendre Transforms",
            "updated": "2023-10-24T16:37:44Z",
            "published": "2023-10-24T16:37:44Z",
            "summary": "The relationship between the Hamiltonian and Lagrangean functions in\nanalytical mechanics is a type of duality. The two functions, while distinct,\nare both descriptive functions encoding the behavior of the same dynamical\nsystem. One difference is that the Lagrangean naturally appears as one\ninvestigates the fundamental equation of classical dynamics. It is not that way\nfor the Hamiltonian. The Hamiltonian comes after Lagrange's equations have been\nfully formed, most commonly through a Legendre transform of the Lagrangean\nfunction. We revisit the Legendre transform approach and offer a more refined\ngeometrical interpretation than what is commonly shown.",
            "author": [
                "John E. Hurtado"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17667v1",
                "http://arxiv.org/pdf/2310.17667v1"
            ],
            "primary_category": "physics.gen-ph",
            "category": [
                "physics.gen-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.15987v1",
            "title": "Dissecting In-Context Learning of Translations in GPTs",
            "updated": "2023-10-24T16:37:18Z",
            "published": "2023-10-24T16:37:18Z",
            "summary": "Most of the recent work in leveraging Large Language Models (LLMs) such as\nGPT-3 for Machine Translation (MT) has focused on selecting the few-shot\nsamples for prompting. In this work, we try to better understand the role of\ndemonstration attributes for the in-context learning of translations through\nperturbations of high-quality, in-domain demonstrations. We find that\nasymmetric perturbation of the source-target mappings yield vastly different\nresults. We show that the perturbation of the source side has surprisingly\nlittle impact, while target perturbation can drastically reduce translation\nquality, suggesting that it is the output text distribution that provides the\nmost important learning signal during in-context learning of translations. We\npropose a method named Zero-Shot-Context to add this signal automatically in\nZero-Shot prompting. We demonstrate that it improves upon the zero-shot\ntranslation performance of GPT-3, even making it competitive with few-shot\nprompted translations.",
            "author": [
                "Vikas Raunak",
                "Hany Hassan Awadalla",
                "Arul Menezes"
            ],
            "link": [
                "http://arxiv.org/abs/2310.15987v1",
                "http://arxiv.org/pdf/2310.15987v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.15985v1",
            "title": "Vision-Language Pseudo-Labels for Single-Positive Multi-Label Learning",
            "updated": "2023-10-24T16:36:51Z",
            "published": "2023-10-24T16:36:51Z",
            "summary": "This paper presents a novel approach to Single-Positive Multi-label Learning.\nIn general multi-label learning, a model learns to predict multiple labels or\ncategories for a single input image. This is in contrast with standard\nmulti-class image classification, where the task is predicting a single label\nfrom many possible labels for an image. Single-Positive Multi-label Learning\n(SPML) specifically considers learning to predict multiple labels when there is\nonly a single annotation per image in the training data. Multi-label learning\nis in many ways a more realistic task than single-label learning as real-world\ndata often involves instances belonging to multiple categories simultaneously;\nhowever, most common computer vision datasets predominantly contain single\nlabels due to the inherent complexity and cost of collecting multiple high\nquality annotations for each instance. We propose a novel approach called\nVision-Language Pseudo-Labeling (VLPL), which uses a vision-language model to\nsuggest strong positive and negative pseudo-labels, and outperforms the current\nSOTA methods by 5.5% on Pascal VOC, 18.4% on MS-COCO, 15.2% on NUS-WIDE, and\n8.4% on CUB-Birds. Our code and data are available at\nhttps://github.com/mvrl/VLPL.",
            "author": [
                "Xin Xing",
                "Zhexiao Xiong",
                "Abby Stylianou",
                "Srikumar Sastry",
                "Liyu Gong",
                "Nathan Jacobs"
            ],
            "link": [
                "http://arxiv.org/abs/2310.15985v1",
                "http://arxiv.org/pdf/2310.15985v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.15984v1",
            "title": "Geometry-Aware Video Quality Assessment for Dynamic Digital Human",
            "updated": "2023-10-24T16:34:03Z",
            "published": "2023-10-24T16:34:03Z",
            "summary": "Dynamic Digital Humans (DDHs) are 3D digital models that are animated using\npredefined motions and are inevitably bothered by noise/shift during the\ngeneration process and compression distortion during the transmission process,\nwhich needs to be perceptually evaluated. Usually, DDHs are displayed as 2D\nrendered animation videos and it is natural to adapt video quality assessment\n(VQA) methods to DDH quality assessment (DDH-QA) tasks. However, the VQA\nmethods are highly dependent on viewpoints and less sensitive to geometry-based\ndistortions. Therefore, in this paper, we propose a novel no-reference (NR)\ngeometry-aware video quality assessment method for DDH-QA challenge. Geometry\ncharacteristics are described by the statistical parameters estimated from the\nDDHs' geometry attribute distributions. Spatial and temporal features are\nacquired from the rendered videos. Finally, all kinds of features are\nintegrated and regressed into quality values. Experimental results show that\nthe proposed method achieves state-of-the-art performance on the DDH-QA\ndatabase.",
            "author": [
                "Zicheng Zhang",
                "Yingjie Zhou",
                "Wei Sun",
                "Xiongkuo Min",
                "Guangtao Zhai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.15984v1",
                "http://arxiv.org/pdf/2310.15984v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.15982v1",
            "title": "Theory of correlated Chern insulators in twisted bilayer graphene",
            "updated": "2023-10-24T16:31:39Z",
            "published": "2023-10-24T16:31:39Z",
            "summary": "Magic-angle twisted bilayer graphene is the best studied physical platform\nfeaturing moire potential induced narrow bands with non-trivial topology and\nstrong electronic correlations. Despite their significance, the Chern\ninsulating states observed at a finite magnetic field -- and extrapolating to a\nband filling, $s$, at zero field -- remain poorly understood. Unraveling their\nnature is among the most important open problems in the province of moir\\'e\nmaterials. Here we present the first comprehensive study of interacting\nelectrons in finite magnetic field while varying the electron density, twist\nangle and heterostrain. Within a panoply of correlated Chern phases emerging at\na range of twist angles, we uncover a unified description for the ubiquitous\nsequence of states with the Chern number $t$ for $(s,t)=\\pm (0,4),\n\\pm(1,3),\\pm(2,2)$ and $\\pm(3,1)$. We also find correlated Chern insulators at\nunconventional sequences with $s+t\\neq \\pm 4$, as well as with fractional $s$,\nand elucidate their nature.",
            "author": [
                "Xiaoyu Wang",
                "Oskar Vafek"
            ],
            "link": [
                "http://arxiv.org/abs/2310.15982v1",
                "http://arxiv.org/pdf/2310.15982v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.15978v1",
            "title": "Graph Deep Learning for Time Series Forecasting",
            "updated": "2023-10-24T16:26:38Z",
            "published": "2023-10-24T16:26:38Z",
            "summary": "Graph-based deep learning methods have become popular tools to process\ncollections of correlated time series. Differently from traditional\nmultivariate forecasting methods, neural graph-based predictors take advantage\nof pairwise relationships by conditioning forecasts on a (possibly dynamic)\ngraph spanning the time series collection. The conditioning can take the form\nof an architectural inductive bias on the neural forecasting architecture,\nresulting in a family of deep learning models called spatiotemporal graph\nneural networks. Such relational inductive biases enable the training of global\nforecasting models on large time-series collections, while at the same time\nlocalizing predictions w.r.t. each element in the set (i.e., graph nodes) by\naccounting for local correlations among them (i.e., graph edges). Indeed,\nrecent theoretical and practical advances in graph neural networks and deep\nlearning for time series forecasting make the adoption of such processing\nframeworks appealing and timely. However, most of the studies in the literature\nfocus on proposing variations of existing neural architectures by taking\nadvantage of modern deep learning practices, while foundational and\nmethodological aspects have not been subject to systematic investigation. To\nfill the gap, this paper aims to introduce a comprehensive methodological\nframework that formalizes the forecasting problem and provides design\nprinciples for graph-based predictive models and methods to assess their\nperformance. At the same time, together with an overview of the field, we\nprovide design guidelines, recommendations, and best practices, as well as an\nin-depth discussion of open challenges and future research directions.",
            "author": [
                "Andrea Cini",
                "Ivan Marisca",
                "Daniele Zambon",
                "Cesare Alippi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.15978v1",
                "http://arxiv.org/pdf/2310.15978v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.15971v1",
            "title": "Characterizing Issue Management in Runtime Systems",
            "updated": "2023-10-24T16:12:52Z",
            "published": "2023-10-24T16:12:52Z",
            "summary": "Modern programming languages like Java require runtime systems to support the\nimplementation and deployment of software applications in diverse computing\nplatforms and operating systems. These runtime systems are normally developed\nin GitHub-hosted repositories based on close collaboration between large\nsoftware companies (e.g., IBM, Microsoft) and OSS developers. However, despite\ntheir popularity and broad usage; to the best of our knowledge, these\nrepositories have never been studied. We report an empirical study of around\n118K issues from 34 runtime system repos in GitHub. We found that issues\nregarding enhancement, test failure and bug are mostly posted on runtime system\nrepositories and solution related discussion are mostly present on issue\ndiscussion. 82.69% issues in the runtime system repositories have been resolved\nand 0.69% issues are ignored; median of issue close rate, ignore rate and\naddressing time in these repositories are 76.1%, 2.2% and 58 days respectively.\n82.65% issues are tagged with labels while only 28.30% issues have designated\nassignees and 90.65% issues contain at least one comment; also presence of\nthese features in an issue report can affect issue closure. Based on the\nfindings, we offer six recommendat",
            "author": [
                "Salma Begum Tamanna",
                "Gias Uddin",
                "Lan Xia",
                "Longyu Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.15971v1",
                "http://arxiv.org/pdf/2310.15971v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.15970v3",
            "title": "Accented Speech Recognition With Accent-specific Codebooks",
            "updated": "2023-10-27T02:54:29Z",
            "published": "2023-10-24T16:10:58Z",
            "summary": "Speech accents pose a significant challenge to state-of-the-art automatic\nspeech recognition (ASR) systems. Degradation in performance across\nunderrepresented accents is a severe deterrent to the inclusive adoption of\nASR. In this work, we propose a novel accent adaptation approach for end-to-end\nASR systems using cross-attention with a trainable set of codebooks. These\nlearnable codebooks capture accent-specific information and are integrated\nwithin the ASR encoder layers. The model is trained on accented English speech,\nwhile the test data also contained accents which were not seen during training.\nOn the Mozilla Common Voice multi-accented dataset, we show that our proposed\napproach yields significant performance gains not only on the seen English\naccents (up to $37\\%$ relative improvement in word error rate) but also on the\nunseen accents (up to $5\\%$ relative improvement in WER). Further, we\nillustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We\nalso compare the performance with other approaches based on accent adversarial\ntraining.",
            "author": [
                "Darshan Prabhu",
                "Preethi Jyothi",
                "Sriram Ganapathy",
                "Vinit Unni"
            ],
            "link": [
                "http://arxiv.org/abs/2310.15970v3",
                "http://arxiv.org/pdf/2310.15970v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.15963v1",
            "title": "Paralinearization and extended lifespan for solutions of the $ \u03b1\n  $-SQG sharp front equation",
            "updated": "2023-10-24T16:05:38Z",
            "published": "2023-10-24T16:05:38Z",
            "summary": "In this paper we paralinearize the contour dynamics equation for sharp-fronts\nof $\\alpha$-SQG, for any $ \\alpha \\in (0,1) \\cup (1,2) $, close to a circular\nvortex. This turns out to be a quasi-linear Hamiltonian PDE. After deriving the\nasymptotic expansion of the linear frequencies of oscillations at the vortex\ndisk and verifying the absence of three wave interactions, we prove that, in\nthe most singular cases $ \\alpha \\in (1,2) $, any initial vortex patch which is\n$ \\varepsilon $-close to the disk exists for a time interval of size at least $\n\\sim \\varepsilon^{-2} $. This quadratic lifespan result relies on a\nparadifferential Birkhoff normal form reduction and exploits cancellations\narising from the Hamiltonian nature of the equation. This is the first normal\nform long time existence result of sharp fronts.",
            "author": [
                "Massimiliano Berti",
                "Scipio Cuccagna",
                "Francisco Gancedo",
                "Stefano Scrobogna"
            ],
            "link": [
                "http://arxiv.org/abs/2310.15963v1",
                "http://arxiv.org/pdf/2310.15963v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2310.15961v1",
            "title": "Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation",
            "updated": "2023-10-24T16:03:57Z",
            "published": "2023-10-24T16:03:57Z",
            "summary": "Despite the promise of Mixture of Experts (MoE) models in increasing\nparameter counts of Transformer models while maintaining training and inference\ncosts, their application carries notable drawbacks. The key strategy of these\nmodels is to, for each processed token, activate at most a few experts -\nsubsets of an extensive feed-forward layer. But this approach is not without\nits challenges. The operation of matching experts and tokens is discrete, which\nmakes MoE models prone to issues like training instability and uneven expert\nutilization. Existing techniques designed to address these concerns, such as\nauxiliary losses or balance-aware matching, result either in lower model\nperformance or are more difficult to train. In response to these issues, we\npropose Mixture of Tokens, a fully-differentiable model that retains the\nbenefits of MoE architectures while avoiding the aforementioned difficulties.\nRather than routing tokens to experts, this approach mixes tokens from\ndifferent examples prior to feeding them to experts, enabling the model to\nlearn from all token-expert combinations. Importantly, this mixing can be\ndisabled to avoid mixing of different sequences during inference. Crucially,\nthis method is fully compatible with both masked and causal Large Language\nModel training and inference.",
            "author": [
                "Szymon Antoniak",
                "Sebastian Jaszczur",
                "Micha\u0142 Krutul",
                "Maciej Pi\u00f3ro",
                "Jakub Krajewski",
                "Jan Ludziejewski",
                "Tomasz Odrzyg\u00f3\u017ad\u017a",
                "Marek Cygan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.15961v1",
                "http://arxiv.org/pdf/2310.15961v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    }
]