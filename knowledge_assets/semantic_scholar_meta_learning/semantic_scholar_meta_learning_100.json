[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bdff94b5e2d2a7f67ff0eb6d239aaf5f6d536cd5",
            "@type": "ScholarlyArticle",
            "paperId": "bdff94b5e2d2a7f67ff0eb6d239aaf5f6d536cd5",
            "corpusId": 222278325,
            "url": "https://www.semanticscholar.org/paper/bdff94b5e2d2a7f67ff0eb6d239aaf5f6d536cd5",
            "title": "BlockMix: Meta Regularization and Self-Calibrated Inference for Metric-Based Meta-Learning",
            "venue": "ACM Multimedia",
            "publicationVenue": {
                "id": "urn:research:f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                "name": "ACM Multimedia",
                "alternate_names": [
                    "MM"
                ],
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/mm/TangLPT20",
                "MAG": "3092900959",
                "DOI": "10.1145/3394171.3413884",
                "CorpusId": 222278325
            },
            "abstract": "Most metric-based meta-learning methods learn only the sophisticated similarity metric for few-shot classification, which may lead to the feature deterioration and unreliable prediction. Toward this end, we propose new mechanisms to learn generalized and discriminative feature embeddings as well as improve the robustness of classifiers against prediction corruptions for meta-learning. For this purpose, a new generation operator BlockMix is proposed by integrating interpolation on the images and labels within metric learning. Based on the above BlockMix, we propose a novel regularization method Meta Regularization as an auxiliary task branch with its own classifier to better constraint the feature embedding module and stabilize the meta-learning process. Furthermore, a novel inference scheme Self-Calibrated Inference is proposed to alleviate the unreliable prediction problem by calibrating the prototype of each category with the confidence-weighted average of the support and generated samples. The proposed mechanisms can be used as supplementary techniques alongside standard metric-based meta-learning algorithms without any pre-training. Experimental results demonstrate the insights and the efficiency of the proposed mechanisms respectively, compared with the state-of-the-art methods on the prevalent few-shot benchmarks.",
            "referenceCount": 59,
            "citationCount": 69,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book"
            ],
            "publicationDate": "2020-10-12",
            "journal": {
                "name": "Proceedings of the 28th ACM International Conference on Multimedia",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tang2020BlockMixMR,\n author = {Hao Tang and Zechao Li and Zhimao Peng and Jinhui Tang},\n booktitle = {ACM Multimedia},\n journal = {Proceedings of the 28th ACM International Conference on Multimedia},\n title = {BlockMix: Meta Regularization and Self-Calibrated Inference for Metric-Based Meta-Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3ef2f34014392a0f826ab43e2e7880adcebf2605",
            "@type": "ScholarlyArticle",
            "paperId": "3ef2f34014392a0f826ab43e2e7880adcebf2605",
            "corpusId": 220484667,
            "url": "https://www.semanticscholar.org/paper/3ef2f34014392a0f826ab43e2e7880adcebf2605",
            "title": "Federated Meta-Learning for Fraudulent Credit Card Detection",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/ijcai/ZhengYG020",
                "MAG": "3034321207",
                "DOI": "10.24963/ijcai.2020/642",
                "CorpusId": 220484667
            },
            "abstract": "Credit card transaction fraud costs billions of dollars to card issuers every year. Besides, the credit card transaction dataset is very skewed, there are much fewer samples of frauds than legitimate transactions. Due to the data security and privacy, different banks are usually not allowed to share their transaction datasets. These problems make traditional model difficult to learn the patterns of frauds and also difficult to detect them. In this paper, we introduce a novel framework termed as federated meta-learning for fraud detection. Different from the traditional technologies trained with data centralized in the cloud, our model enables banks to learn fraud detection model with the training data distributed on their own local database. A shared whole model is constructed by aggregating locallycomputed updates of fraud detection model. Banks can collectively reap the benefits of shared model without sharing the dataset and protect the sensitive information of cardholders. To achieve the good performance of classification, we further formulate an improved triplet-like metric learning, and design a novel meta-learning-based classifier, which allows joint comparison with K negative samples in each mini-batch. Experimental results demonstrate that the proposed approach achieves significantly higher performance compared with the other state-of-the-art approaches.",
            "referenceCount": 30,
            "citationCount": 60,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ijcai.org/proceedings/2020/0642.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Business",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-07-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ferrari2020FederatedMF,\n author = {Maurizio Ferrari and P. Cremonesi and D. Jannach},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {4654-4660},\n title = {Federated Meta-Learning for Fraudulent Credit Card Detection},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c55cc603b74b8ba0cc58dbaa1d8df1af94ab934b",
            "@type": "ScholarlyArticle",
            "paperId": "c55cc603b74b8ba0cc58dbaa1d8df1af94ab934b",
            "corpusId": 221341010,
            "url": "https://www.semanticscholar.org/paper/c55cc603b74b8ba0cc58dbaa1d8df1af94ab934b",
            "title": "learn2learn: A Library for Meta-Learning Research",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2008.12284",
                "DBLP": "journals/corr/abs-2008-12284",
                "MAG": "3080894165",
                "CorpusId": 221341010
            },
            "abstract": "Meta-learning researchers face two fundamental issues in their empirical work: prototyping and reproducibility. Researchers are prone to make mistakes when prototyping new algorithms and tasks because modern meta-learning methods rely on unconventional functionalities of machine learning frameworks. In turn, reproducing existing results becomes a tedious endeavour -- a situation exacerbated by the lack of standardized implementations and benchmarks. As a result, researchers spend inordinate amounts of time on implementing software rather than understanding and developing new ideas. \nThis manuscript introduces learn2learn, a library for meta-learning research focused on solving those prototyping and reproducibility issues. learn2learn provides low-level routines common across a wide-range of meta-learning techniques (e.g. meta-descent, meta-reinforcement learning, few-shot learning), and builds standardized interfaces to algorithms and benchmarks on top of them. In releasing learn2learn under a free and open source license, we hope to foster a community around standardized software for meta-learning research.",
            "referenceCount": 32,
            "citationCount": 61,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-08-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2008.12284"
            },
            "citationStyles": {
                "bibtex": "@Article{Arnold2020learn2learnAL,\n author = {S\u00e9bastien M. R. Arnold and Praateek Mahajan and Debajyoti Datta and Ian Bunner and Konstantinos Saitas Zarkias},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {learn2learn: A Library for Meta-Learning Research},\n volume = {abs/2008.12284},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:33a9513d6bf2c90ec81f98cee3518565b142d028",
            "@type": "ScholarlyArticle",
            "paperId": "33a9513d6bf2c90ec81f98cee3518565b142d028",
            "corpusId": 211817937,
            "url": "https://www.semanticscholar.org/paper/33a9513d6bf2c90ec81f98cee3518565b142d028",
            "title": "Rapidly Adaptable Legged Robots via Evolutionary Meta-Learning",
            "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
            "publicationVenue": {
                "id": "urn:research:37275deb-3fcf-4d16-ae77-95db9899b1f3",
                "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                "alternate_names": [
                    "IROS",
                    "Intelligent Robots and Systems",
                    "Intell Robot Syst",
                    "IEEE/RJS Int Conf Intell Robot Syst"
                ],
                "issn": null,
                "url": "http://www.iros.org/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3009482349",
                "ArXiv": "2003.01239",
                "DBLP": "journals/corr/abs-2003-01239",
                "DOI": "10.1109/IROS45743.2020.9341571",
                "CorpusId": 211817937
            },
            "abstract": "Learning adaptable policies is crucial for robots to operate autonomously in our complex and quickly changing world. In this work, we present a new meta-learning method that allows robots to quickly adapt to changes in dynamics. In contrast to gradient-based meta-learning algorithms that rely on second-order gradient estimation, we introduce a more noise-tolerant Batch Hill-Climbing adaptation operator and combine it with meta-learning based on evolutionary strategies. Our method significantly improves adaptation to changes in dynamics in high noise settings, which are common in robotics applications. We validate our approach on a quadruped robot that learns to walk while subject to changes in dynamics. We observe that our method significantly outperforms prior gradient-based approaches, enabling the robot to adapt its policy to changes based on less than 3 minutes of real data.",
            "referenceCount": 52,
            "citationCount": 61,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2003.01239",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-03-02",
            "journal": {
                "name": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Song2020RapidlyAL,\n author = {Xingyou Song and Yuxiang Yang and K. Choromanski and Ken Caluwaerts and Wenbo Gao and Chelsea Finn and Jie Tan},\n booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},\n journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\n pages = {3769-3776},\n title = {Rapidly Adaptable Legged Robots via Evolutionary Meta-Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7d0029510d7f47d3ee716aa8e01b69e353f9e143",
            "@type": "ScholarlyArticle",
            "paperId": "7d0029510d7f47d3ee716aa8e01b69e353f9e143",
            "corpusId": 216562279,
            "url": "https://www.semanticscholar.org/paper/7d0029510d7f47d3ee716aa8e01b69e353f9e143",
            "title": "Meta-Learning for Few-Shot Land Cover Classification",
            "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2004.13390",
                "MAG": "3022048625",
                "DBLP": "journals/corr/abs-2004-13390",
                "DOI": "10.1109/CVPRW50498.2020.00108",
                "CorpusId": 216562279
            },
            "abstract": "The representations of the Earth\u2019s surface vary from one geographic region to another. For instance, the appearance of urban areas differs between continents, and seasonality influences the appearance of vegetation. To capture the diversity within a single category, such as urban or vegetation, requires a large model capacity and, consequently, large datasets. In this work, we propose a different perspective and view this diversity as an inductive transfer learning problem where few data samples from one region allow a model to adapt to an unseen region. We evaluate the modelagnostic meta-learning (MAML) algorithm on classification and segmentation tasks using globally and regionally distributed datasets. We find that few-shot model adaptation outperforms pre-training with regular gradient descent and fine-tuning on the (1) Sen12MS dataset and (2) DeepGlobe dataset when the source domain and target domain differ. This indicates that model optimization with meta-learning may benefit tasks in the Earth sciences whose data show a high degree of diversity from region to region, while traditional gradient-based supervised learning remains suitable in the absence of a feature or label shift.",
            "referenceCount": 37,
            "citationCount": 60,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2004.13390",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-04-28",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ru\u00dfwurm2020MetaLearningFF,\n author = {M. Ru\u00dfwurm and Sherrie Wang and Marco K\u00f6rner and D. Lobell},\n booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n pages = {788-796},\n title = {Meta-Learning for Few-Shot Land Cover Classification},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:daffabf1d352109c1c459aaffca3144f158aec27",
            "@type": "ScholarlyArticle",
            "paperId": "daffabf1d352109c1c459aaffca3144f158aec27",
            "corpusId": 211132471,
            "url": "https://www.semanticscholar.org/paper/daffabf1d352109c1c459aaffca3144f158aec27",
            "title": "Meta-Learning Extractors for Music Source Separation",
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "publicationVenue": {
                "id": "urn:research:0d6f7fba-7092-46b3-8039-93458dba736b",
                "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                "alternate_names": [
                    "Int Conf Acoust Speech Signal Process",
                    "IEEE Int Conf Acoust Speech Signal Process",
                    "ICASSP",
                    "International Conference on Acoustics, Speech, and Signal Processing"
                ],
                "issn": null,
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3015753416",
                "DBLP": "journals/corr/abs-2002-07016",
                "ArXiv": "2002.07016",
                "DOI": "10.1109/ICASSP40776.2020.9053513",
                "CorpusId": 211132471
            },
            "abstract": "We propose a hierarchical meta-learning-inspired model for music source separation (Meta-TasNet) in which a generator model is used to predict the weights of individual extractor models. This enables efficient parameter-sharing, while still allowing for instrument-specific parameterization. Meta-TasNet is shown to be more effective than the models trained independently or in a multi-task setting, and achieve performance comparable with state-of-the-art methods. In comparison to the latter, our extractors contain fewer parameters and have faster run-time performance. We discuss important architectural considerations, and explore the costs and benefits of this approach.",
            "referenceCount": 21,
            "citationCount": 53,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2002.07016",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-02-17",
            "journal": {
                "name": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Samuel2020MetaLearningEF,\n author = {David Samuel and Aditya Ganeshan and Jason Naradowsky},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {816-820},\n title = {Meta-Learning Extractors for Music Source Separation},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5484768e507217f5959a03041ee81547353fb5e5",
            "@type": "ScholarlyArticle",
            "paperId": "5484768e507217f5959a03041ee81547353fb5e5",
            "corpusId": 226227266,
            "url": "https://www.semanticscholar.org/paper/5484768e507217f5959a03041ee81547353fb5e5",
            "title": "Meta-Learning with Adaptive Hyperparameters",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3097892290",
                "DBLP": "conf/nips/BaikCCKL20",
                "ArXiv": "2011.00209",
                "CorpusId": 226227266
            },
            "abstract": "Despite its popularity, several recent works question the effectiveness of MAML when test tasks are different from training tasks, thus suggesting various task-conditioned methodology to improve the initialization. Instead of searching for better task-aware initialization, we focus on a complementary factor in MAML framework, inner-loop optimization (or fast adaptation). Consequently, we propose a new weight update rule that greatly enhances the fast adaptation process. Specifically, we introduce a small meta-network that can adaptively generate per-step hyperparameters: learning rate and weight decay coefficients. The experimental results validate that the Adaptive Learning of hyperparameters for Fast Adaptation (ALFA) is the equally important ingredient that was often neglected in the recent few-shot learning approaches. Surprisingly, fast adaptation from random initialization with ALFA can already outperform MAML.",
            "referenceCount": 54,
            "citationCount": 76,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-10-31",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2011.00209"
            },
            "citationStyles": {
                "bibtex": "@Article{Baik2020MetaLearningWA,\n author = {Sungyong Baik and Myungsub Choi and Janghoon Choi and Heewon Kim and Kyoung Mu Lee},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Meta-Learning with Adaptive Hyperparameters},\n volume = {abs/2011.00209},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b17fad6604a7d72db9cc29adf93ba0b7a3a91b8f",
            "@type": "ScholarlyArticle",
            "paperId": "b17fad6604a7d72db9cc29adf93ba0b7a3a91b8f",
            "corpusId": 210889050,
            "url": "https://www.semanticscholar.org/paper/b17fad6604a7d72db9cc29adf93ba0b7a3a91b8f",
            "title": "A Literature Survey and Empirical Study of Meta-Learning for Classifier Selection",
            "venue": "IEEE Access",
            "publicationVenue": {
                "id": "urn:research:2633f5b2-c15c-49fe-80f5-07523e770c26",
                "name": "IEEE Access",
                "alternate_names": null,
                "issn": "2169-3536",
                "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/access/KhanZRA20",
                "MAG": "2999917162",
                "DOI": "10.1109/ACCESS.2020.2964726",
                "CorpusId": 210889050
            },
            "abstract": "Classification is the key and most widely studied paradigm in machine learning community. The selection of appropriate classification algorithm for a particular problem is a challenging task, formally known as algorithm selection problem (ASP) in literature. It is increasingly becoming focus of research in machine learning community. Meta-learning has demonstrated substantial success in solving ASP, especially in the domain of classification. Considerable progress has been made in classification algorithm recommendation and researchers have proposed various methods in literature that tackles ASP in many different ways in meta-learning setup. Yet there is a lack of survey and comparative study that critically analyze, summarize and assess the performance of existing methods. To fill these gaps, in this paper we first present a literature survey of classification algorithm recommendation methods. The survey shed light on the motivational reasons for pursuing classifier selection through meta-learning and comprehensively discusses the different phases of classifier selection based on a generic framework that is formed as an outcome of reviewing prior works. Subsequently, we critically analyzed and summarized the existing studies from the literature in three important dimensions i.e., meta-features, meta-learner and meta-target. In the second part of this paper, we present extensive comparative evaluation of all the prominent methods for classifier selection based on 17 classification algorithms and 84 benchmark datasets. The comparative study quantitatively assesses the performance of classifier selection methods and highlight the limitations and strengths of meta-features, meta-learners and meta-target in classification algorithm recommendation system. Finally, we conclude this paper by identifying current challenges and suggesting future work directions. We expect that this work will provide baseline and a solid overview of state of the art works in this domain to new researchers, and will steer future research in this direction.",
            "referenceCount": 82,
            "citationCount": 51,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/08951014.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "IEEE Access",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Khan2020ALS,\n author = {Irfan Khan and Xianchao Zhang and M. Rehman and Rahman Ali},\n booktitle = {IEEE Access},\n journal = {IEEE Access},\n pages = {10262-10281},\n title = {A Literature Survey and Empirical Study of Meta-Learning for Classifier Selection},\n volume = {8},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:19bd467b1c8de94b9bdaef1499788467937f594e",
            "@type": "ScholarlyArticle",
            "paperId": "19bd467b1c8de94b9bdaef1499788467937f594e",
            "corpusId": 225062200,
            "url": "https://www.semanticscholar.org/paper/19bd467b1c8de94b9bdaef1499788467937f594e",
            "title": "Meta-Learning for Domain Generalization in Semantic Parsing",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:01103732-3808-4930-b8e4-7e9e68d5c68d",
                "name": "North American Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "North Am Chapter Assoc Comput Linguistics",
                    "NAACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/naacl"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2010.11988",
                "DBLP": "journals/corr/abs-2010-11988",
                "ACL": "2021.naacl-main.33",
                "MAG": "3169423864",
                "DOI": "10.18653/V1/2021.NAACL-MAIN.33",
                "CorpusId": 225062200
            },
            "abstract": "The importance of building semantic parsers which can be applied to new domains and generate programs unseen at training has long been acknowledged, and datasets testing out-of-domain performance are becoming increasingly available. However, little or no attention has been devoted to learning algorithms or objectives which promote domain generalization, with virtually all existing approaches relying on standard supervised learning. In this work, we use a meta-learning framework which targets zero-shot domain generalization for semantic parsing. We apply a model-agnostic training algorithm that simulates zero-shot parsing by constructing virtual train and test sets from disjoint domains. The learning objective capitalizes on the intuition that gradient steps that improve source-domain performance should also improve target-domain performance, thus encouraging a parser to generalize to unseen target domains. Experimental results on the (English) Spider and Chinese Spider datasets show that the meta-learning objective significantly boosts the performance of a baseline parser.",
            "referenceCount": 58,
            "citationCount": 51,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.naacl-main.33.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-10-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2010.11988"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2020MetaLearningFD,\n author = {Bailin Wang and Mirella Lapata and Ivan Titov},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Meta-Learning for Domain Generalization in Semantic Parsing},\n volume = {abs/2010.11988},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1374711a5ffad8f3d3d3795bc53b937a30c385fe",
            "@type": "ScholarlyArticle",
            "paperId": "1374711a5ffad8f3d3d3795bc53b937a30c385fe",
            "corpusId": 226227344,
            "url": "https://www.semanticscholar.org/paper/1374711a5ffad8f3d3d3795bc53b937a30c385fe",
            "title": "Transfer Learning and Meta Learning-Based Fast Downlink Beamforming Adaptation",
            "venue": "IEEE Transactions on Wireless Communications",
            "publicationVenue": {
                "id": "urn:research:bb40a041-3875-45d5-afd4-e1c75f896fa6",
                "name": "IEEE Transactions on Wireless Communications",
                "alternate_names": [
                    "IEEE Trans Wirel Commun"
                ],
                "issn": "1536-1276",
                "url": "http://www.comsoc.org/twc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/twc/YuanZWOL21",
                "ArXiv": "2011.00903",
                "MAG": "3100120649",
                "DOI": "10.1109/TWC.2020.3035843",
                "CorpusId": 226227344
            },
            "abstract": "This article studies fast adaptive beamforming optimization for the signal-to-interference-plus-noise ratio balancing problem in a multiuser multiple-input single-output downlink system. Existing deep learning based approaches to predict beamforming rely on the assumption that the training and testing channels follow the same distribution which may not hold in practice. As a result, a trained model may lead to performance deterioration when the testing network environment changes. To deal with this task mismatch issue, we propose two offline adaptive algorithms based on deep transfer learning and meta-learning, which are able to achieve fast adaptation with the limited new labelled data when the testing wireless environment changes. Furthermore, we propose an online algorithm to enhance the adaptation capability of the offline meta algorithm in realistic non-stationary environments. Simulation results demonstrate that the proposed adaptive algorithms achieve much better performance than the direct deep learning algorithm without adaptation in new environments. The meta-learning algorithm outperforms the deep transfer learning algorithm and achieves near optimal performance. In addition, compared to the offline meta-learning algorithm, the proposed online meta-learning algorithm shows superior adaption performance in changing environments.",
            "referenceCount": 47,
            "citationCount": 49,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://figshare.com/articles/journal_contribution/Transfer_learning_and_meta_learning-based_fast_downlink_beamforming_adaptation/14242088/1/files/26886647.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-11-02",
            "journal": {
                "name": "IEEE Transactions on Wireless Communications",
                "volume": "20"
            },
            "citationStyles": {
                "bibtex": "@Article{Yuan2020TransferLA,\n author = {Yi Yuan and G. Zheng and Kai\u2010Kit Wong and B. Ottersten and Z. Luo},\n booktitle = {IEEE Transactions on Wireless Communications},\n journal = {IEEE Transactions on Wireless Communications},\n pages = {1742-1755},\n title = {Transfer Learning and Meta Learning-Based Fast Downlink Beamforming Adaptation},\n volume = {20},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1096c9e27782cfb55b863526ab5a79b318a91526",
            "@type": "ScholarlyArticle",
            "paperId": "1096c9e27782cfb55b863526ab5a79b318a91526",
            "corpusId": 218673556,
            "url": "https://www.semanticscholar.org/paper/1096c9e27782cfb55b863526ab5a79b318a91526",
            "title": "Meta-learning with Stochastic Linear Bandits",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2005-08531",
                "MAG": "3034618193",
                "ArXiv": "2005.08531",
                "CorpusId": 218673556
            },
            "abstract": "We investigate meta-learning procedures in the setting of stochastic linear bandits tasks. The goal is to select a learning algorithm which works well on average over a class of bandits tasks, that are sampled from a task-distribution. Inspired by recent work on learning-to-learn linear regression, we consider a class of bandit algorithms that implement a regularized version of the well-known OFUL algorithm, where the regularization is a square euclidean distance to a bias vector. We first study the benefit of the biased OFUL algorithm in terms of regret minimization. We then propose two strategies to estimate the bias within the learning-to-learn setting. We show both theoretically and experimentally, that when the number of tasks grows and the variance of the task-distribution is small, our strategies have a significant advantage over learning the tasks in isolation.",
            "referenceCount": 40,
            "citationCount": 48,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-05-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2005.08531"
            },
            "citationStyles": {
                "bibtex": "@Article{Cella2020MetalearningWS,\n author = {Leonardo Cella and A. Lazaric and M. Pontil},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Meta-learning with Stochastic Linear Bandits},\n volume = {abs/2005.08531},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f5cd085a826825d843bf3a1bceab93418570396c",
            "@type": "ScholarlyArticle",
            "paperId": "f5cd085a826825d843bf3a1bceab93418570396c",
            "corpusId": 222291663,
            "url": "https://www.semanticscholar.org/paper/f5cd085a826825d843bf3a1bceab93418570396c",
            "title": "How Important is the Train-Validation Split in Meta-Learning?",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2010.05843",
                "DBLP": "journals/corr/abs-2010-05843",
                "MAG": "3092407976",
                "CorpusId": 222291663
            },
            "abstract": "Meta-learning aims to perform fast adaptation on a new task through learning a \"prior\" from multiple existing tasks. A common practice in meta-learning is to perform a train-validation split where the prior adapts to the task on one split of the data, and the resulting predictor is evaluated on another split. Despite its prevalence, the importance of the train-validation split is not well understood either in theory or in practice, particularly in comparison to the more direct non-splitting method, which uses all the per-task data for both training and evaluation. \nWe provide a detailed theoretical study on whether and when the train-validation split is helpful on the linear centroid meta-learning problem, in the asymptotic setting where the number of tasks goes to infinity. We show that the splitting method converges to the optimal prior as expected, whereas the non-splitting method does not in general without structural assumptions on the data. In contrast, if the data are generated from linear models (the realizable regime), we show that both the splitting and non-splitting methods converge to the optimal prior. Further, perhaps surprisingly, our main result shows that the non-splitting method achieves a strictly better asymptotic excess risk under this data distribution, even when the regularization parameter and split ratio are optimally tuned for both methods. Our results highlight that data splitting may not always be preferable, especially when the data is realizable by the model. We validate our theories by experimentally showing that the non-splitting method can indeed outperform the splitting method, on both simulations and real meta-learning tasks.",
            "referenceCount": 59,
            "citationCount": 45,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-10-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2010.05843"
            },
            "citationStyles": {
                "bibtex": "@Article{Bai2020HowII,\n author = {Yu Bai and Minshuo Chen and Pan Zhou and T. Zhao and J. Lee and S. Kakade and Haiquan Wang and Caiming Xiong},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {How Important is the Train-Validation Split in Meta-Learning?},\n volume = {abs/2010.05843},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cb4423652b23f12bf19dcbf7828128e0b9364904",
            "@type": "ScholarlyArticle",
            "paperId": "cb4423652b23f12bf19dcbf7828128e0b9364904",
            "corpusId": 211252736,
            "url": "https://www.semanticscholar.org/paper/cb4423652b23f12bf19dcbf7828128e0b9364904",
            "title": "Few-Shot Acoustic Event Detection Via Meta Learning",
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "publicationVenue": {
                "id": "urn:research:0d6f7fba-7092-46b3-8039-93458dba736b",
                "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                "alternate_names": [
                    "Int Conf Acoust Speech Signal Process",
                    "IEEE Int Conf Acoust Speech Signal Process",
                    "ICASSP",
                    "International Conference on Acoustics, Speech, and Signal Processing"
                ],
                "issn": null,
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3015474227",
                "ArXiv": "2002.09143",
                "DBLP": "conf/icassp/ShiSPKMW20",
                "DOI": "10.1109/ICASSP40776.2020.9053336",
                "CorpusId": 211252736
            },
            "abstract": "We study few-shot acoustic event detection (AED) in this paper. Few-shot learning enables detection of new events with very limited labeled data. Compared to other research areas like computer vision, few-shot learning for audio recognition has been under-studied. We formulate few-shot AED problem and explore different ways of utilizing traditional supervised methods for this setting as well as a variety of meta-learning approaches, which are conventionally used to solve few-shot classification problem. Compared to supervised baselines, meta-learning models achieve superior performance, thus showing its effectiveness on generalization to new audio events. Our analysis including impact of initialization and domain discrepancy further validate the advantage of meta-learning approaches in few-shot AED.",
            "referenceCount": 21,
            "citationCount": 52,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2002.09143",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-02-21",
            "journal": {
                "name": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Shi2020FewShotAE,\n author = {Bowen Shi and Ming Sun and Krishna C. Puvvada and Chieh-Chi Kao and Spyros Matsoukas and Chao Wang},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {76-80},\n title = {Few-Shot Acoustic Event Detection Via Meta Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:15a33303f1a132f04dad5aa80bdae4aba714c69f",
            "@type": "ScholarlyArticle",
            "paperId": "15a33303f1a132f04dad5aa80bdae4aba714c69f",
            "corpusId": 219721135,
            "url": "https://www.semanticscholar.org/paper/15a33303f1a132f04dad5aa80bdae4aba714c69f",
            "title": "Convergence of Meta-Learning with Task-Specific Adaptation over Partial Parameters",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3035840208",
                "DBLP": "journals/corr/abs-2006-09486",
                "ArXiv": "2006.09486",
                "CorpusId": 219721135
            },
            "abstract": "Although model-agnostic meta-learning (MAML) is a very successful algorithm in meta-learning practice, it can have high computational cost because it updates all model parameters over both the inner loop of task-specific adaptation and the outer-loop of meta initialization training. A more efficient algorithm ANIL (which refers to almost no inner loop) was proposed recently by Raghu et al. 2019, which adapts only a small subset of parameters in the inner loop and thus has substantially less computational cost than MAML as demonstrated by extensive experiments. However, the theoretical convergence of ANIL has not been studied yet. In this paper, we characterize the convergence rate and the computational complexity for ANIL under two representative inner-loop loss geometries, i.e., strongly-convexity and nonconvexity. Our results show that such a geometric property can significantly affect the overall convergence performance of ANIL. For example, ANIL achieves a faster convergence rate for a strongly-convex inner-loop loss as the number $N$ of inner-loop gradient descent steps increases, but a slower convergence rate for a nonconvex inner-loop loss as $N$ increases. Moreover, our complexity analysis provides a theoretical quantification on the improved efficiency of ANIL over MAML. The experiments on standard few-shot meta-learning benchmarks validate our theoretical findings.",
            "referenceCount": 35,
            "citationCount": 51,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-06-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2006.09486"
            },
            "citationStyles": {
                "bibtex": "@Article{Ji2020ConvergenceOM,\n author = {Kaiyi Ji and J. Lee and Yingbin Liang and H. Poor},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Convergence of Meta-Learning with Task-Specific Adaptation over Partial Parameters},\n volume = {abs/2006.09486},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4bf9f88d438c7d978fb854eba686cf3933879df1",
            "@type": "ScholarlyArticle",
            "paperId": "4bf9f88d438c7d978fb854eba686cf3933879df1",
            "corpusId": 208909759,
            "url": "https://www.semanticscholar.org/paper/4bf9f88d438c7d978fb854eba686cf3933879df1",
            "title": "Meta-Learning without Memorization",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/iclr/YinTZLF20",
                "MAG": "2994871715",
                "ArXiv": "1912.03820",
                "CorpusId": 208909759
            },
            "abstract": "The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of the meta-training tasks zero-shot, but does not adapt effectively to new image classes. This requirement means that the user must take great care in designing the tasks, for example by shuffling labels or removing task identifying information from the inputs. In some domains, this makes meta-learning entirely inapplicable. In this paper, we address this challenge by designing a meta-regularization objective using information theory that places precedence on data-driven adaptation. This causes the meta-learner to decide what must be learned from the task training data and what should be inferred from the task testing input. By doing so, our algorithm can successfully use data from non-mutually-exclusive tasks to efficiently adapt to novel tasks. We demonstrate its applicability to both contextual and gradient-based meta-learning algorithms, and apply it in practical settings where applying standard meta-learning has been difficult. Our approach substantially outperforms standard meta-learning algorithms in these settings.",
            "referenceCount": 41,
            "citationCount": 151,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-12-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1912.03820"
            },
            "citationStyles": {
                "bibtex": "@Article{Yin2019MetaLearningWM,\n author = {Mingzhang Yin and G. Tucker and Mingyuan Zhou and S. Levine and Chelsea Finn},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Meta-Learning without Memorization},\n volume = {abs/1912.03820},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:11236ae4f31b428b6313559fb99300643c172cf9",
            "@type": "ScholarlyArticle",
            "paperId": "11236ae4f31b428b6313559fb99300643c172cf9",
            "corpusId": 212657478,
            "url": "https://www.semanticscholar.org/paper/11236ae4f31b428b6313559fb99300643c172cf9",
            "title": "Meta-learning curiosity algorithms",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3010930801",
                "DBLP": "journals/corr/abs-2003-05325",
                "ArXiv": "2003.05325",
                "CorpusId": 212657478
            },
            "abstract": "Exploration is a key component of successful reinforcement learning, but optimal approaches are computationally intractable, so researchers have focused on hand-designing mechanisms based on exploration bonuses and intrinsic reward, some inspired by curious behavior in natural systems. In this work, we propose a strategy for encoding curiosity algorithms as programs in a domain-specific language and searching, during a meta-learning phase, for algorithms that enable RL agents to perform well in new domains. Our rich language of programs, which can combine neural networks with other building blocks including nearest-neighbor modules and can choose its own loss functions, enables the expression of highly generalizable programs that perform well in domains as disparate as grid navigation with image input, acrobot, lunar lander, ant and hopper. To make this approach feasible, we develop several pruning techniques, including learning to predict a program's success based on its syntactic properties. We demonstrate the effectiveness of the approach empirically, finding curiosity strategies that are similar to those in published literature, as well as novel strategies that are competitive with them and generalize well.",
            "referenceCount": 77,
            "citationCount": 49,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-03-11",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2003.05325"
            },
            "citationStyles": {
                "bibtex": "@Article{Alet2020MetalearningCA,\n author = {Ferran Alet and Martin Schneider and Tomas Lozano-Perez and L. Kaelbling},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Meta-learning curiosity algorithms},\n volume = {abs/2003.05325},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fe476b6d713ea4451cc04d28935c092694908d35",
            "@type": "ScholarlyArticle",
            "paperId": "fe476b6d713ea4451cc04d28935c092694908d35",
            "corpusId": 214083899,
            "url": "https://www.semanticscholar.org/paper/fe476b6d713ea4451cc04d28935c092694908d35",
            "title": "Meta-RCNN: Meta Learning for Few-Shot Object Detection",
            "venue": "ACM Multimedia",
            "publicationVenue": {
                "id": "urn:research:f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                "name": "ACM Multimedia",
                "alternate_names": [
                    "MM"
                ],
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "MAG": "2997616671",
                "DBLP": "conf/mm/WuSH20",
                "DOI": "10.1145/3394171.3413832",
                "CorpusId": 214083899
            },
            "abstract": "Despite significant advances in deep learning based object detection in recent years, training effective detectors in a small data regime remains an open challenge. This is very important since labelling training data for object detection is often very expensive and time-consuming. In this paper, we investigate the problem of few-shot object detection, where a detector has access to only limited amounts of annotated data. Based on the meta-learning principle, we propose a new meta-learning framework for object detection named \"Meta-RCNN\", which learns the ability to perform few-shot detection via meta-learning. Specifically, Meta-RCNN learns an object detector in an episodic learning paradigm on the (meta) training data. This learning scheme helps acquire a prior which enables Meta-RCNN to do few-shot detection on novel tasks. Built on top of the popular Faster RCNN detector, in Meta-RCNN, both the Region Proposal Network (RPN) and the object classification branch are meta-learned. The meta-trained RPN learns to provide class-specific proposals, while the object classifier learns to do few-shot classification. The novel loss objectives and learning strategy of Meta-RCNN can be trained in an end-to-end manner. We demonstrate the effectiveness of Meta-RCNN in few-shot detection on three datasets (Pascal-VOC, ImageNet-LOC and MSCOCO) with promising results.",
            "referenceCount": 29,
            "citationCount": 45,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book"
            ],
            "publicationDate": "2020-10-12",
            "journal": {
                "name": "Proceedings of the 28th ACM International Conference on Multimedia",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wu2020MetaRCNNML,\n author = {Xiongwei Wu and Doyen Sahoo and S. Hoi},\n booktitle = {ACM Multimedia},\n journal = {Proceedings of the 28th ACM International Conference on Multimedia},\n title = {Meta-RCNN: Meta Learning for Few-Shot Object Detection},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dc18c9408ebdb795bd2c9e5f0f5cd3a6b07fa5c2",
            "@type": "ScholarlyArticle",
            "paperId": "dc18c9408ebdb795bd2c9e5f0f5cd3a6b07fa5c2",
            "corpusId": 209862299,
            "url": "https://www.semanticscholar.org/paper/dc18c9408ebdb795bd2c9e5f0f5cd3a6b07fa5c2",
            "title": "From Learning to Meta-Learning: Reduced Training Overhead and Complexity for Communication Systems",
            "venue": "6G Wireless Summit",
            "publicationVenue": {
                "id": "urn:research:ad32af82-64e5-472a-afa9-ba24a2f0cc8a",
                "name": "6G Wireless Summit",
                "alternate_names": [
                    "6G SUMMIT",
                    "6G Wirel Summit"
                ],
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "MAG": "2997249405",
                "ArXiv": "2001.01227",
                "DBLP": "journals/corr/abs-2001-01227",
                "DOI": "10.1109/6GSUMMIT49458.2020.9083856",
                "CorpusId": 209862299
            },
            "abstract": "Machine learning methods adapt the parameters of a model, constrained to lie in a given model class, by using a fixed learning procedure based on data or active observations. Adaptation is done on a per-task basis, and retraining is needed when the system configuration changes. The resulting inefficiency in terms of data and training time requirements can be mitigated, if domain knowledge is available, by selecting a suitable model class and learning procedure, collectively known as inductive bias. However, it is generally difficult to encode prior knowledge into an inductive bias, particularly with black-box model classes such as neural networks. Meta-learning provides a way to automatize the selection of an inductive bias. Meta-learning leverages data or active observations from tasks that are expected to be related to future, and a priori unknown, tasks of interest. With a meta-trained inductive bias, training of a machine learning model can be potentially carried out with reduced training data and/or time complexity. This paper provides a high-level introduction to meta-learning with applications to communication systems.",
            "referenceCount": 34,
            "citationCount": 50,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2001.01227",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-01-05",
            "journal": {
                "name": "2020 2nd 6G Wireless Summit (6G SUMMIT)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Simeone2020FromLT,\n author = {O. Simeone and Sangwoo Park and Joonhyuk Kang},\n booktitle = {6G Wireless Summit},\n journal = {2020 2nd 6G Wireless Summit (6G SUMMIT)},\n pages = {1-5},\n title = {From Learning to Meta-Learning: Reduced Training Overhead and Complexity for Communication Systems},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:acbd97dbb88658aaa0f88499d1e207ea51962871",
            "@type": "ScholarlyArticle",
            "paperId": "acbd97dbb88658aaa0f88499d1e207ea51962871",
            "corpusId": 232096126,
            "url": "https://www.semanticscholar.org/paper/acbd97dbb88658aaa0f88499d1e207ea51962871",
            "title": "Meta-Learning via Hypernetworks",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3130108494",
                "DOI": "10.3929/ETHZ-B-000465883",
                "CorpusId": 232096126
            },
            "abstract": "Recent developments in few-shot learning have shown that during fast adaption, gradient-based meta-learners mostly rely on embedding features of powerful pretrained networks. This leads us to research ways to effectively adapt features and utilize the meta-learner's full potential. Here, we demonstrate the effectiveness of hypernetworks in this context. We propose a soft row-sharing hypernetwork architecture and show that training the hypernetwork with a variant of MAML is tightly linked to meta-learning a curvature matrix used to condition gradients during fast adaptation. We achieve similar results as state-of-art model-agnostic methods in the overparametrized case, while outperforming many MAML variants without using different optimization schemes in the compressive regime. Furthermore, we empirically show that hypernetworks do leverage the inner loop optimization for better adaptation, and analyse how they naturally try to learn the shared curvature of constructed tasks on a toy problem when using our proposed training algorithm.",
            "referenceCount": 26,
            "citationCount": 40,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2020-12-11",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Zhao2020MetaLearningVH,\n author = {Dominic Zhao and J. Oswald and Seijin Kobayashi and J. Sacramento and B. Grewe},\n title = {Meta-Learning via Hypernetworks},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:828887ff314e946f5f288eca96a2bbc206785f51",
            "@type": "ScholarlyArticle",
            "paperId": "828887ff314e946f5f288eca96a2bbc206785f51",
            "corpusId": 218469264,
            "url": "https://www.semanticscholar.org/paper/828887ff314e946f5f288eca96a2bbc206785f51",
            "title": "A meta-learning approach for genomic survival analysis",
            "venue": "Nature Communications",
            "publicationVenue": {
                "id": "urn:research:43b3f0f9-489a-4566-8164-02fafde3cd98",
                "name": "Nature Communications",
                "alternate_names": [
                    "Nat Commun"
                ],
                "issn": "2041-1723",
                "url": "https://www.nature.com/ncomms/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3112653481",
                "PubMedCentral": "7733508",
                "DOI": "10.1038/s41467-020-20167-3",
                "CorpusId": 218469264,
                "PubMed": "33311484"
            },
            "abstract": null,
            "referenceCount": 80,
            "citationCount": 40,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41467-020-20167-3.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Biology",
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-04-23",
            "journal": {
                "name": "Nature Communications",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Qiu2020AMA,\n author = {Y. Qiu and Hong Zheng and A. Devos and H. Selby and O. Gevaert},\n booktitle = {Nature Communications},\n journal = {Nature Communications},\n title = {A meta-learning approach for genomic survival analysis},\n volume = {11},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:76e3ad12881e7ab1c36318d8f8818eca3f828349",
            "@type": "ScholarlyArticle",
            "paperId": "76e3ad12881e7ab1c36318d8f8818eca3f828349",
            "corpusId": 229924109,
            "url": "https://www.semanticscholar.org/paper/76e3ad12881e7ab1c36318d8f8818eca3f828349",
            "title": "Meta Learning Backpropagation And Improving It",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2012-14905",
                "ArXiv": "2012.14905",
                "CorpusId": 229924109
            },
            "abstract": "Many concepts have been proposed for meta learning with neural networks (NNs), e.g., NNs that learn to reprogram fast weights, Hebbian plasticity, learned learning rules, and meta recurrent NNs. Our Variable Shared Meta Learning (VSML) unifies the above and demonstrates that simple weight-sharing and sparsity in an NN is sufficient to express powerful learning algorithms (LAs) in a reusable fashion. A simple implementation of VSML where the weights of a neural network are replaced by tiny LSTMs allows for implementing the backpropagation LA solely by running in forward-mode. It can even meta learn new LAs that differ from online backpropagation and generalize to datasets outside of the meta training distribution without explicit gradient calculation. Introspection reveals that our meta learned LAs learn through fast association in a way that is qualitatively different from gradient descent.",
            "referenceCount": 63,
            "citationCount": 44,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-12-29",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2012.14905"
            },
            "citationStyles": {
                "bibtex": "@Article{Kirsch2020MetaLB,\n author = {Louis Kirsch and J. Schmidhuber},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Meta Learning Backpropagation And Improving It},\n volume = {abs/2012.14905},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9364b669f81ce1454da8fec79810c3ee5d56d85a",
            "@type": "ScholarlyArticle",
            "paperId": "9364b669f81ce1454da8fec79810c3ee5d56d85a",
            "corpusId": 162184501,
            "url": "https://www.semanticscholar.org/paper/9364b669f81ce1454da8fec79810c3ee5d56d85a",
            "title": "Meta-GNN: On Few-shot Node Classification in Graph Meta-learning",
            "venue": "International Conference on Information and Knowledge Management",
            "publicationVenue": {
                "id": "urn:research:7431ff67-91dc-41fa-b322-1b1ca657025f",
                "name": "International Conference on Information and Knowledge Management",
                "alternate_names": [
                    "Conference on Information and Knowledge Management",
                    "Conf Inf Knowl Manag",
                    "Int Conf Inf Knowl Manag",
                    "CIKM"
                ],
                "issn": null,
                "url": "http://www.cikm.org/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2984323660",
                "DBLP": "conf/cikm/0002CZTZG19",
                "ArXiv": "1905.09718",
                "DOI": "10.1145/3357384.3358106",
                "CorpusId": 162184501
            },
            "abstract": "Meta-learning has received a tremendous recent attention as a possible approach for mimicking human intelligence, i.e., acquiring new knowledge and skills with little or even no demonstration. Most of the existing meta-learning methods are proposed to tackle few-shot learning problems such as image and text, in rather Euclidean domain. However, there are very few works applying meta-learning to non-Euclidean domains, and the recently proposed graph neural networks (GNNs) models do not perform effectively on graph few-shot learning problems. Towards this, we propose a novel graph meta-learning framework -- Meta-GNN -- to tackle the few-shot node classification problem in graph meta-learning settings. It obtains the prior knowledge of classifiers by training on many similar few-shot learning tasks and then classifies the nodes from new classes with only few labeled samples. Additionally, Meta-GNN is a general model that can be straightforwardly incorporated into any existing state-of-the-art GNN. Our experiments conducted on three benchmark datasets demonstrate that our proposed approach not only improves the node classification performance by a large margin on few-shot learning problems in meta-learning paradigm, but also learns a more general and flexible model for task adaption.",
            "referenceCount": 17,
            "citationCount": 135,
            "influentialCitationCount": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3357384.3358106",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2019-05-23",
            "journal": {
                "name": "Proceedings of the 28th ACM International Conference on Information and Knowledge Management",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhou2019MetaGNNOF,\n author = {Fan Zhou and Chengtai Cao and Kunpeng Zhang and Goce Trajcevski and Ting Zhong and Ji Geng},\n booktitle = {International Conference on Information and Knowledge Management},\n journal = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},\n title = {Meta-GNN: On Few-shot Node Classification in Graph Meta-learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:36f987b250c75490fd19ba730b8dad7908b6d87d",
            "@type": "ScholarlyArticle",
            "paperId": "36f987b250c75490fd19ba730b8dad7908b6d87d",
            "corpusId": 67856090,
            "url": "https://www.semanticscholar.org/paper/36f987b250c75490fd19ba730b8dad7908b6d87d",
            "title": "Provable Guarantees for Gradient-Based Meta-Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/icml/BalcanKT19",
                "MAG": "2952689429",
                "ArXiv": "1902.10644",
                "CorpusId": 67856090
            },
            "abstract": "We study the problem of meta-learning through the lens of online convex optimization, developing a meta-algorithm bridging the gap between popular gradient-based meta-learning and classical regularization-based multi-task transfer methods. Our method is the first to simultaneously satisfy good sample efficiency guarantees in the convex setting, with generalization bounds that improve with task-similarity, while also being computationally scalable to modern deep learning architectures and the many-task setting. Despite its simplicity, the algorithm matches, up to a constant factor, a lower bound on the performance of any such parameter-transfer method under natural task similarity assumptions. We use experiments in both convex and deep learning settings to verify and demonstrate the applicability of our theory.",
            "referenceCount": 56,
            "citationCount": 129,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-02-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1902.10644"
            },
            "citationStyles": {
                "bibtex": "@Article{Khodak2019ProvableGF,\n author = {M. Khodak and Maria-Florina Balcan and Ameet Talwalkar},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Provable Guarantees for Gradient-Based Meta-Learning},\n volume = {abs/1902.10644},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:44834accf04f5c029f79e7a21f6715b74872ac94",
            "@type": "ScholarlyArticle",
            "paperId": "44834accf04f5c029f79e7a21f6715b74872ac94",
            "corpusId": 201652627,
            "url": "https://www.semanticscholar.org/paper/44834accf04f5c029f79e7a21f6715b74872ac94",
            "title": "Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2019,
            "externalIds": {
                "ACL": "D19-1112",
                "ArXiv": "1908.10423",
                "MAG": "2971078254",
                "DBLP": "conf/emnlp/DouYA19",
                "DOI": "10.18653/v1/D19-1112",
                "CorpusId": 201652627
            },
            "abstract": "Learning general representations of text is a fundamental problem for many natural language understanding (NLU) tasks. Previously, researchers have proposed to use language model pre-training and multi-task learning to learn robust representations. However, these methods can achieve sub-optimal performance in low-resource scenarios. Inspired by the recent success of optimization-based meta-learning algorithms, in this paper, we explore the model-agnostic meta-learning algorithm (MAML) and its variants for low-resource NLU tasks. We validate our methods on the GLUE benchmark and show that our proposed models can outperform several strong baselines. We further empirically demonstrate that the learned representations can be adapted to new tasks efficiently and effectively.",
            "referenceCount": 38,
            "citationCount": 112,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1908.10423",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-08-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1908.10423"
            },
            "citationStyles": {
                "bibtex": "@Article{Dou2019InvestigatingMA,\n author = {Zi-Yi Dou and Keyi Yu and Antonios Anastasopoulos},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks},\n volume = {abs/1908.10423},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1fd96af3a684b04b257e0063d15667cb1e8acb03",
            "@type": "ScholarlyArticle",
            "paperId": "1fd96af3a684b04b257e0063d15667cb1e8acb03",
            "corpusId": 221669134,
            "url": "https://www.semanticscholar.org/paper/1fd96af3a684b04b257e0063d15667cb1e8acb03",
            "title": "On Modulating the Gradient for Meta-learning",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3095891659",
                "DBLP": "conf/eccv/SimonKNH20",
                "DOI": "10.1007/978-3-030-58598-3_33",
                "CorpusId": 221669134
            },
            "abstract": null,
            "referenceCount": 45,
            "citationCount": 36,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Simon2020OnMT,\n author = {Christian Simon and Piotr Koniusz and R. Nock and M. Harandi},\n booktitle = {European Conference on Computer Vision},\n pages = {556-572},\n title = {On Modulating the Gradient for Meta-learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8a57fdca1ee31d67d3e538ea2a895d3f99476860",
            "@type": "ScholarlyArticle",
            "paperId": "8a57fdca1ee31d67d3e538ea2a895d3f99476860",
            "corpusId": 218581599,
            "url": "https://www.semanticscholar.org/paper/8a57fdca1ee31d67d3e538ea2a895d3f99476860",
            "title": "Information-Theoretic Generalization Bounds for Meta-Learning and Applications",
            "venue": "Entropy",
            "publicationVenue": {
                "id": "urn:research:8270cfe1-3713-4325-a7bd-c6a87eed889e",
                "name": "Entropy",
                "alternate_names": null,
                "issn": "1099-4300",
                "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-155606"
            },
            "year": 2020,
            "externalIds": {
                "PubMedCentral": "7835863",
                "MAG": "3023926204",
                "ArXiv": "2005.04372",
                "DBLP": "journals/corr/abs-2005-04372",
                "DOI": "10.3390/e23010126",
                "CorpusId": 218581599,
                "PubMed": "33478002"
            },
            "abstract": "Meta-learning, or \u201clearning to learn\u201d, refers to techniques that infer an inductive bias from data corresponding to multiple related tasks with the goal of improving the sample efficiency for new, previously unobserved, tasks. A key performance measure for meta-learning is the meta-generalization gap, that is, the difference between the average loss measured on the meta-training data and on a new, randomly selected task. This paper presents novel information-theoretic upper bounds on the meta-generalization gap. Two broad classes of meta-learning algorithms are considered that use either separate within-task training and test sets, like model agnostic meta-learning (MAML), or joint within-task training and test sets, like reptile. Extending the existing work for conventional learning, an upper bound on the meta-generalization gap is derived for the former class that depends on the mutual information (MI) between the output of the meta-learning algorithm and its input meta-training data. For the latter, the derived bound includes an additional MI between the output of the per-task learning procedure and corresponding data set to capture within-task uncertainty. Tighter bounds are then developed for the two classes via novel individual task MI (ITMI) bounds. Applications of the derived bounds are finally discussed, including a broad class of noisy iterative algorithms for meta-learning.",
            "referenceCount": 62,
            "citationCount": 36,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/1099-4300/23/1/126/pdf?version=1611059555",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-05-09",
            "journal": {
                "name": "Entropy",
                "volume": "23"
            },
            "citationStyles": {
                "bibtex": "@Article{Jose2020InformationTheoreticGB,\n author = {Sharu Theresa Jose and O. Simeone},\n booktitle = {Entropy},\n journal = {Entropy},\n title = {Information-Theoretic Generalization Bounds for Meta-Learning and Applications},\n volume = {23},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:69a575abf40636c967be86a0bde9fda65ca3375a",
            "@type": "ScholarlyArticle",
            "paperId": "69a575abf40636c967be86a0bde9fda65ca3375a",
            "corpusId": 211204795,
            "url": "https://www.semanticscholar.org/paper/69a575abf40636c967be86a0bde9fda65ca3375a",
            "title": "Meta-learning for mixed linear regression",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3034402086",
                "ArXiv": "2002.08936",
                "DBLP": "journals/corr/abs-2002-08936",
                "CorpusId": 211204795
            },
            "abstract": "In modern supervised learning, there are a large number of tasks, but many of them are associated with only a small amount of labeled data. These include data from medical image processing and robotic interaction. Even though each individual task cannot be meaningfully trained in isolation, one seeks to meta-learn across the tasks from past experiences by exploiting some similarities. We study a fundamental question of interest: When can abundant tasks with small data compensate for lack of tasks with big data? We focus on a canonical scenario where each task is drawn from a mixture of $k$ linear regressions, and identify sufficient conditions for such a graceful exchange to hold; The total number of examples necessary with only small data tasks scales similarly as when big data tasks are available. To this end, we introduce a novel spectral approach and show that we can efficiently utilize small data tasks with the help of $\\tilde\\Omega(k^{3/2})$ medium data tasks each with $\\tilde\\Omega(k^{1/2})$ examples.",
            "referenceCount": 59,
            "citationCount": 48,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-02-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kong2020MetalearningFM,\n author = {Weihao Kong and Raghav Somani and Zhao Song and S. Kakade and Sewoong Oh},\n booktitle = {International Conference on Machine Learning},\n pages = {5394-5404},\n title = {Meta-learning for mixed linear regression},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:508b65cb9a4ddc077dc421dd2fb75f99f03919ac",
            "@type": "ScholarlyArticle",
            "paperId": "508b65cb9a4ddc077dc421dd2fb75f99f03919ac",
            "corpusId": 220496220,
            "url": "https://www.semanticscholar.org/paper/508b65cb9a4ddc077dc421dd2fb75f99f03919ac",
            "title": "Theoretical Convergence of Multi-Step Model-Agnostic Meta-Learning",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/jmlr/JiYL22",
                "MAG": "3042103612",
                "CorpusId": 220496220
            },
            "abstract": "As a popular meta-learning approach, the model-agnostic meta-learning (MAML) algorithm has been widely used due to its simplicity and effectiveness. However, the convergence of the general multi-step MAML still remains unexplored. In this paper, we develop a new theoretical framework to provide such convergence guarantee for two types of objective functions that are of interest in practice: (a) resampling case (e.g., reinforcement learning), where loss functions take the form in expectation and new data are sampled as the algorithm runs; and (b) finite-sum case (e.g., supervised learning), where loss functions take the finite-sum form with given samples. For both cases, we characterize the convergence rate and the computational complexity to attain an $\\epsilon$-accurate solution for multi-step MAML in the general nonconvex setting. In particular, our results suggest that an inner-stage stepsize needs to be chosen inversely proportional to the number $N$ of inner-stage steps in order for $N$-step MAML to have guaranteed convergence. From the technical perspective, we develop novel techniques to deal with the nested structure of the meta gradient for multi-step MAML, which can be of independent interest.",
            "referenceCount": 57,
            "citationCount": 32,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-02-18",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "23"
            },
            "citationStyles": {
                "bibtex": "@Article{Ji2020TheoreticalCO,\n author = {Kaiyi Ji and Junjie Yang and Yingbin Liang},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {29:1-29:41},\n title = {Theoretical Convergence of Multi-Step Model-Agnostic Meta-Learning},\n volume = {23},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4aebc6a6c1390c62e37a6bf0c4ddd390d7bf3983",
            "@type": "ScholarlyArticle",
            "paperId": "4aebc6a6c1390c62e37a6bf0c4ddd390d7bf3983",
            "corpusId": 219956342,
            "url": "https://www.semanticscholar.org/paper/4aebc6a6c1390c62e37a6bf0c4ddd390d7bf3983",
            "title": "Task-Robust Model-Agnostic Meta-Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3101367184",
                "DBLP": "conf/nips/CollinsMS20",
                "CorpusId": 219956342
            },
            "abstract": "Meta-learning methods have shown an impressive ability to train models that rapidly learn new tasks. However, these methods only aim to perform well in expectation over tasks coming from some particular distribution that is typically equivalent across meta-training and meta-testing, rather than considering worst-case task performance. In this work we introduce the notion of \"task-robustness\" by reformulating the popular Model-Agnostic Meta-Learning (MAML) objective [Finn et al. 2017] such that the goal is to minimize the maximum loss over the observed meta-training tasks. The solution to this novel formulation is task-robust in the sense that it places equal importance on even the most difficult and/or rare tasks. This also means that it performs well over all distributions of the observed tasks, making it robust to shifts in the task distribution between meta-training and meta-testing. We present an algorithm to solve the proposed min-max problem, and show that it converges to an $\\epsilon$-accurate point at the optimal rate of $\\mathcal{O}(1/\\epsilon^2)$ in the convex setting and to an $(\\epsilon, \\delta)$-stationary point at the rate of $\\mathcal{O}(\\max\\{1/\\epsilon^5, 1/\\delta^5\\})$ in nonconvex settings. We also provide an upper bound on the new task generalization error that captures the advantage of minimizing the worst-case task loss, and demonstrate this advantage in sinusoid regression and image classification experiments.",
            "referenceCount": 44,
            "citationCount": 35,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-02-12",
            "journal": {
                "name": "arXiv: Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Collins2020TaskRobustMM,\n author = {Liam Collins and Aryan Mokhtari and S. Shakkottai},\n booktitle = {Neural Information Processing Systems},\n journal = {arXiv: Learning},\n title = {Task-Robust Model-Agnostic Meta-Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7177443fefc24c22c139a6ca22d3195e314dad57",
            "@type": "ScholarlyArticle",
            "paperId": "7177443fefc24c22c139a6ca22d3195e314dad57",
            "corpusId": 220633423,
            "url": "https://www.semanticscholar.org/paper/7177443fefc24c22c139a6ca22d3195e314dad57",
            "title": "Adaptive Task Sampling for Meta-Learning",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2007.08735",
                "DBLP": "conf/eccv/LiuWSFZH20",
                "MAG": "3042330800",
                "DOI": "10.1007/978-3-030-58523-5_44",
                "CorpusId": 220633423
            },
            "abstract": null,
            "referenceCount": 64,
            "citationCount": 32,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-07-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2020AdaptiveTS,\n author = {Chenghao Liu and Zhihao Wang and Doyen Sahoo and Yuan Fang and Kun Zhang and S. Hoi},\n booktitle = {European Conference on Computer Vision},\n pages = {752-769},\n title = {Adaptive Task Sampling for Meta-Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f1b5f17f836bf96d144dc2dd9b669440159abc84",
            "@type": "ScholarlyArticle",
            "paperId": "f1b5f17f836bf96d144dc2dd9b669440159abc84",
            "corpusId": 219980948,
            "url": "https://www.semanticscholar.org/paper/f1b5f17f836bf96d144dc2dd9b669440159abc84",
            "title": "On the Global Optimality of Model-Agnostic Meta-Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2006-13182",
                "ArXiv": "2006.13182",
                "MAG": "3034334460",
                "CorpusId": 219980948
            },
            "abstract": "Model-agnostic meta-learning (MAML) formulates meta-learning as a bilevel optimization problem, where the inner level solves each subtask based on a shared prior, while the outer level searches for the optimal shared prior by optimizing its aggregated performance over all the subtasks. Despite its empirical success, MAML remains less understood in theory, especially in terms of its global optimality, due to the nonconvexity of the meta-objective (the outer-level objective). To bridge such a gap between theory and practice, we characterize the optimality gap of the stationary points attained by MAML for both reinforcement learning and supervised learning, where the inner-level and outer-level problems are solved via first-order optimization methods. In particular, our characterization connects the optimality gap of such stationary points with (i) the functional geometry of inner-level objectives and (ii) the representation power of function approximators, including linear models and neural networks. To the best of our knowledge, our analysis establishes the global optimality of MAML with nonconvex meta-objectives for the first time.",
            "referenceCount": 59,
            "citationCount": 32,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-06-23",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2020OnTG,\n author = {Lingxiao Wang and Qi Cai and Zhuoran Yang and Zhaoran Wang},\n booktitle = {International Conference on Machine Learning},\n pages = {9837-9846},\n title = {On the Global Optimality of Model-Agnostic Meta-Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2686aef553af3ebd43cb0ea98a538a9435f7dd5f",
            "@type": "ScholarlyArticle",
            "paperId": "2686aef553af3ebd43cb0ea98a538a9435f7dd5f",
            "corpusId": 59517133,
            "url": "https://www.semanticscholar.org/paper/2686aef553af3ebd43cb0ea98a538a9435f7dd5f",
            "title": "Amortized Bayesian Meta-Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2908369924",
                "DBLP": "conf/iclr/RaviB19",
                "CorpusId": 59517133
            },
            "abstract": "Meta-learning has proven to be a successful strategy in attacking problems in supervised learning and reinforcement learning that involve small amounts of data. State-of-the-art solutions involve learning an initialization and/or learning algorithm using a set of training episodes so that the meta-learner can generalize to an evaluation episode quickly. These methods perform well but often lack good quanti\ufb01cation of uncertainty, which can be vital to real-world applications when data is lacking. We propose a meta-learning method which ef\ufb01ciently amortizes hierarchical variational inference across tasks, learning a prior distribution over neural network weights so that a few steps of Bayes by Backprop will produce a good task-speci\ufb01c approximate posterior. We show that our method produces good uncertainty estimates on contextual bandit and few-shot learning benchmarks",
            "referenceCount": 34,
            "citationCount": 124,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-27",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Ravi2018AmortizedBM,\n author = {S. Ravi and Alex Beatson},\n booktitle = {International Conference on Learning Representations},\n title = {Amortized Bayesian Meta-Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f94dd0b22fd2fe322a45c2143bd8eb48ca485b5f",
            "@type": "ScholarlyArticle",
            "paperId": "f94dd0b22fd2fe322a45c2143bd8eb48ca485b5f",
            "corpusId": 203737365,
            "url": "https://www.semanticscholar.org/paper/f94dd0b22fd2fe322a45c2143bd8eb48ca485b5f",
            "title": "Generalized Inner Loop Meta-Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1910-01727",
                "MAG": "2978409868",
                "ArXiv": "1910.01727",
                "CorpusId": 203737365
            },
            "abstract": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, higher, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.",
            "referenceCount": 40,
            "citationCount": 139,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1910.01727"
            },
            "citationStyles": {
                "bibtex": "@Article{Grefenstette2019GeneralizedIL,\n author = {Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Generalized Inner Loop Meta-Learning},\n volume = {abs/1910.01727},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7d552984f34c44a235fbe980b4c91751a96b4e80",
            "@type": "ScholarlyArticle",
            "paperId": "7d552984f34c44a235fbe980b4c91751a96b4e80",
            "corpusId": 229183211,
            "url": "https://www.semanticscholar.org/paper/7d552984f34c44a235fbe980b4c91751a96b4e80",
            "title": "UAV Maneuvering Target Tracking in Uncertain Environments Based on Deep Reinforcement Learning and Meta-Learning",
            "venue": "Remote Sensing",
            "publicationVenue": {
                "id": "urn:research:8e1bd4b5-d5b2-4e22-ba0a-01fe5568d472",
                "name": "Remote Sensing",
                "alternate_names": [
                    "Remote Sens"
                ],
                "issn": "2315-4675",
                "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-169233"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/remotesensing/LiGCA20",
                "MAG": "3101889877",
                "DOI": "10.3390/rs12223789",
                "CorpusId": 229183211
            },
            "abstract": "This paper combines deep reinforcement learning (DRL) with meta-learning and proposes a novel approach, named meta twin delayed deep deterministic policy gradient (Meta-TD3), to realize the control of unmanned aerial vehicle (UAV), allowing a UAV to quickly track a target in an environment where the motion of a target is uncertain. This approach can be applied to a variety of scenarios, such as wildlife protection, emergency aid, and remote sensing. We consider a multi-task experience replay buffer to provide data for the multi-task learning of the DRL algorithm, and we combine meta-learning to develop a multi-task reinforcement learning update method to ensure the generalization capability of reinforcement learning. Compared with the state-of-the-art algorithms, namely the deep deterministic policy gradient (DDPG) and twin delayed deep deterministic policy gradient (TD3), experimental results show that the Meta-TD3 algorithm has achieved a great improvement in terms of both convergence value and convergence rate. In a UAV target tracking problem, Meta-TD3 only requires a few steps to train to enable a UAV to adapt quickly to a new target movement mode more and maintain a better tracking effectiveness.",
            "referenceCount": 38,
            "citationCount": 33,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2072-4292/12/22/3789/pdf?version=1605787362",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-11-18",
            "journal": {
                "name": "Remote. Sens.",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2020UAVMT,\n author = {Bo Li and Z. Gan and Daqing Chen and Dyachenko Sergey Aleksandrovich},\n booktitle = {Remote Sensing},\n journal = {Remote. Sens.},\n pages = {3789},\n title = {UAV Maneuvering Target Tracking in Uncertain Environments Based on Deep Reinforcement Learning and Meta-Learning},\n volume = {12},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7e78b05613f3750ea7da7c7cb61a7c5481a75233",
            "@type": "ScholarlyArticle",
            "paperId": "7e78b05613f3750ea7da7c7cb61a7c5481a75233",
            "corpusId": 221292863,
            "url": "https://www.semanticscholar.org/paper/7e78b05613f3750ea7da7c7cb61a7c5481a75233",
            "title": "The Advantage of Conditional Meta-Learning for Biased Regularization and Fine-Tuning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3103158340",
                "DBLP": "journals/corr/abs-2008-10857",
                "ArXiv": "2008.10857",
                "CorpusId": 221292863
            },
            "abstract": "Biased regularization and fine-tuning are two recent meta-learning approaches. They have been shown to be effective to tackle distributions of tasks, in which the tasks' target vectors are all close to a common meta-parameter vector. However, these methods may perform poorly on heterogeneous environments of tasks, where the complexity of the tasks' distribution cannot be captured by a single meta-parameter vector. We address this limitation by conditional meta-learning, inferring a conditioning function mapping task's side information into a meta-parameter vector that is appropriate for that task at hand. We characterize properties of the environment under which the conditional approach brings a substantial advantage over standard meta-learning and we highlight examples of environments, such as those with multiple clusters, satisfying these properties. We then propose a convex meta-algorithm providing a comparable advantage also in practice. Numerical experiments confirm our theoretical findings.",
            "referenceCount": 44,
            "citationCount": 34,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-08-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2008.10857"
            },
            "citationStyles": {
                "bibtex": "@Article{Denevi2020TheAO,\n author = {Giulia Denevi and M. Pontil and C. Ciliberto},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {The Advantage of Conditional Meta-Learning for Biased Regularization and Fine-Tuning},\n volume = {abs/2008.10857},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:96645793d9961042f735d114d17095722f42e357",
            "@type": "ScholarlyArticle",
            "paperId": "96645793d9961042f735d114d17095722f42e357",
            "corpusId": 214803065,
            "url": "https://www.semanticscholar.org/paper/96645793d9961042f735d114d17095722f42e357",
            "title": "Meta-Learning for Few-Shot NMT Adaptation",
            "venue": "Workshop on Neural Generation and Translation",
            "publicationVenue": {
                "id": "urn:research:ec854e61-2eee-49b1-a2f2-b51ea8231eb0",
                "name": "Workshop on Neural Generation and Translation",
                "alternate_names": [
                    "NGT",
                    "Workshop Neural Gener Transl"
                ],
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3015078597",
                "DBLP": "conf/aclnmt/SharafHD20",
                "ACL": "2020.ngt-1.5",
                "ArXiv": "2004.02745",
                "DOI": "10.18653/V1/2020.NGT-1.5",
                "CorpusId": 214803065
            },
            "abstract": "We present META-MT, a meta-learning approach to adapt Neural Machine Translation (NMT) systems in a few-shot setting. META-MT provides a new approach to make NMT models easily adaptable to many target do- mains with the minimal amount of in-domain data. We frame the adaptation of NMT systems as a meta-learning problem, where we learn to adapt to new unseen domains based on simulated offline meta-training domain adaptation tasks. We evaluate the proposed meta-learning strategy on ten domains with general large scale NMT systems. We show that META-MT significantly outperforms classical domain adaptation when very few in- domain examples are available. Our experiments shows that META-MT can outperform classical fine-tuning by up to 2.5 BLEU points after seeing only 4, 000 translated words (300 parallel sentences).",
            "referenceCount": 41,
            "citationCount": 29,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2004.02745",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-04-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sharaf2020MetaLearningFF,\n author = {Amr Sharaf and Hany Hassan and Hal Daum'e},\n booktitle = {Workshop on Neural Generation and Translation},\n pages = {43-53},\n title = {Meta-Learning for Few-Shot NMT Adaptation},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2e072998dd7b40e9e514b2bb43c8074bd5aa43d2",
            "@type": "ScholarlyArticle",
            "paperId": "2e072998dd7b40e9e514b2bb43c8074bd5aa43d2",
            "corpusId": 220363897,
            "url": "https://www.semanticscholar.org/paper/2e072998dd7b40e9e514b2bb43c8074bd5aa43d2",
            "title": "Meta-Learning Symmetries by Reparameterization",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2007-02933",
                "MAG": "3039743463",
                "ArXiv": "2007.02933",
                "CorpusId": 220363897
            },
            "abstract": "Many successful deep learning architectures are equivariant to certain transformations in order to conserve parameters and improve generalization: most famously, convolution layers are equivariant to shifts of the input. This approach only works when practitioners know a-priori symmetries of the task and can manually construct an architecture with the corresponding equivariances. Our goal is a general approach for learning equivariances from data, without needing prior knowledge of a task's symmetries or custom task-specific architectures. We present a method for learning and encoding equivariances into networks by learning corresponding parameter sharing patterns from data. Our method can provably encode equivariance-inducing parameter sharing for any finite group of symmetry transformations, and we find experimentally that it can automatically learn a variety of equivariances from symmetries in data. We provide our experiment code and pre-trained models at this https URL.",
            "referenceCount": 57,
            "citationCount": 78,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-07-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2007.02933"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhou2020MetaLearningSB,\n author = {Allan Zhou and Tom Knowles and Chelsea Finn},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Meta-Learning Symmetries by Reparameterization},\n volume = {abs/2007.02933},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:74545aba77cceddff63340fb574b224aee014cda",
            "@type": "ScholarlyArticle",
            "paperId": "74545aba77cceddff63340fb574b224aee014cda",
            "corpusId": 212746020,
            "url": "https://www.semanticscholar.org/paper/74545aba77cceddff63340fb574b224aee014cda",
            "title": "Meta-Learning for Generalized Zero-Shot Learning",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/aaai/VermaBR20",
                "MAG": "2997305434",
                "DOI": "10.1609/AAAI.V34I04.6069",
                "CorpusId": 212746020
            },
            "abstract": "Learning to classify unseen class samples at test time is popularly referred to as zero-shot learning (ZSL). If test samples can be from training (seen) as well as unseen classes, it is a more challenging problem due to the existence of strong bias towards seen classes. This problem is generally known as generalized zero-shot learning (GZSL). Thanks to the recent advances in generative models such as VAEs and GANs, sample synthesis based approaches have gained considerable attention for solving this problem. These approaches are able to handle the problem of class bias by synthesizing unseen class samples. However, these ZSL/GZSL models suffer due to the following key limitations: (i) Their training stage learns a class-conditioned generator using only seen class data and the training stage does not explicitly learn to generate the unseen class samples; (ii) They do not learn a generic optimal parameter which can easily generalize for both seen and unseen class generation; and (iii) If we only have access to a very few samples per seen class, these models tend to perform poorly. In this paper, we propose a meta-learning based generative model that naturally handles these limitations. The proposed model is based on integrating model-agnostic meta learning with a Wasserstein GAN (WGAN) to handle (i) and (iii), and uses a novel task distribution to handle (ii). Our proposed model yields significant improvements on standard ZSL as well as more challenging GZSL setting. In ZSL setting, our model yields 4.5%, 6.0%, 9.8%, and 27.9% relative improvements over the current state-of-the-art on CUB, AWA1, AWA2, and aPY datasets, respectively.",
            "referenceCount": 48,
            "citationCount": 68,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/6069/5925",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-04-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Verma2020MetaLearningFG,\n author = {V. Verma and Dhanajit Brahma and Piyush Rai},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {6062-6069},\n title = {Meta-Learning for Generalized Zero-Shot Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:92b87d499f393f6258549e65e32c9331e49568ca",
            "@type": "ScholarlyArticle",
            "paperId": "92b87d499f393f6258549e65e32c9331e49568ca",
            "corpusId": 225041244,
            "url": "https://www.semanticscholar.org/paper/92b87d499f393f6258549e65e32c9331e49568ca",
            "title": "Online Structured Meta-learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2010.11545",
                "DBLP": "conf/nips/YaoZMLSX20",
                "MAG": "3094251525",
                "CorpusId": 225041244
            },
            "abstract": "Learning quickly is of great importance for machine intelligence deployed in online platforms. With the capability of transferring knowledge from learned tasks, meta-learning has shown its effectiveness in online scenarios by continuously updating the model with the learned prior. However, current online meta-learning algorithms are limited to learn a globally-shared meta-learner, which may lead to sub-optimal results when the tasks contain heterogeneous information that are distinct by nature and difficult to share. We overcome this limitation by proposing an online structured meta-learning (OSML) framework. Inspired by the knowledge organization of human and hierarchical feature representation, OSML explicitly disentangles the meta-learner as a meta-hierarchical graph with different knowledge blocks. When a new task is encountered, it constructs a meta-knowledge pathway by either utilizing the most relevant knowledge blocks or exploring new blocks. Through the meta-knowledge pathway, the model is able to quickly adapt to the new task. In addition, new knowledge is further incorporated into the selected blocks. Experiments on three datasets demonstrate the effectiveness and interpretability of our proposed framework in the context of both homogeneous and heterogeneous tasks.",
            "referenceCount": 43,
            "citationCount": 28,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-10-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2010.11545"
            },
            "citationStyles": {
                "bibtex": "@Article{Yao2020OnlineSM,\n author = {Huaxiu Yao and Yingbo Zhou and M. Mahdavi and Z. Li and R. Socher and Caiming Xiong},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Online Structured Meta-learning},\n volume = {abs/2010.11545},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a3553b374f605f261dfb1d58711045010fc06ac9",
            "@type": "ScholarlyArticle",
            "paperId": "a3553b374f605f261dfb1d58711045010fc06ac9",
            "corpusId": 59222821,
            "url": "https://www.semanticscholar.org/paper/a3553b374f605f261dfb1d58711045010fc06ac9",
            "title": "Learning from Multiple Cities: A Meta-Learning Approach for Spatial-Temporal Prediction",
            "venue": "The Web Conference",
            "publicationVenue": {
                "id": "urn:research:e07422f9-c065-40c3-a37b-75e98dce79fe",
                "name": "The Web Conference",
                "alternate_names": [
                    "Web Conf",
                    "WWW"
                ],
                "issn": null,
                "url": "http://www.iw3c2.org/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2951854149",
                "ArXiv": "1901.08518",
                "DBLP": "journals/corr/abs-1901-08518",
                "DOI": "10.1145/3308558.3313577",
                "CorpusId": 59222821
            },
            "abstract": "Spatial-temporal prediction is a fundamental problem for constructing smart city, which is useful for tasks such as traffic control, taxi dispatching, and environment policy making. Due to data collection mechanism, it is common to see data collection with unbalanced spatial distributions. For example, some cities may release taxi data for multiple years while others only release a few days of data; some regions may have constant water quality data monitored by sensors whereas some regions only have a small collection of water samples. In this paper, we tackle the problem of spatial-temporal prediction for the cities with only a short period of data collection. We aim to utilize the long-period data from other cities via transfer learning. Different from previous studies that transfer knowledge from one single source city to a target city, we are the first to leverage information from multiple cities to increase the stability of transfer. Specifically, our proposed model is designed as a spatial-temporal network with a meta-learning paradigm. The meta-learning paradigm learns a well-generalized initialization of the spatial-temporal network, which can be effectively adapted to target cities. In addition, a pattern-based spatial-temporal memory is designed to distill long-term temporal information (i.e., periodicity). We conduct extensive experiments on two tasks: traffic (taxi and bike) prediction and water quality prediction. The experiments demonstrate the effectiveness of our proposed model over several competitive baseline models.",
            "referenceCount": 48,
            "citationCount": 155,
            "influentialCitationCount": 17,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1901.08518",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-01-24",
            "journal": {
                "name": "The World Wide Web Conference",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Yao2019LearningFM,\n author = {Huaxiu Yao and Yiding Liu and Ying Wei and Xianfeng Tang and Z. Li},\n booktitle = {The Web Conference},\n journal = {The World Wide Web Conference},\n title = {Learning from Multiple Cities: A Meta-Learning Approach for Spatial-Temporal Prediction},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:339e2610de8487ccb54af05cb59b63854d25f02d",
            "@type": "ScholarlyArticle",
            "paperId": "339e2610de8487ccb54af05cb59b63854d25f02d",
            "corpusId": 189762031,
            "url": "https://www.semanticscholar.org/paper/339e2610de8487ccb54af05cb59b63854d25f02d",
            "title": "Meta Learning via Learned Loss",
            "venue": "International Conference on Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                "name": "International Conference on Pattern Recognition",
                "alternate_names": [
                    "Pattern Recognit (ICPR Proc Int Conf",
                    "Int Conf Pattern Recognit",
                    "ICPR",
                    "International conference on pattern recognition",
                    "Int conf pattern recognit",
                    "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                ],
                "issn": "1041-3278",
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1906-05374",
                "ArXiv": "1906.05374",
                "MAG": "2952193948",
                "DOI": "10.1109/ICPR48806.2021.9412010",
                "CorpusId": 189762031
            },
            "abstract": "Typically, loss functions, regularization mechanisms and other important aspects of training parametric models are chosen heuristically from a limited set of options. In this paper, we take the first step towards automating this process, with the view of producing models which train faster and more robustly. Concretely, we present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for \u201cmeta-training\u201d such loss functions, targeted at maximizing the performance of the model trained under them. The loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time. We make our code available at https://sites.google.com/view/mlthree",
            "referenceCount": 42,
            "citationCount": 89,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1906.05374",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-06-12",
            "journal": {
                "name": "2020 25th International Conference on Pattern Recognition (ICPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chebotar2019MetaLV,\n author = {Yevgen Chebotar and Artem Molchanov and Sarah Bechtle and L. Righetti and Franziska Meier and G. Sukhatme},\n booktitle = {International Conference on Pattern Recognition},\n journal = {2020 25th International Conference on Pattern Recognition (ICPR)},\n pages = {4161-4168},\n title = {Meta Learning via Learned Loss},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4514be5d1e4719c4701527ca6569899d8be297ef",
            "@type": "ScholarlyArticle",
            "paperId": "4514be5d1e4719c4701527ca6569899d8be297ef",
            "corpusId": 209376818,
            "url": "https://www.semanticscholar.org/paper/4514be5d1e4719c4701527ca6569899d8be297ef",
            "title": "Federated Meta-Learning with Fast Convergence and Efficient Communication",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2994684563",
                "ArXiv": "1802.07876",
                "CorpusId": 209376818
            },
            "abstract": "Statistical and systematic challenges in collaboratively training machine learning models across distributed networks of mobile devices have been the bottlenecks in the real-world application of federated learning. In this work, we show that meta-learning is a natural choice to handle these issues, and propose a federated meta-learning framework FedMeta, where a parameterized algorithm (or meta-learner) is shared, instead of a global model in previous approaches. We conduct an extensive empirical evaluation on LEAF datasets and a real-world production dataset, and demonstrate that FedMeta achieves a reduction in required communication cost by 2.82-4.33 times with faster convergence, and an increase in accuracy by 3.23%-14.84% as compared to Federated Averaging (FedAvg) which is a leading optimization algorithm in federated learning. Moreover, FedMeta preserves user privacy since only the parameterized algorithm is transmitted between mobile devices and central servers, and no raw data is collected onto the servers.",
            "referenceCount": 39,
            "citationCount": 278,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2018-02-22",
            "journal": {
                "name": "arXiv: Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2018FederatedMW,\n author = {Fei Chen and Mi Luo and Zhenhua Dong and Zhenguo Li and Xiuqiang He},\n journal = {arXiv: Learning},\n title = {Federated Meta-Learning with Fast Convergence and Efficient Communication},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a75869d69cc86f501939c237ae4711aa2885f6a6",
            "@type": "ScholarlyArticle",
            "paperId": "a75869d69cc86f501939c237ae4711aa2885f6a6",
            "corpusId": 52100101,
            "url": "https://www.semanticscholar.org/paper/a75869d69cc86f501939c237ae4711aa2885f6a6",
            "title": "Meta-Learning for Low-Resource Neural Machine Translation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1808-08437",
                "ArXiv": "1808.08437",
                "MAG": "2952518244",
                "ACL": "D18-1398",
                "DOI": "10.18653/v1/D18-1398",
                "CorpusId": 52100101
            },
            "abstract": "In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML, Finn, et al., 2017) for low-resource neural machine translation (NMT). We frame low-resource translation as a meta-learning problem where we learn to adapt to low-resource languages based on multilingual high-resource language tasks. We use the universal lexical representation (Gu et al., 2018b) to overcome the input-output mismatch across different languages. We evaluate the proposed meta-learning strategy using eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro,Lv, Fi, Tr and Ko) as target tasks. We show that the proposed approach significantly outperforms the multilingual, transfer learning based approach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples. For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT\u201916 by seeing only 16,000 translated words (~600 parallel sentences)",
            "referenceCount": 44,
            "citationCount": 278,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D18-1398.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-08-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1808.08437"
            },
            "citationStyles": {
                "bibtex": "@Article{Gu2018MetaLearningFL,\n author = {Jiatao Gu and Yong Wang and Yun Chen and Kyunghyun Cho and V. Li},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Meta-Learning for Low-Resource Neural Machine Translation},\n volume = {abs/1808.08437},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b188462188c52f764e74a8ea269274f690efb29e",
            "@type": "ScholarlyArticle",
            "paperId": "b188462188c52f764e74a8ea269274f690efb29e",
            "corpusId": 211506362,
            "url": "https://www.semanticscholar.org/paper/b188462188c52f764e74a8ea269274f690efb29e",
            "title": "A Sample Complexity Separation between Non-Convex and Convex Meta-Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2002.11172",
                "DBLP": "journals/corr/abs-2002-11172",
                "MAG": "3007891689",
                "CorpusId": 211506362
            },
            "abstract": "One popular trend in meta-learning is to learn from many training tasks a common initialization for a gradient-based method that can be used to solve a new task with few samples. The theory of meta-learning is still in its early stages, with several recent learning-theoretic analyses of methods such as Reptile [Nichol et al., 2018] being for convex models. This work shows that convex-case analysis might be insufficient to understand the success of meta-learning, and that even for non-convex models it is important to look inside the optimization black-box, specifically at properties of the optimization trajectory. We construct a simple meta-learning instance that captures the problem of one-dimensional subspace learning. For the convex formulation of linear regression on this instance, we show that the new task sample complexity of any initialization-based meta-learning algorithm is $\\Omega(d)$, where $d$ is the input dimension. In contrast, for the non-convex formulation of a two layer linear network on the same instance, we show that both Reptile and multi-task representation learning can have new task sample complexity of $\\mathcal{O}(1)$, demonstrating a separation from convex meta-learning. Crucially, analyses of the training dynamics of these methods reveal that they can meta-learn the correct subspace onto which the data should be projected.",
            "referenceCount": 34,
            "citationCount": 25,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-02-25",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Saunshi2020ASC,\n author = {Nikunj Saunshi and Yi Zhang and M. Khodak and Sanjeev Arora},\n booktitle = {International Conference on Machine Learning},\n pages = {8512-8521},\n title = {A Sample Complexity Separation between Non-Convex and Convex Meta-Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d0eb13325d77e50a60102139e84484a9beaf62ff",
            "@type": "ScholarlyArticle",
            "paperId": "d0eb13325d77e50a60102139e84484a9beaf62ff",
            "corpusId": 216080675,
            "url": "https://www.semanticscholar.org/paper/d0eb13325d77e50a60102139e84484a9beaf62ff",
            "title": "A Comprehensive Overview and Survey of Recent Advances in Meta-Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3020638616",
                "DBLP": "journals/corr/abs-2004-11149",
                "ArXiv": "2004.11149",
                "CorpusId": 216080675
            },
            "abstract": "This article reviews meta-learning also known as learning-to-learn which seeks rapid and accurate model adaptation to unseen tasks with applications in strong AI, few-shot learning, natural language processing and robotics. Unlike deep learning, meta-learning can be applied to few-shot high-dimensional datasets and considers further improving model generalization to unseen tasks. Deep learning is focused upon in-sample prediction and meta-learning concerns model adaptation for out-of-sample prediction. Meta-learning can continually perform self-improvement to achieve highly autonomous AI. Meta-learning may serve as an additional generalization block complementary for original deep learning model. Meta-learning seeks adaptation of machine learning models to unseen tasks which are vastly different from trained tasks. Meta-learning with coevolution between agent and environment provides solutions for complex tasks unsolvable by training from scratch. Meta-learning methodology covers a wide range of great minds and thoughts. We briefly summarize meta-learning methodologies into the following categories: black-box meta-learning, metric-based meta-learning, layered meta-learning and Bayesian meta-learning framework. Recent applications concentrate upon the integration of meta-learning with other machine learning framework to provide feasible integrated problem solutions. We briefly present recent meta-learning advances and discuss potential future research directions.",
            "referenceCount": 142,
            "citationCount": 25,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-04-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2004.11149"
            },
            "citationStyles": {
                "bibtex": "@Article{Peng2020ACO,\n author = {Huimin Peng},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Comprehensive Overview and Survey of Recent Advances in Meta-Learning},\n volume = {abs/2004.11149},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:837ca5b8e57262398d3540649bd9545e6d6291d9",
            "@type": "ScholarlyArticle",
            "paperId": "837ca5b8e57262398d3540649bd9545e6d6291d9",
            "corpusId": 203642015,
            "url": "https://www.semanticscholar.org/paper/837ca5b8e57262398d3540649bd9545e6d6291d9",
            "title": "ES-MAML: Simple Hessian-Free Meta Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/iclr/SongGYCPT20",
                "MAG": "2978843034",
                "ArXiv": "1910.01215",
                "CorpusId": 203642015
            },
            "abstract": "We introduce ES-MAML, a new framework for solving the model agnostic meta learning (MAML) problem based on Evolution Strategies (ES). Existing algorithms for MAML are based on policy gradients, and incur significant difficulties when attempting to estimate second derivatives using backpropagation on stochastic policies. We show how ES can be applied to MAML to obtain an algorithm which avoids the problem of estimating second derivatives, and is also conceptually simple and easy to implement. Moreover, ES-MAML can handle new types of nonsmooth adaptation operators, and other techniques for improving performance and estimation of ES methods become applicable. We show empirically that ES-MAML is competitive with existing methods and often yields better adaptation with fewer queries.",
            "referenceCount": 35,
            "citationCount": 103,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1910.01215"
            },
            "citationStyles": {
                "bibtex": "@Article{Song2019ESMAMLSH,\n author = {Xingyou Song and Wenbo Gao and Yuxiang Yang and K. Choromanski and Aldo Pacchiano and Yunhao Tang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {ES-MAML: Simple Hessian-Free Meta Learning},\n volume = {abs/1910.01215},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:104d77f2281a705deee73e1b5e4d064ab28d4561",
            "@type": "ScholarlyArticle",
            "paperId": "104d77f2281a705deee73e1b5e4d064ab28d4561",
            "corpusId": 182952555,
            "url": "https://www.semanticscholar.org/paper/104d77f2281a705deee73e1b5e4d064ab28d4561",
            "title": "Domain Adaptive Dialog Generation via Meta Learning",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2951980657",
                "DBLP": "conf/acl/QianY19",
                "ArXiv": "1906.03520",
                "ACL": "P19-1253",
                "DOI": "10.18653/v1/P19-1253",
                "CorpusId": 182952555
            },
            "abstract": "Domain adaptation is an essential task in dialog system building because there are so many new dialog tasks created for different needs every day. Collecting and annotating training data for these new tasks is costly since it involves real user interactions. We propose a domain adaptive dialog generation method based on meta-learning (DAML). DAML is an end-to-end trainable dialog system model that learns from multiple rich-resource tasks and then adapts to new domains with minimal training samples. We train a dialog system model using multiple rich-resource single-domain dialog data by applying the model-agnostic meta-learning algorithm to dialog domain. The model is capable of learning a competitive dialog system on a new domain with only a few training examples in an efficient manner. The two-step gradient updates in DAML enable the model to learn general features across multiple tasks. We evaluate our method on a simulated dialog dataset and achieve state-of-the-art performance, which is generalizable to new tasks.",
            "referenceCount": 44,
            "citationCount": 114,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/P19-1253.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-06-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Qian2019DomainAD,\n author = {Kun Qian and Zhou Yu},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {2639-2649},\n title = {Domain Adaptive Dialog Generation via Meta Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:688f9db8876fbca45d85e3c313294ab217d7f916",
            "@type": "ScholarlyArticle",
            "paperId": "688f9db8876fbca45d85e3c313294ab217d7f916",
            "corpusId": 225066818,
            "url": "https://www.semanticscholar.org/paper/688f9db8876fbca45d85e3c313294ab217d7f916",
            "title": "Modeling and Optimization Trade-off in Meta-learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3099235981",
                "ArXiv": "2010.12916",
                "DBLP": "conf/nips/GaoS20",
                "CorpusId": 225066818
            },
            "abstract": "By searching for shared inductive biases across tasks, meta-learning promises to accelerate learning on novel tasks, but with the cost of solving a complex bilevel optimization problem. We introduce and rigorously define the trade-off between accurate modeling and optimization ease in meta-learning. At one end, classic meta-learning algorithms account for the structure of meta-learning but solve a complex optimization problem, while at the other end domain randomized search (otherwise known as joint training) ignores the structure of meta-learning and solves a single level optimization problem. Taking MAML as the representative meta-learning algorithm, we theoretically characterize the trade-off for general non-convex risk functions as well as linear regression, for which we are able to provide explicit bounds on the errors associated with modeling and optimization. We also empirically study this trade-off for meta-reinforcement learning benchmarks.",
            "referenceCount": 31,
            "citationCount": 23,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-10-24",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2010.12916"
            },
            "citationStyles": {
                "bibtex": "@Article{Gao2020ModelingAO,\n author = {Katelyn Gao and Ozan Sener},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Modeling and Optimization Trade-off in Meta-learning},\n volume = {abs/2010.12916},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:64bc52c8349cd62cc561f11c78cbb9b914064819",
            "@type": "ScholarlyArticle",
            "paperId": "64bc52c8349cd62cc561f11c78cbb9b914064819",
            "corpusId": 219792972,
            "url": "https://www.semanticscholar.org/paper/64bc52c8349cd62cc561f11c78cbb9b914064819",
            "title": "Unsupervised Meta-Learning through Latent-Space Interpolation in Generative Models",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2006-10236",
                "MAG": "3035838965",
                "ArXiv": "2006.10236",
                "CorpusId": 219792972
            },
            "abstract": "Unsupervised meta-learning approaches rely on synthetic meta-tasks that are created using techniques such as random selection, clustering and/or augmentation. Unfortunately, clustering and augmentation are domain-dependent, and thus they require either manual tweaking or expensive learning. In this work, we describe an approach that generates meta-tasks using generative models. A critical component is a novel approach of sampling from the latent space that generates objects grouped into synthetic classes forming the training and validation data of a meta-task. We find that the proposed approach, LAtent Space Interpolation Unsupervised Meta-learning (LASIUM), outperforms or is competitive with current unsupervised learning baselines on few-shot classification tasks on the most widely used benchmark datasets. In addition, the approach promises to be applicable without manual tweaking over a wider range of domains than previous approaches.",
            "referenceCount": 34,
            "citationCount": 22,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-06-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2006.10236"
            },
            "citationStyles": {
                "bibtex": "@Article{Khodadadeh2020UnsupervisedMT,\n author = {Siavash Khodadadeh and Sharare Zehtabian and Saeed Vahidian and Weijia Wang and Bill Lin and L. Boloni},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Unsupervised Meta-Learning through Latent-Space Interpolation in Generative Models},\n volume = {abs/2006.10236},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1f2577071ca2aa1086f4b1c12cd911061aeea960",
            "@type": "ScholarlyArticle",
            "paperId": "1f2577071ca2aa1086f4b1c12cd911061aeea960",
            "corpusId": 208268598,
            "url": "https://www.semanticscholar.org/paper/1f2577071ca2aa1086f4b1c12cd911061aeea960",
            "title": "Meta-Learning of Neural Architectures for Few-Shot Learning",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2989756097",
                "DBLP": "journals/corr/abs-1911-11090",
                "ArXiv": "1911.11090",
                "DOI": "10.1109/cvpr42600.2020.01238",
                "CorpusId": 208268598
            },
            "abstract": "The recent progress in neural architecture search (NAS) has allowed scaling the automated design of neural architectures to real-world domains, such as object detection and semantic segmentation. However, one prerequisite for the application of NAS are large amounts of labeled data and compute resources. This renders its application challenging in few-shot learning scenarios, where many related tasks need to be learned, each with limited amounts of data and compute time. Thus, few-shot learning is typically done with a fixed neural architecture. To improve upon this, we propose MetaNAS, the first method which fully integrates NAS with gradient-based meta-learning. MetaNAS optimizes a meta-architecture along with the meta-weights during meta-training. During meta-testing, architectures can be adapted to a novel task with a few steps of the task optimizer, that is: task adaptation becomes computationally cheap and requires only little data per task. Moreover, MetaNAS is agnostic in that it can be used with arbitrary model-agnostic meta-learning algorithms and arbitrary gradient-based NAS methods. Empirical results on standard few-shot classification benchmarks show that MetaNAS with a combination of DARTS and REPTILE yields state-of-the-art results.",
            "referenceCount": 60,
            "citationCount": 97,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1911.11090",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-11-25",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Elsken2019MetaLearningON,\n author = {T. Elsken and B. Staffler and J. H. Metzen and F. Hutter},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {12362-12372},\n title = {Meta-Learning of Neural Architectures for Few-Shot Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:96f8e5820c5b59e65c2331d577aa738676e8c605",
            "@type": "ScholarlyArticle",
            "paperId": "96f8e5820c5b59e65c2331d577aa738676e8c605",
            "corpusId": 170078603,
            "url": "https://www.semanticscholar.org/paper/96f8e5820c5b59e65c2331d577aa738676e8c605",
            "title": "Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1905-12917",
                "ArXiv": "1905.12917",
                "MAG": "2947247301",
                "CorpusId": 170078603
            },
            "abstract": "While tasks could come with varying the number of instances and classes in realistic settings, the existing meta-learning approaches for few-shot classification assume that the number of instances per task and class is fixed. Due to such restriction, they learn to equally utilize the meta-knowledge across all the tasks, even when the number of instances per task and class largely varies. Moreover, they do not consider distributional difference in unseen tasks, on which the meta-knowledge may have less usefulness depending on the task relatedness. To overcome these limitations, we propose a novel meta-learning model that adaptively balances the effect of the meta-learning and task-specific learning within each task. Through the learning of the balancing variables, we can decide whether to obtain a solution by relying on the meta-knowledge or task-specific learning. We formulate this objective into a Bayesian inference framework and tackle it using variational inference. We validate our Bayesian Task-Adaptive Meta-Learning (Bayesian TAML) on multiple realistic task- and class-imbalanced datasets, on which it significantly outperforms existing meta-learning approaches. Further ablation study confirms the effectiveness of each balancing component and the Bayesian learning framework.",
            "referenceCount": 38,
            "citationCount": 87,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-05-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1905.12917"
            },
            "citationStyles": {
                "bibtex": "@Article{Lee2019LearningTB,\n author = {Haebeom Lee and Hayeon Lee and Donghyun Na and Saehoon Kim and Minseop Park and Eunho Yang and S. Hwang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks},\n volume = {abs/1905.12917},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:83a7bf49ea7961e097b21ff03709b57698c037bc",
            "@type": "ScholarlyArticle",
            "paperId": "83a7bf49ea7961e097b21ff03709b57698c037bc",
            "corpusId": 204904570,
            "url": "https://www.semanticscholar.org/paper/83a7bf49ea7961e097b21ff03709b57698c037bc",
            "title": "Meta Learning for End-To-End Low-Resource Speech Recognition",
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "publicationVenue": {
                "id": "urn:research:0d6f7fba-7092-46b3-8039-93458dba736b",
                "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                "alternate_names": [
                    "Int Conf Acoust Speech Signal Process",
                    "IEEE Int Conf Acoust Speech Signal Process",
                    "ICASSP",
                    "International Conference on Acoustics, Speech, and Signal Processing"
                ],
                "issn": null,
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "3015585292",
                "DBLP": "conf/icassp/HsuCL20",
                "ArXiv": "1910.12094",
                "DOI": "10.1109/ICASSP40776.2020.9053112",
                "CorpusId": 204904570
            },
            "abstract": "In this paper, we proposed to apply meta learning approach for low-resource automatic speech recognition (ASR). We formulated ASR for different languages as different tasks, and meta-learned the initialization parameters from many pretraining languages to achieve fast adaptation on unseen target language, via recently proposed model-agnostic meta learning algorithm (MAML). We evaluated the proposed approach using six languages as pretraining tasks and four languages as target tasks. Preliminary results showed that the proposed method, MetaASR, significantly outperforms the state-of-the-art multitask pretraining approach on all target languages with different combinations of pretraining languages. In addition, since MAML\u2019s model-agnostic property, this paper also opens new research direction of applying meta learning to more speech-related applications.",
            "referenceCount": 24,
            "citationCount": 78,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1910.12094",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-10-26",
            "journal": {
                "name": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hsu2019MetaLF,\n author = {Jui-Yang Hsu and Yuan-Jui Chen and Hung-yi Lee},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {7844-7848},\n title = {Meta Learning for End-To-End Low-Resource Speech Recognition},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:41c09567e5b3c86cb52bdc5471a306b71730ef49",
            "@type": "ScholarlyArticle",
            "paperId": "41c09567e5b3c86cb52bdc5471a306b71730ef49",
            "corpusId": 202787363,
            "url": "https://www.semanticscholar.org/paper/41c09567e5b3c86cb52bdc5471a306b71730ef49",
            "title": "Efficient Meta Learning via Minibatch Proximal Update",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2970389519",
                "DBLP": "conf/nips/ZhouYXYF19",
                "CorpusId": 202787363
            },
            "abstract": "We address the problem of meta-learning which learns a prior over hypothesis from a sample of meta-training tasks for fast adaptation on meta-testing tasks. A particularly simple yet successful paradigm for this research is model-agnostic meta-learning (MAML). Implementation and analysis of MAML, however, can be tricky; first-order approximation is usually adopted to avoid directly computing Hessian matrix but as a result the convergence and generalization guarantees remain largely mysterious for MAML. To remedy this deficiency, in this paper we propose a minibatch proximal update based meta-learning approach for learning to efficient hypothesis transfer. The principle is to learn a prior hypothesis shared across tasks such that the minibatch risk minimization biased regularized by this prior can quickly converge to the optimal hypothesis in each training task. The prior hypothesis training model can be efficiently optimized via SGD with provable convergence guarantees for both convex and non-convex problems. Moreover, we theoretically justify the benefit of the learnt prior hypothesis for fast adaptation to new few-shot learning tasks via minibatch proximal update. Experimental results on several few-shot regression and classification tasks demonstrate the advantages of our method over state-of-the-arts.",
            "referenceCount": 46,
            "citationCount": 80,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhou2019EfficientML,\n author = {Pan Zhou and Xiaotong Yuan and Huan Xu and Shuicheng Yan and Jiashi Feng},\n booktitle = {Neural Information Processing Systems},\n pages = {1532-1542},\n title = {Efficient Meta Learning via Minibatch Proximal Update},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7ff64aa7e35d6de043e26cd92fd3b967e0ca4fdb",
            "@type": "ScholarlyArticle",
            "paperId": "7ff64aa7e35d6de043e26cd92fd3b967e0ca4fdb",
            "corpusId": 220381014,
            "url": "https://www.semanticscholar.org/paper/7ff64aa7e35d6de043e26cd92fd3b967e0ca4fdb",
            "title": "Meta-Learning with Network Pruning",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/eccv/TianLYL20",
                "ArXiv": "2007.03219",
                "MAG": "3039413608",
                "DOI": "10.1007/978-3-030-58529-7_40",
                "CorpusId": 220381014
            },
            "abstract": null,
            "referenceCount": 41,
            "citationCount": 22,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2007.03219",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-07-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2007.03219"
            },
            "citationStyles": {
                "bibtex": "@Article{Tian2020MetaLearningWN,\n author = {Hongduan Tian and Bo Liu and Xiaotong Yuan and Qingshan Liu},\n booktitle = {European Conference on Computer Vision},\n journal = {ArXiv},\n title = {Meta-Learning with Network Pruning},\n volume = {abs/2007.03219},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4d21334ea3564c89586e1ba176e11382bdd3d394",
            "@type": "ScholarlyArticle",
            "paperId": "4d21334ea3564c89586e1ba176e11382bdd3d394",
            "corpusId": 54435127,
            "url": "https://www.semanticscholar.org/paper/4d21334ea3564c89586e1ba176e11382bdd3d394",
            "title": "Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2962732398",
                "DBLP": "journals/corr/abs-1812-00971",
                "ArXiv": "1812.00971",
                "DOI": "10.1109/CVPR.2019.00691",
                "CorpusId": 54435127
            },
            "abstract": "Learning is an inherently continuous phenomenon. When humans learn a new task there is no explicit distinction between training and inference. As we learn a task, we keep learning about it while performing the task. What we learn and how we learn it varies during different stages of learning. Learning how to learn and adapt is a key property that enables us to generalize effortlessly to new settings. This is in contrast with conventional settings in machine learning where a trained model is frozen during inference. In this paper we study the problem of learning to learn at both training and test time in the context of visual navigation. A fundamental challenge in navigation is generalization to unseen scenes. In this paper we propose a self-adaptive visual navigation method (SAVN) which learns to adapt to new environments without any explicit supervision. Our solution is a meta-reinforcement learning approach where an agent learns a self-supervised interaction loss that encourages effective navigation. Our experiments, performed in the AI2-THOR framework, show major improvements in both success rate and SPL for visual navigation in novel scenes. Our code and data are available at: https://github.com/allenai/savn.",
            "referenceCount": 50,
            "citationCount": 168,
            "influentialCitationCount": 36,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1812.00971",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-12-03",
            "journal": {
                "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wortsman2018LearningTL,\n author = {Mitchell Wortsman and Kiana Ehsani and Mohammad Rastegari and Ali Farhadi and Roozbeh Mottaghi},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {6743-6752},\n title = {Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e35f243b2d2c60d4c3b03379d44901dc597e175d",
            "@type": "ScholarlyArticle",
            "paperId": "e35f243b2d2c60d4c3b03379d44901dc597e175d",
            "corpusId": 233139020,
            "url": "https://www.semanticscholar.org/paper/e35f243b2d2c60d4c3b03379d44901dc597e175d",
            "title": "Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis",
            "venue": "npj Digital Medicine",
            "publicationVenue": {
                "id": "urn:research:ef485645-f75f-4344-8b9d-3c260e69503b",
                "name": "npj Digital Medicine",
                "alternate_names": [
                    "npj Digit Med"
                ],
                "issn": "2398-6352",
                "url": "http://www.nature.com/npjdigitalmed/"
            },
            "year": 2021,
            "externalIds": {
                "PubMedCentral": "8027892",
                "DBLP": "journals/npjdm/AggarwalSMTKKAD21",
                "DOI": "10.1038/s41746-021-00438-z",
                "CorpusId": 233139020,
                "PubMed": "33828217"
            },
            "abstract": null,
            "referenceCount": 141,
            "citationCount": 262,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41746-021-00438-z.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2021-04-07",
            "journal": {
                "name": "NPJ Digital Medicine",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Aggarwal2021DiagnosticAO,\n author = {R. Aggarwal and V. Sounderajah and G. Martin and D. Ting and A. Karthikesalingam and Dominic King and H. Ashrafian and A. Darzi},\n booktitle = {npj Digital Medicine},\n journal = {NPJ Digital Medicine},\n title = {Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis},\n volume = {4},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:42de54e614110c0c0a0bbbfee045e11e53eb4a7d",
            "@type": "ScholarlyArticle",
            "paperId": "42de54e614110c0c0a0bbbfee045e11e53eb4a7d",
            "corpusId": 56475888,
            "url": "https://www.semanticscholar.org/paper/42de54e614110c0c0a0bbbfee045e11e53eb4a7d",
            "title": "Deep Online Learning via Meta-Learning: Continual Adaptation for Model-Based RL",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2964093801",
                "ArXiv": "1812.07671",
                "DBLP": "journals/corr/abs-1812-07671",
                "CorpusId": 56475888
            },
            "abstract": "Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. In this work, we apply our meta-learning for online learning (MOLe) approach to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that MOLe outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.",
            "referenceCount": 46,
            "citationCount": 175,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-12-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1812.07671"
            },
            "citationStyles": {
                "bibtex": "@Article{Nagabandi2018DeepOL,\n author = {Anusha Nagabandi and Chelsea Finn and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Deep Online Learning via Meta-Learning: Continual Adaptation for Model-Based RL},\n volume = {abs/1812.07671},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:faf08cc7a7897f7f183673306abe6fd5b92091f4",
            "@type": "ScholarlyArticle",
            "paperId": "faf08cc7a7897f7f183673306abe6fd5b92091f4",
            "corpusId": 186207051,
            "url": "https://www.semanticscholar.org/paper/faf08cc7a7897f7f183673306abe6fd5b92091f4",
            "title": "Task Agnostic Continual Learning via Meta Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2950184636",
                "DBLP": "journals/corr/abs-1906-05201",
                "ArXiv": "1906.05201",
                "CorpusId": 186207051
            },
            "abstract": "While neural networks are powerful function approximators, they suffer from catastrophic forgetting when the data distribution is not stationary. One particular formalism that studies learning under non-stationary distribution is provided by continual learning, where the non-stationarity is imposed by a sequence of distinct tasks. Most methods in this space assume, however, the knowledge of task boundaries, and focus on alleviating catastrophic forgetting. In this work, we depart from this view and move the focus towards faster remembering -- i.e measuring how quickly the network recovers performance rather than measuring the network's performance without any adaptation. We argue that in many settings this can be more effective and that it opens the door to combining meta-learning and continual learning techniques, leveraging their complementary advantages. We propose a framework specific for the scenario where no information about task boundaries or task identity is given. It relies on a separation of concerns into what task is being solved and how the task should be solved. This framework is implemented by differentiating task specific parameters from task agnostic parameters, where the latter are optimized in a continual meta learning fashion, without access to multiple tasks at the same time. We showcase this framework in a supervised learning scenario and discuss the implication of the proposed formalism.",
            "referenceCount": 38,
            "citationCount": 82,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-06-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1906.05201"
            },
            "citationStyles": {
                "bibtex": "@Article{He2019TaskAC,\n author = {Xu He and Jakub Sygnowski and Alexandre Galashov and Andrei A. Rusu and Y. Teh and Razvan Pascanu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Task Agnostic Continual Learning via Meta Learning},\n volume = {abs/1906.05201},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9cd5cadbc59806f0102dadd091ba49fb3b642bc3",
            "@type": "ScholarlyArticle",
            "paperId": "9cd5cadbc59806f0102dadd091ba49fb3b642bc3",
            "corpusId": 227209573,
            "url": "https://www.semanticscholar.org/paper/9cd5cadbc59806f0102dadd091ba49fb3b642bc3",
            "title": "Meta-learning in natural and artificial intelligence",
            "venue": "Current Opinion in Behavioral Sciences",
            "publicationVenue": {
                "id": "urn:research:68b547ba-0f44-4ad4-a095-b871d674a94f",
                "name": "Current Opinion in Behavioral Sciences",
                "alternate_names": [
                    "Curr Opin Behav Sci",
                    "Curr opin behav sci",
                    "Current opinion in behavioral sciences"
                ],
                "issn": "2352-1546",
                "url": "http://www.elsevier.com/locate/issn/23521546"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3109949687",
                "ArXiv": "2011.13464",
                "DBLP": "journals/corr/abs-2011-13464",
                "DOI": "10.1016/j.cobeha.2021.01.002",
                "CorpusId": 227209573
            },
            "abstract": null,
            "referenceCount": 80,
            "citationCount": 63,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2011.13464",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-11-26",
            "journal": {
                "name": "Current Opinion in Behavioral Sciences",
                "volume": "38"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2020MetalearningIN,\n author = {Jane X. Wang},\n booktitle = {Current Opinion in Behavioral Sciences},\n journal = {Current Opinion in Behavioral Sciences},\n pages = {90-95},\n title = {Meta-learning in natural and artificial intelligence},\n volume = {38},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:202e24f1c7d0e022a3c756b5102cddcc5c49a2f6",
            "@type": "ScholarlyArticle",
            "paperId": "202e24f1c7d0e022a3c756b5102cddcc5c49a2f6",
            "corpusId": 153311737,
            "url": "https://www.semanticscholar.org/paper/202e24f1c7d0e022a3c756b5102cddcc5c49a2f6",
            "title": "Meta-Learning for Low-resource Natural Language Generation in Task-oriented Dialogue Systems",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1905-05644",
                "ArXiv": "1905.05644",
                "MAG": "2945367412",
                "DOI": "10.24963/ijcai.2019/437",
                "CorpusId": 153311737
            },
            "abstract": "Natural language generation (NLG) is an essential component of task-oriented dialogue systems. Despite the recent success of neural approaches for NLG, they are typically developed for particular domains with rich annotated training examples. In this paper, we study NLG in a low-resource setting to generate sentences in new scenarios with handful training examples. We formulate the problem from a meta-learning perspective, and propose a generalized optimization-based approach (Meta-NLG) based on the well-recognized model-agnostic meta-learning (MAML) algorithm. Meta-NLG defines a set of meta tasks, and directly incorporates the objective of adapting to new low-resource NLG tasks into the meta-learning optimization process. Extensive experiments are conducted on a large multi-domain dataset (MultiWoz) with diverse linguistic variations. We show that Meta-NLG\u00a0significantly outperforms other training procedures in various low-resource configurations. We analyze the results, and demonstrate that Meta-NLG\u00a0adapts extremely fast and well to low-resource situations.",
            "referenceCount": 40,
            "citationCount": 93,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ijcai.org/proceedings/2019/0437.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-05-14",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Mi2019MetaLearningFL,\n author = {Fei Mi and Minlie Huang and Jiyong Zhang and B. Faltings},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {3151-3157},\n title = {Meta-Learning for Low-resource Natural Language Generation in Task-oriented Dialogue Systems},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:eb4fa3f726cf583a5c8ca574ca94543117e7dc97",
            "@type": "ScholarlyArticle",
            "paperId": "eb4fa3f726cf583a5c8ca574ca94543117e7dc97",
            "corpusId": 202566017,
            "url": "https://www.semanticscholar.org/paper/eb4fa3f726cf583a5c8ca574ca94543117e7dc97",
            "title": "Differentially Private Meta-Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2996366116",
                "ArXiv": "1909.05830",
                "DBLP": "journals/corr/abs-1909-05830",
                "CorpusId": 202566017
            },
            "abstract": "Parameter-transfer is a well-known and versatile approach for meta-learning, with applications including few-shot learning, federated learning, and reinforcement learning. However, parameter-transfer algorithms often require sharing models that have been trained on the samples from specific tasks, thus leaving the task-owners susceptible to breaches of privacy. We conduct the first formal study of privacy in this setting and formalize the notion of task-global differential privacy as a practical relaxation of more commonly studied threat models. We then propose a new differentially private algorithm for gradient-based parameter transfer that not only satisfies this privacy requirement but also retains provable transfer learning guarantees in convex settings. Empirically, we apply our analysis to the problems of federated learning with personalization and few-shot classification, showing that allowing the relaxation to task-global privacy from the more commonly studied notion of local privacy leads to dramatically increased performance in recurrent neural language modeling and image classification.",
            "referenceCount": 36,
            "citationCount": 84,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1909.05830"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2019DifferentiallyPM,\n author = {Jeffrey Li and M. Khodak and S. Caldas and Ameet Talwalkar},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Differentially Private Meta-Learning},\n volume = {abs/1909.05830},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:592d70aff6a9c5db116719b635055789fd844363",
            "@type": "ScholarlyArticle",
            "paperId": "592d70aff6a9c5db116719b635055789fd844363",
            "corpusId": 202558596,
            "url": "https://www.semanticscholar.org/paper/592d70aff6a9c5db116719b635055789fd844363",
            "title": "Learning to Propagate for Graph Meta-Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2972699843",
                "DBLP": "conf/nips/LiuZLJZ19",
                "ArXiv": "1909.05024",
                "CorpusId": 202558596
            },
            "abstract": "Meta-learning extracts the common knowledge from learning different tasks and uses it for unseen tasks. It can signi\ufb01cantly improve tasks that suffer from insuf\ufb01cient training data, e.g., few-shot learning. In most meta-learning methods, tasks are implicitly related by sharing parameters or optimizer. In this paper, we show that a meta-learner that explicitly relates tasks on a graph describing the relations of their output dimensions (e.g., classes) can signi\ufb01cantly improve few-shot learning. The graph\u2019s structure is usually free or cheap to obtain but has rarely been explored in previous works. We develop a novel meta-learner of this type for prototype based classi\ufb01cation, in which a prototype is generated for each class, such that the nearest neighbor search among the prototypes produces an accurate classi\ufb01cation. The meta-learner, called \u201cGated Propagation Network (GPN)\u201d, learns to propagate messages between prototypes of different classes on the graph, so that learning the prototype of each class bene\ufb01ts from the data of other related classes. In GPN, an attention mechanism aggregates messages from neighboring classes of each class, with a gate choosing between the aggregated message and the message from the class itself. We train GPN on a sequence of tasks from many-shot to few-shot generated by subgraph sampling. During training, it is able to reuse and update previously achieved prototypes from the memory in a life-long learning cycle. In experiments, under different training-test discrepancy and test task generation settings, GPN outperforms recent meta-learning methods on two benchmark datasets. Code of GPN is publicly available at: https://github.com/liulu112601/Gated-Propagation-Net.",
            "referenceCount": 30,
            "citationCount": 87,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-11",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1909.05024"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2019LearningTP,\n author = {Lu Liu and Tianyi Zhou and Guodong Long and Jing Jiang and Chengqi Zhang},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Learning to Propagate for Graph Meta-Learning},\n volume = {abs/1909.05024},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:defc905476eec7c07bbf3447612d797878121687",
            "@type": "ScholarlyArticle",
            "paperId": "defc905476eec7c07bbf3447612d797878121687",
            "corpusId": 260440348,
            "url": "https://www.semanticscholar.org/paper/defc905476eec7c07bbf3447612d797878121687",
            "title": "Meta-Learning",
            "venue": "Automated Machine Learning and Meta-Learning for Multimedia",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "DOI": "10.1007/978-3-030-88132-0_2",
                "CorpusId": 260440348
            },
            "abstract": null,
            "referenceCount": 3,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Automated Machine Learning and Meta-Learning for Multimedia",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhu2021MetaLearning,\n author = {Wenwu Zhu and Xin Wang},\n booktitle = {Automated Machine Learning and Meta-Learning for Multimedia},\n journal = {Automated Machine Learning and Meta-Learning for Multimedia},\n title = {Meta-Learning},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cc7827a17a7759a04aa389290d1a874db56e85e5",
            "@type": "ScholarlyArticle",
            "paperId": "cc7827a17a7759a04aa389290d1a874db56e85e5",
            "corpusId": 260441060,
            "url": "https://www.semanticscholar.org/paper/cc7827a17a7759a04aa389290d1a874db56e85e5",
            "title": "Meta-Learning: A Survey",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1810.03548",
                "DBLP": "journals/corr/abs-1810-03548",
                "MAG": "2895531857",
                "CorpusId": 260441060
            },
            "abstract": "Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.",
            "referenceCount": 189,
            "citationCount": 145,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-10-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1810.03548"
            },
            "citationStyles": {
                "bibtex": "@Article{Vanschoren2018MetaLearningAS,\n author = {J. Vanschoren},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Meta-Learning: A Survey},\n volume = {abs/1810.03548},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:aa55677fa86bb05e3cb8bf403a0317f4bb1d36c1",
            "@type": "ScholarlyArticle",
            "paperId": "aa55677fa86bb05e3cb8bf403a0317f4bb1d36c1",
            "corpusId": 147703875,
            "url": "https://www.semanticscholar.org/paper/aa55677fa86bb05e3cb8bf403a0317f4bb1d36c1",
            "title": "Meta-learning of Sequential Strategies",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1905.03030",
                "DBLP": "journals/corr/abs-1905-03030",
                "MAG": "2944299231",
                "CorpusId": 147703875
            },
            "abstract": "In this report we review memory-based meta-learning as a tool for building sample-efficient strategies that learn from past experience to adapt to any task within a target class. Our goal is to equip the reader with the conceptual foundations of this tool for building new, scalable agents that operate on broad domains. To do so, we present basic algorithmic templates for building near-optimal predictors and reinforcement learners which behave as if they had a probabilistic model that allowed them to efficiently exploit task structure. Furthermore, we recast memory-based meta-learning within a Bayesian framework, showing that the meta-learned strategies are near-optimal because they amortize Bayes-filtered data, where the adaptation is implemented in the memory dynamics as a state-machine of sufficient statistics. Essentially, memory-based meta-learning translates the hard problem of probabilistic sequential inference into a regression problem.",
            "referenceCount": 100,
            "citationCount": 70,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-05-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1905.03030"
            },
            "citationStyles": {
                "bibtex": "@Article{Ortega2019MetalearningOS,\n author = {Pedro A. Ortega and Jane X. Wang and Mark Rowland and Tim Genewein and Z. Kurth-Nelson and Razvan Pascanu and N. Heess and J. Veness and A. Pritzel and P. Sprechmann and Siddhant M. Jayakumar and Tom McGrath and Kevin J. Miller and M. G. Azar and Ian Osband and Neil C. Rabinowitz and A. Gy\u00f6rgy and S. Chiappa and Simon Osindero and Y. Teh and H. V. Hasselt and Nando de Freitas and M. Botvinick and S. Legg},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Meta-learning of Sequential Strategies},\n volume = {abs/1905.03030},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:55729215c9e7b65dfd23a4af919d76796f716ce5",
            "@type": "ScholarlyArticle",
            "paperId": "55729215c9e7b65dfd23a4af919d76796f716ce5",
            "corpusId": 202769820,
            "url": "https://www.semanticscholar.org/paper/55729215c9e7b65dfd23a4af919d76796f716ce5",
            "title": "Unsupervised Meta-Learning for Few-Shot Image Classification",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2986775770",
                "ArXiv": "1811.11819",
                "DBLP": "conf/nips/KhodadadehBS19",
                "CorpusId": 202769820
            },
            "abstract": "Few-shot or one-shot learning of classifiers requires a significant inductive bias towards the type of task to be learned. One way to acquire this is by meta-learning on tasks similar to the target task. In this paper, we propose UMTRA, an algorithm that performs unsupervised, model-agnostic meta-learning for classification tasks. The meta-learning step of UMTRA is performed on a flat collection of unlabeled images. While we assume that these images can be grouped into a diverse set of classes and are relevant to the target task, no explicit information about the classes or any labels are needed. UMTRA uses random sampling and augmentation to create synthetic training tasks for meta-learning phase. Labels are only needed at the final target task learning step, and they can be as little as one sample per class. On the Omniglot and Mini-Imagenet few-shot learning benchmarks, UMTRA outperforms every tested approach based on unsupervised learning of representations, while alternating for the best performance with the recent CACTUs algorithm. Compared to supervised model-agnostic meta-learning approaches, UMTRA trades off some classification accuracy for a reduction in the required labels of several orders of magnitude.",
            "referenceCount": 43,
            "citationCount": 112,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-11-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Khodadadeh2018UnsupervisedMF,\n author = {Siavash Khodadadeh and Ladislau B\u00f6l\u00f6ni and M. Shah},\n booktitle = {Neural Information Processing Systems},\n pages = {10132-10142},\n title = {Unsupervised Meta-Learning for Few-Shot Image Classification},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:48e8fa777c89132b82902ffaa2f2c889c494a9a8",
            "@type": "ScholarlyArticle",
            "paperId": "48e8fa777c89132b82902ffaa2f2c889c494a9a8",
            "corpusId": 209415039,
            "url": "https://www.semanticscholar.org/paper/48e8fa777c89132b82902ffaa2f2c889c494a9a8",
            "title": "Continuous Meta-Learning without Tasks",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/nips/HarrisonSFP20",
                "ArXiv": "1912.08866",
                "MAG": "2996170186",
                "CorpusId": 209415039
            },
            "abstract": "Meta-learning is a promising strategy for learning to efficiently learn within new tasks, using data gathered from a distribution of tasks. However, the meta-learning literature thus far has focused on the task segmented setting, where at train-time, offline data is assumed to be split according to the underlying task, and at test-time, the algorithms are optimized to learn in a single task. In this work, we enable the application of generic meta-learning algorithms to settings where this task segmentation is unavailable, such as continual online learning with a time-varying task. We present meta-learning via online changepoint analysis (MOCA), an approach which augments a meta-learning algorithm with a differentiable Bayesian changepoint detection scheme. The framework allows both training and testing directly on time series data without segmenting it into discrete tasks. We demonstrate the utility of this approach on a nonlinear meta-regression benchmark as well as two meta-image-classification benchmarks.",
            "referenceCount": 54,
            "citationCount": 67,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1912.08866"
            },
            "citationStyles": {
                "bibtex": "@Article{Harrison2019ContinuousMW,\n author = {James Harrison and Apoorva Sharma and Chelsea Finn and M. Pavone},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Continuous Meta-Learning without Tasks},\n volume = {abs/1912.08866},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6c72c4ef7c76b4d64fa4c244ec38d70de053d97e",
            "@type": "ScholarlyArticle",
            "paperId": "6c72c4ef7c76b4d64fa4c244ec38d70de053d97e",
            "corpusId": 202578024,
            "url": "https://www.semanticscholar.org/paper/6c72c4ef7c76b4d64fa4c244ec38d70de053d97e",
            "title": "Torchmeta: A Meta-Learning library for PyTorch",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1909.06576",
                "DBLP": "journals/corr/abs-1909-06576",
                "MAG": "2972518327",
                "CorpusId": 202578024
            },
            "abstract": "The constant introduction of standardized benchmarks in the literature has helped accelerating the recent advances in meta-learning research. They offer a way to get a fair comparison between different algorithms, and the wide range of datasets available allows full control over the complexity of this evaluation. However, for a large majority of code available online, the data pipeline is often specific to one dataset, and testing on another dataset requires significant rework. We introduce Torchmeta, a library built on top of PyTorch that enables seamless and consistent evaluation of meta-learning algorithms on multiple datasets, by providing data-loaders for most of the standard benchmarks in few-shot classification and regression, with a new meta-dataset abstraction. It also features some extensions for PyTorch to simplify the development of models compatible with meta-learning algorithms. The code is available here: this https URL",
            "referenceCount": 21,
            "citationCount": 69,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1909.06576"
            },
            "citationStyles": {
                "bibtex": "@Article{Deleu2019TorchmetaAM,\n author = {T. Deleu and Tobias W\u00fcrfl and Mandana Samiei and Joseph Paul Cohen and Yoshua Bengio},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Torchmeta: A Meta-Learning library for PyTorch},\n volume = {abs/1909.06576},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4241d67f0c37c4efa3f35c740a73174b1c83f61e",
            "@type": "ScholarlyArticle",
            "paperId": "4241d67f0c37c4efa3f35c740a73174b1c83f61e",
            "corpusId": 189897731,
            "url": "https://www.semanticscholar.org/paper/4241d67f0c37c4efa3f35c740a73174b1c83f61e",
            "title": "Learning to Forget for Meta-Learning",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2952425013",
                "DBLP": "conf/cvpr/BaikHL20",
                "ArXiv": "1906.05895",
                "DOI": "10.1109/cvpr42600.2020.00245",
                "CorpusId": 189897731
            },
            "abstract": "Few-shot learning is a challenging problem where the goal is to achieve generalization from only few examples. Model-agnostic meta-learning (MAML) tackles the problem by formulating prior knowledge as a common initialization across tasks, which is then used to quickly adapt to unseen tasks. However, forcibly sharing an initialization can lead to conflicts among tasks and the compromised (undesired by tasks) location on optimization landscape, thereby hindering the task adaptation. Further, we observe that the degree of conflict differs among not only tasks but also layers of a neural network. Thus, we propose task-and-layer-wise attenuation on the compromised initialization to reduce its influence. As the attenuation dynamically controls (or selectively forgets) the influence of prior knowledge for a given task and each layer, we name our method as L2F (Learn to Forget). The experimental results demonstrate that the proposed method provides faster adaptation and greatly improves the performance. Furthermore, L2F can be easily applied and improve other state-of-the-art MAML-based frameworks, illustrating its simplicity and generalizability.",
            "referenceCount": 45,
            "citationCount": 67,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1906.05895",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-06-13",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Baik2019LearningTF,\n author = {Sungyong Baik and Seokil Hong and Kyoung Mu Lee},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2376-2384},\n title = {Learning to Forget for Meta-Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:162d04c2340195b6055f5d28f9e1fdba93075aef",
            "@type": "ScholarlyArticle",
            "paperId": "162d04c2340195b6055f5d28f9e1fdba93075aef",
            "corpusId": 201670816,
            "url": "https://www.semanticscholar.org/paper/162d04c2340195b6055f5d28f9e1fdba93075aef",
            "title": "Learning to Demodulate From Few Pilots via Offline and Online Meta-Learning",
            "venue": "IEEE Transactions on Signal Processing",
            "publicationVenue": {
                "id": "urn:research:1f6f3f05-6a23-42f0-8d31-98ab8089c1f2",
                "name": "IEEE Transactions on Signal Processing",
                "alternate_names": [
                    "IEEE Trans Signal Process"
                ],
                "issn": "1053-587X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=78"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1908-09049",
                "MAG": "2969488136",
                "ArXiv": "1908.09049",
                "DOI": "10.1109/TSP.2020.3043879",
                "CorpusId": 201670816
            },
            "abstract": "This paper considers an Internet-of-Things (IoT) scenario in which devices sporadically transmit short packets with few pilot symbols over a fading channel. Devices are characterized by unique transmission non-idealities, such as I/Q imbalance. The number of pilots is generally insufficient to obtain an accurate estimate of the end-to-end channel, which includes the effects of fading and of the transmission-side distortion. This paper proposes to tackle this problem by using meta-learning. Accordingly, pilots from previous IoT transmissions are used as meta-training data in order to train a demodulator that is able to quickly adapt to new end-to-end channel conditions from few pilots. Various state-of-the-art meta-learning schemes are adapted to the problem at hand and evaluated, including Model-Agnostic Meta-Learning (MAML), First-Order MAML (FOMAML), REPTILE, and fast Context Adaptation VIA meta-learning (CAVIA). Both offline and online solutions are developed. In the latter case, an integrated online meta-learning and adaptive pilot number selection scheme is proposed. Numerical results validate the advantages of meta-learning as compared to training schemes that either do not leverage prior transmissions or apply a standard joint learning algorithms on previously received data.",
            "referenceCount": 57,
            "citationCount": 65,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1908.09049",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-08-23",
            "journal": {
                "name": "IEEE Transactions on Signal Processing",
                "volume": "69"
            },
            "citationStyles": {
                "bibtex": "@Article{Park2019LearningTD,\n author = {Sangwook Park and Hyeryung Jang and O. Simeone and Joonhyuk Kang},\n booktitle = {IEEE Transactions on Signal Processing},\n journal = {IEEE Transactions on Signal Processing},\n pages = {226-239},\n title = {Learning to Demodulate From Few Pilots via Offline and Online Meta-Learning},\n volume = {69},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e2144c8d76241ed8293864243546831f0f0c4cb6",
            "@type": "ScholarlyArticle",
            "paperId": "e2144c8d76241ed8293864243546831f0f0c4cb6",
            "corpusId": 195069420,
            "url": "https://www.semanticscholar.org/paper/e2144c8d76241ed8293864243546831f0f0c4cb6",
            "title": "Reconciling meta-learning and continual learning with online mixtures of tasks",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/nips/JerfelGGH19",
                "MAG": "2952951948",
                "CorpusId": 195069420
            },
            "abstract": "Learning-to-learn or meta-learning leverages data-driven inductive bias to increase the efficiency of learning on a novel task. This approach encounters difficulty when transfer is not advantageous, for instance, when tasks are considerably dissimilar or change over time. We use the connection between gradient-based meta-learning and hierarchical Bayes to propose a Dirichlet process mixture of hierarchical Bayesian models over the parameters of an arbitrary parametric model such as a neural network. In contrast to consolidating inductive biases into a single set of hyperparameters, our approach of task-dependent hyperparameter selection better handles latent distribution shift, as demonstrated on a set of evolving, image-based, few-shot learning benchmarks.",
            "referenceCount": 71,
            "citationCount": 93,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-12-14",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Jerfel2018ReconcilingMA,\n author = {Ghassen Jerfel and Erin Grant and T. Griffiths and K. Heller},\n booktitle = {Neural Information Processing Systems},\n pages = {9119-9130},\n title = {Reconciling meta-learning and continual learning with online mixtures of tasks},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9c0912153128e31743948cd629d6eebe6d916b42",
            "@type": "ScholarlyArticle",
            "paperId": "9c0912153128e31743948cd629d6eebe6d916b42",
            "corpusId": 3740285,
            "url": "https://www.semanticscholar.org/paper/9c0912153128e31743948cd629d6eebe6d916b42",
            "title": "Natural Language to Structured Query Generation via Meta-Learning",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:01103732-3808-4930-b8e4-7e9e68d5c68d",
                "name": "North American Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "North Am Chapter Assoc Comput Linguistics",
                    "NAACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/naacl"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963866663",
                "DBLP": "journals/corr/abs-1803-02400",
                "ACL": "N18-2115",
                "ArXiv": "1803.02400",
                "DOI": "10.18653/v1/N18-2115",
                "CorpusId": 3740285
            },
            "abstract": "In conventional supervised training, a model is trained to fit all the training examples. However, having a monolithic model may not always be the best strategy, as examples could vary widely. In this work, we explore a different learning protocol that treats each example as a unique pseudo-task, by reducing the original learning problem to a few-shot meta-learning scenario with the help of a domain-dependent relevance function. When evaluated on the WikiSQL dataset, our approach leads to faster convergence and achieves 1.1%\u20135.4% absolute accuracy gains over the non-meta-learning counterparts.",
            "referenceCount": 43,
            "citationCount": 113,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/N18-2115.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-03-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Huang2018NaturalLT,\n author = {Po-Sen Huang and Chenglong Wang and Rishabh Singh and Wen-tau Yih and Xiaodong He},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {732-738},\n title = {Natural Language to Structured Query Generation via Meta-Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:35ad6ba10006975c2bc67ecefaa9ee6af2453bdc",
            "@type": "ScholarlyArticle",
            "paperId": "35ad6ba10006975c2bc67ecefaa9ee6af2453bdc",
            "corpusId": 3627225,
            "url": "https://www.semanticscholar.org/paper/35ad6ba10006975c2bc67ecefaa9ee6af2453bdc",
            "title": "Deep Meta-Learning: Learning to Learn in the Concept Space",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1802-03596",
                "ArXiv": "1802.03596",
                "MAG": "2786928087",
                "CorpusId": 3627225
            },
            "abstract": "Few-shot learning remains challenging for meta-learning that learns a learning algorithm (meta-learner) from many related tasks. In this work, we argue that this is due to the lack of a good representation for meta-learning, and propose deep meta-learning to integrate the representation power of deep learning into meta-learning. The framework is composed of three modules, a concept generator, a meta-learner, and a concept discriminator, which are learned jointly. The concept generator, e.g. a deep residual net, extracts a representation for each instance that captures its high-level concept, on which the meta-learner performs few-shot learning, and the concept discriminator recognizes the concepts. By learning to learn in the concept space rather than in the complicated instance space, deep meta-learning can substantially improve vanilla meta-learning, which is demonstrated on various few-shot image recognition problems. For example, on 5-way-1-shot image recognition on CIFAR-100 and CUB-200, it improves Matching Nets from 50.53% and 56.53% to 58.18% and 63.47%, improves MAML from 49.28% and 50.45% to 56.65% and 64.63%, and improves Meta-SGD from 53.83% and 53.34% to 61.62% and 66.95%, respectively.",
            "referenceCount": 49,
            "citationCount": 112,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-10",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.03596"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhou2018DeepML,\n author = {Fengwei Zhou and Bin Wu and Zhenguo Li},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Deep Meta-Learning: Learning to Learn in the Concept Space},\n volume = {abs/1802.03596},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:efa2cb68f619dea3770115884d6a5826f7117654",
            "@type": "ScholarlyArticle",
            "paperId": "efa2cb68f619dea3770115884d6a5826f7117654",
            "corpusId": 52954281,
            "url": "https://www.semanticscholar.org/paper/efa2cb68f619dea3770115884d6a5826f7117654",
            "title": "Few-Shot Human Motion Prediction via Meta-learning",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2894846593",
                "DBLP": "conf/eccv/GuiWRM18",
                "DOI": "10.1007/978-3-030-01237-3_27",
                "CorpusId": 52954281
            },
            "abstract": null,
            "referenceCount": 72,
            "citationCount": 107,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-09-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Gui2018FewShotHM,\n author = {Liangyan Gui and Yu-Xiong Wang and Deva Ramanan and Jos\u00e9 M. F. Moura},\n booktitle = {European Conference on Computer Vision},\n pages = {441-459},\n title = {Few-Shot Human Motion Prediction via Meta-learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a58e9404695acf3165c4b77798e5bd8bdc6cfcf3",
            "@type": "ScholarlyArticle",
            "paperId": "a58e9404695acf3165c4b77798e5bd8bdc6cfcf3",
            "corpusId": 159042059,
            "url": "https://www.semanticscholar.org/paper/a58e9404695acf3165c4b77798e5bd8bdc6cfcf3",
            "title": "Alpha MAML: Adaptive Model-Agnostic Meta-Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2946605054",
                "ArXiv": "1905.07435",
                "DBLP": "journals/corr/abs-1905-07435",
                "CorpusId": 159042059
            },
            "abstract": "Model-agnostic meta-learning (MAML) is a meta-learning technique to train a model on a multitude of learning tasks in a way that primes the model for few-shot learning of new tasks. The MAML algorithm performs well on few-shot learning problems in classification, regression, and fine-tuning of policy gradients in reinforcement learning, but comes with the need for costly hyperparameter tuning for training stability. We address this shortcoming by introducing an extension to MAML, called Alpha MAML, to incorporate an online hyperparameter adaptation scheme that eliminates the need to tune meta-learning and learning rates. Our results with the Omniglot database demonstrate a substantial reduction in the need to tune MAML training hyperparameters and improvement to training stability with less sensitivity to hyperparameter choice.",
            "referenceCount": 18,
            "citationCount": 62,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-05-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1905.07435"
            },
            "citationStyles": {
                "bibtex": "@Article{Behl2019AlphaMA,\n author = {Harkirat Singh Behl and A. G. Baydin and Philip H. S. Torr},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Alpha MAML: Adaptive Model-Agnostic Meta-Learning},\n volume = {abs/1905.07435},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1cdc01832acb5d3451ef03230daec0ad66d52078",
            "@type": "ScholarlyArticle",
            "paperId": "1cdc01832acb5d3451ef03230daec0ad66d52078",
            "corpusId": 209439817,
            "url": "https://www.semanticscholar.org/paper/1cdc01832acb5d3451ef03230daec0ad66d52078",
            "title": "Meta-Graph: Few shot Link Prediction via Meta Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1912.09867",
                "DBLP": "journals/corr/abs-1912-09867",
                "MAG": "2994972767",
                "CorpusId": 209439817
            },
            "abstract": "We consider the task of few shot link prediction on graphs. The goal is to learn from a distribution over graphs so that a model is able to quickly infer missing edges in a new graph after a small amount of training. We show that current link prediction methods are generally ill-equipped to handle this task. They cannot effectively transfer learned knowledge from one graph to another and are unable to effectively learn from sparse samples of edges. To address this challenge, we introduce a new gradient-based meta learning framework, Meta-Graph. Our framework leverages higher-order gradients along with a learned graph signature function that conditionally generates a graph neural network initialization. Using a novel set of few shot link prediction benchmarks, we show that Meta-Graph can learn to quickly adapt to a new graph using only a small sample of true edges, enabling not only fast adaptation but also improved results at convergence.",
            "referenceCount": 44,
            "citationCount": 57,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1912.09867"
            },
            "citationStyles": {
                "bibtex": "@Article{Bose2019MetaGraphFS,\n author = {A. Bose and Ankit Jain and Piero Molino and William L. Hamilton},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Meta-Graph: Few shot Link Prediction via Meta Learning},\n volume = {abs/1912.09867},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8e21d353ba283bee8fd18285558e5e8df39d46e8",
            "@type": "ScholarlyArticle",
            "paperId": "8e21d353ba283bee8fd18285558e5e8df39d46e8",
            "corpusId": 3451040,
            "url": "https://www.semanticscholar.org/paper/8e21d353ba283bee8fd18285558e5e8df39d46e8",
            "title": "Federated Meta-Learning for Recommendation",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1802-07876",
                "MAG": "2788629937",
                "CorpusId": 3451040
            },
            "abstract": "Recommender systems have been widely studied from the machine learning perspective, where it is crucial to share information among users while preserving user privacy. In this work, we present a federated meta-learning framework for recommendation in which user information is shared at the level of algorithm, instead of model or data adopted in previous approaches. In this framework, user-specific recommendation models are locally trained by a shared parameterized algorithm, which preserves user privacy and at the same time utilizes information from other users to help model training. Interestingly, the model thus trained exhibits a high capacity at a small scale, which is energy- and communication-efficient. Experimental results show that recommendation models trained by meta-learning algorithms in the proposed framework outperform the state-of-the-art in accuracy and scale. For example, on a production dataset, a shared model under Google Federated Learning (McMahan et al., 2017) with 900,000 parameters has prediction accuracy 76.72%, while a shared algorithm under federated meta-learning with less than 30,000 parameters achieves accuracy of 86.23%.",
            "referenceCount": 20,
            "citationCount": 120,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.07876"
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2018FederatedMF,\n author = {Fei Chen and Zhenhua Dong and Zhenguo Li and Xiuqiang He},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Federated Meta-Learning for Recommendation},\n volume = {abs/1802.07876},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0fddb9c115f87362ec7315911c614074140041e7",
            "@type": "ScholarlyArticle",
            "paperId": "0fddb9c115f87362ec7315911c614074140041e7",
            "corpusId": 202774014,
            "url": "https://www.semanticscholar.org/paper/0fddb9c115f87362ec7315911c614074140041e7",
            "title": "Online-Within-Online Meta-Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/nips/DeneviSCP19",
                "MAG": "2971285974",
                "CorpusId": 202774014
            },
            "abstract": "We study the problem of learning a series of tasks in a fully online Meta-Learning setting. The goal is to exploit similarities among the tasks to incrementally adapt an inner online algorithm in order to incur a low averaged cumulative error over the tasks. We focus on a family of inner algorithms based on a parametrized variant of online Mirror Descent. The inner algorithm is incrementally adapted by an online Mirror Descent meta-algorithm using the corresponding within-task minimum regularized empirical risk as the meta-loss. In order to keep the process fully online, we approximate the meta-subgradients by the online inner algorithm. An upper bound on the approximation error allows us to derive a cumulative error bound for the proposed method. Our analysis can also be converted to the statistical setting by online-to-batch arguments. We instantiate two examples of the framework in which the meta-parameter is either a common bias vector or feature map. Finally, preliminary numerical experiments confirm our theoretical findings.",
            "referenceCount": 41,
            "citationCount": 46,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Denevi2019OnlineWithinOnlineM,\n author = {Giulia Denevi and Dimitris Stamos and Carlo Ciliberto and Massimiliano Pontil},\n booktitle = {Neural Information Processing Systems},\n pages = {13089-13099},\n title = {Online-Within-Online Meta-Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2636bff7d3bdccf9b39c5e1e7d86a77690f1c07d",
            "@type": "ScholarlyArticle",
            "paperId": "2636bff7d3bdccf9b39c5e1e7d86a77690f1c07d",
            "corpusId": 59316654,
            "url": "https://www.semanticscholar.org/paper/2636bff7d3bdccf9b39c5e1e7d86a77690f1c07d",
            "title": "Reward Shaping via Meta-Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1901-09330",
                "ArXiv": "1901.09330",
                "MAG": "2913350117",
                "CorpusId": 59316654
            },
            "abstract": "Reward shaping is one of the most effective methods to tackle the crucial yet challenging problem of credit assignment in Reinforcement Learning (RL). However, designing shaping functions usually requires much expert knowledge and hand-engineering, and the difficulties are further exacerbated given multiple similar tasks to solve. In this paper, we consider reward shaping on a distribution of tasks, and propose a general meta-learning framework to automatically learn the efficient reward shaping on newly sampled tasks, assuming only shared state space but not necessarily action space. We first derive the theoretically optimal reward shaping in terms of credit assignment in model-free RL. We then propose a value-based meta-learning algorithm to extract an effective prior over the optimal reward shaping. The prior can be applied directly to new tasks, or provably adapted to the task-posterior while solving the task within few gradient updates. We demonstrate the effectiveness of our shaping through significantly improved learning efficiency and interpretable visualizations across various settings, including notably a successful transfer from DQN to DDPG.",
            "referenceCount": 31,
            "citationCount": 55,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-01-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1901.09330"
            },
            "citationStyles": {
                "bibtex": "@Article{Zou2019RewardSV,\n author = {Haosheng Zou and Tongzheng Ren and Dong Yan and Hang Su and Jun Zhu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Reward Shaping via Meta-Learning},\n volume = {abs/1901.09330},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a65ab958b0588d64c43cbeca336d3d2a97d971db",
            "@type": "ScholarlyArticle",
            "paperId": "a65ab958b0588d64c43cbeca336d3d2a97d971db",
            "corpusId": 208637221,
            "url": "https://www.semanticscholar.org/paper/a65ab958b0588d64c43cbeca336d3d2a97d971db",
            "title": "MetaFun: Meta-Learning with Iterative Functional Updates",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/icml/XuTKKT20",
                "ArXiv": "1912.02738",
                "MAG": "3034710639",
                "CorpusId": 208637221
            },
            "abstract": "We develop a functional encoder-decoder approach to supervised meta-learning, where labeled data is encoded into an infinite-dimensional functional representation rather than a finite-dimensional one. Furthermore, rather than directly producing the representation, we learn a neural update rule resembling functional gradient descent which iteratively improves the representation. The final representation is used to condition the decoder to make predictions on unlabeled data. Our approach is the first to demonstrates the success of encoder-decoder style meta-learning methods like conditional neural processes on large-scale few-shot classification benchmarks such as miniImageNet and tieredImageNet, where it achieves state-of-the-art performance.",
            "referenceCount": 45,
            "citationCount": 55,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-12-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Xu2019MetaFunMW,\n author = {Jin Xu and Jean-Francois Ton and Hyunjik Kim and Adam R. Kosiorek and Y. Teh},\n booktitle = {International Conference on Machine Learning},\n pages = {10617-10627},\n title = {MetaFun: Meta-Learning with Iterative Functional Updates},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:56d13402da5ff98ffdfff640fb987b9b13e924cf",
            "@type": "ScholarlyArticle",
            "paperId": "56d13402da5ff98ffdfff640fb987b9b13e924cf",
            "corpusId": 4723494,
            "url": "https://www.semanticscholar.org/paper/56d13402da5ff98ffdfff640fb987b9b13e924cf",
            "title": "Learning to Adapt: Meta-Learning for Model-Based Control",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1803-11347",
                "MAG": "2794757725",
                "CorpusId": 4723494
            },
            "abstract": "Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations can cause proficient but narrowly-learned policies to fail at test time. In this work, we propose to learn how to quickly and effectively adapt online to new situations as well as to perturbations. To enable sample-efficient meta-learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach trains a global model such that, when combined with recent data, the model can be be rapidly adapted to the local context. Our experiments demonstrate that our approach can enable simulated agents to adapt their behavior online to novel terrains, to a crippled leg, and in highly-dynamic environments.",
            "referenceCount": 72,
            "citationCount": 78,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-03-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1803.11347"
            },
            "citationStyles": {
                "bibtex": "@Article{Clavera2018LearningTA,\n author = {I. Clavera and Anusha Nagabandi and R. Fearing and P. Abbeel and S. Levine and Chelsea Finn},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Learning to Adapt: Meta-Learning for Model-Based Control},\n volume = {abs/1803.11347},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b93317f61c6ed99542da9d1d691ded9732c16c1c",
            "@type": "ScholarlyArticle",
            "paperId": "b93317f61c6ed99542da9d1d691ded9732c16c1c",
            "corpusId": 48364213,
            "url": "https://www.semanticscholar.org/paper/b93317f61c6ed99542da9d1d691ded9732c16c1c",
            "title": "Unsupervised Meta-Learning for Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1806-04640",
                "MAG": "2808682055",
                "ArXiv": "1806.04640",
                "CorpusId": 48364213
            },
            "abstract": "Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta-learning methods generalize best to tasks from the same distribution as the meta-training tasks. In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design. If we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automated. In this work, we take a step in this direction, proposing a family of unsupervised meta-learning algorithms for reinforcement learning. We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approach. Our conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can be used to train optimal meta-learners. Our experimental results indicate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design and these procedures exceed the performance of learning from scratch.",
            "referenceCount": 69,
            "citationCount": 93,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.04640"
            },
            "citationStyles": {
                "bibtex": "@Article{Gupta2018UnsupervisedMF,\n author = {Abhishek Gupta and Benjamin Eysenbach and Chelsea Finn and S. Levine},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Unsupervised Meta-Learning for Reinforcement Learning},\n volume = {abs/1806.04640},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e962c301df1d33bc12d8115f4c82093103c94eeb",
            "@type": "ScholarlyArticle",
            "paperId": "e962c301df1d33bc12d8115f4c82093103c94eeb",
            "corpusId": 196175445,
            "url": "https://www.semanticscholar.org/paper/e962c301df1d33bc12d8115f4c82093103c94eeb",
            "title": "Model-Agnostic Meta-Learning for Relation Classification with Limited Supervision",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2953111196",
                "ACL": "P19-1589",
                "DBLP": "conf/acl/ObamuyideV19",
                "DOI": "10.18653/v1/P19-1589",
                "CorpusId": 196175445
            },
            "abstract": "In this paper we frame the task of supervised relation classification as an instance of meta-learning. We propose a model-agnostic meta-learning protocol for training relation classifiers to achieve enhanced predictive performance in limited supervision settings. During training, we aim to not only learn good parameters for classifying relations with sufficient supervision, but also learn model parameters that can be fine-tuned to enhance predictive performance for relations with limited supervision. In experiments conducted on two relation classification datasets, we demonstrate that the proposed meta-learning approach improves the predictive performance of two state-of-the-art supervised relation classification models.",
            "referenceCount": 31,
            "citationCount": 49,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/P19-1589.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-07-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Obamuyide2019ModelAgnosticMF,\n author = {A. Obamuyide and Andreas Vlachos},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {5873-5879},\n title = {Model-Agnostic Meta-Learning for Relation Classification with Limited Supervision},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:482c0cbfffa77154e3c879c497f50b605297d5bc",
            "@type": "ScholarlyArticle",
            "paperId": "482c0cbfffa77154e3c879c497f50b605297d5bc",
            "corpusId": 22221787,
            "url": "https://www.semanticscholar.org/paper/482c0cbfffa77154e3c879c497f50b605297d5bc",
            "title": "One-Shot Visual Imitation Learning via Meta-Learning",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2755546070",
                "ArXiv": "1709.04905",
                "DBLP": "conf/corl/FinnYZAL17",
                "CorpusId": 22221787
            },
            "abstract": "In order for a robot to be a generalist that can perform a wide range of jobs, it must be able to acquire a wide variety of skills quickly and efficiently in complex unstructured environments. High-capacity models such as deep neural networks can enable a robot to represent complex skills, but learning each skill from scratch then becomes infeasible. In this work, we present a meta-imitation learning method that enables a robot to learn how to learn more efficiently, allowing it to acquire new skills from just a single demonstration. Unlike prior methods for one-shot imitation, our method can scale to raw pixel inputs and requires data from significantly fewer prior tasks for effective learning of new skills. Our experiments on both simulated and real robot platforms demonstrate the ability to learn new tasks, end-to-end, from a single visual demonstration.",
            "referenceCount": 34,
            "citationCount": 460,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-09-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1709.04905"
            },
            "citationStyles": {
                "bibtex": "@Article{Finn2017OneShotVI,\n author = {Chelsea Finn and Tianhe Yu and Tianhao Zhang and P. Abbeel and S. Levine},\n booktitle = {Conference on Robot Learning},\n journal = {ArXiv},\n title = {One-Shot Visual Imitation Learning via Meta-Learning},\n volume = {abs/1709.04905},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fd66dfb9ca3eb16b365b621bcdf1145b3a29f51e",
            "@type": "ScholarlyArticle",
            "paperId": "fd66dfb9ca3eb16b365b621bcdf1145b3a29f51e",
            "corpusId": 49191550,
            "url": "https://www.semanticscholar.org/paper/fd66dfb9ca3eb16b365b621bcdf1145b3a29f51e",
            "title": "Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1806-04798",
                "ArXiv": "1806.04798",
                "MAG": "2785494456",
                "CorpusId": 49191550
            },
            "abstract": "Active learning (AL) aims to enable training high performance classifiers with low annotation cost by predicting which subset of unlabelled instances would be most beneficial to label. The importance of AL has motivated extensive research, proposing a wide variety of manually designed AL algorithms with diverse theoretical and intuitive motivations. In contrast to this body of research, we propose to treat active learning algorithm design as a meta-learning problem and learn the best criterion from data. We model an active learning algorithm as a deep neural network that inputs the base learner state and the unlabelled point set and predicts the best point to annotate next. Training this active query policy network with reinforcement learning, produces the best non-myopic policy for a given dataset. The key challenge in achieving a general solution to AL then becomes that of learner generalisation, particularly across heterogeneous datasets. We propose a multi-task dataset-embedding approach that allows dataset-agnostic active learners to be trained. Our evaluation shows that AL algorithms trained in this way can directly generalise across diverse problems.",
            "referenceCount": 16,
            "citationCount": 71,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.04798"
            },
            "citationStyles": {
                "bibtex": "@Article{Pang2018MetaLearningTA,\n author = {Kunkun Pang and Mingzhi Dong and Yang Wu and Timothy M. Hospedales},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning},\n volume = {abs/1806.04798},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:105905e42812a88b5060e6f45805aa7889e6218c",
            "@type": "ScholarlyArticle",
            "paperId": "105905e42812a88b5060e6f45805aa7889e6218c",
            "corpusId": 202542534,
            "url": "https://www.semanticscholar.org/paper/105905e42812a88b5060e6f45805aa7889e6218c",
            "title": "Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label Classification",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/emnlp/WuXW19",
                "ArXiv": "1909.04176",
                "ACL": "D19-1444",
                "MAG": "2972580772",
                "DOI": "10.18653/v1/D19-1444",
                "CorpusId": 202542534
            },
            "abstract": "Many tasks in natural language processing can be viewed as multi-label classification problems. However, most of the existing models are trained with the standard cross-entropy loss function and use a fixed prediction policy (e.g., a threshold of 0.5) for all the labels, which completely ignores the complexity and dependencies among different labels. In this paper, we propose a meta-learning method to capture these complex label dependencies. More specifically, our method utilizes a meta-learner to jointly learn the training policies and prediction policies for different labels. The training policies are then used to train the classifier with the cross-entropy loss function, and the prediction policies are further implemented for prediction. Experimental results on fine-grained entity typing and text classification demonstrate that our proposed method can obtain more accurate multi-label classification results.",
            "referenceCount": 46,
            "citationCount": 51,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1909.04176",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-09-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1909.04176"
            },
            "citationStyles": {
                "bibtex": "@Article{Wu2019LearningTL,\n author = {Jiawei Wu and Wenhan Xiong and William Yang Wang},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label Classification},\n volume = {abs/1909.04176},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f3135b553f592dc42d4202c90739c99486103fc3",
            "@type": "ScholarlyArticle",
            "paperId": "f3135b553f592dc42d4202c90739c99486103fc3",
            "corpusId": 203606047,
            "url": "https://www.semanticscholar.org/paper/f3135b553f592dc42d4202c90739c99486103fc3",
            "title": "Meta-Learning for User Cold-Start Recommendation",
            "venue": "IEEE International Joint Conference on Neural Network",
            "publicationVenue": {
                "id": "urn:research:f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                "name": "IEEE International Joint Conference on Neural Network",
                "alternate_names": [
                    "IJCNN",
                    "IEEE Int Jt Conf Neural Netw",
                    "Int Jt Conf Neural Netw",
                    "International Joint Conference on Neural Network"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=1573"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/ijcnn/Bharadhwaj19",
                "MAG": "2978745145",
                "DOI": "10.1109/IJCNN.2019.8852100",
                "CorpusId": 203606047
            },
            "abstract": "Recent studies in recommender systems emphasize the importance of dealing with the cold-start problem i.e. the modeling of new users or items in the recommendation system. Meta-learning approaches have gained popularity recently in the Machine Learning (ML) community for learning representations useful for a wide-range of tasks. Inspired by the generalizable modeling prowess of Model-Agnostic Meta Learning, we design a recommendation framework that is trained to be reasonably good enough for a wide range of users. During testing, to adapt to a specific user, the model parameters are updated by a few gradient steps. We evaluate our approach on three different benchmark datasets, from Movielens, Netflix, and MyFitnessPal. Through detailed simulation studies, we show that this framework handles the user cold-start model much better than state-of-the art benchmark recommender systems. We also show that the proposed approach performs well on the task of general recommendations to non cold-start users and effectively takes care of routine and eclectic preference trends of users.",
            "referenceCount": 46,
            "citationCount": 54,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-07-01",
            "journal": {
                "name": "2019 International Joint Conference on Neural Networks (IJCNN)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bharadhwaj2019MetaLearningFU,\n author = {Homanga Bharadhwaj},\n booktitle = {IEEE International Joint Conference on Neural Network},\n journal = {2019 International Joint Conference on Neural Networks (IJCNN)},\n pages = {1-8},\n title = {Meta-Learning for User Cold-Start Recommendation},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6c9a0b64498a99015c1d990473f60544f34bdec0",
            "@type": "ScholarlyArticle",
            "paperId": "6c9a0b64498a99015c1d990473f60544f34bdec0",
            "corpusId": 198968022,
            "url": "https://www.semanticscholar.org/paper/6c9a0b64498a99015c1d990473f60544f34bdec0",
            "title": "Uncertainty in Model-Agnostic Meta-Learning using Variational Inference",
            "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:acd15a6d-3248-41fb-8439-9a40aabe5608",
                "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                "alternate_names": [
                    "Workshop on Applications of Computer Vision",
                    "WACV",
                    "IEEE Work Conf Appl Comput Vis",
                    "Workshop Appl Comput Vis"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=2993"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1907-11864",
                "ArXiv": "1907.11864",
                "MAG": "3023119660",
                "DOI": "10.1109/WACV45572.2020.9093536",
                "CorpusId": 198968022
            },
            "abstract": "We introduce a new, rigorously-formulated Bayesian meta-learning algorithm that learns a probability distribution of model parameter prior for few-shot learning. The proposed algorithm employs a gradient-based variational inference to infer the posterior of model parameters for a new task. Our algorithm can be applied to any model architecture and can be implemented in various machine learning paradigms, including regression and classification. We show that the models trained with our proposed meta-learning algorithm are well calibrated and accurate, with state-of-the-art calibration and classification results on three few-shot classification benchmarks (Om- niglot, mini-ImageNet and tiered-ImageNet), and competitive results in a multi-modal task-distribution regression.",
            "referenceCount": 58,
            "citationCount": 43,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1907.11864",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-07-27",
            "journal": {
                "name": "2020 IEEE Winter Conference on Applications of Computer Vision (WACV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nguyen2019UncertaintyIM,\n author = {Cuong Q. Nguyen and Thanh-Toan Do and G. Carneiro},\n booktitle = {IEEE Workshop/Winter Conference on Applications of Computer Vision},\n journal = {2020 IEEE Winter Conference on Applications of Computer Vision (WACV)},\n pages = {3079-3089},\n title = {Uncertainty in Model-Agnostic Meta-Learning using Variational Inference},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c6509a450bdda7ca5b8567103dfe9671dbf3b567",
            "@type": "ScholarlyArticle",
            "paperId": "c6509a450bdda7ca5b8567103dfe9671dbf3b567",
            "corpusId": 67856088,
            "url": "https://www.semanticscholar.org/paper/c6509a450bdda7ca5b8567103dfe9671dbf3b567",
            "title": "Meta-Learning Update Rules for Unsupervised Representation Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/iclr/MetzMCS19",
                "MAG": "2963154787",
                "ArXiv": "1804.00222",
                "CorpusId": 67856088
            },
            "abstract": "A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks. Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.",
            "referenceCount": 69,
            "citationCount": 107,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-03-31",
            "journal": {
                "name": "arXiv: Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Metz2018MetaLearningUR,\n author = {Luke Metz and Niru Maheswaranathan and Brian Cheung and Jascha Narain Sohl-Dickstein},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Learning},\n title = {Meta-Learning Update Rules for Unsupervised Representation Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:89c622f711c10e1bdc0c8e50b9ca4bd6936c73b7",
            "@type": "ScholarlyArticle",
            "paperId": "89c622f711c10e1bdc0c8e50b9ca4bd6936c73b7",
            "corpusId": 145697895,
            "url": "https://www.semanticscholar.org/paper/89c622f711c10e1bdc0c8e50b9ca4bd6936c73b7",
            "title": "Visible learning: a synthesis of over 800 meta\u2010analyses relating to achievement",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2153022270",
                "DOI": "10.1080/01443410903415150",
                "CorpusId": 145697895
            },
            "abstract": null,
            "referenceCount": 7,
            "citationCount": 5513,
            "influentialCitationCount": 593,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2009-11-30",
            "journal": {
                "name": "Educational Psychology",
                "volume": "29"
            },
            "citationStyles": {
                "bibtex": "@Article{Carter2009VisibleLA,\n author = {M. Carter},\n journal = {Educational Psychology},\n pages = {867 - 869},\n title = {Visible learning: a synthesis of over 800 meta\u2010analyses relating to achievement},\n volume = {29},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a031a81d627f00e89fda7f544f4d8d51b4bd8645",
            "@type": "ScholarlyArticle",
            "paperId": "a031a81d627f00e89fda7f544f4d8d51b4bd8645",
            "corpusId": 195767355,
            "url": "https://www.semanticscholar.org/paper/a031a81d627f00e89fda7f544f4d8d51b4bd8645",
            "title": "Difficulty-aware Meta-Learning for Rare Disease Diagnosis",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "publicationVenue": {
                "id": "urn:research:61a709e3-2060-423c-8de5-ffd3885aa31c",
                "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                "alternate_names": [
                    "Medical Image Computing and Computer-Assisted Intervention",
                    "MICCAI",
                    "Med Image Comput Comput Interv",
                    "Int Conf Med Image Comput Comput Interv"
                ],
                "issn": null,
                "url": "http://www.miccai.org/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1907.00354",
                "DBLP": "conf/miccai/LiYJFXH20",
                "MAG": "3092017801",
                "DOI": "10.1007/978-3-030-59710-8_35",
                "CorpusId": 195767355
            },
            "abstract": null,
            "referenceCount": 19,
            "citationCount": 42,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1907.00354",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-06-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1907.00354"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2019DifficultyawareMF,\n author = {X. Li and Lequan Yu and Chi-Wing Fu and P. Heng},\n booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},\n journal = {ArXiv},\n title = {Difficulty-aware Meta-Learning for Rare Disease Diagnosis},\n volume = {abs/1907.00354},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:478b133c57b1b449745358354a3ee6fa46e06c4a",
            "@type": "ScholarlyArticle",
            "paperId": "478b133c57b1b449745358354a3ee6fa46e06c4a",
            "corpusId": 204207649,
            "url": "https://www.semanticscholar.org/paper/478b133c57b1b449745358354a3ee6fa46e06c4a",
            "title": "Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2995957611",
                "ArXiv": "1904.02642",
                "DBLP": "conf/iclr/VolppFFDFHD20",
                "CorpusId": 204207649
            },
            "abstract": "Transferring knowledge across tasks to improve data-efficiency is one of the open key challenges in the field of global black-box optimization. Readily available algorithms are typically designed to be universal optimizers and, therefore, often suboptimal for specific tasks. We propose a novel transfer learning method to obtain customized optimizers within the well-established framework of Bayesian optimization, allowing our algorithm to utilize the proven generalization capabilities of Gaussian processes. Using reinforcement learning to meta-train an acquisition function (AF) on a set of related tasks, the proposed method learns to extract implicit structural information and to exploit it for improved data-efficiency. We present experiments on a simulation-to-real transfer task as well as on several synthetic functions and on two hyperparameter search problems. The results show that our algorithm (1) automatically identifies structural properties of objective functions from available source tasks or simulations, (2) performs favourably in settings with both scarse and abundant source data, and (3) falls back to the performance level of general AFs if no particular structure is present.",
            "referenceCount": 51,
            "citationCount": 57,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-04-04",
            "journal": {
                "name": "arXiv: Machine Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Volpp2019MetaLearningAF,\n author = {Michael Volpp and Lukas P. Fr\u00f6hlich and Kirsten Fischer and Andreas Doerr and S. Falkner and F. Hutter and Christian Daniel},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Machine Learning},\n title = {Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ff2d4863677076b65aa8672b08eca8f1b4805223",
            "@type": "ScholarlyArticle",
            "paperId": "ff2d4863677076b65aa8672b08eca8f1b4805223",
            "corpusId": 195738594,
            "url": "https://www.semanticscholar.org/paper/ff2d4863677076b65aa8672b08eca8f1b4805223",
            "title": "Meta-SSD: Towards Fast Adaptation for Few-Shot Object Detection With Meta-Learning",
            "venue": "IEEE Access",
            "publicationVenue": {
                "id": "urn:research:2633f5b2-c15c-49fe-80f5-07523e770c26",
                "name": "IEEE Access",
                "alternate_names": null,
                "issn": "2169-3536",
                "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2950637875",
                "DBLP": "journals/access/FuZZYCZS19",
                "DOI": "10.1109/ACCESS.2019.2922438",
                "CorpusId": 195738594
            },
            "abstract": "The state-of-the-art object detection frameworks require the training on large-scale datasets, which is the crux of the present dilemma: overfitting or degrading performance with insufficient samples and time-consuming training process. On the basis of meta-learning, this paper proposes a generalized Few-Shot Detection (FSD) framework to overcome the above drawbacks of the current advances in object detection. The proposed framework consists of a meta-learner and an object detector. It can learn the general knowledge and proper fast adaptation strategies across many tasks. The meta-learner can teach the detector how to learn from few examples in just one updating step. Here, the object detector can be any supervised learning detection models in theory. Specifically, the proposed FSD framework employs Single-Shot MultiBox Detector (SSD) as the object detector in this paper, thus called Meta-SSD. Besides, a novel benchmark is constructed from Pascal VOC dataset for training and evaluation of meta-learning FSD. Experiments show that the Meta-SSD yields a promising result for FSD. Furthermore, the properties of Meta-SSD is analyzed. This paper can serve as a strong baseline and provide some inspiration for meta-learning FSD.",
            "referenceCount": 44,
            "citationCount": 42,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/8600701/08735792.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-06-12",
            "journal": {
                "name": "IEEE Access",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Fu2019MetaSSDTF,\n author = {K. Fu and Tengfei Zhang and Yue Zhang and M. Yan and Z. Chang and Zhengyuan Zhang and Xian Sun},\n booktitle = {IEEE Access},\n journal = {IEEE Access},\n pages = {77597-77606},\n title = {Meta-SSD: Towards Fast Adaptation for Few-Shot Object Detection With Meta-Learning},\n volume = {7},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6ac5eb309dd937d801d180c830377e4d551699a2",
            "@type": "ScholarlyArticle",
            "paperId": "6ac5eb309dd937d801d180c830377e4d551699a2",
            "corpusId": 3503217,
            "url": "https://www.semanticscholar.org/paper/6ac5eb309dd937d801d180c830377e4d551699a2",
            "title": "Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2963887494",
                "ArXiv": "1710.03641",
                "DBLP": "journals/corr/abs-1710-03641",
                "CorpusId": 3503217
            },
            "abstract": "Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation strategies. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.",
            "referenceCount": 58,
            "citationCount": 314,
            "influentialCitationCount": 34,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-10-10",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1710.03641"
            },
            "citationStyles": {
                "bibtex": "@Article{Al-Shedivat2017ContinuousAV,\n author = {Maruan Al-Shedivat and Trapit Bansal and Yuri Burda and Ilya Sutskever and Igor Mordatch and P. Abbeel},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments},\n volume = {abs/1710.03641},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f6b9b6561235cd285d1f99e39d57ed420d271f85",
            "@type": "ScholarlyArticle",
            "paperId": "f6b9b6561235cd285d1f99e39d57ed420d271f85",
            "corpusId": 212644757,
            "url": "https://www.semanticscholar.org/paper/f6b9b6561235cd285d1f99e39d57ed420d271f85",
            "title": "A New Meta-Baseline for Few-Shot Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2003-04390",
                "MAG": "3012209922",
                "CorpusId": 212644757
            },
            "abstract": "Meta-learning has become a popular framework for few-shot learning in recent years, with the goal of learning a model from collections of few-shot classification tasks. While more and more novel meta-learning models are being proposed, our research has uncovered simple baselines that have been overlooked. We present a Meta-Baseline method, by pre-training a classifier on all base classes and meta-learning on a nearest-centroid based few-shot classification algorithm, it outperforms recent state-of-the-art methods by a large margin. Why does this simple method work so well? In the meta-learning stage, we observe that a model generalizing better on unseen tasks from base classes can have a decreasing performance on tasks from novel classes, indicating a potential objective discrepancy. We find both pre-training and inheriting a good few-shot classification metric from the pre-trained classifier are important for Meta-Baseline, which potentially helps the model better utilize the pre-trained representations with stronger transferability. Furthermore, we investigate when we need meta-learning in this Meta-Baseline. Our work sets up a new solid benchmark for this field and sheds light on further understanding the phenomenons in the meta-learning framework for few-shot learning.",
            "referenceCount": 33,
            "citationCount": 192,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-03-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2003.04390"
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2020ANM,\n author = {Yinbo Chen and Xiaolong Wang and Zhuang Liu and Huijuan Xu and Trevor Darrell},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A New Meta-Baseline for Few-Shot Learning},\n volume = {abs/2003.04390},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f4ba3aabd5cc4f021f2b316ab7f8d46635d1018a",
            "@type": "ScholarlyArticle",
            "paperId": "f4ba3aabd5cc4f021f2b316ab7f8d46635d1018a",
            "corpusId": 43949877,
            "url": "https://www.semanticscholar.org/paper/f4ba3aabd5cc4f021f2b316ab7f8d46635d1018a",
            "title": "Been There, Done That: Meta-Learning with Episodic Recall",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963281697",
                "ArXiv": "1805.09692",
                "DBLP": "conf/icml/RitterWKJBPB18",
                "CorpusId": 43949877
            },
            "abstract": "Meta-learning agents excel at rapidly learning new tasks from open-ended task distributions; yet, they forget what they learn about each task as soon as the next begins. When tasks reoccur - as they do in natural environments - metalearning agents must explore again instead of immediately exploiting previously discovered solutions. We propose a formalism for generating open-ended yet repetitious environments, then develop a meta-learning architecture for solving these environments. This architecture melds the standard LSTM working memory with a differentiable neural episodic memory. We explore the capabilities of agents with this episodic LSTM in five meta-learning environments with reoccurring tasks, ranging from bandits to navigation and stochastic sequential decision problems.",
            "referenceCount": 32,
            "citationCount": 70,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-05-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ritter2018BeenTD,\n author = {S. Ritter and Jane X. Wang and Z. Kurth-Nelson and Siddhant M. Jayakumar and C. Blundell and Razvan Pascanu and M. Botvinick},\n booktitle = {International Conference on Machine Learning},\n pages = {4351-4360},\n title = {Been There, Done That: Meta-Learning with Episodic Recall},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:54da1705deeaa981e79630f0899816037c18ba2e",
            "@type": "ScholarlyArticle",
            "paperId": "54da1705deeaa981e79630f0899816037c18ba2e",
            "corpusId": 199502009,
            "url": "https://www.semanticscholar.org/paper/54da1705deeaa981e79630f0899816037c18ba2e",
            "title": "Optimizing quantum heuristics with meta-learning",
            "venue": "Quantum Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:582dec1d-d92f-4177-8627-3ca8d8063856",
                "name": "Quantum Machine Intelligence",
                "alternate_names": [
                    "Quantum Mach Intell"
                ],
                "issn": "2524-4906",
                "url": "https://www.springer.com/engineering/computational+intelligence+and+complexity/journal/42484"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/qmi/WilsonSWHTR21",
                "ArXiv": "1908.03185",
                "MAG": "2965090307",
                "DOI": "10.1007/s42484-020-00022-w",
                "CorpusId": 199502009
            },
            "abstract": null,
            "referenceCount": 81,
            "citationCount": 57,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s42484-020-00022-w.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-08-08",
            "journal": {
                "name": "Quantum Machine Intelligence",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Wilson2019OptimizingQH,\n author = {M. Wilson and Rachel Stromswold and F. Wudarski and Stuart Hadfield and N. Tubman and E. Rieffel},\n booktitle = {Quantum Machine Intelligence},\n journal = {Quantum Machine Intelligence},\n title = {Optimizing quantum heuristics with meta-learning},\n volume = {3},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:88df6fac6fa42a5425bb87cf12107d0b1dabd8a6",
            "@type": "ScholarlyArticle",
            "paperId": "88df6fac6fa42a5425bb87cf12107d0b1dabd8a6",
            "corpusId": 70072190,
            "url": "https://www.semanticscholar.org/paper/88df6fac6fa42a5425bb87cf12107d0b1dabd8a6",
            "title": "Meta\u2010learning how to forecast time series",
            "venue": "Journal of Forecasting",
            "publicationVenue": {
                "id": "urn:research:359c137d-519f-42d3-928c-2a5d4c0666e0",
                "name": "Journal of Forecasting",
                "alternate_names": [
                    "J Forecast"
                ],
                "issn": "0277-6693",
                "url": "http://www3.interscience.wiley.com/cgi-bin/jhome/2966"
            },
            "year": 2023,
            "externalIds": {
                "MAG": "2883806129",
                "DOI": "10.1002/for.2963",
                "CorpusId": 70072190
            },
            "abstract": "A crucial task in time series forecasting is the identification of the most suitable forecasting method. We present a general framework for forecast-model selection using meta-learning. A random forest is used to identify the best forecasting method using only time series features. The framework is evaluated using time series from the M1 and M3 competitions and is shown to yield accurate forecasts comparable to several benchmarks and other commonly used automated approaches of time series forecasting. A key advantage of our proposed framework is that the time-consuming process of building a classifier is handled in advance of the forecasting task at hand.",
            "referenceCount": 62,
            "citationCount": 63,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://figshare.com/articles/journal_contribution/Meta-learning_how_to_forecast_time_series/21522471/1/files/38151996.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2023-02-09",
            "journal": {
                "name": "Journal of Forecasting",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Talagala2023MetalearningHT,\n author = {T. Talagala and Rob J Hyndman and G. Athanasopoulos},\n booktitle = {Journal of Forecasting},\n journal = {Journal of Forecasting},\n title = {Meta\u2010learning how to forecast time series},\n year = {2023}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:eefede7052454b77b80d7695f29fa962c604986b",
            "@type": "ScholarlyArticle",
            "paperId": "eefede7052454b77b80d7695f29fa962c604986b",
            "corpusId": 72335401,
            "url": "https://www.semanticscholar.org/paper/eefede7052454b77b80d7695f29fa962c604986b",
            "title": "Doing more with less: meta-reasoning and meta-learning in humans and machines",
            "venue": "Current Opinion in Behavioral Sciences",
            "publicationVenue": {
                "id": "urn:research:68b547ba-0f44-4ad4-a095-b871d674a94f",
                "name": "Current Opinion in Behavioral Sciences",
                "alternate_names": [
                    "Curr Opin Behav Sci",
                    "Curr opin behav sci",
                    "Current opinion in behavioral sciences"
                ],
                "issn": "2352-1546",
                "url": "http://www.elsevier.com/locate/issn/23521546"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2929321571",
                "DOI": "10.1016/j.cobeha.2019.01.005",
                "CorpusId": 72335401
            },
            "abstract": null,
            "referenceCount": 48,
            "citationCount": 78,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2019-02-27",
            "journal": {
                "name": "Current Opinion in Behavioral Sciences",
                "volume": "29"
            },
            "citationStyles": {
                "bibtex": "@Article{Griffiths2019DoingMW,\n author = {T. Griffiths and Frederick Callaway and Michael Chang and Erin Grant and Falk Lieder},\n booktitle = {Current Opinion in Behavioral Sciences},\n journal = {Current Opinion in Behavioral Sciences},\n pages = {24-30},\n title = {Doing more with less: meta-reasoning and meta-learning in humans and machines},\n volume = {29},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:33e47bd1737c499360be783560bc1e1d705d9f52",
            "@type": "ScholarlyArticle",
            "paperId": "33e47bd1737c499360be783560bc1e1d705d9f52",
            "corpusId": 202787064,
            "url": "https://www.semanticscholar.org/paper/33e47bd1737c499360be783560bc1e1d705d9f52",
            "title": "Neural Relational Inference with Fast Modular Meta-learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2023,
            "externalIds": {
                "DBLP": "journals/corr/abs-2310-07015",
                "MAG": "2970433714",
                "ArXiv": "2310.07015",
                "DOI": "10.48550/arXiv.2310.07015",
                "CorpusId": 202787064
            },
            "abstract": "Graph neural networks (GNNs) are effective models for many dynamical systems consisting of entities and relations. Although most GNN applications assume a single type of entity and relation, many situations involve multiple types of interactions. Relational inference is the problem of inferring these interactions and learning the dynamics from observational data. We frame relational inference as a modular meta-learning problem, where neural modules are trained to be composed in different ways to solve many tasks. This meta-learning framework allows us to implicitly encode time invariance and infer relations in context of one another rather than independently, which increases inference capacity. Framing inference as the inner-loop optimization of meta-learning leads to a model-based approach that is more data-efficient and capable of estimating the state of entities that we do not observe directly, but whose existence can be inferred from their effect on observed entities. To address the large search space of graph neural network compositions, we meta-learn a proposal function that speeds up the inner-loop simulated annealing search within the modular meta-learning algorithm, providing two orders of magnitude increase in the size of problems that can be addressed.",
            "referenceCount": 61,
            "citationCount": 49,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.07015",
                "status": "CLOSED"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2023-10-11",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2310.07015"
            },
            "citationStyles": {
                "bibtex": "@Article{Alet2023NeuralRI,\n author = {Ferran Alet and Erica Weng and Tomas Lozano-Perez and L. Kaelbling},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Neural Relational Inference with Fast Modular Meta-learning},\n volume = {abs/2310.07015},\n year = {2023}\n}\n"
            }
        }
    }
]