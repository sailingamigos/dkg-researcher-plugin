[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
            "@type": "ScholarlyArticle",
            "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
            "corpusId": 6719686,
            "url": "https://www.semanticscholar.org/paper/c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
            "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2604763608",
                "DBLP": "journals/corr/FinnAL17",
                "ArXiv": "1703.03400",
                "CorpusId": 6719686
            },
            "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.",
            "referenceCount": 52,
            "citationCount": 8840,
            "influentialCitationCount": 2249,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Finn2017ModelAgnosticMF,\n author = {Chelsea Finn and P. Abbeel and S. Levine},\n booktitle = {International Conference on Machine Learning},\n pages = {1126-1135},\n title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
            "@type": "ScholarlyArticle",
            "paperId": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
            "corpusId": 215744839,
            "url": "https://www.semanticscholar.org/paper/020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
            "title": "Meta-Learning in Neural Networks: A Survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/pami/HospedalesAMS22",
                "MAG": "3015606043",
                "ArXiv": "2004.05439",
                "DOI": "10.1109/TPAMI.2021.3079209",
                "CorpusId": 215744839,
                "PubMed": "33974543"
            },
            "abstract": "The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.",
            "referenceCount": 332,
            "citationCount": 1184,
            "influentialCitationCount": 66,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/34/4359286/09428530.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-04-11",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "44"
            },
            "citationStyles": {
                "bibtex": "@Article{Hospedales2020MetaLearningIN,\n author = {Timothy M. Hospedales and Antreas Antoniou and P. Micaelli and A. Storkey},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {5149-5169},\n title = {Meta-Learning in Neural Networks: A Survey},\n volume = {44},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:90dc22818bd2d97d8deaff168b0137b75a962767",
            "@type": "ScholarlyArticle",
            "paperId": "90dc22818bd2d97d8deaff168b0137b75a962767",
            "corpusId": 4587331,
            "url": "https://www.semanticscholar.org/paper/90dc22818bd2d97d8deaff168b0137b75a962767",
            "title": "On First-Order Meta-Learning Algorithms",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2795900505",
                "ArXiv": "1803.02999",
                "DBLP": "journals/corr/abs-1803-02999",
                "CorpusId": 4587331
            },
            "abstract": "This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.",
            "referenceCount": 22,
            "citationCount": 1701,
            "influentialCitationCount": 249,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-03-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1803.02999"
            },
            "citationStyles": {
                "bibtex": "@Article{Nichol2018OnFM,\n author = {Alex Nichol and Joshua Achiam and J. Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {On First-Order Meta-Learning Algorithms},\n volume = {abs/1803.02999},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fc437af6204008647ea49f81058d5fdaddf75ead",
            "@type": "ScholarlyArticle",
            "paperId": "fc437af6204008647ea49f81058d5fdaddf75ead",
            "corpusId": 102351194,
            "url": "https://www.semanticscholar.org/paper/fc437af6204008647ea49f81058d5fdaddf75ead",
            "title": "Meta-Learning With Differentiable Convex Optimization",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2963070905",
                "DBLP": "journals/corr/abs-1904-03758",
                "ArXiv": "1904.03758",
                "DOI": "10.1109/CVPR.2019.01091",
                "CorpusId": 102351194
            },
            "abstract": "Many meta-learning approaches for few-shot learning rely on simple base learners such as nearest-neighbor classifiers. However, even in the few-shot regime, discriminatively trained linear predictors can offer better generalization. We propose to use these predictors as base learners to learn representations for few-shot learning and show they offer better tradeoffs between feature size and performance across a range of few-shot recognition benchmarks. Our objective is to learn feature embeddings that generalize well under a linear classification rule for novel categories. To efficiently solve the objective, we exploit two properties of linear classifiers: implicit differentiation of the optimality conditions of the convex problem and the dual formulation of the optimization problem. This allows us to use high-dimensional embeddings with improved generalization at a modest increase in computational overhead. Our approach, named MetaOptNet, achieves state-of-the-art performance on miniImageNet, tieredImageNet, CIFAR-FS, and FC100 few-shot learning benchmarks.",
            "referenceCount": 36,
            "citationCount": 1037,
            "influentialCitationCount": 200,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1904.03758",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-04-07",
            "journal": {
                "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lee2019MetaLearningWD,\n author = {Kwonjoon Lee and Subhransu Maji and Avinash Ravichandran and Stefano Soatto},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {10649-10657},\n title = {Meta-Learning With Differentiable Convex Optimization},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a2933c8d0e152264d1cd25ca8248b25d4b49038b",
            "@type": "ScholarlyArticle",
            "paperId": "a2933c8d0e152264d1cd25ca8248b25d4b49038b",
            "corpusId": 202542766,
            "url": "https://www.semanticscholar.org/paper/a2933c8d0e152264d1cd25ca8248b25d4b49038b",
            "title": "Meta-Learning with Implicit Gradients",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1909-04630",
                "MAG": "2970697704",
                "ArXiv": "1909.04630",
                "CorpusId": 202542766
            },
            "abstract": "A core capability of intelligent systems is the ability to quickly learn new tasks by drawing on prior experience. Gradient (or optimization) based meta-learning has recently emerged as an effective approach for few-shot learning. In this formulation, meta-parameters are learned in the outer loop, while task-specific models are learned in the inner-loop, by using only a small amount of data from the current task. A key challenge in scaling these approaches is the need to differentiate through the inner loop learning process, which can impose considerable computational and memory burdens. By drawing upon implicit differentiation, we develop the implicit MAML algorithm, which depends only on the solution to the inner level optimization and not the path taken by the inner loop optimizer. This effectively decouples the meta-gradient computation from the choice of inner loop optimizer. As a result, our approach is agnostic to the choice of inner loop optimizer and can gracefully handle many gradient steps without vanishing gradients or memory constraints. Theoretically, we prove that implicit MAML can compute accurate meta-gradients with a memory footprint that is, up to small constant factors, no more than that which is required to compute a single inner loop gradient and at no overall increase in the total computational cost. Experimentally, we show that these benefits of implicit MAML translate into empirical gains on few-shot image recognition benchmarks.",
            "referenceCount": 63,
            "citationCount": 617,
            "influentialCitationCount": 78,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-10",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rajeswaran2019MetaLearningWI,\n author = {A. Rajeswaran and Chelsea Finn and S. Kakade and S. Levine},\n booktitle = {Neural Information Processing Systems},\n pages = {113-124},\n title = {Meta-Learning with Implicit Gradients},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7f11f1d26f716e7252d6ce967e95dee9db9d3316",
            "@type": "ScholarlyArticle",
            "paperId": "7f11f1d26f716e7252d6ce967e95dee9db9d3316",
            "corpusId": 246828747,
            "url": "https://www.semanticscholar.org/paper/7f11f1d26f716e7252d6ce967e95dee9db9d3316",
            "title": "Contrastive Meta Learning with Behavior Multiplicity for Recommendation",
            "venue": "Web Search and Data Mining",
            "publicationVenue": {
                "id": "urn:research:ea38228f-6ed3-4222-a3ce-d963d8cc9516",
                "name": "Web Search and Data Mining",
                "alternate_names": [
                    "Web Search Data Min",
                    "WSDM"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=3158"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "journals/corr/abs-2202-08523",
                "ArXiv": "2202.08523",
                "DOI": "10.1145/3488560.3498527",
                "CorpusId": 246828747
            },
            "abstract": "A well-informed recommendation framework could not only help users identify their interested items, but also benefit the revenue of various online platforms (e.g., e-commerce, social media). Traditional recommendation models usually assume that only a single type of interaction exists between user and item, and fail to model the multiplex user-item relationships from multi-typed user behavior data, such as page view, add-to-favourite and purchase. While some recent studies propose to capture the dependencies across different types of behaviors, two important challenges have been less explored: i) Dealing with the sparse supervision signal under target behaviors (e.g., purchase). ii) Capturing the personalized multi-behavior patterns with customized dependency modeling. To tackle the above challenges, we devise a new model CML, Contrastive Meta Learning (CML), to maintain dedicated cross-type behavior dependency for different users. In particular, we propose a multi-behavior contrastive learning framework to distill transferable knowledge across different types of behaviors via the constructed contrastive loss. In addition, to capture the diverse multi-behavior patterns, we design a contrastive meta network to encode the customized behavior heterogeneity for different users. Extensive experiments on three real-world datasets indicate that our method consistently outperforms various state-of-the-art recommendation methods. Our empirical studies further suggest that the contrastive meta learning paradigm offers great potential for capturing the behavior multiplicity in recommendation. We release our model implementation at: https://github.com/weiwei1206/CML.git.",
            "referenceCount": 58,
            "citationCount": 67,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2202.08523",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book"
            ],
            "publicationDate": "2022-02-11",
            "journal": {
                "name": "Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wei2022ContrastiveML,\n author = {Wei Wei and Chao Huang and Lianghao Xia and Yong Xu and Jiashu Zhao and Dawei Yin},\n booktitle = {Web Search and Data Mining},\n journal = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},\n title = {Contrastive Meta Learning with Behavior Multiplicity for Recommendation},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:df093d69cd98cf4b26542f53614a79754754eb78",
            "@type": "ScholarlyArticle",
            "paperId": "df093d69cd98cf4b26542f53614a79754754eb78",
            "corpusId": 3507990,
            "url": "https://www.semanticscholar.org/paper/df093d69cd98cf4b26542f53614a79754754eb78",
            "title": "Meta-Learning for Semi-Supervised Few-Shot Classification",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1803.00676",
                "MAG": "2952417504",
                "DBLP": "conf/iclr/RenTRSSTLZ18",
                "CorpusId": 3507990
            },
            "abstract": "In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode. We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided. To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples. We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure. Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would.",
            "referenceCount": 26,
            "citationCount": 1059,
            "influentialCitationCount": 123,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1803.00676"
            },
            "citationStyles": {
                "bibtex": "@Article{Ren2018MetaLearningFS,\n author = {Mengye Ren and Eleni Triantafillou and S. Ravi and Jake Snell and Kevin Swersky and J. Tenenbaum and H. Larochelle and R. Zemel},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Meta-Learning for Semi-Supervised Few-Shot Classification},\n volume = {abs/1803.00676},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:04f739a0c29b75877243731aeead512bf0ed1dff",
            "@type": "ScholarlyArticle",
            "paperId": "04f739a0c29b75877243731aeead512bf0ed1dff",
            "corpusId": 49868626,
            "url": "https://www.semanticscholar.org/paper/04f739a0c29b75877243731aeead512bf0ed1dff",
            "title": "Meta-Learning with Latent Embedding Optimization",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2884901161",
                "DBLP": "journals/corr/abs-1807-05960",
                "ArXiv": "1807.05960",
                "CorpusId": 49868626
            },
            "abstract": "Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this low-dimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space.",
            "referenceCount": 63,
            "citationCount": 1161,
            "influentialCitationCount": 116,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-07-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1807.05960"
            },
            "citationStyles": {
                "bibtex": "@Article{Rusu2018MetaLearningWL,\n author = {Andrei A. Rusu and Dushyant Rao and Jakub Sygnowski and Oriol Vinyals and Razvan Pascanu and Simon Osindero and R. Hadsell},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Meta-Learning with Latent Embedding Optimization},\n volume = {abs/1807.05960},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b1ec347c64d06fa17cd25e6e8b4ed4c908f0fb0b",
            "@type": "ScholarlyArticle",
            "paperId": "b1ec347c64d06fa17cd25e6e8b4ed4c908f0fb0b",
            "corpusId": 250340974,
            "url": "https://www.semanticscholar.org/paper/b1ec347c64d06fa17cd25e6e8b4ed4c908f0fb0b",
            "title": "Transformer Neural Processes: Uncertainty-Aware Meta Learning Via Sequence Modeling",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2022,
            "externalIds": {
                "ArXiv": "2207.04179",
                "DBLP": "journals/corr/abs-2207-04179",
                "DOI": "10.48550/arXiv.2207.04179",
                "CorpusId": 250340974
            },
            "abstract": "Neural Processes (NPs) are a popular class of approaches for meta-learning. Similar to Gaussian Processes (GPs), NPs define distributions over functions and can estimate uncertainty in their predictions. However, unlike GPs, NPs and their variants suffer from underfitting and often have intractable likelihoods, which limit their applications in sequential decision making. We propose Transformer Neural Processes (TNPs), a new member of the NP family that casts uncertainty-aware meta learning as a sequence modeling problem. We learn TNPs via an autoregressive likelihood-based objective and instantiate it with a novel transformer-based architecture. The model architecture respects the inductive biases inherent to the problem structure, such as invariance to the observed data points and equivariance to the unobserved points. We further investigate knobs within the TNP framework that tradeoff expressivity of the decoding distribution with extra computation. Empirically, we show that TNPs achieve state-of-the-art performance on various benchmark problems, outperforming all previous NP variants on meta regression, image completion, contextual multi-armed bandits, and Bayesian optimization.",
            "referenceCount": 48,
            "citationCount": 38,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2207.04179",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2022-07-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2207.04179"
            },
            "citationStyles": {
                "bibtex": "@Article{Nguyen2022TransformerNP,\n author = {Tung Nguyen and Aditya Grover},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Transformer Neural Processes: Uncertainty-Aware Meta Learning Via Sequence Modeling},\n volume = {abs/2207.04179},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0aa3e93b88b879f2c21157c0ee2aa85ac9111138",
            "@type": "ScholarlyArticle",
            "paperId": "0aa3e93b88b879f2c21157c0ee2aa85ac9111138",
            "corpusId": 247720950,
            "url": "https://www.semanticscholar.org/paper/0aa3e93b88b879f2c21157c0ee2aa85ac9111138",
            "title": "Robust Spike-Based Continual Meta-Learning Improved by Restricted Minimum Error Entropy Criterion",
            "venue": "Entropy",
            "publicationVenue": {
                "id": "urn:research:8270cfe1-3713-4325-a7bd-c6a87eed889e",
                "name": "Entropy",
                "alternate_names": null,
                "issn": "1099-4300",
                "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-155606"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "journals/entropy/YangTC22",
                "PubMedCentral": "9031894",
                "DOI": "10.3390/e24040455",
                "CorpusId": 247720950,
                "PubMed": "35455118"
            },
            "abstract": "The spiking neural network (SNN) is regarded as a promising candidate to deal with the great challenges presented by current machine learning techniques, including the high energy consumption induced by deep neural networks. However, there is still a great gap between SNNs and the online meta-learning performance of artificial neural networks. Importantly, existing spike-based online meta-learning models do not target the robust learning based on spatio-temporal dynamics and superior machine learning theory. In this invited article, we propose a novel spike-based framework with minimum error entropy, called MeMEE, using the entropy theory to establish the gradient-based online meta-learning scheme in a recurrent SNN architecture. We examine the performance based on various types of tasks, including autonomous navigation and the working memory test. The experimental results show that the proposed MeMEE model can effectively improve the accuracy and the robustness of the spike-based meta-learning performance. More importantly, the proposed MeMEE model emphasizes the application of the modern information theoretic learning approach on the state-of-the-art spike-based learning algorithms. Therefore, in this invited paper, we provide new perspectives for further integration of advanced information theory in machine learning to improve the learning performance of SNNs, which could be of great merit to applied developments with spike-based neuromorphic systems.",
            "referenceCount": 44,
            "citationCount": 79,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/1099-4300/24/4/455/pdf?version=1648210856",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-03-25",
            "journal": {
                "name": "Entropy",
                "volume": "24"
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2022RobustSC,\n author = {Shuangming Yang and Jiangtong Tan and Badong Chen},\n booktitle = {Entropy},\n journal = {Entropy},\n title = {Robust Spike-Based Continual Meta-Learning Improved by Restricted Minimum Error Entropy Criterion},\n volume = {24},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0ed08d48117947da06725dc95dfb7c22d5d3d7e4",
            "@type": "ScholarlyArticle",
            "paperId": "0ed08d48117947da06725dc95dfb7c22d5d3d7e4",
            "corpusId": 249461664,
            "url": "https://www.semanticscholar.org/paper/0ed08d48117947da06725dc95dfb7c22d5d3d7e4",
            "title": "Sharp-MAML: Sharpness-Aware Model-Agnostic Meta Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "conf/icml/AbbasXCCC22",
                "ArXiv": "2206.03996",
                "DOI": "10.48550/arXiv.2206.03996",
                "CorpusId": 249461664
            },
            "abstract": "Model-agnostic meta learning (MAML) is currently one of the dominating approaches for few-shot meta-learning. Albeit its effectiveness, the optimization of MAML can be challenging due to the innate bilevel problem structure. Specifically, the loss landscape of MAML is much more complex with possibly more saddle points and local minimizers than its empirical risk minimization counterpart. To address this challenge, we leverage the recently invented sharpness-aware minimization and develop a sharpness-aware MAML approach that we term Sharp-MAML. We empirically demonstrate that Sharp-MAML and its computation-efficient variant can outperform the plain-vanilla MAML baseline (e.g., $+3\\%$ accuracy on Mini-Imagenet). We complement the empirical study with the convergence rate analysis and the generalization bound of Sharp-MAML. To the best of our knowledge, this is the first empirical and theoretical study on sharpness-aware minimization in the context of bilevel learning. The code is available at https://github.com/mominabbass/Sharp-MAML.",
            "referenceCount": 74,
            "citationCount": 38,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2206.03996",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2022-06-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Abbas2022SharpMAMLSM,\n author = {Momin Abbas and Quan-Wu Xiao and Lisha Chen and Pin-Yu Chen and Tianyi Chen},\n booktitle = {International Conference on Machine Learning},\n pages = {10-32},\n title = {Sharp-MAML: Sharpness-Aware Model-Agnostic Meta Learning},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:93fdf5cf598aefb0335f001039e83494dc721c3a",
            "@type": "ScholarlyArticle",
            "paperId": "93fdf5cf598aefb0335f001039e83494dc721c3a",
            "corpusId": 254408503,
            "url": "https://www.semanticscholar.org/paper/93fdf5cf598aefb0335f001039e83494dc721c3a",
            "title": "General-Purpose In-Context Learning by Meta-Learning Transformers",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2022,
            "externalIds": {
                "ArXiv": "2212.04458",
                "DBLP": "journals/corr/abs-2212-04458",
                "DOI": "10.48550/arXiv.2212.04458",
                "CorpusId": 254408503
            },
            "abstract": "Modern machine learning requires system designers to specify aspects of the learning pipeline, such as losses, architectures, and optimizers. Meta-learning, or learning-to-learn, instead aims to learn those aspects, and promises to unlock greater capabilities with less manual effort. One particularly ambitious goal of meta-learning is to train general-purpose in-context learning algorithms from scratch, using only black-box models with minimal inductive bias. Such a model takes in training data, and produces test-set predictions across a wide range of problems, without any explicit definition of an inference model, training loss, or optimization algorithm. In this paper we show that Transformers and other black-box models can be meta-trained to act as general-purpose in-context learners. We characterize phase transitions between algorithms that generalize, algorithms that memorize, and algorithms that fail to meta-train at all, induced by changes in model size, number of tasks, and meta-optimization. We further show that the capabilities of meta-trained algorithms are bottlenecked by the accessible state size (memory) determining the next prediction, unlike standard models which are thought to be bottlenecked by parameter count. Finally, we propose practical interventions such as biasing the training distribution that improve the meta-training and meta-generalization of general-purpose learning algorithms.",
            "referenceCount": 64,
            "citationCount": 31,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2212.04458",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-12-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2212.04458"
            },
            "citationStyles": {
                "bibtex": "@Article{Kirsch2022GeneralPurposeIL,\n author = {Louis Kirsch and James Harrison and Jascha Narain Sohl-Dickstein and Luke Metz},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {General-Purpose In-Context Learning by Meta-Learning Transformers},\n volume = {abs/2212.04458},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a5643269e2697cbb2ea70b01a03aaf582dd58931",
            "@type": "ScholarlyArticle",
            "paperId": "a5643269e2697cbb2ea70b01a03aaf582dd58931",
            "corpusId": 247887128,
            "url": "https://www.semanticscholar.org/paper/a5643269e2697cbb2ea70b01a03aaf582dd58931",
            "title": "Progressive Meta-Learning With Curriculum",
            "venue": "IEEE transactions on circuits and systems for video technology (Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "journals/tcsv/ZhangSGLS22",
                "DOI": "10.1109/TCSVT.2022.3164190",
                "CorpusId": 247887128
            },
            "abstract": "Meta-learning offers an effective solution to learn new concepts under scarce supervision through an episodic-training scheme: a series of target-like tasks sampled from base classes are sequentially fed into a meta-learner to extract cross-task knowledge, which can facilitate the quick acquisition of task-specific knowledge of the target task with few samples. Despite its noticeable improvements, the episodic-training strategy samples tasks randomly and uniformly, without considering their hardness and quality, which may not progressively improve the meta-leaner\u2019s generalization. In this paper, we propose Progressive Meta-learning using tasks from easy to hard. First, based on a predefined curriculum, we develop a Curriculum-Based Meta-learning (CubMeta) method. CubMeta is in a stepwise manner, and in each step, we design a BrotherNet module to establish harder tasks and an effective learning scheme for obtaining an ensemble of stronger meta-learners. Then we move a step further to propose an end-to-end Self-Paced Meta-learning (SepMeta) method. The curriculum in SepMeta is effectively integrated as a regularization term into the objective so that the meta-learner can measure the hardness of tasks adaptively, according to what the model has already learned. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed methods. Our code is available at https://github.com/nobody-777.",
            "referenceCount": 56,
            "citationCount": 35,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-09-01",
            "journal": {
                "name": "IEEE Transactions on Circuits and Systems for Video Technology",
                "volume": "32"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2022ProgressiveMW,\n author = {Ji Zhang and Jingkuan Song and Lianli Gao and Ye Liu and Hengtao Shen},\n booktitle = {IEEE transactions on circuits and systems for video technology (Print)},\n journal = {IEEE Transactions on Circuits and Systems for Video Technology},\n pages = {5916-5930},\n title = {Progressive Meta-Learning With Curriculum},\n volume = {32},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9a8a475bb75929c3b4934278cbb7bb4af8546d1e",
            "@type": "ScholarlyArticle",
            "paperId": "9a8a475bb75929c3b4934278cbb7bb4af8546d1e",
            "corpusId": 52938664,
            "url": "https://www.semanticscholar.org/paper/9a8a475bb75929c3b4934278cbb7bb4af8546d1e",
            "title": "Meta-Learning",
            "venue": "Automated Machine Learning",
            "publicationVenue": {
                "id": "urn:research:a688800a-ab93-4531-9258-c10e1b6ddf68",
                "name": "Automated Machine Learning",
                "alternate_names": [
                    "Automation and Machine Learning",
                    "Autom Mach Learn"
                ],
                "issn": "2520-131X",
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "books/sp/19/Vanschoren19",
                "DOI": "10.1007/978-3-030-05318-5_2",
                "CorpusId": 52938664
            },
            "abstract": null,
            "referenceCount": 143,
            "citationCount": 605,
            "influentialCitationCount": 43,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-05318-5_2.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Vanschoren2019MetaLearning,\n author = {Joaquin Vanschoren},\n booktitle = {Automated Machine Learning},\n pages = {35-61},\n title = {Meta-Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f56ae74c8c0842064654aeecb93d42bb3456b5b6",
            "@type": "ScholarlyArticle",
            "paperId": "f56ae74c8c0842064654aeecb93d42bb3456b5b6",
            "corpusId": 227276412,
            "url": "https://www.semanticscholar.org/paper/f56ae74c8c0842064654aeecb93d42bb3456b5b6",
            "title": "Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3099314130",
                "DBLP": "conf/nips/0001MO20",
                "CorpusId": 227276412
            },
            "abstract": "In Federated Learning, we aim to train models across multiple computing units (users), while users can only communicate with a common central server, without exchanging their data samples. This mechanism exploits the computational power of all users and allows users to obtain a richer model as their models are trained over a larger set of data points. However, this scheme only develops a common output for all the users, and, therefore, it does not adapt the model to each user. This is an important missing feature, especially given the heterogeneity of the underlying data distribution for various users. In this paper, we study a personalized variant of the federated learning in which our goal is to \ufb01nd an initial shared model that current or new users can easily adapt to their local dataset by performing one or a few steps of gradient descent with respect to their own data. This approach keeps all the bene\ufb01ts of the federated learning architecture, and, by structure, leads to a more personalized model for each user. We show this problem can be studied within the Model-Agnostic Meta-Learning (MAML) framework. Inspired by this connection, we study a personalized variant of the well-known Federated Averaging algorithm and evaluate its performance in terms of gradient norm for non-convex loss functions. Further, we characterize how this performance is affected by the closeness of underlying distributions of user data, measured in terms of distribution distances such as Total Variation and 1-Wasserstein metric.",
            "referenceCount": 47,
            "citationCount": 454,
            "influentialCitationCount": 70,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": "33"
            },
            "citationStyles": {
                "bibtex": "@Article{Fallah2020PersonalizedFL,\n author = {Alireza Fallah and Aryan Mokhtari and A. Ozdaglar},\n booktitle = {Neural Information Processing Systems},\n title = {Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach},\n volume = {33},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:43b7437ed33a29d3d90239ad66f325a465ff7e91",
            "@type": "ScholarlyArticle",
            "paperId": "43b7437ed33a29d3d90239ad66f325a465ff7e91",
            "corpusId": 248505738,
            "url": "https://www.semanticscholar.org/paper/43b7437ed33a29d3d90239ad66f325a465ff7e91",
            "title": "Meta Learning for Natural Language Processing: A Survey",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:01103732-3808-4930-b8e4-7e9e68d5c68d",
                "name": "North American Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "North Am Chapter Assoc Comput Linguistics",
                    "NAACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/naacl"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "conf/naacl/Lee0V22",
                "ACL": "2022.naacl-main.49",
                "ArXiv": "2205.01500",
                "DOI": "10.48550/arXiv.2205.01500",
                "CorpusId": 248505738
            },
            "abstract": "Deep learning has been the mainstream technique in the natural language processing (NLP) area. However, deep learning requires many labeled data and is less generalizable across domains. Meta-learning is an arising field in machine learning. It studies approaches to learning better learning algorithms and aims to improve algorithms in various aspects, including data efficiency and generalizability. The efficacy of meta-learning has been shown in many NLP tasks, but there is no systematic survey of these approaches in NLP, which hinders more researchers from joining the field. Our goal with this survey paper is to offer researchers pointers to relevant meta-learning works in NLP and attract more attention from the NLP community to drive future innovation. This paper first introduces the general concepts of meta-learning and the common approaches. Then we summarize task construction settings, applications of meta-learning for various NLP problems and review the development of meta-learning in the NLP community.",
            "referenceCount": 164,
            "citationCount": 25,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2205.01500",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2022-05-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lee2022MetaLF,\n author = {Hung-yi Lee and Shang-Wen Li and Ngoc Thang Vu},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {666-684},\n title = {Meta Learning for Natural Language Processing: A Survey},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:208cd4b25768f0096fb2e80e7690473da0e2a563",
            "@type": "ScholarlyArticle",
            "paperId": "208cd4b25768f0096fb2e80e7690473da0e2a563",
            "corpusId": 29153681,
            "url": "https://www.semanticscholar.org/paper/208cd4b25768f0096fb2e80e7690473da0e2a563",
            "title": "Meta-learning with differentiable closed-form solvers",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2952292344",
                "ArXiv": "1805.08136",
                "DBLP": "journals/corr/abs-1805-08136",
                "CorpusId": 29153681
            },
            "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures. Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent. Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently. In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning. The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data. This requires back-propagating errors through the solver steps. While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage. We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components. Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.",
            "referenceCount": 72,
            "citationCount": 752,
            "influentialCitationCount": 191,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-05-21",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1805.08136"
            },
            "citationStyles": {
                "bibtex": "@Article{Bertinetto2018MetalearningWD,\n author = {Luca Bertinetto and Jo\u00e3o F. Henriques and Philip H. S. Torr and A. Vedaldi},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Meta-learning with differentiable closed-form solvers},\n volume = {abs/1805.08136},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bd3df472bc848083068a76e9ce2b2ab49543dc78",
            "@type": "ScholarlyArticle",
            "paperId": "bd3df472bc848083068a76e9ce2b2ab49543dc78",
            "corpusId": 85498737,
            "url": "https://www.semanticscholar.org/paper/bd3df472bc848083068a76e9ce2b2ab49543dc78",
            "title": "MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2924888702",
                "DBLP": "journals/corr/abs-1903-10258",
                "ArXiv": "1903.10258",
                "DOI": "10.1109/ICCV.2019.00339",
                "CorpusId": 85498737
            },
            "abstract": "In this paper, we propose a novel meta learning approach for automatic channel pruning of very deep neural networks. We first train a PruningNet, a kind of meta network, which is able to generate weight parameters for any pruned structure given the target network. We use a simple stochastic structure sampling method for training the PruningNet. Then, we apply an evolutionary procedure to search for good-performing pruned networks. The search is highly efficient because the weights are directly generated by the trained PruningNet and we do not need any finetuning at search time. With a single PruningNet trained for the target network, we can search for various Pruned Networks under different constraints with little human participation. Compared to the state-of-the-art pruning methods, we have demonstrated superior performances on MobileNet V1/V2 and ResNet. Codes are available on https://github.com/liuzechun/MetaPruning.",
            "referenceCount": 62,
            "citationCount": 430,
            "influentialCitationCount": 66,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1903.10258",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-03-25",
            "journal": {
                "name": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2019MetaPruningML,\n author = {Zechun Liu and Haoyuan Mu and Xiangyu Zhang and Zichao Guo and Xin Yang and K. Cheng and Jian Sun},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {3295-3304},\n title = {MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6f5b1076ebacd30849d86e5f5787e3d43b65911f",
            "@type": "ScholarlyArticle",
            "paperId": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
            "corpusId": 67855601,
            "url": "https://www.semanticscholar.org/paper/6f5b1076ebacd30849d86e5f5787e3d43b65911f",
            "title": "Adversarial Attacks on Graph Neural Networks via Meta Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/iclr/ZugnerG19",
                "MAG": "2952132303",
                "ArXiv": "1902.08412",
                "CorpusId": 67855601
            },
            "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.",
            "referenceCount": 34,
            "citationCount": 416,
            "influentialCitationCount": 105,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-02-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1902.08412"
            },
            "citationStyles": {
                "bibtex": "@Article{Z\u00fcgner2019AdversarialAO,\n author = {Daniel Z\u00fcgner and Stephan G\u00fcnnemann},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Adversarial Attacks on Graph Neural Networks via Meta Learning},\n volume = {abs/1902.08412},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:766328cea625644d028d2e87adc8aaf3038450ad",
            "@type": "ScholarlyArticle",
            "paperId": "766328cea625644d028d2e87adc8aaf3038450ad",
            "corpusId": 211171538,
            "url": "https://www.semanticscholar.org/paper/766328cea625644d028d2e87adc8aaf3038450ad",
            "title": "Personalized Federated Learning: A Meta-Learning Approach",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2002-07948",
                "ArXiv": "2002.07948",
                "MAG": "3007345209",
                "CorpusId": 211171538
            },
            "abstract": "In Federated Learning, we aim to train models across multiple computing units (users), while users can only communicate with a common central server, without exchanging their data samples. This mechanism exploits the computational power of all users and allows users to obtain a richer model as their models are trained over a larger set of data points. However, this scheme only develops a common output for all the users, and, therefore, it does not adapt the model to each user. This is an important missing feature, especially given the heterogeneity of the underlying data distribution for various users. In this paper, we study a personalized variant of the federated learning in which our goal is to find an initial shared model that current or new users can easily adapt to their local dataset by performing one or a few steps of gradient descent with respect to their own data. This approach keeps all the benefits of the federated learning architecture, and, by structure, leads to a more personalized model for each user. We show this problem can be studied within the Model-Agnostic Meta-Learning (MAML) framework. Inspired by this connection, we study a personalized variant of the well-known Federated Averaging algorithm and evaluate its performance in terms of gradient norm for non-convex loss functions. Further, we characterize how this performance is affected by the closeness of underlying distributions of user data, measured in terms of distribution distances such as Total Variation and 1-Wasserstein metric.",
            "referenceCount": 50,
            "citationCount": 390,
            "influentialCitationCount": 53,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-02-19",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2002.07948"
            },
            "citationStyles": {
                "bibtex": "@Article{Fallah2020PersonalizedFL,\n author = {Alireza Fallah and Aryan Mokhtari and A. Ozdaglar},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Personalized Federated Learning: A Meta-Learning Approach},\n volume = {abs/2002.07948},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b39b45a59c27a0cb3214d5a84547f54722d40c69",
            "@type": "ScholarlyArticle",
            "paperId": "b39b45a59c27a0cb3214d5a84547f54722d40c69",
            "corpusId": 1883787,
            "url": "https://www.semanticscholar.org/paper/b39b45a59c27a0cb3214d5a84547f54722d40c69",
            "title": "Learning to Generalize: Meta-Learning for Domain Generalization",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2963043696",
                "DBLP": "journals/corr/abs-1710-03463",
                "ArXiv": "1710.03463",
                "DOI": "10.1609/aaai.v32i1.11596",
                "CorpusId": 1883787
            },
            "abstract": "\n \n Domain shift refers to the well known problem that a model trained in one source domain performs poorly when appliedto a target domain with different statistics. Domain Generalization (DG) techniques attempt to alleviate this issue by producing models which by design generalize well to novel testing domains. We propose a novel meta-learning method for domain generalization. Rather than designing a specific model that is robust to domain shift as in most previous DG work, we propose a model agnostic training procedure for DG. Our algorithm simulates train/test domain shift during training by synthesizing virtual testing domains within each mini-batch. The meta-optimization objective requires that steps to improve training domain performance should also improve testing domain performance. This meta-learning procedure trains models with good generalization ability to novel domains. We evaluate our method and achieve state of the art results on a recent cross-domain image classification benchmark, as well demonstrating its potential on two classic reinforcement learning tasks.\n \n",
            "referenceCount": 40,
            "citationCount": 930,
            "influentialCitationCount": 134,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11596/11455",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-10-10",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Li2017LearningTG,\n author = {Da Li and Yongxin Yang and Yi-Zhe Song and Timothy M. Hospedales},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {3490-3497},\n title = {Learning to Generalize: Meta-Learning for Domain Generalization},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:38e2f851b705faa0d0a698ed9885bd6834440073",
            "@type": "ScholarlyArticle",
            "paperId": "38e2f851b705faa0d0a698ed9885bd6834440073",
            "corpusId": 46977722,
            "url": "https://www.semanticscholar.org/paper/38e2f851b705faa0d0a698ed9885bd6834440073",
            "title": "Probabilistic Model-Agnostic Meta-Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2807352135",
                "DBLP": "journals/corr/abs-1806-02817",
                "ArXiv": "1806.02817",
                "CorpusId": 46977722
            },
            "abstract": "Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems. We also show how reasoning about ambiguity can also be used for downstream active learning problems.",
            "referenceCount": 43,
            "citationCount": 582,
            "influentialCitationCount": 60,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.02817"
            },
            "citationStyles": {
                "bibtex": "@Article{Finn2018ProbabilisticMM,\n author = {Chelsea Finn and Kelvin Xu and S. Levine},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Probabilistic Model-Agnostic Meta-Learning},\n volume = {abs/1806.02817},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:15561ab20c298e113b0008b7a029486a422e7ca3",
            "@type": "ScholarlyArticle",
            "paperId": "15561ab20c298e113b0008b7a029486a422e7ca3",
            "corpusId": 49194806,
            "url": "https://www.semanticscholar.org/paper/15561ab20c298e113b0008b7a029486a422e7ca3",
            "title": "Bilevel Programming for Hyperparameter Optimization and Meta-Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2949709344",
                "DBLP": "journals/corr/abs-1806-04910",
                "ArXiv": "1806.04910",
                "CorpusId": 49194806
            },
            "abstract": "We introduce a framework based on bilevel programming that unifies gradient-based hyperparameter optimization and meta-learning. We show that an approximate version of the bilevel problem can be solved by taking into explicit account the optimization dynamics for the inner objective. Depending on the specific setting, the outer variables take either the meaning of hyperparameters in a supervised learning problem or parameters of a meta-learner. We provide sufficient conditions under which solutions of the approximate problem converge to those of the exact problem. We instantiate our approach for meta-learning in the case of deep learning where representation layers are treated as hyperparameters shared across a set of training episodes. In experiments, we confirm our theoretical findings, present encouraging results for few-shot learning and contrast the bilevel approach against classical approaches for learning-to-learn.",
            "referenceCount": 47,
            "citationCount": 554,
            "influentialCitationCount": 80,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-06-13",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Franceschi2018BilevelPF,\n author = {Luca Franceschi and P. Frasconi and Saverio Salzo and Riccardo Grazzi and M. Pontil},\n booktitle = {International Conference on Machine Learning},\n pages = {1563-1572},\n title = {Bilevel Programming for Hyperparameter Optimization and Meta-Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:763eb8707c26347c259f6523c22e4590fec69306",
            "@type": "ScholarlyArticle",
            "paperId": "763eb8707c26347c259f6523c22e4590fec69306",
            "corpusId": 233181672,
            "url": "https://www.semanticscholar.org/paper/763eb8707c26347c259f6523c22e4590fec69306",
            "title": "Open Domain Generalization with Domain-Augmented Meta-Learning",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2104.03620",
                "DBLP": "conf/cvpr/ShuCW0L21",
                "DOI": "10.1109/CVPR46437.2021.00950",
                "CorpusId": 233181672
            },
            "abstract": "Leveraging datasets available to learn a model with high generalization ability to unseen domains is important for computer vision, especially when the unseen domain\u2019s annotated data are unavailable. We study a novel and practical problem of Open Domain Generalization (OpenDG), which learns from different source domains to achieve high performance on an unknown target domain, where the distributions and label sets of each individual source domain and the target domain can be different. The problem can be generally applied to diverse source domains and widely applicable to real-world applications. We propose a Domain-Augmented Meta-Learning framework to learn open-domain generalizable representations. We augment domains on both feature-level by a new Dirichlet mixup and label-level by distilled soft-labeling, which complements each domain with missing classes and other domain knowledge. We conduct meta-learning over domains by designing new meta-learning tasks and losses to preserve domain unique knowledge and generalize knowledge across domains simultaneously. Experiment results on various multi-domain datasets demonstrate that the proposed Domain-Augmented Meta-Learning (DAML) outperforms prior methods for unseen domain recognition.",
            "referenceCount": 56,
            "citationCount": 83,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2104.03620",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-04-08",
            "journal": {
                "name": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Shu2021OpenDG,\n author = {Yang Shu and Zhangjie Cao and Chenyu Wang and Jianmin Wang and Mingsheng Long},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {9619-9628},\n title = {Open Domain Generalization with Domain-Augmented Meta-Learning},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6bd91a3183ddb844641acb9f3fe9faec6a9ff617",
            "@type": "ScholarlyArticle",
            "paperId": "6bd91a3183ddb844641acb9f3fe9faec6a9ff617",
            "corpusId": 239009828,
            "url": "https://www.semanticscholar.org/paper/6bd91a3183ddb844641acb9f3fe9faec6a9ff617",
            "title": "Meta-learning via Language Model In-context Tuning",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2110.07814",
                "DBLP": "conf/acl/ChenZZK022",
                "ACL": "2022.acl-long.53",
                "DOI": "10.18653/v1/2022.acl-long.53",
                "CorpusId": 239009828
            },
            "abstract": "The goal of meta-learning is to learn to adapt to a new task with only a few labeled examples. Inspired by the recent progress in large language models, we propose \\textit{in-context tuning} (ICT), which recasts task adaptation and prediction as a simple sequence prediction problem: to form the input sequence, we concatenate the task instruction, labeled in-context examples, and the target input to predict; to meta-train the model to learn from in-context examples, we fine-tune a pre-trained language model (LM) to predict the target label given the input sequence on a collection of tasks.We benchmark our method on two collections of text classification tasks: LAMA and BinaryClfs. Compared to MAML which adapts the model through gradient descent, our method leverages the inductive bias of pre-trained LMs to perform pattern matching, and outperforms MAML by an absolute 6% average AUC-ROC score on BinaryClfs, gaining more advantage with increasing model size. Compared to non-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning meta-trains the model to learn from in-context examples. On BinaryClfs, ICT improves the average AUC-ROC score by an absolute 10%, and reduces the variance due to example ordering by 6x and example choices by 2x.",
            "referenceCount": 64,
            "citationCount": 80,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.acl-long.53.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-10-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2110.07814"
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2021MetalearningVL,\n author = {Yanda Chen and Ruiqi Zhong and Sheng Zha and G. Karypis and He He},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Meta-learning via Language Model In-context Tuning},\n volume = {abs/2110.07814},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3904315e2eca50d0086e4b7273f7fd707c652230",
            "@type": "ScholarlyArticle",
            "paperId": "3904315e2eca50d0086e4b7273f7fd707c652230",
            "corpusId": 6466088,
            "url": "https://www.semanticscholar.org/paper/3904315e2eca50d0086e4b7273f7fd707c652230",
            "title": "Meta-Learning with Memory-Augmented Neural Networks",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/icml/SantoroBBWL16",
                "MAG": "2472819217",
                "CorpusId": 6466088
            },
            "abstract": "Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of \"one-shot learning.\" Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.",
            "referenceCount": 20,
            "citationCount": 1478,
            "influentialCitationCount": 81,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-19",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Santoro2016MetaLearningWM,\n author = {Adam Santoro and Sergey Bartunov and M. Botvinick and Daan Wierstra and T. Lillicrap},\n booktitle = {International Conference on Machine Learning},\n pages = {1842-1850},\n title = {Meta-Learning with Memory-Augmented Neural Networks},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c18c92a925c4b670b1702c0a674cb6879ebe5e25",
            "@type": "ScholarlyArticle",
            "paperId": "c18c92a925c4b670b1702c0a674cb6879ebe5e25",
            "corpusId": 235446399,
            "url": "https://www.semanticscholar.org/paper/c18c92a925c4b670b1702c0a674cb6879ebe5e25",
            "title": "Bridging Multi-Task Learning and Meta-Learning: Towards Efficient Training and Effective Adaptation",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2106-09017",
                "ArXiv": "2106.09017",
                "CorpusId": 235446399
            },
            "abstract": "Multi-task learning (MTL) aims to improve the generalization of several related tasks by learning them jointly. As a comparison, in addition to the joint training scheme, modern meta-learning allows unseen tasks with limited labels during the test phase, in the hope of fast adaptation over them. Despite the subtle difference between MTL and meta-learning in the problem formulation, both learning paradigms share the same insight that the shared structure between existing training tasks could lead to better generalization and adaptation. In this paper, we take one important step further to understand the close connection between these two learning paradigms, through both theoretical analysis and empirical investigation. Theoretically, we first demonstrate that MTL shares the same optimization formulation with a class of gradient-based meta-learning (GBML) algorithms. We then prove that for over-parameterized neural networks with sufficient depth, the learned predictive functions of MTL and GBML are close. In particular, this result implies that the predictions given by these two models are similar over the same unseen task. Empirically, we corroborate our theoretical findings by showing that, with proper implementation, MTL is competitive against state-of-the-art GBML algorithms on a set of few-shot image classification benchmarks. Since existing GBML algorithms often involve costly second-order bi-level optimization, our first-order MTL method is an order of magnitude faster on large-scale datasets such as mini-ImageNet. We believe this work could help bridge the gap between these two learning paradigms, and provide a computationally efficient alternative to GBML that also supports fast task adaptation.",
            "referenceCount": 69,
            "citationCount": 59,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-06-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2106.09017"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2021BridgingML,\n author = {Haoxiang Wang and Han Zhao and Bo Li},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Bridging Multi-Task Learning and Meta-Learning: Towards Efficient Training and Effective Adaptation},\n volume = {abs/2106.09017},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:70a136547d81290b9f4dbc1fac49d31bc010bd3c",
            "@type": "ScholarlyArticle",
            "paperId": "70a136547d81290b9f4dbc1fac49d31bc010bd3c",
            "corpusId": 235367710,
            "url": "https://www.semanticscholar.org/paper/70a136547d81290b9f4dbc1fac49d31bc010bd3c",
            "title": "Meta-Learning to Compositionally Generalize",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2021,
            "externalIds": {
                "ACL": "2021.acl-long.258",
                "DBLP": "conf/acl/ConklinWST20",
                "ArXiv": "2106.04252",
                "DOI": "10.18653/v1/2021.acl-long.258",
                "CorpusId": 235367710
            },
            "abstract": "Natural language is compositional; the meaning of a sentence is a function of the meaning of its parts. This property allows humans to create and interpret novel sentences, generalizing robustly outside their prior experience. Neural networks have been shown to struggle with this kind of generalization, in particular performing poorly on tasks designed to assess compositional generalization (i.e. where training and testing distributions differ in ways that would be trivial for a compositional strategy to resolve). Their poor performance on these tasks may in part be due to the nature of supervised learning which assumes training and testing data to be drawn from the same distribution. We implement a meta-learning augmented version of supervised learning whose objective directly optimizes for out-of-distribution generalization. We construct pairs of tasks for meta-learning by sub-sampling existing training data. Each pair of tasks is constructed to contain relevant examples, as determined by a similarity metric, in an effort to inhibit models from memorizing their input. Experimental results on the COGS and SCAN datasets show that our similarity-driven meta-learning can improve generalization performance.",
            "referenceCount": 48,
            "citationCount": 58,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.acl-long.258.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-06-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2106.04252"
            },
            "citationStyles": {
                "bibtex": "@Article{Conklin2021MetaLearningTC,\n author = {Henry Conklin and Bailin Wang and Kenny Smith and Ivan Titov},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Meta-Learning to Compositionally Generalize},\n volume = {abs/2106.04252},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:39c244a77f06b9bf67f21bdec855967caba1f3db",
            "@type": "ScholarlyArticle",
            "paperId": "39c244a77f06b9bf67f21bdec855967caba1f3db",
            "corpusId": 232306962,
            "url": "https://www.semanticscholar.org/paper/39c244a77f06b9bf67f21bdec855967caba1f3db",
            "title": "Meta-DETR: Few-Shot Object Detection via Unified Image-Level Meta-Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2103-11731",
                "CorpusId": 232306962
            },
            "abstract": "Few-shot object detection aims at detecting novel objects with only a few annotated examples. Prior works have proved meta-learning a promising solution, and most of them essentially address detection by meta-learning over regions for their classi\ufb01cation and location \ufb01ne-tuning. However, these methods substantially rely on initially well-located region proposals, which are usually hard to obtain under the few-shot settings. This paper presents a novel meta-detector framework, namely Meta-DETR, which eliminates region-wise prediction and instead meta-learns object localization and classi\ufb01cation at image level in a uni\ufb01ed and complementary manner. Speci\ufb01cally, it \ufb01rst en-codes both support and query images into category-speci\ufb01c features and then feeds them into a category-agnostic decoder to directly generate predictions for speci\ufb01c categories. To facilitate meta-learning with deep networks, we design a simple but effective Semantic Alignment Mechanism (SAM), which aligns high-level and low-level feature semantics to improve the generalization of meta-learned representations. Experiments over multiple few-shot object detection benchmarks show that Meta-DETR outperforms state-of-the-art methods by large margins.",
            "referenceCount": 95,
            "citationCount": 52,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2103.11731"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2021MetaDETRFO,\n author = {Gongjie Zhang and Zhipeng Luo and Kaiwen Cui and Shijian Lu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Meta-DETR: Few-Shot Object Detection via Unified Image-Level Meta-Learning},\n volume = {abs/2103.11731},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8427455565ca12bcb6572ff7f29eb58515f36f28",
            "@type": "ScholarlyArticle",
            "paperId": "8427455565ca12bcb6572ff7f29eb58515f36f28",
            "corpusId": 238531408,
            "url": "https://www.semanticscholar.org/paper/8427455565ca12bcb6572ff7f29eb58515f36f28",
            "title": "Meta-Learning with Task-Adaptive Loss Function for Few-Shot Learning",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2110-03909",
                "ArXiv": "2110.03909",
                "DOI": "10.1109/iccv48922.2021.00933",
                "CorpusId": 238531408
            },
            "abstract": "In few-shot learning scenarios, the challenge is to generalize and perform well on new unseen examples when only very few labeled examples are available for each task. Model-agnostic meta-learning (MAML) has gained the popularity as one of the representative few-shot learning methods for its flexibility and applicability to diverse problems. However, MAML and its variants often resort to a simple loss function without any auxiliary loss function or regularization terms that can help achieve better generalization. The problem lies in that each application and task may require different auxiliary loss function, especially when tasks are diverse and distinct. Instead of attempting to hand-design an auxiliary loss function for each application and task, we introduce a new meta-learning framework with a loss function that adapts to each task. Our proposed framework, named Meta-Learning with Task-Adaptive Loss Function (MeTAL), demonstrates the effectiveness and the flexibility across various domains, such as few-shot classification and few-shot regression.",
            "referenceCount": 67,
            "citationCount": 46,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2110.03909",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-10-01",
            "journal": {
                "name": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Baik2021MetaLearningWT,\n author = {Sungyong Baik and Janghoon Choi and Heewon Kim and Dohee Cho and Jaesik Min and Kyoung Mu Lee},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {9445-9454},\n title = {Meta-Learning with Task-Adaptive Loss Function for Few-Shot Learning},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a5266f10b50d7bce78af2f49320bdd7aae158ef6",
            "@type": "ScholarlyArticle",
            "paperId": "a5266f10b50d7bce78af2f49320bdd7aae158ef6",
            "corpusId": 237250417,
            "url": "https://www.semanticscholar.org/paper/a5266f10b50d7bce78af2f49320bdd7aae158ef6",
            "title": "BERT Learns to Teach: Knowledge Distillation with Meta Learning",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2021,
            "externalIds": {
                "ACL": "2022.acl-long.485",
                "DBLP": "conf/acl/ZhouXM22",
                "ArXiv": "2106.04570",
                "DOI": "10.18653/v1/2022.acl-long.485",
                "CorpusId": 237250417
            },
            "abstract": "We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner. Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models.",
            "referenceCount": 63,
            "citationCount": 47,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.acl-long.485.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-06-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhou2021BERTLT,\n author = {Wangchunshu Zhou and Canwen Xu and Julian McAuley},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {7037-7049},\n title = {BERT Learns to Teach: Knowledge Distillation with Meta Learning},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f8befa0bc3442979ff19e070f7c6b16d66a776c5",
            "@type": "ScholarlyArticle",
            "paperId": "f8befa0bc3442979ff19e070f7c6b16d66a776c5",
            "corpusId": 237485233,
            "url": "https://www.semanticscholar.org/paper/f8befa0bc3442979ff19e070f7c6b16d66a776c5",
            "title": "Bootstrapped Meta-Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2109.04504",
                "DBLP": "journals/corr/abs-2109-04504",
                "CorpusId": 237485233
            },
            "abstract": "Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that the metric can control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an epsilon-greedy Q-learning agent, without backpropagating through the update rule.",
            "referenceCount": 75,
            "citationCount": 47,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-09-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2109.04504"
            },
            "citationStyles": {
                "bibtex": "@Article{Flennerhag2021BootstrappedM,\n author = {Sebastian Flennerhag and Yannick Schroecker and Tom Zahavy and Hado Philip van Hasselt and David Silver and Satinder Singh},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Bootstrapped Meta-Learning},\n volume = {abs/2109.04504},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e1f1d466ec9f827f21d81dfbdf1bd4efbdd5037b",
            "@type": "ScholarlyArticle",
            "paperId": "e1f1d466ec9f827f21d81dfbdf1bd4efbdd5037b",
            "corpusId": 236980096,
            "url": "https://www.semanticscholar.org/paper/e1f1d466ec9f827f21d81dfbdf1bd4efbdd5037b",
            "title": "Curriculum Meta-Learning for Next POI Recommendation",
            "venue": "Knowledge Discovery and Data Mining",
            "publicationVenue": {
                "id": "urn:research:a0edb93b-1e95-4128-a295-6b1659149cef",
                "name": "Knowledge Discovery and Data Mining",
                "alternate_names": [
                    "KDD",
                    "Knowl Discov Data Min"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigkdd/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "conf/kdd/Chen0FHYZ21",
                "DOI": "10.1145/3447548.3467132",
                "CorpusId": 236980096
            },
            "abstract": "Next point-of-interest (POI) recommendation is a hot research field where a recent emerging scenario, next POI to search recommendation, has been deployed in many online map services such as Baidu Maps. One of the key issues in this scenario is providing satisfactory recommendation services for cold-start cities with a limited number of user-POI interactions, which requires transferring the knowledge hidden in rich data from many other cities to these cold-start cities. Existing literature either does not consider the city-transfer issue or cannot simultaneously tackle the data sparsity and pattern diversity issues among various users in multiple cities. To address these issues, we explore city-transfer next POI to search recommendation that transfers the knowledge from multiple cities with rich data to cold-start cities with scarce data. We propose a novel Curriculum Hardness Aware Meta-Learning (CHAML) framework, which incorporates hard sample mining and curriculum learning into a meta-learning paradigm. Concretely, the CHAML framework considers both city-level and user-level hardness to enhance the conditional sampling during meta training, and uses an easy-to-hard curriculum for the city-sampling pool to help the meta-learner converge to a better state. Extensive experiments on two real-world map search datasets from Baidu Maps demonstrate the superiority of CHAML framework.",
            "referenceCount": 49,
            "citationCount": 45,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2021-08-14",
            "journal": {
                "name": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2021CurriculumMF,\n author = {Yudong Chen and Xin Wang and M. Fan and Jizhou Huang and Shengwen Yang and Wenwu Zhu},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining},\n title = {Curriculum Meta-Learning for Next POI Recommendation},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:781fe7b9957a8b3a316f5ab63cef79e8d78fb88d",
            "@type": "ScholarlyArticle",
            "paperId": "781fe7b9957a8b3a316f5ab63cef79e8d78fb88d",
            "corpusId": 235358966,
            "url": "https://www.semanticscholar.org/paper/781fe7b9957a8b3a316f5ab63cef79e8d78fb88d",
            "title": "Meta-Learning with Fewer Tasks through Task Interpolation",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2106.02695",
                "DBLP": "journals/corr/abs-2106-02695",
                "CorpusId": 235358966
            },
            "abstract": "Meta-learning enables algorithms to quickly learn a newly encountered task with just a few labeled examples by transferring previously learned knowledge. However, the bottleneck of current meta-learning algorithms is the requirement of a large number of meta-training tasks, which may not be accessible in real-world scenarios. To address the challenge that available tasks may not densely sample the space of tasks, we propose to augment the task set through interpolation. By meta-learning with task interpolation (MLTI), our approach effectively generates additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels. Under both gradient-based and metric-based meta-learning settings, our theoretical analysis shows MLTI corresponds to a data-adaptive meta-regularization and further improves the generalization. Empirically, in our experiments on eight datasets from diverse domains including image recognition, pose prediction, molecule property prediction, and medical image classification, we find that the proposed general MLTI framework is compatible with representative meta-learning algorithms and consistently outperforms other state-of-the-art strategies.",
            "referenceCount": 65,
            "citationCount": 41,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-06-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2106.02695"
            },
            "citationStyles": {
                "bibtex": "@Article{Yao2021MetaLearningWF,\n author = {Huaxiu Yao and Linjun Zhang and Chelsea Finn},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Meta-Learning with Fewer Tasks through Task Interpolation},\n volume = {abs/2106.02695},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4bf7f8950f863a22dcf2feb74d156a92cbd036d0",
            "@type": "ScholarlyArticle",
            "paperId": "4bf7f8950f863a22dcf2feb74d156a92cbd036d0",
            "corpusId": 219038317,
            "url": "https://www.semanticscholar.org/paper/4bf7f8950f863a22dcf2feb74d156a92cbd036d0",
            "title": "Ensemble Meta-Learning for Few-Shot Soot Density Recognition",
            "venue": "IEEE Transactions on Industrial Informatics",
            "publicationVenue": {
                "id": "urn:research:2135230a-3b24-4b71-9583-60624389377a",
                "name": "IEEE Transactions on Industrial Informatics",
                "alternate_names": [
                    "IEEE Trans Ind Informatics"
                ],
                "issn": "1551-3203",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=9424"
            },
            "year": 2021,
            "externalIds": {
                "MAG": "3023064465",
                "DBLP": "journals/tii/GuZQ21",
                "DOI": "10.1109/TII.2020.2991208",
                "CorpusId": 219038317
            },
            "abstract": "In each petrochemical plant around the world, the flare stack as a requisite facility produces a large amount of soot due to the incomplete combustion of flare gas, and this strongly endangers air quality and human health. Despite severe damages, the abovementioned abnormal conditions rarely occur, and, thus, only few-shot samples are available. To address such difficulty, in this article, we design an image-based flare soot density recognition network (FSDR-Net) via a new ensemble meta-learning technology. More particularly, we first train a deep convolutional neural network (CNN) by applying the model-agnostic meta-learning algorithm on a variety of learning tasks that are relevant to the flare soot recognition so as to obtain the general-purpose optimized initial parameters (GOIP). Second, for the new task of recognizing the flare soot density via only few-shot instances, a new ensemble is developed to selectively aggregate several predictions that are generated based on a wide range of learning rates and a small number of gradient steps. Results of experiments conducted on the density recognition of flare soot corroborate the superiority of our proposed FSDR-Net as compared with the popular and state-of-the-art deep CNNs.",
            "referenceCount": 40,
            "citationCount": 70,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-03-01",
            "journal": {
                "name": "IEEE Transactions on Industrial Informatics",
                "volume": "17"
            },
            "citationStyles": {
                "bibtex": "@Article{Gu2021EnsembleMF,\n author = {Ke Gu and Yonghui Zhang and J. Qiao},\n booktitle = {IEEE Transactions on Industrial Informatics},\n journal = {IEEE Transactions on Industrial Informatics},\n pages = {2261-2270},\n title = {Ensemble Meta-Learning for Few-Shot Soot Density Recognition},\n volume = {17},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:51f5b13da6cccd4ab00231a03e2e82f29aff953f",
            "@type": "ScholarlyArticle",
            "paperId": "51f5b13da6cccd4ab00231a03e2e82f29aff953f",
            "corpusId": 232147745,
            "url": "https://www.semanticscholar.org/paper/51f5b13da6cccd4ab00231a03e2e82f29aff953f",
            "title": "Adaptive-Control-Oriented Meta-Learning for Nonlinear Systems",
            "venue": "Robotics: Science and Systems",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "conf/rss/RichardsASP21",
                "ArXiv": "2103.04490",
                "DOI": "10.15607/RSS.2021.XVII.056",
                "CorpusId": 232147745
            },
            "abstract": "Real-time adaptation is imperative to the control of robots operating in complex, dynamic environments. Adaptive control laws can endow even nonlinear systems with good trajectory tracking performance, provided that any uncertain dynamics terms are linearly parameterizable with known nonlinear features. However, it is often difficult to specify such features a priori, such as for aerodynamic disturbances on rotorcraft or interaction forces between a manipulator arm and various objects. In this paper, we turn to data-driven modeling with neural networks to learn, offline from past data, an adaptive controller with an internal parametric model of these nonlinear features. Our key insight is that we can better prepare the controller for deployment with control-oriented meta-learning of features in closed-loop simulation, rather than regression-oriented meta-learning of features to fit input-output data. Specifically, we meta-learn the adaptive controller with closed-loop tracking simulation as the base-learner and the average tracking error as the meta-objective. With a nonlinear planar rotorcraft subject to wind, we demonstrate that our adaptive controller outperforms other controllers trained with regression-oriented meta-learning when deployed in closed-loop for trajectory tracking control.",
            "referenceCount": 83,
            "citationCount": 48,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.15607/rss.2021.xvii.056",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-03-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2103.04490"
            },
            "citationStyles": {
                "bibtex": "@Article{Richards2021AdaptiveControlOrientedMF,\n author = {Spencer M. Richards and Navid Azizan and J. Slotine and M. Pavone},\n booktitle = {Robotics: Science and Systems},\n journal = {ArXiv},\n title = {Adaptive-Control-Oriented Meta-Learning for Nonlinear Systems},\n volume = {abs/2103.04490},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:361e953f792a585496834ee14216b94d0ce9ae74",
            "@type": "ScholarlyArticle",
            "paperId": "361e953f792a585496834ee14216b94d0ce9ae74",
            "corpusId": 204788663,
            "url": "https://www.semanticscholar.org/paper/361e953f792a585496834ee14216b94d0ce9ae74",
            "title": "VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1910.08348",
                "DBLP": "journals/corr/abs-1910-08348",
                "MAG": "2996148148",
                "CorpusId": 204788663
            },
            "abstract": "Trading off exploration and exploitation in an unknown environment is key to maximising expected return during learning. A Bayes-optimal policy, which does so optimally, conditions its actions not only on the environment state but on the agent's uncertainty about the environment. Computing a Bayes-optimal policy is however intractable for all but the smallest tasks. In this paper, we introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference in an unknown environment, and incorporate task uncertainty directly during action selection. In a grid-world domain, we illustrate how variBAD performs structured online exploration as a function of task uncertainty. We further evaluate variBAD on MuJoCo domains widely used in meta-RL and show that it achieves higher online return than existing methods.",
            "referenceCount": 67,
            "citationCount": 192,
            "influentialCitationCount": 42,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-10-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1910.08348"
            },
            "citationStyles": {
                "bibtex": "@Article{Zintgraf2019VariBADAV,\n author = {L. Zintgraf and K. Shiarlis and Maximilian Igl and Sebastian Schulze and Y. Gal and Katja Hofmann and Shimon Whiteson},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning},\n volume = {abs/1910.08348},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ebae0a96098bb84eee418c76f5b6a2cb87614ac1",
            "@type": "ScholarlyArticle",
            "paperId": "ebae0a96098bb84eee418c76f5b6a2cb87614ac1",
            "corpusId": 219720969,
            "url": "https://www.semanticscholar.org/paper/ebae0a96098bb84eee418c76f5b6a2cb87614ac1",
            "title": "MetaSDF: Meta-learning Signed Distance Functions",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2006.09662",
                "MAG": "3035944497",
                "DBLP": "conf/nips/SitzmannCTSW20",
                "CorpusId": 219720969
            },
            "abstract": "Neural implicit shape representations are an emerging paradigm that offers many potential benefits over conventional discrete representations, including memory efficiency at a high spatial resolution. Generalizing across shapes with such neural implicit representations amounts to learning priors over the respective function space and enables geometry reconstruction from partial or noisy observations. Existing generalization methods rely on conditioning a neural network on a low-dimensional latent code that is either regressed by an encoder or jointly optimized in the auto-decoder framework. Here, we formalize learning of a shape space as a meta-learning problem and leverage gradient-based meta-learning algorithms to solve this task. We demonstrate that this approach performs on par with auto-decoder based approaches while being an order of magnitude faster at test-time inference. We further demonstrate that the proposed gradient-based method outperforms encoder-decoder based methods that leverage pooling-based set encoders.",
            "referenceCount": 48,
            "citationCount": 192,
            "influentialCitationCount": 28,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-06-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2006.09662"
            },
            "citationStyles": {
                "bibtex": "@Article{Sitzmann2020MetaSDFMS,\n author = {V. Sitzmann and Eric Chan and Richard Tucker and Noah Snavely and Gordon Wetzstein},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {MetaSDF: Meta-learning Signed Distance Functions},\n volume = {abs/2006.09662},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f2693c6c9a601d5a40a57db2d2e67aebf4b73b87",
            "@type": "ScholarlyArticle",
            "paperId": "f2693c6c9a601d5a40a57db2d2e67aebf4b73b87",
            "corpusId": 239998074,
            "url": "https://www.semanticscholar.org/paper/f2693c6c9a601d5a40a57db2d2e67aebf4b73b87",
            "title": "Meta-learning with an Adaptive Task Scheduler",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2110.14057",
                "DBLP": "journals/corr/abs-2110-14057",
                "CorpusId": 239998074
            },
            "abstract": "To benefit the learning of a new task, meta-learning has been proposed to transfer a well-generalized meta-model learned from various meta-training tasks. Existing meta-learning algorithms randomly sample meta-training tasks with a uniform probability, under the assumption that tasks are of equal importance. However, it is likely that tasks are detrimental with noise or imbalanced given a limited number of meta-training tasks. To prevent the meta-model from being corrupted by such detrimental tasks or dominated by tasks in the majority, in this paper, we propose an adaptive task scheduler (ATS) for the meta-training process. In ATS, for the first time, we design a neural scheduler to decide which meta-training tasks to use next by predicting the probability being sampled for each candidate task, and train the scheduler to optimize the generalization capacity of the meta-model to unseen tasks. We identify two meta-model-related factors as the input of the neural scheduler, which characterize the difficulty of a candidate task to the meta-model. Theoretically, we show that a scheduler taking the two factors into account improves the meta-training loss and also the optimization landscape. Under the setting of meta-learning with noise and limited budgets, ATS improves the performance on both miniImageNet and a real-world drug discovery benchmark by up to 13% and 18%, respectively, compared to state-of-the-art task schedulers.",
            "referenceCount": 36,
            "citationCount": 26,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-10-26",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2110.14057"
            },
            "citationStyles": {
                "bibtex": "@Article{Yao2021MetalearningWA,\n author = {Huaxiu Yao and Yu Wang and Ying Wei and P. Zhao and M. Mahdavi and Defu Lian and Chelsea Finn},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Meta-learning with an Adaptive Task Scheduler},\n volume = {abs/2110.14057},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:118a605ad954c8f8e1ad65941429d0fd2c14c918",
            "@type": "ScholarlyArticle",
            "paperId": "118a605ad954c8f8e1ad65941429d0fd2c14c918",
            "corpusId": 231985673,
            "url": "https://www.semanticscholar.org/paper/118a605ad954c8f8e1ad65941429d0fd2c14c918",
            "title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "conf/iclr/0008X0CWGW21",
                "ArXiv": "2102.10454",
                "CorpusId": 231985673
            },
            "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a meta-initialization} of model parameters (that we call meta-model) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how adversarial robustness can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study WHEN a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate HOW robust regularization can efficiently be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.",
            "referenceCount": 50,
            "citationCount": 27,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-02-20",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2102.10454"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2021OnFA,\n author = {Ren Wang and Kaidi Xu and Sijia Liu and Pin-Yu Chen and Tsui-Wei Weng and Chuang Gan and Meng Wang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning},\n volume = {abs/2102.10454},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7eb5a2e73e26de6b980b516c0d594d900035ec7d",
            "@type": "ScholarlyArticle",
            "paperId": "7eb5a2e73e26de6b980b516c0d594d900035ec7d",
            "corpusId": 5689540,
            "url": "https://www.semanticscholar.org/paper/7eb5a2e73e26de6b980b516c0d594d900035ec7d",
            "title": "The impact of enhancing students' social and emotional learning: a meta-analysis of school-based universal interventions.",
            "venue": "Child Development",
            "publicationVenue": {
                "id": "urn:research:e8d08953-c24b-41bf-97e3-72ba5f0c0d70",
                "name": "Child Development",
                "alternate_names": [
                    "Child Dev"
                ],
                "issn": "0009-3920",
                "url": "http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1467-8624"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2128689084",
                "DOI": "10.1111/j.1467-8624.2010.01564.x",
                "CorpusId": 5689540,
                "PubMed": "21291449"
            },
            "abstract": "This article presents findings from a meta-analysis of 213 school-based, universal social and emotional learning (SEL) programs involving 270,034 kindergarten through high school students. Compared to controls, SEL participants demonstrated significantly improved social and emotional skills, attitudes, behavior, and academic performance that reflected an 11-percentile-point gain in achievement. School teaching staff successfully conducted SEL programs. The use of 4 recommended practices for developing skills and the presence of implementation problems moderated program outcomes. The findings add to the growing empirical evidence regarding the positive impact of SEL programs. Policy makers, educators, and the public can contribute to healthy development of children by supporting the incorporation of evidence-based SEL programming into standard educational practice.",
            "referenceCount": 301,
            "citationCount": 6003,
            "influentialCitationCount": 474,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "MetaAnalysis",
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Child development",
                "volume": "82 1"
            },
            "citationStyles": {
                "bibtex": "@Article{Durlak2011TheIO,\n author = {J. Durlak and R. Weissberg and Allison B. Dymnicki and Rebecca D. Taylor and Kriston B. Schellinger},\n booktitle = {Child Development},\n journal = {Child development},\n pages = {\n          405-32\n        },\n title = {The impact of enhancing students' social and emotional learning: a meta-analysis of school-based universal interventions.},\n volume = {82 1},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:849c91ff8bb3ff67d278adb5295fee78049c6288",
            "@type": "ScholarlyArticle",
            "paperId": "849c91ff8bb3ff67d278adb5295fee78049c6288",
            "corpusId": 47020609,
            "url": "https://www.semanticscholar.org/paper/849c91ff8bb3ff67d278adb5295fee78049c6288",
            "title": "Bayesian Model-Agnostic Meta-Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2805481182",
                "ArXiv": "1806.03836",
                "DBLP": "conf/nips/YoonKDKBA18",
                "CorpusId": 47020609
            },
            "abstract": "Learning to infer Bayesian posterior from a few-shot dataset is an important step towards robust meta-learning due to the model uncertainty inherent in the problem. In this paper, we propose a novel Bayesian model-agnostic meta-learning method. The proposed method combines scalable gradient-based meta-learning with nonparametric variational inference in a principled probabilistic framework. During fast adaptation, the method is capable of learning complex uncertainty structure beyond a point estimate or a simple Gaussian approximation. In addition, a robust Bayesian meta-update mechanism with a new meta-loss prevents overfitting during meta-update. Remaining an efficient gradient-based meta-learner, the method is also model-agnostic and simple to implement. Experiment results show the accuracy and robustness of the proposed method in various tasks: sinusoidal regression, image classification, active learning, and reinforcement learning.",
            "referenceCount": 38,
            "citationCount": 435,
            "influentialCitationCount": 55,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-11",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.03836"
            },
            "citationStyles": {
                "bibtex": "@Article{Kim2018BayesianMM,\n author = {Taesup Kim and Jaesik Yoon and Ousmane Amadou Dia and Sungwoong Kim and Yoshua Bengio and Sungjin Ahn},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Bayesian Model-Agnostic Meta-Learning},\n volume = {abs/1806.03836},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1a385b975983da493645a675a8641916af765323",
            "@type": "ScholarlyArticle",
            "paperId": "1a385b975983da493645a675a8641916af765323",
            "corpusId": 211505772,
            "url": "https://www.semanticscholar.org/paper/1a385b975983da493645a675a8641916af765323",
            "title": "Provable Meta-Learning of Linear Representations",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3006977545",
                "DBLP": "conf/icml/TripuraneniJJ21",
                "ArXiv": "2002.11684",
                "CorpusId": 211505772
            },
            "abstract": "Meta-learning, or learning-to-learn, seeks to design algorithms that can utilize previous experience to rapidly learn new skills or adapt to new environments. Representation learning---a key tool for performing meta-learning---learns a data representation that can transfer knowledge across multiple tasks, which is essential in regimes where data is scarce. Despite a recent surge of interest in the practice of meta-learning, the theoretical underpinnings of meta-learning algorithms are lacking, especially in the context of learning transferable representations. In this paper, we focus on the problem of multi-task linear regression---in which multiple linear regression models share a common, low-dimensional linear representation. Here, we provide provably fast, sample-efficient algorithms to address the dual challenges of (1) learning a common set of features from multiple, related tasks, and (2) transferring this knowledge to new, unseen tasks. Both are central to the general problem of meta-learning. Finally, we complement these results by providing information-theoretic lower bounds on the sample complexity of learning these linear features.",
            "referenceCount": 46,
            "citationCount": 134,
            "influentialCitationCount": 29,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-02-26",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tripuraneni2020ProvableMO,\n author = {Nilesh Tripuraneni and Chi Jin and Michael I. Jordan},\n booktitle = {International Conference on Machine Learning},\n pages = {10434-10443},\n title = {Provable Meta-Learning of Linear Representations},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:475d35a691580d0082ecae212a81d9a4b6be787d",
            "@type": "ScholarlyArticle",
            "paperId": "475d35a691580d0082ecae212a81d9a4b6be787d",
            "corpusId": 168169908,
            "url": "https://www.semanticscholar.org/paper/475d35a691580d0082ecae212a81d9a4b6be787d",
            "title": "Meta-Learning Representations for Continual Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1905-12588",
                "MAG": "2970505118",
                "ArXiv": "1905.12588",
                "CorpusId": 168169908
            },
            "abstract": "A continual learning agent should be able to build on top of existing knowledge to learn on new data quickly while minimizing forgetting. Current intelligent systems based on neural network function approximators arguably do the opposite---they are highly prone to forgetting and rarely trained to facilitate future learning. One reason for this poor behavior is that they learn from a representation that is not explicitly trained for these two goals. In this paper, we propose OML, an objective that directly minimizes catastrophic interference by learning representations that accelerate future learning and are robust to forgetting under online updates in continual learning. We show that it is possible to learn naturally sparse representations that are more effective for online updating. Moreover, our algorithm is complementary to existing continual learning strategies, such as MER and GEM. Finally, we demonstrate that a basic online updating strategy on representations learned by OML is competitive with rehearsal based methods for continual learning. We release an implementation of our method at this https URL .",
            "referenceCount": 32,
            "citationCount": 249,
            "influentialCitationCount": 39,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-05-29",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1905.12588"
            },
            "citationStyles": {
                "bibtex": "@Article{Javed2019MetaLearningRF,\n author = {Khurram Javed and Martha White},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Meta-Learning Representations for Continual Learning},\n volume = {abs/1905.12588},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6bc3d38a63821534d587cad614eeea908593ed04",
            "@type": "ScholarlyArticle",
            "paperId": "6bc3d38a63821534d587cad614eeea908593ed04",
            "corpusId": 219786126,
            "url": "https://www.semanticscholar.org/paper/6bc3d38a63821534d587cad614eeea908593ed04",
            "title": "Retail sales forecasting with meta-learning",
            "venue": "European Journal of Operational Research",
            "publicationVenue": {
                "id": "urn:research:0acd87e7-c1b4-434a-955a-c26983a4b9f4",
                "name": "European Journal of Operational Research",
                "alternate_names": [
                    "Eur J Oper Res"
                ],
                "issn": "0377-2217",
                "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/505543/description#description"
            },
            "year": 2021,
            "externalIds": {
                "MAG": "3029422813",
                "DBLP": "journals/eor/MaF21",
                "DOI": "10.1016/j.ejor.2020.05.038",
                "CorpusId": 219786126
            },
            "abstract": null,
            "referenceCount": 73,
            "citationCount": 67,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://eprints.lancs.ac.uk/id/eprint/145186/1/Retail_sales_forecasting_with_meta_learning_0329.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Business",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-01-01",
            "journal": {
                "name": "Eur. J. Oper. Res.",
                "volume": "288"
            },
            "citationStyles": {
                "bibtex": "@Article{Ma2021RetailSF,\n author = {Shaohui Ma and R. Fildes},\n booktitle = {European Journal of Operational Research},\n journal = {Eur. J. Oper. Res.},\n pages = {111-128},\n title = {Retail sales forecasting with meta-learning},\n volume = {288},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:743bb270d501bb4c4c2760ee68f8864344c83114",
            "@type": "ScholarlyArticle",
            "paperId": "743bb270d501bb4c4c2760ee68f8864344c83114",
            "corpusId": 203591432,
            "url": "https://www.semanticscholar.org/paper/743bb270d501bb4c4c2760ee68f8864344c83114",
            "title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1909-12488",
                "ArXiv": "1909.12488",
                "MAG": "2976335444",
                "CorpusId": 203591432
            },
            "abstract": "Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. We present FL as a natural source of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.",
            "referenceCount": 25,
            "citationCount": 417,
            "influentialCitationCount": 29,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1909.12488"
            },
            "citationStyles": {
                "bibtex": "@Article{Jiang2019ImprovingFL,\n author = {Yihan Jiang and Jakub Konecn\u00fd and Keith Rush and Sreeram Kannan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Improving Federated Learning Personalization via Model Agnostic Meta Learning},\n volume = {abs/1909.12488},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:06b8e82542d1873928d007548a23d3b77daa11f8",
            "@type": "ScholarlyArticle",
            "paperId": "06b8e82542d1873928d007548a23d3b77daa11f8",
            "corpusId": 174773261,
            "url": "https://www.semanticscholar.org/paper/06b8e82542d1873928d007548a23d3b77daa11f8",
            "title": "Urban Traffic Prediction from Spatio-Temporal Data Using Deep Meta Learning",
            "venue": "Knowledge Discovery and Data Mining",
            "publicationVenue": {
                "id": "urn:research:a0edb93b-1e95-4128-a295-6b1659149cef",
                "name": "Knowledge Discovery and Data Mining",
                "alternate_names": [
                    "KDD",
                    "Knowl Discov Data Min"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigkdd/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2950817888",
                "DBLP": "conf/kdd/PanLW00Z19",
                "DOI": "10.1145/3292500.3330884",
                "CorpusId": 174773261
            },
            "abstract": "Predicting urban traffic is of great importance to intelligent transportation systems and public safety, yet is very challenging because of two aspects: 1) complex spatio-temporal correlations of urban traffic, including spatial correlations between locations along with temporal correlations among timestamps; 2) diversity of such spatio-temporal correlations, which vary from location to location and depend on the surrounding geographical information, e.g., points of interests and road networks. To tackle these challenges, we proposed a deep-meta-learning based model, entitled ST-MetaNet, to collectively predict traffic in all location at once. ST-MetaNet employs a sequence-to-sequence architecture, consisting of an encoder to learn historical information and a decoder to make predictions step by step. In specific, the encoder and decoder have the same network structure, consisting of a recurrent neural network to encode the traffic, a meta graph attention network to capture diverse spatial correlations, and a meta recurrent neural network to consider diverse temporal correlations. Extensive experiments were conducted based on two real-world datasets to illustrate the effectiveness of ST-MetaNet beyond several state-of-the-art methods.",
            "referenceCount": 29,
            "citationCount": 347,
            "influentialCitationCount": 33,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-07-25",
            "journal": {
                "name": "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Pan2019UrbanTP,\n author = {Zheyi Pan and Yuxuan Liang and Weifeng Wang and Yong Yu and Yu Zheng and Junbo Zhang},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},\n title = {Urban Traffic Prediction from Spatio-Temporal Data Using Deep Meta Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:642c98fd8266ef3c295b4c5c47d444807daf9621",
            "@type": "ScholarlyArticle",
            "paperId": "642c98fd8266ef3c295b4c5c47d444807daf9621",
            "corpusId": 231925366,
            "url": "https://www.semanticscholar.org/paper/642c98fd8266ef3c295b4c5c47d444807daf9621",
            "title": "Multi-Objective Meta Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2102.07121",
                "DBLP": "conf/nips/YeLYGXZ21",
                "CorpusId": 231925366
            },
            "abstract": "Meta learning with multiple objectives can be formulated as a Multi-Objective Bi-Level optimization Problem (MOBLP) where the upper-level subproblem is to solve several possible conflicting targets for the meta learner. However, existing studies either apply an inefficient evolutionary algorithm or linearly combine multiple objectives as a single-objective problem with the need to tune combination weights. In this paper, we propose a unified gradient-based Multi-Objective Meta Learning (MOML) framework and devise the first gradient-based optimization algorithm to solve the MOBLP by alternatively solving the lower-level and upper-level subproblems via the gradient descent method and the gradient-based multi-objective optimization method, respectively. Theoretically, we prove the convergence properties of the proposed gradient-based optimization algorithm. Empirically, we show the effectiveness of the proposed MOML framework in several meta learning problems, including few-shot learning, neural architecture search, domain adaptation, and multi-task learning.",
            "referenceCount": 85,
            "citationCount": 28,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-02-14",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ye2021MultiObjectiveML,\n author = {Feiyang Ye and Baijiong Lin and Zhixiong Yue and Pengxin Guo and Qiao Xiao and Yu Zhang},\n booktitle = {Neural Information Processing Systems},\n pages = {21338-21351},\n title = {Multi-Objective Meta Learning},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:46b072cf918ec6f50403568a73d4347ea86b7e66",
            "@type": "ScholarlyArticle",
            "paperId": "46b072cf918ec6f50403568a73d4347ea86b7e66",
            "corpusId": 3484654,
            "url": "https://www.semanticscholar.org/paper/46b072cf918ec6f50403568a73d4347ea86b7e66",
            "title": "Recasting Gradient-Based Meta-Learning as Hierarchical Bayes",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1801-08930",
                "ArXiv": "1801.08930",
                "MAG": "2963547174",
                "CorpusId": 3484654
            },
            "abstract": "Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al. (2017) as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.",
            "referenceCount": 61,
            "citationCount": 458,
            "influentialCitationCount": 45,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-01-26",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1801.08930"
            },
            "citationStyles": {
                "bibtex": "@Article{Grant2018RecastingGM,\n author = {Erin Grant and Chelsea Finn and S. Levine and Trevor Darrell and T. Griffiths},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Recasting Gradient-Based Meta-Learning as Hierarchical Bayes},\n volume = {abs/1801.08930},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c1630825f42ae036658e0e7cfe103bd3b3dd2444",
            "@type": "ScholarlyArticle",
            "paperId": "c1630825f42ae036658e0e7cfe103bd3b3dd2444",
            "corpusId": 233864561,
            "url": "https://www.semanticscholar.org/paper/c1630825f42ae036658e0e7cfe103bd3b3dd2444",
            "title": "Meta-Learning-Based Deep Reinforcement Learning for Multiobjective Optimization Problems",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/tnn/ZhangWZW23",
                "ArXiv": "2105.02741",
                "DOI": "10.1109/TNNLS.2022.3148435",
                "CorpusId": 233864561,
                "PubMed": "35171781"
            },
            "abstract": "Deep reinforcement learning (DRL) has recently shown its success in tackling complex combinatorial optimization problems. When these problems are extended to multiobjective ones, it becomes difficult for the existing DRL approaches to flexibly and efficiently deal with multiple subproblems determined by the weight decomposition of objectives. This article proposes a concise meta-learning-based DRL approach. It first trains a meta-model by meta-learning. The meta-model is fine-tuned with a few update steps to derive submodels for the corresponding subproblems. The Pareto front is then built accordingly. Compared with other learning-based methods, our method can greatly shorten the training time of multiple submodels. Due to the rapid and excellent adaptability of the meta-model, more submodels can be derived so as to increase the quality and diversity of the found solutions. The computational experiments on multiobjective traveling salesman problems and multiobjective vehicle routing problems with time windows demonstrate the superiority of our method over most of the learning-based and iteration-based approaches.",
            "referenceCount": 61,
            "citationCount": 22,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2105.02741",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-05-06",
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "34"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2021MetaLearningBasedDR,\n author = {Zizhen Zhang and Zhiyuan Wu and Hang Zhang and Jiahai Wang},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {7978-7991},\n title = {Meta-Learning-Based Deep Reinforcement Learning for Multiobjective Optimization Problems},\n volume = {34},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a9872078cc6dabd2428750543862b45f4a482dfc",
            "@type": "ScholarlyArticle",
            "paperId": "a9872078cc6dabd2428750543862b45f4a482dfc",
            "corpusId": 219687849,
            "url": "https://www.semanticscholar.org/paper/a9872078cc6dabd2428750543862b45f4a482dfc",
            "title": "Graph Meta Learning via Local Subgraphs",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3100807776",
                "DBLP": "conf/nips/HuangZ20",
                "ArXiv": "2006.07889",
                "CorpusId": 219687849
            },
            "abstract": "Prevailing methods for graphs require abundant label and edge information for learning. When data for a new task are scarce, meta-learning can learn from prior experiences and form much-needed inductive biases for fast adaption to new tasks. Here, we introduce G-Meta, a novel meta-learning algorithm for graphs. G-Meta uses local subgraphs to transfer subgraph-specific information and learn transferable knowledge faster via meta gradients. G-Meta learns how to quickly adapt to a new task using only a handful of nodes or edges in the new task and does so by learning from data points in other graphs or related, albeit disjoint label sets. G-Meta is theoretically justified as we show that the evidence for a prediction can be found in the local subgraph surrounding the target node or edge. Experiments on seven datasets and nine baseline methods show that G-Meta outperforms existing methods by up to 16.3%. Unlike previous methods, G-Meta successfully learns in challenging, few-shot learning settings that require generalization to completely new graphs and never-before-seen labels. Finally, G-Meta scales to large graphs, which we demonstrate on a new Tree-of-Life dataset comprising of 1,840 graphs, a two-orders of magnitude increase in the number of graphs used in prior work.",
            "referenceCount": 70,
            "citationCount": 108,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-06-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2006.07889"
            },
            "citationStyles": {
                "bibtex": "@Article{Huang2020GraphML,\n author = {Kexin Huang and M. Zitnik},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Graph Meta Learning via Local Subgraphs},\n volume = {abs/2006.07889},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3f0e82d56d18787feac8ddbc7c3c8490eb76a4c7",
            "@type": "ScholarlyArticle",
            "paperId": "3f0e82d56d18787feac8ddbc7c3c8490eb76a4c7",
            "corpusId": 60440365,
            "url": "https://www.semanticscholar.org/paper/3f0e82d56d18787feac8ddbc7c3c8490eb76a4c7",
            "title": "Task2Vec: Task Embedding for Meta-Learning",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2990761674",
                "DBLP": "journals/corr/abs-1902-03545",
                "ArXiv": "1902.03545",
                "DOI": "10.1109/ICCV.2019.00653",
                "CorpusId": 60440365
            },
            "abstract": "We introduce a method to generate vectorial representations of visual classification tasks which can be used to reason about the nature of those tasks and their relations. Given a dataset with ground-truth labels and a loss function, we process images through a \"probe network\" and compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a fixed-dimensional embedding of the task that is independent of details such as the number of classes and requires no understanding of the class label semantics. We demonstrate that this embedding is capable of predicting task similarities that match our intuition about semantic and taxonomic relations between different visual tasks. We demonstrate the practical value of this framework for the meta-task of selecting a pre-trained feature extractor for a novel task. We present a simple meta-learning framework for learning a metric on embeddings that is capable of predicting which feature extractors will perform well on which task. Selecting a feature extractor with task embedding yields performance close to the best available feature extractor, with substantially less computational effort than exhaustively training and evaluating all available models.",
            "referenceCount": 46,
            "citationCount": 237,
            "influentialCitationCount": 33,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://authors.library.caltech.edu/94203/1/1902.03545.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-02-10",
            "journal": {
                "name": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Achille2019Task2VecTE,\n author = {A. Achille and Michael Lam and Rahul Tewari and Avinash Ravichandran and Subhransu Maji and Charless C. Fowlkes and Stefano Soatto and P. Perona},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {6429-6438},\n title = {Task2Vec: Task Embedding for Meta-Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:86e1e0fbb4388d7216973c6385428c11a505e7b5",
            "@type": "ScholarlyArticle",
            "paperId": "86e1e0fbb4388d7216973c6385428c11a505e7b5",
            "corpusId": 231846526,
            "url": "https://www.semanticscholar.org/paper/86e1e0fbb4388d7216973c6385428c11a505e7b5",
            "title": "Meta-Learning with Neural Tangent Kernels",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "conf/iclr/ZhouWXC021",
                "ArXiv": "2102.03909",
                "CorpusId": 231846526
            },
            "abstract": "Model Agnostic Meta-Learning (MAML) has emerged as a standard framework for meta-learning, where a meta-model is learned with the ability of fast adapting to new tasks. However, as a double-looped optimization problem, MAML needs to differentiate through the whole inner-loop optimization path for every outer-loop training step, which may lead to both computational inefficiency and sub-optimal solutions. In this paper, we generalize MAML to allow meta-learning to be defined in function spaces, and propose the first meta-learning paradigm in the Reproducing Kernel Hilbert Space (RKHS) induced by the meta-model's Neural Tangent Kernel (NTK). Within this paradigm, we introduce two meta-learning algorithms in the RKHS, which no longer need a sub-optimal iterative inner-loop adaptation as in the MAML framework. We achieve this goal by 1) replacing the adaptation with a fast-adaptive regularizer in the RKHS; and 2) solving the adaptation analytically based on the NTK theory. Extensive experimental studies demonstrate advantages of our paradigm in both efficiency and quality of solutions compared to related meta-learning algorithms. Another interesting feature of our proposed methods is that they are demonstrated to be more robust to adversarial attacks and out-of-distribution adaptation than popular baselines, as demonstrated in our experiments.",
            "referenceCount": 41,
            "citationCount": 18,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-02-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2102.03909"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhou2021MetaLearningWN,\n author = {Yufan Zhou and Zhenyi Wang and Jiayi Xian and Changyou Chen and Jinhui Xu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Meta-Learning with Neural Tangent Kernels},\n volume = {abs/2102.03909},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2f240424ca0761d0549252dacfbbeece14bb3cb6",
            "@type": "ScholarlyArticle",
            "paperId": "2f240424ca0761d0549252dacfbbeece14bb3cb6",
            "corpusId": 59413758,
            "url": "https://www.semanticscholar.org/paper/2f240424ca0761d0549252dacfbbeece14bb3cb6",
            "title": "Fast Context Adaptation via Meta-Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/icml/ZintgrafSKHW19",
                "MAG": "2914752403",
                "CorpusId": 59413758
            },
            "abstract": "We propose CAVIA, a meta-learning method for fast adaptation that is scalable, flexible, and easy to implement. CAVIA partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), CAVIA can be scaled up to larger networks without overfitting on a single task, is easier to implement, and is more robust to the inner-loop learning rate. We show empirically that CAVIA outperforms MAML on regression, classification, and reinforcement learning problems.",
            "referenceCount": 34,
            "citationCount": 300,
            "influentialCitationCount": 36,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-10-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zintgraf2018FastCA,\n author = {L. Zintgraf and K. Shiarlis and Vitaly Kurin and Katja Hofmann and Shimon Whiteson},\n booktitle = {International Conference on Machine Learning},\n pages = {7693-7702},\n title = {Fast Context Adaptation via Meta-Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d237f3e88adfa58f1b2e6bfedd1678b883c4dc53",
            "@type": "ScholarlyArticle",
            "paperId": "d237f3e88adfa58f1b2e6bfedd1678b883c4dc53",
            "corpusId": 201651077,
            "url": "https://www.semanticscholar.org/paper/d237f3e88adfa58f1b2e6bfedd1678b883c4dc53",
            "title": "On the Convergence Theory of Gradient-Based Model-Agnostic Meta-Learning Algorithms",
            "venue": "International Conference on Artificial Intelligence and Statistics",
            "publicationVenue": {
                "id": "urn:research:2d136b11-c2b5-484b-b008-7f4a852fd61e",
                "name": "International Conference on Artificial Intelligence and Statistics",
                "alternate_names": [
                    "AISTATS",
                    "Int Conf Artif Intell Stat"
                ],
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1908.10400",
                "MAG": "3037724881",
                "DBLP": "journals/corr/abs-1908-10400",
                "CorpusId": 201651077
            },
            "abstract": "We study the convergence of a class of gradient-based Model-Agnostic Meta-Learning (MAML) methods and characterize their overall complexity as well as their best achievable accuracy in terms of gradient norm for nonconvex loss functions. We start with the MAML method and its first-order approximation (FO-MAML) and highlight the challenges that emerge in their analysis. By overcoming these challenges not only we provide the first theoretical guarantees for MAML and FO-MAML in nonconvex settings, but also we answer some of the unanswered questions for the implementation of these algorithms including how to choose their learning rate and the batch size for both tasks and datasets corresponding to tasks. In particular, we show that MAML can find an $\\epsilon$-first-order stationary point ($\\epsilon$-FOSP) for any positive $\\epsilon$ after at most $\\mathcal{O}(1/\\epsilon^2)$ iterations at the expense of requiring second-order information. We also show that FO-MAML which ignores the second-order information required in the update of MAML cannot achieve any small desired level of accuracy, i.e., FO-MAML cannot find an $\\epsilon$-FOSP for any $\\epsilon>0$. We further propose a new variant of the MAML algorithm called Hessian-free MAML which preserves all theoretical guarantees of MAML, without requiring access to second-order information.",
            "referenceCount": 33,
            "citationCount": 181,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-08-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1908.10400"
            },
            "citationStyles": {
                "bibtex": "@Article{Fallah2019OnTC,\n author = {Alireza Fallah and Aryan Mokhtari and A. Ozdaglar},\n booktitle = {International Conference on Artificial Intelligence and Statistics},\n journal = {ArXiv},\n title = {On the Convergence Theory of Gradient-Based Model-Agnostic Meta-Learning Algorithms},\n volume = {abs/1908.10400},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:84bec62d065c6e14df23d7e289d84bf7b7838a0a",
            "@type": "ScholarlyArticle",
            "paperId": "84bec62d065c6e14df23d7e289d84bf7b7838a0a",
            "corpusId": 208002959,
            "url": "https://www.semanticscholar.org/paper/84bec62d065c6e14df23d7e289d84bf7b7838a0a",
            "title": "Meta-Learning to Detect Rare Objects",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "3009213340",
                "DBLP": "conf/iccv/WangRH19",
                "DOI": "10.1109/ICCV.2019.01002",
                "CorpusId": 208002959
            },
            "abstract": "Few-shot learning, i.e., learning novel concepts from few examples, is fundamental to practical visual recognition systems. While most of existing work has focused on few-shot classification, we make a step towards few-shot object detection, a more challenging yet under-explored task. We develop a conceptually simple but powerful meta-learning based framework that simultaneously tackles few-shot classification and few-shot localization in a unified, coherent way. This framework leverages meta-level knowledge about \"model parameter generation\" from base classes with abundant data to facilitate the generation of a detector for novel classes. Our key insight is to disentangle the learning of category-agnostic and category-specific components in a CNN based detection model. In particular, we introduce a weight prediction meta-model that enables predicting the parameters of category-specific components from few examples. We systematically benchmark the performance of modern detectors in the small-sample size regime. Experiments in a variety of realistic scenarios, including within-domain, cross-domain, and long-tailed settings, demonstrate the effectiveness and generality of our approach under different notions of novel classes.",
            "referenceCount": 75,
            "citationCount": 223,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-10-01",
            "journal": {
                "name": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2019MetaLearningTD,\n author = {Yu-Xiong Wang and Deva Ramanan and M. Hebert},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {9924-9933},\n title = {Meta-Learning to Detect Rare Objects},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:37b3d9ab049c5671fed29f56cacee858d98c2ea8",
            "@type": "ScholarlyArticle",
            "paperId": "37b3d9ab049c5671fed29f56cacee858d98c2ea8",
            "corpusId": 52922125,
            "url": "https://www.semanticscholar.org/paper/37b3d9ab049c5671fed29f56cacee858d98c2ea8",
            "title": "Unsupervised Learning via Meta-Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2964032613",
                "DBLP": "conf/iclr/HsuLF19",
                "ArXiv": "1810.02334",
                "CorpusId": 52922125
            },
            "abstract": "A central goal of unsupervised learning is to acquire representations from unlabeled data or experience that can be used for more effective learning of downstream tasks from modest amounts of labeled data. Many prior unsupervised learning works aim to do so by developing proxy objectives based on reconstruction, disentanglement, prediction, and other metrics. Instead, we develop an unsupervised meta-learning method that explicitly optimizes for the ability to learn a variety of tasks from small amounts of data. To do so, we construct tasks from unlabeled data in an automatic way and run meta-learning over the constructed tasks. Surprisingly, we find that, when integrated with meta-learning, relatively simple task construction mechanisms, such as clustering embeddings, lead to good performance on a variety of downstream, human-specified tasks. Our experiments across four image datasets indicate that our unsupervised meta-learning approach acquires a learning algorithm without any labeled data that is applicable to a wide range of downstream classification tasks, improving upon the embedding learned by four prior unsupervised learning methods.",
            "referenceCount": 59,
            "citationCount": 205,
            "influentialCitationCount": 39,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1810.02334"
            },
            "citationStyles": {
                "bibtex": "@Article{Hsu2018UnsupervisedLV,\n author = {Kyle Hsu and S. Levine and Chelsea Finn},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Unsupervised Learning via Meta-Learning},\n volume = {abs/1810.02334},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a03be7e9055632cbccdf9cee6171089c1e167557",
            "@type": "ScholarlyArticle",
            "paperId": "a03be7e9055632cbccdf9cee6171089c1e167557",
            "corpusId": 202772999,
            "url": "https://www.semanticscholar.org/paper/a03be7e9055632cbccdf9cee6171089c1e167557",
            "title": "Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1910-13616",
                "MAG": "2982366826",
                "ArXiv": "1910.13616",
                "CorpusId": 202772999
            },
            "abstract": "Model-agnostic meta-learners aim to acquire meta-learned parameters from similar tasks to adapt to novel tasks from the same distribution with few gradient updates. With the flexibility in the choice of models, those frameworks demonstrate appealing performance on a variety of domains such as few-shot image classification and reinforcement learning. However, one important limitation of such frameworks is that they seek a common initialization shared across the entire task distribution, substantially limiting the diversity of the task distributions that they are able to learn from. In this paper, we augment MAML with the capability to identify the mode of tasks sampled from a multimodal task distribution and adapt quickly through gradient updates. Specifically, we propose a multimodal MAML (MMAML) framework, which is able to modulate its meta-learned prior parameters according to the identified mode, allowing more efficient fast adaptation. We evaluate the proposed model on a diverse set of few-shot learning tasks, including regression, image classification, and reinforcement learning. The results not only demonstrate the effectiveness of our model in modulating the meta-learned prior in response to the characteristics of tasks but also show that training on a multimodal distribution can produce an improvement over unimodal training. The code for this project is publicly available at https://vuoristo.github.io/MMAML.",
            "referenceCount": 64,
            "citationCount": 184,
            "influentialCitationCount": 32,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-10-30",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Vuorio2019MultimodalMM,\n author = {Risto Vuorio and Shao-Hua Sun and Hexiang Hu and Joseph J. Lim},\n booktitle = {Neural Information Processing Systems},\n pages = {1-12},\n title = {Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a491eab02566d7901314e5b1512cdb5e0b28e752",
            "@type": "ScholarlyArticle",
            "paperId": "a491eab02566d7901314e5b1512cdb5e0b28e752",
            "corpusId": 174802574,
            "url": "https://www.semanticscholar.org/paper/a491eab02566d7901314e5b1512cdb5e0b28e752",
            "title": "Adaptive Gradient-Based Meta-Learning Methods",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2970814018",
                "ArXiv": "1906.02717",
                "DBLP": "conf/nips/KhodakBT19",
                "CorpusId": 174802574
            },
            "abstract": "We build a theoretical framework for designing and understanding practical meta-learning methods that integrates sophisticated formalizations of task-similarity with the extensive literature on online convex optimization and sequential prediction algorithms. Our approach enables the task-similarity to be learned adaptively, provides sharper transfer-risk bounds in the setting of statistical learning-to-learn, and leads to straightforward derivations of average-case regret bounds for efficient algorithms in settings where the task-environment changes dynamically or the tasks share a certain geometric structure. We use our theory to modify several popular meta-learning algorithms and improve their meta-test-time performance on standard problems in few-shot learning and federated learning.",
            "referenceCount": 57,
            "citationCount": 282,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-06-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Khodak2019AdaptiveGM,\n author = {M. Khodak and Maria-Florina Balcan and Ameet Talwalkar},\n booktitle = {Neural Information Processing Systems},\n pages = {5915-5926},\n title = {Adaptive Gradient-Based Meta-Learning Methods},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:35172f0513d6a9fb9b1a40b7ad466da96a112d68",
            "@type": "ScholarlyArticle",
            "paperId": "35172f0513d6a9fb9b1a40b7ad466da96a112d68",
            "corpusId": 237213737,
            "url": "https://www.semanticscholar.org/paper/35172f0513d6a9fb9b1a40b7ad466da96a112d68",
            "title": "Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/iccv/Chen00D021",
                "ArXiv": "2003.04390",
                "DOI": "10.1109/ICCV48922.2021.00893",
                "CorpusId": 237213737
            },
            "abstract": "Meta-learning has been the most common framework for few-shot learning in recent years. It learns the model from collections of few-shot classification tasks, which is believed to have a key advantage of making the training objective consistent with the testing objective. However, some recent works report that by training for whole-classification, i.e. classification on the whole label-set, it can get comparable or even better embedding than many meta-learning algorithms. The edge between these two lines of works has yet been underexplored, and the effectiveness of meta-learning in few-shot learning remains unclear. In this paper, we explore a simple process: meta-learning over a whole-classification pre-trained model on its evaluation metric. We observe this simple method achieves competitive performance to state-of-the-art methods on standard bench-marks. Our further analysis shed some light on understanding the trade-offs between the meta-learning objective and the whole-classification objective in few-shot learning. Our code is available at https://github.com/yinboc/few-shot-meta-baseline.",
            "referenceCount": 32,
            "citationCount": 176,
            "influentialCitationCount": 25,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2003.04390",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-03-09",
            "journal": {
                "name": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2020MetaBaselineES,\n author = {Yinbo Chen and Zhuang Liu and Huijuan Xu and Trevor Darrell and Xiaolong Wang},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {9042-9051},\n title = {Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cbf914243ff5051fff0d17333aae6459ade0b4f9",
            "@type": "ScholarlyArticle",
            "paperId": "cbf914243ff5051fff0d17333aae6459ade0b4f9",
            "corpusId": 221191103,
            "url": "https://www.semanticscholar.org/paper/cbf914243ff5051fff0d17333aae6459ade0b4f9",
            "title": "Meta-learning on Heterogeneous Information Networks for Cold-start Recommendation",
            "venue": "Knowledge Discovery and Data Mining",
            "publicationVenue": {
                "id": "urn:research:a0edb93b-1e95-4128-a295-6b1659149cef",
                "name": "Knowledge Discovery and Data Mining",
                "alternate_names": [
                    "KDD",
                    "Knowl Discov Data Min"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigkdd/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/kdd/Lu0S20",
                "MAG": "3043239945",
                "DOI": "10.1145/3394486.3403207",
                "CorpusId": 221191103
            },
            "abstract": "Cold-start recommendation has been a challenging problem due to sparse user-item interactions for new users or items. Existing efforts have alleviated the cold-start issue to some extent, most of which approach the problem at the data level. Earlier methods often incorporate auxiliary data as user or item features, while more recent methods leverage heterogeneous information networks (HIN) to capture richer semantics via higher-order graph structures. On the other hand, recent meta-learning paradigm sheds light on addressing cold-start recommendation at the model level, given its ability to rapidly adapt to new tasks with scarce labeled data, or in the context of cold-start recommendation, new users and items with very few interactions. Thus, we are inspired to develop a novel meta-learning approach named MetaHIN to address cold-start recommendation on HINs, to exploit the power of meta-learning at the model level and HINs at the data level simultaneously. The solution is non-trivial, for how to capture HIN-based semantics in the meta-learning setting, and how to learn the general knowledge that can be easily adapted to multifaceted semantics, remain open questions. In MetaHIN, we propose a novel semantic-enhanced tasks constructor and a co-adaptation meta-learner to address the two questions. Extensive experiments demonstrate that MetaHIN significantly outperforms the state of the arts in various cold-start scenarios. (Code and dataset are available at https://github.com/rootlu/MetaHIN.)",
            "referenceCount": 34,
            "citationCount": 170,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2020-07-06",
            "journal": {
                "name": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lu2020MetalearningOH,\n author = {Yuanfu Lu and Yuan Fang and C. Shi},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},\n title = {Meta-learning on Heterogeneous Information Networks for Cold-start Recommendation},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:557e9371711c7409c78c96a6a2bea290a28cb365",
            "@type": "ScholarlyArticle",
            "paperId": "557e9371711c7409c78c96a6a2bea290a28cb365",
            "corpusId": 215745094,
            "url": "https://www.semanticscholar.org/paper/557e9371711c7409c78c96a6a2bea290a28cb365",
            "title": "MetaIQA: Deep Meta-Learning for No-Reference Image Quality Assessment",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3034882073",
                "ArXiv": "2004.05508",
                "DBLP": "conf/cvpr/ZhuLWDS20",
                "DOI": "10.1109/CVPR42600.2020.01415",
                "CorpusId": 215745094
            },
            "abstract": "Recently, increasing interest has been drawn in exploiting deep convolutional neural networks (DCNNs) for no-reference image quality assessment (NR-IQA). Despite of the notable success achieved, there is a broad consensus that training DCNNs heavily relies on massive annotated data. Unfortunately, IQA is a typical small sample problem. Therefore, most of the existing DCNN-based IQA metrics operate based on pre-trained networks. However, these pre-trained networks are not designed for IQA task, leading to generalization problem when evaluating different types of distortions. With this motivation, this paper presents a no-reference IQA metric based on deep meta-learning. The underlying idea is to learn the meta-knowledge shared by human when evaluating the quality of images with various distortions, which can then be adapted to unknown distortions easily. Specifically, we first collect a number of NR-IQA tasks for different distortions. Then meta-learning is adopted to learn the prior knowledge shared by diversified distortions. Finally, the quality prior model is fine-tuned on a target NR-IQA task for quickly obtaining the quality model. Extensive experiments demonstrate that the proposed metric outperforms the state-of-the-arts by a large margin. Furthermore, the meta-model learned from synthetic distortions can also be easily generalized to authentic distortions, which is highly desired in real-world applications of IQA metrics.",
            "referenceCount": 56,
            "citationCount": 142,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2004.05508",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-04-11",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhu2020MetaIQADM,\n author = {Hancheng Zhu and Leida Li and Jinjian Wu and W. Dong and Guangming Shi},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {14131-14140},\n title = {MetaIQA: Deep Meta-Learning for No-Reference Image Quality Assessment},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1e3a498c3e4cc2b3b40be6f1f139cd2654e7d249",
            "@type": "ScholarlyArticle",
            "paperId": "1e3a498c3e4cc2b3b40be6f1f139cd2654e7d249",
            "corpusId": 236213878,
            "url": "https://www.semanticscholar.org/paper/1e3a498c3e4cc2b3b40be6f1f139cd2654e7d249",
            "title": "Auto-Sklearn 2.0: Hands-free AutoML via Meta-Learning",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/jmlr/FeurerEFLH22",
                "ArXiv": "2007.04074",
                "CorpusId": 236213878
            },
            "abstract": "Automated Machine Learning (AutoML) supports practitioners and researchers with the tedious task of designing machine learning pipelines and has recently achieved substantial success. In this paper, we introduce new AutoML approaches motivated by our winning submission to the second ChaLearn AutoML challenge. We develop PoSH Auto-sklearn, which enables AutoML systems to work well on large datasets under rigid time limits by using a new, simple and meta-feature-free meta-learning technique and by employing a successful bandit strategy for budget allocation. However, PoSH Auto-sklearn introduces even more ways of running AutoML and might make it harder for users to set it up correctly. Therefore, we also go one step further and study the design space of AutoML itself, proposing a solution towards truly hands-free AutoML. Together, these changes give rise to the next generation of our AutoML system, Auto-sklearn 2.0. We verify the improvements by these additions in an extensive experimental study on 39 AutoML benchmark datasets. We conclude the paper by comparing to other popular AutoML frameworks and Auto-sklearn 1.0, reducing the relative error by up to a factor of 4.5, and yielding a performance in 10 minutes that is substantially better than what Auto-sklearn 1.0 achieves within an hour.",
            "referenceCount": 140,
            "citationCount": 132,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-07-08",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "23"
            },
            "citationStyles": {
                "bibtex": "@Article{Feurer2020AutoSklearn2H,\n author = {Matthias Feurer and Katharina Eggensperger and S. Falkner and M. Lindauer and F. Hutter},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {261:1-261:61},\n title = {Auto-Sklearn 2.0: Hands-free AutoML via Meta-Learning},\n volume = {23},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:332c44793b70776b9b966128c52e694222b1ab73",
            "@type": "ScholarlyArticle",
            "paperId": "332c44793b70776b9b966128c52e694222b1ab73",
            "corpusId": 222177134,
            "url": "https://www.semanticscholar.org/paper/332c44793b70776b9b966128c52e694222b1ab73",
            "title": "A survey of deep meta-learning",
            "venue": "Artificial Intelligence Review",
            "publicationVenue": {
                "id": "urn:research:ea8553fe-2467-4367-afee-c4deb3754820",
                "name": "Artificial Intelligence Review",
                "alternate_names": [
                    "Artif Intell Rev"
                ],
                "issn": "0269-2821",
                "url": "https://link.springer.com/journal/10462"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2010.03522",
                "DBLP": "journals/corr/abs-2010-03522",
                "MAG": "3092053846",
                "DOI": "10.1007/s10462-021-10004-4",
                "CorpusId": 222177134
            },
            "abstract": null,
            "referenceCount": 100,
            "citationCount": 180,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10462-021-10004-4.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-10-07",
            "journal": {
                "name": "Artificial Intelligence Review",
                "volume": "54"
            },
            "citationStyles": {
                "bibtex": "@Article{Huisman2020ASO,\n author = {M. Huisman and Jan N. van Rijn and A. Plaat},\n booktitle = {Artificial Intelligence Review},\n journal = {Artificial Intelligence Review},\n pages = {4483 - 4541},\n title = {A survey of deep meta-learning},\n volume = {54},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a5782ff434f20c13e6ce689b698dfe3446db3a30",
            "@type": "ScholarlyArticle",
            "paperId": "a5782ff434f20c13e6ce689b698dfe3446db3a30",
            "corpusId": 220363991,
            "url": "https://www.semanticscholar.org/paper/a5782ff434f20c13e6ce689b698dfe3446db3a30",
            "title": "Shape-aware Meta-learning for Generalizing Prostate MRI Segmentation to Unseen Domains",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "publicationVenue": {
                "id": "urn:research:61a709e3-2060-423c-8de5-ffd3885aa31c",
                "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                "alternate_names": [
                    "Medical Image Computing and Computer-Assisted Intervention",
                    "MICCAI",
                    "Med Image Comput Comput Interv",
                    "Int Conf Med Image Comput Comput Interv"
                ],
                "issn": null,
                "url": "http://www.miccai.org/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2007.02035",
                "DBLP": "journals/corr/abs-2007-02035",
                "MAG": "3038410202",
                "DOI": "10.1007/978-3-030-59713-9_46",
                "CorpusId": 220363991
            },
            "abstract": null,
            "referenceCount": 31,
            "citationCount": 121,
            "influentialCitationCount": 20,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2007.02035",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-07-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2020ShapeawareMF,\n author = {Quande Liu and Q. Dou and P. Heng},\n booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},\n pages = {475-485},\n title = {Shape-aware Meta-learning for Generalizing Prostate MRI Segmentation to Unseen Domains},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e95e3a314cab21171e206cd0824fe93c1c47677c",
            "@type": "ScholarlyArticle",
            "paperId": "e95e3a314cab21171e206cd0824fe93c1c47677c",
            "corpusId": 214667523,
            "url": "https://www.semanticscholar.org/paper/e95e3a314cab21171e206cd0824fe93c1c47677c",
            "title": "iTAML: An Incremental Task-Agnostic Meta-learning Approach",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3013343111",
                "DBLP": "journals/corr/abs-2003-11652",
                "ArXiv": "2003.11652",
                "DOI": "10.1109/cvpr42600.2020.01360",
                "CorpusId": 214667523
            },
            "abstract": "Humans can continuously learn new knowledge as their experience grows. In contrast, previous learning in deep neural networks can quickly fade out when they are trained on a new task. In this paper, we hypothesize this problem can be avoided by learning a set of generalized parameters, that are neither specific to old nor new tasks. In this pursuit, we introduce a novel meta-learning approach that seeks to maintain an equilibrium between all the encountered tasks. This is ensured by a new meta-update rule which avoids catastrophic forgetting. In comparison to previous meta-learning techniques, our approach is task-agnostic. When presented with a continuum of data, our model automatically identifies the task and quickly adapts to it with just a single update. We perform extensive experiments on five datasets in a class-incremental setting, leading to significant improvements over the state of the art methods (e.g., a 21.3% boost on CIFAR100 with 10 incremental tasks). Specifically, on large-scale datasets that generally prove difficult cases for incremental learning, our approach delivers absolute gains as high as 19.1% and 7.4% on ImageNet and MS-Celeb datasets, respectively.",
            "referenceCount": 34,
            "citationCount": 121,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2003.11652",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-03-25",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rajasegaran2020iTAMLAI,\n author = {Jathushan Rajasegaran and Salman Hameed Khan and Munawar Hayat and F. Khan and M. Shah},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {13585-13594},\n title = {iTAML: An Incremental Task-Agnostic Meta-learning Approach},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:af985dea540bd489397e7d28affa10f32a4d7167",
            "@type": "ScholarlyArticle",
            "paperId": "af985dea540bd489397e7d28affa10f32a4d7167",
            "corpusId": 214774780,
            "url": "https://www.semanticscholar.org/paper/af985dea540bd489397e7d28affa10f32a4d7167",
            "title": "Tracking by Instance Detection: A Meta-Learning Approach",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2004-00830",
                "ArXiv": "2004.00830",
                "MAG": "3034617042",
                "DOI": "10.1109/CVPR42600.2020.00632",
                "CorpusId": 214774780
            },
            "abstract": "We consider the tracking problem as a special type of object detection problem, which we call instance detection. With proper initialization, a detector can be quickly converted into a tracker by learning the new instance from a single image. We find that model-agnostic meta-learning (MAML) offers a strategy to initialize the detector that satisfies our needs. We propose a principled three-step approach to build a high-performance tracker. First, pick any modern object detector trained with gradient descent. Second, conduct offline training (or initialization) with MAML. Third, perform domain adaptation using the initial frame. We follow this procedure to build two trackers, named Retina-MAML and FCOS-MAML, based on two modern detectors RetinaNet and FCOS. Evaluations on four benchmarks show that both trackers are competitive against state-of-the-art trackers. On OTB-100, Retina-MAML achieves the highest ever AUC of 0.712. On TrackingNet, FCOS-MAML ranks the first on the leader board with an AUC of 0.757 and the normalized precision of 0.822. Both trackers run in real-time at 40 FPS.",
            "referenceCount": 45,
            "citationCount": 120,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2004.00830",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-04-02",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2020TrackingBI,\n author = {Guangting Wang and Chong Luo and Xiaoyan Sun and Zhiwei Xiong and Wenjun Zeng},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {6287-6296},\n title = {Tracking by Instance Detection: A Meta-Learning Approach},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8212605d274d5e68bcedf990728f4f5c26f88168",
            "@type": "ScholarlyArticle",
            "paperId": "8212605d274d5e68bcedf990728f4f5c26f88168",
            "corpusId": 226226438,
            "url": "https://www.semanticscholar.org/paper/8212605d274d5e68bcedf990728f4f5c26f88168",
            "title": "Dataset Meta-Learning from Kernel Ridge-Regression",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3097629404",
                "DBLP": "journals/corr/abs-2011-00050",
                "ArXiv": "2011.00050",
                "CorpusId": 226226438
            },
            "abstract": "One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. We introduce the novel concept of $\\epsilon$-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar model performance. We introduce a meta-learning algorithm called Kernel Inducing Points (KIP) for obtaining such remarkable datasets, inspired by the recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR-10 classification. Furthermore, our KIP-learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime, which leads to state of the art results for neural network dataset distillation with potential applications to privacy-preservation.",
            "referenceCount": 44,
            "citationCount": 116,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-10-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2011.00050"
            },
            "citationStyles": {
                "bibtex": "@Article{Nguyen2020DatasetMF,\n author = {Timothy Nguyen and Zhourung Chen and Jaehoon Lee},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Dataset Meta-Learning from Kernel Ridge-Regression},\n volume = {abs/2011.00050},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:82dc9a015ebcd4162a1d4bc525e032bd6a9abe90",
            "@type": "ScholarlyArticle",
            "paperId": "82dc9a015ebcd4162a1d4bc525e032bd6a9abe90",
            "corpusId": 227239008,
            "url": "https://www.semanticscholar.org/paper/82dc9a015ebcd4162a1d4bc525e032bd6a9abe90",
            "title": "Learning to Generalize Unseen Domains via Memory-based Multi-Source Meta-Learning for Person Re-Identification",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2012-00417",
                "MAG": "3108651391",
                "ArXiv": "2012.00417",
                "DOI": "10.1109/CVPR46437.2021.00621",
                "CorpusId": 227239008
            },
            "abstract": "Recent advances in person re-identification (ReID) obtain impressive accuracy in the supervised and unsupervised learning settings. However, most of the existing methods need to train a new model for a new domain by accessing data. Due to public privacy, the new domain data are not always accessible, leading to a limited applicability of these methods. In this paper, we study the problem of multi-source domain generalization in ReID, which aims to learn a model that can perform well on unseen domains with only several labeled source domains. To address this problem, we propose the Memory-based Multi-Source Meta-Learning (M3L) framework to train a generalizable model for unseen domains. Specifically, a meta-learning strategy is introduced to simulate the train-test process of domain generalization for learning more generalizable models. To overcome the unstable meta-optimization caused by the parametric classifier, we propose a memory-based identification loss that is non-parametric and harmonizes with meta-learning. We also present a meta batch normalization layer (MetaBN) to diversify meta-test features, further establishing the advantage of meta-learning. Experiments demonstrate that our M3L can effectively enhance the generalization ability of the model for unseen domains and can outperform the state-of-the-art methods on four large-scale ReID datasets.",
            "referenceCount": 57,
            "citationCount": 108,
            "influentialCitationCount": 23,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2012.00417",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-12-01",
            "journal": {
                "name": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhao2020LearningTG,\n author = {Yuyang Zhao and Zhun Zhong and Fengxiang Yang and Zhiming Luo and Yaojin Lin and Shaozi Li and N. Sebe},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {6273-6282},\n title = {Learning to Generalize Unseen Domains via Memory-based Multi-Source Meta-Learning for Person Re-Identification},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1ae1c18632d6d4d61b70c918354d633ce24c0bcd",
            "@type": "ScholarlyArticle",
            "paperId": "1ae1c18632d6d4d61b70c918354d633ce24c0bcd",
            "corpusId": 222317967,
            "url": "https://www.semanticscholar.org/paper/1ae1c18632d6d4d61b70c918354d633ce24c0bcd",
            "title": "Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3105036711",
                "DBLP": "conf/nips/PatacchiolaTCOS20",
                "CorpusId": 222317967
            },
            "abstract": "Recently, different machine learning methods have been introduced to tackle the challenging few-shot learning scenario that is, learning from a small labeled dataset related to a specific task. Common approaches have taken the form of meta-learning: learning to learn on the new problem given the old. Following the recognition that meta-learning is implementing learning in a multi-level model, we present a Bayesian treatment for the meta-learning inner loop through the use of deep kernels. As a result we can learn a kernel that transfers to new tasks; we call this Deep Kernel Transfer (DKT). This approach has many advantages: is straightforward to implement as a single optimizer, provides uncertainty quantification, and does not require estimation of task-specific parameters. We empirically demonstrate that DKT outperforms several state-of-the-art algorithms in few-shot classification, and is the state of the art for cross-domain adaptation and regression. We conclude that complex meta-learning routines can be replaced by a simpler Bayesian model without loss of accuracy.",
            "referenceCount": 48,
            "citationCount": 101,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-10-13",
            "journal": {
                "name": "arXiv: Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Patacchiola2020BayesianMF,\n author = {Massimiliano Patacchiola and Jack Turner and Elliot J. Crowley and M. O\u2019Boyle and A. Storkey},\n booktitle = {Neural Information Processing Systems},\n journal = {arXiv: Learning},\n title = {Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1bb6e907c8f03ce2daca7659accd03933e2463d5",
            "@type": "ScholarlyArticle",
            "paperId": "1bb6e907c8f03ce2daca7659accd03933e2463d5",
            "corpusId": 211096963,
            "url": "https://www.semanticscholar.org/paper/1bb6e907c8f03ce2daca7659accd03933e2463d5",
            "title": "PACOH: Bayes-Optimal Meta-Learning with PAC-Guarantees",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/icml/RothfussFJ021",
                "ArXiv": "2002.05551",
                "MAG": "3005896558",
                "CorpusId": 211096963
            },
            "abstract": "Meta-learning can successfully acquire useful inductive biases from data, especially when a large number of meta-tasks are available. Yet, its generalization properties to unseen tasks are poorly understood. Particularly if the number of meta-tasks is small, this raises concerns about overfitting. We provide a theoretical analysis using the PAC-Bayesian framework and derive novel generalization bounds for meta-learning with unbounded loss functions and Bayesian base learners. Using these bounds, we develop a class of PAC-optimal meta-learning algorithms with performance guarantees and a principled meta-regularization. When instantiating our PAC-optimal hyper-posterior (PACOH) with Gaussian processes as base learners, the resulting method consistently outperforms several popular meta-learning methods, both in terms of predictive accuracy and the quality of uncertainty estimates.",
            "referenceCount": 73,
            "citationCount": 92,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-02-13",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rothfuss2020PACOHBM,\n author = {Jonas Rothfuss and Vincent Fortuin and A. Krause},\n booktitle = {International Conference on Machine Learning},\n pages = {9116-9126},\n title = {PACOH: Bayes-Optimal Meta-Learning with PAC-Guarantees},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:96b279f5e8ef539a7ef01fd1fdfa6bc10f36a07c",
            "@type": "ScholarlyArticle",
            "paperId": "96b279f5e8ef539a7ef01fd1fdfa6bc10f36a07c",
            "corpusId": 214222435,
            "url": "https://www.semanticscholar.org/paper/96b279f5e8ef539a7ef01fd1fdfa6bc10f36a07c",
            "title": "Empirical Bayes Transductive Meta-Learning with Synthetic Gradients",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2004.12696",
                "MAG": "3020429927",
                "DBLP": "conf/iclr/HuMXSOLD20",
                "CorpusId": 214222435
            },
            "abstract": "We propose a meta-learning approach that learns from multiple tasks in a transductive setting, by leveraging unlabeled information in the query set to learn a more powerful meta-model. To develop our framework we revisit the empirical Bayes formulation for multi-task learning. The evidence lower bound of the marginal log-likelihood of empirical Bayes decomposes as a sum of local KL divergences between the variational posterior and the true posterior of each task. We derive a novel amortized variational inference that couples all the variational posteriors into a meta-model, which consists of a synthetic gradient network and an initialization network. The combination of local KL divergences and synthetic gradient network allows for backpropagating information from unlabeled data, thereby enabling transduction. Our results on the Mini-ImageNet and CIFAR-FS benchmarks for episodic few-shot classification significantly outperform previous state-of-the-art methods.",
            "referenceCount": 55,
            "citationCount": 101,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-04-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2004.12696"
            },
            "citationStyles": {
                "bibtex": "@Article{Hu2020EmpiricalBT,\n author = {S. Hu and Pablo G. Moreno and Yanghua Xiao and Xin Shen and G. Obozinski and Neil D. Lawrence and Andreas C. Damianou},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Empirical Bayes Transductive Meta-Learning with Synthetic Gradients},\n volume = {abs/2004.12696},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7b0aad12a6917b7d444ba2f87c7f8ccc5357797a",
            "@type": "ScholarlyArticle",
            "paperId": "7b0aad12a6917b7d444ba2f87c7f8ccc5357797a",
            "corpusId": 53802740,
            "url": "https://www.semanticscholar.org/paper/7b0aad12a6917b7d444ba2f87c7f8ccc5357797a",
            "title": "Meta-Learning Probabilistic Inference for Prediction",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2949517868",
                "ArXiv": "1805.09921",
                "DBLP": "conf/iclr/GordonBBNT19",
                "CorpusId": 53802740
            },
            "abstract": "This paper introduces a new framework for data efficient and versatile learning. Specifically: 1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction. ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. 2) We introduce VERSA, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. VERSA substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training. 3) We evaluate VERSA on benchmark datasets where the method sets new state-of-the-art results, handles arbitrary numbers of shots, and for classification, arbitrary numbers of classes at train and test time. The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task.",
            "referenceCount": 61,
            "citationCount": 216,
            "influentialCitationCount": 33,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-05-24",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Gordon2018MetaLearningPI,\n author = {Jonathan Gordon and J. Bronskill and M. Bauer and Sebastian Nowozin and Richard E. Turner},\n booktitle = {International Conference on Learning Representations},\n title = {Meta-Learning Probabilistic Inference for Prediction},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:56d753c5f161322470721e3441d0910f568a2031",
            "@type": "ScholarlyArticle",
            "paperId": "56d753c5f161322470721e3441d0910f568a2031",
            "corpusId": 212415221,
            "url": "https://www.semanticscholar.org/paper/56d753c5f161322470721e3441d0910f568a2031",
            "title": "Zero-Shot Cross-Lingual Transfer with Meta Learning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2003.02739",
                "DBLP": "journals/corr/abs-2003-02739",
                "ACL": "2020.emnlp-main.368",
                "MAG": "3104820280",
                "DOI": "10.18653/v1/2020.emnlp-main.368",
                "CorpusId": 212415221
            },
            "abstract": "Learning what to share between tasks has been a topic of great importance recently, as strategic sharing of knowledge has been shown to improve downstream task performance. This is particularly important for multilingual applications, as most languages in the world are under-resourced. Here, we consider the setting of training models on multiple different languages at the same time, when little or no data is available for languages other than English. We show that this challenging setup can be approached using meta-learning, where, in addition to training a source language model, another model learns to select which training instances are the most beneficial to the first. We experiment using standard supervised, zero-shot cross-lingual, as well as few-shot cross-lingual settings for different natural language understanding tasks (natural language inference, question answering). Our extensive experimental setup demonstrates the consistent effectiveness of meta-learning for a total of 15 languages. We improve upon the state-of-the-art for zero-shot and few-shot NLI (on MultiNLI and XNLI) and QA (on the MLQA dataset). A comprehensive error analysis indicates that the correlation of typological features between languages can partly explain when parameter sharing learned via meta-learning is beneficial.",
            "referenceCount": 53,
            "citationCount": 89,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.emnlp-main.368.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-03-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2003.02739"
            },
            "citationStyles": {
                "bibtex": "@Article{Nooralahzadeh2020ZeroShotCT,\n author = {F. Nooralahzadeh and Giannis Bekoulis and Johannes Bjerva and Isabelle Augenstein},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Zero-Shot Cross-Lingual Transfer with Meta Learning},\n volume = {abs/2003.02739},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2b48dd15ca88012e66d2c039879d46f79597780e",
            "@type": "ScholarlyArticle",
            "paperId": "2b48dd15ca88012e66d2c039879d46f79597780e",
            "corpusId": 215548450,
            "url": "https://www.semanticscholar.org/paper/2b48dd15ca88012e66d2c039879d46f79597780e",
            "title": "Online Meta-Learning for Multi-Source and Semi-Supervised Domain Adaptation",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3096541549",
                "ArXiv": "2004.04398",
                "DBLP": "conf/eccv/LiH20",
                "DOI": "10.1007/978-3-030-58517-4_23",
                "CorpusId": 215548450
            },
            "abstract": null,
            "referenceCount": 56,
            "citationCount": 86,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.pure.ed.ac.uk/ws/files/157540620/Online_Meta_Learning_LI_DOA02072020_AFV.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-04-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Li2020OnlineMF,\n author = {Da Li and Timothy M. Hospedales},\n booktitle = {European Conference on Computer Vision},\n pages = {382-403},\n title = {Online Meta-Learning for Multi-Source and Semi-Supervised Domain Adaptation},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a699899e9a34deab3a511b8af0dc72ea5c776759",
            "@type": "ScholarlyArticle",
            "paperId": "a699899e9a34deab3a511b8af0dc72ea5c776759",
            "corpusId": 220831405,
            "url": "https://www.semanticscholar.org/paper/a699899e9a34deab3a511b8af0dc72ea5c776759",
            "title": "La-MAML: Look-ahead Meta Learning for Continual Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/nips/GuptaYP20",
                "MAG": "3104773308",
                "ArXiv": "2007.13904",
                "CorpusId": 220831405
            },
            "abstract": "The continual learning problem involves training models with limited capacity to perform well on a set of an unknown number of sequentially arriving tasks. While meta-learning shows great potential for reducing interference between old and new tasks, the current training procedures tend to be either slow or offline, and sensitive to many hyper-parameters. In this work, we propose Look-ahead MAML (La-MAML), a fast optimisation-based meta-learning algorithm for online-continual learning, aided by a small episodic memory. Our proposed modulation of per-parameter learning rates in our meta-learning update allows us to draw connections to prior work on hypergradients and meta-descent. This provides a more flexible and efficient way to mitigate catastrophic forgetting compared to conventional prior-based methods. La-MAML achieves performance superior to other replay-based, prior-based and meta-learning based approaches for continual learning on real-world visual classification benchmarks. Source code can be found here: this https URL",
            "referenceCount": 33,
            "citationCount": 73,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-07-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2007.13904"
            },
            "citationStyles": {
                "bibtex": "@Article{Gupta2020LaMAMLLM,\n author = {Gunshi Gupta and Karmesh Yadav and L. Paull},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {La-MAML: Look-ahead Meta Learning for Continual Learning},\n volume = {abs/2007.13904},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:59467dd8020d493a52116c5be18c958fb1925717",
            "@type": "ScholarlyArticle",
            "paperId": "59467dd8020d493a52116c5be18c958fb1925717",
            "corpusId": 221761685,
            "url": "https://www.semanticscholar.org/paper/59467dd8020d493a52116c5be18c958fb1925717",
            "title": "Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3099403624",
                "DBLP": "conf/emnlp/BansalJMM20",
                "ACL": "2020.emnlp-main.38",
                "ArXiv": "2009.08445",
                "DOI": "10.18653/v1/2020.emnlp-main.38",
                "CorpusId": 221761685
            },
            "abstract": "Self-supervised pre-training of transformer models has revolutionized NLP applications. Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with fine-tuning. However, fine-tuning is still data inefficient -- when there are few labeled examples, accuracy can be low. Data efficiency can be improved by optimizing pre-training directly for future fine-tuning with few examples; this can be treated as a meta-learning problem. However, standard meta-learning techniques require many training tasks in order to generalize; unfortunately, finding a diverse set of such supervised tasks is usually difficult. This paper proposes a self-supervised approach to generate a large, rich, meta-learning task distribution from unlabeled text. This is achieved using a cloze-style objective, but creating separate multi-class classification tasks by gathering tokens-to-be blanked from among only a handful of vocabulary terms. This yields as many unique meta-training tasks as the number of subsets of vocabulary terms. We meta-train a transformer model on this distribution of tasks using a recent meta-learning framework. On 17 NLP tasks, we show that this meta-training leads to better few-shot generalization than language-model pre-training followed by finetuning. Furthermore, we show how the self-supervised tasks can be combined with supervised tasks for meta-learning, providing substantial accuracy gains over previous supervised meta-learning.",
            "referenceCount": 64,
            "citationCount": 75,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.emnlp-main.38.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-09-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bansal2020SelfSupervisedMF,\n author = {Trapit Bansal and Rishikesh Jha and Tsendsuren Munkhdalai and A. McCallum},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {522-534},\n title = {Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a30a89b94b0162ad2db2a2f70b135ce0689e13f6",
            "@type": "ScholarlyArticle",
            "paperId": "a30a89b94b0162ad2db2a2f70b135ce0689e13f6",
            "corpusId": 220363755,
            "url": "https://www.semanticscholar.org/paper/a30a89b94b0162ad2db2a2f70b135ce0689e13f6",
            "title": "Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3038665551",
                "ArXiv": "2007.02387",
                "DBLP": "journals/corr/abs-2007-02387",
                "CorpusId": 220363755
            },
            "abstract": "This paper studies few-shot relation extraction, which aims at predicting the relation for a pair of entities in a sentence by training with a few labeled examples in each relation. To more effectively generalize to new relations, in this paper we study the relationships between different relations and propose to leverage a global relation graph. We propose a novel Bayesian meta-learning approach to effectively learn the posterior distribution of the prototype vectors of relations, where the initial prior of the prototype vectors is parameterized with a graph neural network on the global relation graph. Moreover, to effectively optimize the posterior distribution of the prototype vectors, we propose to use the stochastic gradient Langevin dynamics, which is related to the MAML algorithm but is able to handle the uncertainty of the prototype vectors. The whole framework can be effectively and efficiently optimized in an end-to-end fashion. Experiments on two benchmark datasets prove the effectiveness of our proposed approach against competitive baselines in both the few-shot and zero-shot settings.",
            "referenceCount": 39,
            "citationCount": 75,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-07-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2007.02387"
            },
            "citationStyles": {
                "bibtex": "@Article{Qu2020FewshotRE,\n author = {Meng Qu and Tianyu Gao and Louis-Pascal Xhonneux and Jian Tang},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs},\n volume = {abs/2007.02387},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:06f0fe16fcb8f80bf88746c5ef6d3780531918ab",
            "@type": "ScholarlyArticle",
            "paperId": "06f0fe16fcb8f80bf88746c5ef6d3780531918ab",
            "corpusId": 29156972,
            "url": "https://www.semanticscholar.org/paper/06f0fe16fcb8f80bf88746c5ef6d3780531918ab",
            "title": "Task Agnostic Meta-Learning for Few-Shot Learning",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1805.07722",
                "DBLP": "journals/corr/abs-1805-07722",
                "MAG": "2804962156",
                "DOI": "10.1109/CVPR.2019.01199",
                "CorpusId": 29156972
            },
            "abstract": "Meta-learning approaches have been proposed to tackle the few-shot learning problem. Typically, a meta-learner is trained on a variety of tasks in the hopes of being generalizable to new tasks. However, the generalizability on new tasks of a meta-learner could be fragile when it is over-trained on existing tasks during meta-training phase. In other words, the initial model of a meta-learner could be too biased towards existing tasks to adapt to new tasks, especially when only very few examples are available to update the model. To avoid a biased meta-learner and improve its generalizability, we propose a novel paradigm of Task-Agnostic Meta-Learning (TAML) algorithms. Specifically, we present an entropy-based approach that meta-learns an unbiased initial model with the largest uncertainty over the output labels by preventing it from over-performing in classification tasks. Alternatively, a more general inequality-minimization TAML is presented for more ubiquitous scenarios by directly minimizing the inequality of initial losses beyond the classification tasks wherever a suitable loss can be defined. Experiments on benchmarked datasets demonstrate that the proposed approaches outperform compared meta-learning algorithms in both few-shot classification and reinforcement learning tasks.",
            "referenceCount": 32,
            "citationCount": 355,
            "influentialCitationCount": 23,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1805.07722",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-05-20",
            "journal": {
                "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Jamal2018TaskAM,\n author = {Muhammad Abdullah Jamal and Guo-Jun Qi and M. Shah},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {11711-11719},\n title = {Task Agnostic Meta-Learning for Few-Shot Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7",
            "@type": "ScholarlyArticle",
            "paperId": "23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7",
            "corpusId": 248377629,
            "url": "https://www.semanticscholar.org/paper/23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7",
            "title": "Skill-based Meta-Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "journals/corr/abs-2204-11828",
                "ArXiv": "2204.11828",
                "DOI": "10.48550/arXiv.2204.11828",
                "CorpusId": 248377629
            },
            "abstract": "While deep reinforcement learning methods have shown impressive results in robot learning, their sample inefficiency makes the learning of complex, long-horizon behaviors with real robot systems infeasible. To mitigate this issue, meta-reinforcement learning methods aim to enable fast learning on novel tasks by learning how to learn. Yet, the application has been limited to short-horizon tasks with dense rewards. To enable learning long-horizon behaviors, recent works have explored leveraging prior experience in the form of offline datasets without reward or task annotations. While these approaches yield improved sample efficiency, millions of interactions with environments are still required to solve complex tasks. In this work, we devise a method that enables meta-learning on long-horizon, sparse-reward tasks, allowing us to solve unseen target tasks with orders of magnitude fewer environment interactions. Our core idea is to leverage prior experience extracted from offline datasets during meta-learning. Specifically, we propose to (1) extract reusable skills and a skill prior from offline datasets, (2) meta-train a high-level policy that learns to efficiently compose learned skills into long-horizon behaviors, and (3) rapidly adapt the meta-trained policy to solve an unseen target task. Experimental results on continuous control tasks in navigation and manipulation demonstrate that the proposed method can efficiently solve long-horizon novel target tasks by combining the strengths of meta-learning and the usage of offline datasets, while prior approaches in RL, meta-RL, and multi-task RL require substantially more environment interactions to solve the tasks.",
            "referenceCount": 59,
            "citationCount": 26,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2204.11828",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-04-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2204.11828"
            },
            "citationStyles": {
                "bibtex": "@Article{Nam2022SkillbasedML,\n author = {Taewook Nam and Shao-Hua Sun and Karl Pertsch and S. Hwang and Joseph J. Lim},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Skill-based Meta-Reinforcement Learning},\n volume = {abs/2204.11828},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:001a9f506055901129c3792982469f778be5974c",
            "@type": "ScholarlyArticle",
            "paperId": "001a9f506055901129c3792982469f778be5974c",
            "corpusId": 212628682,
            "url": "https://www.semanticscholar.org/paper/001a9f506055901129c3792982469f778be5974c",
            "title": "TaskNorm: Rethinking Batch Normalization for Meta-Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2003.03284",
                "DBLP": "conf/icml/Bronskill0RNT20",
                "MAG": "3035543253",
                "CorpusId": 212628682
            },
            "abstract": "Modern meta-learning approaches for image classification rely on increasingly deep networks to achieve state-of-the-art performance, making batch normalization an essential component of meta-learning pipelines. However, the hierarchical nature of the meta-learning setting presents several challenges that can render conventional batch normalization ineffective, giving rise to the need to rethink normalization in this setting. We evaluate a range of approaches to batch normalization for meta-learning scenarios, and develop a novel approach that we call TaskNorm. Experiments on fourteen datasets demonstrate that the choice of batch normalization has a dramatic effect on both classification accuracy and training time for both gradient based and gradient-free meta-learning approaches. Importantly, TaskNorm is found to consistently improve performance. Finally, we provide a set of best practices for normalization that will allow fair comparison of meta-learning algorithms.",
            "referenceCount": 43,
            "citationCount": 75,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-03-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bronskill2020TaskNormRB,\n author = {J. Bronskill and Jonathan Gordon and James Requeima and Sebastian Nowozin and Richard E. Turner},\n booktitle = {International Conference on Machine Learning},\n pages = {1153-1164},\n title = {TaskNorm: Rethinking Batch Normalization for Meta-Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:027d05ab0c9e3ad91103eec2fe6ee2d2951a1afa",
            "@type": "ScholarlyArticle",
            "paperId": "027d05ab0c9e3ad91103eec2fe6ee2d2951a1afa",
            "corpusId": 213025655,
            "url": "https://www.semanticscholar.org/paper/027d05ab0c9e3ad91103eec2fe6ee2d2951a1afa",
            "title": "Towards Fast Adaptation of Neural Architectures with Meta Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "2994653652",
                "DBLP": "conf/iclr/LianZXLLZHG20",
                "CorpusId": 213025655
            },
            "abstract": "Recently, Neural Architecture Search (NAS) has been successfully applied to multiple artificial intelligence areas and shows better performance compared with hand-designed networks. However, the existing NAS methods only target a specific task. Most of them usually do well in searching an architecture for single task but are troublesome for multiple datasets or multiple tasks. Generally, the architecture for a new task is either searched from scratch, which is neither efficient nor flexible enough for practical application scenarios, or borrowed from the ones searched on other tasks, which might be not optimal. In order to tackle the transferability of NAS and conduct fast adaptation of neural architectures, we propose a novel Transferable Neural Architecture Search method based on meta-learning in this paper, which is termed as T-NAS. T-NAS learns a meta-architecture that is able to adapt to a new task quickly through a few gradient steps, which makes the transferred architecture suitable for the specific task. Extensive experiments show that T-NAS achieves state-of-the-art performance in few-shot learning and comparable performance in supervised learning but with 50x less searching cost, which demonstrates the effectiveness of our method.",
            "referenceCount": 32,
            "citationCount": 72,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-04-30",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Lian2020TowardsFA,\n author = {Dongze Lian and Yin Zheng and Yintao Xu and Yanxiong Lu and Leyu Lin and P. Zhao and Junzhou Huang and Shenghua Gao},\n booktitle = {International Conference on Learning Representations},\n title = {Towards Fast Adaptation of Neural Architectures with Meta Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:069a05c0ae7ac49ad7eb8e0a0744c212c58bd863",
            "@type": "ScholarlyArticle",
            "paperId": "069a05c0ae7ac49ad7eb8e0a0744c212c58bd863",
            "corpusId": 220647206,
            "url": "https://www.semanticscholar.org/paper/069a05c0ae7ac49ad7eb8e0a0744c212c58bd863",
            "title": "Meta-learning for Few-shot Natural Language Processing: A Survey",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3043372854",
                "DBLP": "journals/corr/abs-2007-09604",
                "ArXiv": "2007.09604",
                "CorpusId": 220647206
            },
            "abstract": "Few-shot natural language processing (NLP) refers to NLP tasks that are accompanied with merely a handful of labeled examples. This is a real-world challenge that an AI system must learn to handle. Usually we rely on collecting more auxiliary information or developing a more efficient learning algorithm. However, the general gradient-based optimization in high capacity models, if training from scratch, requires many parameter-updating steps over a large number of labeled examples to perform well (Snell et al., 2017). If the target task itself cannot provide more information, how about collecting more tasks equipped with rich annotations to help the model learning? The goal of meta-learning is to train a model on a variety of tasks with rich annotations, such that it can solve a new task using only a few labeled samples. The key idea is to train the model's initial parameters such that the model has maximal performance on a new task after the parameters have been updated through zero or a couple of gradient steps. There are already some surveys for meta-learning, such as (Vilalta and Drissi, 2002; Vanschoren, 2018; Hospedales et al., 2020). Nevertheless, this paper focuses on NLP domain, especially few-shot applications. We try to provide clearer definitions, progress summary and some common datasets of applying meta-learning to few-shot NLP.",
            "referenceCount": 34,
            "citationCount": 66,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-07-19",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2007.09604"
            },
            "citationStyles": {
                "bibtex": "@Article{Yin2020MetalearningFF,\n author = {Wenpeng Yin},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Meta-learning for Few-shot Natural Language Processing: A Survey},\n volume = {abs/2007.09604},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2c2b2d35ec9006ed83a034a05d316bf6f712d0f9",
            "@type": "ScholarlyArticle",
            "paperId": "2c2b2d35ec9006ed83a034a05d316bf6f712d0f9",
            "corpusId": 209832429,
            "url": "https://www.semanticscholar.org/paper/2c2b2d35ec9006ed83a034a05d316bf6f712d0f9",
            "title": "Automated Relational Meta-learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "2998139906",
                "ArXiv": "2001.00745",
                "DBLP": "journals/corr/abs-2001-00745",
                "CorpusId": 209832429
            },
            "abstract": "In order to efficiently learn with small amount of data on new tasks, meta-learning transfers knowledge learned from previous tasks to the new ones. However, a critical challenge in meta-learning is the task heterogeneity which cannot be well handled by traditional globally shared meta-learning methods. In addition, current task-specific meta-learning methods may either suffer from hand-crafted structure design or lack the capability to capture complex relations between tasks. In this paper, motivated by the way of knowledge organization in knowledge bases, we propose an automated relational meta-learning (ARML) framework that automatically extracts the cross-task relations and constructs the meta-knowledge graph. When a new task arrives, it can quickly find the most relevant structure and tailor the learned structure knowledge to the meta-learner. As a result, the proposed framework not only addresses the challenge of task heterogeneity by a learned meta-knowledge graph, but also increases the model interpretability. We conduct extensive experiments on 2D toy regression and few-shot image classification and the results demonstrate the superiority of ARML over state-of-the-art baselines.",
            "referenceCount": 36,
            "citationCount": 72,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-01-03",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2001.00745"
            },
            "citationStyles": {
                "bibtex": "@Article{Yao2020AutomatedRM,\n author = {Huaxiu Yao and Xian Wu and Zhiqiang Tao and Yaliang Li and Bolin Ding and Ruirui Li and Z. Li},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Automated Relational Meta-learning},\n volume = {abs/2001.00745},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d300ce5c990ba885ac73bdac8ec4318bbf43af25",
            "@type": "ScholarlyArticle",
            "paperId": "d300ce5c990ba885ac73bdac8ec4318bbf43af25",
            "corpusId": 219507099,
            "url": "https://www.semanticscholar.org/paper/d300ce5c990ba885ac73bdac8ec4318bbf43af25",
            "title": "Spatio-Temporal Meta Learning for Urban Traffic Prediction",
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "publicationVenue": {
                "id": "urn:research:c6840156-ee10-4d78-8832-7f8909811576",
                "name": "IEEE Transactions on Knowledge and Data Engineering",
                "alternate_names": [
                    "IEEE Trans Knowl Data Eng"
                ],
                "issn": "1041-4347",
                "url": "https://www.computer.org/web/tkde"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3026400623",
                "DBLP": "journals/tkde/PanZLZYZZ22",
                "DOI": "10.1109/tkde.2020.2995855",
                "CorpusId": 219507099
            },
            "abstract": "Predicting urban traffic is of great importance to intelligent transportation systems and public safety, yet is very challenging in three aspects: 1) complex spatio-temporal correlations of urban traffic, including spatial correlations between locations along with temporal correlations among timestamps; 2) spatial diversity of such spatio-temporal correlations, which varies from location to location and depends on the surrounding geographical information, e.g., points of interests and road networks; and 3) temporal diversity of such spatio-temporal correlations, which is highly influenced by dynamic traffic states. To tackle these challenges, we proposed a deep meta learning based model, entitled ST-MetaNet<inline-formula><tex-math notation=\"LaTeX\">$^+$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"pan-ieq1-2995855.gif\"/></alternatives></inline-formula>, to <italic>collectively</italic> predict traffic in all locations at the same time. ST-MetaNet<inline-formula><tex-math notation=\"LaTeX\">$^+$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"pan-ieq2-2995855.gif\"/></alternatives></inline-formula> employs a sequence-to-sequence architecture, consisting of an encoder to learn historical information and a decoder to make predictions step by step. Specifically, the encoder and decoder have the same network structure, consisting of meta graph attention networks and meta recurrent neural networks, to capture diverse spatial and temporal correlations, respectively. Furthermore, the weights (parameters) of meta graph attention networks and meta recurrent neural networks are generated from the embeddings of geo-graph attributes and the traffic context learned from dynamic traffic states. Extensive experiments were conducted based on three real-world datasets to illustrate the effectiveness of ST-MetaNet<inline-formula><tex-math notation=\"LaTeX\">$^+$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"pan-ieq3-2995855.gif\"/></alternatives></inline-formula> beyond several state-of-the-art methods.",
            "referenceCount": 51,
            "citationCount": 74,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-05-19",
            "journal": {
                "name": "IEEE Transactions on Knowledge and Data Engineering",
                "volume": "34"
            },
            "citationStyles": {
                "bibtex": "@Article{Pan2020SpatioTemporalML,\n author = {Zheyi Pan and Wentao Zhang and Yuxuan Liang and Weinan Zhang and Yong Yu and Junbo Zhang and Yu Zheng},\n booktitle = {IEEE Transactions on Knowledge and Data Engineering},\n journal = {IEEE Transactions on Knowledge and Data Engineering},\n pages = {1462-1476},\n title = {Spatio-Temporal Meta Learning for Urban Traffic Prediction},\n volume = {34},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:132a8547513c1a2a1e10e827282d50683d6542ce",
            "@type": "ScholarlyArticle",
            "paperId": "132a8547513c1a2a1e10e827282d50683d6542ce",
            "corpusId": 218971733,
            "url": "https://www.semanticscholar.org/paper/132a8547513c1a2a1e10e827282d50683d6542ce",
            "title": "Few-Shot Open-Set Recognition Using Meta-Learning",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3035654071",
                "ArXiv": "2005.13713",
                "DBLP": "conf/cvpr/LiuKL0V20",
                "DOI": "10.1109/cvpr42600.2020.00882",
                "CorpusId": 218971733
            },
            "abstract": "The problem of open-set recognition is considered. While previous approaches only consider this problem in the context of large-scale classifier training, we seek a unified solution for this and the low-shot classification setting. It is argued that the classic softmax classifier is a poor solution for open-set recognition, since it tends to overfit on the training classes. Randomization is then proposed as a solution to this problem. This suggests the use of meta-learning techniques, commonly used for few-shot classification, for the solution of open-set recognition. A new oPen sEt mEta LEaRning (PEELER) algorithm is then introduced. This combines the random selection of a set of novel classes per episode, a loss that maximizes the posterior entropy for examples of those classes, and a new metric learning formulation based on the Mahalanobis distance. Experimental results show that PEELER achieves state of the art open set recognition performance for both few-shot and large-scale recognition. On CIFAR and miniImageNet, it achieves substantial gains in seen/unseen class detection AUROC for a given seen-class classification accuracy.",
            "referenceCount": 40,
            "citationCount": 63,
            "influentialCitationCount": 22,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2005.13713",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-05-27",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2020FewShotOR,\n author = {Bo Liu and Hao Kang and Haoxiang Li and G. Hua and N. Vasconcelos},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {8795-8804},\n title = {Few-Shot Open-Set Recognition Using Meta-Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:79b21b5ee081cdc0b791c2cce4a5d47046ac5b4a",
            "@type": "ScholarlyArticle",
            "paperId": "79b21b5ee081cdc0b791c2cce4a5d47046ac5b4a",
            "corpusId": 213005255,
            "url": "https://www.semanticscholar.org/paper/79b21b5ee081cdc0b791c2cce4a5d47046ac5b4a",
            "title": "Incremental Object Detection via Meta-Learning",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2003-08798",
                "MAG": "3011311310",
                "ArXiv": "2003.08798",
                "DOI": "10.1109/TPAMI.2021.3124133",
                "CorpusId": 213005255,
                "PubMed": "34727027"
            },
            "abstract": "In a real-world setting, object instances from new classes can be continuously encountered by object detectors. When existing object detectors are applied to such scenarios, their performance on old classes deteriorates significantly. A few efforts have been reported to address this limitation, all of which apply variants of knowledge distillation to avoid catastrophic forgetting. We note that although distillation helps to retain previous learning, it obstructs fast adaptability to new tasks, which is a critical requirement for incremental learning. In this pursuit, we propose a meta-learning approach that learns to reshape model gradients, such that information across incremental tasks is optimally shared. This ensures a seamless information transfer via a meta-learned gradient preconditioning that minimizes forgetting and maximizes knowledge transfer. In comparison to existing meta-learning methods, our approach is task-agnostic, allows incremental addition of new-classes and scales to high-capacity models for object detection. We evaluate our approach on a variety of incremental learning settings defined on PASCAL-VOC and MS COCO datasets, where our approach performs favourably well against state-of-the-art methods. Code and trained models: https://github.com/JosephKJ/iOD.",
            "referenceCount": 75,
            "citationCount": 57,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://raiith.iith.ac.in/10325/1/IEEE_Transactions_.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-03-17",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "44"
            },
            "citationStyles": {
                "bibtex": "@Article{Joseph2020IncrementalOD,\n author = {K. J. Joseph and Jathushan Rajasegaran and Salman Hameed Khan and F. Khan and V. Balasubramanian and Ling Shao},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {9209-9216},\n title = {Incremental Object Detection via Meta-Learning},\n volume = {44},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:03542a713c3b92bc09b3a9ccda20c84846910544",
            "@type": "ScholarlyArticle",
            "paperId": "03542a713c3b92bc09b3a9ccda20c84846910544",
            "corpusId": 222341683,
            "url": "https://www.semanticscholar.org/paper/03542a713c3b92bc09b3a9ccda20c84846910544",
            "title": "Data Augmentation for Meta-Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2010-07092",
                "MAG": "3092742756",
                "ArXiv": "2010.07092",
                "CorpusId": 222341683
            },
            "abstract": "Conventional image classifiers are trained by randomly sampling mini-batches of images. To achieve state-of-the-art performance, sophisticated data augmentation schemes are used to expand the amount of training data available for sampling. In contrast, meta-learning algorithms sample not only images, but classes as well. We investigate how data augmentation can be used not only to expand the number of images available per class, but also to generate entirely new classes. We systematically dissect the meta-learning pipeline and investigate the distinct ways in which data augmentation can be integrated at both the image and class levels. Our proposed meta-specific data augmentation significantly improves the performance of meta-learners on few-shot classification benchmarks.",
            "referenceCount": 34,
            "citationCount": 58,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-10-14",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ni2020DataAF,\n author = {Renkun Ni and Micah Goldblum and Amr Sharaf and Kezhi Kong and T. Goldstein},\n booktitle = {International Conference on Machine Learning},\n pages = {8152-8161},\n title = {Data Augmentation for Meta-Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ed77af97d637dd44fcecc359abb0669d71fc876d",
            "@type": "ScholarlyArticle",
            "paperId": "ed77af97d637dd44fcecc359abb0669d71fc876d",
            "corpusId": 220480943,
            "url": "https://www.semanticscholar.org/paper/ed77af97d637dd44fcecc359abb0669d71fc876d",
            "title": "Multi-attention Meta Learning for Few-shot Fine-grained Image Recognition",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3034978645",
                "DBLP": "conf/ijcai/ZhuLJ20",
                "DOI": "10.24963/ijcai.2020/152",
                "CorpusId": 220480943
            },
            "abstract": "The goal of few-shot image recognition is to distinguish different categories with only one or a few training samples. Previous works of few-shot learning mainly work on general object images. And current solutions usually learn a global image representation from training tasks to adapt novel tasks. However, fine-gained categories are distinguished by subtle and local parts, which could not be captured by global representations effectively. This may hinder existing few-shot learning approaches from dealing with fine-gained categories well. In this work, we propose a multi-attention meta-learning (MattML) method for few-shot fine-grained image recognition (FSFGIR). Instead of using only base learner for general feature learning, the proposed meta-learning method uses attention mechanisms of the base learner and task learner to capture discriminative parts of images. The base learner is equipped with two convolutional block attention modules (CBAM) and a classifier. The two CBAM can learn diverse and informative parts. And the initial weights of classifier are attended by the task learner, which gives the classifier a task-related sensitive initialization. For adaptation, the gradient-based meta-learning approach is employed by updating the parameters of two CBAM and the attended classifier, which facilitates the updated base learner to adaptively focus on discriminative parts. We experimentally analyze the different components of our method, and experimental results on four benchmark datasets demonstrate the effectiveness and superiority of our method.",
            "referenceCount": 37,
            "citationCount": 61,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ijcai.org/proceedings/2020/0152.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-07-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhu2020MultiattentionML,\n author = {Yaohui Zhu and Chenlong Liu and Shuqiang Jiang},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {1090-1096},\n title = {Multi-attention Meta Learning for Few-shot Fine-grained Image Recognition},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f8562e48844bba90c55a505a93aa1ac652839a45",
            "@type": "ScholarlyArticle",
            "paperId": "f8562e48844bba90c55a505a93aa1ac652839a45",
            "corpusId": 210157044,
            "url": "https://www.semanticscholar.org/paper/f8562e48844bba90c55a505a93aa1ac652839a45",
            "title": "A Collaborative Learning Framework via Federated Meta-Learning",
            "venue": "IEEE International Conference on Distributed Computing Systems",
            "publicationVenue": {
                "id": "urn:research:ffe5bb5c-04ed-488e-985d-d3a7b39542cf",
                "name": "IEEE International Conference on Distributed Computing Systems",
                "alternate_names": [
                    "International Conference on Distributed Computing Systems",
                    "IEEE Int Conf Distrib Comput Syst",
                    "Int Conf Device Circuit Syst",
                    "ICDCS",
                    "Int Conf Distrib Comput Syst",
                    "International Conference on Devices, Circuits and Systems"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000213/all-proceedings"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2001.03229",
                "MAG": "3021521239",
                "DBLP": "journals/corr/abs-2001-03229",
                "DOI": "10.1109/ICDCS47774.2020.00032",
                "CorpusId": 210157044
            },
            "abstract": "Many IoT applications at the network edge demand intelligent decisions in a real-time manner. The edge device alone, however, often cannot achieve real-time edge intelligence due to its constrained computing resources and limited local data. To tackle these challenges, we propose a platform-aided collaborative learning framework where a model is first trained across a set of source edge nodes by a federated meta-learning approach, and then it is rapidly adapted to learn a new task at the target edge node, using a few samples only. Further, we investigate the convergence of the proposed federated meta-learning algorithm under mild conditions on node similarity and the adaptation performance at the target edge. To combat against the vulnerability of meta-learning algorithms to possible adversarial attacks, we further propose a robust version of the federated meta-learning algorithm based on distributionally robust optimization, and establish its convergence under mild conditions. Experiments on different datasets demonstrate the effectiveness of the proposed Federated Meta-Learning based framework.",
            "referenceCount": 26,
            "citationCount": 61,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-01-09",
            "journal": {
                "name": "2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lin2020ACL,\n author = {Sen Lin and Guang Yang and Junshan Zhang},\n booktitle = {IEEE International Conference on Distributed Computing Systems},\n journal = {2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)},\n pages = {289-299},\n title = {A Collaborative Learning Framework via Federated Meta-Learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:759af360f450ef76d903a907e8a39b10845cdeaf",
            "@type": "ScholarlyArticle",
            "paperId": "759af360f450ef76d903a907e8a39b10845cdeaf",
            "corpusId": 220496701,
            "url": "https://www.semanticscholar.org/paper/759af360f450ef76d903a907e8a39b10845cdeaf",
            "title": "Meta-Learning Requires Meta-Augmentation",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/nips/RajendranIJ20",
                "ArXiv": "2007.05549",
                "MAG": "3040863728",
                "CorpusId": 220496701
            },
            "abstract": "Meta-learning algorithms aim to learn two components: a model that predicts targets for a task, and a base learner that quickly updates that model when given examples from a new task. This additional level of learning can be powerful, but it also creates another potential source for overfitting, since we can now overfit in either the model or the base learner. We describe both of these forms of metalearning overfitting, and demonstrate that they appear experimentally in common meta-learning benchmarks. We then use an information-theoretic framework to discuss meta-augmentation, a way to add randomness that discourages the base learner and model from learning trivial solutions that do not generalize to new tasks. We demonstrate that meta-augmentation produces large complementary benefits to recently proposed meta-regularization techniques.",
            "referenceCount": 41,
            "citationCount": 61,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-07-10",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2007.05549"
            },
            "citationStyles": {
                "bibtex": "@Article{Rajendran2020MetaLearningRM,\n author = {Janarthanan Rajendran and A. Irpan and Eric Jang},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Meta-Learning Requires Meta-Augmentation},\n volume = {abs/2007.05549},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d4c4a5f0c71eba13d2827b24f70bbfdc3bd858ec",
            "@type": "ScholarlyArticle",
            "paperId": "d4c4a5f0c71eba13d2827b24f70bbfdc3bd858ec",
            "corpusId": 3350728,
            "url": "https://www.semanticscholar.org/paper/d4c4a5f0c71eba13d2827b24f70bbfdc3bd858ec",
            "title": "Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963580001",
                "DBLP": "conf/icml/LeeC18",
                "CorpusId": 3350728
            },
            "abstract": "Gradient-based meta-learning methods leverage gradient descent to learn the commonalities among various tasks. While previous such methods have been successful in meta-learning tasks, they resort to simple gradient descent during meta-testing. Our primary contribution is the {\\em MT-net}, which enables the meta-learner to learn on each layer's activation space a subspace that the task-specific learner performs gradient descent on. Additionally, a task-specific learner of an {\\em MT-net} performs gradient descent with respect to a meta-learned distance metric, which warps the activation space to be more sensitive to task identity. We demonstrate that the dimension of this learned subspace reflects the complexity of the task-specific learner's adaptation task, and also that our model is less sensitive to the choice of initial learning rates than previous gradient-based meta-learning methods. Our method achieves state-of-the-art or comparable performance on few-shot classification and regression tasks.",
            "referenceCount": 37,
            "citationCount": 298,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-01-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lee2018GradientBasedMW,\n author = {Yoonho Lee and Seungjin Choi},\n booktitle = {International Conference on Machine Learning},\n pages = {2933-2942},\n title = {Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f4eff7c0127a2ef92c441f028c3bb15b64cabcc8",
            "@type": "ScholarlyArticle",
            "paperId": "f4eff7c0127a2ef92c441f028c3bb15b64cabcc8",
            "corpusId": 3618072,
            "url": "https://www.semanticscholar.org/paper/f4eff7c0127a2ef92c441f028c3bb15b64cabcc8",
            "title": "One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning",
            "venue": "Robotics: Science and Systems",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1802.01557",
                "DBLP": "journals/corr/abs-1802-01557",
                "MAG": "2963703448",
                "DOI": "10.15607/RSS.2018.XIV.002",
                "CorpusId": 3618072
            },
            "abstract": "Humans and animals are capable of learning a new behavior by observing others perform the skill just once. We consider the problem of allowing a robot to do the same -- learning from a raw video pixels of a human, even when there is substantial domain shift in the perspective, environment, and embodiment between the robot and the observed human. Prior approaches to this problem have hand-specified how human and robot actions correspond and often relied on explicit human pose detection systems. In this work, we present an approach for one-shot learning from a video of a human by using human and robot demonstration data from a variety of previous tasks to build up prior knowledge through meta-learning. Then, combining this prior knowledge and only a single video demonstration from a human, the robot can perform the task that the human demonstrated. We show experiments on both a PR2 arm and a Sawyer arm, demonstrating that after meta-learning, the robot can learn to place, push, and pick-and-place new objects using just one video of a human performing the manipulation.",
            "referenceCount": 65,
            "citationCount": 306,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.15607/rss.2018.xiv.002",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.01557"
            },
            "citationStyles": {
                "bibtex": "@Article{Yu2018OneShotIF,\n author = {Tianhe Yu and Chelsea Finn and A. Xie and Sudeep Dasari and Tianhao Zhang and P. Abbeel and S. Levine},\n booktitle = {Robotics: Science and Systems},\n journal = {ArXiv},\n title = {One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning},\n volume = {abs/1802.01557},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fc65f4c043e16834c95cbb55a51531a11b605ed7",
            "@type": "ScholarlyArticle",
            "paperId": "fc65f4c043e16834c95cbb55a51531a11b605ed7",
            "corpusId": 235390900,
            "url": "https://www.semanticscholar.org/paper/fc65f4c043e16834c95cbb55a51531a11b605ed7",
            "title": "Improving Generalization in Meta-learning via Task Augmentation",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/icml/YaoHZ0TZHL21",
                "ArXiv": "2007.13040",
                "CorpusId": 235390900
            },
            "abstract": "Meta-learning has proven to be a powerful paradigm for transferring the knowledge from previous tasks to facilitate the learning of a novel task. Current dominant algorithms train a well-generalized model initialization which is adapted to each task via the support set. The crux lies in optimizing the generalization capability of the initialization, which is measured by the performance of the adapted model on the query set of each task. Unfortunately, this generalization measure, evidenced by empirical results, pushes the initialization to overfit the meta-training tasks, which significantly impairs the generalization and adaptation to novel tasks. To address this issue, we actively augment a meta-training task with\"more data\"when evaluating the generalization. Concretely, we propose two task augmentation methods, including MetaMix and Channel Shuffle. MetaMix linearly combines features and labels of samples from both the support and query sets. For each class of samples, Channel Shuffle randomly replaces a subset of their channels with the corresponding ones from a different class. Theoretical studies show how task augmentation improves the generalization of meta-learning. Moreover, both MetaMix and Channel Shuffle outperform state-of-the-art results by a large margin across many datasets and are compatible with existing meta-learning algorithms.",
            "referenceCount": 51,
            "citationCount": 59,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-07-26",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yao2020ImprovingGI,\n author = {Huaxiu Yao and Long-Kai Huang and Linjun Zhang and Ying Wei and Li Tian and James Y. Zou and Junzhou Huang and Z. Li},\n booktitle = {International Conference on Machine Learning},\n pages = {11887-11897},\n title = {Improving Generalization in Meta-learning via Task Augmentation},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8617b063cae6c682ba257389c432d9d76b013289",
            "@type": "ScholarlyArticle",
            "paperId": "8617b063cae6c682ba257389c432d9d76b013289",
            "corpusId": 222115317,
            "url": "https://www.semanticscholar.org/paper/8617b063cae6c682ba257389c432d9d76b013289",
            "title": "Few-Shot Classification of Aerial Scene Images via Meta-Learning",
            "venue": "Remote Sensing",
            "publicationVenue": {
                "id": "urn:research:8e1bd4b5-d5b2-4e22-ba0a-01fe5568d472",
                "name": "Remote Sensing",
                "alternate_names": [
                    "Remote Sens"
                ],
                "issn": "2315-4675",
                "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-169233"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/remotesensing/ZhangBWBL21",
                "MAG": "3098643429",
                "DOI": "10.20944/preprints202010.0033.v1",
                "CorpusId": 222115317
            },
            "abstract": "Convolutional neural network (CNN) based methods have dominated the field of aerial scene classification for the past few years. While achieving remarkable success, CNN-based methods suffer from excessive parameters and notoriously rely on large amounts of training data. In this work, we introduce few-shot learning to the aerial scene classification problem. Few-shot learning aims to learn a model on base-set that can quickly adapt to unseen categories in novel-set, using only a few labeled samples. To this end, we proposed a meta-learning method for few-shot classification of aerial scene images. First, we train a feature extractor on all base categories to learn a representation of inputs. Then in the meta-training stage, the classifier is optimized in the metric space by cosine distance with a learnable scale parameter. At last, in the meta-testing stage, the query sample in the unseen category is predicted by the adapted classifier given a few support samples. We conduct extensive experiments on two challenging datasets: NWPU-RESISC45 and RSD46-WHU. The experimental results show that our method yields state-of-the-art performance. Furthermore, several ablation experiments are conducted to investigate the effects of dataset scale, the impact of different metrics and the number of support shots; the experiment results confirm that our model is specifically effective in few-shot settings.",
            "referenceCount": 56,
            "citationCount": 63,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2072-4292/13/1/108/pdf?version=1609400239",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-10-02",
            "journal": {
                "name": "Remote. Sens.",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2020FewShotCO,\n author = {Pei Zhang and Ying Li and Dong Wang and Yunpeng Bai and Bendu Bai},\n booktitle = {Remote Sensing},\n journal = {Remote. Sens.},\n pages = {108},\n title = {Few-Shot Classification of Aerial Scene Images via Meta-Learning},\n volume = {13},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:baba60a1c57833ee88ff7585f68a0f02d0efa29f",
            "@type": "ScholarlyArticle",
            "paperId": "baba60a1c57833ee88ff7585f68a0f02d0efa29f",
            "corpusId": 211132835,
            "url": "https://www.semanticscholar.org/paper/baba60a1c57833ee88ff7585f68a0f02d0efa29f",
            "title": "Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/icml/GoldblumRFNCG20",
                "ArXiv": "2002.06753",
                "MAG": "3006505669",
                "CorpusId": 211132835
            },
            "abstract": "Meta-learning algorithms produce feature extractors which achieve state-of-the-art performance on few-shot classification. While the literature is rich with meta-learning methods, little is known about why the resulting feature extractors perform so well. We develop a better understanding of the underlying mechanics of meta-learning and the difference between models trained using meta-learning and models which are trained classically. In doing so, we introduce and verify several hypotheses for why meta-learned models perform better. Furthermore, we develop a regularizer which boosts the performance of standard training routines for few-shot classification. In many cases, our routine outperforms meta-learning while simultaneously running an order of magnitude faster.",
            "referenceCount": 27,
            "citationCount": 60,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-02-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2002.06753"
            },
            "citationStyles": {
                "bibtex": "@Article{Goldblum2020UnravelingMU,\n author = {Micah Goldblum and Steven Reich and Liam H. Fowl and Renkun Ni and Valeriia Cherepanova and T. Goldstein},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks},\n volume = {abs/2002.06753},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2e6a8914745319cae682b807a6b4ae470b08c54a",
            "@type": "ScholarlyArticle",
            "paperId": "2e6a8914745319cae682b807a6b4ae470b08c54a",
            "corpusId": 211066252,
            "url": "https://www.semanticscholar.org/paper/2e6a8914745319cae682b807a6b4ae470b08c54a",
            "title": "Meta-learning framework with applications to zero-shot time-series forecasting",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2002.02887",
                "MAG": "3005326476",
                "DBLP": "journals/corr/abs-2002-02887",
                "DOI": "10.1609/aaai.v35i10.17115",
                "CorpusId": 211066252
            },
            "abstract": "Can meta-learning discover generic ways of processing time series (TS) from a diverse dataset so as to greatly improve generalization on new TS coming from different datasets? This work provides positive evidence to this using a broad meta-learning framework which we show subsumes many existing meta-learning algorithms. Our theoretical analysis suggests that residual connections act as a meta-learning adaptation mechanism, generating a subset of task-specific parameters based on a given TS input, thus gradually expanding the expressive power of the architecture on-the-fly. The same mechanism is shown via linearization analysis to have the interpretation of a sequential update of the final linear layer. Our empirical results on a wide range of data emphasize the importance of the identified meta-learning mechanisms for successful zero-shot univariate forecasting, suggesting that it is viable to train a neural network on a source TS dataset and deploy it on a different target TS dataset without retraining, resulting in performance that is at least as good as that of state-of-practice univariate forecasting models.",
            "referenceCount": 77,
            "citationCount": 57,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/17115/16922",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-02-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2002.02887"
            },
            "citationStyles": {
                "bibtex": "@Article{Oreshkin2020MetalearningFW,\n author = {Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Meta-learning framework with applications to zero-shot time-series forecasting},\n volume = {abs/2002.02887},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:445afbb169bb88414b9b48e64fb687b923c98ee8",
            "@type": "ScholarlyArticle",
            "paperId": "445afbb169bb88414b9b48e64fb687b923c98ee8",
            "corpusId": 153312504,
            "url": "https://www.semanticscholar.org/paper/445afbb169bb88414b9b48e64fb687b923c98ee8",
            "title": "Hierarchically Structured Meta-learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2946757877",
                "DBLP": "conf/icml/YaoWHL19",
                "ArXiv": "1905.05301",
                "CorpusId": 153312504
            },
            "abstract": "In order to learn quickly with few samples, meta-learning utilizes prior knowledge learned from previous tasks. However, a critical challenge in meta-learning is task uncertainty and heterogeneity, which can not be handled via globally sharing knowledge among tasks. In this paper, based on gradient-based meta-learning, we propose a hierarchically structured meta-learning (HSML) algorithm that explicitly tailors the transferable knowledge to different clusters of tasks. Inspired by the way human beings organize knowledge, we resort to a hierarchical task clustering structure to cluster tasks. As a result, the proposed approach not only addresses the challenge via the knowledge customization to different clusters of tasks, but also preserves knowledge generalization among a cluster of similar tasks. To tackle the changing of task relationship, in addition, we extend the hierarchical structure to a continual learning environment. The experimental results show that our approach can achieve state-of-the-art performance in both toy-regression and few-shot image classification problems.",
            "referenceCount": 48,
            "citationCount": 167,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-05-13",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1905.05301"
            },
            "citationStyles": {
                "bibtex": "@Article{Yao2019HierarchicallySM,\n author = {Huaxiu Yao and Ying Wei and Junzhou Huang and Z. Li},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Hierarchically Structured Meta-learning},\n volume = {abs/1905.05301},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bbc1fa2c9c54d8916469f413fdceb6d4087267a4",
            "@type": "ScholarlyArticle",
            "paperId": "bbc1fa2c9c54d8916469f413fdceb6d4087267a4",
            "corpusId": 165163819,
            "url": "https://www.semanticscholar.org/paper/bbc1fa2c9c54d8916469f413fdceb6d4087267a4",
            "title": "Personalizing Dialogue Agents via Meta-Learning",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/acl/MadottoLWF19",
                "ArXiv": "1905.10033",
                "ACL": "P19-1542",
                "MAG": "2949557686",
                "DOI": "10.18653/v1/P19-1542",
                "CorpusId": 165163819
            },
            "abstract": "Existing personalized dialogue models use human designed persona descriptions to improve dialogue consistency. Collecting such descriptions from existing dialogues is expensive and requires hand-crafted feature designs. In this paper, we propose to extend Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) to personalized dialogue learning without using any persona descriptions. Our model learns to quickly adapt to new personas by leveraging only a few dialogue samples collected from the same user, which is fundamentally different from conditioning the response on the persona descriptions. Empirical results on Persona-chat dataset (Zhang et al., 2018) indicate that our solution outperforms non-meta-learning baselines using automatic evaluation metrics, and in terms of human-evaluated fluency and consistency.",
            "referenceCount": 40,
            "citationCount": 153,
            "influentialCitationCount": 22,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/P19-1542.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-05-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lin2019PersonalizingDA,\n author = {Zhaojiang Lin and Andrea Madotto and Chien-Sheng Wu and Pascale Fung},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {5454-5459},\n title = {Personalizing Dialogue Agents via Meta-Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e13ca1bd60af3325afc64dc09979e3322818e365",
            "@type": "ScholarlyArticle",
            "paperId": "e13ca1bd60af3325afc64dc09979e3322818e365",
            "corpusId": 202539544,
            "url": "https://www.semanticscholar.org/paper/e13ca1bd60af3325afc64dc09979e3322818e365",
            "title": "Meta-Learning with Warped Gradient Descent",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2971857369",
                "ArXiv": "1909.00025",
                "DBLP": "journals/corr/abs-1909-00025",
                "CorpusId": 202539544
            },
            "abstract": "Learning an efficient update rule from data that promotes rapid learning of new tasks from the same distribution remains an open problem in meta-learning. Typically, previous works have approached this issue either by attempting to train a neural network that directly produces updates or by attempting to learn better initialisations or scaling factors for a gradient-based update rule. Both of these approaches pose challenges. On one hand, directly producing an update forgoes a useful inductive bias and can easily lead to non-converging behaviour. On the other hand, approaches that try to control a gradient-based update rule typically resort to computing gradients through the learning process to obtain their meta-gradients, leading to methods that can not scale beyond few-shot task adaptation. In this work, we propose Warped Gradient Descent (WarpGrad), a method that intersects these approaches to mitigate their limitations. WarpGrad meta-learns an efficiently parameterised preconditioning matrix that facilitates gradient descent across the task distribution. Preconditioning arises by interleaving non-linear layers, referred to as warp-layers, between the layers of a task-learner. Warp-layers are meta-learned without backpropagating through the task training process in a manner similar to methods that learn to directly produce updates. WarpGrad is computationally efficient, easy to implement, and can scale to arbitrarily large meta-learning problems. We provide a geometrical interpretation of the approach and evaluate its effectiveness in a variety of settings, including few-shot, standard supervised, continual and reinforcement learning.",
            "referenceCount": 82,
            "citationCount": 181,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-08-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1909.00025"
            },
            "citationStyles": {
                "bibtex": "@Article{Flennerhag2019MetaLearningWW,\n author = {Sebastian Flennerhag and Andrei A. Rusu and Razvan Pascanu and Hujun Yin and R. Hadsell},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Meta-Learning with Warped Gradient Descent},\n volume = {abs/1909.00025},\n year = {2019}\n}\n"
            }
        }
    }
]