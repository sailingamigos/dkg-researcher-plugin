[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
            "@type": "ScholarlyArticle",
            "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
            "corpusId": 202786778,
            "url": "https://www.semanticscholar.org/paper/3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
            "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2970971581",
                "DBLP": "conf/nips/PaszkeGMLBCKLGA19",
                "ArXiv": "1912.01703",
                "CorpusId": 202786778
            },
            "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.",
            "referenceCount": 39,
            "citationCount": 28593,
            "influentialCitationCount": 2964,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-12-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Paszke2019PyTorchAI,\n author = {Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and N. Gimelshein and L. Antiga and Alban Desmaison and Andreas K\u00f6pf and E. Yang and Zach DeVito and Martin Raison and A. Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},\n booktitle = {Neural Information Processing Systems},\n pages = {8024-8035},\n title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7aa38b85fa8cba64d6a4010543f6695dbf5f1386",
            "@type": "ScholarlyArticle",
            "paperId": "7aa38b85fa8cba64d6a4010543f6695dbf5f1386",
            "corpusId": 3488815,
            "url": "https://www.semanticscholar.org/paper/7aa38b85fa8cba64d6a4010543f6695dbf5f1386",
            "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/iclr/MadryMSTV18",
                "ArXiv": "1706.06083",
                "MAG": "2952649158",
                "CorpusId": 3488815
            },
            "abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
            "referenceCount": 44,
            "citationCount": 8776,
            "influentialCitationCount": 2954,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-06-19",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1706.06083"
            },
            "citationStyles": {
                "bibtex": "@Article{Madry2017TowardsDL,\n author = {A. Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Towards Deep Learning Models Resistant to Adversarial Attacks},\n volume = {abs/1706.06083},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3813b88a4ec3c63919df47e9694b577f4691f7e5",
            "@type": "ScholarlyArticle",
            "paperId": "3813b88a4ec3c63919df47e9694b577f4691f7e5",
            "corpusId": 195811894,
            "url": "https://www.semanticscholar.org/paper/3813b88a4ec3c63919df47e9694b577f4691f7e5",
            "title": "A survey on Image Data Augmentation for Deep Learning",
            "venue": "Journal of Big Data",
            "publicationVenue": {
                "id": "urn:research:d60da343-ab92-4310-b3d7-2c0860287a9d",
                "name": "Journal of Big Data",
                "alternate_names": [
                    "J Big Data",
                    "Journal on Big Data"
                ],
                "issn": "2196-1115",
                "url": "http://www.journalofbigdata.com/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/jbd/ShortenK19",
                "MAG": "2954996726",
                "DOI": "10.1186/s40537-019-0197-0",
                "CorpusId": 195811894
            },
            "abstract": null,
            "referenceCount": 142,
            "citationCount": 5801,
            "influentialCitationCount": 160,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journalofbigdata.springeropen.com/track/pdf/10.1186/s40537-019-0197-0",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-07-06",
            "journal": {
                "name": "Journal of Big Data",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Shorten2019ASO,\n author = {Connor Shorten and T. Khoshgoftaar},\n booktitle = {Journal of Big Data},\n journal = {Journal of Big Data},\n pages = {1-48},\n title = {A survey on Image Data Augmentation for Deep Learning},\n volume = {6},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a4cec122a08216fe8a3bc19b22e78fbaea096256",
            "@type": "ScholarlyArticle",
            "paperId": "a4cec122a08216fe8a3bc19b22e78fbaea096256",
            "corpusId": 3074096,
            "url": "https://www.semanticscholar.org/paper/a4cec122a08216fe8a3bc19b22e78fbaea096256",
            "title": "Deep Learning",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/nature/LeCunBH15",
                "MAG": "2557283755",
                "DOI": "10.1038/nature14539",
                "CorpusId": 3074096,
                "PubMed": "26017442"
            },
            "abstract": null,
            "referenceCount": 820,
            "citationCount": 58244,
            "influentialCitationCount": 2325,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2015-05-27",
            "journal": {
                "name": "Nature",
                "volume": "521"
            },
            "citationStyles": {
                "bibtex": "@Article{LeCun2015DeepL,\n author = {Yann LeCun and Yoshua Bengio and Geoffrey E. Hinton},\n booktitle = {Nature},\n journal = {Nature},\n pages = {436-444},\n title = {Deep Learning},\n volume = {521},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
            "@type": "ScholarlyArticle",
            "paperId": "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
            "corpusId": 160705,
            "url": "https://www.semanticscholar.org/paper/f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
            "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2964059111",
                "DBLP": "conf/icml/GalG16",
                "ArXiv": "1506.02142",
                "CorpusId": 160705
            },
            "abstract": "Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.",
            "referenceCount": 56,
            "citationCount": 6984,
            "influentialCitationCount": 1195,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-06-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Gal2015DropoutAA,\n author = {Y. Gal and Zoubin Ghahramani},\n booktitle = {International Conference on Machine Learning},\n pages = {1050-1059},\n title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5b6ec746d309b165f9f9def873a2375b6fb40f3d",
            "@type": "ScholarlyArticle",
            "paperId": "5b6ec746d309b165f9f9def873a2375b6fb40f3d",
            "corpusId": 2375110,
            "url": "https://www.semanticscholar.org/paper/5b6ec746d309b165f9f9def873a2375b6fb40f3d",
            "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/cvpr/Chollet17",
                "ArXiv": "1610.02357",
                "MAG": "2531409750",
                "DOI": "10.1109/CVPR.2017.195",
                "CorpusId": 2375110
            },
            "abstract": "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",
            "referenceCount": 27,
            "citationCount": 10862,
            "influentialCitationCount": 1756,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1610.02357",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-10-07",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chollet2016XceptionDL,\n author = {Fran\u00e7ois Chollet},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {1800-1807},\n title = {Xception: Deep Learning with Depthwise Separable Convolutions},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e9a986c8ff6c2f381d026fe014f6aaa865f34da7",
            "@type": "ScholarlyArticle",
            "paperId": "e9a986c8ff6c2f381d026fe014f6aaa865f34da7",
            "corpusId": 207241585,
            "url": "https://www.semanticscholar.org/paper/e9a986c8ff6c2f381d026fe014f6aaa865f34da7",
            "title": "Deep Learning with Differential Privacy",
            "venue": "Conference on Computer and Communications Security",
            "publicationVenue": {
                "id": "urn:research:73f7fe95-b68b-468f-b7ba-3013ca879e50",
                "name": "Conference on Computer and Communications Security",
                "alternate_names": [
                    "Int Workshop Cogn Cell Syst",
                    "CCS",
                    "Comput Commun Secur",
                    "CcS",
                    "International Symposium on Community-centric Systems",
                    "International Workshop on Cognitive Cellular Systems",
                    "Conf Comput Commun Secur",
                    "Comb Comput Sci",
                    "Int Symp Community-centric Syst",
                    "Combinatorics and Computer Science",
                    "Circuits, Signals, and Systems",
                    "Computer and Communications Security",
                    "Circuit Signal Syst"
                ],
                "issn": null,
                "url": "https://dl.acm.org/conference/ccs"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "3098586851",
                "ArXiv": "1607.00133",
                "DBLP": "conf/ccs/AbadiCGMMT016",
                "DOI": "10.1145/2976749.2978318",
                "CorpusId": 207241585
            },
            "abstract": "Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.",
            "referenceCount": 63,
            "citationCount": 4269,
            "influentialCitationCount": 963,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1607.00133",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-07-01",
            "journal": {
                "name": "Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Abadi2016DeepLW,\n author = {Mart\u00edn Abadi and Andy Chu and I. Goodfellow and H. B. McMahan and Ilya Mironov and Kunal Talwar and Li Zhang},\n booktitle = {Conference on Computer and Communications Security},\n journal = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},\n title = {Deep Learning with Differential Privacy},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4",
            "@type": "ScholarlyArticle",
            "paperId": "6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4",
            "corpusId": 459456,
            "url": "https://www.semanticscholar.org/paper/6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4",
            "title": "Deep Learning Face Attributes in the Wild",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "1834627138",
                "ArXiv": "1411.7766",
                "DBLP": "journals/corr/LiuLWT14",
                "DOI": "10.1109/ICCV.2015.425",
                "CorpusId": 459456
            },
            "abstract": "Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.",
            "referenceCount": 44,
            "citationCount": 6600,
            "influentialCitationCount": 1748,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1411.7766",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-11-27",
            "journal": {
                "name": "2015 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2014DeepLF,\n author = {Ziwei Liu and Ping Luo and Xiaogang Wang and Xiaoou Tang},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2015 IEEE International Conference on Computer Vision (ICCV)},\n pages = {3730-3738},\n title = {Deep Learning Face Attributes in the Wild},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d997beefc0922d97202789d2ac307c55c2c52fba",
            "@type": "ScholarlyArticle",
            "paperId": "d997beefc0922d97202789d2ac307c55c2c52fba",
            "corpusId": 5115938,
            "url": "https://www.semanticscholar.org/paper/d997beefc0922d97202789d2ac307c55c2c52fba",
            "title": "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2950642167",
                "DBLP": "conf/cvpr/QiSMG17",
                "ArXiv": "1612.00593",
                "DOI": "10.1109/CVPR.2017.16",
                "CorpusId": 5115938
            },
            "abstract": "Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.",
            "referenceCount": 34,
            "citationCount": 10247,
            "influentialCitationCount": 2735,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1612.00593",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-12-02",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Qi2016PointNetDL,\n author = {C. Qi and Hao Su and Kaichun Mo and L. Guibas},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {77-85},\n title = {PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d86084808994ac54ef4840ae65295f3c0ec4decd",
            "@type": "ScholarlyArticle",
            "paperId": "d86084808994ac54ef4840ae65295f3c0ec4decd",
            "corpusId": 57379996,
            "url": "https://www.semanticscholar.org/paper/d86084808994ac54ef4840ae65295f3c0ec4decd",
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "Journal of Computational Physics",
            "publicationVenue": {
                "id": "urn:research:58606051-2e63-4034-8496-9d4ed773bb49",
                "name": "Journal of Computational Physics",
                "alternate_names": [
                    "J Comput Phys"
                ],
                "issn": "0021-9991",
                "url": "http://www.elsevier.com/locate/jcp"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/jcphy/RaissiPK19",
                "MAG": "2899283552",
                "DOI": "10.1016/j.jcp.2018.10.045",
                "CorpusId": 57379996
            },
            "abstract": null,
            "referenceCount": 48,
            "citationCount": 5499,
            "influentialCitationCount": 523,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://manuscript.elsevier.com/S0021999118307125/pdf/S0021999118307125.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-02-01",
            "journal": {
                "name": "J. Comput. Phys.",
                "volume": "378"
            },
            "citationStyles": {
                "bibtex": "@Article{Raissi2019PhysicsinformedNN,\n author = {M. Raissi and P. Perdikaris and G. Karniadakis},\n booktitle = {Journal of Computational Physics},\n journal = {J. Comput. Phys.},\n pages = {686-707},\n title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},\n volume = {378},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4f8d648c52edf74e41b0996128aa536e13cc7e82",
            "@type": "ScholarlyArticle",
            "paperId": "4f8d648c52edf74e41b0996128aa536e13cc7e82",
            "corpusId": 1779661,
            "url": "https://www.semanticscholar.org/paper/4f8d648c52edf74e41b0996128aa536e13cc7e82",
            "title": "Deep Learning",
            "venue": "International Journal of Semantic Computing",
            "publicationVenue": {
                "id": "urn:research:fe4efc6e-695e-432e-8463-5dd08835dfce",
                "name": "International Journal of Semantic Computing",
                "alternate_names": [
                    "Int J Semantic Comput"
                ],
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/escri/HaoZ17",
                "DOI": "10.1142/S1793351X16500045",
                "CorpusId": 1779661
            },
            "abstract": null,
            "referenceCount": 2,
            "citationCount": 23226,
            "influentialCitationCount": 1201,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Int. J. Semantic Comput.",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Hao2016DeepL,\n author = {Xing Hao and Guigang Zhang and Shang Ma},\n booktitle = {International Journal of Semantic Computing},\n journal = {Int. J. Semantic Comput.},\n pages = {417-},\n title = {Deep Learning},\n volume = {10},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5c45a5d05ac564adb67811eeb9d41d6460c70135",
            "@type": "ScholarlyArticle",
            "paperId": "5c45a5d05ac564adb67811eeb9d41d6460c70135",
            "corpusId": 26657811,
            "url": "https://www.semanticscholar.org/paper/5c45a5d05ac564adb67811eeb9d41d6460c70135",
            "title": "Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs.",
            "venue": "Journal of the American Medical Association (JAMA)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2557738935",
                "DOI": "10.1001/jama.2016.17216",
                "CorpusId": 26657811,
                "PubMed": "27898976"
            },
            "abstract": "Importance\nDeep learning is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior, removing the need to specify rules explicitly. Application of these methods to medical imaging requires further assessment and validation.\n\n\nObjective\nTo apply deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs.\n\n\nDesign and Setting\nA specific type of neural network optimized for image classification called a deep convolutional neural network was trained using a retrospective development data set of 128\u202f175 retinal images, which were graded 3 to 7 times for diabetic retinopathy, diabetic macular edema, and image gradability by a panel of 54 US licensed ophthalmologists and ophthalmology senior residents between May and December 2015. The resultant algorithm was validated in January and February 2016 using 2 separate data sets, both graded by at least 7 US board-certified ophthalmologists with high intragrader consistency.\n\n\nExposure\nDeep learning-trained algorithm.\n\n\nMain Outcomes and Measures\nThe sensitivity and specificity of the algorithm for detecting referable diabetic retinopathy (RDR), defined as moderate and worse diabetic retinopathy, referable diabetic macular edema, or both, were generated based on the reference standard of the majority decision of the ophthalmologist panel. The algorithm was evaluated at 2 operating points selected from the development set, one selected for high specificity and another for high sensitivity.\n\n\nResults\nThe EyePACS-1 data set consisted of 9963 images from 4997 patients (mean age, 54.4 years; 62.2% women; prevalence of RDR, 683/8878 fully gradable images [7.8%]); the Messidor-2 data set had 1748 images from 874 patients (mean age, 57.6 years; 42.6% women; prevalence of RDR, 254/1745 fully gradable images [14.6%]). For detecting RDR, the algorithm had an area under the receiver operating curve of 0.991 (95% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95% CI, 0.986-0.995) for Messidor-2. Using the first operating cut point with high specificity, for EyePACS-1, the sensitivity was 90.3% (95% CI, 87.5%-92.7%) and the specificity was 98.1% (95% CI, 97.8%-98.5%). For Messidor-2, the sensitivity was 87.0% (95% CI, 81.1%-91.0%) and the specificity was 98.5% (95% CI, 97.7%-99.1%). Using a second operating point with high sensitivity in the development set, for EyePACS-1 the sensitivity was 97.5% and specificity was 93.4% and for Messidor-2 the sensitivity was 96.1% and specificity was 93.9%.\n\n\nConclusions and Relevance\nIn this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.",
            "referenceCount": 28,
            "citationCount": 4860,
            "influentialCitationCount": 144,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jamanetwork.com/journals/jama/articlepdf/2588763/joi160132.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Study",
                "JournalArticle"
            ],
            "publicationDate": "2016-12-13",
            "journal": {
                "name": "JAMA",
                "volume": "316 22"
            },
            "citationStyles": {
                "bibtex": "@Article{Gulshan2016DevelopmentAV,\n author = {Varun Gulshan and L. Peng and Marc Coram and Martin C. Stumpe and Derek J. Wu and Arunachalam Narayanaswamy and Subhashini Venugopalan and Kasumi Widner and T. Madams and Jorge A Cuadros and R. Kim and R. Raman and Philip Nelson and J. Mega and D. Webster},\n booktitle = {Journal of the American Medical Association (JAMA)},\n journal = {JAMA},\n pages = {\n          2402-2410\n        },\n title = {Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs.},\n volume = {316 22},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:54ddb00fa691728944fd8becea90a373d21597cf",
            "@type": "ScholarlyArticle",
            "paperId": "54ddb00fa691728944fd8becea90a373d21597cf",
            "corpusId": 6212000,
            "url": "https://www.semanticscholar.org/paper/54ddb00fa691728944fd8becea90a373d21597cf",
            "title": "Understanding deep learning requires rethinking generalization",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2950220847",
                "ArXiv": "1611.03530",
                "DBLP": "journals/corr/ZhangBHRV16",
                "CorpusId": 6212000
            },
            "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. \nThrough extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. \nWe interpret our experimental findings by comparison with traditional models.",
            "referenceCount": 34,
            "citationCount": 4203,
            "influentialCitationCount": 345,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.03530"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2016UnderstandingDL,\n author = {Chiyuan Zhang and Samy Bengio and Moritz Hardt and B. Recht and Oriol Vinyals},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Understanding deep learning requires rethinking generalization},\n volume = {abs/1611.03530},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f28e387d4229c5f690ce4570a391c0f47e7155c7",
            "@type": "ScholarlyArticle",
            "paperId": "f28e387d4229c5f690ce4570a391c0f47e7155c7",
            "corpusId": 227947847,
            "url": "https://www.semanticscholar.org/paper/f28e387d4229c5f690ce4570a391c0f47e7155c7",
            "title": "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation",
            "venue": "Nature Methods",
            "publicationVenue": {
                "id": "urn:research:099483df-e8f2-4bee-805d-8a69f07b6cbf",
                "name": "Nature Methods",
                "alternate_names": [
                    "Nat Method"
                ],
                "issn": "1548-7091",
                "url": "http://www.nature.com/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3112701542",
                "DOI": "10.1038/s41592-020-01008-z",
                "CorpusId": 227947847,
                "PubMed": "33288961"
            },
            "abstract": null,
            "referenceCount": 58,
            "citationCount": 2238,
            "influentialCitationCount": 214,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1904.08128",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-12-07",
            "journal": {
                "name": "Nature Methods",
                "volume": "18"
            },
            "citationStyles": {
                "bibtex": "@Article{Isensee2020nnUNetAS,\n author = {Fabian Isensee and P. Jaeger and Simon A. A. Kohl and Jens Petersen and Klaus Maier-Hein},\n booktitle = {Nature Methods},\n journal = {Nature Methods},\n pages = {203 - 211},\n title = {nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation},\n volume = {18},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6ff909c6fe089fc8ebfc64eca0f0c3cc34ba277f",
            "@type": "ScholarlyArticle",
            "paperId": "6ff909c6fe089fc8ebfc64eca0f0c3cc34ba277f",
            "corpusId": 2088679,
            "url": "https://www.semanticscholar.org/paper/6ff909c6fe089fc8ebfc64eca0f0c3cc34ba277f",
            "title": "A survey on deep learning in medical image analysis",
            "venue": "Medical Image Anal.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1702.05747",
                "DBLP": "journals/corr/LitjensKBSCGLGS17",
                "MAG": "2592929672",
                "DOI": "10.1016/j.media.2017.07.005",
                "CorpusId": 2088679,
                "PubMed": "28778026"
            },
            "abstract": null,
            "referenceCount": 401,
            "citationCount": 8413,
            "influentialCitationCount": 187,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://repository.ubn.ru.nl//bitstream/handle/2066/179538/179538.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2017-02-19",
            "journal": {
                "name": "Medical image analysis",
                "volume": "42"
            },
            "citationStyles": {
                "bibtex": "@Article{Litjens2017ASO,\n author = {G. Litjens and Thijs Kooi and B. Bejnordi and A. Setio and F. Ciompi and Mohsen Ghafoorian and J. Laak and B. Ginneken and C. I. S\u00e1nchez},\n booktitle = {Medical Image Anal.},\n journal = {Medical image analysis},\n pages = {\n          60-88\n        },\n title = {A survey on deep learning in medical image analysis},\n volume = {42},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0084f3cb0a1754272151c5268a783f24bf5676a0",
            "@type": "ScholarlyArticle",
            "paperId": "0084f3cb0a1754272151c5268a783f24bf5676a0",
            "corpusId": 232434552,
            "url": "https://www.semanticscholar.org/paper/0084f3cb0a1754272151c5268a783f24bf5676a0",
            "title": "Review of deep learning: concepts, CNN architectures, challenges, applications, future directions",
            "venue": "Journal of Big Data",
            "publicationVenue": {
                "id": "urn:research:d60da343-ab92-4310-b3d7-2c0860287a9d",
                "name": "Journal of Big Data",
                "alternate_names": [
                    "J Big Data",
                    "Journal on Big Data"
                ],
                "issn": "2196-1115",
                "url": "http://www.journalofbigdata.com/"
            },
            "year": 2021,
            "externalIds": {
                "PubMedCentral": "8010506",
                "DBLP": "journals/jbd/AlzubaidiZHADAS21a",
                "DOI": "10.1186/s40537-021-00444-8",
                "CorpusId": 232434552,
                "PubMed": "33816053"
            },
            "abstract": null,
            "referenceCount": 351,
            "citationCount": 1810,
            "influentialCitationCount": 81,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journalofbigdata.springeropen.com/counter/pdf/10.1186/s40537-021-00444-8",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-03-31",
            "journal": {
                "name": "Journal of Big Data",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Alzubaidi2021ReviewOD,\n author = {Laith Alzubaidi and Jinglan Zhang and Amjad J. Humaidi and Ayad Al-dujaili and Y. Duan and O. Al-Shamma and J. Santamar\u00eda and M. Fadhel and Muthana Al-Amidie and Laith Farhan},\n booktitle = {Journal of Big Data},\n journal = {Journal of Big Data},\n title = {Review of deep learning: concepts, CNN architectures, challenges, applications, future directions},\n volume = {8},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ff7bcaa4556cb13fc7bf03e477172493546172cd",
            "@type": "ScholarlyArticle",
            "paperId": "ff7bcaa4556cb13fc7bf03e477172493546172cd",
            "corpusId": 71134,
            "url": "https://www.semanticscholar.org/paper/ff7bcaa4556cb13fc7bf03e477172493546172cd",
            "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/KendallG17",
                "MAG": "2600383743",
                "ArXiv": "1703.04977",
                "CorpusId": 71134
            },
            "abstract": "There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.",
            "referenceCount": 41,
            "citationCount": 3601,
            "influentialCitationCount": 480,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1703.04977"
            },
            "citationStyles": {
                "bibtex": "@Article{Kendall2017WhatUD,\n author = {Alex Kendall and Y. Gal},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?},\n volume = {abs/1703.04977},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:193edd20cae92c6759c18ce93eeea96afd9528eb",
            "@type": "ScholarlyArticle",
            "paperId": "193edd20cae92c6759c18ce93eeea96afd9528eb",
            "corpusId": 11715509,
            "url": "https://www.semanticscholar.org/paper/193edd20cae92c6759c18ce93eeea96afd9528eb",
            "title": "Deep learning in neural networks: An overview",
            "venue": "Neural Networks",
            "publicationVenue": {
                "id": "urn:research:a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                "name": "Neural Networks",
                "alternate_names": [
                    "Neural Netw"
                ],
                "issn": "0893-6080",
                "url": "http://www.elsevier.com/locate/neunet"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2076063813",
                "ArXiv": "1404.7828",
                "DBLP": "journals/nn/Schmidhuber15",
                "DOI": "10.1016/j.neunet.2014.09.003",
                "CorpusId": 11715509,
                "PubMed": "25462637"
            },
            "abstract": null,
            "referenceCount": 995,
            "citationCount": 14499,
            "influentialCitationCount": 460,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1404.7828",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2014-04-30",
            "journal": {
                "name": "Neural networks : the official journal of the International Neural Network Society",
                "volume": "61"
            },
            "citationStyles": {
                "bibtex": "@Article{Schmidhuber2014DeepLI,\n author = {J. Schmidhuber},\n booktitle = {Neural Networks},\n journal = {Neural networks : the official journal of the International Neural Network Society},\n pages = {\n          85-117\n        },\n title = {Deep learning in neural networks: An overview},\n volume = {61},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6001895c2fcf69528d01306e9d293d9d2a4cc67b",
            "@type": "ScholarlyArticle",
            "paperId": "6001895c2fcf69528d01306e9d293d9d2a4cc67b",
            "corpusId": 4748395,
            "url": "https://www.semanticscholar.org/paper/6001895c2fcf69528d01306e9d293d9d2a4cc67b",
            "title": "DeepLabCut: markerless pose estimation of user-defined body parts with deep learning",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2797291297",
                "DBLP": "journals/corr/abs-1804-03142",
                "ArXiv": "1804.03142",
                "DOI": "10.1038/s41593-018-0209-y",
                "CorpusId": 4748395,
                "PubMed": "30127430"
            },
            "abstract": null,
            "referenceCount": 81,
            "citationCount": 2189,
            "influentialCitationCount": 259,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-04-09",
            "journal": {
                "name": "Nature Neuroscience",
                "volume": "21"
            },
            "citationStyles": {
                "bibtex": "@Article{Mathis2018DeepLabCutMP,\n author = {Alexander Mathis and Pranav Mamidanna and Kevin M. Cury and Taiga Abe and V. Murthy and Mackenzie W. Mathis and M. Bethge},\n booktitle = {Nature Neuroscience},\n journal = {Nature Neuroscience},\n pages = {1281 - 1289},\n title = {DeepLabCut: markerless pose estimation of user-defined body parts with deep learning},\n volume = {21},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3a58efcc4558727cc5c131c44923635da4524f33",
            "@type": "ScholarlyArticle",
            "paperId": "3a58efcc4558727cc5c131c44923635da4524f33",
            "corpusId": 46935302,
            "url": "https://www.semanticscholar.org/paper/3a58efcc4558727cc5c131c44923635da4524f33",
            "title": "Relational inductive biases, deep learning, and graph networks",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1806.01261",
                "DBLP": "journals/corr/abs-1806-01261",
                "MAG": "2805516822",
                "CorpusId": 46935302
            },
            "abstract": "Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. \nThe following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between \"hand-engineering\" and \"end-to-end\" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.",
            "referenceCount": 200,
            "citationCount": 2470,
            "influentialCitationCount": 244,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-06-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.01261"
            },
            "citationStyles": {
                "bibtex": "@Article{Battaglia2018RelationalIB,\n author = {P. Battaglia and Jessica B. Hamrick and V. Bapst and Alvaro Sanchez-Gonzalez and V. Zambaldi and Mateusz Malinowski and A. Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and \u00c7aglar G\u00fcl\u00e7ehre and H. F. Song and A. J. Ballard and J. Gilmer and George E. Dahl and Ashish Vaswani and Kelsey R. Allen and C. Nash and Victoria Langston and Chris Dyer and N. Heess and Daan Wierstra and Pushmeet Kohli and M. Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Relational inductive biases, deep learning, and graph networks},\n volume = {abs/1806.01261},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:819167ace2f0caae7745d2f25a803979be5fbfae",
            "@type": "ScholarlyArticle",
            "paperId": "819167ace2f0caae7745d2f25a803979be5fbfae",
            "corpusId": 7004303,
            "url": "https://www.semanticscholar.org/paper/819167ace2f0caae7745d2f25a803979be5fbfae",
            "title": "The Limitations of Deep Learning in Adversarial Settings",
            "venue": "European Symposium on Security and Privacy",
            "publicationVenue": {
                "id": "urn:research:4c2b8cb8-e51c-4ece-9122-89595989b56f",
                "name": "European Symposium on Security and Privacy",
                "alternate_names": [
                    "EuroS&P",
                    "IEEE European Symposium on Security and Privacy",
                    "Eur Symp Secur Priv",
                    "IEEE Eur Symp Secur Priv",
                    "EUROS&P"
                ],
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1511.07528",
                "DBLP": "conf/eurosp/PapernotMJFCS16",
                "MAG": "2949152835",
                "DOI": "10.1109/EuroSP.2016.36",
                "CorpusId": 7004303
            },
            "abstract": "Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.",
            "referenceCount": 45,
            "citationCount": 3408,
            "influentialCitationCount": 432,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1511.07528",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-24",
            "journal": {
                "name": "2016 IEEE European Symposium on Security and Privacy (EuroS&P)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Papernot2015TheLO,\n author = {Nicolas Papernot and P. Mcdaniel and S. Jha and Matt Fredrikson and Z. B. Celik and A. Swami},\n booktitle = {European Symposium on Security and Privacy},\n journal = {2016 IEEE European Symposium on Security and Privacy (EuroS&P)},\n pages = {372-387},\n title = {The Limitations of Deep Learning in Adversarial Settings},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:aa7bfd2304201afbb19971ebde87b17e40242e91",
            "@type": "ScholarlyArticle",
            "paperId": "aa7bfd2304201afbb19971ebde87b17e40242e91",
            "corpusId": 10940950,
            "url": "https://www.semanticscholar.org/paper/aa7bfd2304201afbb19971ebde87b17e40242e91",
            "title": "On the importance of initialization and momentum in deep learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "conf/icml/SutskeverMDH13",
                "MAG": "104184427",
                "CorpusId": 10940950
            },
            "abstract": "Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. \n \nOur success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.",
            "referenceCount": 32,
            "citationCount": 4327,
            "influentialCitationCount": 504,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-06-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sutskever2013OnTI,\n author = {Ilya Sutskever and James Martens and George E. Dahl and Geoffrey E. Hinton},\n booktitle = {International Conference on Machine Learning},\n pages = {1139-1147},\n title = {On the importance of initialization and momentum in deep learning},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7998468d99ab07bb982294d1c9b53a3bf3934fa6",
            "@type": "ScholarlyArticle",
            "paperId": "7998468d99ab07bb982294d1c9b53a3bf3934fa6",
            "corpusId": 49862415,
            "url": "https://www.semanticscholar.org/paper/7998468d99ab07bb982294d1c9b53a3bf3934fa6",
            "title": "Object Detection With Deep Learning: A Review",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1807-05511",
                "MAG": "2884367402",
                "ArXiv": "1807.05511",
                "DOI": "10.1109/TNNLS.2018.2876865",
                "CorpusId": 49862415,
                "PubMed": "30703038"
            },
            "abstract": "Due to object detection\u2019s close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.",
            "referenceCount": 254,
            "citationCount": 2938,
            "influentialCitationCount": 104,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1807.05511",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-07-15",
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "30"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhao2018ObjectDW,\n author = {Zhong-Qiu Zhao and Peng Zheng and Shou-tao Xu and Xindong Wu},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {3212-3232},\n title = {Object Detection With Deep Learning: A Review},\n volume = {30},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8674494bd7a076286b905912d26d47f7501c4046",
            "@type": "ScholarlyArticle",
            "paperId": "8674494bd7a076286b905912d26d47f7501c4046",
            "corpusId": 1745976,
            "url": "https://www.semanticscholar.org/paper/8674494bd7a076286b905912d26d47f7501c4046",
            "title": "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/nips/QiYSG17",
                "MAG": "2963121255",
                "ArXiv": "1706.02413",
                "CorpusId": 1745976
            },
            "abstract": "Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.",
            "referenceCount": 34,
            "citationCount": 7467,
            "influentialCitationCount": 1733,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-06-07",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Qi2017PointNetDH,\n author = {C. Qi and L. Yi and Hao Su and L. Guibas},\n booktitle = {Neural Information Processing Systems},\n pages = {5099-5108},\n title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c",
            "@type": "ScholarlyArticle",
            "paperId": "2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c",
            "corpusId": 231991101,
            "url": "https://www.semanticscholar.org/paper/2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c",
            "title": "Understanding deep learning (still) requires rethinking generalization",
            "venue": "Communications of the ACM",
            "publicationVenue": {
                "id": "urn:research:4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
                "name": "Communications of the ACM",
                "alternate_names": [
                    "Commun ACM",
                    "Communications of The ACM"
                ],
                "issn": "0001-0782",
                "url": "http://www.acm.org/pubs/cacm/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/cacm/ZhangBHRV21",
                "DOI": "10.1145/3446776",
                "CorpusId": 231991101
            },
            "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models. We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.",
            "referenceCount": 44,
            "citationCount": 1151,
            "influentialCitationCount": 46,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3446776",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-02-22",
            "journal": {
                "name": "Communications of the ACM",
                "volume": "64"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2021UnderstandingDL,\n author = {Chiyuan Zhang and Samy Bengio and Moritz Hardt and B. Recht and Oriol Vinyals},\n booktitle = {Communications of the ACM},\n journal = {Communications of the ACM},\n pages = {107 - 115},\n title = {Understanding deep learning (still) requires rethinking generalization},\n volume = {64},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:657fbf29ea0b4904a3e98d1556f9acf38dddae5f",
            "@type": "ScholarlyArticle",
            "paperId": "657fbf29ea0b4904a3e98d1556f9acf38dddae5f",
            "corpusId": 3352400,
            "url": "https://www.semanticscholar.org/paper/657fbf29ea0b4904a3e98d1556f9acf38dddae5f",
            "title": "Wide & Deep Learning for Recommender Systems",
            "venue": "DLRS@RecSys",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/recsys/Cheng0HSCAACCIA16",
                "MAG": "2951581544",
                "ArXiv": "1606.07792",
                "DOI": "10.1145/2988450.2988454",
                "CorpusId": 3352400
            },
            "abstract": "Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.",
            "referenceCount": 8,
            "citationCount": 2887,
            "influentialCitationCount": 406,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://dl.acm.org/ft_gateway.cfm?id=2988454&type=pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book"
            ],
            "publicationDate": "2016-06-24",
            "journal": {
                "name": "Proceedings of the 1st Workshop on Deep Learning for Recommender Systems",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Cheng2016WideD,\n author = {Heng-Tze Cheng and L. Koc and Jeremiah Harmsen and Tal Shaked and Tushar Chandra and H. Aradhye and Glen Anderson and G. Corrado and Wei Chai and M. Ispir and Rohan Anil and Zakaria Haque and Lichan Hong and Vihan Jain and Xiaobing Liu and Hemal Shah},\n booktitle = {DLRS@RecSys},\n journal = {Proceedings of the 1st Workshop on Deep Learning for Recommender Systems},\n title = {Wide & Deep Learning for Recommender Systems},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:712e32e2da67428ba6c6add1605410e1c3792883",
            "@type": "ScholarlyArticle",
            "paperId": "712e32e2da67428ba6c6add1605410e1c3792883",
            "corpusId": 210702798,
            "url": "https://www.semanticscholar.org/paper/712e32e2da67428ba6c6add1605410e1c3792883",
            "title": "Image Segmentation Using Deep Learning: A Survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2001.05566",
                "DBLP": "journals/corr/abs-2001-05566",
                "MAG": "2999607073",
                "DOI": "10.1109/TPAMI.2021.3059968",
                "CorpusId": 210702798,
                "PubMed": "33596172"
            },
            "abstract": "Image segmentation is a key task in computer vision and image processing with important applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among others, and numerous segmentation algorithms are found in the literature. Against this backdrop, the broad success of deep learning (DL) has prompted the development of new image segmentation approaches leveraging DL models. We provide a comprehensive review of this recent literature, covering the spectrum of pioneering efforts in semantic and instance segmentation, including convolutional pixel-labeling networks, encoder-decoder architectures, multiscale and pyramid-based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the relationships, strengths, and challenges of these DL-based segmentation models, examine the widely used datasets, compare performances, and discuss promising research directions.",
            "referenceCount": 205,
            "citationCount": 1620,
            "influentialCitationCount": 43,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2001.05566",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-01-15",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "44"
            },
            "citationStyles": {
                "bibtex": "@Article{Minaee2020ImageSU,\n author = {Shervin Minaee and Yuri Boykov and F. Porikli and A. Plaza and N. Kehtarnavaz and Demetri Terzopoulos},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {3523-3542},\n title = {Image Segmentation Using Deep Learning: A Survey},\n volume = {44},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b2114228411d367cfa6ca091008291f250a2c490",
            "@type": "ScholarlyArticle",
            "paperId": "b2114228411d367cfa6ca091008291f250a2c490",
            "corpusId": 61156451,
            "url": "https://www.semanticscholar.org/paper/b2114228411d367cfa6ca091008291f250a2c490",
            "title": "Deep learning and process understanding for data-driven Earth system science",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2913323966",
                "DBLP": "journals/nature/ReichsteinCSJDC19",
                "DOI": "10.1038/s41586-019-0912-1",
                "CorpusId": 61156451,
                "PubMed": "30760912"
            },
            "abstract": null,
            "referenceCount": 101,
            "citationCount": 2034,
            "influentialCitationCount": 46,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://pure.mpg.de/pubman/item/item_3029184_9/component/file_3282959/BGC3001P.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Geology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2019-02-01",
            "journal": {
                "name": "Nature",
                "volume": "566"
            },
            "citationStyles": {
                "bibtex": "@Article{Reichstein2019DeepLA,\n author = {M. Reichstein and Gustau Camps-Valls and B. Stevens and M. Jung and Joachim Denzler and N. Carvalhais and Prabhat},\n booktitle = {Nature},\n journal = {Nature},\n pages = {195 - 204},\n title = {Deep learning and process understanding for data-driven Earth system science},\n volume = {566},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a33fe07eb62a9f29f7073060a91d792ff50eaf1a",
            "@type": "ScholarlyArticle",
            "paperId": "a33fe07eb62a9f29f7073060a91d792ff50eaf1a",
            "corpusId": 1287651,
            "url": "https://www.semanticscholar.org/paper/a33fe07eb62a9f29f7073060a91d792ff50eaf1a",
            "title": "Privacy-Preserving Deep Learning: A Comprehensive Survey",
            "venue": "SpringerBriefs on Cyber Security Systems and Networks",
            "publicationVenue": {
                "id": "urn:research:ebbd9eef-9bbb-4c22-8f0e-1f6db8fc7ed5",
                "name": "SpringerBriefs on Cyber Security Systems and Networks",
                "alternate_names": [
                    "Springerbriefs Cyber Secur Syst Netw"
                ],
                "issn": "2522-5561",
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "books/sp/KimT21",
                "DOI": "10.1007/978-981-16-3764-3",
                "CorpusId": 1287651
            },
            "abstract": null,
            "referenceCount": 261,
            "citationCount": 1017,
            "influentialCitationCount": 68,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-981-16-3764-3/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Privacy-Preserving Deep Learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Kim2021PrivacyPreservingDL,\n author = {Kwangjo Kim and Harry Chandra Tanuwidjaja},\n booktitle = {SpringerBriefs on Cyber Security Systems and Networks},\n journal = {Privacy-Preserving Deep Learning},\n title = {Privacy-Preserving Deep Learning: A Comprehensive Survey},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:33e280f61bc4d83cd23566c9d3520d78b6a355f9",
            "@type": "ScholarlyArticle",
            "paperId": "33e280f61bc4d83cd23566c9d3520d78b6a355f9",
            "corpusId": 210221987,
            "url": "https://www.semanticscholar.org/paper/33e280f61bc4d83cd23566c9d3520d78b6a355f9",
            "title": "Improved protein structure prediction using potentials from deep learning",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "2999044305",
                "DBLP": "journals/nature/Senior0JKSGQZNB20",
                "DOI": "10.1038/s41586-019-1923-7",
                "CorpusId": 210221987,
                "PubMed": "31942072"
            },
            "abstract": null,
            "referenceCount": 57,
            "citationCount": 1960,
            "influentialCitationCount": 77,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://discovery.ucl.ac.uk/10089234/1/343019_3_art_0_py4t4l_convrt.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-01-01",
            "journal": {
                "name": "Nature",
                "volume": "577"
            },
            "citationStyles": {
                "bibtex": "@Article{Senior2020ImprovedPS,\n author = {A. Senior and Richard Evans and J. Jumper and J. Kirkpatrick and L. Sifre and Tim Green and Chongli Qin and Augustin Z\u00eddek and A. W. R. Nelson and Alex Bridgland and Hugo Penedones and Stig Petersen and K. Simonyan and Steve Crossan and Pushmeet Kohli and David T. Jones and David Silver and K. Kavukcuoglu and D. Hassabis},\n booktitle = {Nature},\n journal = {Nature},\n pages = {706-710},\n title = {Improved protein structure prediction using potentials from deep learning},\n volume = {577},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:364128bcce9836d60e685bb717b80f30e25092e0",
            "@type": "ScholarlyArticle",
            "paperId": "364128bcce9836d60e685bb717b80f30e25092e0",
            "corpusId": 3516426,
            "url": "https://www.semanticscholar.org/paper/364128bcce9836d60e685bb717b80f30e25092e0",
            "title": "Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning",
            "venue": "Cell",
            "publicationVenue": {
                "id": "urn:research:e4782337-db2d-4ab2-8eda-a71d1c60709b",
                "name": "Cell",
                "alternate_names": [
                    "La Cellule"
                ],
                "issn": "0092-8674",
                "url": "https://www.cell.com/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2788633781",
                "DOI": "10.1016/j.cell.2018.02.010",
                "CorpusId": 3516426,
                "PubMed": "29474911"
            },
            "abstract": null,
            "referenceCount": 22,
            "citationCount": 2783,
            "influentialCitationCount": 206,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cell.com/article/S0092867418301545/pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Biology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-22",
            "journal": {
                "name": "Cell",
                "volume": "172"
            },
            "citationStyles": {
                "bibtex": "@Article{Kermany2018IdentifyingMD,\n author = {Daniel S. Kermany and Daniel S. Kermany and M. Goldbaum and Wenjia Cai and C. Valentim and Huiying Liang and Sally L. Baxter and A. McKeown and Ge Yang and Xiaokang Wu and Fangbing Yan and J. Dong and Made K. Prasadha and Jacqueline Pei and Jacqueline Pei and M. Y. Ting and Jie Zhu and Christina M. Li and Sierra Hewett and Sierra Hewett and Jason Dong and Ian Ziyar and Alexander Shi and Runze Zhang and Lianghong Zheng and Rui Hou and W. Shi and Xin Fu and Xin Fu and Yaou Duan and V. N. Huu and V. N. Huu and Cindy Wen and Edward Zhang and Edward Zhang and Charlotte L. Zhang and Charlotte L. Zhang and Oulan Li and Oulan Li and Xiaobo Wang and M. Singer and Xiaodong Sun and Jie Xu and A. Tafreshi and M. Lewis and H. Xia and Kang Zhang},\n booktitle = {Cell},\n journal = {Cell},\n pages = {1122-1131.e9},\n title = {Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning},\n volume = {172},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1342c1e1684620c019972e2679d5131f1e8a4a13",
            "@type": "ScholarlyArticle",
            "paperId": "1342c1e1684620c019972e2679d5131f1e8a4a13",
            "corpusId": 2759724,
            "url": "https://www.semanticscholar.org/paper/1342c1e1684620c019972e2679d5131f1e8a4a13",
            "title": "Weight-averaged consistency targets improve semi-supervised deep learning results",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/TarvainenV17",
                "ArXiv": "1703.01780",
                "MAG": "2645998928",
                "CorpusId": 2759724
            },
            "abstract": "The recently proposed temporal ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, temporal ensembling becomes unwieldy when using large datasets. To overcome this problem, we propose a method that averages model weights instead of label predictions. As an additional benefit, the method improves test accuracy and enables training with fewer labels than earlier methods. We report state-of-the-art results on semi-supervised SVHN, reducing the error rate from 5.12% to 4.41% with 500 labels, and achieving 5.39% error rate with 250 labels. By using extra unlabeled data, we reduce the error rate to 2.76% on 500-label SVHN.",
            "referenceCount": 28,
            "citationCount": 2744,
            "influentialCitationCount": 512,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-03-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1703.01780"
            },
            "citationStyles": {
                "bibtex": "@Article{Tarvainen2017WeightaveragedCT,\n author = {Antti Tarvainen and H. Valpola},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Weight-averaged consistency targets improve semi-supervised deep learning results},\n volume = {abs/1703.01780},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ce2d5b5856bb6c9ab5c2390eb8b180c75a162055",
            "@type": "ScholarlyArticle",
            "paperId": "ce2d5b5856bb6c9ab5c2390eb8b180c75a162055",
            "corpusId": 3397190,
            "url": "https://www.semanticscholar.org/paper/ce2d5b5856bb6c9ab5c2390eb8b180c75a162055",
            "title": "Recent Trends in Deep Learning Based Natural Language Processing",
            "venue": "IEEE Computational Intelligence Magazine",
            "publicationVenue": {
                "id": "urn:research:ee372de7-efda-4907-a03f-359292ea27f6",
                "name": "IEEE Computational Intelligence Magazine",
                "alternate_names": [
                    "IEEE Comput Intell Mag"
                ],
                "issn": "1556-603X",
                "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=10207"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2742947407",
                "DBLP": "journals/corr/abs-1708-02709",
                "ArXiv": "1708.02709",
                "DOI": "10.1109/MCI.2018.2840738",
                "CorpusId": 3397190
            },
            "abstract": "Deep learning methods employ multiple processing layers to learn hierarchical representations of data and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.",
            "referenceCount": 204,
            "citationCount": 2533,
            "influentialCitationCount": 84,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-08-09",
            "journal": {
                "name": "IEEE Comput. Intell. Mag.",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Young2017RecentTI,\n author = {Tom Young and Devamanyu Hazarika and Soujanya Poria and E. Cambria},\n booktitle = {IEEE Computational Intelligence Magazine},\n journal = {IEEE Comput. Intell. Mag.},\n pages = {55-75},\n title = {Recent Trends in Deep Learning Based Natural Language Processing},\n volume = {13},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea",
            "@type": "ScholarlyArticle",
            "paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea",
            "corpusId": 174802812,
            "url": "https://www.semanticscholar.org/paper/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea",
            "title": "Energy and Policy Considerations for Deep Learning in NLP",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1906.02243",
                "ACL": "P19-1355",
                "MAG": "2963809228",
                "DBLP": "journals/corr/abs-1906-02243",
                "DOI": "10.18653/v1/P19-1355",
                "CorpusId": 174802812
            },
            "abstract": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
            "referenceCount": 19,
            "citationCount": 1958,
            "influentialCitationCount": 104,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/P19-1355.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Political Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-06-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1906.02243"
            },
            "citationStyles": {
                "bibtex": "@Article{Strubell2019EnergyAP,\n author = {Emma Strubell and Ananya Ganesh and A. McCallum},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Energy and Policy Considerations for Deep Learning in NLP},\n volume = {abs/1906.02243},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:811df72e210e20de99719539505da54762a11c6d",
            "@type": "ScholarlyArticle",
            "paperId": "811df72e210e20de99719539505da54762a11c6d",
            "corpusId": 28202810,
            "url": "https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d",
            "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2932819148",
                "ArXiv": "1801.01290",
                "DBLP": "conf/icml/HaarnojaZAL18",
                "CorpusId": 28202810
            },
            "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
            "referenceCount": 42,
            "citationCount": 5264,
            "influentialCitationCount": 1318,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-01-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1801.01290"
            },
            "citationStyles": {
                "bibtex": "@Article{Haarnoja2018SoftAO,\n author = {Tuomas Haarnoja and Aurick Zhou and P. Abbeel and S. Levine},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},\n volume = {abs/1801.01290},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:14014c024674991149f3ecf9314c93f7e029ef1a",
            "@type": "ScholarlyArticle",
            "paperId": "14014c024674991149f3ecf9314c93f7e029ef1a",
            "corpusId": 233423603,
            "url": "https://www.semanticscholar.org/paper/14014c024674991149f3ecf9314c93f7e029ef1a",
            "title": "Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2104.13478",
                "DBLP": "journals/corr/abs-2104-13478",
                "CorpusId": 233423603
            },
            "abstract": "The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.",
            "referenceCount": 0,
            "citationCount": 697,
            "influentialCitationCount": 60,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-04-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2104.13478"
            },
            "citationStyles": {
                "bibtex": "@Article{Bronstein2021GeometricDL,\n author = {M. Bronstein and Joan Bruna and Taco Cohen and Petar Velivckovi'c},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},\n volume = {abs/2104.13478},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:916455d97cd792c2eb5b00663689592e25cbc8d8",
            "@type": "ScholarlyArticle",
            "paperId": "916455d97cd792c2eb5b00663689592e25cbc8d8",
            "corpusId": 247060741,
            "url": "https://www.semanticscholar.org/paper/916455d97cd792c2eb5b00663689592e25cbc8d8",
            "title": "Deep learning in optical metrology: a review",
            "venue": "Light: Science & Applications",
            "publicationVenue": {
                "id": "urn:research:d4a5e814-e8a0-4c10-aaa6-ebdc39421fec",
                "name": "Light: Science & Applications",
                "alternate_names": [
                    "Light-Science & Applications",
                    "Light Sci  Appl",
                    "Light  Appl"
                ],
                "issn": "2047-7538",
                "url": "http://www.nature.com/lsa/"
            },
            "year": 2022,
            "externalIds": {
                "PubMedCentral": "8866517",
                "DOI": "10.1038/s41377-022-00714-x",
                "CorpusId": 247060741,
                "PubMed": "35197457"
            },
            "abstract": null,
            "referenceCount": 158,
            "citationCount": 188,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41377-022-00714-x.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2022-02-23",
            "journal": {
                "name": "Light, Science & Applications",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Zuo2022DeepLI,\n author = {C. Zuo and Jiaming Qian and Shijie Feng and Wei Yin and Yixuan Li and Pengfei Fan and Jing Han and K. Qian and Qian Chen},\n booktitle = {Light: Science & Applications},\n journal = {Light, Science & Applications},\n title = {Deep learning in optical metrology: a review},\n volume = {11},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:44e1dd74f0446ec91221189ad3a65edb1a0208fe",
            "@type": "ScholarlyArticle",
            "paperId": "44e1dd74f0446ec91221189ad3a65edb1a0208fe",
            "corpusId": 205572964,
            "url": "https://www.semanticscholar.org/paper/44e1dd74f0446ec91221189ad3a65edb1a0208fe",
            "title": "A guide to deep learning in healthcare",
            "venue": "Nature Network Boston",
            "publicationVenue": {
                "id": "urn:research:9e995b6d-f30b-4ab4-a13b-3dc2cc992f47",
                "name": "Nature Network Boston",
                "alternate_names": [
                    "Nat Netw Boston",
                    "Nat Med",
                    "Nature Medicine"
                ],
                "issn": "1744-7933",
                "url": "https://www.nature.com/nature/articles?code=archive_news"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2905810301",
                "DOI": "10.1038/s41591-018-0316-z",
                "CorpusId": 205572964,
                "PubMed": "30617335"
            },
            "abstract": null,
            "referenceCount": 65,
            "citationCount": 1773,
            "influentialCitationCount": 41,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2019-01-01",
            "journal": {
                "name": "Nature Medicine",
                "volume": "25"
            },
            "citationStyles": {
                "bibtex": "@Article{Esteva2019AGT,\n author = {A. Esteva and Alexandre Robicquet and Bharath Ramsundar and Volodymyr Kuleshov and M. DePristo and Katherine Chou and Claire Cui and Greg S. Corrado and S. Thrun and J. Dean},\n booktitle = {Nature Network Boston},\n journal = {Nature Medicine},\n pages = {24 - 29},\n title = {A guide to deep learning in healthcare},\n volume = {25},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ca011427853d34ce4ec9ccafde8a70c9eacc3e21",
            "@type": "ScholarlyArticle",
            "paperId": "ca011427853d34ce4ec9ccafde8a70c9eacc3e21",
            "corpusId": 3557281,
            "url": "https://www.semanticscholar.org/paper/ca011427853d34ce4ec9ccafde8a70c9eacc3e21",
            "title": "Deep Learning for Computer Vision: A Brief Review",
            "venue": "Computational Intelligence and Neuroscience",
            "publicationVenue": {
                "id": "urn:research:f32b7322-b69c-4e63-801d-8f50784ef778",
                "name": "Computational Intelligence and Neuroscience",
                "alternate_names": [
                    "Comput Intell Neurosci"
                ],
                "issn": "1687-5265",
                "url": "https://www.hindawi.com/journals/cin/"
            },
            "year": 2018,
            "externalIds": {
                "PubMedCentral": "5816885",
                "DBLP": "journals/cin/VoulodimosDDP18",
                "MAG": "2794284562",
                "DOI": "10.1155/2018/7068349",
                "CorpusId": 3557281,
                "PubMed": "29487619"
            },
            "abstract": "Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.",
            "referenceCount": 115,
            "citationCount": 2020,
            "influentialCitationCount": 29,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://downloads.hindawi.com/journals/cin/2018/7068349.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2018-02-01",
            "journal": {
                "name": "Computational Intelligence and Neuroscience",
                "volume": "2018"
            },
            "citationStyles": {
                "bibtex": "@Article{Voulodimos2018DeepLF,\n author = {A. Voulodimos and N. Doulamis and A. Doulamis and Eftychios E. Protopapadakis},\n booktitle = {Computational Intelligence and Neuroscience},\n journal = {Computational Intelligence and Neuroscience},\n title = {Deep Learning for Computer Vision: A Brief Review},\n volume = {2018},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b79e5e4622a95417deec313cd543617b19611bea",
            "@type": "ScholarlyArticle",
            "paperId": "b79e5e4622a95417deec313cd543617b19611bea",
            "corpusId": 4090379,
            "url": "https://www.semanticscholar.org/paper/b79e5e4622a95417deec313cd543617b19611bea",
            "title": "Deep Learning using Rectified Linear Units (ReLU)",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1803-08375",
                "ArXiv": "1803.08375",
                "MAG": "2792643794",
                "CorpusId": 4090379
            },
            "abstract": "We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer $h_{n - 1}$ in a neural network, then multiply it by weight parameters $\\theta$ to get the raw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$, i.e. $f(o) = \\max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide class predictions $\\hat{y}$ through argmax function, i.e. argmax $f(x)$.",
            "referenceCount": 17,
            "citationCount": 2053,
            "influentialCitationCount": 155,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-03-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1803.08375"
            },
            "citationStyles": {
                "bibtex": "@Article{Agarap2018DeepLU,\n author = {Abien Fred Agarap},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Deep Learning using Rectified Linear Units (ReLU)},\n volume = {abs/1803.08375},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:518a7c79968a56d63a691d42f8378be6c776167e",
            "@type": "ScholarlyArticle",
            "paperId": "518a7c79968a56d63a691d42f8378be6c776167e",
            "corpusId": 206907269,
            "url": "https://www.semanticscholar.org/paper/518a7c79968a56d63a691d42f8378be6c776167e",
            "title": "Deep learning in agriculture: A survey",
            "venue": "Computers and Electronics in Agriculture",
            "publicationVenue": {
                "id": "urn:research:80fdf70e-8520-4bb7-b387-3abebc9970b7",
                "name": "Computers and Electronics in Agriculture",
                "alternate_names": [
                    "Comput Electron Agric"
                ],
                "issn": "0168-1699",
                "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/503304/description#description"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1807.11809",
                "DBLP": "journals/cea/KamilarisP18",
                "MAG": "2790979755",
                "DOI": "10.1016/j.compag.2018.02.016",
                "CorpusId": 206907269
            },
            "abstract": null,
            "referenceCount": 75,
            "citationCount": 2126,
            "influentialCitationCount": 51,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://repositori.irta.cat/bitstream/20.500.12327/314/1/kamilaris_deep_2018.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Agricultural and Food Sciences",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-04-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1807.11809"
            },
            "citationStyles": {
                "bibtex": "@Article{Kamilaris2018DeepLI,\n author = {A. Kamilaris and F. Prenafeta-Bold\u00fa},\n booktitle = {Computers and Electronics in Agriculture},\n journal = {ArXiv},\n title = {Deep learning in agriculture: A survey},\n volume = {abs/1807.11809},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
            "@type": "ScholarlyArticle",
            "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
            "corpusId": 6719686,
            "url": "https://www.semanticscholar.org/paper/c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
            "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2604763608",
                "DBLP": "journals/corr/FinnAL17",
                "ArXiv": "1703.03400",
                "CorpusId": 6719686
            },
            "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.",
            "referenceCount": 52,
            "citationCount": 8839,
            "influentialCitationCount": 2248,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Finn2017ModelAgnosticMF,\n author = {Chelsea Finn and P. Abbeel and S. Levine},\n booktitle = {International Conference on Machine Learning},\n pages = {1126-1135},\n title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0504945cc2d03550fecb6ff02e637f9421107c25",
            "@type": "ScholarlyArticle",
            "paperId": "0504945cc2d03550fecb6ff02e637f9421107c25",
            "corpusId": 18874645,
            "url": "https://www.semanticscholar.org/paper/0504945cc2d03550fecb6ff02e637f9421107c25",
            "title": "Learning a Deep Convolutional Network for Image Super-Resolution",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "54257720",
                "DBLP": "conf/eccv/DongLHT14",
                "DOI": "10.1007/978-3-319-10593-2_13",
                "CorpusId": 18874645
            },
            "abstract": null,
            "referenceCount": 31,
            "citationCount": 4306,
            "influentialCitationCount": 785,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-319-10593-2_13.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-09-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dong2014LearningAD,\n author = {Chao Dong and Chen Change Loy and Kaiming He and Xiaoou Tang},\n booktitle = {European Conference on Computer Vision},\n pages = {184-199},\n title = {Learning a Deep Convolutional Network for Image Super-Resolution},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d1dbf643447405984eeef098b1b320dee0b3b8a7",
            "@type": "ScholarlyArticle",
            "paperId": "d1dbf643447405984eeef098b1b320dee0b3b8a7",
            "corpusId": 14955348,
            "url": "https://www.semanticscholar.org/paper/d1dbf643447405984eeef098b1b320dee0b3b8a7",
            "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
            "venue": "International Conference on Artificial Intelligence and Statistics",
            "publicationVenue": {
                "id": "urn:research:2d136b11-c2b5-484b-b008-7f4a852fd61e",
                "name": "International Conference on Artificial Intelligence and Statistics",
                "alternate_names": [
                    "AISTATS",
                    "Int Conf Artif Intell Stat"
                ],
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2950745363",
                "DBLP": "conf/aistats/McMahanMRHA17",
                "ArXiv": "1602.05629",
                "CorpusId": 14955348
            },
            "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. \nWe present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.",
            "referenceCount": 51,
            "citationCount": 9829,
            "influentialCitationCount": 2639,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-02-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{McMahan2016CommunicationEfficientLO,\n author = {H. B. McMahan and Eider Moore and Daniel Ramage and S. Hampson and B. A. Y. Arcas},\n booktitle = {International Conference on Artificial Intelligence and Statistics},\n pages = {1273-1282},\n title = {Communication-Efficient Learning of Deep Networks from Decentralized Data},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:adc61e21eafecfbf6ebecc570f9f913659a2bfb2",
            "@type": "ScholarlyArticle",
            "paperId": "adc61e21eafecfbf6ebecc570f9f913659a2bfb2",
            "corpusId": 235386502,
            "url": "https://www.semanticscholar.org/paper/adc61e21eafecfbf6ebecc570f9f913659a2bfb2",
            "title": "Deep Learning Based Text Classification: A Comprehensive Review",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "CorpusId": 235386502
            },
            "abstract": ". Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this paper, we provide a comprehensive review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.",
            "referenceCount": 226,
            "citationCount": 574,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Minaee2021DeepLB,\n author = {Shervin Minaee and E. Cambria and Jianfeng Gao},\n title = {Deep Learning Based Text Classification: A Comprehensive Review},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0e779fd59353a7f1f5b559b9d65fa4bfe367890c",
            "@type": "ScholarlyArticle",
            "paperId": "0e779fd59353a7f1f5b559b9d65fa4bfe367890c",
            "corpusId": 15195762,
            "url": "https://www.semanticscholar.org/paper/0e779fd59353a7f1f5b559b9d65fa4bfe367890c",
            "title": "Geometric Deep Learning: Going beyond Euclidean data",
            "venue": "IEEE Signal Processing Magazine",
            "publicationVenue": {
                "id": "urn:research:f62e5eab-173a-4e0a-a963-ed8de9835d22",
                "name": "IEEE Signal Processing Magazine",
                "alternate_names": [
                    "IEEE Signal Process Mag"
                ],
                "issn": "1053-5888",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=79"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/spm/BronsteinBLSV17",
                "MAG": "3102013575",
                "ArXiv": "1611.08097",
                "DOI": "10.1109/MSP.2017.2693418",
                "CorpusId": 15195762
            },
            "abstract": "Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.",
            "referenceCount": 125,
            "citationCount": 2753,
            "influentialCitationCount": 155,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1611.08097",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-24",
            "journal": {
                "name": "IEEE Signal Process. Mag.",
                "volume": "34"
            },
            "citationStyles": {
                "bibtex": "@Article{Bronstein2016GeometricDL,\n author = {M. Bronstein and Joan Bruna and Yann LeCun and Arthur Szlam and P. Vandergheynst},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Process. Mag.},\n pages = {18-42},\n title = {Geometric Deep Learning: Going beyond Euclidean data},\n volume = {34},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:addae423490bbe82da4fb2fc265237178686b4e8",
            "@type": "ScholarlyArticle",
            "paperId": "addae423490bbe82da4fb2fc265237178686b4e8",
            "corpusId": 196814162,
            "url": "https://www.semanticscholar.org/paper/addae423490bbe82da4fb2fc265237178686b4e8",
            "title": "Clinical-grade computational pathology using weakly supervised deep learning on whole slide images",
            "venue": "Nature Network Boston",
            "publicationVenue": {
                "id": "urn:research:9e995b6d-f30b-4ab4-a13b-3dc2cc992f47",
                "name": "Nature Network Boston",
                "alternate_names": [
                    "Nat Netw Boston",
                    "Nat Med",
                    "Nature Medicine"
                ],
                "issn": "1744-7933",
                "url": "https://www.nature.com/nature/articles?code=archive_news"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2956228567",
                "DOI": "10.1038/s41591-019-0508-1",
                "CorpusId": 196814162,
                "PubMed": "31308507"
            },
            "abstract": null,
            "referenceCount": 41,
            "citationCount": 1294,
            "influentialCitationCount": 76,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-07-15",
            "journal": {
                "name": "Nature Medicine",
                "volume": "25"
            },
            "citationStyles": {
                "bibtex": "@Article{Campanella2019ClinicalgradeCP,\n author = {Gabriele Campanella and M. Hanna and Luke Geneslaw and Allen P. Miraflor and Vitor Werneck Krauss Silva and K. Busam and E. Brogi and V. Reuter and D. Klimstra and Thomas J. Fuchs},\n booktitle = {Nature Network Boston},\n journal = {Nature Medicine},\n pages = {1301 - 1309},\n title = {Clinical-grade computational pathology using weakly supervised deep learning on whole slide images},\n volume = {25},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:69e76e16740ed69f4dc55361a3d319ac2f1293dd",
            "@type": "ScholarlyArticle",
            "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
            "corpusId": 6875312,
            "url": "https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd",
            "title": "Asynchronous Methods for Deep Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/MnihBMGLHSK16",
                "MAG": "2964043796",
                "ArXiv": "1602.01783",
                "CorpusId": 6875312
            },
            "abstract": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",
            "referenceCount": 43,
            "citationCount": 7254,
            "influentialCitationCount": 1303,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-02-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Mnih2016AsynchronousMF,\n author = {Volodymyr Mnih and Adri\u00e0 Puigdom\u00e8nech Badia and Mehdi Mirza and Alex Graves and T. Lillicrap and Tim Harley and David Silver and K. Kavukcuoglu},\n booktitle = {International Conference on Machine Learning},\n pages = {1928-1937},\n title = {Asynchronous Methods for Deep Reinforcement Learning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a2e667e4382aaa8e02a17d0522c1a910790ab65b",
            "@type": "ScholarlyArticle",
            "paperId": "a2e667e4382aaa8e02a17d0522c1a910790ab65b",
            "corpusId": 57825713,
            "url": "https://www.semanticscholar.org/paper/a2e667e4382aaa8e02a17d0522c1a910790ab65b",
            "title": "Deep Learning for Anomaly Detection: A Survey",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1901-03407",
                "MAG": "2910068345",
                "ArXiv": "1901.03407",
                "CorpusId": 57825713
            },
            "abstract": "Anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. The aim of this survey is two-fold, firstly we present a structured and comprehensive overview of research methods in deep learning-based anomaly detection. Furthermore, we review the adoption of these methods for anomaly across various application domains and assess their effectiveness. We have grouped state-of-the-art research techniques into different categories based on the underlying assumptions and approach adopted. Within each category we outline the basic anomaly detection technique, along with its variants and present key assumptions, to differentiate between normal and anomalous behavior. For each category, we present we also present the advantages and limitations and discuss the computational complexity of the techniques in real application domains. Finally, we outline open issues in research and challenges faced while adopting these techniques.",
            "referenceCount": 477,
            "citationCount": 1180,
            "influentialCitationCount": 79,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Geology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Geology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-01-10",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1901.03407"
            },
            "citationStyles": {
                "bibtex": "@Article{Chalapathy2019DeepLF,\n author = {Raghavendra Chalapathy and Sanjay Chawla},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Deep Learning for Anomaly Detection: A Survey},\n volume = {abs/1901.03407},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5a3ed94cf02abc159b66d6807405a2574886bbe3",
            "@type": "ScholarlyArticle",
            "paperId": "5a3ed94cf02abc159b66d6807405a2574886bbe3",
            "corpusId": 210164273,
            "url": "https://www.semanticscholar.org/paper/5a3ed94cf02abc159b66d6807405a2574886bbe3",
            "title": "Deep Learning for Person Re-Identification: A Survey and Outlook",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2001-04193",
                "MAG": "2998792609",
                "ArXiv": "2001.04193",
                "DOI": "10.1109/TPAMI.2021.3054775",
                "CorpusId": 210164273,
                "PubMed": "33497329"
            },
            "abstract": "Person re-identification (Re-ID) aims at retrieving a person of interest across multiple non-overlapping cameras. With the advancement of deep neural networks and increasing demand of intelligent video surveillance, it has gained significantly increased interest in the computer vision community. By dissecting the involved components in developing a person Re-ID system, we categorize it into the closed-world and open-world settings. The widely studied closed-world setting is usually applied under various research-oriented assumptions, and has achieved inspiring success using deep learning techniques on a number of datasets. We first conduct a comprehensive overview with in-depth analysis for closed-world person Re-ID from three different perspectives, including deep feature representation learning, deep metric learning and ranking optimization. With the performance saturation under closed-world setting, the research focus for person Re-ID has recently shifted to the open-world setting, facing more challenging issues. This setting is closer to practical applications under specific scenarios. We summarize the open-world Re-ID in terms of five different aspects. By analyzing the advantages of existing methods, we design a powerful AGW baseline, achieving state-of-the-art or at least comparable performance on twelve datasets for four different Re-ID tasks. Meanwhile, we introduce a new evaluation metric (mINP) for person Re-ID, indicating the cost for finding all the correct matches, which provides an additional criteria to evaluate the Re-ID system for real applications. Finally, some important yet under-investigated open issues are discussed.",
            "referenceCount": 241,
            "citationCount": 920,
            "influentialCitationCount": 98,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-01-13",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "44"
            },
            "citationStyles": {
                "bibtex": "@Article{Ye2020DeepLF,\n author = {Mang Ye and Jianbing Shen and Gaojie Lin and T. Xiang and Ling Shao and S. Hoi},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {2872-2893},\n title = {Deep Learning for Person Re-Identification: A Survey and Outlook},\n volume = {44},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0c00a328fa7cd56ee60338c54e89bd48310db80b",
            "@type": "ScholarlyArticle",
            "paperId": "0c00a328fa7cd56ee60338c54e89bd48310db80b",
            "corpusId": 996788,
            "url": "https://www.semanticscholar.org/paper/0c00a328fa7cd56ee60338c54e89bd48310db80b",
            "title": "Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising",
            "venue": "IEEE Transactions on Image Processing",
            "publicationVenue": {
                "id": "urn:research:e117fa7f-05b7-4dd6-b64b-0a0a7c0393d8",
                "name": "IEEE Transactions on Image Processing",
                "alternate_names": [
                    "IEEE Trans Image Process"
                ],
                "issn": "1057-7149",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=83"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/ZhangZCM016",
                "MAG": "3099906833",
                "ArXiv": "1608.03981",
                "DOI": "10.1109/TIP.2017.2662206",
                "CorpusId": 996788,
                "PubMed": "28166495"
            },
            "abstract": "The discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, we take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, residual learning and batch normalization are utilized to speed up the training process as well as boost the denoising performance. Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise at a certain noise level, our DnCNN model is able to handle Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising). With the residual learning strategy, DnCNN implicitly removes the latent clean image in the hidden layers. This property motivates us to train a single DnCNN model to tackle with several general image denoising tasks, such as Gaussian denoising, single image super-resolution, and JPEG image deblocking. Our extensive experiments demonstrate that our DnCNN model can not only exhibit high effectiveness in several general image denoising tasks, but also be efficiently implemented by benefiting from GPU computing.",
            "referenceCount": 51,
            "citationCount": 5478,
            "influentialCitationCount": 1056,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1608.03981",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-08-13",
            "journal": {
                "name": "IEEE Transactions on Image Processing",
                "volume": "26"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2016BeyondAG,\n author = {K. Zhang and W. Zuo and Yunjin Chen and Deyu Meng and Lei Zhang},\n booktitle = {IEEE Transactions on Image Processing},\n journal = {IEEE Transactions on Image Processing},\n pages = {3142-3155},\n title = {Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising},\n volume = {26},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1ed33896e82cc3810b349cdffc2039f0b8edd82e",
            "@type": "ScholarlyArticle",
            "paperId": "1ed33896e82cc3810b349cdffc2039f0b8edd82e",
            "corpusId": 125608550,
            "url": "https://www.semanticscholar.org/paper/1ed33896e82cc3810b349cdffc2039f0b8edd82e",
            "title": "Deep learning and its applications to machine health monitoring",
            "venue": "Mechanical systems and signal processing",
            "publicationVenue": {
                "id": "urn:research:dc4b3846-1e31-4c19-a196-e8b1d091037f",
                "name": "Mechanical systems and signal processing",
                "alternate_names": [
                    "Mech syst signal process",
                    "Mech Syst Signal Process",
                    "Mechanical Systems and Signal Processing"
                ],
                "issn": "0888-3270",
                "url": "https://www.journals.elsevier.com/mechanical-systems-and-signal-processing"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2810292802",
                "DOI": "10.1016/J.YMSSP.2018.05.050",
                "CorpusId": 125608550
            },
            "abstract": null,
            "referenceCount": 110,
            "citationCount": 1552,
            "influentialCitationCount": 42,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2019-01-01",
            "journal": {
                "name": "Mechanical Systems and Signal Processing",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhao2019DeepLA,\n author = {Rui Zhao and Ruqiang Yan and Zhenghua Chen and K. Mao and Peng Wang and R. Gao},\n booktitle = {Mechanical systems and signal processing},\n journal = {Mechanical Systems and Signal Processing},\n title = {Deep learning and its applications to machine health monitoring},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a92b103b81a48878e76f1fcfc3e2a1454f895555",
            "@type": "ScholarlyArticle",
            "paperId": "a92b103b81a48878e76f1fcfc3e2a1454f895555",
            "corpusId": 52089505,
            "url": "https://www.semanticscholar.org/paper/a92b103b81a48878e76f1fcfc3e2a1454f895555",
            "title": "Neural networks and Deep Learning",
            "venue": "Machine Learning Guide for Oil and Gas Using Python",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2798659734",
                "DOI": "10.1017/CBO9781316576533.019",
                "CorpusId": 52089505
            },
            "abstract": "This chapter contains sections titled: Artificial Neural Networks, Neural Network Learning Algorithms, What a Perceptron Can and Cannot Do, Connectionist Models in Cognitive Science, Neural Networks as a Paradigm for Parallel Processing, Hierarchical Representations in Multiple Layers, Deep Learning",
            "referenceCount": 402,
            "citationCount": 2335,
            "influentialCitationCount": 217,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2016-07-01",
            "journal": {
                "name": "Machine Learning Guide for Oil and Gas Using Python",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Efron2016NeuralNA,\n author = {B. Efron and Trevor J. Hastie},\n booktitle = {Machine Learning Guide for Oil and Gas Using Python},\n journal = {Machine Learning Guide for Oil and Gas Using Python},\n title = {Neural networks and Deep Learning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8ec5896b4490c6e127d1718ffc36a3439d84cb81",
            "@type": "ScholarlyArticle",
            "paperId": "8ec5896b4490c6e127d1718ffc36a3439d84cb81",
            "corpusId": 5834589,
            "url": "https://www.semanticscholar.org/paper/8ec5896b4490c6e127d1718ffc36a3439d84cb81",
            "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1609.04836",
                "MAG": "2963959597",
                "DBLP": "conf/iclr/KeskarMNST17",
                "CorpusId": 5834589
            },
            "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say $32$-$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.",
            "referenceCount": 55,
            "citationCount": 2380,
            "influentialCitationCount": 230,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-09-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1609.04836"
            },
            "citationStyles": {
                "bibtex": "@Article{Keskar2016OnLT,\n author = {N. Keskar and Dheevatsa Mudigere and J. Nocedal and M. Smelyanskiy and P. T. P. Tang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},\n volume = {abs/1609.04836},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:009560d2710138a446e6e254d8ddcb65eaa0e687",
            "@type": "ScholarlyArticle",
            "paperId": "009560d2710138a446e6e254d8ddcb65eaa0e687",
            "corpusId": 260435623,
            "url": "https://www.semanticscholar.org/paper/009560d2710138a446e6e254d8ddcb65eaa0e687",
            "title": "Tabular Data: Deep Learning is Not All You Need",
            "venue": "Information Fusion",
            "publicationVenue": {
                "id": "urn:research:06afdd0b-0d85-413f-af8a-c3045c12c561",
                "name": "Information Fusion",
                "alternate_names": [
                    "Inf Fusion"
                ],
                "issn": "1566-2535",
                "url": "https://www.journals.elsevier.com/information-fusion"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/inffus/Shwartz-ZivA22",
                "ArXiv": "2106.03253",
                "DOI": "10.1016/j.inffus.2021.11.011",
                "CorpusId": 260435623
            },
            "abstract": null,
            "referenceCount": 59,
            "citationCount": 520,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2106.03253",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-06-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2106.03253"
            },
            "citationStyles": {
                "bibtex": "@Article{Shwartz-Ziv2021TabularDD,\n author = {Ravid Shwartz-Ziv and Amitai Armon},\n booktitle = {Information Fusion},\n journal = {ArXiv},\n title = {Tabular Data: Deep Learning is Not All You Need},\n volume = {abs/2106.03253},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a0f303b6e22ef52943355993f57d65938997066a",
            "@type": "ScholarlyArticle",
            "paperId": "a0f303b6e22ef52943355993f57d65938997066a",
            "corpusId": 233210772,
            "url": "https://www.semanticscholar.org/paper/a0f303b6e22ef52943355993f57d65938997066a",
            "title": "Machine learning and deep learning",
            "venue": "Electronic Markets",
            "publicationVenue": {
                "id": "urn:research:21b370ff-cb87-4599-b39a-25cd5a1b03cb",
                "name": "Electronic Markets",
                "alternate_names": [
                    "Electron Mark"
                ],
                "issn": "1019-6781",
                "url": "http://www.metapress.com/openurl.asp?genre=journal&issn=1019-6781"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2104-05314",
                "ArXiv": "2104.05314",
                "DOI": "10.1007/s12525-021-00475-2",
                "CorpusId": 233210772
            },
            "abstract": null,
            "referenceCount": 64,
            "citationCount": 427,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s12525-021-00475-2.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-04-08",
            "journal": {
                "name": "Electronic Markets",
                "volume": "31"
            },
            "citationStyles": {
                "bibtex": "@Article{Janiesch2021MachineLA,\n author = {Christian Janiesch and Patrick Zschech and K. Heinrich},\n booktitle = {Electronic Markets},\n journal = {Electronic Markets},\n pages = {685 - 695},\n title = {Machine learning and deep learning},\n volume = {31},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:db6ad6ded1cfa26fdc7437f27fb823ec533e96fe",
            "@type": "ScholarlyArticle",
            "paperId": "db6ad6ded1cfa26fdc7437f27fb823ec533e96fe",
            "corpusId": 237241776,
            "url": "https://www.semanticscholar.org/paper/db6ad6ded1cfa26fdc7437f27fb823ec533e96fe",
            "title": "Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions",
            "venue": "SN Computer Science",
            "publicationVenue": {
                "id": "urn:research:7a7dc89b-e1a6-44df-a496-46c330a87840",
                "name": "SN Computer Science",
                "alternate_names": [
                    "SN Comput Sci"
                ],
                "issn": "2661-8907",
                "url": "https://link.springer.com/journal/42979"
            },
            "year": 2021,
            "externalIds": {
                "PubMedCentral": "8372231",
                "MAG": "3191230654",
                "DBLP": "journals/sncs/Sarker21c",
                "DOI": "10.1007/s42979-021-00815-1",
                "CorpusId": 237241776,
                "PubMed": "34426802"
            },
            "abstract": null,
            "referenceCount": 141,
            "citationCount": 519,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s42979-021-00815-1.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2021-08-02",
            "journal": {
                "name": "Sn Computer Science",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Sarker2021DeepLA,\n author = {Iqbal H. Sarker},\n booktitle = {SN Computer Science},\n journal = {Sn Computer Science},\n title = {Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions},\n volume = {2},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c3df199cbca74763c4ae9889409bbd4aa29b6255",
            "@type": "ScholarlyArticle",
            "paperId": "c3df199cbca74763c4ae9889409bbd4aa29b6255",
            "corpusId": 231202901,
            "url": "https://www.semanticscholar.org/paper/c3df199cbca74763c4ae9889409bbd4aa29b6255",
            "title": "Deep learning-enabled medical computer vision",
            "venue": "npj Digital Medicine",
            "publicationVenue": {
                "id": "urn:research:ef485645-f75f-4344-8b9d-3c260e69503b",
                "name": "npj Digital Medicine",
                "alternate_names": [
                    "npj Digit Med"
                ],
                "issn": "2398-6352",
                "url": "http://www.nature.com/npjdigitalmed/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/npjdm/EstevaCYNMM0TDS21",
                "PubMedCentral": "7794558",
                "DOI": "10.1038/s41746-020-00376-2",
                "CorpusId": 231202901,
                "PubMed": "33420381"
            },
            "abstract": null,
            "referenceCount": 170,
            "citationCount": 416,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41746-020-00376-2.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2021-01-08",
            "journal": {
                "name": "NPJ Digital Medicine",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Esteva2021DeepLM,\n author = {A. Esteva and Katherine Chou and Serena Yeung and N. Naik and Ali Madani and A. Mottaghi and Yun Liu and E. Topol and J. Dean and R. Socher},\n booktitle = {npj Digital Medicine},\n journal = {NPJ Digital Medicine},\n title = {Deep learning-enabled medical computer vision},\n volume = {4},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7d873a9c49d3864709aa762f8740edcdbd7369c5",
            "@type": "ScholarlyArticle",
            "paperId": "7d873a9c49d3864709aa762f8740edcdbd7369c5",
            "corpusId": 234597294,
            "url": "https://www.semanticscholar.org/paper/7d873a9c49d3864709aa762f8740edcdbd7369c5",
            "title": "Deep learning in histopathology: the path to the clinic",
            "venue": "Nature Network Boston",
            "publicationVenue": {
                "id": "urn:research:9e995b6d-f30b-4ab4-a13b-3dc2cc992f47",
                "name": "Nature Network Boston",
                "alternate_names": [
                    "Nat Netw Boston",
                    "Nat Med",
                    "Nature Medicine"
                ],
                "issn": "1744-7933",
                "url": "https://www.nature.com/nature/articles?code=archive_news"
            },
            "year": 2021,
            "externalIds": {
                "DOI": "10.1038/s41591-021-01343-4",
                "CorpusId": 234597294,
                "PubMed": "33990804"
            },
            "abstract": null,
            "referenceCount": 150,
            "citationCount": 314,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://repository.ubn.ru.nl//bitstream/handle/2066/235736/235736.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2021-05-01",
            "journal": {
                "name": "Nature Medicine",
                "volume": "27"
            },
            "citationStyles": {
                "bibtex": "@Article{Laak2021DeepLI,\n author = {J. A. van der Laak and G. Litjens and F. Ciompi},\n booktitle = {Nature Network Boston},\n journal = {Nature Medicine},\n pages = {775 - 784},\n title = {Deep learning in histopathology: the path to the clinic},\n volume = {27},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3337e4b2e63e5e99171f9da9d161771f5810c27a",
            "@type": "ScholarlyArticle",
            "paperId": "3337e4b2e63e5e99171f9da9d161771f5810c27a",
            "corpusId": 234003524,
            "url": "https://www.semanticscholar.org/paper/3337e4b2e63e5e99171f9da9d161771f5810c27a",
            "title": "A Review of Deep-Learning-Based Medical Image Segmentation Methods",
            "venue": "Sustainability",
            "publicationVenue": {
                "id": "urn:research:8775599f-4f9a-45f0-900e-7f4de68e6843",
                "name": "Sustainability",
                "alternate_names": null,
                "issn": "2071-1050",
                "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-172127"
            },
            "year": 2021,
            "externalIds": {
                "MAG": "3123982987",
                "DOI": "10.3390/SU13031224",
                "CorpusId": 234003524
            },
            "abstract": "As an emerging biomedical image processing technology, medical image segmentation has made great contributions to sustainable medical care. Now it has become an important research direction in the field of computer vision. With the rapid development of deep learning, medical image processing based on deep convolutional neural networks has become a research hotspot. This paper focuses on the research of medical image segmentation based on deep learning. First, the basic ideas and characteristics of medical image segmentation based on deep learning are introduced. By explaining its research status and summarizing the three main methods of medical image segmentation and their own limitations, the future development direction is expanded. Based on the discussion of different pathological tissues and organs, the specificity between them and their classic segmentation algorithms are summarized. Despite the great achievements of medical image segmentation in recent years, medical image segmentation based on deep learning has still encountered difficulties in research. For example, the segmentation accuracy is not high, the number of medical images in the data set is small and the resolution is low. The inaccurate segmentation results are unable to meet the actual clinical requirements. Aiming at the above problems, a comprehensive review of current medical image segmentation methods based on deep learning is provided to help researchers solve existing problems.",
            "referenceCount": 112,
            "citationCount": 246,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2071-1050/13/3/1224/pdf?version=1612330114",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2021-01-25",
            "journal": {
                "name": "Sustainability",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2021ARO,\n author = {Xiangbin Liu and Liping Song and Shuai Liu and Yudong Zhang},\n booktitle = {Sustainability},\n journal = {Sustainability},\n title = {A Review of Deep-Learning-Based Medical Image Segmentation Methods},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b415adc22ac3133c82078c987303c422c56b3b5e",
            "@type": "ScholarlyArticle",
            "paperId": "b415adc22ac3133c82078c987303c422c56b3b5e",
            "corpusId": 232042081,
            "url": "https://www.semanticscholar.org/paper/b415adc22ac3133c82078c987303c422c56b3b5e",
            "title": "Plant diseases and pests detection based on deep learning: a review",
            "venue": "Plant Methods",
            "publicationVenue": {
                "id": "urn:research:7218da31-236c-47af-8931-874dcec1be68",
                "name": "Plant Methods",
                "alternate_names": [
                    "Plant Method"
                ],
                "issn": "1746-4811",
                "url": "http://www.plantmethods.com/"
            },
            "year": 2021,
            "externalIds": {
                "PubMedCentral": "7903739",
                "DOI": "10.1186/s13007-021-00722-9",
                "CorpusId": 232042081,
                "PubMed": "33627131"
            },
            "abstract": null,
            "referenceCount": 103,
            "citationCount": 262,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://plantmethods.biomedcentral.com/track/pdf/10.1186/s13007-021-00722-9",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Agricultural and Food Sciences",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2021-02-24",
            "journal": {
                "name": "Plant Methods",
                "volume": "17"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2021PlantDA,\n author = {Jun Liu and Xuewei Wang},\n booktitle = {Plant Methods},\n journal = {Plant Methods},\n title = {Plant diseases and pests detection based on deep learning: a review},\n volume = {17},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:26a1094ea12e3ea1f734e1670e6ba0e96c6b4bcf",
            "@type": "ScholarlyArticle",
            "paperId": "26a1094ea12e3ea1f734e1670e6ba0e96c6b4bcf",
            "corpusId": 233736197,
            "url": "https://www.semanticscholar.org/paper/26a1094ea12e3ea1f734e1670e6ba0e96c6b4bcf",
            "title": "A DEEP LEARNING APPROACH TO",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "CorpusId": 233736197
            },
            "abstract": "Wildfires are a serious disaster, which often cause severe damages to forests and plants. Without an early detection and suitable control action, a small wildfire could grow into a big and serious one. The problem is especially fatal at night, as firefighters in general miss the chance to detect the wildfires in the very first few hours. Low-light satellites, which take pictures at night, offer an opportunity to detect night fire timely. However, previous studies identify night fires based on threshold methods or conventional machine learning approaches, which are not robust and accurate enough. In this paper, we develop a new deep learning approach, which determines night fire locations by a pixel-level classification on low-light remote sensing image. Experimental results on VIIRS data demonstrate the superiority and effectiveness of the proposed method, which outperforms conventional threshold and machine learning approaches.",
            "referenceCount": 10,
            "citationCount": 274,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Wang2021ADL,\n author = {Yue Wang and Ye Ni and Xutao Li and Yunming Ye},\n title = {A DEEP LEARNING APPROACH TO},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b11468ed3a6215f1ee109fe5b2e0bd0e0e2f0eb1",
            "@type": "ScholarlyArticle",
            "paperId": "b11468ed3a6215f1ee109fe5b2e0bd0e0e2f0eb1",
            "corpusId": 233259634,
            "url": "https://www.semanticscholar.org/paper/b11468ed3a6215f1ee109fe5b2e0bd0e0e2f0eb1",
            "title": "Democratising deep learning for microscopy with ZeroCostDL4Mic",
            "venue": "Nature Communications",
            "publicationVenue": {
                "id": "urn:research:43b3f0f9-489a-4566-8164-02fafde3cd98",
                "name": "Nature Communications",
                "alternate_names": [
                    "Nat Commun"
                ],
                "issn": "2041-1723",
                "url": "https://www.nature.com/ncomms/"
            },
            "year": 2021,
            "externalIds": {
                "PubMedCentral": "8050272",
                "DOI": "10.1038/s41467-021-22518-0",
                "CorpusId": 233259634,
                "PubMed": "33859193"
            },
            "abstract": null,
            "referenceCount": 95,
            "citationCount": 249,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41467-021-22518-0.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Materials Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-04-15",
            "journal": {
                "name": "Nature Communications",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Chamier2021DemocratisingDL,\n author = {Lucas von Chamier and Romain F. Laine and Johanna Jukkala and Christoph Spahn and Daniel Krentzel and E. Nehme and Martina Lerche and Sara Hern\u00e1ndez-P\u00e9rez and P. Mattila and Eleni Karinou and S. Holden and A. Solak and Alexander Krull and T. Buchholz and Martin L. Jones and Loic A. Royer and Christophe Leterrier and Y. Shechtman and F. Jug and M. Heilemann and Guillaume Jacquemet and Ricardo Henriques},\n booktitle = {Nature Communications},\n journal = {Nature Communications},\n title = {Democratising deep learning for microscopy with ZeroCostDL4Mic},\n volume = {12},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:35adeef964fd344288febc7def2780007587724f",
            "@type": "ScholarlyArticle",
            "paperId": "35adeef964fd344288febc7def2780007587724f",
            "corpusId": 237946859,
            "url": "https://www.semanticscholar.org/paper/35adeef964fd344288febc7def2780007587724f",
            "title": "Deep learning in cancer diagnosis, prognosis and treatment selection",
            "venue": "Genome Medicine",
            "publicationVenue": {
                "id": "urn:research:a1eabcca-9d3f-45ac-bb78-c024734a2b68",
                "name": "Genome Medicine",
                "alternate_names": [
                    "Genome Med"
                ],
                "issn": "1756-994X",
                "url": "https://genomemedicine.biomedcentral.com/"
            },
            "year": 2021,
            "externalIds": {
                "PubMedCentral": "8477474",
                "DOI": "10.1186/s13073-021-00968-x",
                "CorpusId": 237946859,
                "PubMed": "34579788"
            },
            "abstract": null,
            "referenceCount": 174,
            "citationCount": 217,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://genomemedicine.biomedcentral.com/track/pdf/10.1186/s13073-021-00968-x",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2021-09-27",
            "journal": {
                "name": "Genome Medicine",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Tran2021DeepLI,\n author = {Khoa A Tran and O. Kondrashova and Andrew Bradley and E. Williams and J. Pearson and N. Waddell},\n booktitle = {Genome Medicine},\n journal = {Genome Medicine},\n title = {Deep learning in cancer diagnosis, prognosis and treatment selection},\n volume = {13},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bc44c0c64a473e035b11ae60a1993ad3db1acd2e",
            "@type": "ScholarlyArticle",
            "paperId": "bc44c0c64a473e035b11ae60a1993ad3db1acd2e",
            "corpusId": 232240630,
            "url": "https://www.semanticscholar.org/paper/bc44c0c64a473e035b11ae60a1993ad3db1acd2e",
            "title": "Deep learning: a statistical viewpoint",
            "venue": "Acta Numerica",
            "publicationVenue": {
                "id": "urn:research:086b0aef-7d17-4f72-8dc3-22252e8a5201",
                "name": "Acta Numerica",
                "alternate_names": [
                    "Acta Numer"
                ],
                "issn": "0962-4929",
                "url": "https://www.cambridge.org/core/journals/acta-numerica/all-issues"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2103-09177",
                "ArXiv": "2103.09177",
                "DOI": "10.1017/S0962492921000027",
                "CorpusId": 232240630
            },
            "abstract": "The remarkable practical success of deep learning has revealed some major surprises from a theoretical perspective. In particular, simple gradient methods easily find near-optimal solutions to non-convex optimization problems, and despite giving a near-perfect fit to training data without any explicit effort to control model complexity, these methods exhibit excellent predictive accuracy. We conjecture that specific principles underlie these phenomena: that overparametrization allows gradient methods to find interpolating solutions, that these methods implicitly impose regularization, and that overparametrization leads to benign overfitting, that is, accurate predictions despite overfitting training data. In this article, we survey recent progress in statistical learning theory that provides examples illustrating these principles in simpler settings. We first review classical uniform convergence results and why they fall short of explaining aspects of the behaviour of deep learning methods. We give examples of implicit regularization in simple settings, where gradient methods lead to minimal norm functions that perfectly fit the training data. Then we review prediction methods that exhibit benign overfitting, focusing on regression problems with quadratic loss. For these methods, we can decompose the prediction rule into a simple component that is useful for prediction and a spiky component that is useful for overfitting but, in a favourable setting, does not harm prediction accuracy. We focus specifically on the linear regime for neural networks, where the network can be approximated by a linear model. In this regime, we demonstrate the success of gradient flow, and we consider benign overfitting with two-layer networks, giving an exact asymptotic analysis that precisely demonstrates the impact of overparametrization. We conclude by highlighting the key challenges that arise in extending these insights to realistic deep learning settings.",
            "referenceCount": 24,
            "citationCount": 176,
            "influentialCitationCount": 20,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2103.09177",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-03-16",
            "journal": {
                "name": "Acta Numerica",
                "volume": "30"
            },
            "citationStyles": {
                "bibtex": "@Article{Bartlett2021DeepLA,\n author = {P. Bartlett and A. Montanari and A. Rakhlin},\n booktitle = {Acta Numerica},\n journal = {Acta Numerica},\n pages = {87 - 201},\n title = {Deep learning: a statistical viewpoint},\n volume = {30},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0b24f27d920bd1d23c8f6a1ab2b603c5c14f0a36",
            "@type": "ScholarlyArticle",
            "paperId": "0b24f27d920bd1d23c8f6a1ab2b603c5c14f0a36",
            "corpusId": 231574662,
            "url": "https://www.semanticscholar.org/paper/0b24f27d920bd1d23c8f6a1ab2b603c5c14f0a36",
            "title": "Deep Learning applications for COVID-19",
            "venue": "Journal of Big Data",
            "publicationVenue": {
                "id": "urn:research:d60da343-ab92-4310-b3d7-2c0860287a9d",
                "name": "Journal of Big Data",
                "alternate_names": [
                    "J Big Data",
                    "Journal on Big Data"
                ],
                "issn": "2196-1115",
                "url": "http://www.journalofbigdata.com/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/jbd/ShortenKF21",
                "PubMedCentral": "7797891",
                "DOI": "10.1186/s40537-020-00392-9",
                "CorpusId": 231574662,
                "PubMed": "33457181"
            },
            "abstract": null,
            "referenceCount": 153,
            "citationCount": 184,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journalofbigdata.springeropen.com/counter/pdf/10.1186/s40537-020-00392-9",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-01-11",
            "journal": {
                "name": "Journal of Big Data",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Shorten2021DeepLA,\n author = {Connor Shorten and T. Khoshgoftaar and B. Furht},\n booktitle = {Journal of Big Data},\n journal = {Journal of Big Data},\n title = {Deep Learning applications for COVID-19},\n volume = {8},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:749744b5c797bbcff42dc27820f748ae94cc1932",
            "@type": "ScholarlyArticle",
            "paperId": "749744b5c797bbcff42dc27820f748ae94cc1932",
            "corpusId": 16032859,
            "url": "https://www.semanticscholar.org/paper/749744b5c797bbcff42dc27820f748ae94cc1932",
            "title": "Introduction to Deep Learning",
            "venue": "Advanced Deep Learning for Engineers and Scientists",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "MAG": "3186768639",
                "DOI": "10.1007/978-981-13-8285-7_6",
                "CorpusId": 16032859
            },
            "abstract": null,
            "referenceCount": 26,
            "citationCount": 210,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://hal.science/hal-01352061/document",
                "status": "CLOSED"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2021-12-10",
            "journal": {
                "name": "Advanced Deep Learning for Engineers and Scientists",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sewak2021IntroductionTD,\n author = {Mohit Sewak},\n booktitle = {Advanced Deep Learning for Engineers and Scientists},\n journal = {Advanced Deep Learning for Engineers and Scientists},\n title = {Introduction to Deep Learning},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f0c27af6c330d5c3b0a8eb376a69ce92c85badd7",
            "@type": "ScholarlyArticle",
            "paperId": "f0c27af6c330d5c3b0a8eb376a69ce92c85badd7",
            "corpusId": 235195811,
            "url": "https://www.semanticscholar.org/paper/f0c27af6c330d5c3b0a8eb376a69ce92c85badd7",
            "title": "A Comprehensive Survey on Community Detection with Deep Learning",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2105-12584",
                "ArXiv": "2105.12584",
                "DOI": "10.1109/TNNLS.2021.3137396",
                "CorpusId": 235195811,
                "PubMed": "35263257"
            },
            "abstract": "Detecting a community in a network is a matter of discerning the distinct features and connections of a group of members that are different from those in other communities. The ability to do this is of great significance in network analysis. However, beyond the classic spectral clustering and statistical inference methods, there have been significant developments with deep learning techniques for community detection in recent years--particularly when it comes to handling high-dimensional network data. Hence, a comprehensive review of the latest progress in community detection through deep learning is timely. To frame the survey, we have devised a new taxonomy covering different state-of-the-art methods, including deep learning models based on deep neural networks (DNNs), deep nonnegative matrix factorization, and deep sparse filtering. The main category, i.e., DNNs, is further divided into convolutional networks, graph attention networks, generative adversarial networks, and autoencoders. The popular benchmark datasets, evaluation metrics, and open-source implementations to address experimentation settings are also summarized. This is followed by a discussion on the practical applications of community detection in various domains. The survey concludes with suggestions of challenging topics that would make for fruitful future research directions in this fast-growing deep learning field.",
            "referenceCount": 203,
            "citationCount": 176,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2105.12584",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-05-26",
            "journal": {
                "name": "IEEE transactions on neural networks and learning systems",
                "volume": "PP"
            },
            "citationStyles": {
                "bibtex": "@Article{Su2021ACS,\n author = {Xing Su and Shan Xue and Fanzhen Liu and Jia Wu and Jian Yang and Chuan Zhou and Wenbin Hu and Cecile Paris and S. Nepal and Di Jin and Quan Z. Sheng and Philip S. Yu},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE transactions on neural networks and learning systems},\n title = {A Comprehensive Survey on Community Detection with Deep Learning},\n volume = {PP},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2eda2921a8da4b325f9d05f556594a5884c398a7",
            "@type": "ScholarlyArticle",
            "paperId": "2eda2921a8da4b325f9d05f556594a5884c398a7",
            "corpusId": 211505864,
            "url": "https://www.semanticscholar.org/paper/2eda2921a8da4b325f9d05f556594a5884c398a7",
            "title": "Overfitting in adversarially robust deep learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3008105014",
                "DBLP": "conf/icml/RiceWK20",
                "ArXiv": "2002.11569",
                "CorpusId": 211505864
            },
            "abstract": "It is common practice in deep learning to use overparameterized networks and train for as long as possible; there are numerous studies that show, both theoretically and empirically, that such practices surprisingly do not unduly harm the generalization performance of the classifier. In this paper, we empirically study this phenomenon in the setting of adversarially trained deep networks, which are trained to minimize the loss under worst-case adversarial perturbations. We find that overfitting to the training set does in fact harm robust performance to a very large degree in adversarially robust training across multiple datasets (SVHN, CIFAR-10, CIFAR-100, and ImageNet) and perturbation models ($\\ell_\\infty$ and $\\ell_2$). Based upon this observed effect, we show that the performance gains of virtually all recent algorithmic improvements upon adversarial training can be matched by simply using early stopping. We also show that effects such as the double descent curve do still occur in adversarially trained models, yet fail to explain the observed overfitting. Finally, we study several classical and modern deep learning remedies for overfitting, including regularization and data augmentation, and find that no approach in isolation improves significantly upon the gains achieved by early stopping. All code for reproducing the experiments as well as pretrained model weights and training logs can be found at this https URL.",
            "referenceCount": 84,
            "citationCount": 551,
            "influentialCitationCount": 89,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-02-26",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wong2020OverfittingIA,\n author = {Eric Wong and Leslie Rice and Zico Kolter},\n booktitle = {International Conference on Machine Learning},\n pages = {8093-8104},\n title = {Overfitting in adversarially robust deep learning},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5b5535418882e9543a33819592c5bf371e68b2c3",
            "@type": "ScholarlyArticle",
            "paperId": "5b5535418882e9543a33819592c5bf371e68b2c3",
            "corpusId": 235485320,
            "url": "https://www.semanticscholar.org/paper/5b5535418882e9543a33819592c5bf371e68b2c3",
            "title": "The Principles of Deep Learning Theory",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2106-10165",
                "ArXiv": "2106.10165",
                "DOI": "10.1017/9781009023405",
                "CorpusId": 235485320
            },
            "abstract": "This textbook establishes a theoretical framework for understanding deep learning models of practical relevance. With an approach that borrows from theoretical physics, Roberts and Yaida provide clear and pedagogical explanations of how realistic deep neural networks actually work. To make results from the theoretical forefront accessible, the authors eschew the subject's traditional emphasis on intimidating formality without sacrificing accuracy. Straightforward and approachable, this volume balances detailed first-principle derivations of novel results with insight and intuition for theorists and practitioners alike. This self-contained textbook is ideal for students and researchers interested in artificial intelligence with minimal prerequisites of linear algebra, calculus, and informal probability theory, and it can easily fill a semester-long course on deep learning theory. For the first time, the exciting practical advances in modern artificial intelligence capabilities can be matched with a set of effective principles, providing a timeless blueprint for theoretical research in deep learning.",
            "referenceCount": 0,
            "citationCount": 169,
            "influentialCitationCount": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/9E54A59B9D1D04773CF9EF5B778C2527/stamped-9781316519332c2_11-36.pdf/pretraining.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-06-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2106.10165"
            },
            "citationStyles": {
                "bibtex": "@Article{Roberts2021ThePO,\n author = {Daniel A. Roberts and Sho Yaida and B. Hanin},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {The Principles of Deep Learning Theory},\n volume = {abs/2106.10165},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2319a491378867c7049b3da055c5df60e1671158",
            "@type": "ScholarlyArticle",
            "paperId": "2319a491378867c7049b3da055c5df60e1671158",
            "corpusId": 15238391,
            "url": "https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158",
            "title": "Playing Atari with Deep Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "journals/corr/MnihKSGAWR13",
                "MAG": "1757796397",
                "ArXiv": "1312.5602",
                "CorpusId": 15238391
            },
            "abstract": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.",
            "referenceCount": 32,
            "citationCount": 9735,
            "influentialCitationCount": 1495,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-12-19",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1312.5602"
            },
            "citationStyles": {
                "bibtex": "@Article{Mnih2013PlayingAW,\n author = {Volodymyr Mnih and K. Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin A. Riedmiller},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Playing Atari with Deep Reinforcement Learning},\n volume = {abs/1312.5602},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1c2efb418f79b5d29913e014a1dfd78865221c39",
            "@type": "ScholarlyArticle",
            "paperId": "1c2efb418f79b5d29913e014a1dfd78865221c39",
            "corpusId": 52195012,
            "url": "https://www.semanticscholar.org/paper/1c2efb418f79b5d29913e014a1dfd78865221c39",
            "title": "Deep learning for time series classification: a review",
            "venue": "Data mining and knowledge discovery",
            "publicationVenue": {
                "id": "urn:research:d263025a-9eaf-443f-9bbf-72377e8d22a6",
                "name": "Data mining and knowledge discovery",
                "alternate_names": [
                    "Data Mining and Knowledge Discovery",
                    "Data Min Knowl Discov",
                    "Data min knowl discov"
                ],
                "issn": "1384-5810",
                "url": "https://www.springer.com/computer/database+management+&+information+retrieval/journal/10618"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1809.04356",
                "MAG": "3102970590",
                "DBLP": "journals/corr/abs-1809-04356",
                "DOI": "10.1007/s10618-019-00619-1",
                "CorpusId": 52195012
            },
            "abstract": null,
            "referenceCount": 153,
            "citationCount": 1904,
            "influentialCitationCount": 141,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10618-019-00619-1.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-09-12",
            "journal": {
                "name": "Data Mining and Knowledge Discovery",
                "volume": "33"
            },
            "citationStyles": {
                "bibtex": "@Article{Fawaz2018DeepLF,\n author = {Hassan Ismail Fawaz and G. Forestier and J. Weber and L. Idoumghar and Pierre-Alain Muller},\n booktitle = {Data mining and knowledge discovery},\n journal = {Data Mining and Knowledge Discovery},\n pages = {917 - 963},\n title = {Deep learning for time series classification: a review},\n volume = {33},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ba91490d9775474e0e7e53f3dd43005ca547240e",
            "@type": "ScholarlyArticle",
            "paperId": "ba91490d9775474e0e7e53f3dd43005ca547240e",
            "corpusId": 219792180,
            "url": "https://www.semanticscholar.org/paper/ba91490d9775474e0e7e53f3dd43005ca547240e",
            "title": "Deep Learning Enabled Semantic Communication Systems",
            "venue": "IEEE Transactions on Signal Processing",
            "publicationVenue": {
                "id": "urn:research:1f6f3f05-6a23-42f0-8d31-98ab8089c1f2",
                "name": "IEEE Transactions on Signal Processing",
                "alternate_names": [
                    "IEEE Trans Signal Process"
                ],
                "issn": "1053-587X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=78"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3036851434",
                "DBLP": "journals/tsp/XieQLJ21",
                "ArXiv": "2006.10685",
                "DOI": "10.1109/TSP.2021.3071210",
                "CorpusId": 219792180
            },
            "abstract": "Recently, deep learned enabled end-to-end communication systems have been developed to merge all physical layer blocks in the traditional communication systems, which make joint transceiver optimization possible. Powered by deep learning, natural language processing has achieved great success in analyzing and understanding a large amount of language texts. Inspired by research results in both areas, we aim to provide a new view on communication systems from the semantic level. Particularly, we propose a deep learning based semantic communication system, named DeepSC, for text transmission. Based on the Transformer, the DeepSC aims at maximizing the system capacity and minimizing the semantic errors by recovering the meaning of sentences, rather than bit- or symbol-errors in traditional communications. Moreover, transfer learning is used to ensure the DeepSC applicable to different communication environments and to accelerate the model training process. To justify the performance of semantic communications accurately, we also initialize a new metric, named sentence similarity. Compared with the traditional communication system without considering semantic information exchange, the proposed DeepSC is more robust to channel variation and is able to achieve better performance, especially in the low signal-to-noise (SNR) regime, as demonstrated by the extensive simulation results.",
            "referenceCount": 39,
            "citationCount": 380,
            "influentialCitationCount": 66,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/78/9307529/09398576.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-06-18",
            "journal": {
                "name": "IEEE Transactions on Signal Processing",
                "volume": "69"
            },
            "citationStyles": {
                "bibtex": "@Article{Xie2020DeepLE,\n author = {Huiqiang Xie and Zhijin Qin and Geoffrey Y. Li and B. Juang},\n booktitle = {IEEE Transactions on Signal Processing},\n journal = {IEEE Transactions on Signal Processing},\n pages = {2663-2675},\n title = {Deep Learning Enabled Semantic Communication Systems},\n volume = {69},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:91e611c3e8705002438fb4439733e47ddec85b5d",
            "@type": "ScholarlyArticle",
            "paperId": "91e611c3e8705002438fb4439733e47ddec85b5d",
            "corpusId": 211082837,
            "url": "https://www.semanticscholar.org/paper/91e611c3e8705002438fb4439733e47ddec85b5d",
            "title": "fastai: A Layered API for Deep Learning",
            "venue": "Inf.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3006436762",
                "ArXiv": "2002.04688",
                "DBLP": "journals/information/HowardG20",
                "DOI": "10.3390/info11020108",
                "CorpusId": 211082837
            },
            "abstract": "fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4\u20135 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching.",
            "referenceCount": 69,
            "citationCount": 647,
            "influentialCitationCount": 44,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2078-2489/11/2/108/pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-02-11",
            "journal": {
                "name": "Inf.",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Howard2020fastaiAL,\n author = {Jeremy Howard and Sylvain Gugger},\n booktitle = {Inf.},\n journal = {Inf.},\n pages = {108},\n title = {fastai: A Layered API for Deep Learning},\n volume = {11},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "@type": "ScholarlyArticle",
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "corpusId": 206594692,
            "url": "https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/cvpr/HeZRS16",
                "MAG": "2949650786",
                "ArXiv": "1512.03385",
                "DOI": "10.1109/cvpr.2016.90",
                "CorpusId": 206594692
            },
            "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
            "referenceCount": 54,
            "citationCount": 148082,
            "influentialCitationCount": 30519,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://repositorio.unal.edu.co/bitstream/unal/81443/1/98670607.2022.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-12-10",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{He2015DeepRL,\n author = {Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {770-778},\n title = {Deep Residual Learning for Image Recognition},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:024006d4c2a89f7acacc6e4438d156525b60a98f",
            "@type": "ScholarlyArticle",
            "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
            "corpusId": 16326763,
            "url": "https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f",
            "title": "Continuous control with deep reinforcement learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2173248099",
                "ArXiv": "1509.02971",
                "DBLP": "journals/corr/LillicrapHPHETS15",
                "CorpusId": 16326763
            },
            "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
            "referenceCount": 34,
            "citationCount": 10207,
            "influentialCitationCount": 2177,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-09-09",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1509.02971"
            },
            "citationStyles": {
                "bibtex": "@Article{Lillicrap2015ContinuousCW,\n author = {T. Lillicrap and Jonathan J. Hunt and A. Pritzel and N. Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Continuous control with deep reinforcement learning},\n volume = {abs/1509.02971},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8388f1be26329fa45e5807e968a641ce170ea078",
            "@type": "ScholarlyArticle",
            "paperId": "8388f1be26329fa45e5807e968a641ce170ea078",
            "corpusId": 11758569,
            "url": "https://www.semanticscholar.org/paper/8388f1be26329fa45e5807e968a641ce170ea078",
            "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2949811265",
                "ArXiv": "1511.06434",
                "DBLP": "journals/corr/RadfordMC15",
                "CorpusId": 11758569
            },
            "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",
            "referenceCount": 46,
            "citationCount": 12395,
            "influentialCitationCount": 1830,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-19",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1511.06434"
            },
            "citationStyles": {
                "bibtex": "@Article{Radford2015UnsupervisedRL,\n author = {Alec Radford and Luke Metz and Soumith Chintala},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},\n volume = {abs/1511.06434},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a",
            "@type": "ScholarlyArticle",
            "paperId": "68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a",
            "corpusId": 54560071,
            "url": "https://www.semanticscholar.org/paper/68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a",
            "title": "ADMM-CSNet: A Deep Learning Approach for Image Compressive Sensing",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/pami/YangSLX20",
                "MAG": "2902719825",
                "DOI": "10.1109/TPAMI.2018.2883941",
                "CorpusId": 54560071,
                "PubMed": "30507495"
            },
            "abstract": "Compressive sensing (CS) is an effective technique for reconstructing image from a small amount of sampled data. It has been widely applied in medical imaging, remote sensing, image compression, etc. In this paper, we propose two versions of a novel deep learning architecture, dubbed as ADMM-CSNet, by combining the traditional model-based CS method and data-driven deep learning method for image reconstruction from sparsely sampled measurements. We first consider a generalized CS model for image reconstruction with undetermined regularizations in undetermined transform domains, and then two efficient solvers using Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing the model are proposed. We further unroll and generalize the ADMM algorithm to be two deep architectures, in which all parameters of the CS model and the ADMM algorithm are discriminatively learned by end-to-end training. For both applications of fast CS complex-valued MR imaging and CS imaging of real-valued natural images, the proposed ADMM-CSNet achieved favorable reconstruction accuracy in fast computational speed compared with the traditional and the other deep learning methods.",
            "referenceCount": 67,
            "citationCount": 423,
            "influentialCitationCount": 41,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-03-01",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "42"
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2020ADMMCSNetAD,\n author = {Yan Yang and Jian Sun and Huibin Li and Zongben Xu},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {521-538},\n title = {ADMM-CSNet: A Deep Learning Approach for Image Compressive Sensing},\n volume = {42},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f9163156eeba67762a7441db48fe6720106137cd",
            "@type": "ScholarlyArticle",
            "paperId": "f9163156eeba67762a7441db48fe6720106137cd",
            "corpusId": 102354936,
            "url": "https://www.semanticscholar.org/paper/f9163156eeba67762a7441db48fe6720106137cd",
            "title": "Survey on deep learning with class imbalance",
            "venue": "Journal of Big Data",
            "publicationVenue": {
                "id": "urn:research:d60da343-ab92-4310-b3d7-2c0860287a9d",
                "name": "Journal of Big Data",
                "alternate_names": [
                    "J Big Data",
                    "Journal on Big Data"
                ],
                "issn": "2196-1115",
                "url": "http://www.journalofbigdata.com/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2936503027",
                "DBLP": "journals/jbd/JohnsonK19",
                "DOI": "10.1186/s40537-019-0192-5",
                "CorpusId": 102354936
            },
            "abstract": null,
            "referenceCount": 105,
            "citationCount": 1348,
            "influentialCitationCount": 37,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journalofbigdata.springeropen.com/track/pdf/10.1186/s40537-019-0192-5",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-03-19",
            "journal": {
                "name": "Journal of Big Data",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Johnson2019SurveyOD,\n author = {Justin M. Johnson and T. Khoshgoftaar},\n booktitle = {Journal of Big Data},\n journal = {Journal of Big Data},\n pages = {1-54},\n title = {Survey on deep learning with class imbalance},\n volume = {6},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
            "@type": "ScholarlyArticle",
            "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
            "corpusId": 6208256,
            "url": "https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
            "title": "Deep Reinforcement Learning with Double Q-Learning",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/corr/HasseltGS15",
                "MAG": "2952523895",
                "ArXiv": "1509.06461",
                "DOI": "10.1609/aaai.v30i1.10295",
                "CorpusId": 6208256
            },
            "abstract": "\n \n The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.\n \n",
            "referenceCount": 29,
            "citationCount": 5634,
            "influentialCitationCount": 953,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10295/10154",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-09-22",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hasselt2015DeepRL,\n author = {H. V. Hasselt and A. Guez and David Silver},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {2094-2100},\n title = {Deep Reinforcement Learning with Double Q-Learning},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:07cca761749bfe21c2d096ff60f32b574d5c84c4",
            "@type": "ScholarlyArticle",
            "paperId": "07cca761749bfe21c2d096ff60f32b574d5c84c4",
            "corpusId": 220042066,
            "url": "https://www.semanticscholar.org/paper/07cca761749bfe21c2d096ff60f32b574d5c84c4",
            "title": "Normalized Loss Functions for Deep Learning with Noisy Labels",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/icml/MaH00E020",
                "MAG": "3035429562",
                "ArXiv": "2006.13554",
                "CorpusId": 220042066
            },
            "abstract": "Robust loss functions are essential for training accurate deep neural networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown that the commonly used Cross Entropy (CE) loss is not robust to noisy labels. Whilst new loss functions have been designed, they are only partially robust. In this paper, we theoretically show by applying a simple normalization that: any loss can be made robust to noisy labels. However, in practice, simply being robust is not sufficient for a loss function to train accurate DNNs. By investigating several robust loss functions, we find that they suffer from a problem of underfitting. To address this, we propose a framework to build robust loss functions called Active Passive Loss (APL). APL combines two robust loss functions that mutually boost each other. Experiments on benchmark datasets demonstrate that the family of new loss functions created by our APL framework can consistently outperform state-of-the-art methods by large margins, especially under large noise rates such as 60% or 80% incorrect labels.",
            "referenceCount": 39,
            "citationCount": 278,
            "influentialCitationCount": 43,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-06-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ma2020NormalizedLF,\n author = {Xingjun Ma and Hanxun Huang and Yisen Wang and Simone Romano and S. Erfani and J. Bailey},\n booktitle = {International Conference on Machine Learning},\n pages = {6543-6553},\n title = {Normalized Loss Functions for Deep Learning with Noisy Labels},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:31f9eb39d840821979e5df9f34a6e92dd9c879f2",
            "@type": "ScholarlyArticle",
            "paperId": "31f9eb39d840821979e5df9f34a6e92dd9c879f2",
            "corpusId": 6789015,
            "url": "https://www.semanticscholar.org/paper/31f9eb39d840821979e5df9f34a6e92dd9c879f2",
            "title": "Learning Deep Features for Discriminative Localization",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/cvpr/ZhouKLOT16",
                "ArXiv": "1512.04150",
                "MAG": "2950328304",
                "DOI": "10.1109/CVPR.2016.319",
                "CorpusId": 6789015
            },
            "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.",
            "referenceCount": 37,
            "citationCount": 7553,
            "influentialCitationCount": 1462,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1512.04150",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-12-14",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhou2015LearningDF,\n author = {Bolei Zhou and A. Khosla and \u00c0gata Lapedriza and A. Oliva and A. Torralba},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2921-2929},\n title = {Learning Deep Features for Discriminative Localization},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a56bf7ee9a56d8f84079684339a953c2df9ce76b",
            "@type": "ScholarlyArticle",
            "paperId": "a56bf7ee9a56d8f84079684339a953c2df9ce76b",
            "corpusId": 233562906,
            "url": "https://www.semanticscholar.org/paper/a56bf7ee9a56d8f84079684339a953c2df9ce76b",
            "title": "A review on the attention mechanism of deep learning",
            "venue": "Neurocomputing",
            "publicationVenue": {
                "id": "urn:research:df12d289-f447-47d3-8846-75e39de3ab57",
                "name": "Neurocomputing",
                "alternate_names": null,
                "issn": "0925-2312",
                "url": "http://www.elsevier.com/locate/neucom"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/ijon/NiuZY21",
                "MAG": "3146366485",
                "DOI": "10.1016/J.NEUCOM.2021.03.091",
                "CorpusId": 233562906
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 746,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-04-01",
            "journal": {
                "name": "Neurocomputing",
                "volume": "452"
            },
            "citationStyles": {
                "bibtex": "@Article{Niu2021ARO,\n author = {Zhaoyang Niu and G. Zhong and Hui Yu},\n booktitle = {Neurocomputing},\n journal = {Neurocomputing},\n pages = {48-62},\n title = {A review on the attention mechanism of deep learning},\n volume = {452},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7536bce1007a765fd097a7cc8ea62208a8c89b85",
            "@type": "ScholarlyArticle",
            "paperId": "7536bce1007a765fd097a7cc8ea62208a8c89b85",
            "corpusId": 52177403,
            "url": "https://www.semanticscholar.org/paper/7536bce1007a765fd097a7cc8ea62208a8c89b85",
            "title": "Deep Learning for Generic Object Detection: A Survey",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2890715498",
                "DBLP": "journals/ijcv/LiuOWFCLP20",
                "ArXiv": "1809.02165",
                "DOI": "10.1007/s11263-019-01247-4",
                "CorpusId": 52177403
            },
            "abstract": null,
            "referenceCount": 374,
            "citationCount": 1888,
            "influentialCitationCount": 69,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s11263-019-01247-4.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-09-06",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "128"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2018DeepLF,\n author = {Li Liu and Wanli Ouyang and Xiaogang Wang and P. Fieguth and Jie Chen and Xinwang Liu and M. Pietik\u00e4inen},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {261 - 318},\n title = {Deep Learning for Generic Object Detection: A Survey},\n volume = {128},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:89cff2d83ee5ddf9d631bc53668eb8baf260f3af",
            "@type": "ScholarlyArticle",
            "paperId": "89cff2d83ee5ddf9d631bc53668eb8baf260f3af",
            "corpusId": 233033495,
            "url": "https://www.semanticscholar.org/paper/89cff2d83ee5ddf9d631bc53668eb8baf260f3af",
            "title": "Ensemble deep learning: A review",
            "venue": "Engineering applications of artificial intelligence",
            "publicationVenue": {
                "id": "urn:research:1a24ea21-4c37-41d8-9e76-ab802d4afb3e",
                "name": "Engineering applications of artificial intelligence",
                "alternate_names": [
                    "Eng appl artif intell",
                    "Eng Appl Artif Intell",
                    "Engineering Applications of Artificial Intelligence"
                ],
                "issn": "0952-1976",
                "url": "http://www.sciencedirect.com/science/journal/09521976"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/eaai/GanaieHMTS22",
                "ArXiv": "2104.02395",
                "DOI": "10.1016/j.engappai.2022.105151",
                "CorpusId": 233033495
            },
            "abstract": null,
            "referenceCount": 249,
            "citationCount": 473,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://qspace.qu.edu.qa/bitstream/10576/39985/1/Ensemble%20deep%20learning%20%20A%20review.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-04-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2104.02395"
            },
            "citationStyles": {
                "bibtex": "@Article{Ganaie2021EnsembleDL,\n author = {M. A. Ganaie and Minghui Hu and M. Tanveer and P. Suganthan},\n booktitle = {Engineering applications of artificial intelligence},\n journal = {ArXiv},\n title = {Ensemble deep learning: A review},\n volume = {abs/2104.02395},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3af679c9c4f727dd4e6dc69dfde22a871e85da1e",
            "@type": "ScholarlyArticle",
            "paperId": "3af679c9c4f727dd4e6dc69dfde22a871e85da1e",
            "corpusId": 233547699,
            "url": "https://www.semanticscholar.org/paper/3af679c9c4f727dd4e6dc69dfde22a871e85da1e",
            "title": "A survey on deep learning and its applications",
            "venue": "Computer Science Review",
            "publicationVenue": {
                "id": "urn:research:3aa92b7f-af7a-4ebd-8925-1152710bfbc7",
                "name": "Computer Science Review",
                "alternate_names": [
                    "Comput Sci Rev"
                ],
                "issn": "1574-0137",
                "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/710138/description#description"
            },
            "year": 2021,
            "externalIds": {
                "MAG": "3136021864",
                "DBLP": "journals/csr/DongWA21",
                "DOI": "10.1016/J.COSREV.2021.100379",
                "CorpusId": 233547699
            },
            "abstract": null,
            "referenceCount": 199,
            "citationCount": 439,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-05-01",
            "journal": {
                "name": "Comput. Sci. Rev.",
                "volume": "40"
            },
            "citationStyles": {
                "bibtex": "@Article{Dong2021ASO,\n author = {S. Dong and Ping Wang and Khushnood Abbas},\n booktitle = {Computer Science Review},\n journal = {Comput. Sci. Rev.},\n pages = {100379},\n title = {A survey on deep learning and its applications},\n volume = {40},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:87d5b61f5b6fdb0a57fc66b5c5bb428c398eaa86",
            "@type": "ScholarlyArticle",
            "paperId": "87d5b61f5b6fdb0a57fc66b5c5bb428c398eaa86",
            "corpusId": 235495130,
            "url": "https://www.semanticscholar.org/paper/87d5b61f5b6fdb0a57fc66b5c5bb428c398eaa86",
            "title": "Deep learning for AI",
            "venue": "Communications of the ACM",
            "publicationVenue": {
                "id": "urn:research:4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
                "name": "Communications of the ACM",
                "alternate_names": [
                    "Commun ACM",
                    "Communications of The ACM"
                ],
                "issn": "0001-0782",
                "url": "http://www.acm.org/pubs/cacm/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/cacm/BengioLH21",
                "DOI": "10.1145/3448250",
                "CorpusId": 235495130
            },
            "abstract": "How can neural networks learn the rich internal representations required for difficult tasks such as recognizing objects or understanding language?",
            "referenceCount": 87,
            "citationCount": 302,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3448250",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Art",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-06-21",
            "journal": {
                "name": "Communications of the ACM",
                "volume": "64"
            },
            "citationStyles": {
                "bibtex": "@Article{Bengio2021DeepLF,\n author = {Yoshua Bengio and Yann LeCun and Geoffrey E. Hinton},\n booktitle = {Communications of the ACM},\n journal = {Communications of the ACM},\n pages = {58 - 65},\n title = {Deep learning for AI},\n volume = {64},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e02f91d625cd32290d4ede0f31284da115844316",
            "@type": "ScholarlyArticle",
            "paperId": "e02f91d625cd32290d4ede0f31284da115844316",
            "corpusId": 195874065,
            "url": "https://www.semanticscholar.org/paper/e02f91d625cd32290d4ede0f31284da115844316",
            "title": "DeepXDE: A Deep Learning Library for Solving Differential Equations",
            "venue": "AAAI Spring Symposium: MLPS",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/siamrev/LuMMK21",
                "MAG": "2957969499",
                "ArXiv": "1907.04502",
                "DOI": "10.1137/19M1274067",
                "CorpusId": 195874065
            },
            "abstract": "Deep learning has achieved remarkable success in diverse applications; however, its use in solving partial differential equations (PDEs) has emerged only recently. Here, we present an overview of physics-informed neural networks (PINNs), which embed a PDE into the loss of the neural network using automatic differentiation. The PINN algorithm is simple, and it can be applied to different types of PDEs, including integro-differential equations, fractional PDEs, and stochastic PDEs. Moreover, from the implementation point of view, PINNs solve inverse problems as easily as forward problems. We propose a new residual-based adaptive refinement (RAR) method to improve the training efficiency of PINNs. For pedagogical reasons, we compare the PINN algorithm to a standard finite element method. We also present a Python library for PINNs, DeepXDE, which is designed to serve both as an education tool to be used in the classroom as well as a research tool for solving problems in computational science and engineering. Specifically, DeepXDE can solve forward problems given initial and boundary conditions, as well as inverse problems given some extra measurements. DeepXDE supports complex-geometry domains based on the technique of constructive solid geometry, and enables the user code to be compact, resembling closely the mathematical formulation. We introduce the usage of DeepXDE and its customizability, and we also demonstrate the capability of PINNs and the user-friendliness of DeepXDE for five different examples. More broadly, DeepXDE contributes to the more rapid development of the emerging Scientific Machine Learning field.",
            "referenceCount": 62,
            "citationCount": 860,
            "influentialCitationCount": 87,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1907.04502",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-07-10",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1907.04502"
            },
            "citationStyles": {
                "bibtex": "@Article{Lu2019DeepXDEAD,\n author = {Lu Lu and Xuhui Meng and Zhiping Mao and G. Karniadakis},\n booktitle = {AAAI Spring Symposium: MLPS},\n journal = {ArXiv},\n title = {DeepXDE: A Deep Learning Library for Solving Differential Equations},\n volume = {abs/1907.04502},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1f6c3f1def78919f06efe050e9403e85d5fa3ac9",
            "@type": "ScholarlyArticle",
            "paperId": "1f6c3f1def78919f06efe050e9403e85d5fa3ac9",
            "corpusId": 12219403,
            "url": "https://www.semanticscholar.org/paper/1f6c3f1def78919f06efe050e9403e85d5fa3ac9",
            "title": "The Effectiveness of Data Augmentation in Image Classification using Deep Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2775795276",
                "ArXiv": "1712.04621",
                "DBLP": "journals/corr/abs-1712-04621",
                "CorpusId": 12219403
            },
            "abstract": "In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classification. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and flipping input images. We artificially constrain our access to data to a small subset of the ImageNet dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with GANs to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classifier, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.",
            "referenceCount": 18,
            "citationCount": 2323,
            "influentialCitationCount": 48,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-12-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1712.04621"
            },
            "citationStyles": {
                "bibtex": "@Article{Perez2017TheEO,\n author = {Luis Perez and Jason Wang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {The Effectiveness of Data Augmentation in Image Classification using Deep Learning},\n volume = {abs/1712.04621},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c468bbde6a22d961829e1970e6ad5795e05418d1",
            "@type": "ScholarlyArticle",
            "paperId": "c468bbde6a22d961829e1970e6ad5795e05418d1",
            "corpusId": 4766599,
            "url": "https://www.semanticscholar.org/paper/c468bbde6a22d961829e1970e6ad5795e05418d1",
            "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1801.03924",
                "MAG": "2783879794",
                "DBLP": "journals/corr/abs-1801-03924",
                "DOI": "10.1109/CVPR.2018.00068",
                "CorpusId": 4766599
            },
            "abstract": "While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
            "referenceCount": 71,
            "citationCount": 6150,
            "influentialCitationCount": 1602,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1801.03924",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-01-11",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2018TheUE,\n author = {Richard Zhang and Phillip Isola and Alexei A. Efros and Eli Shechtman and Oliver Wang},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {586-595},\n title = {The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f63e917638553414526a0cc8550de4ad2d83fe7a",
            "@type": "ScholarlyArticle",
            "paperId": "f63e917638553414526a0cc8550de4ad2d83fe7a",
            "corpusId": 5273326,
            "url": "https://www.semanticscholar.org/paper/f63e917638553414526a0cc8550de4ad2d83fe7a",
            "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1511.07289",
                "DBLP": "journals/corr/ClevertUH15",
                "MAG": "2949643667",
                "CorpusId": 5273326
            },
            "abstract": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network.",
            "referenceCount": 69,
            "citationCount": 4761,
            "influentialCitationCount": 604,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-23",
            "journal": {
                "name": "arXiv: Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Clevert2015FastAA,\n author = {Djork-Arn\u00e9 Clevert and Thomas Unterthiner and S. Hochreiter},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Learning},\n title = {Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:155f27879f185f1ab04107c91c2ae7cf6a910a03",
            "@type": "ScholarlyArticle",
            "paperId": "155f27879f185f1ab04107c91c2ae7cf6a910a03",
            "corpusId": 62841491,
            "url": "https://www.semanticscholar.org/paper/155f27879f185f1ab04107c91c2ae7cf6a910a03",
            "title": "Deep Learning for Image Super-Resolution: A Survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1902-06068",
                "ArXiv": "1902.06068",
                "MAG": "3013529009",
                "DOI": "10.1109/TPAMI.2020.2982166",
                "CorpusId": 62841491,
                "PubMed": "32217470"
            },
            "abstract": "Image Super-Resolution (SR) is an important class of image processing techniqueso enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. This article aims to provide a comprehensive survey on recent advances of image super-resolution using deep learning approaches. In general, we can roughly group the existing studies of SR techniques into three major categories: supervised SR, unsupervised SR, and domain-specific SR. In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future.",
            "referenceCount": 235,
            "citationCount": 895,
            "influentialCitationCount": 45,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-02-16",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "43"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2019DeepLF,\n author = {Zhihao Wang and Jian Chen and S. Hoi},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {3365-3387},\n title = {Deep Learning for Image Super-Resolution: A Survey},\n volume = {43},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b514949ad8344071c0f342f182390d2d88bcc26d",
            "@type": "ScholarlyArticle",
            "paperId": "b514949ad8344071c0f342f182390d2d88bcc26d",
            "corpusId": 3536399,
            "url": "https://www.semanticscholar.org/paper/b514949ad8344071c0f342f182390d2d88bcc26d",
            "title": "Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey",
            "venue": "IEEE Access",
            "publicationVenue": {
                "id": "urn:research:2633f5b2-c15c-49fe-80f5-07523e770c26",
                "name": "IEEE Access",
                "alternate_names": null,
                "issn": "2169-3536",
                "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2951179338",
                "DBLP": "journals/corr/abs-1801-00553",
                "ArXiv": "1801.00553",
                "DOI": "10.1109/ACCESS.2018.2807385",
                "CorpusId": 3536399
            },
            "abstract": "Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.",
            "referenceCount": 205,
            "citationCount": 1572,
            "influentialCitationCount": 63,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-01-02",
            "journal": {
                "name": "IEEE Access",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Akhtar2018ThreatOA,\n author = {Naveed Akhtar and A. Mian},\n booktitle = {IEEE Access},\n journal = {IEEE Access},\n pages = {14410-14430},\n title = {Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey},\n volume = {6},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1f7eec4c76963a4ba7516ca00e6a2f855667b3f2",
            "@type": "ScholarlyArticle",
            "paperId": "1f7eec4c76963a4ba7516ca00e6a2f855667b3f2",
            "corpusId": 7979241,
            "url": "https://www.semanticscholar.org/paper/1f7eec4c76963a4ba7516ca00e6a2f855667b3f2",
            "title": "Scalable and accurate deep learning with electronic health records",
            "venue": "npj Digital Medicine",
            "publicationVenue": {
                "id": "urn:research:ef485645-f75f-4344-8b9d-3c260e69503b",
                "name": "npj Digital Medicine",
                "alternate_names": [
                    "npj Digit Med"
                ],
                "issn": "2398-6352",
                "url": "http://www.nature.com/npjdigitalmed/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2784499877",
                "PubMedCentral": "6550175",
                "DBLP": "journals/corr/abs-1801-07860",
                "ArXiv": "1801.07860",
                "DOI": "10.1038/s41746-018-0029-1",
                "CorpusId": 7979241,
                "PubMed": "31304302"
            },
            "abstract": null,
            "referenceCount": 121,
            "citationCount": 1603,
            "influentialCitationCount": 58,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41746-018-0029-1.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-01-24",
            "journal": {
                "name": "NPJ Digital Medicine",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Rajkomar2018ScalableAA,\n author = {A. Rajkomar and Eyal Oren and Kai Chen and Andrew M. Dai and Nissan Hajaj and Michaela Hardt and Peter J. Liu and Xiaobing Liu and J. Marcus and Mimi Sun and Patrik Sundberg and H. Yee and Kun Zhang and Yi Zhang and Gerardo Flores and Gavin E Duggan and Jamie Irvine and Quoc V. Le and Kurt Litsch and Alexander Mossin and Justin Tansuwan and De Wang and James Wexler and Jimbo Wilson and Dana Ludwig and S. Volchenboum and Katherine Chou and Michael Pearson and Srinivasan Madabushi and N. Shah and A. Butte and M. Howell and Claire Cui and Greg S. Corrado and Jeffrey Dean},\n booktitle = {npj Digital Medicine},\n journal = {NPJ Digital Medicine},\n title = {Scalable and accurate deep learning with electronic health records},\n volume = {1},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:42ec3db12a2e4628885451b13035c2e975220a25",
            "@type": "ScholarlyArticle",
            "paperId": "42ec3db12a2e4628885451b13035c2e975220a25",
            "corpusId": 53250107,
            "url": "https://www.semanticscholar.org/paper/42ec3db12a2e4628885451b13035c2e975220a25",
            "title": "A Convergence Theory for Deep Learning via Over-Parameterization",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1811.03962",
                "MAG": "2964098911",
                "DBLP": "journals/corr/abs-1811-03962",
                "CorpusId": 53250107
            },
            "abstract": "Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works has been focusing on training neural networks with one hidden layer. The theory of multi-layer networks remains largely unsettled. \nIn this work, we prove why stochastic gradient descent (SGD) can find $\\textit{global minima}$ on the training objective of DNNs in $\\textit{polynomial time}$. We only make two assumptions: the inputs are non-degenerate and the network is over-parameterized. The latter means the network width is sufficiently large: $\\textit{polynomial}$ in $L$, the number of layers and in $n$, the number of samples. \nOur key technique is to derive that, in a sufficiently large neighborhood of the random initialization, the optimization landscape is almost-convex and semi-smooth even with ReLU activations. This implies an equivalence between over-parameterized neural networks and neural tangent kernel (NTK) in the finite (and polynomial) width setting. \nAs concrete examples, starting from randomly initialized weights, we prove that SGD can attain 100% training accuracy in classification tasks, or minimize regression loss in linear convergence speed, with running time polynomial in $n,L$. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).",
            "referenceCount": 67,
            "citationCount": 1220,
            "influentialCitationCount": 191,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-09-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1811.03962"
            },
            "citationStyles": {
                "bibtex": "@Article{Allen-Zhu2018ACT,\n author = {Zeyuan Allen-Zhu and Yuanzhi Li and Zhao Song},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {A Convergence Theory for Deep Learning via Over-Parameterization},\n volume = {abs/1811.03962},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7b9b756ab509cb9f52dbac95e3e901d571f0784f",
            "@type": "ScholarlyArticle",
            "paperId": "7b9b756ab509cb9f52dbac95e3e901d571f0784f",
            "corpusId": 51872504,
            "url": "https://www.semanticscholar.org/paper/7b9b756ab509cb9f52dbac95e3e901d571f0784f",
            "title": "A Survey of the Usages of Deep Learning for Natural Language Processing",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/tnn/OtterMK21",
                "MAG": "3019166713",
                "DOI": "10.1109/TNNLS.2020.2979670",
                "CorpusId": 51872504,
                "PubMed": "32324570"
            },
            "abstract": "Over the last several years, the field of natural language processing has been propelled forward by an explosion in the use of deep learning models. This article provides a brief introduction to the field and a quick overview of deep learning architectures and methods. It then sifts through the plethora of recent studies and summarizes a large assortment of relevant contributions. Analyzed research areas include several core linguistic processing issues in addition to many applications of computational linguistics. A discussion of the current state of the art is then provided along with recommendations for future research in the field.",
            "referenceCount": 278,
            "citationCount": 838,
            "influentialCitationCount": 30,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-04-21",
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "32"
            },
            "citationStyles": {
                "bibtex": "@Article{Otter2020ASO,\n author = {Dan Otter and Julian R. Medina and J. Kalita},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {604-624},\n title = {A Survey of the Usages of Deep Learning for Natural Language Processing},\n volume = {32},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:340f48901f72278f6bf78a04ee5b01df208cc508",
            "@type": "ScholarlyArticle",
            "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
            "corpusId": 205242740,
            "url": "https://www.semanticscholar.org/paper/340f48901f72278f6bf78a04ee5b01df208cc508",
            "title": "Human-level control through deep reinforcement learning",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/nature/MnihKSRVBGRFOPB15",
                "MAG": "2145339207",
                "DOI": "10.1038/nature14236",
                "CorpusId": 205242740,
                "PubMed": "25719670"
            },
            "abstract": null,
            "referenceCount": 38,
            "citationCount": 22188,
            "influentialCitationCount": 3289,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-02-25",
            "journal": {
                "name": "Nature",
                "volume": "518"
            },
            "citationStyles": {
                "bibtex": "@Article{Mnih2015HumanlevelCT,\n author = {Volodymyr Mnih and K. Kavukcuoglu and David Silver and Andrei A. Rusu and J. Veness and Marc G. Bellemare and Alex Graves and Martin A. Riedmiller and A. Fidjeland and Georg Ostrovski and Stig Petersen and Charlie Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and D. Kumaran and Daan Wierstra and S. Legg and D. Hassabis},\n booktitle = {Nature},\n journal = {Nature},\n pages = {529-533},\n title = {Human-level control through deep reinforcement learning},\n volume = {518},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0311ace1d499cadd1cc0c515a625d1d045f60d25",
            "@type": "ScholarlyArticle",
            "paperId": "0311ace1d499cadd1cc0c515a625d1d045f60d25",
            "corpusId": 209501181,
            "url": "https://www.semanticscholar.org/paper/0311ace1d499cadd1cc0c515a625d1d045f60d25",
            "title": "Deep Learning for 3D Point Clouds: A Survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2997320365",
                "DBLP": "journals/pami/GuoWHLLB21",
                "ArXiv": "1912.12033",
                "DOI": "10.1109/TPAMI.2020.3005434",
                "CorpusId": 209501181,
                "PubMed": "32750799"
            },
            "abstract": "Point cloud learning has lately attracted increasing attention due to its wide applications in many areas, such as computer vision, autonomous driving, and robotics. As a dominating technique in AI, deep learning has been successfully used to solve various 2D vision problems. However, deep learning on point clouds is still in its infancy due to the unique challenges faced by the processing of point clouds with deep neural networks. Recently, deep learning on point clouds has become even thriving, with numerous methods being proposed to address different problems in this area. To stimulate future research, this paper presents a comprehensive review of recent progress in deep learning methods for point clouds. It covers three major tasks, including 3D shape classification, 3D object detection and tracking, and 3D point cloud segmentation. It also presents comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions.",
            "referenceCount": 279,
            "citationCount": 1047,
            "influentialCitationCount": 39,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://jultika.oulu.fi/files/nbnfi-fe2022030121340.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-12-27",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "43"
            },
            "citationStyles": {
                "bibtex": "@Article{Guo2019DeepLF,\n author = {Yulan Guo and Hanyun Wang and Qingyong Hu and Hao Liu and Li Liu and Bennamoun},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {4338-4364},\n title = {Deep Learning for 3D Point Clouds: A Survey},\n volume = {43},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:31f10a6f602bef0306ac37322f84f6163c8a8ecb",
            "@type": "ScholarlyArticle",
            "paperId": "31f10a6f602bef0306ac37322f84f6163c8a8ecb",
            "corpusId": 40094999,
            "url": "https://www.semanticscholar.org/paper/31f10a6f602bef0306ac37322f84f6163c8a8ecb",
            "title": "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/abs-1711-05225",
                "MAG": "2770241596",
                "ArXiv": "1711.05225",
                "CorpusId": 40094999
            },
            "abstract": "We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.",
            "referenceCount": 30,
            "citationCount": 2144,
            "influentialCitationCount": 246,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-11-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1711.05225"
            },
            "citationStyles": {
                "bibtex": "@Article{Rajpurkar2017CheXNetRP,\n author = {Pranav Rajpurkar and J. Irvin and Kaylie Zhu and Brandon Yang and Hershel Mehta and Tony Duan and D. Ding and Aarti Bagul and C. Langlotz and K. Shpanskaya and M. Lungren and A. Ng},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning},\n volume = {abs/1711.05225},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d81fc968196e06ccafd7ea4c008b13e1cad1be64",
            "@type": "ScholarlyArticle",
            "paperId": "d81fc968196e06ccafd7ea4c008b13e1cad1be64",
            "corpusId": 4770492,
            "url": "https://www.semanticscholar.org/paper/d81fc968196e06ccafd7ea4c008b13e1cad1be64",
            "title": "An End-to-End Deep Learning Architecture for Graph Classification",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/aaai/ZhangCNC18",
                "MAG": "2788919350",
                "DOI": "10.1609/aaai.v32i1.11782",
                "CorpusId": 4770492
            },
            "abstract": "\n \n Neural networks are typically designed to deal with data in tensor forms. In this paper, we propose a novel neural network architecture accepting graphs of arbitrary structure. Given a dataset containing graphs in the form of (G,y) where G is a graph and y is its class, we aim to develop neural networks that read the graphs directly and learn a classification function. There are two main challenges: 1) how to extract useful features characterizing the rich information encoded in a graph for classification purpose, and 2) how to sequentially read a graph in a meaningful and consistent order. To address the first challenge, we design a localized graph convolution model and show its connection with two graph kernels. To address the second challenge, we design a novel SortPooling layer which sorts graph vertices in a consistent order so that traditional neural networks can be trained on the graphs. Experiments on benchmark graph classification datasets demonstrate that the proposed architecture achieves highly competitive performance with state-of-the-art graph kernels and other graph neural network methods. Moreover, the architecture allows end-to-end gradient-based training with original graphs, without the need to first transform graphs into vectors.\n \n",
            "referenceCount": 36,
            "citationCount": 1193,
            "influentialCitationCount": 229,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11782/11641",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-04-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2018AnED,\n author = {Muhan Zhang and Zhicheng Cui and Marion Neumann and Yixin Chen},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {4438-4445},\n title = {An End-to-End Deep Learning Architecture for Graph Classification},\n year = {2018}\n}\n"
            }
        }
    }
]