[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4edd98e3947d8406ec95518c294721757afffb5d",
            "@type": "ScholarlyArticle",
            "paperId": "4edd98e3947d8406ec95518c294721757afffb5d",
            "corpusId": 38125055,
            "url": "https://www.semanticscholar.org/paper/4edd98e3947d8406ec95518c294721757afffb5d",
            "title": "Deep reinforcement learning for de novo drug design",
            "venue": "Science Advances",
            "publicationVenue": {
                "id": "urn:research:cb30f0c9-2980-4b7d-bbcb-68fc5472b97c",
                "name": "Science Advances",
                "alternate_names": [
                    "Sci Adv"
                ],
                "issn": "2375-2548",
                "url": "http://www.scienceadvances.org/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "3100751385",
                "DBLP": "journals/corr/abs-1711-10907",
                "PubMedCentral": "6059760",
                "ArXiv": "1711.10907",
                "DOI": "10.1126/sciadv.aap7885",
                "CorpusId": 38125055,
                "PubMed": "30050984"
            },
            "abstract": "We introduce an artificial intelligence approach to de novo design of molecules with desired physical or biological properties. We have devised and implemented a novel computational strategy for de novo design of molecules with desired properties termed ReLeaSE (Reinforcement Learning for Structural Evolution). On the basis of deep and reinforcement learning (RL) approaches, ReLeaSE integrates two deep neural networks\u2014generative and predictive\u2014that are trained separately but are used jointly to generate novel targeted chemical libraries. ReLeaSE uses simple representation of molecules by their simplified molecular-input line-entry system (SMILES) strings only. Generative models are trained with a stack-augmented memory network to produce chemically feasible SMILES strings, and predictive models are derived to forecast the desired properties of the de novo\u2013generated compounds. In the first phase of the method, generative and predictive models are trained separately with a supervised learning algorithm. In the second phase, both models are trained jointly with the RL approach to bias the generation of new chemical structures toward those with the desired physical and/or biological properties. In the proof-of-concept study, we have used the ReLeaSE method to design chemical libraries with a bias toward structural complexity or toward compounds with maximal, minimal, or specific range of physical properties, such as melting point or hydrophobicity, or toward compounds with inhibitory activity against Janus protein kinase 2. The approach proposed herein can find a general use for generating targeted chemical libraries of novel compounds optimized for either a single desired property or multiple properties.",
            "referenceCount": 83,
            "citationCount": 781,
            "influentialCitationCount": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://advances.sciencemag.org/content/advances/4/7/eaap7885.full.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-11-29",
            "journal": {
                "name": "Science Advances",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Popova2017DeepRL,\n author = {Mariya Popova and O. Isayev and A. Tropsha},\n booktitle = {Science Advances},\n journal = {Science Advances},\n title = {Deep reinforcement learning for de novo drug design},\n volume = {4},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:754dbc09783980f383e16b8728fb9c21a39bfff0",
            "@type": "ScholarlyArticle",
            "paperId": "754dbc09783980f383e16b8728fb9c21a39bfff0",
            "corpusId": 14269196,
            "url": "https://www.semanticscholar.org/paper/754dbc09783980f383e16b8728fb9c21a39bfff0",
            "title": "On Deep Multi-View Representation Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/corr/WangALB16",
                "MAG": "1883346539",
                "ArXiv": "1602.01024",
                "CorpusId": 14269196
            },
            "abstract": "We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for representation learning while only one view is available at test time. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them experimentally on visual, speech, and language domains. To our knowledge this is the first head-to-head comparison of a variety of such techniques on multiple tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE).",
            "referenceCount": 90,
            "citationCount": 698,
            "influentialCitationCount": 143,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-07-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2015OnDM,\n author = {Weiran Wang and R. Arora and Karen Livescu and J. Bilmes},\n booktitle = {International Conference on Machine Learning},\n pages = {1083-1092},\n title = {On Deep Multi-View Representation Learning},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:707b5787330d5e66d97a3da27f586f58a68780e8",
            "@type": "ScholarlyArticle",
            "paperId": "707b5787330d5e66d97a3da27f586f58a68780e8",
            "corpusId": 8383593,
            "url": "https://www.semanticscholar.org/paper/707b5787330d5e66d97a3da27f586f58a68780e8",
            "title": "Feature Learning Based Deep Supervised Hashing with Pairwise Labels",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1511.03855",
                "DBLP": "journals/corr/LiWK15",
                "MAG": "2207125444",
                "CorpusId": 8383593
            },
            "abstract": "Recent years have witnessed wide application of hashing for large-scale image retrieval. However, most existing hashing methods are based on handcrafted features which might not be optimally compatible with the hashing procedure. Recently, deep hashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks, which have shown better performance than traditional hashing methods with hand-crafted features. Most of these deep hashing methods are supervised whose supervised information is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and hash-code learning. In this paper, we propose a novel deep hashing method, called deep pairwise-supervised hashing (DPSH), to perform simultaneous feature learning and hashcode learning for applications with pairwise labels. Experiments on real datasets show that our DPSH method can outperform other methods to achieve the state-of-the-art performance in image retrieval applications.",
            "referenceCount": 55,
            "citationCount": 593,
            "influentialCitationCount": 148,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-11-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1511.03855"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2015FeatureLB,\n author = {Wu-Jun Li and Sheng Wang and Wang-Cheng Kang},\n booktitle = {International Joint Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Feature Learning Based Deep Supervised Hashing with Pairwise Labels},\n volume = {abs/1511.03855},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3f12f001fdabf293e273e960c7756cb379c50b66",
            "@type": "ScholarlyArticle",
            "paperId": "3f12f001fdabf293e273e960c7756cb379c50b66",
            "corpusId": 12646079,
            "url": "https://www.semanticscholar.org/paper/3f12f001fdabf293e273e960c7756cb379c50b66",
            "title": "Discriminative Learning of Deep Convolutional Feature Point Descriptors",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1869500417",
                "DBLP": "conf/iccv/Simo-SerraTFKFM15",
                "DOI": "10.1109/ICCV.2015.22",
                "CorpusId": 12646079
            },
            "abstract": "Deep learning has revolutionalized image-level tasks such as classification, but patch-level tasks, such as correspondence, still rely on hand-crafted features, e.g. SIFT. In this paper we use Convolutional Neural Networks (CNNs) to learn discriminant patch representations and in particular train a Siamese network with pairs of (non-)corresponding patches. We deal with the large number of potential pairs with the combination of a stochastic sampling of the training set and an aggressive mining strategy biased towards patches that are hard to classify. By using the L2 distance during both training and testing we develop 128-D descriptors whose euclidean distances reflect patch similarity, and which can be used as a drop-in replacement for any task involving SIFT. We demonstrate consistent performance gains over the state of the art, and generalize well against scaling and rotation, perspective transformation, non-rigid deformation, and illumination changes. Our descriptors are efficient to compute and amenable to modern GPUs, and are publicly available.",
            "referenceCount": 35,
            "citationCount": 733,
            "influentialCitationCount": 91,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://upcommons.upc.edu/bitstream/2117/84259/1/1694-Discriminative-Learning-of-Deep-Convolutional-Feature-Point-Descriptors%281%29.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-12-07",
            "journal": {
                "name": "2015 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Simo-Serra2015DiscriminativeLO,\n author = {E. Simo-Serra and Eduard Trulls and Luis Ferraz and Iasonas Kokkinos and P. Fua and F. Moreno-Noguer},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2015 IEEE International Conference on Computer Vision (ICCV)},\n pages = {118-126},\n title = {Discriminative Learning of Deep Convolutional Feature Point Descriptors},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8db9df2eadea654f128c1887722c677c708e8a47",
            "@type": "ScholarlyArticle",
            "paperId": "8db9df2eadea654f128c1887722c677c708e8a47",
            "corpusId": 12064877,
            "url": "https://www.semanticscholar.org/paper/8db9df2eadea654f128c1887722c677c708e8a47",
            "title": "Deep Reinforcement Learning framework for Autonomous Driving",
            "venue": "Autonomous Vehicles and Machines",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1704.02532",
                "MAG": "3100944043",
                "DBLP": "conf/ei-avm/SallabAPY17",
                "DOI": "10.2352/ISSN.2470-1173.2017.19.AVM-023",
                "CorpusId": 12064877
            },
            "abstract": "Reinforcement learning is considered to be a strong AI paradigm which can be used to teach machines through interaction with the environment and learning from their mistakes. Despite its perceived utility, it has not yet been successfully applied in automotive applications. Motivated by the successful demonstrations of learning of Atari games and Go by Google DeepMind, we propose a framework for autonomous driving using deep reinforcement learning. This is of particular relevance as it is difficult to pose autonomous driving as a supervised learning problem due to strong interactions with the environment including other vehicles, pedestrians and roadworks. As it is a relatively new area of research for autonomous driving, we provide a short overview of deep reinforcement learning and then describe our proposed framework. It incorporates Recurrent Neural Networks for information integration, enabling the car to handle partially observable scenarios. It also integrates the recent work on attention models to focus on relevant information, thereby reducing the computational complexity for deployment on embedded hardware. The framework was tested in an open source 3D car racing simulator called TORCS. Our simulation results demonstrate learning of autonomous maneuvering in a scenario of complex road curvatures and simple interaction of other vehicles.",
            "referenceCount": 24,
            "citationCount": 810,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1704.02532",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-04-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sallab2017DeepRL,\n author = {Ahmad El Sallab and Mohammed Abdou and E. Perot and S. Yogamani},\n booktitle = {Autonomous Vehicles and Machines},\n pages = {70-76},\n title = {Deep Reinforcement Learning framework for Autonomous Driving},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:17da94c4491c80b923dd8e6fe0be2b9d4d3c1926",
            "@type": "ScholarlyArticle",
            "paperId": "17da94c4491c80b923dd8e6fe0be2b9d4d3c1926",
            "corpusId": 6134427,
            "url": "https://www.semanticscholar.org/paper/17da94c4491c80b923dd8e6fe0be2b9d4d3c1926",
            "title": "What Is Machine Learning?",
            "venue": "Machine Learning and AI for Healthcare",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2912420099",
                "DOI": "10.1007/978-1-4842-3799-1_3",
                "CorpusId": 6134427
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 2527,
            "influentialCitationCount": 199,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning and AI for Healthcare",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dietterich2019WhatIM,\n author = {Thomas G. Dietterich},\n booktitle = {Machine Learning and AI for Healthcare},\n journal = {Machine Learning and AI for Healthcare},\n title = {What Is Machine Learning?},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:84d2e6eb7772e2fa4fc0b14c28446d00370a45d2",
            "@type": "ScholarlyArticle",
            "paperId": "84d2e6eb7772e2fa4fc0b14c28446d00370a45d2",
            "corpusId": 3356807,
            "url": "https://www.semanticscholar.org/paper/84d2e6eb7772e2fa4fc0b14c28446d00370a45d2",
            "title": "Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "publicationVenue": {
                "id": "urn:research:8dce23a9-44e0-4381-a39e-2acc1edff700",
                "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "alternate_names": [
                    "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "Int ACM SIGIR Conf Res Dev Inf Retr",
                    "SIGIR",
                    "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigir/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/sigir/SeverynM15",
                "MAG": "1966443646",
                "DOI": "10.1145/2766462.2767738",
                "CorpusId": 3356807
            },
            "abstract": "Learning a similarity function between pairs of objects is at the core of learning to rank approaches. In information retrieval tasks we typically deal with query-document pairs, in question answering -- question-answer pairs. However, before learning can take place, such pairs needs to be mapped from the original space of symbolic words into some feature space encoding various aspects of their relatedness, e.g. lexical, syntactic and semantic. Feature engineering is often a laborious task and may require external knowledge sources that are not always available or difficult to obtain. Recently, deep learning approaches have gained a lot of attention from the research community and industry for their ability to automatically learn optimal feature representation for a given task, while claiming state-of-the-art performance in many tasks in computer vision, speech recognition and natural language processing. In this paper, we present a convolutional neural network architecture for reranking pairs of short texts, where we learn the optimal representation of text pairs and a similarity function to relate them in a supervised way from the available training data. Our network takes only words in the input, thus requiring minimal preprocessing. In particular, we consider the task of reranking short text pairs where elements of the pair are sentences. We test our deep learning system on two popular retrieval tasks from TREC: Question Answering and Microblog Retrieval. Our model demonstrates strong performance on the first task beating previous state-of-the-art systems by about 3\\% absolute points in both MAP and MRR and shows comparable results on tweet reranking, while enjoying the benefits of no manual feature engineering and no additional syntactic parsers.",
            "referenceCount": 42,
            "citationCount": 716,
            "influentialCitationCount": 80,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2015-08-09",
            "journal": {
                "name": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Severyn2015LearningTR,\n author = {Aliaksei Severyn and Alessandro Moschitti},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n title = {Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9c21b3ffdac5f2450b82dd6660ac69f72bb9018b",
            "@type": "ScholarlyArticle",
            "paperId": "9c21b3ffdac5f2450b82dd6660ac69f72bb9018b",
            "corpusId": 15774646,
            "url": "https://www.semanticscholar.org/paper/9c21b3ffdac5f2450b82dd6660ac69f72bb9018b",
            "title": "Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1803059841",
                "DBLP": "journals/pami/LiuSL016",
                "ArXiv": "1502.07411",
                "DOI": "10.1109/TPAMI.2015.2505283",
                "CorpusId": 15774646,
                "PubMed": "26660697"
            },
            "abstract": "In this article, we tackle the problem of depth estimation from single monocular images. Compared with depth estimation using multiple images such as stereo depth perception, depth from monocular images is much more challenging. Prior work typically focuses on exploiting geometric priors or additional sources of information, most using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) set new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimation can be naturally formulated as a continuous conditional random field (CRF) learning problem. Therefore, here we present a deep convolutional neural field model for estimating depths from single monocular images, aiming to jointly explore the capacity of deep CNN and continuous CRF. In particular, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework. We then further propose an equally effective model based on fully convolutional networks and a novel superpixel pooling method, which is about 10 times faster, to speedup the patch-wise convolutions in the deep model. With this more efficient model, we are able to design deeper networks to pursue better performance. Our proposed method can be used for depth estimation of general scenes with no geometric priors nor any extra information injected. In our case, the integral of the partition function can be calculated in a closed form such that we can exactly solve the log-likelihood maximization. Moreover, solving the inference problem for predicting depths of a test image is highly efficient as closed-form solutions exist. Experiments on both indoor and outdoor scene datasets demonstrate that the proposed method outperforms state-of-the-art depth estimation approaches.",
            "referenceCount": 44,
            "citationCount": 1080,
            "influentialCitationCount": 78,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1502.07411",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-02-25",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "38"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2015LearningDF,\n author = {Fayao Liu and Chunhua Shen and Guosheng Lin and I. Reid},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {2024-2039},\n title = {Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields},\n volume = {38},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:32f75852c5eb5ec6d254818d054e57a90e76b3ba",
            "@type": "ScholarlyArticle",
            "paperId": "32f75852c5eb5ec6d254818d054e57a90e76b3ba",
            "corpusId": 16673459,
            "url": "https://www.semanticscholar.org/paper/32f75852c5eb5ec6d254818d054e57a90e76b3ba",
            "title": "Deep Architecture for Traffic Flow Prediction: Deep Belief Networks With Multitask Learning",
            "venue": "IEEE transactions on intelligent transportation systems (Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "journals/tits/HuangSHX14",
                "MAG": "2165991108",
                "DOI": "10.1109/TITS.2014.2311123",
                "CorpusId": 16673459
            },
            "abstract": "Traffic flow prediction is a fundamental problem in transportation modeling and management. Many existing approaches fail to provide favorable results due to being: 1) shallow in architecture; 2) hand engineered in features; and 3) separate in learning. In this paper we propose a deep architecture that consists of two parts, i.e., a deep belief network (DBN) at the bottom and a multitask regression layer at the top. A DBN is employed here for unsupervised feature learning. It can learn effective features for traffic flow prediction in an unsupervised fashion, which has been examined and found to be effective for many areas such as image and audio classification. To the best of our knowledge, this is the first paper that applies the deep learning approach to transportation research. To incorporate multitask learning (MTL) in our deep architecture, a multitask regression layer is used above the DBN for supervised prediction. We further investigate homogeneous MTL and heterogeneous MTL for traffic flow prediction. To take full advantage of weight sharing in our deep architecture, we propose a grouping method based on the weights in the top layer to make MTL more effective. Experiments on transportation data sets show good performance of our deep architecture. Abundant experiments show that our approach achieved close to 5% improvements over the state of the art. It is also presented that MTL can improve the generalization performance of shared tasks. These positive results demonstrate that deep learning and MTL are promising in transportation research.",
            "referenceCount": 39,
            "citationCount": 911,
            "influentialCitationCount": 39,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2014-04-10",
            "journal": {
                "name": "IEEE Transactions on Intelligent Transportation Systems",
                "volume": "15"
            },
            "citationStyles": {
                "bibtex": "@Article{Huang2014DeepAF,\n author = {Wenhao Huang and Guojie Song and Haikun Hong and Kunqing Xie},\n booktitle = {IEEE transactions on intelligent transportation systems (Print)},\n journal = {IEEE Transactions on Intelligent Transportation Systems},\n pages = {2191-2201},\n title = {Deep Architecture for Traffic Flow Prediction: Deep Belief Networks With Multitask Learning},\n volume = {15},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:04ea4e24a418b80eea17faacfa00ee717d8867bb",
            "@type": "ScholarlyArticle",
            "paperId": "04ea4e24a418b80eea17faacfa00ee717d8867bb",
            "corpusId": 6700794,
            "url": "https://www.semanticscholar.org/paper/04ea4e24a418b80eea17faacfa00ee717d8867bb",
            "title": "Manifold Learning of Brain MRIs by Deep Learning",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "publicationVenue": {
                "id": "urn:research:61a709e3-2060-423c-8de5-ffd3885aa31c",
                "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                "alternate_names": [
                    "Medical Image Computing and Computer-Assisted Intervention",
                    "MICCAI",
                    "Med Image Comput Comput Interv",
                    "Int Conf Med Image Comput Comput Interv"
                ],
                "issn": null,
                "url": "http://www.miccai.org/"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "conf/miccai/BroschT13",
                "MAG": "1908634864",
                "DOI": "10.1007/978-3-642-40763-5_78",
                "CorpusId": 6700794,
                "PubMed": "24579194"
            },
            "abstract": null,
            "referenceCount": 14,
            "citationCount": 222,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-642-40763-5_78.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-09-22",
            "journal": {
                "name": "Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",
                "volume": "16 Pt 2"
            },
            "citationStyles": {
                "bibtex": "@Article{Brosch2013ManifoldLO,\n author = {T. Brosch and R. Tam},\n booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},\n journal = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},\n pages = {\n          633-40\n        },\n title = {Manifold Learning of Brain MRIs by Deep Learning},\n volume = {16 Pt 2},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:184ac0766262312ba76bbdece4e7ffad0aa8180b",
            "@type": "ScholarlyArticle",
            "paperId": "184ac0766262312ba76bbdece4e7ffad0aa8180b",
            "corpusId": 393948,
            "url": "https://www.semanticscholar.org/paper/184ac0766262312ba76bbdece4e7ffad0aa8180b",
            "title": "Representation Learning: A Review and New Perspectives",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2012,
            "externalIds": {
                "ArXiv": "1206.5538",
                "MAG": "2952111767",
                "DBLP": "journals/pami/BengioCV13",
                "DOI": "10.1109/TPAMI.2013.50",
                "CorpusId": 393948,
                "PubMed": "23787338"
            },
            "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.",
            "referenceCount": 267,
            "citationCount": 10782,
            "influentialCitationCount": 551,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.princeton.edu/courses/archive/spring13/cos598C/Representation Learning - A Review and New Perspectives.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2012-06-24",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{Bengio2012RepresentationLA,\n author = {Yoshua Bengio and Aaron C. Courville and Pascal Vincent},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {1798-1828},\n title = {Representation Learning: A Review and New Perspectives},\n volume = {35},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e0b65d3839e3bf703d156b524d7db7a5e10a2623",
            "@type": "ScholarlyArticle",
            "paperId": "e0b65d3839e3bf703d156b524d7db7a5e10a2623",
            "corpusId": 13973139,
            "url": "https://www.semanticscholar.org/paper/e0b65d3839e3bf703d156b524d7db7a5e10a2623",
            "title": "Playing FPS Games with Deep Reinforcement Learning",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1609.05521",
                "MAG": "2522489477",
                "DBLP": "journals/corr/LampleC16",
                "DOI": "10.1609/aaai.v31i1.10827",
                "CorpusId": 13973139
            },
            "abstract": "\n \n Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in AI agents of the game as well as average humans in deathmatch scenarios.\n \n",
            "referenceCount": 20,
            "citationCount": 505,
            "influentialCitationCount": 30,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10827/10686",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-09-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1609.05521"
            },
            "citationStyles": {
                "bibtex": "@Article{Lample2016PlayingFG,\n author = {Guillaume Lample and Devendra Singh Chaplot},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Playing FPS Games with Deep Reinforcement Learning},\n volume = {abs/1609.05521},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:31f5864ada5fb08b69da74b6d5ad99e385dcc737",
            "@type": "ScholarlyArticle",
            "paperId": "31f5864ada5fb08b69da74b6d5ad99e385dcc737",
            "corpusId": 7473831,
            "url": "https://www.semanticscholar.org/paper/31f5864ada5fb08b69da74b6d5ad99e385dcc737",
            "title": "Sentence Simplification with Deep Reinforcement Learning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2605243085",
                "DBLP": "journals/corr/ZhangL17d",
                "ACL": "D17-1062",
                "ArXiv": "1703.10931",
                "DOI": "10.18653/v1/D17-1062",
                "CorpusId": 7473831
            },
            "abstract": "Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call DRESS (as shorthand for Deep REinforcement Sentence Simplification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.",
            "referenceCount": 51,
            "citationCount": 347,
            "influentialCitationCount": 70,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D17-1062.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-31",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2017SentenceSW,\n author = {Xingxing Zhang and Mirella Lapata},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {584-594},\n title = {Sentence Simplification with Deep Reinforcement Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b89b8ff76a1c9b2f71d968fe817cae29209329eb",
            "@type": "ScholarlyArticle",
            "paperId": "b89b8ff76a1c9b2f71d968fe817cae29209329eb",
            "corpusId": 204082601,
            "url": "https://www.semanticscholar.org/paper/b89b8ff76a1c9b2f71d968fe817cae29209329eb",
            "title": "Deep Residual Shrinkage Networks for Fault Diagnosis",
            "venue": "IEEE Transactions on Industrial Informatics",
            "publicationVenue": {
                "id": "urn:research:2135230a-3b24-4b71-9583-60624389377a",
                "name": "IEEE Transactions on Industrial Informatics",
                "alternate_names": [
                    "IEEE Trans Ind Informatics"
                ],
                "issn": "1551-3203",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=9424"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/tii/ZhaoZFTP20",
                "MAG": "2977117446",
                "DOI": "10.1109/TII.2019.2943898",
                "CorpusId": 204082601
            },
            "abstract": "This article develops new deep learning methods, namely, deep residual shrinkage networks, to improve the feature learning ability from highly noised vibration signals and achieve a high fault diagnosing accuracy. Soft thresholding is inserted as nonlinear transformation layers into the deep architectures to eliminate unimportant features. Moreover, considering that it is generally challenging to set proper values for the thresholds, the developed deep residual shrinkage networks integrate a few specialized neural networks as trainable modules to automatically determine the thresholds, so that professional expertise on signal processing is not required. The efficacy of the developed methods is validated through experiments with various types of noise.",
            "referenceCount": 25,
            "citationCount": 479,
            "influentialCitationCount": 53,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-07-01",
            "journal": {
                "name": "IEEE Transactions on Industrial Informatics",
                "volume": "16"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhao2020DeepRS,\n author = {Minghang Zhao and S. Zhong and Xu-yun Fu and B. Tang and M. Pecht},\n booktitle = {IEEE Transactions on Industrial Informatics},\n journal = {IEEE Transactions on Industrial Informatics},\n pages = {4681-4690},\n title = {Deep Residual Shrinkage Networks for Fault Diagnosis},\n volume = {16},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cc75c81148245198d4c6d4c8fcbbae487215903b",
            "@type": "ScholarlyArticle",
            "paperId": "cc75c81148245198d4c6d4c8fcbbae487215903b",
            "corpusId": 16900529,
            "url": "https://www.semanticscholar.org/paper/cc75c81148245198d4c6d4c8fcbbae487215903b",
            "title": "Convolutional-Recursive Deep Learning for 3D Object Classification",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "conf/nips/SocherHBMN12",
                "MAG": "2109992539",
                "CorpusId": 16900529
            },
            "abstract": "Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We introduce a model based on a combination of convolutional and recursive neural networks (CNN and RNN) for learning features and classifying RGB-D images. The CNN layer learns low-level translationally invariant features which are then given as inputs to multiple, fixed-tree RNNs in order to compose higher order features. RNNs can be seen as combining convolution and pooling into one efficient, hierarchical operation. Our main result is that even RNNs with random weights compose powerful features. Our model obtains state of the art performance on a standard RGB-D object dataset while being more accurate and faster during training and testing than comparable architectures such as two-layer CNNs.",
            "referenceCount": 32,
            "citationCount": 636,
            "influentialCitationCount": 47,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2012-12-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Socher2012ConvolutionalRecursiveDL,\n author = {R. Socher and Brody Huval and Bharath Putta Bath and Christopher D. Manning and A. Ng},\n booktitle = {Neural Information Processing Systems},\n pages = {665-673},\n title = {Convolutional-Recursive Deep Learning for 3D Object Classification},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:aca80df832d1f0130a6d0d8fa09bea697d761031",
            "@type": "ScholarlyArticle",
            "paperId": "aca80df832d1f0130a6d0d8fa09bea697d761031",
            "corpusId": 6380284,
            "url": "https://www.semanticscholar.org/paper/aca80df832d1f0130a6d0d8fa09bea697d761031",
            "title": "Deep Learning",
            "venue": "Encyclopedia of Machine Learning and Data Mining",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "reference/ml/Schmidhuber17",
                "DOI": "10.1007/978-1-4899-7687-1_909",
                "CorpusId": 6380284
            },
            "abstract": null,
            "referenceCount": 23,
            "citationCount": 8,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1411.4555v1.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Schmidhuber2015DeepL,\n author = {J. Schmidhuber},\n booktitle = {Encyclopedia of Machine Learning and Data Mining},\n pages = {338-348},\n title = {Deep Learning},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e49c80f8b12a100c5f4518897c4cbf72710c252",
            "@type": "ScholarlyArticle",
            "paperId": "5e49c80f8b12a100c5f4518897c4cbf72710c252",
            "corpusId": 46939951,
            "url": "https://www.semanticscholar.org/paper/5e49c80f8b12a100c5f4518897c4cbf72710c252",
            "title": "Relational Deep Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2807340089",
                "DBLP": "journals/corr/abs-1806-01830",
                "ArXiv": "1806.01830",
                "CorpusId": 46939951
            },
            "abstract": "We introduce an approach for deep reinforcement learning (RL) that improves upon the efficiency, generalization capacity, and interpretability of conventional approaches through structured perception and relational reasoning. It uses self-attention to iteratively reason about the relations between entities in a scene and to guide a model-free policy. Our results show that in a novel navigation and planning task called Box-World, our agent finds interpretable solutions that improve upon baselines in terms of sample complexity, ability to generalize to more complex scenes than experienced during training, and overall performance. In the StarCraft II Learning Environment, our agent achieves state-of-the-art performance on six mini-games -- surpassing human grandmaster performance on four. By considering architectural inductive biases, our work opens new directions for overcoming important, but stubborn, challenges in deep RL.",
            "referenceCount": 39,
            "citationCount": 196,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.01830"
            },
            "citationStyles": {
                "bibtex": "@Article{Zambaldi2018RelationalDR,\n author = {V. Zambaldi and David Raposo and Adam Santoro and V. Bapst and Yujia Li and Igor Babuschkin and K. Tuyls and David P. Reichert and T. Lillicrap and Edward Lockhart and M. Shanahan and Victoria Langston and Razvan Pascanu and M. Botvinick and Oriol Vinyals and P. Battaglia},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Relational Deep Reinforcement Learning},\n volume = {abs/1806.01830},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6337bd57197c818fb5ab539004d64b0cf51ce2d5",
            "@type": "ScholarlyArticle",
            "paperId": "6337bd57197c818fb5ab539004d64b0cf51ce2d5",
            "corpusId": 2535201,
            "url": "https://www.semanticscholar.org/paper/6337bd57197c818fb5ab539004d64b0cf51ce2d5",
            "title": "Reinforcement learning",
            "venue": "Scholarpedia",
            "publicationVenue": {
                "id": "urn:research:856e61df-ae80-4713-a1f1-6afd81e6e2b1",
                "name": "Scholarpedia",
                "alternate_names": null,
                "issn": "1941-6016",
                "url": "http://www.scholarpedia.org/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "reference/ml/Stone10a",
                "DOI": "10.4249/scholarpedia.1448",
                "CorpusId": 2535201
            },
            "abstract": "The discussion here considers a much more common learning condition where an agent, such as a human or a robot, has to learn to make decisions in the environment from simple feedback. Such feedback is provided only after periods of actions in the form of reward or punishment without detailing which of the actions has contributed to the outcome. This type of learning scenario is called reinforcement learning. This learning problem is formalized in a Markov decision-making process with a variety of related algorithms. The second part of this chapter will use function approximators with neural networks which have made recent progress as deep reinforcement learning.",
            "referenceCount": 114,
            "citationCount": 3263,
            "influentialCitationCount": 223,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-08-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{W\u00f6rg\u00f6tter2019ReinforcementL,\n author = {F. W\u00f6rg\u00f6tter and B. Porr},\n booktitle = {Scholarpedia},\n pages = {1088-1090},\n title = {Reinforcement learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e061d23b68e7d4aac5aece4794c044c80e638dca",
            "@type": "ScholarlyArticle",
            "paperId": "e061d23b68e7d4aac5aece4794c044c80e638dca",
            "corpusId": 52065462,
            "url": "https://www.semanticscholar.org/paper/e061d23b68e7d4aac5aece4794c044c80e638dca",
            "title": "Co-teaching: Robust training of deep neural networks with extremely noisy labels",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2803237185",
                "DBLP": "conf/nips/HanYYNXHTS18",
                "ArXiv": "1804.06872",
                "CorpusId": 52065462
            },
            "abstract": "Deep learning with noisy labels is practically challenging, as the capacity of deep models is so high that they can totally memorize these noisy labels sooner or later during training. Nonetheless, recent studies on the memorization effects of deep neural networks show that they would first memorize training data of clean labels and then those of noisy labels. Therefore in this paper, we propose a new deep learning paradigm called Co-teaching for combating with noisy labels. Namely, we train two deep neural networks simultaneously, and let them teach each other given every mini-batch: firstly, each network feeds forward all data and selects some data of possibly clean labels; secondly, two networks communicate with each other what data in this mini-batch should be used for training; finally, each network back propagates the data selected by its peer network and updates itself. Empirical results on noisy versions of MNIST, CIFAR-10 and CIFAR-100 demonstrate that Co-teaching is much superior to the state-of-the-art methods in the robustness of trained deep models.",
            "referenceCount": 46,
            "citationCount": 1449,
            "influentialCitationCount": 336,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-04-18",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Han2018CoteachingRT,\n author = {Bo Han and Quanming Yao and Xingrui Yu and Gang Niu and Miao Xu and Weihua Hu and I. Tsang and Masashi Sugiyama},\n booktitle = {Neural Information Processing Systems},\n pages = {8536-8546},\n title = {Co-teaching: Robust training of deep neural networks with extremely noisy labels},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:df787a974fff59f557ed1ec620fc345568aec491",
            "@type": "ScholarlyArticle",
            "paperId": "df787a974fff59f557ed1ec620fc345568aec491",
            "corpusId": 2701448,
            "url": "https://www.semanticscholar.org/paper/df787a974fff59f557ed1ec620fc345568aec491",
            "title": "Learning Deep Representations for Graph Clustering",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "conf/aaai/TianGCCL14",
                "MAG": "2405933695",
                "DOI": "10.1609/aaai.v28i1.8916",
                "CorpusId": 2701448
            },
            "abstract": "\n \n Recently deep learning has been successfully adopted in many applications such as speech recognition and image classification. In this work, we explore the possibility of employing deep learning in graph clustering. We propose a simple method, which first learns a nonlinear embedding of the original graph by stacked autoencoder, and then runs $k$-means algorithm on the embedding to obtain the clustering result. We show that this simple method has solid theoretical foundation, due to the similarity between autoencoder and spectral clustering in terms of what they actually optimize. Then, we demonstrate that the proposed method is more efficient and flexible than spectral clustering. First, the computational complexity of autoencoder is much lower than spectral clustering: the former can be linear to the number of nodes in a sparse graph while the latter is super quadratic due to eigenvalue decomposition. Second, when additional sparsity constraint is imposed, we can simply employ the sparse autoencoder developed in the literature of deep learning; however, it is non-straightforward to implement a sparse spectral method. The experimental results on various graph datasets show that the proposed method significantly outperforms conventional spectral clustering which clearly indicates the effectiveness of deep learning in graph clustering.\n \n",
            "referenceCount": 33,
            "citationCount": 557,
            "influentialCitationCount": 48,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/8916/8775",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-06-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tian2014LearningDR,\n author = {Fei Tian and Bin Gao and Qing Cui and Enhong Chen and Tie-Yan Liu},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {1293-1299},\n title = {Learning Deep Representations for Graph Clustering},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6af440915b8a0718c93be1cf61905e41e620484a",
            "@type": "ScholarlyArticle",
            "paperId": "6af440915b8a0718c93be1cf61905e41e620484a",
            "corpusId": 49312162,
            "url": "https://www.semanticscholar.org/paper/6af440915b8a0718c93be1cf61905e41e620484a",
            "title": "Deep One-Class Classification",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/icml/RuffGDSVBMK18",
                "MAG": "2803697594",
                "CorpusId": 49312162
            },
            "abstract": "Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objec-tive. In this paper we introduce a new anomaly detection method\u2014Deep Support Vector Data Description\u2014, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GT-SRB stop signs.",
            "referenceCount": 58,
            "citationCount": 1360,
            "influentialCitationCount": 297,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-07-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ruff2018DeepOC,\n author = {Lukas Ruff and Nico G\u00f6rnitz and Lucas Deecke and Shoaib Ahmed Siddiqui and Robert A. Vandermeulen and Alexander Binder and Emmanuel M\u00fcller and M. Kloft},\n booktitle = {International Conference on Machine Learning},\n pages = {4390-4399},\n title = {Deep One-Class Classification},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2623847aeeac58f976a4f94d556bf8488b5d5b63",
            "@type": "ScholarlyArticle",
            "paperId": "2623847aeeac58f976a4f94d556bf8488b5d5b63",
            "corpusId": 17267607,
            "url": "https://www.semanticscholar.org/paper/2623847aeeac58f976a4f94d556bf8488b5d5b63",
            "title": "Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/corr/BashivanRYC15",
                "MAG": "2962699674",
                "ArXiv": "1511.06448",
                "CorpusId": 17267607
            },
            "abstract": "One of the challenges in modeling cognitive events from electroencephalogram (EEG) data is finding representations that are invariant to inter- and intra-subject differences, as well as to inherent noise associated with such data. Herein, we propose a novel approach for learning such representations from multi-channel EEG time-series, and demonstrate its advantages in the context of mental load classification task. First, we transform EEG activities into a sequence of topology-preserving multi-spectral images, as opposed to standard EEG analysis techniques that ignore such spatial information. Next, we train a deep recurrent-convolutional network inspired by state-of-the-art video classification to learn robust representations from the sequence of images. The proposed approach is designed to preserve the spatial, spectral, and temporal structure of EEG which leads to finding features that are less sensitive to variations and distortions within each dimension. Empirical evaluation on the cognitive load classification task demonstrated significant improvements in classification accuracy over current state-of-the-art approaches in this field.",
            "referenceCount": 35,
            "citationCount": 564,
            "influentialCitationCount": 64,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-19",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1511.06448"
            },
            "citationStyles": {
                "bibtex": "@Article{Bashivan2015LearningRF,\n author = {P. Bashivan and I. Rish and M. Yeasin and N. Codella},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks},\n volume = {abs/1511.06448},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b27e791e843c924ef052981b79490ab59fc0433d",
            "@type": "ScholarlyArticle",
            "paperId": "b27e791e843c924ef052981b79490ab59fc0433d",
            "corpusId": 9059202,
            "url": "https://www.semanticscholar.org/paper/b27e791e843c924ef052981b79490ab59fc0433d",
            "title": "Learning Deep Structure-Preserving Image-Text Embeddings",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2950344816",
                "ArXiv": "1511.06078",
                "DBLP": "journals/corr/WangLL15",
                "DOI": "10.1109/CVPR.2016.541",
                "CorpusId": 9059202
            },
            "abstract": "This paper proposes a method for learning joint embeddings of images and text using a two-branch neural network with multiple layers of linear projections followed by nonlinearities. The network is trained using a large-margin objective that combines cross-view ranking constraints with within-view neighborhood structure preservation constraints inspired by metric learning literature. Extensive experiments show that our approach gains significant improvements in accuracy for image-to-text and text-to-image retrieval. Our method achieves new state-of-the-art results on the Flickr30K and MSCOCO image-sentence datasets and shows promise on the new task of phrase localization on the Flickr30K Entities dataset.",
            "referenceCount": 55,
            "citationCount": 711,
            "influentialCitationCount": 79,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1511.06078",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-11-19",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2015LearningDS,\n author = {Liwei Wang and Yin Li and S. Lazebnik},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5005-5013},\n title = {Learning Deep Structure-Preserving Image-Text Embeddings},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:83bf91012997019f432179aad798e6d3fbb95c36",
            "@type": "ScholarlyArticle",
            "paperId": "83bf91012997019f432179aad798e6d3fbb95c36",
            "corpusId": 12046082,
            "url": "https://www.semanticscholar.org/paper/83bf91012997019f432179aad798e6d3fbb95c36",
            "title": "Multiagent cooperation and competition with deep reinforcement learning",
            "venue": "PLoS ONE",
            "publicationVenue": {
                "id": "urn:research:0aed7a40-85f3-4c66-9e1b-c1556c57001b",
                "name": "PLoS ONE",
                "alternate_names": [
                    "Plo ONE",
                    "PLOS ONE",
                    "PLO ONE"
                ],
                "issn": "1932-6203",
                "url": "https://journals.plos.org/plosone/"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1511.08779",
                "MAG": "2951120231",
                "DBLP": "journals/corr/TampuuMKKKAAV15",
                "PubMedCentral": "5381785",
                "DOI": "10.1371/journal.pone.0172395",
                "CorpusId": 12046082,
                "PubMed": "28380078"
            },
            "abstract": "Evolution of cooperation and competition can appear when multiple adaptive agents share a biological, social, or technological niche. In the present work we study how cooperation and competition emerge between autonomous agents that learn by reinforcement while using only their raw visual input as the state representation. In particular, we extend the Deep Q-Learning framework to multiagent environments to investigate the interaction between two learning agents in the well-known video game Pong. By manipulating the classical rewarding scheme of Pong we show how competitive and collaborative behaviors emerge. We also describe the progression from competitive to collaborative behavior when the incentive to cooperate is increased. Finally we show how learning by playing against another adaptive agent, instead of against a hard-wired algorithm, results in more robust strategies. The present work shows that Deep Q-Networks can become a useful tool for studying decentralized learning of multiagent systems coping with high-dimensional environments.",
            "referenceCount": 39,
            "citationCount": 657,
            "influentialCitationCount": 59,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0172395&type=printable",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Biology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-27",
            "journal": {
                "name": "PLoS ONE",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Tampuu2015MultiagentCA,\n author = {Ardi Tampuu and Tambet Matiisen and Dorian Kodelja and Ilya Kuzovkin and Kristjan Korjus and Juhan Aru and Jaan Aru and Raul Vicente},\n booktitle = {PLoS ONE},\n journal = {PLoS ONE},\n title = {Multiagent cooperation and competition with deep reinforcement learning},\n volume = {12},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:36652428740cd30d245d55889f01a7fb04a91c93",
            "@type": "ScholarlyArticle",
            "paperId": "36652428740cd30d245d55889f01a7fb04a91c93",
            "corpusId": 11118105,
            "url": "https://www.semanticscholar.org/paper/36652428740cd30d245d55889f01a7fb04a91c93",
            "title": "Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2964051675",
                "DBLP": "conf/aaai/LiHW18",
                "ArXiv": "1801.07606",
                "DOI": "10.1609/aaai.v32i1.11604",
                "CorpusId": 11118105
            },
            "abstract": "\n \n Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semi-supervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires considerable amount of labeled data for validation and model selection. In this paper, we develop deeper insights into the GCN model and address its fundamental limits. First, we show that the graph convolution of the GCN model is actually a special form of Laplacian smoothing, which is the key reason why GCNs work, but it also brings potential concerns of over-smoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures, we propose both co-training and self-training approaches to train GCNs. Our approaches significantly improve GCNs in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks have verified our theory and proposals.\n \n",
            "referenceCount": 33,
            "citationCount": 2038,
            "influentialCitationCount": 193,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11604/11463",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-01-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1801.07606"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2018DeeperII,\n author = {Qimai Li and Zhichao Han and Xiao-Ming Wu},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning},\n volume = {abs/1801.07606},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4d8e5ea39213287d36889d9b791e6fb13313cc5e",
            "@type": "ScholarlyArticle",
            "paperId": "4d8e5ea39213287d36889d9b791e6fb13313cc5e",
            "corpusId": 211044145,
            "url": "https://www.semanticscholar.org/paper/4d8e5ea39213287d36889d9b791e6fb13313cc5e",
            "title": "How Good is the Bayes Posterior in Deep Neural Networks Really?",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/icml/WenzelRVSTMSSJN20",
                "ArXiv": "2002.02405",
                "MAG": "3034669169",
                "CorpusId": 211044145
            },
            "abstract": "During the past five years the Bayesian deep learning community has developed increasingly accurate and efficient approximate inference procedures that allow for Bayesian inference in deep neural networks. However, despite this algorithmic progress and the promise of improved uncertainty quantification and sample efficiency there are---as of early 2020---no publicized deployments of Bayesian neural networks in industrial practice. In this work we cast doubt on the current understanding of Bayes posteriors in popular deep neural networks: we demonstrate through careful MCMC sampling that the posterior predictive induced by the Bayes posterior yields systematically worse predictions compared to simpler methods including point estimates obtained from SGD. Furthermore, we demonstrate that predictive performance is improved significantly through the use of a \"cold posterior\" that overcounts evidence. Such cold posteriors sharply deviate from the Bayesian paradigm but are commonly used as heuristic in Bayesian deep learning papers. We put forward several hypotheses that could explain cold posteriors and evaluate the hypotheses through experiments. Our work questions the goal of accurate posterior approximations in Bayesian deep learning: If the true Bayes posterior is poor, what is the use of more accurate approximations? Instead, we argue that it is timely to focus on understanding the origin of the improved performance of cold posteriors.",
            "referenceCount": 102,
            "citationCount": 280,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-02-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wenzel2020HowGI,\n author = {F. Wenzel and Kevin Roth and Bastiaan S. Veeling and J. Swiatkowski and L. Tran and S. Mandt and Jasper Snoek and Tim Salimans and Rodolphe Jenatton and Sebastian Nowozin},\n booktitle = {International Conference on Machine Learning},\n pages = {10248-10259},\n title = {How Good is the Bayes Posterior in Deep Neural Networks Really?},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:053912e76e50c9f923a1fc1c173f1365776060cc",
            "@type": "ScholarlyArticle",
            "paperId": "053912e76e50c9f923a1fc1c173f1365776060cc",
            "corpusId": 6076653,
            "url": "https://www.semanticscholar.org/paper/053912e76e50c9f923a1fc1c173f1365776060cc",
            "title": "On optimization methods for deep learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2102017903",
                "DBLP": "conf/icml/LeNCLPN11",
                "CorpusId": 6076653
            },
            "abstract": "The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs. In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms. In our experiments, the difference between L-BFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69% on the standard MNIST dataset. This is a state-of-the-art result on MNIST among algorithms that do not use distortions or pretraining.",
            "referenceCount": 43,
            "citationCount": 982,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-06-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Le2011OnOM,\n author = {Quoc V. Le and Jiquan Ngiam and Adam Coates and A. Lahiri and B. Prochnow and A. Ng},\n booktitle = {International Conference on Machine Learning},\n pages = {265-272},\n title = {On optimization methods for deep learning},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:36358eff7c34de64c0ce8aa42cf7c4da24bf8e93",
            "@type": "ScholarlyArticle",
            "paperId": "36358eff7c34de64c0ce8aa42cf7c4da24bf8e93",
            "corpusId": 14997888,
            "url": "https://www.semanticscholar.org/paper/36358eff7c34de64c0ce8aa42cf7c4da24bf8e93",
            "title": "Deep Metric Learning for Person Re-identification",
            "venue": "International Conference on Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                "name": "International Conference on Pattern Recognition",
                "alternate_names": [
                    "Pattern Recognit (ICPR Proc Int Conf",
                    "Int Conf Pattern Recognit",
                    "ICPR",
                    "International conference on pattern recognition",
                    "Int conf pattern recognit",
                    "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                ],
                "issn": "1041-3278",
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2135442311",
                "DBLP": "conf/icpr/YiLLL14",
                "DOI": "10.1109/ICPR.2014.16",
                "CorpusId": 14997888
            },
            "abstract": "Various hand-crafted features and metric learning methods prevail in the field of person re-identification. Compared to these methods, this paper proposes a more general way that can learn a similarity metric from image pixels directly. By using a \"siamese\" deep neural network, the proposed method can jointly learn the color feature, texture feature and metric in a unified framework. The network has a symmetry structure with two sub-networks which are connected by a cosine layer. Each sub network includes two convolutional layers and a full connected layer. To deal with the big variations of person images, binomial deviance is used to evaluate the cost between similarities and labels, which is proved to be robust to outliers. Experiments on VIPeR illustrate the superior performance of our method and a cross database experiment also shows its good generalization.",
            "referenceCount": 26,
            "citationCount": 839,
            "influentialCitationCount": 62,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-08-24",
            "journal": {
                "name": "2014 22nd International Conference on Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yi2014DeepML,\n author = {Dong Yi and Zhen Lei and Shengcai Liao and S. Li},\n booktitle = {International Conference on Pattern Recognition},\n journal = {2014 22nd International Conference on Pattern Recognition},\n pages = {34-39},\n title = {Deep Metric Learning for Person Re-identification},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1def5d3711ebd1d86787b1ed57c91832c5ddc90b",
            "@type": "ScholarlyArticle",
            "paperId": "1def5d3711ebd1d86787b1ed57c91832c5ddc90b",
            "corpusId": 8241258,
            "url": "https://www.semanticscholar.org/paper/1def5d3711ebd1d86787b1ed57c91832c5ddc90b",
            "title": "Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2174786457",
                "ArXiv": "1511.06342",
                "DBLP": "journals/corr/ParisottoBS15",
                "CorpusId": 8241258
            },
            "abstract": "The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed \"Actor-Mimic\", exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.",
            "referenceCount": 22,
            "citationCount": 537,
            "influentialCitationCount": 44,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-19",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1511.06342"
            },
            "citationStyles": {
                "bibtex": "@Article{Parisotto2015ActorMimicDM,\n author = {Emilio Parisotto and Jimmy Ba and R. Salakhutdinov},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning},\n volume = {abs/1511.06342},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:edd846e76cacfba5be37da99c006e3ccc9b861b0",
            "@type": "ScholarlyArticle",
            "paperId": "edd846e76cacfba5be37da99c006e3ccc9b861b0",
            "corpusId": 3759573,
            "url": "https://www.semanticscholar.org/paper/edd846e76cacfba5be37da99c006e3ccc9b861b0",
            "title": "FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2560474170",
                "ArXiv": "1612.01925",
                "DBLP": "conf/cvpr/IlgMSKDB17",
                "DOI": "10.1109/CVPR.2017.179",
                "CorpusId": 3759573
            },
            "abstract": "The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a subnetwork specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.",
            "referenceCount": 35,
            "citationCount": 2682,
            "influentialCitationCount": 363,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1612.01925",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-12-06",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ilg2016FlowNet2E,\n author = {Eddy Ilg and N. Mayer and Tonmoy Saikia and M. Keuper and A. Dosovitskiy and T. Brox},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {1647-1655},\n title = {FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5832724acb9606ad7d34f8a92acf51e78b935d75",
            "@type": "ScholarlyArticle",
            "paperId": "5832724acb9606ad7d34f8a92acf51e78b935d75",
            "corpusId": 206592854,
            "url": "https://www.semanticscholar.org/paper/5832724acb9606ad7d34f8a92acf51e78b935d75",
            "title": "Deep hashing for compact binary codes learning",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1956333070",
                "DBLP": "conf/cvpr/LiongLWMZ15",
                "DOI": "10.1109/CVPR.2015.7298862",
                "CorpusId": 206592854
            },
            "abstract": "In this paper, we propose a new deep hashing (DH) approach to learn compact binary codes for large scale visual search. Unlike most existing binary codes learning methods which seek a single linear projection to map each sample into a binary vector, we develop a deep neural network to seek multiple hierarchical non-linear transformations to learn these binary codes, so that the nonlinear relationship of samples can be well exploited. Our model is learned under three constraints at the top layer of the deep network: 1) the loss between the original real-valued feature descriptor and the learned binary vector is minimized, 2) the binary codes distribute evenly on each bit, and 3) different bits are as independent as possible. To further improve the discriminative power of the learned binary codes, we extend DH into supervised DH (SDH) by including one discriminative term into the objective function of DH which simultaneously maximizes the inter-class variations and minimizes the intra-class variations of the learned binary codes. Experimental results show the superiority of the proposed approach over the state-of-the-arts.",
            "referenceCount": 36,
            "citationCount": 504,
            "influentialCitationCount": 49,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-06-07",
            "journal": {
                "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Liong2015DeepHF,\n author = {Venice Erin Liong and Jiwen Lu and Gang Wang and P. Moulin and Jie Zhou},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2475-2483},\n title = {Deep hashing for compact binary codes learning},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b2180fc4f5cb46b5b5394487842399c501381d67",
            "@type": "ScholarlyArticle",
            "paperId": "b2180fc4f5cb46b5b5394487842399c501381d67",
            "corpusId": 1622067,
            "url": "https://www.semanticscholar.org/paper/b2180fc4f5cb46b5b5394487842399c501381d67",
            "title": "Learning a Deep Compact Image Representation for Visual Tracking",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "conf/nips/WangY13",
                "MAG": "2118097920",
                "CorpusId": 1622067
            },
            "abstract": "In this paper, we study the challenging problem of tracking the trajectory of a moving object in a video with possibly very complex background. In contrast to most existing trackers which only learn the appearance of the tracked object online, we take a different approach, inspired by recent advances in deep learning architectures, by putting more emphasis on the (unsupervised) feature learning problem. Specifically, by using auxiliary natural images, we train a stacked de-noising autoencoder offline to learn generic image features that are more robust against variations. This is then followed by knowledge transfer from offline training to the online tracking process. Online tracking involves a classification neural network which is constructed from the encoder part of the trained autoencoder as a feature extractor and an additional classification layer. Both the feature extractor and the classifier can be further tuned to adapt to appearance changes of the moving object. Comparison with the state-of-the-art trackers on some challenging benchmark video sequences shows that our deep learning tracker is more accurate while maintaining low computational cost with real-time performance when our MATLAB implementation of the tracker is used with a modest graphics processing unit (GPU).",
            "referenceCount": 25,
            "citationCount": 951,
            "influentialCitationCount": 102,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-12-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2013LearningAD,\n author = {Naiyan Wang and D. Yeung},\n booktitle = {Neural Information Processing Systems},\n pages = {809-817},\n title = {Learning a Deep Compact Image Representation for Visual Tracking},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5726c7b40fcc454b77d989656c085520bf6c15fa",
            "@type": "ScholarlyArticle",
            "paperId": "5726c7b40fcc454b77d989656c085520bf6c15fa",
            "corpusId": 710430,
            "url": "https://www.semanticscholar.org/paper/5726c7b40fcc454b77d989656c085520bf6c15fa",
            "title": "Multimodal learning with deep Boltzmann machines",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2164587673",
                "DBLP": "conf/nips/SrivastavaS12",
                "DOI": "10.5555/2627435.2697059",
                "CorpusId": 710430
            },
            "abstract": "Data often consists of multiple diverse modalities. For example, images are tagged with textual information and videos are accompanied by audio. Each modality is characterized by having distinct statistical properties. We propose a Deep Boltzmann Machine for learning a generative model of such multimodal data. We show that the model can be used to create fused representations by combining features across modalities. These learned representations are useful for classification and information retrieval. By sampling from the conditional distributions over each data modality, it is possible to create these representations even when some data modalities are missing. We conduct experiments on bimodal image-text and audio-video data. The fused representation achieves good classification results on the MIR-Flickr data set matching or outperforming other deep models as well as SVM based models that use Multiple Kernel Learning. We further demonstrate that this multimodal model helps classification and retrieval even when only unimodal data is available at test time.",
            "referenceCount": 44,
            "citationCount": 1638,
            "influentialCitationCount": 119,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-12-03",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "15"
            },
            "citationStyles": {
                "bibtex": "@Article{Srivastava2012MultimodalLW,\n author = {Nitish Srivastava and R. Salakhutdinov},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {2949-2980},\n title = {Multimodal learning with deep Boltzmann machines},\n volume = {15},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e383584ccbc8b920eaf3cfce3869da646ff5550",
            "@type": "ScholarlyArticle",
            "paperId": "5e383584ccbc8b920eaf3cfce3869da646ff5550",
            "corpusId": 207240067,
            "url": "https://www.semanticscholar.org/paper/5e383584ccbc8b920eaf3cfce3869da646ff5550",
            "title": "Deep Neural Networks for YouTube Recommendations",
            "venue": "ACM Conference on Recommender Systems",
            "publicationVenue": {
                "id": "urn:research:61275a16-1e0d-479f-ac4e-f295310761f0",
                "name": "ACM Conference on Recommender Systems",
                "alternate_names": [
                    "Conf Recomm Syst",
                    "RecSys",
                    "ACM Conf Recomm Syst",
                    "Conference on Recommender Systems"
                ],
                "issn": null,
                "url": "http://recsys.acm.org/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2512971201",
                "DBLP": "conf/recsys/CovingtonAS16",
                "DOI": "10.1145/2959100.2959190",
                "CorpusId": 207240067
            },
            "abstract": "YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous user-facing impact.",
            "referenceCount": 27,
            "citationCount": 2471,
            "influentialCitationCount": 229,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://dl.acm.org/ft_gateway.cfm?id=2959190&type=pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle"
            ],
            "publicationDate": "2016-09-07",
            "journal": {
                "name": "Proceedings of the 10th ACM Conference on Recommender Systems",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Covington2016DeepNN,\n author = {Paul Covington and Jay K. Adams and Emre Sargin},\n booktitle = {ACM Conference on Recommender Systems},\n journal = {Proceedings of the 10th ACM Conference on Recommender Systems},\n title = {Deep Neural Networks for YouTube Recommendations},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e3fc5b5627af62ee6981a02090cf6bae368202d7",
            "@type": "ScholarlyArticle",
            "paperId": "e3fc5b5627af62ee6981a02090cf6bae368202d7",
            "corpusId": 246432496,
            "url": "https://www.semanticscholar.org/paper/e3fc5b5627af62ee6981a02090cf6bae368202d7",
            "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/jmlr/RaffinHGKED21",
                "CorpusId": 246432496
            },
            "abstract": "Stable-Baselines3 provides open-source implementations of deep reinforcement learning (RL) algorithms in Python. The implementations have been benchmarked against reference codebases, and automated unit tests cover 95% of the code. The algorithms follow a consistent interface and are accompanied by extensive documentation, making it simple to train and compare di\ufb00erent RL algorithms. Our documentation, examples, and source-code are available at https://github.com/DLR-RM/stable-baselines3 .",
            "referenceCount": 33,
            "citationCount": 738,
            "influentialCitationCount": 58,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Raffin2021StableBaselines3RR,\n author = {A. Raffin and Ashley Hill and A. Gleave and A. Kanervisto and M. Ernestus and Noah Dormann},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {268:1-268:8},\n title = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},\n volume = {22},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8b2a6808ce5cec406b41a8e77e67570246105bd0",
            "@type": "ScholarlyArticle",
            "paperId": "8b2a6808ce5cec406b41a8e77e67570246105bd0",
            "corpusId": 49867653,
            "url": "https://www.semanticscholar.org/paper/8b2a6808ce5cec406b41a8e77e67570246105bd0",
            "title": "Deep Reinforcement Learning for Swarm Systems",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1807-06613",
                "ArXiv": "1807.06613",
                "MAG": "2883532348",
                "CorpusId": 49867653
            },
            "abstract": "Recently, deep reinforcement learning (RL) methods have been applied successfully to multi-agent scenarios. Typically, the observation vector for decentralized decision making is represented by a concatenation of the (local) information an agent gathers about other agents. However, concatenation scales poorly to swarm systems with a large number of homogeneous agents as it does not exploit the fundamental properties inherent to these systems: (i) the agents in the swarm are interchangeable and (ii) the exact number of agents in the swarm is irrelevant. Therefore, we propose a new state representation for deep multi-agent RL based on mean embeddings of distributions, where we treat the agents as samples and use the empirical mean embedding as input for a decentralized policy. We define different feature spaces of the mean embedding using histograms, radial basis functions and neural networks trained end-to-end. We evaluate the representation on two well-known problems from the swarm literature in a globally and locally observable setup. For the local setup we furthermore introduce simple communication protocols. Of all approaches, the mean embedding representation using neural network features enables the richest information exchange between neighboring agents, facilitating the development of complex collective strategies.",
            "referenceCount": 46,
            "citationCount": 155,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-07-17",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "20"
            },
            "citationStyles": {
                "bibtex": "@Article{H\u00fcttenrauch2018DeepRL,\n author = {Maximilian H\u00fcttenrauch and Adrian \u0160o\u0161i\u0107 and G. Neumann},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {54:1-54:31},\n title = {Deep Reinforcement Learning for Swarm Systems},\n volume = {20},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1a2118bed729579528deb51e745d58dd3629baf6",
            "@type": "ScholarlyArticle",
            "paperId": "1a2118bed729579528deb51e745d58dd3629baf6",
            "corpusId": 3385018,
            "url": "https://www.semanticscholar.org/paper/1a2118bed729579528deb51e745d58dd3629baf6",
            "title": "Learning Important Features Through Propagating Activation Differences",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2605409611",
                "DBLP": "conf/icml/ShrikumarGK17",
                "ArXiv": "1704.02685",
                "CorpusId": 3385018
            },
            "abstract": "The purported \"black box\" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, code: http://goo.gl/RM8jvH.",
            "referenceCount": 17,
            "citationCount": 2903,
            "influentialCitationCount": 350,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2017-04-10",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Shrikumar2017LearningIF,\n author = {Avanti Shrikumar and Peyton Greenside and A. Kundaje},\n booktitle = {International Conference on Machine Learning},\n pages = {3145-3153},\n title = {Learning Important Features Through Propagating Activation Differences},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c",
            "@type": "ScholarlyArticle",
            "paperId": "2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c",
            "corpusId": 69997935,
            "url": "https://www.semanticscholar.org/paper/2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c",
            "title": "Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2884745705",
                "CorpusId": 69997935
            },
            "abstract": "Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks-scikit-learn and TensorFlow-author Aurelien Geron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started. Explore the machine learning landscape, particularly neural nets Use scikit-learn to track an example machine-learning project end-to-end Explore several training models, including support vector machines, decision trees, random forests, and ensemble methods Use the TensorFlow library to build and train neural nets Dive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learning Learn techniques for training and scaling deep neural nets Apply practical code examples without acquiring excessive machine learning theory or algorithm details",
            "referenceCount": 0,
            "citationCount": 2319,
            "influentialCitationCount": 326,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2017-04-18",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{G\u00e9ron2017HandsOnML,\n author = {Aur\u00e9lien G\u00e9ron},\n title = {Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2c5174a4f3f6978cc28e2d3c15feae8610372bcd",
            "@type": "ScholarlyArticle",
            "paperId": "2c5174a4f3f6978cc28e2d3c15feae8610372bcd",
            "corpusId": 3580738,
            "url": "https://www.semanticscholar.org/paper/2c5174a4f3f6978cc28e2d3c15feae8610372bcd",
            "title": "Learning from Between-class Examples for Deep Sound Recognition",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1711.10282",
                "MAG": "2768188490",
                "DBLP": "conf/iclr/TokozumeUH18",
                "CorpusId": 3580738
            },
            "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher's criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.",
            "referenceCount": 31,
            "citationCount": 218,
            "influentialCitationCount": 31,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-11-28",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1711.10282"
            },
            "citationStyles": {
                "bibtex": "@Article{Tokozume2017LearningFB,\n author = {Yuji Tokozume and Y. Ushiku and T. Harada},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning from Between-class Examples for Deep Sound Recognition},\n volume = {abs/1711.10282},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:967a21a111757d6af7f7a25ca7ea2bdf6d505098",
            "@type": "ScholarlyArticle",
            "paperId": "967a21a111757d6af7f7a25ca7ea2bdf6d505098",
            "corpusId": 52877454,
            "url": "https://www.semanticscholar.org/paper/967a21a111757d6af7f7a25ca7ea2bdf6d505098",
            "title": "Deep Graph Infomax",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963782635",
                "DBLP": "conf/iclr/VelickovicFHLBH19",
                "ArXiv": "1809.10341",
                "CorpusId": 52877454
            },
            "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.",
            "referenceCount": 51,
            "citationCount": 1582,
            "influentialCitationCount": 449,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1809.10341"
            },
            "citationStyles": {
                "bibtex": "@Article{Velickovic2018DeepGI,\n author = {Petar Velickovic and W. Fedus and William L. Hamilton and P. Lio\u2019 and Yoshua Bengio and R. Devon Hjelm},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Deep Graph Infomax},\n volume = {abs/1809.10341},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c982cc1ee52f5963a570399d619e8edb112fc9b6",
            "@type": "ScholarlyArticle",
            "paperId": "c982cc1ee52f5963a570399d619e8edb112fc9b6",
            "corpusId": 492338,
            "url": "https://www.semanticscholar.org/paper/c982cc1ee52f5963a570399d619e8edb112fc9b6",
            "title": "Cost-Sensitive Learning of Deep Feature Representations From Imbalanced Data",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/corr/KhanBST15",
                "ArXiv": "1508.03422",
                "MAG": "1753659477",
                "DOI": "10.1109/TNNLS.2017.2732482",
                "CorpusId": 492338,
                "PubMed": "28829320"
            },
            "abstract": "Class imbalance is a common problem in the case of real-world object detection and classification tasks. Data of some classes are abundant, making them an overrepresented majority, and data of other classes are scarce, making them an underrepresented minority. This imbalance makes it challenging for a classifier to appropriately learn the discriminating boundaries of the majority and minority classes. In this paper, we propose a cost-sensitive (CoSen) deep neural network, which can automatically learn robust feature representations for both the majority and minority classes. During training, our learning procedure jointly optimizes the class-dependent costs and the neural network parameters. The proposed approach is applicable to both binary and multiclass problems without any modification. Moreover, as opposed to data-level approaches, we do not alter the original data distribution, which results in a lower computational cost during the training process. We report the results of our experiments on six major image classification data sets and show that the proposed approach significantly outperforms the baseline algorithms. Comparisons with popular data sampling techniques and CoSen classifiers demonstrate the superior performance of our proposed method.",
            "referenceCount": 72,
            "citationCount": 741,
            "influentialCitationCount": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1508.03422",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-08-14",
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "29"
            },
            "citationStyles": {
                "bibtex": "@Article{Khan2015CostSensitiveLO,\n author = {Salman Hameed Khan and Munawar Hayat and Bennamoun and Ferdous Sohel and R. Togneri},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {3573-3587},\n title = {Cost-Sensitive Learning of Deep Feature Representations From Imbalanced Data},\n volume = {29},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b6438f4fc1b0e89d1319f2de1ca663255b8ee190",
            "@type": "ScholarlyArticle",
            "paperId": "b6438f4fc1b0e89d1319f2de1ca663255b8ee190",
            "corpusId": 3217198,
            "url": "https://www.semanticscholar.org/paper/b6438f4fc1b0e89d1319f2de1ca663255b8ee190",
            "title": "Discriminative Deep Metric Learning for Face Verification in the Wild",
            "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "conf/cvpr/HuLT14",
                "MAG": "2076434944",
                "DOI": "10.1109/CVPR.2014.242",
                "CorpusId": 3217198
            },
            "abstract": "This paper presents a new discriminative deep metric learning (DDML) method for face verification in the wild. Different from existing metric learning-based face verification methods which aim to learn a Mahalanobis distance metric to maximize the inter-class variations and minimize the intra-class variations, simultaneously, the proposed DDML trains a deep neural network which learns a set of hierarchical nonlinear transformations to project face pairs into the same feature subspace, under which the distance of each positive face pair is less than a smaller threshold and that of each negative pair is higher than a larger threshold, respectively, so that discriminative information can be exploited in the deep network. Our method achieves very competitive face verification performance on the widely used LFW and YouTube Faces (YTF) datasets.",
            "referenceCount": 40,
            "citationCount": 698,
            "influentialCitationCount": 45,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dr.ntu.edu.sg/bitstream/10356/100336/1/Discriminative%20deep%20metric%20learning%20for%20face%20verification%20in%20the%20wild.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-06-23",
            "journal": {
                "name": "2014 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hu2014DiscriminativeDM,\n author = {Junlin Hu and Jiwen Lu and Yap-Peng Tan},\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {1875-1882},\n title = {Discriminative Deep Metric Learning for Face Verification in the Wild},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c1f05b723e53ac4eb1133249b445c0011d42ca79",
            "@type": "ScholarlyArticle",
            "paperId": "c1f05b723e53ac4eb1133249b445c0011d42ca79",
            "corpusId": 3290651,
            "url": "https://www.semanticscholar.org/paper/c1f05b723e53ac4eb1133249b445c0011d42ca79",
            "title": "Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review",
            "venue": "Neural Computation",
            "publicationVenue": {
                "id": "urn:research:69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                "name": "Neural Computation",
                "alternate_names": [
                    "Neural Comput"
                ],
                "issn": "0899-7667",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/neco/RawatW17",
                "MAG": "2622826443",
                "DOI": "10.1162/neco_a_00990",
                "CorpusId": 3290651,
                "PubMed": "28599112"
            },
            "abstract": "Convolutional neural networks (CNNs) have been applied to visual tasks since the late 1980s. However, despite a few scattered applications, they were dormant until the mid-2000s when developments in computing power and the advent of large amounts of labeled data, supplemented by improved algorithms, contributed to their advancement and brought them to the forefront of a neural network renaissance that has seen rapid progression since 2012. In this review, which focuses on the application of CNNs to image classification tasks, we cover their development, from their predecessors up to recent state-of-the-art deep learning systems. Along the way, we analyze (1) their early successes, (2) their role in the deep learning renaissance, (3) selected symbolic works that have contributed to their recent popularity, and (4) several improvement attempts by reviewing contributions and challenges of over 300 publications. We also introduce some of their current trends and remaining challenges.",
            "referenceCount": 302,
            "citationCount": 2273,
            "influentialCitationCount": 81,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-09-01",
            "journal": {
                "name": "Neural Computation",
                "volume": "29"
            },
            "citationStyles": {
                "bibtex": "@Article{Rawat2017DeepCN,\n author = {Waseem Rawat and Zenghui Wang},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {2352-2449},\n title = {Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review},\n volume = {29},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b6b8a1b80891c96c28cc6340267b58186157e536",
            "@type": "ScholarlyArticle",
            "paperId": "b6b8a1b80891c96c28cc6340267b58186157e536",
            "corpusId": 7242892,
            "url": "https://www.semanticscholar.org/paper/b6b8a1b80891c96c28cc6340267b58186157e536",
            "title": "End-to-End Training of Deep Visuomotor Policies",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/corr/LevineFDA15",
                "MAG": "2155007355",
                "ArXiv": "1504.00702",
                "DOI": "10.5555/2946645.2946684",
                "CorpusId": 7242892
            },
            "abstract": "Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a partially observed guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.",
            "referenceCount": 99,
            "citationCount": 3011,
            "influentialCitationCount": 110,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-04-02",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "17"
            },
            "citationStyles": {
                "bibtex": "@Article{Levine2015EndtoEndTO,\n author = {S. Levine and Chelsea Finn and Trevor Darrell and P. Abbeel},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {39:1-39:40},\n title = {End-to-End Training of Deep Visuomotor Policies},\n volume = {17},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1d65848c563b2c3a7f0153551c1b39e0e5c2d776",
            "@type": "ScholarlyArticle",
            "paperId": "1d65848c563b2c3a7f0153551c1b39e0e5c2d776",
            "corpusId": 1562290,
            "url": "https://www.semanticscholar.org/paper/1d65848c563b2c3a7f0153551c1b39e0e5c2d776",
            "title": "Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks",
            "venue": "IAPR International Conference on Machine Learning and Data Mining in Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:c22cbb06-d23e-491c-886e-7f7d402229ea",
                "name": "IAPR International Conference on Machine Learning and Data Mining in Pattern Recognition",
                "alternate_names": [
                    "IAPR Int Conf Mach Learn Data Min Pattern Recognit",
                    "Machine Learning and Data Mining in Pattern Recognition",
                    "Mach Learn Data Min Pattern Recognit",
                    "MLDM"
                ],
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1701.04143",
                "MAG": "2572659264",
                "DBLP": "conf/mldm/BehzadanM17",
                "DOI": "10.1007/978-3-319-62416-7_19",
                "CorpusId": 1562290
            },
            "abstract": null,
            "referenceCount": 23,
            "citationCount": 236,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1701.04143",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-01-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Behzadan2017VulnerabilityOD,\n author = {Vahid Behzadan and Arslan Munir},\n booktitle = {IAPR International Conference on Machine Learning and Data Mining in Pattern Recognition},\n pages = {262-275},\n title = {Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c8f41160130980c1ffead5a812cf2b3c6b03049f",
            "@type": "ScholarlyArticle",
            "paperId": "c8f41160130980c1ffead5a812cf2b3c6b03049f",
            "corpusId": 1616437,
            "url": "https://www.semanticscholar.org/paper/c8f41160130980c1ffead5a812cf2b3c6b03049f",
            "title": "Deep spatial autoencoders for visuomotor learning",
            "venue": "IEEE International Conference on Robotics and Automation",
            "publicationVenue": {
                "id": "urn:research:3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                "name": "IEEE International Conference on Robotics and Automation",
                "alternate_names": [
                    "International Conference on Robotics and Automation",
                    "Int Conf Robot Autom",
                    "ICRA",
                    "IEEE Int Conf Robot Autom"
                ],
                "issn": "2152-4092",
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/icra/FinnTDDLA16",
                "MAG": "2210483910",
                "ArXiv": "1509.06113",
                "DOI": "10.1109/ICRA.2016.7487173",
                "CorpusId": 1616437
            },
            "abstract": "Reinforcement learning provides a powerful and flexible framework for automated acquisition of robotic motion skills. However, applying reinforcement learning requires a sufficiently detailed representation of the state, including the configuration of task-relevant objects. We present an approach that automates state-space construction by learning a state representation directly from camera images. Our method uses a deep spatial autoencoder to acquire a set of feature points that describe the environment for the current task, such as the positions of objects, and then learns a motion skill with these feature points using an efficient reinforcement learning method based on local linear models. The resulting controller reacts continuously to the learned feature points, allowing the robot to dynamically manipulate objects in the world with closed-loop control. We demonstrate our method with a PR2 robot on tasks that include pushing a free-standing toy block, picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions. In each task, our method automatically learns to track task-relevant objects and manipulate their configuration with the robot's arm.",
            "referenceCount": 40,
            "citationCount": 491,
            "influentialCitationCount": 28,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1509.06113",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-09-21",
            "journal": {
                "name": "2016 IEEE International Conference on Robotics and Automation (ICRA)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Finn2015DeepSA,\n author = {Chelsea Finn and X. Tan and Yan Duan and Trevor Darrell and S. Levine and P. Abbeel},\n booktitle = {IEEE International Conference on Robotics and Automation},\n journal = {2016 IEEE International Conference on Robotics and Automation (ICRA)},\n pages = {512-519},\n title = {Deep spatial autoencoders for visuomotor learning},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:13497bd108d4412d02050e646235f456568cf822",
            "@type": "ScholarlyArticle",
            "paperId": "13497bd108d4412d02050e646235f456568cf822",
            "corpusId": 11590585,
            "url": "https://www.semanticscholar.org/paper/13497bd108d4412d02050e646235f456568cf822",
            "title": "Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2949640717",
                "DBLP": "journals/corr/AmodeiABCCCCCCD15",
                "ArXiv": "1512.02595",
                "CorpusId": 11590585
            },
            "abstract": "We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech-two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",
            "referenceCount": 75,
            "citationCount": 2717,
            "influentialCitationCount": 255,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-12-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Amodei2015DeepS2,\n author = {Dario Amodei and S. Ananthanarayanan and Rishita Anubhai and Jin Bai and Eric Battenberg and Carl Case and J. Casper and Bryan Catanzaro and Jingdong Chen and Mike Chrzanowski and Adam Coates and G. Diamos and Erich Elsen and Jesse Engel and Linxi (Jim) Fan and Christopher Fougner and Awni Y. Hannun and Billy Jun and T. Han and P. LeGresley and Xiangang Li and Libby Lin and Sharan Narang and A. Ng and Sherjil Ozair and R. Prenger and Sheng Qian and Jonathan Raiman and S. Satheesh and David Seetapun and Shubho Sengupta and Anuroop Sriram and Chong-Jun Wang and Yi Wang and Zhiqian Wang and Bo Xiao and Yan Xie and Dani Yogatama and J. Zhan and Zhenyao Zhu},\n booktitle = {International Conference on Machine Learning},\n pages = {173-182},\n title = {Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
            "@type": "ScholarlyArticle",
            "paperId": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
            "corpusId": 215744839,
            "url": "https://www.semanticscholar.org/paper/020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
            "title": "Meta-Learning in Neural Networks: A Survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/pami/HospedalesAMS22",
                "MAG": "3015606043",
                "ArXiv": "2004.05439",
                "DOI": "10.1109/TPAMI.2021.3079209",
                "CorpusId": 215744839,
                "PubMed": "33974543"
            },
            "abstract": "The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.",
            "referenceCount": 332,
            "citationCount": 1184,
            "influentialCitationCount": 66,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/34/4359286/09428530.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-04-11",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "44"
            },
            "citationStyles": {
                "bibtex": "@Article{Hospedales2020MetaLearningIN,\n author = {Timothy M. Hospedales and Antreas Antoniou and P. Micaelli and A. Storkey},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {5149-5169},\n title = {Meta-Learning in Neural Networks: A Survey},\n volume = {44},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:eb9243a3b98a819539ad57b7b4f05b969510d075",
            "@type": "ScholarlyArticle",
            "paperId": "eb9243a3b98a819539ad57b7b4f05b969510d075",
            "corpusId": 13953660,
            "url": "https://www.semanticscholar.org/paper/eb9243a3b98a819539ad57b7b4f05b969510d075",
            "title": "New types of deep neural network learning for speech recognition and related applications: an overview",
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "publicationVenue": {
                "id": "urn:research:0d6f7fba-7092-46b3-8039-93458dba736b",
                "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                "alternate_names": [
                    "Int Conf Acoust Speech Signal Process",
                    "IEEE Int Conf Acoust Speech Signal Process",
                    "ICASSP",
                    "International Conference on Acoustics, Speech, and Signal Processing"
                ],
                "issn": null,
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2091432990",
                "DBLP": "conf/icassp/DengHK13",
                "DOI": "10.1109/ICASSP.2013.6639344",
                "CorpusId": 13953660
            },
            "abstract": "In this paper, we provide an overview of the invited and contributed papers presented at the special session at ICASSP-2013, entitled \u201cNew Types of Deep Neural Network Learning for Speech Recognition and Related Applications,\u201d as organized by the authors. We also describe the historical context in which acoustic models based on deep neural networks have been developed. The technical overview of the papers presented in our special session is organized into five ways of improving deep learning methods: (1) better optimization; (2) better types of neural activation function and better network architectures; (3) better ways to determine the myriad hyper-parameters of deep neural networks; (4) more appropriate ways to preprocess speech for deep neural networks; and (5) ways of leveraging multiple languages or dialects that are more easily achieved with deep neural networks than with Gaussian mixture models.",
            "referenceCount": 56,
            "citationCount": 1065,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2013-05-26",
            "journal": {
                "name": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Deng2013NewTO,\n author = {L. Deng and Geoffrey E. Hinton and Brian Kingsbury},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},\n pages = {8599-8603},\n title = {New types of deep neural network learning for speech recognition and related applications: an overview},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f8c8619ea7d68e604e40b814b40c72888a755e95",
            "@type": "ScholarlyArticle",
            "paperId": "f8c8619ea7d68e604e40b814b40c72888a755e95",
            "corpusId": 4493778,
            "url": "https://www.semanticscholar.org/paper/f8c8619ea7d68e604e40b814b40c72888a755e95",
            "title": "Unsupervised Feature Learning and Deep Learning: A Review and New Perspectives",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "60493759",
                "DBLP": "journals/corr/abs-1206-5538",
                "CorpusId": 4493778
            },
            "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although domain knowledge can be used to help design representations, learning can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, manifold learning, and deep learning. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.",
            "referenceCount": 209,
            "citationCount": 523,
            "influentialCitationCount": 32,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2012-06-24",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1206.5538"
            },
            "citationStyles": {
                "bibtex": "@Article{Bengio2012UnsupervisedFL,\n author = {Yoshua Bengio and Aaron C. Courville and Pascal Vincent},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Unsupervised Feature Learning and Deep Learning: A Review and New Perspectives},\n volume = {abs/1206.5538},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:aab368284210c1bb917ec2d31b84588e3d2d7eb4",
            "@type": "ScholarlyArticle",
            "paperId": "aab368284210c1bb917ec2d31b84588e3d2d7eb4",
            "corpusId": 4009713,
            "url": "https://www.semanticscholar.org/paper/aab368284210c1bb917ec2d31b84588e3d2d7eb4",
            "title": "Unsupervised Representation Learning by Predicting Image Rotations",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/iclr/GidarisSK18",
                "ArXiv": "1803.07728",
                "MAG": "2785325870",
                "CorpusId": 4009713
            },
            "abstract": "Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: this https URL .",
            "referenceCount": 36,
            "citationCount": 2681,
            "influentialCitationCount": 293,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1803.07728"
            },
            "citationStyles": {
                "bibtex": "@Article{Gidaris2018UnsupervisedRL,\n author = {Spyros Gidaris and Praveer Singh and N. Komodakis},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Unsupervised Representation Learning by Predicting Image Rotations},\n volume = {abs/1803.07728},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4c46347fbc272b21468efe3d9af34b4b2bad6684",
            "@type": "ScholarlyArticle",
            "paperId": "4c46347fbc272b21468efe3d9af34b4b2bad6684",
            "corpusId": 11154521,
            "url": "https://www.semanticscholar.org/paper/4c46347fbc272b21468efe3d9af34b4b2bad6684",
            "title": "Deep learning via Hessian-free optimization",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "196761320",
                "DBLP": "conf/icml/Martens10",
                "CorpusId": 11154521
            },
            "abstract": "We develop a 2nd-order optimization method based on the \"Hessian-free\" approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton & Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very large datasets, and isn't limited in applicability to auto-encoders, or any specific model class. We also discuss the issue of \"pathological curvature\" as a possible explanation for the difficulty of deep-learning and how 2nd-order optimization, and our method in particular, effectively deals with it.",
            "referenceCount": 9,
            "citationCount": 940,
            "influentialCitationCount": 125,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-06-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Martens2010DeepLV,\n author = {James Martens},\n booktitle = {International Conference on Machine Learning},\n pages = {735-742},\n title = {Deep learning via Hessian-free optimization},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b8ebda42e272d3617375118542d4675a0c0e501d",
            "@type": "ScholarlyArticle",
            "paperId": "b8ebda42e272d3617375118542d4675a0c0e501d",
            "corpusId": 2928248,
            "url": "https://www.semanticscholar.org/paper/b8ebda42e272d3617375118542d4675a0c0e501d",
            "title": "Deep Hashing Network for Unsupervised Domain Adaptation",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2951111165",
                "ArXiv": "1706.07522",
                "DBLP": "conf/cvpr/VenkateswaraECP17",
                "DOI": "10.1109/CVPR.2017.572",
                "CorpusId": 2928248
            },
            "abstract": "In recent years, deep neural networks have emerged as a dominant machine learning tool for a wide variety of application domains. However, training a deep neural network requires a large amount of labeled data, which is an expensive process in terms of time, labor and human expertise. Domain adaptation or transfer learning algorithms address this challenge by leveraging labeled data in a different, but related source domain, to develop a model for the target domain. Further, the explosive growth of digital data has posed a fundamental challenge concerning its storage and retrieval. Due to its storage and retrieval efficiency, recent years have witnessed a wide application of hashing in a variety of computer vision applications. In this paper, we first introduce a new dataset, Office-Home, to evaluate domain adaptation algorithms. The dataset contains images of a variety of everyday objects from multiple domains. We then propose a novel deep learning framework that can exploit labeled source data and unlabeled target data to learn informative hash codes, to accurately classify unseen target data. To the best of our knowledge, this is the first research effort to exploit the feature learning capabilities of deep neural networks to learn representative hash codes to address the domain adaptation problem. Our extensive empirical studies on multiple transfer tasks corroborate the usefulness of the framework in learning efficient hash codes which outperform existing competitive baselines for unsupervised domain adaptation.",
            "referenceCount": 51,
            "citationCount": 1480,
            "influentialCitationCount": 386,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1706.07522",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-06-22",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Venkateswara2017DeepHN,\n author = {Hemanth Venkateswara and Jos\u00e9 Eus\u00e9bio and Shayok Chakraborty and S. Panchanathan},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5385-5394},\n title = {Deep Hashing Network for Unsupervised Domain Adaptation},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6adf016e7531c91100d3cf4a74f5d4c87b26b528",
            "@type": "ScholarlyArticle",
            "paperId": "6adf016e7531c91100d3cf4a74f5d4c87b26b528",
            "corpusId": 2672720,
            "url": "https://www.semanticscholar.org/paper/6adf016e7531c91100d3cf4a74f5d4c87b26b528",
            "title": "Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks",
            "venue": "IEEE Symposium on Security and Privacy",
            "publicationVenue": {
                "id": "urn:research:29b9c461-963e-4d11-b2ab-92c182243942",
                "name": "IEEE Symposium on Security and Privacy",
                "alternate_names": [
                    "S&P",
                    "IEEE Symp Secur Priv"
                ],
                "issn": null,
                "url": "http://www.ieee-security.org/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2174868984",
                "DBLP": "journals/corr/PapernotMWJS15",
                "ArXiv": "1511.04508",
                "DOI": "10.1109/SP.2016.41",
                "CorpusId": 2672720
            },
            "abstract": "Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested.",
            "referenceCount": 53,
            "citationCount": 2688,
            "influentialCitationCount": 199,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1511.04508",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-14",
            "journal": {
                "name": "2016 IEEE Symposium on Security and Privacy (SP)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Papernot2015DistillationAA,\n author = {Nicolas Papernot and P. Mcdaniel and Xi Wu and S. Jha and A. Swami},\n booktitle = {IEEE Symposium on Security and Privacy},\n journal = {2016 IEEE Symposium on Security and Privacy (SP)},\n pages = {582-597},\n title = {Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bfe284e4338e62f0a61bb33398353efd687f206f",
            "@type": "ScholarlyArticle",
            "paperId": "bfe284e4338e62f0a61bb33398353efd687f206f",
            "corpusId": 4412459,
            "url": "https://www.semanticscholar.org/paper/bfe284e4338e62f0a61bb33398353efd687f206f",
            "title": "Learning to Compare: Relation Network for Few-Shot Learning",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2768228940",
                "ArXiv": "1711.06025",
                "DBLP": "conf/cvpr/SungYZXTH18",
                "DOI": "10.1109/CVPR.2018.00131",
                "CorpusId": 4412459
            },
            "abstract": "We present a conceptually simple, flexible, and general framework for few-shot learning, where a classifier must learn to recognise new classes given only few examples from each. Our method, called the Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting. Once trained, a RN is able to classify images of new classes by computing relation scores between query images and the few examples of each new class without further updating the network. Besides providing improved performance on few-shot learning, our framework is easily extended to zero-shot learning. Extensive experiments on five benchmarks demonstrate that our simple approach provides a unified and effective approach for both of these two tasks.",
            "referenceCount": 48,
            "citationCount": 3252,
            "influentialCitationCount": 466,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.pure.ed.ac.uk/ws/files/57835996/LearningToCompare.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-11-16",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sung2017LearningTC,\n author = {Flood Sung and Yongxin Yang and Li Zhang and T. Xiang and Philip H. S. Torr and Timothy M. Hospedales},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {1199-1208},\n title = {Learning to Compare: Relation Network for Few-Shot Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7743150b3fe21b5cc2ebe9e8a67f54031311f7ae",
            "@type": "ScholarlyArticle",
            "paperId": "7743150b3fe21b5cc2ebe9e8a67f54031311f7ae",
            "corpusId": 2637786,
            "url": "https://www.semanticscholar.org/paper/7743150b3fe21b5cc2ebe9e8a67f54031311f7ae",
            "title": "Smart Mining for Deep Metric Learning",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1704.01285",
                "DBLP": "conf/iccv/HarwoodGCRD17",
                "MAG": "2950188904",
                "DOI": "10.1109/ICCV.2017.307",
                "CorpusId": 2637786
            },
            "abstract": "To solve deep metric learning problems and producing feature embeddings, current methodologies will commonly use a triplet model to minimise the relative distance between samples from the same class and maximise the relative distance between samples from different classes. Though successful, the training convergence of this triplet model can be compromised by the fact that the vast majority of the training samples will produce gradients with magnitudes that are close to zero. This issue has motivated the development of methods that explore the global structure of the embedding and other methods that explore hard negative/positive mining. The effectiveness of such mining methods is often associated with intractable computational requirements. In this paper, we propose a novel deep metric learning method that combines the triplet model and the global structure of the embedding space. We rely on a smart mining procedure that produces effective training samples for a low computational cost. In addition, we propose an adaptive controller that automatically adjusts the smart mining hyper-parameters and speeds up the convergence of the training process. We show empirically that our proposed method allows for fast and more accurate training of triplet ConvNets than other competing mining methods. Additionally, we show that our method achieves new state-of-the-art embedding results for CUB-200-2011 and Cars196 datasets.",
            "referenceCount": 34,
            "citationCount": 312,
            "influentialCitationCount": 24,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1704.01285",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-04-05",
            "journal": {
                "name": "2017 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Harwood2017SmartMF,\n author = {Ben Harwood and B. V. Kumar and G. Carneiro and I. Reid and T. Drummond},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\n pages = {2840-2848},\n title = {Smart Mining for Deep Metric Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:80196cdfcd0c6ce2953bf65a7f019971e2026386",
            "@type": "ScholarlyArticle",
            "paperId": "80196cdfcd0c6ce2953bf65a7f019971e2026386",
            "corpusId": 3645060,
            "url": "https://www.semanticscholar.org/paper/80196cdfcd0c6ce2953bf65a7f019971e2026386",
            "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2950708659",
                "DBLP": "conf/icml/EspeholtSMSMWDF18",
                "ArXiv": "1802.01561",
                "CorpusId": 3645060
            },
            "abstract": "In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.",
            "referenceCount": 42,
            "citationCount": 1296,
            "influentialCitationCount": 250,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-02-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.01561"
            },
            "citationStyles": {
                "bibtex": "@Article{Espeholt2018IMPALASD,\n author = {Lasse Espeholt and Hubert Soyer and R. Munos and K. Simonyan and Volodymyr Mnih and Tom Ward and Yotam Doron and Vlad Firoiu and Tim Harley and Iain Dunning and S. Legg and K. Kavukcuoglu},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures},\n volume = {abs/1802.01561},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b0c34618ffd1154f35863e2ce7250ac6b6f2c424",
            "@type": "ScholarlyArticle",
            "paperId": "b0c34618ffd1154f35863e2ce7250ac6b6f2c424",
            "corpusId": 209379623,
            "url": "https://www.semanticscholar.org/paper/b0c34618ffd1154f35863e2ce7250ac6b6f2c424",
            "title": "Interpretable Machine Learning",
            "venue": "Hands-On Machine Learning with R",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "MAG": "3098109070",
                "DOI": "10.1201/9780367816377-16",
                "CorpusId": 209379623
            },
            "abstract": "Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.",
            "referenceCount": 199,
            "citationCount": 1887,
            "influentialCitationCount": 181,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://openresearch.surrey.ac.uk/view/delivery/44SUR_INST/12184120880002346/13184120870002346",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2019-11-07",
            "journal": {
                "name": "Hands-On Machine Learning with R",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Boehmke2019InterpretableML,\n author = {Bradley C. Boehmke and Brandon M. Greenwell},\n booktitle = {Hands-On Machine Learning with R},\n journal = {Hands-On Machine Learning with R},\n title = {Interpretable Machine Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2e689bdce24cf3644432505ce2783f03a1445ed2",
            "@type": "ScholarlyArticle",
            "paperId": "2e689bdce24cf3644432505ce2783f03a1445ed2",
            "corpusId": 54465161,
            "url": "https://www.semanticscholar.org/paper/2e689bdce24cf3644432505ce2783f03a1445ed2",
            "title": "Occupancy Networks: Learning 3D Reconstruction in Function Space",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1812-03828",
                "MAG": "2963926543",
                "ArXiv": "1812.03828",
                "DOI": "10.1109/CVPR.2019.00459",
                "CorpusId": 54465161
            },
            "abstract": "With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.",
            "referenceCount": 83,
            "citationCount": 2002,
            "influentialCitationCount": 330,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1812.03828",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-12-10",
            "journal": {
                "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Mescheder2018OccupancyNL,\n author = {L. Mescheder and Michael Oechsle and Michael Niemeyer and Sebastian Nowozin and Andreas Geiger},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4455-4465},\n title = {Occupancy Networks: Learning 3D Reconstruction in Function Space},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dbc7401e3e75c40d3c720e7db3c906d48bd742d7",
            "@type": "ScholarlyArticle",
            "paperId": "dbc7401e3e75c40d3c720e7db3c906d48bd742d7",
            "corpusId": 51805340,
            "url": "https://www.semanticscholar.org/paper/dbc7401e3e75c40d3c720e7db3c906d48bd742d7",
            "title": "Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/iclr/ZongSMCLCC18",
                "MAG": "2786088545",
                "CorpusId": 51805340
            },
            "abstract": "Unsupervised anomaly detection on multior high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core. Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space. In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM). Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model. The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training. Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score.",
            "referenceCount": 34,
            "citationCount": 1175,
            "influentialCitationCount": 204,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-15",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Zong2018DeepAG,\n author = {Bo Zong and Qi Song and Martin Renqiang Min and Wei Cheng and C. Lumezanu and Dae-ki Cho and Haifeng Chen},\n booktitle = {International Conference on Learning Representations},\n title = {Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ece3b623232c90bb8a9021a3eb25223c4fde7069",
            "@type": "ScholarlyArticle",
            "paperId": "ece3b623232c90bb8a9021a3eb25223c4fde7069",
            "corpusId": 15402687,
            "url": "https://www.semanticscholar.org/paper/ece3b623232c90bb8a9021a3eb25223c4fde7069",
            "title": "Learning Deep Embeddings with Histogram Loss",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/nips/UstinovaL16",
                "MAG": "2953046108",
                "ArXiv": "1611.00822",
                "CorpusId": 15402687
            },
            "abstract": "We suggest a new loss for learning deep embeddings. The key characteristics of the new loss is the absence of tunable parameters and very good results obtained across a range of datasets and problems. The loss is computed by estimating two distribution of similarities for positive (matching) and negative (non-matching) point pairs, and then computing the probability of a positive pair to have a lower similarity score than a negative pair based on these probability estimates. We show that these operations can be performed in a simple and piecewise-differentiable manner using 1D histograms with soft assignment operations. This makes the proposed loss suitable for learning deep embeddings using stochastic optimization. The experiments reveal favourable results compared to recently proposed loss functions.",
            "referenceCount": 30,
            "citationCount": 325,
            "influentialCitationCount": 53,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-11-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ustinova2016LearningDE,\n author = {E. Ustinova and V. Lempitsky},\n booktitle = {Neural Information Processing Systems},\n pages = {4170-4178},\n title = {Learning Deep Embeddings with Histogram Loss},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6e0406e765a56f673ea4ff3402d92fd1172826a9",
            "@type": "ScholarlyArticle",
            "paperId": "6e0406e765a56f673ea4ff3402d92fd1172826a9",
            "corpusId": 7546954,
            "url": "https://www.semanticscholar.org/paper/6e0406e765a56f673ea4ff3402d92fd1172826a9",
            "title": "Deep Transfer Learning for Person Re-Identification",
            "venue": "IEEE International Conference on Multimedia Big Data",
            "publicationVenue": {
                "id": "urn:research:b96248c1-022a-4515-bae1-4ca0a18cc65d",
                "name": "IEEE International Conference on Multimedia Big Data",
                "alternate_names": [
                    "BigMM",
                    "IEEE Int Conf Multimedia Big Data"
                ],
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2953350812",
                "DBLP": "conf/bigmm/ChenW0YG0X18",
                "ArXiv": "1611.05244",
                "DOI": "10.1109/BigMM.2018.8499067",
                "CorpusId": 7546954
            },
            "abstract": "Person re-identification (Re-ID) poses an inevitable challenge to deep learning: how to learn a robust deep model with millions of parameters on a small training set of few or no labels. In this paper, two deep transfer learning methods are proposed to address the training data sparsity problem, respectively from the supervised and unsupervised settings. First, a two-stepped fine-tuning strategy with proxy classifier learning is developed to transfer knowledge from auxiliary datasets. Second, given an unlabelled Re-Iddataset, an unsupervised deep transfer learning model is proposed based on a co-training strategy. Extensive experiments show that the proposed models achieve a good performance of deep Re-ID models.",
            "referenceCount": 68,
            "citationCount": 282,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1611.05244",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-11-16",
            "journal": {
                "name": "2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Geng2016DeepTL,\n author = {Mengyue Geng and Yaowei Wang and T. Xiang and Yonghong Tian},\n booktitle = {IEEE International Conference on Multimedia Big Data},\n journal = {2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)},\n pages = {1-5},\n title = {Deep Transfer Learning for Person Re-Identification},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f44ff4fc0ed0142cb18472a5ba421bb538aa837e",
            "@type": "ScholarlyArticle",
            "paperId": "f44ff4fc0ed0142cb18472a5ba421bb538aa837e",
            "corpusId": 6779105,
            "url": "https://www.semanticscholar.org/paper/f44ff4fc0ed0142cb18472a5ba421bb538aa837e",
            "title": "Unsupervised Deep Embedding for Clustering Analysis",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1511.06335",
                "MAG": "2173649752",
                "DBLP": "journals/corr/XieGF15",
                "CorpusId": 6779105
            },
            "abstract": "Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.",
            "referenceCount": 44,
            "citationCount": 2193,
            "influentialCitationCount": 518,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-11-19",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Xie2015UnsupervisedDE,\n author = {Junyuan Xie and Ross B. Girshick and Ali Farhadi},\n booktitle = {International Conference on Machine Learning},\n pages = {478-487},\n title = {Unsupervised Deep Embedding for Clustering Analysis},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2d8c97db4bae00ff243d122b957091a236a697a7",
            "@type": "ScholarlyArticle",
            "paperId": "2d8c97db4bae00ff243d122b957091a236a697a7",
            "corpusId": 54558282,
            "url": "https://www.semanticscholar.org/paper/2d8c97db4bae00ff243d122b957091a236a697a7",
            "title": "Deep Anomaly Detection with Outlier Exposure",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2951883849",
                "DBLP": "journals/corr/abs-1812-04606",
                "ArXiv": "1812.04606",
                "CorpusId": 54558282
            },
            "abstract": "It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.",
            "referenceCount": 53,
            "citationCount": 1079,
            "influentialCitationCount": 237,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1812.04606"
            },
            "citationStyles": {
                "bibtex": "@Article{Hendrycks2018DeepAD,\n author = {Dan Hendrycks and Mantas Mazeika and Thomas G. Dietterich},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Deep Anomaly Detection with Outlier Exposure},\n volume = {abs/1812.04606},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:468a80bcd4ff9b3f47beb9145ff81140777bb3f3",
            "@type": "ScholarlyArticle",
            "paperId": "468a80bcd4ff9b3f47beb9145ff81140777bb3f3",
            "corpusId": 3047732,
            "url": "https://www.semanticscholar.org/paper/468a80bcd4ff9b3f47beb9145ff81140777bb3f3",
            "title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/iclr/YangH17",
                "MAG": "2407277018",
                "ArXiv": "1605.06391",
                "CorpusId": 3047732
            },
            "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.",
            "referenceCount": 41,
            "citationCount": 224,
            "influentialCitationCount": 31,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-05-20",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1605.06391"
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2016DeepMR,\n author = {Yongxin Yang and Timothy M. Hospedales},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Deep Multi-task Representation Learning: A Tensor Factorisation Approach},\n volume = {abs/1605.06391},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:04214e4c491366c1ead2cc85a2e36f717a2b8cd1",
            "@type": "ScholarlyArticle",
            "paperId": "04214e4c491366c1ead2cc85a2e36f717a2b8cd1",
            "corpusId": 10356927,
            "url": "https://www.semanticscholar.org/paper/04214e4c491366c1ead2cc85a2e36f717a2b8cd1",
            "title": "Learning the Number of Neurons in Deep Networks",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2951134251",
                "DBLP": "journals/corr/AlvarezS16",
                "ArXiv": "1611.06321",
                "CorpusId": 10356927
            },
            "abstract": "Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80\\% while retaining or even improving the network accuracy.",
            "referenceCount": 35,
            "citationCount": 371,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-11-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{\u00c1lvarez2016LearningTN,\n author = {J. \u00c1lvarez and M. Salzmann},\n booktitle = {Neural Information Processing Systems},\n pages = {2262-2270},\n title = {Learning the Number of Neurons in Deep Networks},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3ee01ec27e4e66e089b72a9989724be611c2ad90",
            "@type": "ScholarlyArticle",
            "paperId": "3ee01ec27e4e66e089b72a9989724be611c2ad90",
            "corpusId": 534043,
            "url": "https://www.semanticscholar.org/paper/3ee01ec27e4e66e089b72a9989724be611c2ad90",
            "title": "Neural Map: Structured Memory for Deep Reinforcement Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2594903727",
                "DBLP": "conf/iclr/ParisottoS18",
                "ArXiv": "1702.08360",
                "CorpusId": 534043
            },
            "abstract": "A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training.",
            "referenceCount": 23,
            "citationCount": 233,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-02-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1702.08360"
            },
            "citationStyles": {
                "bibtex": "@Article{Parisotto2017NeuralMS,\n author = {Emilio Parisotto and R. Salakhutdinov},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Neural Map: Structured Memory for Deep Reinforcement Learning},\n volume = {abs/1702.08360},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:381411d740562de1e766dc8cc833844eb99dde01",
            "@type": "ScholarlyArticle",
            "paperId": "381411d740562de1e766dc8cc833844eb99dde01",
            "corpusId": 221304724,
            "url": "https://www.semanticscholar.org/paper/381411d740562de1e766dc8cc833844eb99dde01",
            "title": "Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks.",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "MAG": "3080555959",
                "CorpusId": 221304724
            },
            "abstract": "Advancing research in the emerging field of deep graph learning requires new tools to support tensor computation over graphs. In this paper, we present the design principles and implementation of Deep Graph Library (DGL). DGL distills the computational patterns of GNNs into a few generalized sparse tensor operations suitable for extensive parallelization. By advocating graph as the central programming abstraction, DGL can perform optimizations transparently. By cautiously adopting a framework-neutral design, DGL allows users to easily port and leverage the existing components across multiple deep learning frameworks. Our evaluation shows that DGL significantly outperforms other popular GNN-oriented frameworks in both speed and memory consumption over a variety of benchmarks and has little overhead for small scale workloads.",
            "referenceCount": 52,
            "citationCount": 752,
            "influentialCitationCount": 103,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2019-09-03",
            "journal": {
                "name": "arXiv: Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2019DeepGL,\n author = {Minjie Wang and Da Zheng and Zihao Ye and Quan Gan and Mufei Li and Xiang Song and Jinjing Zhou and Chao Ma and Lingfan Yu and Yujie Gai and Tianjun Xiao and Tong He and G. Karypis and Jinyang Li and Zheng Zhang},\n journal = {arXiv: Learning},\n title = {Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks.},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f06ff5f719eb9cd939dde8fc9b199b17adcbc75f",
            "@type": "ScholarlyArticle",
            "paperId": "f06ff5f719eb9cd939dde8fc9b199b17adcbc75f",
            "corpusId": 206437632,
            "url": "https://www.semanticscholar.org/paper/f06ff5f719eb9cd939dde8fc9b199b17adcbc75f",
            "title": "Road Extraction by Deep Residual U-Net",
            "venue": "IEEE Geoscience and Remote Sensing Letters",
            "publicationVenue": {
                "id": "urn:research:290335d6-cddc-465d-87f1-807e86d8efee",
                "name": "IEEE Geoscience and Remote Sensing Letters",
                "alternate_names": [
                    "IEEE Geosci Remote Sens Lett"
                ],
                "issn": "1545-598X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=8859"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2774320778",
                "DBLP": "journals/corr/abs-1711-10684",
                "ArXiv": "1711.10684",
                "DOI": "10.1109/LGRS.2018.2802944",
                "CorpusId": 206437632
            },
            "abstract": "Road extraction from aerial images has been a hot research topic in the field of remote sensing image analysis. In this letter, a semantic segmentation neural network, which combines the strengths of residual learning and U-Net, is proposed for road area extraction. The network is built with residual units and has similar architecture to that of U-Net. The benefits of this model are twofold: first, residual units ease training of deep networks. Second, the rich skip connections within the network could facilitate information propagation, allowing us to design networks with fewer parameters, however, better performance. We test our network on a public road data set and compare it with U-Net and other two state-of-the-art deep-learning-based road extraction methods. The proposed approach outperforms all the comparing methods, which demonstrates its superiority over recently developed state of the arts.",
            "referenceCount": 27,
            "citationCount": 1454,
            "influentialCitationCount": 173,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1711.10684",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-11-29",
            "journal": {
                "name": "IEEE Geoscience and Remote Sensing Letters",
                "volume": "15"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2017RoadEB,\n author = {Zhengxin Zhang and Qingjie Liu and Yunhong Wang},\n booktitle = {IEEE Geoscience and Remote Sensing Letters},\n journal = {IEEE Geoscience and Remote Sensing Letters},\n pages = {749-753},\n title = {Road Extraction by Deep Residual U-Net},\n volume = {15},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea415809bf87ef4b99966c6c50de6cb996a02a97",
            "@type": "ScholarlyArticle",
            "paperId": "ea415809bf87ef4b99966c6c50de6cb996a02a97",
            "corpusId": 207808916,
            "url": "https://www.semanticscholar.org/paper/ea415809bf87ef4b99966c6c50de6cb996a02a97",
            "title": "Deep double descent: where bigger models and more data hurt",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1912-02292",
                "ArXiv": "1912.02292",
                "MAG": "2994081359",
                "DOI": "10.1088/1742-5468/ac3a74",
                "CorpusId": 207808916
            },
            "abstract": "We show that a variety of modern deep learning tasks exhibit a \u2018double-descent\u2019 phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.",
            "referenceCount": 50,
            "citationCount": 679,
            "influentialCitationCount": 87,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://iopscience.iop.org/article/10.1088/1742-5468/ac3a74/pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Physics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-12-04",
            "journal": {
                "name": "Journal of Statistical Mechanics: Theory and Experiment",
                "volume": "2021"
            },
            "citationStyles": {
                "bibtex": "@Article{Nakkiran2019DeepDD,\n author = {Preetum Nakkiran and Gal Kaplun and Yamini Bansal and Tristan Yang and B. Barak and Ilya Sutskever},\n booktitle = {International Conference on Learning Representations},\n journal = {Journal of Statistical Mechanics: Theory and Experiment},\n title = {Deep double descent: where bigger models and more data hurt},\n volume = {2021},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:12b6551a0f9f5aa62f7d37f03ebc66631e529c4b",
            "@type": "ScholarlyArticle",
            "paperId": "12b6551a0f9f5aa62f7d37f03ebc66631e529c4b",
            "corpusId": 12591063,
            "url": "https://www.semanticscholar.org/paper/12b6551a0f9f5aa62f7d37f03ebc66631e529c4b",
            "title": "Sequential Deep Learning for Human Action Recognition",
            "venue": "International Workshop on Human Behavior Unterstanding",
            "publicationVenue": {
                "id": "urn:research:f94f6a5b-5229-41d2-a69c-009a5bedf885",
                "name": "International Workshop on Human Behavior Unterstanding",
                "alternate_names": [
                    "Hum Behav Unterstanding",
                    "Human Behavior Unterstanding",
                    "Int Workshop Hum Behav Unterstanding",
                    "HBU"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=1173"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "conf/hbu/BaccoucheMWGB11",
                "MAG": "28988658",
                "DOI": "10.1007/978-3-642-25446-8_4",
                "CorpusId": 12591063
            },
            "abstract": null,
            "referenceCount": 30,
            "citationCount": 758,
            "influentialCitationCount": 48,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://liris.cnrs.fr/Documents/Liris-5228.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-11-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Baccouche2011SequentialDL,\n author = {M. Baccouche and Franck Mamalet and Christian Wolf and Christophe Garcia and A. Baskurt},\n booktitle = {International Workshop on Human Behavior Unterstanding},\n pages = {29-39},\n title = {Sequential Deep Learning for Human Action Recognition},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:aa8e4263ef59d095dc0f87fb0dae19b441bfa6c5",
            "@type": "ScholarlyArticle",
            "paperId": "aa8e4263ef59d095dc0f87fb0dae19b441bfa6c5",
            "corpusId": 3393747,
            "url": "https://www.semanticscholar.org/paper/aa8e4263ef59d095dc0f87fb0dae19b441bfa6c5",
            "title": "Opponent Modeling in Deep Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2951360189",
                "DBLP": "journals/corr/HeBKD16",
                "ArXiv": "1609.05559",
                "CorpusId": 3393747
            },
            "abstract": "Opponent modeling is necessary in multi-agent settings where secondary agents with competing goals also adapt their strategies, yet it remains challenging because strategies interact with each other and change. Most previous work focuses on developing probabilistic models or parameterized strategies for specific applications. Inspired by the recent success of deep reinforcement learning, we present neural-based models that jointly learn a policy and the behavior of opponents. Instead of explicitly predicting the opponent's action, we encode observation of the opponents into a deep Q-Network (DQN); however, we retain explicit modeling (if desired) using multitasking. By using a Mixture-of-Experts architecture, our model automatically discovers different strategy patterns of opponents without extra supervision. We evaluate our models on a simulated soccer game and a popular trivia game, showing superior performance over DQN and its variants.",
            "referenceCount": 25,
            "citationCount": 267,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-19",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1609.05559"
            },
            "citationStyles": {
                "bibtex": "@Article{He2016OpponentMI,\n author = {He He and Jordan L. Boyd-Graber},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Opponent Modeling in Deep Reinforcement Learning},\n volume = {abs/1609.05559},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a55970013b984f344dfbbbba677d89dce0ba5f81",
            "@type": "ScholarlyArticle",
            "paperId": "a55970013b984f344dfbbbba677d89dce0ba5f81",
            "corpusId": 21618854,
            "url": "https://www.semanticscholar.org/paper/a55970013b984f344dfbbbba677d89dce0ba5f81",
            "title": "Image Super-Resolution via Deep Recursive Residual Network",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/cvpr/TaiY017",
                "MAG": "2747898905",
                "DOI": "10.1109/CVPR.2017.298",
                "CorpusId": 21618854
            },
            "abstract": "Recently, Convolutional Neural Network (CNN) based models have achieved great success in Single Image Super-Resolution (SISR). Owing to the strength of deep networks, these CNN models learn an effective nonlinear mapping from the low-resolution input image to the high-resolution target image, at the cost of requiring enormous parameters. This paper proposes a very deep CNN model (up to 52 convolutional layers) named Deep Recursive Residual Network (DRRN) that strives for deep yet concise networks. Specifically, residual learning is adopted, both in global and local manners, to mitigate the difficulty of training very deep networks, recursive learning is used to control the model parameters while increasing the depth. Extensive benchmark evaluation shows that DRRN significantly outperforms state of the art in SISR, while utilizing far fewer parameters. Code is available at https://github.com/tyshiwo/DRRN_CVPR17.",
            "referenceCount": 38,
            "citationCount": 1701,
            "influentialCitationCount": 292,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-07-01",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tai2017ImageSV,\n author = {Ying Tai and Jian Yang and Xiaoming Liu},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2790-2798},\n title = {Image Super-Resolution via Deep Recursive Residual Network},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:29ebb4ded00ac91538460e8c06e5733cc00ba44e",
            "@type": "ScholarlyArticle",
            "paperId": "29ebb4ded00ac91538460e8c06e5733cc00ba44e",
            "corpusId": 10848380,
            "url": "https://www.semanticscholar.org/paper/29ebb4ded00ac91538460e8c06e5733cc00ba44e",
            "title": "Deep multiple instance learning for image classification and auto-annotation",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/cvpr/WuYHY15",
                "MAG": "1938425378",
                "DOI": "10.1109/CVPR.2015.7298968",
                "CorpusId": 10848380
            },
            "abstract": "The recent development in learning deep representations has demonstrated its wide applications in traditional vision tasks like classification and detection. However, there has been little investigation on how we could build up a deep learning framework in a weakly supervised setting. In this paper, we attempt to model deep learning in a weakly supervised learning (multiple instance learning) framework. In our setting, each image follows a dual multi-instance assumption, where its object proposals and possible text annotations can be regarded as two instance sets. We thus design effective systems to exploit the MIL property with deep learning strategies from the two ends; we also try to jointly learn the relationship between object and annotation proposals. We conduct extensive experiments and prove that our weakly supervised deep learning framework not only achieves convincing performance in vision tasks including classification and image annotation, but also extracts reasonable region-keyword pairs with little supervision, on both widely used benchmarks like PASCAL VOC and MIT Indoor Scene 67, and also a dataset for image-and patch-level annotations.",
            "referenceCount": 48,
            "citationCount": 384,
            "influentialCitationCount": 25,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://jiajunwu.com/papers/dmil_cvpr.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-06-07",
            "journal": {
                "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wu2015DeepMI,\n author = {Jiajun Wu and Yinan Yu and Chang Huang and Kai Yu},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {3460-3469},\n title = {Deep multiple instance learning for image classification and auto-annotation},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf",
            "@type": "ScholarlyArticle",
            "paperId": "5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf",
            "corpusId": 11455421,
            "url": "https://www.semanticscholar.org/paper/5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf",
            "title": "A Closer Look at Memorization in Deep Networks",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2682189153",
                "DBLP": "journals/corr/ArpitJBKBKMFCBL17",
                "ArXiv": "1706.05394",
                "CorpusId": 11455421
            },
            "abstract": "We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.",
            "referenceCount": 42,
            "citationCount": 1379,
            "influentialCitationCount": 134,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-06-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Arpit2017ACL,\n author = {Devansh Arpit and Stanislaw Jastrzebski and Nicolas Ballas and David Krueger and Emmanuel Bengio and Maxinder S. Kanwal and Tegan Maharaj and Asja Fischer and Aaron C. Courville and Yoshua Bengio and Simon Lacoste-Julien},\n booktitle = {International Conference on Machine Learning},\n pages = {233-242},\n title = {A Closer Look at Memorization in Deep Networks},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a456265138c088a894301c0433dae938705a9bec",
            "@type": "ScholarlyArticle",
            "paperId": "a456265138c088a894301c0433dae938705a9bec",
            "corpusId": 4870287,
            "url": "https://www.semanticscholar.org/paper/a456265138c088a894301c0433dae938705a9bec",
            "title": "Deep Sets",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1703.06114",
                "CorpusId": 4870287
            },
            "abstract": "In this paper, we study the problem of designing objective functions for machine learning problems defined on finite \\emph{sets}. In contrast to traditional objective functions defined for machine learning problems operating on finite dimensional vectors, the new objective functions we propose are operating on finite sets and are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics \\citep{poczos13aistats}, via anomaly detection in piezometer data of embankment dams \\citep{Jung15Exploration}, to cosmology \\citep{Ntampaka16Dynamical,Ravanbakhsh16ICML1}. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and image tagging.",
            "referenceCount": 54,
            "citationCount": 1836,
            "influentialCitationCount": 231,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2017-03-10",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Zaheer2017DeepS,\n author = {M. Zaheer and Satwik Kottur and Siamak Ravanbakhsh and B. P\u00f3czos and R. Salakhutdinov and Alex Smola},\n title = {Deep Sets},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1703631a938b397ba7e858161ce16448f6046d6f",
            "@type": "ScholarlyArticle",
            "paperId": "1703631a938b397ba7e858161ce16448f6046d6f",
            "corpusId": 206596260,
            "url": "https://www.semanticscholar.org/paper/1703631a938b397ba7e858161ce16448f6046d6f",
            "title": "iCaRL: Incremental Classifier and Representation Learning",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2964189064",
                "ArXiv": "1611.07725",
                "DBLP": "conf/cvpr/RebuffiKSL17",
                "DOI": "10.1109/CVPR.2017.587",
                "CorpusId": 206596260
            },
            "abstract": "A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail.",
            "referenceCount": 45,
            "citationCount": 2445,
            "influentialCitationCount": 553,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1611.07725",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-11-23",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rebuffi2016iCaRLIC,\n author = {Sylvestre-Alvise Rebuffi and Alexander Kolesnikov and G. Sperl and Christoph H. Lampert},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5533-5542},\n title = {iCaRL: Incremental Classifier and Representation Learning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:43428880d75b3a14257c3ee9bda054e61eb869c0",
            "@type": "ScholarlyArticle",
            "paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0",
            "corpusId": 3648736,
            "url": "https://www.semanticscholar.org/paper/43428880d75b3a14257c3ee9bda054e61eb869c0",
            "title": "Convolutional Sequence to Sequence Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1705.03122",
                "DBLP": "journals/corr/GehringAGYD17",
                "MAG": "2613904329",
                "CorpusId": 3648736
            },
            "abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.",
            "referenceCount": 51,
            "citationCount": 2981,
            "influentialCitationCount": 333,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-05-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1705.03122"
            },
            "citationStyles": {
                "bibtex": "@Article{Gehring2017ConvolutionalST,\n author = {Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann Dauphin},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Convolutional Sequence to Sequence Learning},\n volume = {abs/1705.03122},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cf986bfe13a24d4739f95df3a856a3c6e4ed4c1c",
            "@type": "ScholarlyArticle",
            "paperId": "cf986bfe13a24d4739f95df3a856a3c6e4ed4c1c",
            "corpusId": 623137,
            "url": "https://www.semanticscholar.org/paper/cf986bfe13a24d4739f95df3a856a3c6e4ed4c1c",
            "title": "Learning Deconvolution Network for Semantic Segmentation",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1505.04366",
                "MAG": "2809451205",
                "DBLP": "conf/iccv/NohHH15",
                "DOI": "10.1109/ICCV.2015.178",
                "CorpusId": 623137
            },
            "abstract": "We propose a novel semantic segmentation algorithm by learning a deep deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixelwise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction, our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5%) among the methods trained without using Microsoft COCO dataset through ensemble with the fully convolutional network.",
            "referenceCount": 31,
            "citationCount": 3876,
            "influentialCitationCount": 258,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1505.04366",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-05-17",
            "journal": {
                "name": "2015 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Noh2015LearningDN,\n author = {Hyeonwoo Noh and Seunghoon Hong and Bohyung Han},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2015 IEEE International Conference on Computer Vision (ICCV)},\n pages = {1520-1528},\n title = {Learning Deconvolution Network for Semantic Segmentation},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3ec10bc12d9e40b1a43eaf0f4d7e3320ac5e7dc8",
            "@type": "ScholarlyArticle",
            "paperId": "3ec10bc12d9e40b1a43eaf0f4d7e3320ac5e7dc8",
            "corpusId": 62497190,
            "url": "https://www.semanticscholar.org/paper/3ec10bc12d9e40b1a43eaf0f4d7e3320ac5e7dc8",
            "title": "Theano: Deep Learning on GPUs with Python",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2254715784",
                "CorpusId": 62497190
            },
            "abstract": "In this paper, we present Theano 1 , a framework in the Python programming language for defining, optimizing and evaluating expressions involving high-level operations on tensors. Theano offers most of NumPy\u2019s functionality, but adds automatic symbolic differentiation, GPU support, and faster expression evaluation. Theano is a general mathematical tool, but it was developed with the goal of facilitating research in deep learning. The Deep Learning Tutorials 2 introduce recent advances in deep learning, and showcase how Theano",
            "referenceCount": 5,
            "citationCount": 253,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Bergstra2012TheanoDL,\n author = {J. Bergstra and Fr\u00e9d\u00e9ric Bastien and Olivier Breuleux and Pascal Lamblin and Razvan Pascanu and Olivier Delalleau and Guillaume Desjardins and David Warde-Farley and I. Goodfellow and Arnaud Bergeron and Yoshua Bengio},\n title = {Theano: Deep Learning on GPUs with Python},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3c3861c607fb79f3fbf79552018724617fc8ba1b",
            "@type": "ScholarlyArticle",
            "paperId": "3c3861c607fb79f3fbf79552018724617fc8ba1b",
            "corpusId": 46900,
            "url": "https://www.semanticscholar.org/paper/3c3861c607fb79f3fbf79552018724617fc8ba1b",
            "title": "A Deep Hierarchical Approach to Lifelong Learning in Minecraft",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1604.07255",
                "DBLP": "journals/corr/TesslerGZMM16",
                "MAG": "2952781612",
                "DOI": "10.1609/aaai.v31i1.10744",
                "CorpusId": 46900
            },
            "abstract": "\n \n We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks, are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using two techniques: (1) a deep skill array and (2) skill distillation, our novel variation of policy distillation (Rusu et. al. 2015) for learning skills. Skill distillation enables the H-DRLN to efficiently retain knowledge and therefore scale in lifelong learning, by accumulating knowledge and encapsulating multiple reusable skills into a single distilled network. The H-DRLN exhibits superior performance and lower learning sample complexity compared to the regular Deep Q Network (Mnih et. al. 2015) in sub-domains of Minecraft.\n \n",
            "referenceCount": 50,
            "citationCount": 327,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10744/10603",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-04-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1604.07255"
            },
            "citationStyles": {
                "bibtex": "@Article{Tessler2016ADH,\n author = {Chen Tessler and Shahar Givony and Tom Zahavy and D. Mankowitz and Shie Mannor},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {A Deep Hierarchical Approach to Lifelong Learning in Minecraft},\n volume = {abs/1604.07255},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:541266753f17f6ce469e2b3a157fc7419017b919",
            "@type": "ScholarlyArticle",
            "paperId": "541266753f17f6ce469e2b3a157fc7419017b919",
            "corpusId": 15019934,
            "url": "https://www.semanticscholar.org/paper/541266753f17f6ce469e2b3a157fc7419017b919",
            "title": "Supervised Representation Learning: Transfer Learning with Deep Autoencoders",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2261310161",
                "DBLP": "conf/ijcai/ZhuangCLPH15",
                "CorpusId": 15019934
            },
            "abstract": "Transfer learning has attracted a lot of attention in the past decade. One crucial research issue in transfer learning is how to find a good representation for instances of different domains such that the divergence between domains can be reduced with the new representation. Recently, deep learning has been proposed to learn more robust or higherlevel features for transfer learning. However, to the best of our knowledge, most of the previous approaches neither minimize the difference between domains explicitly nor encode label information in learning the representation. In this paper, we propose a supervised representation learning method based on deep autoencoders for transfer learning. The proposed deep autoencoder consists of two encoding layers: an embedding layer and a label encoding layer. In the embedding layer, the distance in distributions of the embedded instances between the source and target domains is minimized in terms of KL-Divergence. In the label encoding layer, label information of the source domain is encoded using a softmax regression model. Extensive experiments conducted on three real-world image datasets demonstrate the effectiveness of our proposed method compared with several state-of-the-art baseline methods.",
            "referenceCount": 36,
            "citationCount": 300,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-07-25",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhuang2015SupervisedRL,\n author = {Fuzhen Zhuang and X. Cheng and Ping Luo and Sinno Jialin Pan and Qing He},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {4119-4125},\n title = {Supervised Representation Learning: Transfer Learning with Deep Autoencoders},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3127190433230b3dc1abd0680bb58dced4bcd90e",
            "@type": "ScholarlyArticle",
            "paperId": "3127190433230b3dc1abd0680bb58dced4bcd90e",
            "corpusId": 372467,
            "url": "https://www.semanticscholar.org/paper/3127190433230b3dc1abd0680bb58dced4bcd90e",
            "title": "Large Scale Distributed Deep Networks",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "conf/nips/DeanCMCDLMRSTYN12",
                "MAG": "2168231600",
                "CorpusId": 372467
            },
            "abstract": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.",
            "referenceCount": 31,
            "citationCount": 3470,
            "influentialCitationCount": 307,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2012-12-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dean2012LargeSD,\n author = {J. Dean and G. Corrado and R. Monga and Kai Chen and Matthieu Devin and Quoc V. Le and Mark Z. Mao and Marc'Aurelio Ranzato and A. Senior and P. Tucker and Ke Yang and A. Ng},\n booktitle = {Neural Information Processing Systems},\n pages = {1232-1240},\n title = {Large Scale Distributed Deep Networks},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:53b047e503f4c24602f376a774d653f7ed56c024",
            "@type": "ScholarlyArticle",
            "paperId": "53b047e503f4c24602f376a774d653f7ed56c024",
            "corpusId": 1090603,
            "url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024",
            "title": "Practical Black-Box Attacks against Machine Learning",
            "venue": "ACM Asia Conference on Computer and Communications Security",
            "publicationVenue": {
                "id": "urn:research:87fc9c3c-cc7f-42aa-ba71-2700729a6788",
                "name": "ACM Asia Conference on Computer and Communications Security",
                "alternate_names": [
                    "AsiaCCS",
                    "ACM Asia Conf Comput Commun Secur",
                    "ACM Symposium on Information, Computer and Communications Security",
                    "ACM Symp Inf Comput Commun Secur",
                    "ASIACCS"
                ],
                "issn": null,
                "url": "https://dl.acm.org/conference/asia-ccs"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1602.02697",
                "MAG": "2603766943",
                "DBLP": "conf/ccs/PapernotMGJCS17",
                "DOI": "10.1145/3052973.3053009",
                "CorpusId": 1090603
            },
            "abstract": "Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",
            "referenceCount": 41,
            "citationCount": 3080,
            "influentialCitationCount": 243,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1602.02697",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-02-08",
            "journal": {
                "name": "Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Papernot2016PracticalBA,\n author = {Nicolas Papernot and P. Mcdaniel and I. Goodfellow and S. Jha and Z. B. Celik and A. Swami},\n booktitle = {ACM Asia Conference on Computer and Communications Security},\n journal = {Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security},\n title = {Practical Black-Box Attacks against Machine Learning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0ed3e5eb846f112f4aa485b9f8b4f50e5d61c365",
            "@type": "ScholarlyArticle",
            "paperId": "0ed3e5eb846f112f4aa485b9f8b4f50e5d61c365",
            "corpusId": 17923042,
            "url": "https://www.semanticscholar.org/paper/0ed3e5eb846f112f4aa485b9f8b4f50e5d61c365",
            "title": "Deep and surface learning in problem-based learning: a review of the literature",
            "venue": "Advances in health sciences education : theory and practice",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "PubMedCentral": "5119847",
                "MAG": "2151217332",
                "DOI": "10.1007/s10459-015-9645-6",
                "CorpusId": 17923042,
                "PubMed": "26563722"
            },
            "abstract": null,
            "referenceCount": 55,
            "citationCount": 264,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2015-11-13",
            "journal": {
                "name": "Advances in Health Sciences Education",
                "volume": "21"
            },
            "citationStyles": {
                "bibtex": "@Article{Dolmans2015DeepAS,\n author = {D. Dolmans and Sofie M. M. Loyens and H\u00e9l\u00e8ne Marcq and D. Gijbels},\n booktitle = {Advances in health sciences education : theory and practice},\n journal = {Advances in Health Sciences Education},\n pages = {1087 - 1112},\n title = {Deep and surface learning in problem-based learning: a review of the literature},\n volume = {21},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:032274e57f7d8b456bd255fe76b909b2c1d7458e",
            "@type": "ScholarlyArticle",
            "paperId": "032274e57f7d8b456bd255fe76b909b2c1d7458e",
            "corpusId": 21850704,
            "url": "https://www.semanticscholar.org/paper/032274e57f7d8b456bd255fe76b909b2c1d7458e",
            "title": "A Deep Reinforced Model for Abstractive Summarization",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1705.04304",
                "DBLP": "journals/corr/PaulusXS17",
                "MAG": "2612675303",
                "CorpusId": 21850704
            },
            "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.",
            "referenceCount": 43,
            "citationCount": 1369,
            "influentialCitationCount": 193,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-05-11",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1705.04304"
            },
            "citationStyles": {
                "bibtex": "@Article{Paulus2017ADR,\n author = {Romain Paulus and Caiming Xiong and R. Socher},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {A Deep Reinforced Model for Abstractive Summarization},\n volume = {abs/1705.04304},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a6f835ca6e12245a835ab6074bc6ec2c3c60b85a",
            "@type": "ScholarlyArticle",
            "paperId": "a6f835ca6e12245a835ab6074bc6ec2c3c60b85a",
            "corpusId": 2698863,
            "url": "https://www.semanticscholar.org/paper/a6f835ca6e12245a835ab6074bc6ec2c3c60b85a",
            "title": "One Pixel Attack for Fooling Deep Neural Networks",
            "venue": "IEEE Transactions on Evolutionary Computation",
            "publicationVenue": {
                "id": "urn:research:79644985-a91b-42a7-ac72-bb961c283f5e",
                "name": "IEEE Transactions on Evolutionary Computation",
                "alternate_names": [
                    "IEEE Trans Evol Comput"
                ],
                "issn": "1089-778X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=4235"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2964006983",
                "DBLP": "journals/tec/SuVS19",
                "ArXiv": "1710.08864",
                "DOI": "10.1109/TEVC.2019.2890858",
                "CorpusId": 2698863
            },
            "abstract": "Recent research has revealed that the output of deep neural networks (DNNs) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03% and 22.91% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.",
            "referenceCount": 67,
            "citationCount": 1925,
            "influentialCitationCount": 111,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1710.08864",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-10-24",
            "journal": {
                "name": "IEEE Transactions on Evolutionary Computation",
                "volume": "23"
            },
            "citationStyles": {
                "bibtex": "@Article{Su2017OnePA,\n author = {Jiawei Su and Danilo Vasconcellos Vargas and K. Sakurai},\n booktitle = {IEEE Transactions on Evolutionary Computation},\n journal = {IEEE Transactions on Evolutionary Computation},\n pages = {828-841},\n title = {One Pixel Attack for Fooling Deep Neural Networks},\n volume = {23},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d592ad48973b2bb0e268d3b404fb7262d56587b1",
            "@type": "ScholarlyArticle",
            "paperId": "d592ad48973b2bb0e268d3b404fb7262d56587b1",
            "corpusId": 29819269,
            "url": "https://www.semanticscholar.org/paper/d592ad48973b2bb0e268d3b404fb7262d56587b1",
            "title": "Deep Spectral Clustering Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/icml/LawUZ17",
                "MAG": "2740643007",
                "CorpusId": 29819269
            },
            "abstract": "Clustering is the task of grouping a set of examples so that similar examples are grouped into the same cluster while dissimilar examples are in different clusters. The quality of a clustering depends on two problem-dependent factors which are i) the chosen similarity metric and ii) the data representation. Supervised clustering approaches, which exploit labeled partitioned datasets have thus been proposed, for instance to learn a metric optimized to perform clustering. However, most of these approaches assume that the representation of the data is \ufb01xed and then learn an appropriate linear transformation. Some deep supervised clustering learning approaches have also been proposed. However, they rely on iterative methods to compute gradients resulting in high algorithmic complexity. In this paper, we propose a deep supervised clustering metric learning method that formulates a novel loss function. We derive a closed-form expression for the gradient that is ef\ufb01cient to compute: the complexity to compute the gradient is linear in the size of the training mini-batch and quadratic in the representation dimensionality. We further reveal how our approach can be seen as learning spectral clustering. Experiments on standard real-world datasets con\ufb01rm state-of-the-art Recall@K performance.",
            "referenceCount": 37,
            "citationCount": 128,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-07-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Law2017DeepSC,\n author = {M. Law and R. Urtasun and R. Zemel},\n booktitle = {International Conference on Machine Learning},\n pages = {1985-1994},\n title = {Deep Spectral Clustering Learning},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f98788f32b0d33d200c9bc7d900d0ef39519c927",
            "@type": "ScholarlyArticle",
            "paperId": "f98788f32b0d33d200c9bc7d900d0ef39519c927",
            "corpusId": 4800342,
            "url": "https://www.semanticscholar.org/paper/f98788f32b0d33d200c9bc7d900d0ef39519c927",
            "title": "Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/cvpr/KendallGC18",
                "ArXiv": "1705.07115",
                "MAG": "2950753041",
                "DOI": "10.1109/CVPR.2018.00781",
                "CorpusId": 4800342
            },
            "abstract": "Numerous deep learning applications benefit from multitask learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.",
            "referenceCount": 47,
            "citationCount": 2248,
            "influentialCitationCount": 239,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ora.ox.ac.uk/objects/uuid:3903e961-25b0-40de-b797-1c455a198d5b/download_file?safe_filename=Cipolla%2Bet%2Bal%2BMulti-task%2Blearning%2Busing%2Buncertainty.pdf&file_format=pdf&type_of_work=Conference+item",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-05-19",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kendall2017MultitaskLU,\n author = {Alex Kendall and Y. Gal and R. Cipolla},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {7482-7491},\n title = {Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ce435482acc0e195be8d8f002b2655b4c7b08be6",
            "@type": "ScholarlyArticle",
            "paperId": "ce435482acc0e195be8d8f002b2655b4c7b08be6",
            "corpusId": 211146562,
            "url": "https://www.semanticscholar.org/paper/ce435482acc0e195be8d8f002b2655b4c7b08be6",
            "title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "2996108195",
                "ArXiv": "2002.07394",
                "DBLP": "conf/iclr/LiSH20",
                "CorpusId": 211146562
            },
            "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at this https URL .",
            "referenceCount": 46,
            "citationCount": 633,
            "influentialCitationCount": 192,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-02-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2002.07394"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2020DivideMixLW,\n author = {Junnan Li and R. Socher and S. Hoi},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {DivideMix: Learning with Noisy Labels as Semi-supervised Learning},\n volume = {abs/2002.07394},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:370b680057a6e324e67576a6bf1bf580af9fdd74",
            "@type": "ScholarlyArticle",
            "paperId": "370b680057a6e324e67576a6bf1bf580af9fdd74",
            "corpusId": 219687051,
            "url": "https://www.semanticscholar.org/paper/370b680057a6e324e67576a6bf1bf580af9fdd74",
            "title": "Self-Supervised Learning: Generative or Contrastive",
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "publicationVenue": {
                "id": "urn:research:c6840156-ee10-4d78-8832-7f8909811576",
                "name": "IEEE Transactions on Knowledge and Data Engineering",
                "alternate_names": [
                    "IEEE Trans Knowl Data Eng"
                ],
                "issn": "1041-4347",
                "url": "https://www.computer.org/web/tkde"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/tkde/LiuZHMWZT23",
                "ArXiv": "2006.08218",
                "MAG": "3035725276",
                "DOI": "10.1109/TKDE.2021.3090866",
                "CorpusId": 219687051
            },
            "abstract": "Deep supervised learning has achieved great success in the last decade. However, its defects of heavy dependence on manual labels and vulnerability to attacks have driven people to find other paradigms. As an alternative, self-supervised learning (SSL) attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further collect related theoretical analysis on self-supervised learning to provide deeper thoughts on why self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided<inline-formula><tex-math notation=\"LaTeX\">$^1$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>1</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"liu-ieq1-3090866.gif\"/></alternatives></inline-formula>.",
            "referenceCount": 184,
            "citationCount": 870,
            "influentialCitationCount": 53,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2006.08218",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-06-15",
            "journal": {
                "name": "IEEE Transactions on Knowledge and Data Engineering",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2020SelfSupervisedLG,\n author = {Xiao Liu and Fanjin Zhang and Zhenyu Hou and Zhaoyu Wang and Li Mian and Jing Zhang and Jie Tang},\n booktitle = {IEEE Transactions on Knowledge and Data Engineering},\n journal = {IEEE Transactions on Knowledge and Data Engineering},\n pages = {857-876},\n title = {Self-Supervised Learning: Generative or Contrastive},\n volume = {35},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:93100ebe89840bc235c586ddc6daccd262707fec",
            "@type": "ScholarlyArticle",
            "paperId": "93100ebe89840bc235c586ddc6daccd262707fec",
            "corpusId": 7179166,
            "url": "https://www.semanticscholar.org/paper/93100ebe89840bc235c586ddc6daccd262707fec",
            "title": "Learning Activation Functions to Improve Deep Neural Networks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2963920515",
                "DBLP": "journals/corr/AgostinelliHSB14",
                "ArXiv": "1412.6830",
                "CorpusId": 7179166
            },
            "abstract": "Artificial neural networks typically have a fixed, non-linear activation function at each neuron. We have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes.",
            "referenceCount": 27,
            "citationCount": 443,
            "influentialCitationCount": 47,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2014-12-21",
            "journal": {
                "name": "arXiv: Neural and Evolutionary Computing",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Agostinelli2014LearningAF,\n author = {Forest Agostinelli and M. Hoffman and Peter Sadowski and P. Baldi},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Neural and Evolutionary Computing},\n title = {Learning Activation Functions to Improve Deep Neural Networks},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5d90f06bb70a0a3dced62413346235c02b1aa086",
            "@type": "ScholarlyArticle",
            "paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "corpusId": 18268744,
            "url": "https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086",
            "title": "Learning Multiple Layers of Features from Tiny Images",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2945315962",
                "CorpusId": 18268744
            },
            "abstract": "Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images.",
            "referenceCount": 15,
            "citationCount": 26458,
            "influentialCitationCount": 8109,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Krizhevsky2009LearningML,\n author = {A. Krizhevsky},\n title = {Learning Multiple Layers of Features from Tiny Images},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:49da57cf2900cd50d64a6d63d45e1bccd454fcbb",
            "@type": "ScholarlyArticle",
            "paperId": "49da57cf2900cd50d64a6d63d45e1bccd454fcbb",
            "corpusId": 16553385,
            "url": "https://www.semanticscholar.org/paper/49da57cf2900cd50d64a6d63d45e1bccd454fcbb",
            "title": "Deep Reinforcement Learning in Parameterized Action Space",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/corr/HausknechtS15a",
                "MAG": "2950193671",
                "ArXiv": "1511.04143",
                "CorpusId": 16553385
            },
            "abstract": "Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces. However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces. To fill this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which features a small set of discrete action types, each of which is parameterized with continuous variables. The best learned agent can score goals more reliably than the 2012 RoboCup champion agent. As such, this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space MDPs.",
            "referenceCount": 25,
            "citationCount": 262,
            "influentialCitationCount": 29,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-13",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1511.04143"
            },
            "citationStyles": {
                "bibtex": "@Article{Hausknecht2015DeepRL,\n author = {Matthew J. Hausknecht and P. Stone},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Deep Reinforcement Learning in Parameterized Action Space},\n volume = {abs/1511.04143},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:998620638cfc7c6b5d7b95fa8645f75723d78372",
            "@type": "ScholarlyArticle",
            "paperId": "998620638cfc7c6b5d7b95fa8645f75723d78372",
            "corpusId": 211132598,
            "url": "https://www.semanticscholar.org/paper/998620638cfc7c6b5d7b95fa8645f75723d78372",
            "title": "Federated Learning with Matched Averaging",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2002-06440",
                "MAG": "3006017224",
                "ArXiv": "2002.06440",
                "CorpusId": 211132598
            },
            "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, while improving the communication efficiency.",
            "referenceCount": 29,
            "citationCount": 691,
            "influentialCitationCount": 84,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-02-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2002.06440"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2020FederatedLW,\n author = {Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Y. Khazaeni},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Federated Learning with Matched Averaging},\n volume = {abs/2002.06440},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5aea95e1ae78a66474051a330ded374e199b658c",
            "@type": "ScholarlyArticle",
            "paperId": "5aea95e1ae78a66474051a330ded374e199b658c",
            "corpusId": 47018956,
            "url": "https://www.semanticscholar.org/paper/5aea95e1ae78a66474051a330ded374e199b658c",
            "title": "Representation Learning on Graphs with Jumping Knowledge Networks",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2804057010",
                "ArXiv": "1806.03536",
                "DBLP": "conf/icml/XuLTSKJ18",
                "CorpusId": 47018956
            },
            "abstract": "Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of \"neighboring\" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance.",
            "referenceCount": 36,
            "citationCount": 1442,
            "influentialCitationCount": 229,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-06-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.03536"
            },
            "citationStyles": {
                "bibtex": "@Article{Xu2018RepresentationLO,\n author = {Keyulu Xu and Chengtao Li and Yonglong Tian and Tomohiro Sonobe and K. Kawarabayashi and S. Jegelka},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Representation Learning on Graphs with Jumping Knowledge Networks},\n volume = {abs/1806.03536},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3d2c6941a9b4608ba52b328369a3352db2092ae0",
            "@type": "ScholarlyArticle",
            "paperId": "3d2c6941a9b4608ba52b328369a3352db2092ae0",
            "corpusId": 151231,
            "url": "https://www.semanticscholar.org/paper/3d2c6941a9b4608ba52b328369a3352db2092ae0",
            "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2284050935",
                "DBLP": "conf/nips/SalimansK16",
                "ArXiv": "1602.07868",
                "CorpusId": 151231
            },
            "abstract": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.",
            "referenceCount": 34,
            "citationCount": 1640,
            "influentialCitationCount": 120,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-02-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1602.07868"
            },
            "citationStyles": {
                "bibtex": "@Article{Salimans2016WeightNA,\n author = {Tim Salimans and Diederik P. Kingma},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks},\n volume = {abs/1602.07868},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a6c1a120f6c84eff4fb0facf404094f840105b9f",
            "@type": "ScholarlyArticle",
            "paperId": "a6c1a120f6c84eff4fb0facf404094f840105b9f",
            "corpusId": 6860810,
            "url": "https://www.semanticscholar.org/paper/a6c1a120f6c84eff4fb0facf404094f840105b9f",
            "title": "Workshop on Unsupervised and Transfer Learning Deep Learning of Representations for Unsupervised and Transfer Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": null,
            "externalIds": {
                "CorpusId": 6860810
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 1115,
            "influentialCitationCount": 47,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Misc{None,\n title = {Workshop on Unsupervised and Transfer Learning Deep Learning of Representations for Unsupervised and Transfer Learning}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c66758c1029a463489f26aeb3955f333b37f727a",
            "@type": "ScholarlyArticle",
            "paperId": "c66758c1029a463489f26aeb3955f333b37f727a",
            "corpusId": 10787826,
            "url": "https://www.semanticscholar.org/paper/c66758c1029a463489f26aeb3955f333b37f727a",
            "title": "Learning Deep Structured Models",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2928160594",
                "DBLP": "conf/icml/ChenSYU15",
                "ArXiv": "1407.2538",
                "CorpusId": 10787826
            },
            "abstract": "Many problems in real-world applications involve predicting several random variables that are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such dependencies. The goal of this paper is to combine MRFs with deep learning to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as tagging of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains.",
            "referenceCount": 74,
            "citationCount": 236,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-07-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2014LearningDS,\n author = {Liang-Chieh Chen and A. Schwing and A. Yuille and R. Urtasun},\n booktitle = {International Conference on Machine Learning},\n pages = {1785-1794},\n title = {Learning Deep Structured Models},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:11f4765ea4404742dc12dbdb06430873563a0a9d",
            "@type": "ScholarlyArticle",
            "paperId": "11f4765ea4404742dc12dbdb06430873563a0a9d",
            "corpusId": 53038373,
            "url": "https://www.semanticscholar.org/paper/11f4765ea4404742dc12dbdb06430873563a0a9d",
            "title": "Pyro: Deep Universal Probabilistic Programming",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1810.09538",
                "MAG": "2950526225",
                "DBLP": "journals/jmlr/BinghamCJOPKSSH19",
                "CorpusId": 53038373
            },
            "abstract": "Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in AI research. To scale to large datasets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of PyTorch, a modern GPU-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs.",
            "referenceCount": 15,
            "citationCount": 792,
            "influentialCitationCount": 102,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-10-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1810.09538"
            },
            "citationStyles": {
                "bibtex": "@Article{Bingham2018PyroDU,\n author = {Eli Bingham and Jonathan P. Chen and M. Jankowiak and F. Obermeyer and Neeraj Pradhan and Theofanis Karaletsos and Rohit Singh and Paul A. Szerlip and Paul Horsfall and Noah D. Goodman},\n booktitle = {Journal of machine learning research},\n journal = {ArXiv},\n title = {Pyro: Deep Universal Probabilistic Programming},\n volume = {abs/1810.09538},\n year = {2018}\n}\n"
            }
        }
    }
]