[
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04567v1",
            "title": "Scaling Laws of Synthetic Images for Model Training ... for Now",
            "updated": "2023-12-07T18:59:59Z",
            "published": "2023-12-07T18:59:59Z",
            "summary": "Recent significant advances in text-to-image models unlock the possibility of\ntraining vision systems using synthetic images, potentially overcoming the\ndifficulty of collecting curated data at scale. It is unclear, however, how\nthese models behave at scale, as more synthetic data is added to the training\nset. In this paper we study the scaling laws of synthetic images generated by\nstate of the art text-to-image models, for the training of supervised models:\nimage classifiers with label supervision, and CLIP with language supervision.\nWe identify several factors, including text prompts, classifier-free guidance\nscale, and types of text-to-image models, that significantly affect scaling\nbehavior. After tuning these factors, we observe that synthetic images\ndemonstrate a scaling trend similar to, but slightly less effective than, real\nimages in CLIP training, while they significantly underperform in scaling when\ntraining supervised image classifiers. Our analysis indicates that the main\nreason for this underperformance is the inability of off-the-shelf\ntext-to-image models to generate certain concepts, a limitation that\nsignificantly impairs the training of image classifiers. Our findings also\nsuggest that scaling synthetic data can be particularly effective in scenarios\nsuch as: (1) when there is a limited supply of real images for a supervised\nproblem (e.g., fewer than 0.5 million images in ImageNet), (2) when the\nevaluation dataset diverges significantly from the training data, indicating\nthe out-of-distribution scenario, or (3) when synthetic data is used in\nconjunction with real images, as demonstrated in the training of CLIP models.",
            "author": [
                "Lijie Fan",
                "Kaifeng Chen",
                "Dilip Krishnan",
                "Dina Katabi",
                "Phillip Isola",
                "Yonglong Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04567v1",
                "http://arxiv.org/pdf/2312.04567v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04566v1",
            "title": "Gen2Det: Generate to Detect",
            "updated": "2023-12-07T18:59:58Z",
            "published": "2023-12-07T18:59:58Z",
            "summary": "Recently diffusion models have shown improvement in synthetic image quality\nas well as better control in generation. We motivate and present Gen2Det, a\nsimple modular pipeline to create synthetic training data for object detection\nfor free by leveraging state-of-the-art grounded image generation methods.\nUnlike existing works which generate individual object instances, require\nidentifying foreground followed by pasting on other images, we simplify to\ndirectly generating scene-centric images. In addition to the synthetic data,\nGen2Det also proposes a suite of techniques to best utilize the generated data,\nincluding image-level filtering, instance-level filtering, and better training\nrecipe to account for imperfections in the generation. Using Gen2Det, we show\nhealthy improvements on object detection and segmentation tasks under various\nsettings and agnostic to detection methods. In the long-tailed detection\nsetting on LVIS, Gen2Det improves the performance on rare categories by a large\nmargin while also significantly improving the performance on other categories,\ne.g. we see an improvement of 2.13 Box AP and 1.84 Mask AP over just training\non real data on LVIS with Mask R-CNN. In the low-data regime setting on COCO,\nGen2Det consistently improves both Box and Mask AP by 2.27 and 1.85 points. In\nthe most general detection setting, Gen2Det still demonstrates robust\nperformance gains, e.g. it improves the Box and Mask AP on COCO by 0.45 and\n0.32 points.",
            "author": [
                "Saksham Suri",
                "Fanyi Xiao",
                "Animesh Sinha",
                "Sean Chang Culatana",
                "Raghuraman Krishnamoorthi",
                "Chenchen Zhu",
                "Abhinav Shrivastava"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04566v1",
                "http://arxiv.org/pdf/2312.04566v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04565v1",
            "title": "MuRF: Multi-Baseline Radiance Fields",
            "updated": "2023-12-07T18:59:56Z",
            "published": "2023-12-07T18:59:56Z",
            "summary": "We present Multi-Baseline Radiance Fields (MuRF), a general feed-forward\napproach to solving sparse view synthesis under multiple different baseline\nsettings (small and large baselines, and different number of input views). To\nrender a target novel view, we discretize the 3D space into planes parallel to\nthe target image plane, and accordingly construct a target view frustum volume.\nSuch a target volume representation is spatially aligned with the target view,\nwhich effectively aggregates relevant information from the input views for\nhigh-quality rendering. It also facilitates subsequent radiance field\nregression with a convolutional network thanks to its axis-aligned nature. The\n3D context modeled by the convolutional network enables our method to synthesis\nsharper scene structures than prior works. Our MuRF achieves state-of-the-art\nperformance across multiple different baseline settings and diverse scenarios\nranging from simple objects (DTU) to complex indoor and outdoor scenes\n(RealEstate10K and LLFF). We also show promising zero-shot generalization\nabilities on the Mip-NeRF 360 dataset, demonstrating the general applicability\nof MuRF.",
            "author": [
                "Haofei Xu",
                "Anpei Chen",
                "Yuedong Chen",
                "Christos Sakaridis",
                "Yulun Zhang",
                "Marc Pollefeys",
                "Andreas Geiger",
                "Fisher Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04565v1",
                "http://arxiv.org/pdf/2312.04565v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04564v1",
            "title": "EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS",
            "updated": "2023-12-07T18:59:55Z",
            "published": "2023-12-07T18:59:55Z",
            "summary": "Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view\nscene synthesis. It addresses the challenges of lengthy training times and slow\nrendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid,\ndifferentiable rasterization of 3D Gaussians, 3D-GS achieves real-time\nrendering and accelerated training. They, however, demand substantial memory\nresources for both training and storage, as they require millions of Gaussians\nin their point cloud representation for each scene. We present a technique\nutilizing quantized embeddings to significantly reduce memory storage\nrequirements and a coarse-to-fine training strategy for a faster and more\nstable optimization of the Gaussian point clouds. Our approach results in scene\nrepresentations with fewer Gaussians and quantized representations, leading to\nfaster training times and rendering speeds for real-time rendering of high\nresolution scenes. We reduce memory by more than an order of magnitude all\nwhile maintaining the reconstruction quality. We validate the effectiveness of\nour approach on a variety of datasets and scenes preserving the visual quality\nwhile consuming 10-20x less memory and faster training/inference speed. Project\npage and code is available https://efficientgaussian.github.io",
            "author": [
                "Sharath Girish",
                "Kamal Gupta",
                "Abhinav Shrivastava"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04564v1",
                "http://arxiv.org/pdf/2312.04564v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04563v1",
            "title": "Visual Geometry Grounded Deep Structure From Motion",
            "updated": "2023-12-07T18:59:52Z",
            "published": "2023-12-07T18:59:52Z",
            "summary": "Structure-from-motion (SfM) is a long-standing problem in the computer vision\ncommunity, which aims to reconstruct the camera poses and 3D structure of a\nscene from a set of unconstrained 2D images. Classical frameworks solve this\nproblem in an incremental manner by detecting and matching keypoints,\nregistering images, triangulating 3D points, and conducting bundle adjustment.\nRecent research efforts have predominantly revolved around harnessing the power\nof deep learning techniques to enhance specific elements (e.g., keypoint\nmatching), but are still based on the original, non-differentiable pipeline.\nInstead, we propose a new deep pipeline VGGSfM, where each component is fully\ndifferentiable and thus can be trained in an end-to-end manner. To this end, we\nintroduce new mechanisms and simplifications. First, we build on recent\nadvances in deep 2D point tracking to extract reliable pixel-accurate tracks,\nwhich eliminates the need for chaining pairwise matches. Furthermore, we\nrecover all cameras simultaneously based on the image and track features\ninstead of gradually registering cameras. Finally, we optimise the cameras and\ntriangulate 3D points via a differentiable bundle adjustment layer. We attain\nstate-of-the-art performance on three popular datasets, CO3D, IMC Phototourism,\nand ETH3D.",
            "author": [
                "Jianyuan Wang",
                "Nikita Karaev",
                "Christian Rupprecht",
                "David Novotny"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04563v1",
                "http://arxiv.org/pdf/2312.04563v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04562v1",
            "title": "Glassy word problems: ultraslow relaxation, Hilbert space jamming, and\n  computational complexity",
            "updated": "2023-12-07T18:59:46Z",
            "published": "2023-12-07T18:59:46Z",
            "summary": "We introduce a family of local models of dynamics based on ``word problems''\nfrom computer science and group theory, for which we can place rigorous lower\nbounds on relaxation timescales. These models can be regarded either as random\ncircuit or local Hamiltonian dynamics, and include many familiar examples of\nconstrained dynamics as special cases. The configuration space of these models\nsplits into dynamically disconnected sectors, and for initial states to relax,\nthey must ``work out'' the other states in the sector to which they belong.\nWhen this problem has a high time complexity, relaxation is slow. In some of\nthe cases we study, this problem also has high space complexity. When the space\ncomplexity is larger than the system size, an unconventional type of jamming\ntransition can occur, whereby a system of a fixed size is not ergodic, but can\nbe made ergodic by appending a large reservoir of sites in a trivial product\nstate. This manifests itself in a new type of Hilbert space fragmentation that\nwe call fragile fragmentation. We present explicit examples where slow\nrelaxation and jamming strongly modify the hydrodynamics of conserved\ndensities. In one example, density modulations of wavevector $q$ exhibit almost\nno relaxation until times $O(\\exp(1/q))$, at which point they abruptly\ncollapse. We also comment on extensions of our results to higher dimensions.",
            "author": [
                "Shankar Balasubramanian",
                "Sarang Gopalakrishnan",
                "Alexey Khudorozhkov",
                "Ethan Lake"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04562v1",
                "http://arxiv.org/pdf/2312.04562v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.dis-nn",
                "cond-mat.stat-mech",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04560v1",
            "title": "NeRFiller: Completing Scenes via Generative 3D Inpainting",
            "updated": "2023-12-07T18:59:41Z",
            "published": "2023-12-07T18:59:41Z",
            "summary": "We propose NeRFiller, an approach that completes missing portions of a 3D\ncapture via generative 3D inpainting using off-the-shelf 2D visual generative\nmodels. Often parts of a captured 3D scene or object are missing due to mesh\nreconstruction failures or a lack of observations (e.g., contact regions, such\nas the bottom of objects, or hard-to-reach areas). We approach this challenging\n3D inpainting problem by leveraging a 2D inpainting diffusion model. We\nidentify a surprising behavior of these models, where they generate more 3D\nconsistent inpaints when images form a 2$\\times$2 grid, and show how to\ngeneralize this behavior to more than four images. We then present an iterative\nframework to distill these inpainted regions into a single consistent 3D scene.\nIn contrast to related works, we focus on completing scenes rather than\ndeleting foreground objects, and our approach does not require tight 2D object\nmasks or text. We compare our approach to relevant baselines adapted to our\nsetting on a variety of scenes, where NeRFiller creates the most 3D consistent\nand plausible scene completions. Our project page is at\nhttps://ethanweber.me/nerfiller.",
            "author": [
                "Ethan Weber",
                "Aleksander Ho\u0142y\u0144ski",
                "Varun Jampani",
                "Saurabh Saxena",
                "Noah Snavely",
                "Abhishek Kar",
                "Angjoo Kanazawa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04560v1",
                "http://arxiv.org/pdf/2312.04560v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04561v1",
            "title": "GenDeF: Learning Generative Deformation Field for Video Generation",
            "updated": "2023-12-07T18:59:41Z",
            "published": "2023-12-07T18:59:41Z",
            "summary": "We offer a new perspective on approaching the task of video generation.\nInstead of directly synthesizing a sequence of frames, we propose to render a\nvideo by warping one static image with a generative deformation field (GenDeF).\nSuch a pipeline enjoys three appealing advantages. First, we can sufficiently\nreuse a well-trained image generator to synthesize the static image (also\ncalled canonical image), alleviating the difficulty in producing a video and\nthereby resulting in better visual quality. Second, we can easily convert a\ndeformation field to optical flows, making it possible to apply explicit\nstructural regularizations for motion modeling, leading to temporally\nconsistent results. Third, the disentanglement between content and motion\nallows users to process a synthesized video through processing its\ncorresponding static image without any tuning, facilitating many applications\nlike video editing, keypoint tracking, and video segmentation. Both qualitative\nand quantitative results on three common video generation benchmarks\ndemonstrate the superiority of our GenDeF method.",
            "author": [
                "Wen Wang",
                "Kecheng Zheng",
                "Qiuyu Wang",
                "Hao Chen",
                "Zifan Shi",
                "Ceyuan Yang",
                "Yujun Shen",
                "Chunhua Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04561v1",
                "http://arxiv.org/pdf/2312.04561v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04559v1",
            "title": "PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation",
            "updated": "2023-12-07T18:59:33Z",
            "published": "2023-12-07T18:59:33Z",
            "summary": "We present PrimDiffusion, the first diffusion-based framework for 3D human\ngeneration. Devising diffusion models for 3D human generation is difficult due\nto the intensive computational cost of 3D representations and the articulated\ntopology of 3D humans. To tackle these challenges, our key insight is operating\nthe denoising diffusion process directly on a set of volumetric primitives,\nwhich models the human body as a number of small volumes with radiance and\nkinematic information. This volumetric primitives representation marries the\ncapacity of volumetric representations with the efficiency of primitive-based\nrendering. Our PrimDiffusion framework has three appealing properties: 1)\ncompact and expressive parameter space for the diffusion model, 2) flexible 3D\nrepresentation that incorporates human prior, and 3) decoder-free rendering for\nefficient novel-view and novel-pose synthesis. Extensive experiments validate\nthat PrimDiffusion outperforms state-of-the-art methods in 3D human generation.\nNotably, compared to GAN-based methods, our PrimDiffusion supports real-time\nrendering of high-quality 3D humans at a resolution of $512\\times512$ once the\ndenoising process is done. We also demonstrate the flexibility of our framework\non training-free conditional generation such as texture transfer and 3D\ninpainting.",
            "author": [
                "Zhaoxi Chen",
                "Fangzhou Hong",
                "Haiyi Mei",
                "Guangcong Wang",
                "Lei Yang",
                "Ziwei Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04559v1",
                "http://arxiv.org/pdf/2312.04559v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04558v1",
            "title": "MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar",
            "updated": "2023-12-07T18:59:31Z",
            "published": "2023-12-07T18:59:31Z",
            "summary": "The ability to animate photo-realistic head avatars reconstructed from\nmonocular portrait video sequences represents a crucial step in bridging the\ngap between the virtual and real worlds. Recent advancements in head avatar\ntechniques, including explicit 3D morphable meshes (3DMM), point clouds, and\nneural implicit representation have been exploited for this ongoing research.\nHowever, 3DMM-based methods are constrained by their fixed topologies,\npoint-based approaches suffer from a heavy training burden due to the extensive\nquantity of points involved, and the last ones suffer from limitations in\ndeformation flexibility and rendering efficiency. In response to these\nchallenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head\nAvatar), a novel approach that harnesses 3D Gaussian point representation\ncoupled with a Gaussian deformation field to learn explicit head avatars from\nmonocular portrait videos. We define our head avatars with Gaussian points\ncharacterized by adaptable shapes, enabling flexible topology. These points\nexhibit movement with a Gaussian deformation field in alignment with the target\npose and expression of a person, facilitating efficient deformation.\nAdditionally, the Gaussian points have controllable shape, size, color, and\nopacity combined with Gaussian splatting, allowing for efficient training and\nrendering. Experiments demonstrate the superior performance of our method,\nwhich achieves state-of-the-art results among previous methods.",
            "author": [
                "Yufan Chen",
                "Lizhen Wang",
                "Qijing Li",
                "Hongjiang Xiao",
                "Shengping Zhang",
                "Hongxun Yao",
                "Yebin Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04558v1",
                "http://arxiv.org/pdf/2312.04558v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04557v1",
            "title": "GenTron: Delving Deep into Diffusion Transformers for Image and Video\n  Generation",
            "updated": "2023-12-07T18:59:30Z",
            "published": "2023-12-07T18:59:30Z",
            "summary": "In this study, we explore Transformer-based diffusion models for image and\nvideo generation. Despite the dominance of Transformer architectures in various\nfields due to their flexibility and scalability, the visual generative domain\nprimarily utilizes CNN-based U-Net architectures, particularly in\ndiffusion-based models. We introduce GenTron, a family of Generative models\nemploying Transformer-based diffusion, to address this gap. Our initial step\nwas to adapt Diffusion Transformers (DiTs) from class to text conditioning, a\nprocess involving thorough empirical exploration of the conditioning mechanism.\nWe then scale GenTron from approximately 900M to over 3B parameters, observing\nsignificant improvements in visual quality. Furthermore, we extend GenTron to\ntext-to-video generation, incorporating novel motion-free guidance to enhance\nvideo quality. In human evaluations against SDXL, GenTron achieves a 51.1% win\nrate in visual quality (with a 19.8% draw rate), and a 42.3% win rate in text\nalignment (with a 42.9% draw rate). GenTron also excels in the T2I-CompBench,\nunderscoring its strengths in compositional generation. We believe this work\nwill provide meaningful insights and serve as a valuable reference for future\nresearch.",
            "author": [
                "Shoufa Chen",
                "Mengmeng Xu",
                "Jiawei Ren",
                "Yuren Cong",
                "Sen He",
                "Yanping Xie",
                "Animesh Sinha",
                "Ping Luo",
                "Tao Xiang",
                "Juan-Manuel Perez-Rua"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04557v1",
                "http://arxiv.org/pdf/2312.04557v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04556v1",
            "title": "Large Language Models for Mathematicians",
            "updated": "2023-12-07T18:59:29Z",
            "published": "2023-12-07T18:59:29Z",
            "summary": "Large language models (LLMs) such as ChatGPT have received immense interest\nfor their general-purpose language understanding and, in particular, their\nability to generate high-quality text or computer code. For many professions,\nLLMs represent an invaluable tool that can speed up and improve the quality of\nwork. In this note, we discuss to what extent they can aid professional\nmathematicians. We first provide a mathematical description of the transformer\nmodel used in all modern language models. Based on recent studies, we then\noutline best practices and potential issues and report on the mathematical\nabilities of language models. Finally, we shed light on the potential of LMMs\nto change how mathematicians work.",
            "author": [
                "Simon Frieder",
                "Julius Berner",
                "Philipp Petersen",
                "Thomas Lukasiewicz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04556v1",
                "http://arxiv.org/pdf/2312.04556v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "math.HO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04554v1",
            "title": "Improved Visual Grounding through Self-Consistent Explanations",
            "updated": "2023-12-07T18:59:22Z",
            "published": "2023-12-07T18:59:22Z",
            "summary": "Vision-and-language models trained to match images with text can be combined\nwith visual explanation methods to point to the locations of specific objects\nin an image. Our work shows that the localization --\"grounding\"-- abilities of\nthese models can be further improved by finetuning for self-consistent visual\nexplanations. We propose a strategy for augmenting existing text-image datasets\nwith paraphrases using a large language model, and SelfEQ, a weakly-supervised\nstrategy on visual explanation maps for paraphrases that encourages\nself-consistency. Specifically, for an input textual phrase, we attempt to\ngenerate a paraphrase and finetune the model so that the phrase and paraphrase\nmap to the same region in the image. We posit that this both expands the\nvocabulary that the model is able to handle, and improves the quality of the\nobject locations highlighted by gradient-based visual explanation methods (e.g.\nGradCAM). We demonstrate that SelfEQ improves performance on Flickr30k,\nReferIt, and RefCOCO+ over a strong baseline method and several prior works.\nParticularly, comparing to other methods that do not use any type of box\nannotations, we obtain 84.07% on Flickr30k (an absolute improvement of 4.69%),\n67.40% on ReferIt (an absolute improvement of 7.68%), and 75.10%, 55.49% on\nRefCOCO+ test sets A and B respectively (an absolute improvement of 3.74% on\naverage).",
            "author": [
                "Ruozhen He",
                "Paola Cascante-Bonilla",
                "Ziyan Yang",
                "Alexander C. Berg",
                "Vicente Ordonez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04554v1",
                "http://arxiv.org/pdf/2312.04554v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04553v1",
            "title": "SPIDeRS: Structured Polarization for Invisible Depth and Reflectance\n  Sensing",
            "updated": "2023-12-07T18:59:21Z",
            "published": "2023-12-07T18:59:21Z",
            "summary": "Can we capture shape and reflectance in stealth? Such capability would be\nvaluable for many application domains in vision, xR, robotics, and HCI. We\nintroduce Structured Polarization, the first depth and reflectance sensing\nmethod using patterns of polarized light (SPIDeRS). The key idea is to modulate\nthe angle of linear polarization (AoLP) of projected light at each pixel. The\nuse of polarization makes it invisible and lets us recover not only depth but\nalso directly surface normals and even reflectance. We implement SPIDeRS with a\nliquid crystal spatial light modulator (SLM) and a polarimetric camera. We\nderive a novel method for robustly extracting the projected structured\npolarization pattern from the polarimetric object appearance. We evaluate the\neffectiveness of SPIDeRS by applying it to a number of real-world objects. The\nresults show that our method successfully reconstructs object shapes of various\nmaterials and is robust to diffuse reflection and ambient light. We also\ndemonstrate relighting using recovered surface normals and reflectance. We\nbelieve SPIDeRS opens a new avenue of polarization use in visual sensing.",
            "author": [
                "Tomoki Ichikawa",
                "Shohei Nobuhara",
                "Ko Nishino"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04553v1",
                "http://arxiv.org/pdf/2312.04553v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04552v1",
            "title": "Generating Illustrated Instructions",
            "updated": "2023-12-07T18:59:20Z",
            "published": "2023-12-07T18:59:20Z",
            "summary": "We introduce the new task of generating Illustrated Instructions, i.e.,\nvisual instructions customized to a user's needs. We identify desiderata unique\nto this task, and formalize it through a suite of automatic and human\nevaluation metrics, designed to measure the validity, consistency, and efficacy\nof the generations. We combine the power of large language models (LLMs)\ntogether with strong text-to-image generation diffusion models to propose a\nsimple approach called StackedDiffusion, which generates such illustrated\ninstructions given text as input. The resulting model strongly outperforms\nbaseline approaches and state-of-the-art multimodal LLMs; and in 30% of cases,\nusers even prefer it to human-generated articles. Most notably, it enables\nvarious new and exciting applications far beyond what static articles on the\nweb can provide, such as personalized instructions complete with intermediate\nsteps and pictures in response to a user's individual situation.",
            "author": [
                "Sachit Menon",
                "Ishan Misra",
                "Rohit Girdhar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04552v1",
                "http://arxiv.org/pdf/2312.04552v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04551v1",
            "title": "Free3D: Consistent Novel View Synthesis without 3D Representation",
            "updated": "2023-12-07T18:59:18Z",
            "published": "2023-12-07T18:59:18Z",
            "summary": "We introduce Free3D, a simple approach designed for open-set novel view\nsynthesis (NVS) from a single image. Similar to Zero-1-to-3, we start from a\npre-trained 2D image generator for generalization, and fine-tune it for NVS.\nCompared to recent and concurrent works, we obtain significant improvements\nwithout resorting to an explicit 3D representation, which is slow and\nmemory-consuming or training an additional 3D network. We do so by encoding\nbetter the target camera pose via a new per-pixel ray conditioning\nnormalization (RCN) layer. The latter injects pose information in the\nunderlying 2D image generator by telling each pixel its specific viewing\ndirection. We also improve multi-view consistency via a light-weight multi-view\nattention layer and multi-view noise sharing. We train Free3D on the Objaverse\ndataset and demonstrate excellent generalization to various new categories in\nseveral new datasets, including OminiObject3D and GSO. We hope our simple and\neffective approach will serve as a solid baseline and help future research in\nNVS with more accuracy pose. The project page is available at\nhttps://chuanxiaz.com/free3d/.",
            "author": [
                "Chuanxia Zheng",
                "Andrea Vedaldi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04551v1",
                "http://arxiv.org/pdf/2312.04551v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04548v1",
            "title": "Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve\n  Aerial Visual Perception?",
            "updated": "2023-12-07T18:59:14Z",
            "published": "2023-12-07T18:59:14Z",
            "summary": "Despite the commercial abundance of UAVs, aerial data acquisition remains\nchallenging, and the existing Asia and North America-centric open-source UAV\ndatasets are small-scale or low-resolution and lack diversity in scene\ncontextuality. Additionally, the color content of the scenes, solar-zenith\nangle, and population density of different geographies influence the data\ndiversity. These two factors conjointly render suboptimal aerial-visual\nperception of the deep neural network (DNN) models trained primarily on the\nground-view data, including the open-world foundational models.\n  To pave the way for a transformative era of aerial detection, we present\nMultiview Aerial Visual RECognition or MAVREC, a video dataset where we record\nsynchronized scenes from different perspectives -- ground camera and\ndrone-mounted camera. MAVREC consists of around 2.5 hours of industry-standard\n2.7K resolution video sequences, more than 0.5 million frames, and 1.1 million\nannotated bounding boxes. This makes MAVREC the largest ground and aerial-view\ndataset, and the fourth largest among all drone-based datasets across all\nmodalities and tasks. Through our extensive benchmarking on MAVREC, we\nrecognize that augmenting object detectors with ground-view images from the\ncorresponding geographical location is a superior pre-training strategy for\naerial detection. Building on this strategy, we benchmark MAVREC with a\ncurriculum-based semi-supervised object detection approach that leverages\nlabeled (ground and aerial) and unlabeled (only aerial) images to enhance the\naerial detection. We publicly release the MAVREC dataset:\nhttps://mavrec.github.io.",
            "author": [
                "Aritra Dutta",
                "Srijan Das",
                "Jacob Nielsen",
                "Rajatsubhra Chakraborty",
                "Mubarak Shah"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04548v1",
                "http://arxiv.org/pdf/2312.04548v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "I.4.0; I.4.8; I.5.1; I.5.4; I.2.10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04549v1",
            "title": "PlayFusion: Skill Acquisition via Diffusion from Language-Annotated Play",
            "updated": "2023-12-07T18:59:14Z",
            "published": "2023-12-07T18:59:14Z",
            "summary": "Learning from unstructured and uncurated data has become the dominant\nparadigm for generative approaches in language and vision. Such unstructured\nand unguided behavior data, commonly known as play, is also easier to collect\nin robotics but much more difficult to learn from due to its inherently\nmultimodal, noisy, and suboptimal nature. In this paper, we study this problem\nof learning goal-directed skill policies from unstructured play data which is\nlabeled with language in hindsight. Specifically, we leverage advances in\ndiffusion models to learn a multi-task diffusion model to extract robotic\nskills from play data. Using a conditional denoising diffusion process in the\nspace of states and actions, we can gracefully handle the complexity and\nmultimodality of play data and generate diverse and interesting robot\nbehaviors. To make diffusion models more useful for skill learning, we\nencourage robotic agents to acquire a vocabulary of skills by introducing\ndiscrete bottlenecks into the conditional behavior generation process. In our\nexperiments, we demonstrate the effectiveness of our approach across a wide\nvariety of environments in both simulation and the real world. Results\nvisualizations and videos at https://play-fusion.github.io",
            "author": [
                "Lili Chen",
                "Shikhar Bahl",
                "Deepak Pathak"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04549v1",
                "http://arxiv.org/pdf/2312.04549v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04547v1",
            "title": "Digital Life Project: Autonomous 3D Characters with Social Intelligence",
            "updated": "2023-12-07T18:58:59Z",
            "published": "2023-12-07T18:58:59Z",
            "summary": "In this work, we present Digital Life Project, a framework utilizing language\nas the universal medium to build autonomous 3D characters, who are capable of\nengaging in social interactions and expressing with articulated body motions,\nthereby simulating life in a digital environment. Our framework comprises two\nprimary components: 1) SocioMind: a meticulously crafted digital brain that\nmodels personalities with systematic few-shot exemplars, incorporates a\nreflection process based on psychology principles, and emulates autonomy by\ninitiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis\nparadigm for controlling the character's digital body. It integrates motion\nmatching, a proven industry technique to ensure motion quality, with\ncutting-edge advancements in motion generation for diversity. Extensive\nexperiments demonstrate that each module achieves state-of-the-art performance\nin its respective domain. Collectively, they enable virtual characters to\ninitiate and sustain dialogues autonomously, while evolving their\nsocio-psychological states. Concurrently, these characters can perform\ncontextually relevant bodily movements. Additionally, a motion captioning\nmodule further allows the virtual character to recognize and appropriately\nrespond to human players' actions. Homepage: https://digital-life-project.com/",
            "author": [
                "Zhongang Cai",
                "Jianping Jiang",
                "Zhongfei Qing",
                "Xinying Guo",
                "Mingyuan Zhang",
                "Zhengyu Lin",
                "Haiyi Mei",
                "Chen Wei",
                "Ruisi Wang",
                "Wanqi Yin",
                "Xiangyu Fan",
                "Han Du",
                "Liang Pan",
                "Peng Gao",
                "Zhitao Yang",
                "Yang Gao",
                "Jiaqi Li",
                "Tianxiang Ren",
                "Yukun Wei",
                "Xiaogang Wang",
                "Chen Change Loy",
                "Lei Yang",
                "Ziwei Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04547v1",
                "http://arxiv.org/pdf/2312.04547v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04543v1",
            "title": "HyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a\n  Single Image",
            "updated": "2023-12-07T18:58:09Z",
            "published": "2023-12-07T18:58:09Z",
            "summary": "3D content creation from a single image is a long-standing yet highly\ndesirable task. Recent advances introduce 2D diffusion priors, yielding\nreasonable results. However, existing methods are not hyper-realistic enough\nfor post-generation usage, as users cannot view, render and edit the resulting\n3D content from a full range. To address these challenges, we introduce\nHyperDreamer with several key designs and appealing properties: 1) Viewable:\n360 degree mesh modeling with high-resolution textures enables the creation of\nvisually compelling 3D models from a full range of observation points. 2)\nRenderable: Fine-grained semantic segmentation and data-driven priors are\nincorporated as guidance to learn reasonable albedo, roughness, and specular\nproperties of the materials, enabling semantic-aware arbitrary material\nestimation. 3) Editable: For a generated model or their own data, users can\ninteractively select any region via a few clicks and efficiently edit the\ntexture with text-based guidance. Extensive experiments demonstrate the\neffectiveness of HyperDreamer in modeling region-aware materials with\nhigh-resolution textures and enabling user-friendly editing. We believe that\nHyperDreamer holds promise for advancing 3D content creation and finding\napplications in various domains.",
            "author": [
                "Tong Wu",
                "Zhibing Li",
                "Shuai Yang",
                "Pan Zhang",
                "Xinggang Pan",
                "Jiaqi Wang",
                "Dahua Lin",
                "Ziwei Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04543v1",
                "http://arxiv.org/pdf/2312.04543v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04540v1",
            "title": "Sim-to-Real Causal Transfer: A Metric Learning Approach to\n  Causally-Aware Interaction Representations",
            "updated": "2023-12-07T18:57:03Z",
            "published": "2023-12-07T18:57:03Z",
            "summary": "Modeling spatial-temporal interactions among neighboring agents is at the\nheart of multi-agent problems such as motion forecasting and crowd navigation.\nDespite notable progress, it remains unclear to which extent modern\nrepresentations can capture the causal relationships behind agent interactions.\nIn this work, we take an in-depth look at the causal awareness of these\nrepresentations, from computational formalism to real-world practice. First, we\ncast doubt on the notion of non-causal robustness studied in the recent\nCausalAgents benchmark. We show that recent representations are already\npartially resilient to perturbations of non-causal agents, and yet modeling\nindirect causal effects involving mediator agents remains challenging. To\naddress this challenge, we introduce a metric learning approach that\nregularizes latent representations with causal annotations. Our controlled\nexperiments show that this approach not only leads to higher degrees of causal\nawareness but also yields stronger out-of-distribution robustness. To further\noperationalize it in practice, we propose a sim-to-real causal transfer method\nvia cross-domain multi-task learning. Experiments on pedestrian datasets show\nthat our method can substantially boost generalization, even in the absence of\nreal-world causal annotations. We hope our work provides a new perspective on\nthe challenges and potential pathways towards causally-aware representations of\nmulti-agent interactions. Our code is available at\nhttps://github.com/socialcausality.",
            "author": [
                "Yuejiang Liu",
                "Ahmad Rahimi",
                "Po-Chien Luan",
                "Frano Raji\u010d",
                "Alexandre Alahi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04540v1",
                "http://arxiv.org/pdf/2312.04540v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV",
                "cs.MA",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04539v1",
            "title": "Self-Guided Open-Vocabulary Semantic Segmentation",
            "updated": "2023-12-07T18:55:52Z",
            "published": "2023-12-07T18:55:52Z",
            "summary": "Vision-Language Models (VLMs) have emerged as promising tools for open-ended\nimage understanding tasks, including open vocabulary segmentation. Yet, direct\napplication of such VLMs to segmentation is non-trivial, since VLMs are trained\nwith image-text pairs and naturally lack pixel-level granularity. Recent works\nhave made advancements in bridging this gap, often by leveraging the shared\nimage-text space in which the image and a provided text prompt are represented.\nIn this paper, we challenge the capabilities of VLMs further and tackle\nopen-vocabulary segmentation without the need for any textual input. To this\nend, we propose a novel Self-Guided Semantic Segmentation (Self-Seg) framework.\nSelf-Seg is capable of automatically detecting relevant class names from\nclustered BLIP embeddings and using these for accurate semantic segmentation.\nIn addition, we propose an LLM-based Open-Vocabulary Evaluator (LOVE) to\neffectively assess predicted open-vocabulary class names. We achieve\nstate-of-the-art results on Pascal VOC, ADE20K and CityScapes for\nopen-vocabulary segmentation without given class names, as well as competitive\nperformance with methods where class names are given. All code and data will be\nreleased.",
            "author": [
                "Osman \u00dclger",
                "Maksymilian Kulicki",
                "Yuki Asano",
                "Martin R. Oswald"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04539v1",
                "http://arxiv.org/pdf/2312.04539v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04534v1",
            "title": "PICTURE: PhotorealistIC virtual Try-on from UnconstRained dEsigns",
            "updated": "2023-12-07T18:53:18Z",
            "published": "2023-12-07T18:53:18Z",
            "summary": "In this paper, we propose a novel virtual try-on from unconstrained designs\n(ucVTON) task to enable photorealistic synthesis of personalized composite\nclothing on input human images. Unlike prior arts constrained by specific input\ntypes, our method allows flexible specification of style (text or image) and\ntexture (full garment, cropped sections, or texture patches) conditions. To\naddress the entanglement challenge when using full garment images as\nconditions, we develop a two-stage pipeline with explicit disentanglement of\nstyle and texture. In the first stage, we generate a human parsing map\nreflecting the desired style conditioned on the input. In the second stage, we\ncomposite textures onto the parsing map areas based on the texture input. To\nrepresent complex and non-stationary textures that have never been achieved in\nprevious fashion editing works, we first propose extracting hierarchical and\nbalanced CLIP features and applying position encoding in VTON. Experiments\ndemonstrate superior synthesis quality and personalization enabled by our\nmethod. The flexible control over style and texture mixing brings virtual\ntry-on to a new level of user experience for online shopping and fashion\ndesign.",
            "author": [
                "Shuliang Ning",
                "Duomin Wang",
                "Yipeng Qin",
                "Zirong Jin",
                "Baoyuan Wang",
                "Xiaoguang Han"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04534v1",
                "http://arxiv.org/pdf/2312.04534v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04533v1",
            "title": "Dream2Real: Zero-Shot 3D Object Rearrangement with Vision-Language\n  Models",
            "updated": "2023-12-07T18:51:19Z",
            "published": "2023-12-07T18:51:19Z",
            "summary": "We introduce Dream2Real, a robotics framework which integrates\nvision-language models (VLMs) trained on 2D data into a 3D object rearrangement\npipeline. This is achieved by the robot autonomously constructing a 3D\nrepresentation of the scene, where objects can be rearranged virtually and an\nimage of the resulting arrangement rendered. These renders are evaluated by a\nVLM, so that the arrangement which best satisfies the user instruction is\nselected and recreated in the real world with pick-and-place. This enables\nlanguage-conditioned rearrangement to be performed zero-shot, without needing\nto collect a training dataset of example arrangements. Results on a series of\nreal-world tasks show that this framework is robust to distractors,\ncontrollable by language, capable of understanding complex multi-object\nrelations, and readily applicable to both tabletop and 6-DoF rearrangement\ntasks.",
            "author": [
                "Ivan Kapelyukh",
                "Yifei Ren",
                "Ignacio Alzugaray",
                "Edward Johns"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04533v1",
                "http://arxiv.org/pdf/2312.04533v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04530v1",
            "title": "Camera Height Doesn't Change: Unsupervised Monocular Scale-Aware\n  Road-Scene Depth Estimation",
            "updated": "2023-12-07T18:50:01Z",
            "published": "2023-12-07T18:50:01Z",
            "summary": "Monocular depth estimators either require explicit scale supervision through\nauxiliary sensors or suffer from scale ambiguity, which renders them difficult\nto deploy in downstream applications. A possible source of scale is the sizes\nof objects found in the scene, but inaccurate localization makes them difficult\nto exploit. In this paper, we introduce a novel scale-aware monocular depth\nestimation method called StableCamH that does not require any auxiliary sensor\nor supervision. The key idea is to exploit prior knowledge of object heights in\nthe scene but aggregate the height cues into a single invariant measure common\nto all frames in a road video sequence, namely the camera height. By\nformulating monocular depth estimation as camera height optimization, we\nachieve robust and accurate unsupervised end-to-end training. To realize\nStableCamH, we devise a novel learning-based size prior that can directly\nconvert car appearance into its dimensions. Extensive experiments on KITTI and\nCityscapes show the effectiveness of StableCamH, its state-of-the-art accuracy\ncompared with related methods, and its generalizability. The training framework\nof StableCamH can be used for any monocular depth estimation method and will\nhopefully become a fundamental building block for further work.",
            "author": [
                "Genki Kinoshita",
                "Ko Nishino"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04530v1",
                "http://arxiv.org/pdf/2312.04530v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04529v1",
            "title": "Diffusion Reflectance Map: Single-Image Stochastic Inverse Rendering of\n  Illumination and Reflectance",
            "updated": "2023-12-07T18:50:00Z",
            "published": "2023-12-07T18:50:00Z",
            "summary": "Reflectance bounds the frequency spectrum of illumination in the object\nappearance. In this paper, we introduce the first stochastic inverse rendering\nmethod, which recovers the full frequency spectrum of an illumination jointly\nwith the object reflectance from a single image. Our key idea is to solve this\nblind inverse problem in the reflectance map, an appearance representation\ninvariant to the underlying geometry, by learning to reverse the image\nformation with a novel diffusion model which we refer to as the Diffusion\nReflectance Map Network (DRMNet). Given an observed reflectance map converted\nand completed from the single input image, DRMNet generates a reflectance map\ncorresponding to a perfect mirror sphere while jointly estimating the\nreflectance. The forward process can be understood as gradually filtering a\nnatural illumination with lower and lower frequency reflectance and additive\nGaussian noise. DRMNet learns to invert this process with two subnetworks,\nIllNet and RefNet, which work in concert towards this joint estimation. The\nnetwork is trained on an extensive synthetic dataset and is demonstrated to\ngeneralize to real images, showing state-of-the-art accuracy on established\ndatasets.",
            "author": [
                "Yuto Enyo",
                "Ko Nishino"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04529v1",
                "http://arxiv.org/pdf/2312.04529v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04527v1",
            "title": "Correspondences of the Third Kind: Camera Pose Estimation from Object\n  Reflection",
            "updated": "2023-12-07T18:46:47Z",
            "published": "2023-12-07T18:46:47Z",
            "summary": "Computer vision has long relied on two kinds of correspondences: pixel\ncorrespondences in images and 3D correspondences on object surfaces. Is there\nanother kind, and if there is, what can they do for us? In this paper, we\nintroduce correspondences of the third kind we call reflection correspondences\nand show that they can help estimate camera pose by just looking at objects\nwithout relying on the background. Reflection correspondences are point\ncorrespondences in the reflected world, i.e., the scene reflected by the object\nsurface. The object geometry and reflectance alters the scene geometrically and\nradiometrically, respectively, causing incorrect pixel correspondences.\nGeometry recovered from each image is also hampered by distortions, namely\ngeneralized bas-relief ambiguity, leading to erroneous 3D correspondences. We\nshow that reflection correspondences can resolve the ambiguities arising from\nthese distortions. We introduce a neural correspondence estimator and a RANSAC\nalgorithm that fully leverages all three kinds of correspondences for robust\nand accurate joint camera pose and object shape estimation just from the object\nappearance. The method expands the horizon of numerous downstream tasks,\nincluding camera pose estimation for appearance modeling (e.g., NeRF) and\nmotion estimation of reflective objects (e.g., cars on the road), to name a\nfew, as it relieves the requirement of overlapping background.",
            "author": [
                "Kohei Yamashita",
                "Vincent Lepetit",
                "Ko Nishino"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04527v1",
                "http://arxiv.org/pdf/2312.04527v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04524v1",
            "title": "RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing\n  with Diffusion Models",
            "updated": "2023-12-07T18:43:45Z",
            "published": "2023-12-07T18:43:45Z",
            "summary": "Recent advancements in diffusion-based models have demonstrated significant\nsuccess in generating images from text. However, video editing models have not\nyet reached the same level of visual quality and user control. To address this,\nwe introduce RAVE, a zero-shot video editing method that leverages pre-trained\ntext-to-image diffusion models without additional training. RAVE takes an input\nvideo and a text prompt to produce high-quality videos while preserving the\noriginal motion and semantic structure. It employs a novel noise shuffling\nstrategy, leveraging spatio-temporal interactions between frames, to produce\ntemporally consistent videos faster than existing methods. It is also efficient\nin terms of memory requirements, allowing it to handle longer videos. RAVE is\ncapable of a wide range of edits, from local attribute modifications to shape\ntransformations. In order to demonstrate the versatility of RAVE, we create a\ncomprehensive video evaluation dataset ranging from object-focused scenes to\ncomplex human activities like dancing and typing, and dynamic scenes featuring\nswimming fish and boats. Our qualitative and quantitative experiments highlight\nthe effectiveness of RAVE in diverse video editing scenarios compared to\nexisting methods. Our code, dataset and videos can be found in\nhttps://rave-video.github.io.",
            "author": [
                "Ozgur Kara",
                "Bariscan Kurtkaya",
                "Hidir Yesiltepe",
                "James M. Rehg",
                "Pinar Yanardag"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04524v1",
                "http://arxiv.org/pdf/2312.04524v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04521v1",
            "title": "Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping",
            "updated": "2023-12-07T18:41:21Z",
            "published": "2023-12-07T18:41:21Z",
            "summary": "The paper explores the industrial multimodal Anomaly Detection (AD) task,\nwhich exploits point clouds and RGB images to localize anomalies. We introduce\na novel light and fast framework that learns to map features from one modality\nto the other on nominal samples. At test time, anomalies are detected by\npinpointing inconsistencies between observed and mapped features. Extensive\nexperiments show that our approach achieves state-of-the-art detection and\nsegmentation performance in both the standard and few-shot settings on the\nMVTec 3D-AD dataset while achieving faster inference and occupying less memory\nthan previous multimodal AD methods. Moreover, we propose a layer-pruning\ntechnique to improve memory and time efficiency with a marginal sacrifice in\nperformance.",
            "author": [
                "Alex Costanzino",
                "Pierluigi Zama Ramirez",
                "Giuseppe Lisanti",
                "Luigi Di Stefano"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04521v1",
                "http://arxiv.org/pdf/2312.04521v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04519v1",
            "title": "Bootstrapping Autonomous Radars with Self-Supervised Learning",
            "updated": "2023-12-07T18:38:39Z",
            "published": "2023-12-07T18:38:39Z",
            "summary": "The perception of autonomous vehicles using radars has attracted increased\nresearch interest due its ability to operate in fog and bad weather. However,\ntraining radar models is hindered by the cost and difficulty of annotating\nlarge-scale radar data. To overcome this bottleneck, we propose a\nself-supervised learning framework to leverage the large amount of unlabeled\nradar data to pre-train radar-only embeddings for self-driving perception\ntasks. The proposed method combines radar-to-radar and radar-to-vision\ncontrastive losses to learn a general representation from unlabeled radar\nheatmaps paired with their corresponding camera images. When used for\ndownstream object detection, we demonstrate that the proposed self-supervision\nframework can improve the accuracy of state-of-the-art supervised baselines by\n5.8% in mAP.",
            "author": [
                "Yiduo Hao",
                "Sohrab Madani",
                "Junfeng Guan",
                "Mohammed Alloulah",
                "Saurabh Gupta",
                "Haitham Hassanieh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04519v1",
                "http://arxiv.org/pdf/2312.04519v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04516v1",
            "title": "Computing Perfect Bayesian Equilibria in Sequential Auctions",
            "updated": "2023-12-07T18:36:53Z",
            "published": "2023-12-07T18:36:53Z",
            "summary": "We present a best-response based algorithm for computing verifiable\n$\\varepsilon$-perfect Bayesian equilibria for sequential auctions with\ncombinatorial bidding spaces and incomplete information. Previous work has\nfocused only on computing Bayes-Nash equilibria for static single-round\nauctions, which our work captures as a special case. Additionally, we prove an\nupper bound $\\varepsilon$ on the utility loss of our approximate equilibria and\npresent an algorithm to efficiently compute $\\varepsilon$ based on the\nimmediate loss at each subgame. We evaluate the performance of our algorithm by\nreproducing known results from several auctions previously introduced in the\nliterature, including a model of combinatorial split-award auctions used in\nprocurement.",
            "author": [
                "Vinzenz Thoma",
                "Vitor Bosshard",
                "Sven Seuken"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04516v1",
                "http://arxiv.org/pdf/2312.04516v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04515v1",
            "title": "Efficient Monotonic Multihead Attention",
            "updated": "2023-12-07T18:34:57Z",
            "published": "2023-12-07T18:34:57Z",
            "summary": "We introduce the Efficient Monotonic Multihead Attention (EMMA), a\nstate-of-the-art simultaneous translation model with numerically-stable and\nunbiased monotonic alignment estimation. In addition, we present improved\ntraining and inference strategies, including simultaneous fine-tuning from an\noffline translation model and reduction of monotonic alignment variance. The\nexperimental results demonstrate that the proposed model attains\nstate-of-the-art performance in simultaneous speech-to-text translation on the\nSpanish and English translation task.",
            "author": [
                "Xutai Ma",
                "Anna Sun",
                "Siqi Ouyang",
                "Hirofumi Inaguma",
                "Paden Tomasello"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04515v1",
                "http://arxiv.org/pdf/2312.04515v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04514v1",
            "title": "Channel Charting for Streaming CSI Data",
            "updated": "2023-12-07T18:34:25Z",
            "published": "2023-12-07T18:34:25Z",
            "summary": "Channel charting (CC) applies dimensionality reduction to channel state\ninformation (CSI) data at the infrastructure basestation side with the goal of\nextracting pseudo-position information for each user. The self-supervised\nnature of CC enables predictive tasks that depend on user position without\nrequiring any ground-truth position information. In this work, we focus on the\npractically relevant streaming CSI data scenario, in which CSI is constantly\nestimated. To deal with storage limitations, we develop a novel streaming CC\narchitecture that maintains a small core CSI dataset from which the channel\ncharts are learned. Curation of the core CSI dataset is achieved using a\nmin-max-similarity criterion. Numerical validation with measured CSI data\ndemonstrates that our method approaches the accuracy obtained from the complete\nCSI dataset while using only a fraction of CSI storage and avoiding\ncatastrophic forgetting of old CSI data.",
            "author": [
                "Sueda Taner",
                "Maxime Guillaud",
                "Olav Tirkkonen",
                "Christoph Studer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04514v1",
                "http://arxiv.org/pdf/2312.04514v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04513v1",
            "title": "On-shell approach to (spinning) gravitational absorption processes",
            "updated": "2023-12-07T18:32:54Z",
            "published": "2023-12-07T18:32:54Z",
            "summary": "We utilize three point amplitudes with (spinning) particles of unequal mass\nand a graviton to capture the dynamics of absorption processes. We demonstrate\nthat the construction can represent the spheroidal harmonics appearing in the\nTeukolsky equations. The absolute square of the ``Wilson coefficients'' in this\neffective description can be fixed by matching to the known absorptive\ncross-sections. As an application, we compute corrections to the gravitational\nCompton amplitude from the exchange of states corresponding to such absorption\neffects. In the super-extremal limit, the corrections generate the non-analytic\n$|a|$-dependent contribution of the Compton amplitude found in\nref.\\cite{Bautista:2022wjf}.",
            "author": [
                "Yu-Jui Chen",
                "Tien Hsieh",
                "Yu-Tin Huang",
                "Jung-Wook Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04513v1",
                "http://arxiv.org/pdf/2312.04513v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04511v1",
            "title": "An LLM Compiler for Parallel Function Calling",
            "updated": "2023-12-07T18:32:04Z",
            "published": "2023-12-07T18:32:04Z",
            "summary": "Large Language Models (LLMs) have shown remarkable results on various complex\nreasoning benchmarks. The reasoning capabilities of LLMs enable them to execute\nfunction calls, using user-provided functions to overcome their inherent\nlimitations, such as knowledge cutoffs, poor arithmetic skills, or lack of\naccess to private data. This development has expanded LLMs' scope to include\nmulti-function calling, where LLMs are equipped with a variety of functions and\nselect the proper functions based on the context. Multi-function calling\nabilities of LLMs have catalyzed LLM-based software development, allowing them\nto tackle more complex problems. However, current methods for multi-function\ncalling often require sequential reasoning and acting for each function which\ncan result in high latency, cost, and sometimes inaccurate behavior. To address\nthis, we introduce LLMCompiler, which executes functions in parallel to\nefficiently orchestrate multi-function calling. Drawing from the principles of\nclassical compilers, LLMCompiler streamlines parallel function calling with\nthree components: (i) an LLM Planner, formulating execution strategies and\ndependencies; (ii) a Task Fetching Unit, dispatching function calling tasks;\nand (iii) an Executor, executing these tasks in parallel. LLMCompiler\nautomatically computes an optimized orchestration for the function calls and\ncan be used with open-source models such as LLaMA-2. We have benchmarked\nLLMCompiler on a range of tasks including cases with non-trivial\ninter-dependency between function calls, as well as cases that require dynamic\nreplanning based on intermediate results. We observe consistent latency speedup\nof up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to\n~9% as compared to ReAct. Additionally, LLMCompiler achieves up to 1.35x\nlatency gain over OpenAI's recent parallel function calling, while achieving\nsimilar accuracy.",
            "author": [
                "Sehoon Kim",
                "Suhong Moon",
                "Ryan Tabrizi",
                "Nicholas Lee",
                "Michael W. Mahoney",
                "Kurt Keutzer",
                "Amir Gholami"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04511v1",
                "http://arxiv.org/pdf/2312.04511v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04510v1",
            "title": "A Block Metropolis-Hastings Sampler for Controllable Energy-based Text\n  Generation",
            "updated": "2023-12-07T18:30:15Z",
            "published": "2023-12-07T18:30:15Z",
            "summary": "Recent work has shown that energy-based language modeling is an effective\nframework for controllable text generation because it enables flexible\nintegration of arbitrary discriminators. However, because energy-based LMs are\nglobally normalized, approximate techniques like Metropolis-Hastings (MH) are\nrequired for inference. Past work has largely explored simple proposal\ndistributions that modify a single token at a time, like in Gibbs sampling. In\nthis paper, we develop a novel MH sampler that, in contrast, proposes re-writes\nof the entire sequence in each step via iterative prompting of a large language\nmodel. Our new sampler (a) allows for more efficient and accurate sampling from\na target distribution and (b) allows generation length to be determined through\nthe sampling procedure rather than fixed in advance, as past work has required.\nWe perform experiments on two controlled generation tasks, showing both\ndownstream performance gains and more accurate target distribution sampling in\ncomparison with single-token proposal techniques.",
            "author": [
                "Jarad Forristal",
                "Niloofar Mireshghallah",
                "Greg Durrett",
                "Taylor Berg-Kirkpatrick"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04510v1",
                "http://arxiv.org/pdf/2312.04510v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04505v1",
            "title": "On the change of epsilon factors for symmetric square transfers under\n  twisting and applications",
            "updated": "2023-12-07T18:26:15Z",
            "published": "2023-12-07T18:26:15Z",
            "summary": "Let us consider the symmetric square transfer of the automorphic\nrepresentation $\\pi$ associated to a modular form $f \\in S_k(N, \\epsilon)$. In\nthis article, we study the variation of the epsilon factor of ${\\rm\nsym}^2(\\pi)$ under twisting in terms of the local Weil-Deligne representation\nat each prime $p$. As an application, we detect the possible types of the\nsymmetric square transfer of the local representation at $p$. Furthermore, as\nthe conductor of ${\\rm sym}^2(\\pi)$ is involved in the variation number, we\ncompute it in terms of $N$.",
            "author": [
                "Tathagata Mandal",
                "Sudipa Mondal"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04505v1",
                "http://arxiv.org/pdf/2312.04505v1"
            ],
            "primary_category": "math.NT",
            "category": [
                "math.NT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04504v1",
            "title": "Coordination-free Decentralised Federated Learning on Complex Networks:\n  Overcoming Heterogeneity",
            "updated": "2023-12-07T18:24:19Z",
            "published": "2023-12-07T18:24:19Z",
            "summary": "Federated Learning (FL) is a well-known framework for successfully performing\na learning task in an edge computing scenario where the devices involved have\nlimited resources and incomplete data representation. The basic assumption of\nFL is that the devices communicate directly or indirectly with a parameter\nserver that centrally coordinates the whole process, overcoming several\nchallenges associated with it. However, in highly pervasive edge scenarios, the\npresence of a central controller that oversees the process cannot always be\nguaranteed, and the interactions (i.e., the connectivity graph) between devices\nmight not be predetermined, resulting in a complex network structure. Moreover,\nthe heterogeneity of data and devices further complicates the learning process.\nThis poses new challenges from a learning standpoint that we address by\nproposing a communication-efficient Decentralised Federated Learning (DFL)\nalgorithm able to cope with them. Our solution allows devices communicating\nonly with their direct neighbours to train an accurate model, overcoming the\nheterogeneity induced by data and different training histories. Our results\nshow that the resulting local models generalise better than those trained with\ncompeting approaches, and do so in a more communication-efficient way.",
            "author": [
                "Lorenzo Valerio",
                "Chiara Boldrini",
                "Andrea Passarella",
                "J\u00e1nos Kert\u00e9sz",
                "M\u00e1rton Karsai",
                "Gerardo I\u00f1iguez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04504v1",
                "http://arxiv.org/pdf/2312.04504v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DC",
                "cs.MA",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04503v1",
            "title": "Data-Driven Robust Reinforcement Learning Control of Uncertain Nonlinear\n  Systems: Towards a Fully-Automated, Insulin-Based Artificial Pancreas",
            "updated": "2023-12-07T18:24:03Z",
            "published": "2023-12-07T18:24:03Z",
            "summary": "In this paper, a novel robust tracking control scheme for a general class of\ndiscrete-time nonlinear systems affected by unknown bounded uncertainty is\npresented. By solving a parameterized optimal tracking control problem subject\nto the unknown nominal system and a suitable cost function, the resulting\noptimal tracking control policy can ensure closed-loop stability by achieving a\nsufficiently small tracking error for the original uncertain nonlinear system.\nThe computation of the optimal tracking controller is accomplished through the\nderivation of a novel Q-function-based $\\lambda$-Policy Iteration algorithm.\nThe proposed algorithm not only enjoys rigorous theoretical guarantees, but\nalso avoids technical weaknesses of conventional reinforcement learning\nmethods. By employing a data-driven, critic-only least squares implementation,\nthe performance of the proposed algorithm is evaluated to the problem of\nfully-automated, insulin-based, closed-loop glucose control for patients\ndiagnosed with Type 1 and Type 2 Diabetes Mellitus. The U.S. FDA-accepted\nDMMS.R simulator from the Epsilon Group is used to conduct a comprehensive in\nsilico clinical campaign on a rich set of virtual subjects under completely\nunannounced meal and exercise settings. Simulation results underline the\nsuperior glycaemic behavior achieved by the derived approach, as well as its\noverall maturity for the design of highly-effective, closed-loop drug delivery\nsystems for personalized medicine.",
            "author": [
                "Alexandros Tanzanakis",
                "John Lygeros"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04503v1",
                "http://arxiv.org/pdf/2312.04503v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04497v1",
            "title": "Automatic Calculation of the Transition Temperatures for two-dimensional\n  Heisenberg type Magnets",
            "updated": "2023-12-07T18:17:42Z",
            "published": "2023-12-07T18:17:42Z",
            "summary": "Theoretical prediction of the 2nd-order magnetic transition temperature (TM)\nused to be arduous. Here, we develop a first principle-based, fully automatic\nstructure-to-TM method for two-dimensional (2D) magnets whose effective\nHamiltonians follow the Heisenberg model. The Heisenberg exchanges, which can\nbe calculated to an arbitrary shell, are transferred into the Monte Carlo\ncalculation. Using Cr-based magnets as the showcases, we show that our method\nis a powerful tool to study the 2D magnets in two aspects. First, considering\nlong-range exchanges enables us to identify the spin frustration in the\nsuspended CrTe2 monolayer, whereas the heterostructure calculations reveal that\nthe ferromagnetism can be recovered if the monolayer CrTe2 is grown onto\nvarious 2D substrates. Second, we realize a high-throughput screening of novel\nmagnets discovered by random structure searches. Six 2D Cr chalcogenides are\nselected to have high TM. Our work provides a new insight for the study of 2D\nmagnets and helps accelerate the pace of magnetic materials data-mining.",
            "author": [
                "Haichang Lu",
                "Tai Yang",
                "Zhimei Sun",
                "John Robertson",
                "Weisheng Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04497v1",
                "http://arxiv.org/pdf/2312.04497v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04494v1",
            "title": "AVA: Towards Autonomous Visualization Agents through Visual\n  Perception-Driven Decision-Making",
            "updated": "2023-12-07T18:13:42Z",
            "published": "2023-12-07T18:13:42Z",
            "summary": "With recent advances in multi-modal foundation models, the previously\ntext-only large language models (LLM) have evolved to incorporate visual input,\nopening up unprecedented opportunities for various applications in\nvisualization. Our work explores the utilization of the visual perception\nability of multi-modal LLMs to develop Autonomous Visualization Agents (AVAs)\nthat can interpret and accomplish user-defined visualization objectives through\nnatural language. We propose the first framework for the design of AVAs and\npresent several usage scenarios intended to demonstrate the general\napplicability of the proposed paradigm. The addition of visual perception\nallows AVAs to act as the virtual visualization assistant for domain experts\nwho may lack the knowledge or expertise in fine-tuning visualization outputs.\nOur preliminary exploration and proof-of-concept agents suggest that this\napproach can be widely applicable whenever the choices of appropriate\nvisualization parameters require the interpretation of previous visual output.\nFeedback from unstructured interviews with experts in AI research, medical\nvisualization, and radiology has been incorporated, highlighting the\npracticality and potential of AVAs. Our study indicates that AVAs represent a\ngeneral paradigm for designing intelligent visualization systems that can\nachieve high-level visualization goals, which pave the way for developing\nexpert-level visualization agents in the future.",
            "author": [
                "Shusen Liu",
                "Haichao Miao",
                "Zhimin Li",
                "Matthew Olson",
                "Valerio Pascucci",
                "Peer-Timo Bremer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04494v1",
                "http://arxiv.org/pdf/2312.04494v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI",
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04487v1",
            "title": "On The Maximum Linear Arrangement Problem for Trees",
            "updated": "2023-12-07T18:02:48Z",
            "published": "2023-12-07T18:02:48Z",
            "summary": "Linear arrangements of graphs are a well-known type of graph labeling and are\nfound at the heart of many important computational problems, such as the\nMinimum Linear Arrangement Problem (minLA). A linear arrangement is usually\ndefined as a permutation of the $n$ vertices of a graph. An intuitive geometric\nsetting is that of vertices lying on consecutive integer positions in the real\nline, starting at 1; edges are typically drawn as semicircles above the real\nline. In this paper we study the Maximum Linear Arrangement problem (MaxLA),\nthe maximization variant of minLA and a less studied problem than minLA. We a\ndevise new characterization of maximum arrangements of general graphs, and\nprove that MaxLA can be solved for cycle graphs in constant time, and for\n$k$-linear trees ($k\\le2$) in time $O(n)$. We present a simple algorithm that\nsolves a constrained variant of MaxLA, which we call bipartite MaxLA, in time\n$O(n)$. This algorithm has two promising characteristics. First, it solves\nMaxLA for most trees consisting of a few tenths of nodes. Second, it produces a\nhigh quality approximation to MaxLA for trees where the algorithm fails to\nsolve MaxLA. Furthermore, we conjecture this algorithm solves MaxLA for at\nleast $50\\%$ of all free trees.",
            "author": [
                "Llu\u00eds Alemany-Puig",
                "Juan Luis Esteban",
                "Ramon Ferrer-i-Cancho"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04487v1",
                "http://arxiv.org/pdf/2312.04487v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DM",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04485v1",
            "title": "Relativistic quantum Otto engine: Instant work extraction from a quantum\n  field",
            "updated": "2023-12-07T18:00:01Z",
            "published": "2023-12-07T18:00:01Z",
            "summary": "In this study, we carry out a non-perturbative approach to a quantum Otto\nengine, employing an Unruh-DeWitt particle detector to extract work from a\nquantum Klein-Gordon field in an arbitrary globally hyperbolic curved\nspacetime. We broaden the scope by considering the field in any quasi-free\nstate, which includes vacuum, thermal, and squeezed states. A key aspect of our\nmethod is the instantaneous interaction between the detector and the field,\nwhich enables a thorough non-perturbative analysis. We demonstrate that the\ndetector can successfully extract positive work from the quantum Otto cycle,\neven when two isochoric processes occur instantaneously, provided the detector\nin the second isochoric process receives a signal from the first interaction.\nThis signaling allows the detector to release heat into the field, thereby the\nthermodynamic cycle is completed. As a demonstration, we consider a detector at\nrest in flat spacetime and compute the work extracted from the Minkowski vacuum\nstate.",
            "author": [
                "Kensuke Gallock-Yoshimura"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04485v1",
                "http://arxiv.org/pdf/2312.04485v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "gr-qc",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04484v1",
            "title": "FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation",
            "updated": "2023-12-07T17:59:53Z",
            "published": "2023-12-07T17:59:53Z",
            "summary": "LiDAR segmentation is crucial for autonomous driving systems. The recent\nrange-view approaches are promising for real-time processing. However, they\nsuffer inevitably from corrupted contextual information and rely heavily on\npost-processing techniques for prediction refinement. In this work, we propose\na simple yet powerful FRNet that restores the contextual information of the\nrange image pixels with corresponding frustum LiDAR points. Firstly, a frustum\nfeature encoder module is used to extract per-point features within the frustum\nregion, which preserves scene consistency and is crucial for point-level\npredictions. Next, a frustum-point fusion module is introduced to update\nper-point features hierarchically, which enables each point to extract more\nsurrounding information via the frustum features. Finally, a head fusion module\nis used to fuse features at different levels for final semantic prediction.\nExtensive experiments on four popular LiDAR segmentation benchmarks under\nvarious task setups demonstrate our superiority. FRNet achieves competitive\nperformance while maintaining high efficiency. The code is publicly available.",
            "author": [
                "Xiang Xu",
                "Lingdong Kong",
                "Hui Shuai",
                "Qingshan Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04484v1",
                "http://arxiv.org/pdf/2312.04484v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04483v1",
            "title": "Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation",
            "updated": "2023-12-07T17:59:07Z",
            "published": "2023-12-07T17:59:07Z",
            "summary": "Despite diffusion models having shown powerful abilities to generate\nphotorealistic images, generating videos that are realistic and diverse still\nremains in its infancy. One of the key reasons is that current methods\nintertwine spatial content and temporal dynamics together, leading to a notably\nincreased complexity of text-to-video generation (T2V). In this work, we\npropose HiGen, a diffusion model-based method that improves performance by\ndecoupling the spatial and temporal factors of videos from two perspectives,\ni.e., structure level and content level. At the structure level, we decompose\nthe T2V task into two steps, including spatial reasoning and temporal\nreasoning, using a unified denoiser. Specifically, we generate spatially\ncoherent priors using text during spatial reasoning and then generate\ntemporally coherent motions from these priors during temporal reasoning. At the\ncontent level, we extract two subtle cues from the content of the input video\nthat can express motion and appearance changes, respectively. These two cues\nthen guide the model's training for generating videos, enabling flexible\ncontent variations and enhancing temporal stability. Through the decoupled\nparadigm, HiGen can effectively reduce the complexity of this task and generate\nrealistic videos with semantics accuracy and motion stability. Extensive\nexperiments demonstrate the superior performance of HiGen over the\nstate-of-the-art T2V methods.",
            "author": [
                "Zhiwu Qing",
                "Shiwei Zhang",
                "Jiayu Wang",
                "Xiang Wang",
                "Yujie Wei",
                "Yingya Zhang",
                "Changxin Gao",
                "Nong Sang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04483v1",
                "http://arxiv.org/pdf/2312.04483v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04482v1",
            "title": "False vacuum decay rates, more precisely",
            "updated": "2023-12-07T17:57:46Z",
            "published": "2023-12-07T17:57:46Z",
            "summary": "We develop a method for accurately calculating vacuum decay rates beyond the\nthin-wall regime in a pure scalar field theory at the one-loop level of the\neffective action. It accounts for radiative effects resulting from quantum\ncorrections to the classical bounce, including gradient effects stemming from\nthe inhomogeneity of the bounce background. To achieve this, it is necessary to\ncompute not only the functional determinant of the fluctuation operator in the\nbackground of the classical bounce but also its functional derivative evaluated\nat the classical bounce. The former is efficiently calculated using the\nGel'fand-Yaglom method. We illustrate how the latter can also be calculated\nwith the same method, combined with a computation of various Green's functions.",
            "author": [
                "Wen-Yuan Ai",
                "Jean Alexandre",
                "Sarben Sarkar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04482v1",
                "http://arxiv.org/pdf/2312.04482v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "astro-ph.CO",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04479v1",
            "title": "GSGFormer: Generative Social Graph Transformer for Multimodal Pedestrian\n  Trajectory Prediction",
            "updated": "2023-12-07T17:53:02Z",
            "published": "2023-12-07T17:53:02Z",
            "summary": "Pedestrian trajectory prediction, vital for selfdriving cars and\nsocially-aware robots, is complicated due to intricate interactions between\npedestrians, their environment, and other Vulnerable Road Users. This paper\npresents GSGFormer, an innovative generative model adept at predicting\npedestrian trajectories by considering these complex interactions and offering\na plethora of potential modal behaviors. We incorporate a heterogeneous graph\nneural network to capture interactions between pedestrians, semantic maps, and\npotential destinations. The Transformer module extracts temporal features,\nwhile our novel CVAE-Residual-GMM module promotes diverse behavioral modality\ngeneration. Through evaluations on multiple public datasets, GSGFormer not only\noutperforms leading methods with ample data but also remains competitive when\ndata is limited.",
            "author": [
                "Zhongchang Luo",
                "Marion Robin",
                "Pavan Vasishta"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04479v1",
                "http://arxiv.org/pdf/2312.04479v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04476v1",
            "title": "Electromagnetic radiation from ultrafast-light-driven spintronic THz\n  emitters: A time-dependent density functional theory plus Jefimenko equations\n  approach",
            "updated": "2023-12-07T17:51:54Z",
            "published": "2023-12-07T17:51:54Z",
            "summary": "Microscopic origins of charge currents and electromagnetic (EM) radiation\ngenerated by them in spintronic THz emitters -- such as, femtosecond laser\npulse-driven single magnetic layer or its heterostructures with a nonmagnetic\nlayer hosting strong spin-orbit coupling (SOC) -- remain poorly understood\ndespite nearly three decades since the discovery of ultrafast demagnetization.\nWe introduce a first-principles method to compute these quantities, where the\ndynamics of charge and current densities is obtained from real-time\ntime-dependent density functional theory (TDDFT), which are then fed into the\nJefimenko equations for properly retarded electric and magnetic field solutions\nof the Maxwell equations. By Fourier transforming different time-dependent\nterms in the Jefimenko equations, we unravel that in 0.1--30 THz range the\nelectric field of far-field EM radiation by Ni layer, chosen as an example, is\ndominated by charge current pumped by demagnetization, while often invoked\nmagnetic dipole radiation from time-dependent magnetization of a single\nmagnetic layer is a negligible effect. Such overlooked case of charge current\npumping by time-dependent quantum system, whose magnetization is shrinking\nwhile its vector does not rotate, does not require any spin-to-charge\nconversion via SOC effects. In Ni/Pt bilayer, EM radiation remains dominated by\ncharge current within Ni layer, whose magnitude is larger than in the case of\nsingle Ni layer due to faster demagnetization, while often invoked\nspin-to-charge conversion within Pt layer provides additional but smaller\ncontribution. By using Poynting vector and its flux, we also quantify\nefficiency of conversion of light into emitted EM radiation, as well as the\nangular dependence of the latter.",
            "author": [
                "Ali Kefayati",
                "Branislav K. Nikolic"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04476v1",
                "http://arxiv.org/pdf/2312.04476v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "cond-mat.mtrl-sci",
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04474v1",
            "title": "Chain of Code: Reasoning with a Language Model-Augmented Code Emulator",
            "updated": "2023-12-07T17:51:43Z",
            "published": "2023-12-07T17:51:43Z",
            "summary": "Code provides a general syntactic structure to build complex programs and\nperform precise computations when paired with a code interpreter -- we\nhypothesize that language models (LMs) can leverage code-writing to improve\nChain of Thought reasoning not only for logic and arithmetic tasks, but also\nfor linguistic ones (and in particular, those that are a mix of both). For\nexample, consider prompting an LM to write code that counts the number of times\nit detects sarcasm in an essay: the LM may struggle to write an implementation\nfor \"detect_sarcasm(string)\" that can be executed by the interpreter (handling\nthe edge cases would be insurmountable). However, LMs may still produce a valid\nsolution if they are used not only to write the code, but also to selectively\n\"emulate\" the interpreter by generating the expected output of\n\"detect_sarcasm(string)\" and other lines of code (e.g., that the interpreter\ncould not compile). In this work, we propose Chain of Code (CoT), a simple yet\nsurprisingly effective extension that improves LM code-driven reasoning. The\nkey idea is to encourage LMs to format linguistic sub-tasks in a program as\nflexible pseudocode that the compiler can explicitly catch undefined behaviors\nand hand off to simulate with an LM (as an \"LMulator\"). Experiments demonstrate\nthat Chain of Code outperforms Chain of Thought and other baselines across a\nvariety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of\n12% over Chain of Thought. CoT scales well with large and small models alike,\nand broadens the scope of reasoning questions that LMs can correctly answer by\n\"thinking in code\". Project webpage: https://chain-of-code.github.io/.",
            "author": [
                "Chengshu Li",
                "Jacky Liang",
                "Andy Zeng",
                "Xinyun Chen",
                "Karol Hausman",
                "Dorsa Sadigh",
                "Sergey Levine",
                "Li Fei-Fei",
                "Fei Xia",
                "Brian Ichter"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04474v1",
                "http://arxiv.org/pdf/2312.04474v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04469v1",
            "title": "On the Learnability of Watermarks for Language Models",
            "updated": "2023-12-07T17:41:44Z",
            "published": "2023-12-07T17:41:44Z",
            "summary": "Watermarking of language model outputs enables statistical detection of\nmodel-generated text, which has many applications in the responsible deployment\nof language models. Existing watermarking strategies operate by altering the\ndecoder of an existing language model, and the ability for a language model to\ndirectly learn to generate the watermark would have significant implications\nfor the real-world deployment of watermarks. First, learned watermarks could be\nused to build open models that naturally generate watermarked text, allowing\nfor open models to benefit from watermarking. Second, if watermarking is used\nto determine the provenance of generated text, an adversary can hurt the\nreputation of a victim model by spoofing its watermark and generating damaging\nwatermarked text. To investigate the learnability of watermarks, we propose\nwatermark distillation, which trains a student model to behave like a teacher\nmodel that uses decoding-based watermarking. We test our approach on three\ndistinct decoding-based watermarking strategies and various hyperparameter\nsettings, finding that models can learn to generate watermarked text with high\ndetectability. We also find limitations to learnability, including the loss of\nwatermarking capabilities under fine-tuning on normal text and high sample\ncomplexity when learning low-distortion watermarks.",
            "author": [
                "Chenchen Gu",
                "Xiang Lisa Li",
                "Percy Liang",
                "Tatsunori Hashimoto"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04469v1",
                "http://arxiv.org/pdf/2312.04469v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04466v1",
            "title": "Emotional Speech-driven 3D Body Animation via Disentangled Latent\n  Diffusion",
            "updated": "2023-12-07T17:39:25Z",
            "published": "2023-12-07T17:39:25Z",
            "summary": "Existing methods for synthesizing 3D human gestures from speech have shown\npromising results, but they do not explicitly model the impact of emotions on\nthe generated gestures. Instead, these methods directly output animations from\nspeech without control over the expressed emotion. To address this limitation,\nwe present AMUSE, an emotional speech-driven body animation model based on\nlatent diffusion. Our observation is that content (i.e., gestures related to\nspeech rhythm and word utterances), emotion, and personal style are separable.\nTo account for this, AMUSE maps the driving audio to three disentangled latent\nvectors: one for content, one for emotion, and one for personal style. A latent\ndiffusion model, trained to generate gesture motion sequences, is then\nconditioned on these latent vectors. Once trained, AMUSE synthesizes 3D human\ngestures directly from speech with control over the expressed emotions and\nstyle by combining the content from the driving speech with the emotion and\nstyle of another speech sequence. Randomly sampling the noise of the diffusion\nmodel further generates variations of the gesture with the same emotional\nexpressivity. Qualitative, quantitative, and perceptual evaluations demonstrate\nthat AMUSE outputs realistic gesture sequences. Compared to the state of the\nart, the generated gestures are better synchronized with the speech content and\nbetter represent the emotion expressed by the input speech. Our project website\nis amuse.is.tue.mpg.de.",
            "author": [
                "Kiran Chhatre",
                "Radek Dan\u011b\u010dek",
                "Nikos Athanasiou",
                "Giorgio Becherini",
                "Christopher Peters",
                "Michael J. Black",
                "Timo Bolkart"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04466v1",
                "http://arxiv.org/pdf/2312.04466v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04465v1",
            "title": "FitDiff: Robust monocular 3D facial shape and reflectance estimation\n  using Diffusion Models",
            "updated": "2023-12-07T17:35:49Z",
            "published": "2023-12-07T17:35:49Z",
            "summary": "The remarkable progress in 3D face reconstruction has resulted in high-detail\nand photorealistic facial representations. Recently, Diffusion Models have\nrevolutionized the capabilities of generative methods by achieving far better\nperformance than GANs. In this work, we present FitDiff, a diffusion-based 3D\nfacial avatar generative model. This model accurately generates relightable\nfacial avatars, utilizing an identity embedding extracted from an \"in-the-wild\"\n2D facial image. Our multi-modal diffusion model concurrently outputs facial\nreflectance maps (diffuse and specular albedo and normals) and shapes,\nshowcasing great generalization capabilities. It is solely trained on an\nannotated subset of a public facial dataset, paired with 3D reconstructions. We\nrevisit the typical 3D facial fitting approach by guiding a reverse diffusion\nprocess using perceptual and face recognition losses. Being the first LDM\nconditioned on face recognition embeddings, FitDiff reconstructs relightable\nhuman avatars, that can be used as-is in common rendering engines, starting\nonly from an unconstrained facial image, and achieving state-of-the-art\nperformance.",
            "author": [
                "Stathis Galanakis",
                "Alexandros Lattas",
                "Stylianos Moschoglou",
                "Stefanos Zafeiriou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04465v1",
                "http://arxiv.org/pdf/2312.04465v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04461v1",
            "title": "PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding",
            "updated": "2023-12-07T17:32:29Z",
            "published": "2023-12-07T17:32:29Z",
            "summary": "Recent advances in text-to-image generation have made remarkable progress in\nsynthesizing realistic human photos conditioned on given text prompts. However,\nexisting personalized generation methods cannot simultaneously satisfy the\nrequirements of high efficiency, promising identity (ID) fidelity, and flexible\ntext controllability. In this work, we introduce PhotoMaker, an efficient\npersonalized text-to-image generation method, which mainly encodes an arbitrary\nnumber of input ID images into a stack ID embedding for preserving ID\ninformation. Such an embedding, serving as a unified ID representation, can not\nonly encapsulate the characteristics of the same input ID comprehensively, but\nalso accommodate the characteristics of different IDs for subsequent\nintegration. This paves the way for more intriguing and practically valuable\napplications. Besides, to drive the training of our PhotoMaker, we propose an\nID-oriented data construction pipeline to assemble the training data. Under the\nnourishment of the dataset constructed through the proposed pipeline, our\nPhotoMaker demonstrates better ID preservation ability than test-time\nfine-tuning based methods, yet provides significant speed improvements,\nhigh-quality generation results, strong generalization capabilities, and a wide\nrange of applications. Our project page is available at\nhttps://photo-maker.github.io/",
            "author": [
                "Zhen Li",
                "Mingdeng Cao",
                "Xintao Wang",
                "Zhongang Qi",
                "Ming-Ming Cheng",
                "Ying Shan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04461v1",
                "http://arxiv.org/pdf/2312.04461v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04459v1",
            "title": "On the Role of Charge Transfer Excitations in Non-Fullerene Acceptors\n  for Organic Photovoltaics",
            "updated": "2023-12-07T17:29:21Z",
            "published": "2023-12-07T17:29:21Z",
            "summary": "Through the development of new non-fullerene electron acceptor (NFA)\nmaterials, such as Y6 and its molecular derivatives, the power conversion\nefficiencies of organic photovoltaics (OPVs) have now exceeded 19%. However,\ndespite this rapid progress, our fundamental understanding of the unique\noptical and electronic properties of these Y-series NFAs is lacking, and this\ncurrently limits progress in material design. In this work, we provide a\ndetailed computational-experimental characterisation of the archetypal NFA, Y6.\nTo explain the significant broadening and red shift of the absorption spectrum\nobserved when moving from the solution phase to the solid state, we first rule\nout more typical causes, such as J-aggregation. Instead, by considering the\nrole of charge transfer (CT) excitations and their mixing with Frenkel exciton\n(FE) states, we can computationally reproduce the experimental absorption\nspectra of Y6 with excellent accuracy. Using transient absorption spectroscopy,\nwe provide evidence for this dense manifold of FE-CT hybrid electronic\nexcitations in Y6 through the prominent sub-picosecond relaxation events\nfollowing supra band gap excitation. Furthermore, through sub band gap\nexcitation, we also find states with polaronic character in Y6 that are in a\ndynamic equilibrium with the FE-CT hybrid states. Magnetic resonance\nspectroscopies reveal that these polaronic states are polaron pairs, most\nlikely located on neighbouring Y6 molecules, not free charge carriers, as has\nbeen previously suggested. Thus, this new understanding of how the solid-state\npacking motif directly controls the optical and electronic properties of\nY-series NFAs opens the door to intelligently design NFA materials to further\nincrease OPV performance.",
            "author": [
                "Samuele Giannini",
                "Daniel J. C. Sowood",
                "Jesus Cerda",
                "Siebe Frederix",
                "Jeannine Grune",
                "Giacomo Londi",
                "Thomas Marsh",
                "Pratyush Ghosh",
                "Ivan Duchemin",
                "Neil C. Greenham",
                "Koen Vandewal",
                "Gabriele D'Avino",
                "Alexander J. Gillett",
                "David Beljonne"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04459v1",
                "http://arxiv.org/pdf/2312.04459v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04458v1",
            "title": "Generalized Alchemical Integral Transform and the multi-electron atom\n  energy",
            "updated": "2023-12-07T17:27:24Z",
            "published": "2023-12-07T17:27:24Z",
            "summary": "Within computational quantum alchemy, observables of one system $B$, such as\nthe energy $E_B$, can be obtained by perturbing the external potential of a\ndifferent iso-electronic system $A$, simply by Taylor expansion of $E_A$ in an\nalchemical change $\\lambda$. We recently introduced the Alchemical Integral\nTransform (AIT) enabling the effective prediction of $E_B$ but requiring us to\nparametrize space $\\pmb{r}$ in terms of $\\lambda$ for which we used an Ansatz\nfound by trial and error. The kernel $\\mathcal{K}$ of AIT also required\npre-computed solutions of certain Diophantine equations which increased\ncomputational complexity and cost. Here, we present the exact one-dimensional\nsolution to $\\pmb{r}(\\lambda)$, together with the full one-dimensional kernel\n$\\mathcal{K}$. Numerical evidence and a proof are presented in form of the\nquantum harmonic oscillator (QHO) and the Morse potential. Furthermore, we\nderive an analytical expression of $\\mathcal{K}$ for the free atom which\namounts to proof that the energy varies quadratically exactly, not\napproximately, in nuclear charge $Z_B$ for the entire iso-electronic series of\nall the possible charged ions, i.e. $E_B(Z, N_e)\\propto-\\frac{1}{2}Z_B^2$.\nComparison to Levy's averaging formula for relative energies of iso-electronic\nsystems also enables us to derive his residual error term. Finally, we rewrite\n$\\mathcal{K}$ in a simple, compact and easy to implement form for arbitrary\nchanges in external potentials $$E_B-E_A\n=\\int_{\\mathbb{R}^n}d\\pmb{y}_A\\,\\rho_A(\\pmb{y}_A)\\,\\mathcal{K}[v_A,v_B](\\pmb{y}_A)=\\int_{\\mathbb{R}^n}d\\pmb{y}_A\\,\\rho_A(\\pmb{y}_A)\\frac{v_B(\\pmb{y}_A)-v_A(\\pmb{y}_A)}{(2\\pi)^n}\\int_{\\mathbb{R}^n}d\\pmb{\\phi}\\int_{\\mathbb{R}^n}d\\pmb{r}_A\\int_0^1d\\lambda\\,e^{i\\pmb{\\phi}\\cdot\\left(\\pmb{y}_A-\\pmb{r}(\\lambda)\\right)}$$\nHere, $v_A,v_B$ are initial and final external potential, $\\pmb{r}(\\lambda)$\nthe $n$-dimensional parametrization.",
            "author": [
                "Simon Le\u00f3n Krug",
                "O. Anatole von Lilienfeld"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04458v1",
                "http://arxiv.org/pdf/2312.04458v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04457v1",
            "title": "Guided simulation of conditioned chemical reaction networks",
            "updated": "2023-12-07T17:26:10Z",
            "published": "2023-12-07T17:26:10Z",
            "summary": "Let $X$ be a chemical reaction process, modeled as a multi-dimensional\ncontinuous-time jump process. Assume that at given times $0<t_1 < \\cdots <\nt_n$, linear combinations $v_i = L_i X(t_i),\\, i = 1,\\dots, n$ are observed for\ngiven matrices $L_i$. We show how to sample the process conditioned on hitting\nthe states $v_1,\\dots, v_n$ by a change of measure on the law of the\nunconditioned process. We derive sufficient conditions for this change of\nmeasure and complement our results using numerical illustrations.",
            "author": [
                "Marc Corstanje",
                "Frank van der Meulen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04457v1",
                "http://arxiv.org/pdf/2312.04457v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "stat.CO",
                "60J27, 60J28, 60J74"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04455v1",
            "title": "Fortify the Shortest Stave in Attention: Enhancing Context Awareness of\n  Large Language Models for Effective Tool Use",
            "updated": "2023-12-07T17:24:51Z",
            "published": "2023-12-07T17:24:51Z",
            "summary": "Recent advancements in large language models (LLMs) have significantly\nexpanded their functionality and skills as tool agents. In this paper, we argue\nthat a waveform pattern in the model's attention allocation has an impact on\nthe tool use performance, which degrades when the position of essential\ninformation hits the trough zone. To address this issue, we propose a novel\ninference method named Attention Buckets. This approach enables LLMs to handle\ncontext by conducting parallel processes, each featuring a unique RoPE angle\nbase that shapes the attention waveform. Attention Buckets ensures that an\nattention trough of a particular process can be compensated with an attention\npeak of another run, reducing the risk of the LLM missing essential information\nresiding within the attention trough. Our extensive experiments on the widely\nrecognized tool use benchmark demonstrate the efficacy of our approach, where a\n7B-parameter open-source model enhanced by Attention Buckets achieves SOTA\nperformance on par with GPT-4.",
            "author": [
                "Yuhan Chen",
                "Ang Lv",
                "Ting-En Lin",
                "Changyu Chen",
                "Yuchuan Wu",
                "Fei Huang",
                "Yongbin Li",
                "Rui Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04455v1",
                "http://arxiv.org/pdf/2312.04455v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04447v1",
            "title": "Privacy-preserving quantum federated learning via gradient hiding",
            "updated": "2023-12-07T17:16:30Z",
            "published": "2023-12-07T17:16:30Z",
            "summary": "Distributed quantum computing, particularly distributed quantum machine\nlearning, has gained substantial prominence for its capacity to harness the\ncollective power of distributed quantum resources, transcending the limitations\nof individual quantum nodes. Meanwhile, the critical concern of privacy within\ndistributed computing protocols remains a significant challenge, particularly\nin standard classical federated learning (FL) scenarios where data of\nparticipating clients is susceptible to leakage via gradient inversion attacks\nby the server. This paper presents innovative quantum protocols with quantum\ncommunication designed to address the FL problem, strengthen privacy measures,\nand optimize communication efficiency. In contrast to previous works that\nleverage expressive variational quantum circuits or differential privacy\ntechniques, we consider gradient information concealment using quantum states\nand propose two distinct FL protocols, one based on private inner-product\nestimation and the other on incremental learning. These protocols offer\nsubstantial advancements in privacy preservation with low communication\nresources, forging a path toward efficient quantum communication-assisted FL\nprotocols and contributing to the development of secure distributed quantum\nmachine learning, thus addressing critical privacy concerns in the quantum\ncomputing era.",
            "author": [
                "Changhao Li",
                "Niraj Kumar",
                "Zhixin Song",
                "Shouvanik Chakrabarti",
                "Marco Pistoia"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04447v1",
                "http://arxiv.org/pdf/2312.04447v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.CR",
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04440v1",
            "title": "OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization",
            "updated": "2023-12-07T17:06:20Z",
            "published": "2023-12-07T17:06:20Z",
            "summary": "The performance of automatic summarization models has improved dramatically\nin recent years. Yet, there is still a gap in meeting specific information\nneeds of users in real-world scenarios, particularly when a targeted summary is\nsought, such as in the useful aspect-based summarization setting targeted in\nthis paper. Previous datasets and studies for this setting have predominantly\nconcentrated on a limited set of pre-defined aspects, focused solely on single\ndocument inputs, or relied on synthetic data. To advance research on more\nrealistic scenarios, we introduce OpenAsp, a benchmark for multi-document\n\\textit{open} aspect-based summarization. This benchmark is created using a\nnovel and cost-effective annotation protocol, by which an open aspect dataset\nis derived from existing generic multi-document summarization datasets. We\nanalyze the properties of OpenAsp showcasing its high-quality content. Further,\nwe show that the realistic open-aspect setting realized in OpenAsp poses a\nchallenge for current state-of-the-art summarization models, as well as for\nlarge language models.",
            "author": [
                "Shmuel Amar",
                "Liat Schiff",
                "Ori Ernst",
                "Asi Shefer",
                "Ori Shapira",
                "Ido Dagan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04440v1",
                "http://arxiv.org/pdf/2312.04440v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04433v1",
            "title": "DreamVideo: Composing Your Dream Videos with Customized Subject and\n  Motion",
            "updated": "2023-12-07T16:57:26Z",
            "published": "2023-12-07T16:57:26Z",
            "summary": "Customized generation using diffusion models has made impressive progress in\nimage generation, but remains unsatisfactory in the challenging video\ngeneration task, as it requires the controllability of both subjects and\nmotions. To that end, we present DreamVideo, a novel approach to generating\npersonalized videos from a few static images of the desired subject and a few\nvideos of target motion. DreamVideo decouples this task into two stages,\nsubject learning and motion learning, by leveraging a pre-trained video\ndiffusion model. The subject learning aims to accurately capture the fine\nappearance of the subject from provided images, which is achieved by combining\ntextual inversion and fine-tuning of our carefully designed identity adapter.\nIn motion learning, we architect a motion adapter and fine-tune it on the given\nvideos to effectively model the target motion pattern. Combining these two\nlightweight and efficient adapters allows for flexible customization of any\nsubject with any motion. Extensive experimental results demonstrate the\nsuperior performance of our DreamVideo over the state-of-the-art methods for\ncustomized video generation. Our project page is at\nhttps://dreamvideo-t2v.github.io.",
            "author": [
                "Yujie Wei",
                "Shiwei Zhang",
                "Zhiwu Qing",
                "Hangjie Yuan",
                "Zhiheng Liu",
                "Yu Liu",
                "Yingya Zhang",
                "Jingren Zhou",
                "Hongming Shan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04433v1",
                "http://arxiv.org/pdf/2312.04433v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04431v1",
            "title": "Content Moderation on Social Media in the EU: Insights From the DSA\n  Transparency Database",
            "updated": "2023-12-07T16:56:19Z",
            "published": "2023-12-07T16:56:19Z",
            "summary": "The Digital Services Act (DSA) requires large social media platforms in the\nEU to provide clear and specific information whenever they remove or restrict\naccess to certain content. These \"Statements of Reasons\" (SoRs) are collected\nin the DSA Transparency Database to ensure transparency and scrutiny of content\nmoderation decisions of the providers of online platforms. In this work, we\nempirically analyze 156 million SoRs within an observation period of two months\nto provide an early look at content moderation decisions of social media\nplatforms in the EU. Our empirical analysis yields the following main findings:\n(i) There are vast differences in the frequency of content moderation across\nplatforms. For instance, TikTok performs more than 350 times more content\nmoderation decisions per user than X/Twitter. (ii) Content moderation is most\ncommonly applied for text and videos, whereas images and other content formats\nundergo moderation less frequently. (ii) The primary reasons for moderation\ninclude content falling outside the platform's scope of service,\nillegal/harmful speech, and pornography/sexualized content, with moderation of\nmisinformation being relatively uncommon. (iii) The majority of rule-breaking\ncontent is detected and decided upon via automated means rather than manual\nintervention. However, X/Twitter reports that it relies solely on non-automated\nmethods. (iv) There is significant variation in the content moderation actions\ntaken across platforms. Altogether, our study implies inconsistencies in how\nsocial media platforms implement their obligations under the DSA -- resulting\nin a fragmented outcome that the DSA is meant to avoid. Our findings have\nimportant implications for regulators to clarify existing guidelines or lay out\nmore specific rules that ensure common standards on how social media providers\nhandle rule-breaking content on their platforms.",
            "author": [
                "Chiara Drolsbach",
                "Nicolas Pr\u00f6llochs"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04431v1",
                "http://arxiv.org/pdf/2312.04431v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04429v1",
            "title": "Approximate Caching for Efficiently Serving Diffusion Models",
            "updated": "2023-12-07T16:55:04Z",
            "published": "2023-12-07T16:55:04Z",
            "summary": "Text-to-image generation using diffusion models has seen explosive popularity\nowing to their ability in producing high quality images adhering to text\nprompts. However, production-grade diffusion model serving is a resource\nintensive task that not only require high-end GPUs which are expensive but also\nincurs considerable latency. In this paper, we introduce a technique called\napproximate-caching that can reduce such iterative denoising steps for an image\ngeneration based on a prompt by reusing intermediate noise states created\nduring a prior image generation for similar prompts. Based on this idea, we\npresent an end to end text-to-image system, Nirvana, that uses the\napproximate-caching with a novel cache management-policy Least Computationally\nBeneficial and Frequently Used (LCBFU) to provide % GPU compute savings, 19.8%\nend-to-end latency reduction and 19% dollar savings, on average, on two real\nproduction workloads. We further present an extensive characterization of real\nproduction text-to-image prompts from the perspective of caching, popularity\nand reuse of intermediate states in a large production environment.",
            "author": [
                "Shubham Agarwal",
                "Subrata Mitra",
                "Sarthak Chakraborty",
                "Srikrishna Karanam",
                "Koyel Mukherjee",
                "Shiv Saini"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04429v1",
                "http://arxiv.org/pdf/2312.04429v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04427v1",
            "title": "Spheroidal Molecular Communication via Diffusion: Signaling Between\n  Homogeneous Cell Aggregates",
            "updated": "2023-12-07T16:52:59Z",
            "published": "2023-12-07T16:52:59Z",
            "summary": "Recent molecular communication (MC) research has integrated more detailed\ncomputational models to capture the dynamics of practical biophysical systems.\nThis research focuses on developing realistic models for MC transceivers\ninspired by spheroids - three-dimensional cell aggregates commonly used in\norgan-on-chip experimental systems. Potential applications that can be used or\nmodeled with spheroids include nutrient transport in an organ-on-chip system,\nthe release of biomarkers or reception of drug molecules by a cancerous tumor\nsite, or transceiver nanomachines participating in information exchange. In\nthis paper, a simple diffusive MC system is considered where a spheroidal\ntransmitter and receiver are in an unbounded fluid environment. These\nspheroidal antennas are modeled as porous media for diffusive signaling\nmolecules, then their boundary conditions and effective diffusion coefficients\nare characterized. Further, for either a point source or spheroidal\ntransmitter, Green's function for concentration (GFC) outside and inside the\nreceiving spheroid is analytically derived and formulated in terms of an\ninfinite series and confirmed by a particle-based simulator (PBS). The provided\nGFCs enable computation of the transmitted and received signals in the\nspheroidal communication system. This study shows that the porous structure of\nthe receiving spheroid amplifies diffusion signals but also disperses them,\nthus there is a trade-off between porosity and information transmission rate.\nAlso, the results reveal that the porous arrangement of the transmitting\nspheroid not only disperses the received signal but also attenuates it. System\nperformance is also evaluated in terms of bit error rate (BER). Decreasing the\nporosity of the receiving spheroid is shown to enhance system performance.\nConversely, reducing the porosity of the transmitting spheroid can adversely\naffect system performance.",
            "author": [
                "Mitra Rezaei",
                "Hamidreza Arjmandi",
                "Mohammad Zoofaghari",
                "Kajsa Kanebratt",
                "Liisa Vilen",
                "David Janzen",
                "Peter Gennemark",
                "Adam Noel"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04427v1",
                "http://arxiv.org/pdf/2312.04427v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT",
                "q-bio.CB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04424v1",
            "title": "Cascade-Zero123: One Image to Highly Consistent 3D with Self-Prompted\n  Nearby Views",
            "updated": "2023-12-07T16:49:09Z",
            "published": "2023-12-07T16:49:09Z",
            "summary": "Synthesizing multi-view 3D from one single image is a significant and\nchallenging task. For this goal, Zero-1-to-3 methods aim to extend a 2D latent\ndiffusion model to the 3D scope. These approaches generate the target-view\nimage with a single-view source image and the camera pose as condition\ninformation. However, the one-to-one manner adopted in Zero-1-to-3 incurs\nchallenges for building geometric and visual consistency across views,\nespecially for complex objects. We propose a cascade generation framework\nconstructed with two Zero-1-to-3 models, named Cascade-Zero123, to tackle this\nissue, which progressively extracts 3D information from the source image.\nSpecifically, a self-prompting mechanism is designed to generate several nearby\nviews at first. These views are then fed into the second-stage model along with\nthe source image as generation conditions. With self-prompted multiple views as\nthe supplementary information, our Cascade-Zero123 generates more highly\nconsistent novel-view images than Zero-1-to-3. The promotion is significant for\nvarious complex and challenging scenes, involving insects, humans, transparent\nobjects, and stacked multiple objects etc. The project page is at\nhttps://cascadezero123.github.io/.",
            "author": [
                "Yabo Chen",
                "Jiemin Fang",
                "Yuyang Huang",
                "Taoran Yi",
                "Xiaopeng Zhang",
                "Lingxi Xie",
                "Xinggang Wang",
                "Wenrui Dai",
                "Hongkai Xiong",
                "Qi Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04424v1",
                "http://arxiv.org/pdf/2312.04424v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04417v1",
            "title": "Temporal Fairness in Multiwinner Voting",
            "updated": "2023-12-07T16:38:32Z",
            "published": "2023-12-07T16:38:32Z",
            "summary": "Multiwinner voting captures a wide variety of settings, from parliamentary\nelections in democratic systems to product placement in online shopping\nplatforms. There is a large body of work dealing with axiomatic\ncharacterizations, computational complexity, and algorithmic analysis of\nmultiwinner voting rules. Although many challenges remain, significant progress\nhas been made in showing existence of fair and representative outcomes as well\nas efficient algorithmic solutions for many commonly studied settings. However,\nmuch of this work focuses on single-shot elections, even though in numerous\nreal-world settings elections are held periodically and repeatedly. Hence, it\nis imperative to extend the study of multiwinner voting to temporal settings.\nRecently, there have been several efforts to address this challenge. However,\nthese works are difficult to compare, as they model multi-period voting in very\ndifferent ways. We propose a unified framework for studying temporal fairness\nin this domain, drawing connections with various existing bodies of work, and\nconsolidating them within a general framework. We also identify gaps in\nexisting literature, outline multiple opportunities for future work, and put\nforward a vision for the future of multiwinner voting in temporal settings.",
            "author": [
                "Edith Elkind",
                "Svetlana Obratzsova",
                "Nicholas Teh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04417v1",
                "http://arxiv.org/pdf/2312.04417v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.AI",
                "econ.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04416v1",
            "title": "Monitoring Sustainable Global Development Along Shared Socioeconomic\n  Pathways",
            "updated": "2023-12-07T16:38:20Z",
            "published": "2023-12-07T16:38:20Z",
            "summary": "Sustainable global development is one of the most prevalent challenges facing\nthe world today, hinging on the equilibrium between socioeconomic growth and\nenvironmental sustainability. We propose approaches to monitor and quantify\nsustainable development along the Shared Socioeconomic Pathways (SSPs),\nincluding mathematically derived scoring algorithms, and machine learning\nmethods. These integrate socioeconomic and environmental datasets, to produce\nan interpretable metric for SSP alignment. An initial study demonstrates\npromising results, laying the groundwork for the application of different\nmethods to the monitoring of sustainable global development.",
            "author": [
                "Michelle W. L. Wan",
                "Jeffrey N. Clark",
                "Edward A. Small",
                "Elena Fillola Mayoral",
                "Ra\u00fal Santos-Rodr\u00edguez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04416v1",
                "http://arxiv.org/pdf/2312.04416v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04414v1",
            "title": "A single-phonon directional coupler",
            "updated": "2023-12-07T16:36:17Z",
            "published": "2023-12-07T16:36:17Z",
            "summary": "Integrated photonics has enabled countless technologies in\ntelecommunications, spectroscopy, metrology, quantum optics, and quantum\ninformation processing. Using highly confined guided optical modes is the key\nthat has made integrated circuits possible and has lead to scaling of complex\ndesigns, benefiting from their small footprint. At the same time, the field of\nquantum acoustics has recently gained significant attention due to its various\npotential advantages over its photonic counterparts, including smaller mode\nvolume, lower energy, and orders of magnitude slower propagation speeds, as\nwell as the potential for interconnecting distinct quantum systems. Developing\nanalogous integrated phononic technology is critical for realizing the full\npotential of phonons and could lead to groundbreaking new applications, such as\nscalable quantum computing and hybrid quantum devices. In this work, we\ndemonstrate for the first time a 4-port directional coupler for quantum\nmechanical excitations - a crucial component for integrated phononic circuits.\nAdjusting the length of the coupling region allows to realize phononic beam\nsplitters with varying splitting ratios. By sending a single-phonon Fock state\nonto one of these phononic splitters, we demonstrate the capability of using\nthe directional coupler directly in the quantum regime. Our work provides an\nessential step towards an integrated phononic platform for both classical and\nquantum technologies applications.",
            "author": [
                "Amirparsa Zivari",
                "Niccol\u00f2 Fiaschi",
                "Lorenzo Scarpelli",
                "Menno Jansen",
                "Roel Burgwal",
                "Ewold Verhagen",
                "Simon Gr\u00f6blacher"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04414v1",
                "http://arxiv.org/pdf/2312.04414v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.mes-hall",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04412v1",
            "title": "Developing Elementary Federated Learning Algorithms Leveraging the\n  ChatGPT",
            "updated": "2023-12-07T16:34:47Z",
            "published": "2023-12-07T16:34:47Z",
            "summary": "The Python Testbed for Federated Learning Algorithms is a simple Python FL\nframework easy to use by ML&AI developers who do not need to be professional\nprogrammers, and this paper shows that it is also amenable to emerging AI\ntools. In this paper, we successfully developed three elementary FL algorithms\nusing the following three steps process: (i) specify context, (ii) ask ChatGPT\nto complete server and clients' callback functions, and (iii) verify the\ngenerated code.",
            "author": [
                "Miroslav Popovic",
                "Marko Popovic",
                "Ivan Kastelan",
                "Miodrag Djukic",
                "Ilija Basicevic"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04412v1",
                "http://arxiv.org/pdf/2312.04412v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04410v1",
            "title": "Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models",
            "updated": "2023-12-07T16:26:23Z",
            "published": "2023-12-07T16:26:23Z",
            "summary": "Recently, diffusion models have made remarkable progress in text-to-image\n(T2I) generation, synthesizing images with high fidelity and diverse contents.\nDespite this advancement, latent space smoothness within diffusion models\nremains largely unexplored. Smooth latent spaces ensure that a perturbation on\nan input latent corresponds to a steady change in the output image. This\nproperty proves beneficial in downstream tasks, including image interpolation,\ninversion, and editing. In this work, we expose the non-smoothness of diffusion\nlatent spaces by observing noticeable visual fluctuations resulting from minor\nlatent variations. To tackle this issue, we propose Smooth Diffusion, a new\ncategory of diffusion models that can be simultaneously high-performing and\nsmooth. Specifically, we introduce Step-wise Variation Regularization to\nenforce the proportion between the variations of an arbitrary input latent and\nthat of the output image is a constant at any diffusion training step. In\naddition, we devise an interpolation standard deviation (ISTD) metric to\neffectively assess the latent space smoothness of a diffusion model. Extensive\nquantitative and qualitative experiments demonstrate that Smooth Diffusion\nstands out as a more desirable solution not only in T2I generation but also\nacross various downstream tasks. Smooth Diffusion is implemented as a\nplug-and-play Smooth-LoRA to work with various community models. Code is\navailable at https://github.com/SHI-Labs/Smooth-Diffusion.",
            "author": [
                "Jiayi Guo",
                "Xingqian Xu",
                "Yifan Pu",
                "Zanlin Ni",
                "Chaofei Wang",
                "Manushree Vasu",
                "Shiji Song",
                "Gao Huang",
                "Humphrey Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04410v1",
                "http://arxiv.org/pdf/2312.04410v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04404v1",
            "title": "On the Impact of Multi-dimensional Local Differential Privacy on\n  Fairness",
            "updated": "2023-12-07T16:17:34Z",
            "published": "2023-12-07T16:17:34Z",
            "summary": "Automated decision systems are increasingly used to make consequential\ndecisions in people's lives. Due to the sensitivity of the manipulated data as\nwell as the resulting decisions, several ethical concerns need to be addressed\nfor the appropriate use of such technologies, in particular, fairness and\nprivacy. Unlike previous work, which focused on centralized differential\nprivacy (DP) or local DP (LDP) for a single sensitive attribute, in this paper,\nwe examine the impact of LDP in the presence of several sensitive attributes\n(i.e., multi-dimensional data) on fairness. Detailed empirical analysis on\nsynthetic and benchmark datasets revealed very relevant observations. In\nparticular, (1) multi-dimensional LDP is an efficient approach to reduce\ndisparity, (2) the multi-dimensional approach of LDP (independent vs. combined)\nmatters only at low privacy guarantees, and (3) the outcome Y distribution has\nan important effect on which group is more sensitive to the obfuscation. Last,\nwe summarize our findings in the form of recommendations to guide practitioners\nin adopting effective privacy-preserving practices while maintaining fairness\nand utility in ML applications.",
            "author": [
                "karima Makhlouf",
                "Heber H. Arcolezi",
                "Sami Zhioua",
                "Ghassen Ben Brahim",
                "Catuscia Palamidessi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04404v1",
                "http://arxiv.org/pdf/2312.04404v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04403v1",
            "title": "OT-Attack: Enhancing Adversarial Transferability of Vision-Language\n  Models via Optimal Transport Optimization",
            "updated": "2023-12-07T16:16:50Z",
            "published": "2023-12-07T16:16:50Z",
            "summary": "Vision-language pre-training (VLP) models demonstrate impressive abilities in\nprocessing both images and text. However, they are vulnerable to multi-modal\nadversarial examples (AEs). Investigating the generation of\nhigh-transferability adversarial examples is crucial for uncovering VLP models'\nvulnerabilities in practical scenarios. Recent works have indicated that\nleveraging data augmentation and image-text modal interactions can enhance the\ntransferability of adversarial examples for VLP models significantly. However,\nthey do not consider the optimal alignment problem between dataaugmented\nimage-text pairs. This oversight leads to adversarial examples that are overly\ntailored to the source model, thus limiting improvements in transferability. In\nour research, we first explore the interplay between image sets produced\nthrough data augmentation and their corresponding text sets. We find that\naugmented image samples can align optimally with certain texts while exhibiting\nless relevance to others. Motivated by this, we propose an Optimal\nTransport-based Adversarial Attack, dubbed OT-Attack. The proposed method\nformulates the features of image and text sets as two distinct distributions\nand employs optimal transport theory to determine the most efficient mapping\nbetween them. This optimal mapping informs our generation of adversarial\nexamples to effectively counteract the overfitting issues. Extensive\nexperiments across various network architectures and datasets in image-text\nmatching tasks reveal that our OT-Attack outperforms existing state-of-the-art\nmethods in terms of adversarial transferability.",
            "author": [
                "Dongchen Han",
                "Xiaojun Jia",
                "Yang Bai",
                "Jindong Gu",
                "Yang Liu",
                "Xiaochun Cao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04403v1",
                "http://arxiv.org/pdf/2312.04403v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04402v1",
            "title": "Semi-Supervised Active Learning for Semantic Segmentation in Unknown\n  Environments Using Informative Path Planning",
            "updated": "2023-12-07T16:16:47Z",
            "published": "2023-12-07T16:16:47Z",
            "summary": "Semantic segmentation enables robots to perceive and reason about their\nenvironments beyond geometry. Most of such systems build upon deep learning\napproaches. As autonomous robots are commonly deployed in initially unknown\nenvironments, pre-training on static datasets cannot always capture the variety\nof domains and limits the robot's perception performance during missions.\nRecently, self-supervised and fully supervised active learning methods emerged\nto improve a robot's vision. These approaches rely on large in-domain\npre-training datasets or require substantial human labelling effort. We propose\na planning method for semi-supervised active learning of semantic segmentation\nthat substantially reduces human labelling requirements compared to fully\nsupervised approaches. We leverage an adaptive map-based planner guided towards\nthe frontiers of unexplored space with high model uncertainty collecting\ntraining data for human labelling. A key aspect of our approach is to combine\nthe sparse high-quality human labels with pseudo labels automatically extracted\nfrom highly certain environment map areas. Experimental results show that our\nmethod reaches segmentation performance close to fully supervised approaches\nwith drastically reduced human labelling effort while outperforming\nself-supervised approaches.",
            "author": [
                "Julius R\u00fcckin",
                "Federico Magistri",
                "Cyrill Stachniss",
                "Marija Popovi\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04402v1",
                "http://arxiv.org/pdf/2312.04402v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04400v1",
            "title": "On the role of numerical diffusivity in MHD simulations of global\n  accretion disc dynamos",
            "updated": "2023-12-07T16:14:39Z",
            "published": "2023-12-07T16:14:39Z",
            "summary": "Observations, mainly of outbursts in dwarf novae, imply that the anomalous\nviscosity in highly ionized accretion discs is magnetic in origin, and requires\nthat the plasma $\\beta \\sim 1$. Until now most simulations of the magnetic\ndynamo in accretion discs have used a local approximation (known as the\nshearing box). While these simulations demonstrate the possibility of a\nself-sustaining dynamo, the magnetic activity generated in these models\nsaturates at $\\beta \\gg 1$. This long-standing discrepancy has previously been\nattributed to the local approximation itself. There have been recent attempts\nat simulating magnetic activity in global accretion discs with parameters\nrelevant to the dwarf novae. These too find values of $\\beta \\gg 1$. We\nspeculate that the tension between these models and the observations may be\ncaused by numerical magnetic diffusivity. As a pedagogical example, we present\nexact time-dependent solutions for the evolution of weak magnetic fields in an\nincompressible fluid subject to linear shear and magnetic diffusivity. We find\nthat the maximum factor by which the initial magnetic energy can be increased\ndepends on the magnetic Reynolds number as ${\\mathcal R}_{\\rm m}^{2/3}$. We\nestimate that current global numerical simulations of dwarf nova discs have\nnumerical magnetic Reynolds numbers around 6 orders of magnitude less than the\nphysical value found in dwarf nova discs of ${\\mathcal R}_{\\rm m} \\sim\n10^{10}$. We suggest that, given the current limitations on computing power,\nexpecting to be able to compute realistic dynamo action in observable accretion\ndiscs using numerical MHD is, for the time being, a step too far.",
            "author": [
                "C. J. Nixon",
                "C. C. T. Pringle",
                "J. E. Pringle"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04400v1",
                "http://arxiv.org/pdf/2312.04400v1"
            ],
            "primary_category": "physics.plasm-ph",
            "category": [
                "physics.plasm-ph",
                "astro-ph.HE",
                "astro-ph.SR",
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04398v1",
            "title": "Intelligent Anomaly Detection for Lane Rendering Using Transformer with\n  Self-Supervised Pre-Training and Customized Fine-Tuning",
            "updated": "2023-12-07T16:10:10Z",
            "published": "2023-12-07T16:10:10Z",
            "summary": "The burgeoning navigation services using digital maps provide great\nconvenience to drivers. Nevertheless, the presence of anomalies in lane\nrendering map images occasionally introduces potential hazards, as such\nanomalies can be misleading to human drivers and consequently contribute to\nunsafe driving conditions. In response to this concern and to accurately and\neffectively detect the anomalies, this paper transforms lane rendering image\nanomaly detection into a classification problem and proposes a four-phase\npipeline consisting of data pre-processing, self-supervised pre-training with\nthe masked image modeling (MiM) method, customized fine-tuning using\ncross-entropy based loss with label smoothing, and post-processing to tackle it\nleveraging state-of-the-art deep learning techniques, especially those\ninvolving Transformer models. Various experiments verify the effectiveness of\nthe proposed pipeline. Results indicate that the proposed pipeline exhibits\nsuperior performance in lane rendering image anomaly detection, and notably,\nthe self-supervised pre-training with MiM can greatly enhance the detection\naccuracy while significantly reducing the total training time. For instance,\nemploying the Swin Transformer with Uniform Masking as self-supervised\npretraining (Swin-Trans-UM) yielded a heightened accuracy at 94.77% and an\nimproved Area Under The Curve (AUC) score of 0.9743 compared with the pure Swin\nTransformer without pre-training (Swin-Trans) with an accuracy of 94.01% and an\nAUC of 0.9498. The fine-tuning epochs were dramatically reduced to 41 from the\noriginal 280. In conclusion, the proposed pipeline, with its incorporation of\nself-supervised pre-training using MiM and other advanced deep learning\ntechniques, emerges as a robust solution for enhancing the accuracy and\nefficiency of lane rendering image anomaly detection in digital navigation\nsystems.",
            "author": [
                "Yongqi Dong",
                "Xingmin Lu",
                "Ruohan Li",
                "Wei Song",
                "Bart van Arem",
                "Haneen Farah"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04398v1",
                "http://arxiv.org/pdf/2312.04398v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "eess.IV",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04396v1",
            "title": "Canonical scattering problem in topological metamaterials: Valley-Hall\n  modes through a bend",
            "updated": "2023-12-07T16:08:39Z",
            "published": "2023-12-07T16:08:39Z",
            "summary": "We study the amount of backscattering of Valley Hall modes in a classical\ntopological insulator. In reciprocal systems, the conservation of the valley\nindex has been argued to be at the root of the high-transmission of Valley Hall\nmodes, observed in many experimental realisations. Here, we reconsider this\nhypothesis by quantitatively analysing the canonical scattering problem of\ninterface Valley Hall modes impinging on sharp bends which may or may not\nconserve the valley index. We consider a tight binding model of graphene\nribbons with an interface and compute the reflection and transmission\ncoefficients using a transfer matrix formalism. We find that, in all\nconfigurations, the transmission of Valley Hall modes is close to being\nmaximal, even in cases where the valley index is not conserved. Our results\nreinforce the alternative interpretation of the high-transmission of Valley\nHall modes in reciprocal metamaterials as a consequence of a favorable mode\nmatching on each side of the defect and serve as a reference case for the\ndesign of Valley Hall type metamaterial.",
            "author": [
                "Theo Torres",
                "C\u00e9dric Bellis",
                "R\u00e9gis Cottereau",
                "Antonin Coutant"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04396v1",
                "http://arxiv.org/pdf/2312.04396v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "physics.class-ph",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04395v1",
            "title": "On Czerwinski's \"${\\rm P} \\neq {\\rm NP}$ relative to a ${\\rm\n  P}$-complete oracle\"",
            "updated": "2023-12-07T16:08:27Z",
            "published": "2023-12-07T16:08:27Z",
            "summary": "In this paper, we take a closer look at Czerwinski's \"${\\rm P}\\neq{\\rm NP}$\nrelative to a ${\\rm P}$-complete oracle\" [Cze23]. There are (uncountably)\ninfinitely-many relativized worlds where ${\\rm P}$ and ${\\rm NP}$ differ, and\nit is well-known that for any ${\\rm P}$-complete problem $A$, ${\\rm P}^A \\neq\n{\\rm NP}^A \\iff {\\rm P}\\neq {\\rm NP}$. The paper defines two sets ${\\rm D}_{\\rm\nP}$ and ${\\rm D}_{\\rm NP}$ and builds the purported proof of their main theorem\non the claim that an oracle Turing machine with ${\\rm D}_{\\rm NP}$ as its\noracle and that accepts ${\\rm D}_{\\rm P}$ must make $\\Theta(2^n)$ queries to\nthe oracle. We invalidate the latter by proving that there is an oracle Turing\nmachine with ${\\rm D}_{\\rm NP}$ as its oracle that accepts ${\\rm D}_{\\rm P}$\nand yet only makes one query to the oracle. We thus conclude that Czerwinski's\npaper [Cze23] fails to establish that ${\\rm P} \\neq {\\rm NP}$.",
            "author": [
                "Michael C. Chavrimootoo",
                "Tran Duy Anh Le",
                "Michael P. Reidy",
                "Eliot J. Smith"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04395v1",
                "http://arxiv.org/pdf/2312.04395v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04393v1",
            "title": "PhysHOI: Physics-Based Imitation of Dynamic Human-Object Interaction",
            "updated": "2023-12-07T16:06:31Z",
            "published": "2023-12-07T16:06:31Z",
            "summary": "Humans interact with objects all the time. Enabling a humanoid to learn\nhuman-object interaction (HOI) is a key step for future smart animation and\nintelligent robotics systems. However, recent progress in physics-based HOI\nrequires carefully designed task-specific rewards, making the system unscalable\nand labor-intensive. This work focuses on dynamic HOI imitation: teaching\nhumanoid dynamic interaction skills through imitating kinematic HOI\ndemonstrations. It is quite challenging because of the complexity of the\ninteraction between body parts and objects and the lack of dynamic HOI data. To\nhandle the above issues, we present PhysHOI, the first physics-based whole-body\nHOI imitation approach without task-specific reward designs. Except for the\nkinematic HOI representations of humans and objects, we introduce the contact\ngraph to model the contact relations between body parts and objects explicitly.\nA contact graph reward is also designed, which proved to be critical for\nprecise HOI imitation. Based on the key designs, PhysHOI can imitate diverse\nHOI tasks simply yet effectively without prior knowledge. To make up for the\nlack of dynamic HOI scenarios in this area, we introduce the BallPlay dataset\nthat contains eight whole-body basketball skills. We validate PhysHOI on\ndiverse HOI tasks, including whole-body grasping and basketball skills.",
            "author": [
                "Yinhuai Wang",
                "Jing Lin",
                "Ailing Zeng",
                "Zhengyi Luo",
                "Jian Zhang",
                "Lei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04393v1",
                "http://arxiv.org/pdf/2312.04393v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04392v1",
            "title": "Contextual Subspace Variational Quantum Eigensolver Calculation of the\n  Dissociation Curve of Molecular Nitrogen on a Superconducting Quantum\n  Computer",
            "updated": "2023-12-07T16:05:52Z",
            "published": "2023-12-07T16:05:52Z",
            "summary": "In this work we present an experimental demonstration of the Contextual\nSubspace Variational Quantum Eigensolver on superconducting quantum hardware.\nIn particular, we compute the potential energy curve for molecular nitrogen,\nwhere a dominance of static correlation in the dissociation limit proves\nchallenging for many conventional quantum chemistry techniques. Our quantum\nsimulations retain good agreement with the full configuration interaction\nenergy in the chosen STO-3G basis, outperforming coupled cluster singles\ndoubles with perturbative triples as one stretches the triple bond past a\nseparation of 1.73 {\\AA}. To achieve this result we deploy a quantum error\nmitigation strategy made up of measurement-error mitigation, dynamical\ndecoupling and zero-noise extrapolation, in addition to circuit parallelization\nthat not only provides passive noise averaging but improves the effective shot\nyield to reduce the measurement overhead. Furthermore, we introduce a\nmodification to previous adaptive ansatz construction algorithms that\nincorporates hardware-awareness.",
            "author": [
                "Tim Weaving",
                "Alexis Ralli",
                "Peter J. Love",
                "Sauro Succi",
                "Peter V. Coveney"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04392v1",
                "http://arxiv.org/pdf/2312.04392v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04385v1",
            "title": "AniRes2D: Anisotropic Residual-enhanced Diffusion for 2D MR\n  Super-Resolution",
            "updated": "2023-12-07T15:55:31Z",
            "published": "2023-12-07T15:55:31Z",
            "summary": "Anisotropic low-resolution (LR) magnetic resonance (MR) images are fast to\nobtain but hinder automated processing. We propose to use denoising diffusion\nprobabilistic models (DDPMs) to super-resolve these 2D-acquired LR MR slices.\nThis paper introduces AniRes2D, a novel approach combining DDPM with a residual\nprediction for 2D super-resolution (SR). Results demonstrate that AniRes2D\noutperforms several other DDPM-based models in quantitative metrics, visual\nquality, and out-of-domain evaluation. We use a trained AniRes2D to\nsuper-resolve 3D volumes slice by slice, where comparative quantitative results\nand reduced skull aliasing are achieved compared to a recent state-of-the-art\nself-supervised 3D super-resolution method. Furthermore, we explored the use of\nnoise conditioning augmentation (NCA) as an alternative augmentation technique\nfor DDPM-based SR models, but it was found to reduce performance. Our findings\ncontribute valuable insights to the application of DDPMs for SR of anisotropic\nMR images.",
            "author": [
                "Zejun Wu",
                "Samuel W. Remedios",
                "Blake E. Dewey",
                "Aaron Carass",
                "Jerry L. Prince"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04385v1",
                "http://arxiv.org/pdf/2312.04385v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04377v1",
            "title": "HARQ-IR Aided Short Packet Communications: BLER Analysis and Throughput\n  Maximization",
            "updated": "2023-12-07T15:47:18Z",
            "published": "2023-12-07T15:47:18Z",
            "summary": "This paper introduces hybrid automatic repeat request with incremental\nredundancy (HARQ-IR) to boost the reliability of short packet communications.\nThe finite blocklength information theory and correlated decoding events\ntremendously preclude the analysis of average block error rate (BLER).\nFortunately, the recursive form of average BLER motivates us to calculate its\nvalue through the trapezoidal approximation and Gauss-Laguerre quadrature.\nMoreover, the asymptotic analysis is performed to derive a simple expression\nfor the average BLER at high signal-to-noise ratio (SNR). Then, we study the\nmaximization of long term average throughput (LTAT) via power allocation\nmeanwhile ensuring the power and the BLER constraints. For tractability, the\nasymptotic BLER is employed to solve the problem through geometric programming\n(GP). However, the GP-based solution underestimates the LTAT at low SNR due to\na large approximation error in this case. Alternatively, we also develop a deep\nreinforcement learning (DRL)-based framework to learn power allocation policy.\nIn particular, the optimization problem is transformed into a constrained\nMarkov decision process, which is solved by integrating deep deterministic\npolicy gradient (DDPG) with subgradient method. The numerical results finally\ndemonstrate that the DRL-based method outperforms the GP-based one at low SNR,\nalbeit at the cost of increasing computational burden.",
            "author": [
                "Fuchao He",
                "Zheng Shi",
                "Guanghua Yang",
                "Xiaofan Li",
                "Xinrong Ye",
                "Shaodan Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04377v1",
                "http://arxiv.org/pdf/2312.04377v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04375v1",
            "title": "Generating Multiphase Fluid Configurations in Fractures using Diffusion\n  Models",
            "updated": "2023-12-07T15:46:54Z",
            "published": "2023-12-07T15:46:54Z",
            "summary": "Pore-scale simulations accurately describe transport properties of fluids in\nthe subsurface. These simulations enhance our understanding of applications\nsuch as assessing hydrogen storage efficiency and forecasting CO$_2$\nsequestration processes in underground reservoirs. Nevertheless, they are\ncomputationally expensive due to their mesoscopic nature. In addition, their\nstationary solutions are not guaranteed to be unique, so multiple runs with\ndifferent initial conditions must be performed to ensure sufficient sample\ncoverage. These factors complicate the task of obtaining representative and\nreliable forecasts. To overcome the high computational cost hurdle, we propose\na hybrid method that couples generative diffusion models and physics-based\nmodeling. Upon training a generative model, we synthesize samples that serve as\nthe initial conditions for physics-based simulations. We measure the relaxation\ntime (to stationary solutions) of the simulations, which serves as a validation\nmetric and early-stopping criterion. Our numerical experiments revealed that\nthe hybrid method exhibits a speed-up of up to 8.2 times compared to commonly\nused initialization methods. This finding offers compelling initial support\nthat the proposed diffusion model-based hybrid scheme has potentials to\nsignificantly decrease the time required for convergence of numerical\nsimulations without compromising the physical robustness.",
            "author": [
                "Jaehong Chung",
                "Agnese Marcato",
                "Eric J. Guiltinan",
                "Tapan Mukerji",
                "Yen Ting Lin",
                "Javier E. Santos"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04375v1",
                "http://arxiv.org/pdf/2312.04375v1"
            ],
            "primary_category": "physics.geo-ph",
            "category": [
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04374v1",
            "title": "Deep Dynamics: Vehicle Dynamics Modeling with a Physics-Informed Neural\n  Network for Autonomous Racing",
            "updated": "2023-12-07T15:44:56Z",
            "published": "2023-12-07T15:44:56Z",
            "summary": "Autonomous racing is a critical research area for autonomous driving,\npresenting significant challenges in vehicle dynamics modeling, such as\nbalancing model precision and computational efficiency at high speeds\n(>280kmph), where minor errors in modeling have severe consequences. Existing\nphysics-based models for vehicle dynamics require elaborate testing setups and\ntuning, which are hard to implement, time-intensive, and cost-prohibitive.\nConversely, purely data-driven approaches do not generalize well and cannot\nadequately ensure physical constraints on predictions. This paper introduces\nDeep Dynamics, a physics-informed neural network (PINN) for vehicle dynamics\nmodeling of an autonomous racecar. It combines physics coefficient estimation\nand dynamical equations to accurately predict vehicle states at high speeds and\nincludes a unique Physics Guard layer to ensure internal coefficient estimates\nremain within their nominal physical ranges. Open-loop and closed-loop\nperformance assessments, using a physics-based simulator and full-scale\nautonomous Indy racecar data, highlight Deep Dynamics as a promising approach\nfor modeling racecar vehicle dynamics.",
            "author": [
                "John Chrosniak",
                "Jingyun Ning",
                "Madhur Behl"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04374v1",
                "http://arxiv.org/pdf/2312.04374v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.LG",
                "I.2.9; I.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04372v1",
            "title": "LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language\n  Model Programs",
            "updated": "2023-12-07T15:43:52Z",
            "published": "2023-12-07T15:43:52Z",
            "summary": "We present LaMPilot, a novel framework for planning in the field of\nautonomous driving, rethinking the task as a code-generation process that\nleverages established behavioral primitives. This approach aims to address the\nchallenge of interpreting and executing spontaneous user instructions such as\n\"overtake the car ahead,\" which have typically posed difficulties for existing\nframeworks. We introduce the LaMPilot benchmark specifically designed to\nquantitatively evaluate the efficacy of Large Language Models (LLMs) in\ntranslating human directives into actionable driving policies. We then evaluate\na wide range of state-of-the-art code generation language models on tasks from\nthe LaMPilot Benchmark. The results of the experiments showed that GPT-4, with\nhuman feedback, achieved an impressive task completion rate of 92.7% and a\nminimal collision rate of 0.9%. To encourage further investigation in this\narea, our code and dataset will be made available.",
            "author": [
                "Yunsheng Ma",
                "Can Cui",
                "Xu Cao",
                "Wenqian Ye",
                "Peiran Liu",
                "Juanwu Lu",
                "Amr Abdelraouf",
                "Rohit Gupta",
                "Kyungtae Han",
                "Aniket Bera",
                "James M. Rehg",
                "Ziran Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04372v1",
                "http://arxiv.org/pdf/2312.04372v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04370v1",
            "title": "Investigating the Design Space of Diffusion Models for Speech\n  Enhancement",
            "updated": "2023-12-07T15:40:55Z",
            "published": "2023-12-07T15:40:55Z",
            "summary": "Diffusion models are a new class of generative models that have shown\noutstanding performance in image generation literature. As a consequence,\nstudies have attempted to apply diffusion models to other tasks, such as speech\nenhancement. A popular approach in adapting diffusion models to speech\nenhancement consists in modelling a progressive transformation between the\nclean and noisy speech signals. However, one popular diffusion model framework\npreviously laid in image generation literature did not account for such a\ntransformation towards the system input, which prevents from relating the\nexisting diffusion-based speech enhancement systems with the aforementioned\ndiffusion model framework. To address this, we extend this framework to account\nfor the progressive transformation between the clean and noisy speech signals.\nThis allows us to apply recent developments from image generation literature,\nand to systematically investigate design aspects of diffusion models that\nremain largely unexplored for speech enhancement, such as the neural network\npreconditioning, the training loss weighting, the stochastic differential\nequation (SDE), or the amount of stochasticity injected in the reverse process.\nWe show that the performance of previous diffusion-based speech enhancement\nsystems cannot be attributed to the progressive transformation between the\nclean and noisy speech signals. Moreover, we show that a proper choice of\npreconditioning, training loss weighting, SDE and sampler allows to outperform\na popular diffusion-based speech enhancement system in terms of perceptual\nmetrics while using fewer sampling steps, thus reducing the computational cost\nby a factor of four.",
            "author": [
                "Philippe Gonzalez",
                "Zheng-Hua Tan",
                "Jan \u00d8stergaard",
                "Jesper Jensen",
                "Tommy Sonne Alstr\u00f8m",
                "Tobias May"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04370v1",
                "http://arxiv.org/pdf/2312.04370v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.LG",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04369v1",
            "title": "SingingHead: A Large-scale 4D Dataset for Singing Head Animation",
            "updated": "2023-12-07T15:40:36Z",
            "published": "2023-12-07T15:40:36Z",
            "summary": "Singing, as a common facial movement second only to talking, can be regarded\nas a universal language across ethnicities and cultures, plays an important\nrole in emotional communication, art, and entertainment. However, it is often\noverlooked in the field of audio-driven facial animation due to the lack of\nsinging head datasets and the domain gap between singing and talking in rhythm\nand amplitude. To this end, we collect a high-quality large-scale singing head\ndataset, SingingHead, which consists of more than 27 hours of synchronized\nsinging video, 3D facial motion, singing audio, and background music from 76\nindividuals and 8 types of music. Along with the SingingHead dataset, we argue\nthat 3D and 2D facial animation tasks can be solved together, and propose a\nunified singing facial animation framework named UniSinger to achieve both\nsinging audio-driven 3D singing head animation and 2D singing portrait video\nsynthesis. Extensive comparative experiments with both SOTA 3D facial animation\nand 2D portrait animation methods demonstrate the necessity of singing-specific\ndatasets in singing head animation tasks and the promising performance of our\nunified facial animation framework.",
            "author": [
                "Sijing Wu",
                "Yunhao Li",
                "Weitian Zhang",
                "Jun Jia",
                "Yucheng Zhu",
                "Yichao Yan",
                "Guangtao Zhai"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04369v1",
                "http://arxiv.org/pdf/2312.04369v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04367v1",
            "title": "Paraconsistent Existential Graphs Gamma Peirce System",
            "updated": "2023-12-07T15:36:34Z",
            "published": "2023-12-07T15:36:34Z",
            "summary": "In this paper, the paraconsistent propositional logic LG is presented, along\nwith its semantic characterization. It is shown that LG's set of theorems\ncorresponds to the set of valid existential graphs, GET, which turns out to be\nan extension of Peirce's Gamma system, without becoming Zeman's gamma-4 system.\nAll evidence is presented in a complete, rigorous, and detailed manner. This\nresult is generalized by constructing the paraconsistent system of existential\ngraphs GET4, and its semantic-deductive characterization. Finally, Zeman's\nGamma-4, Gamma-4.2, and Gamma-5 existential graph systems are proven to be\nparaconsistent.",
            "author": [
                "Manuel Sierra-Aristizabal"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04367v1",
                "http://arxiv.org/pdf/2312.04367v1"
            ],
            "primary_category": "math.LO",
            "category": [
                "math.LO",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04364v1",
            "title": "DemoCaricature: Democratising Caricature Generation with a Rough Sketch",
            "updated": "2023-12-07T15:35:42Z",
            "published": "2023-12-07T15:35:42Z",
            "summary": "In this paper, we democratise caricature generation, empowering individuals\nto effortlessly craft personalised caricatures with just a photo and a\nconceptual sketch. Our objective is to strike a delicate balance between\nabstraction and identity, while preserving the creativity and subjectivity\ninherent in a sketch. To achieve this, we present Explicit Rank-1 Model Editing\nalongside single-image personalisation, selectively applying nuanced edits to\ncross-attention layers for a seamless merge of identity and style.\nAdditionally, we propose Random Mask Reconstruction to enhance robustness,\ndirecting the model to focus on distinctive identity and style features.\nCrucially, our aim is not to replace artists but to eliminate accessibility\nbarriers, allowing enthusiasts to engage in the artistry.",
            "author": [
                "Dar-Yen Chen",
                "Subhadeep Koley",
                "Aneeshan Sain",
                "Pinaki Nath Chowdhury",
                "Tao Xiang",
                "Ayan Kumar Bhunia",
                "Yi-Zhe Song"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04364v1",
                "http://arxiv.org/pdf/2312.04364v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04362v1",
            "title": "PCoQA: Persian Conversational Question Answering Dataset",
            "updated": "2023-12-07T15:29:34Z",
            "published": "2023-12-07T15:29:34Z",
            "summary": "Humans seek information regarding a specific topic through performing a\nconversation containing a series of questions and answers. In the pursuit of\nconversational question answering research, we introduce the PCoQA, the first\n\\textbf{P}ersian \\textbf{Co}nversational \\textbf{Q}uestion \\textbf{A}nswering\ndataset, a resource comprising information-seeking dialogs encompassing a total\nof 9,026 contextually-driven questions. Each dialog involves a questioner, a\nresponder, and a document from the Wikipedia; The questioner asks several\ninter-connected questions from the text and the responder provides a span of\nthe document as the answer for each question. PCoQA is designed to present\nnovel challenges compared to previous question answering datasets including\nhaving more open-ended non-factual answers, longer answers, and fewer lexical\noverlaps. This paper not only presents the comprehensive PCoQA dataset but also\nreports the performance of various benchmark models. Our models include\nbaseline models and pre-trained models, which are leveraged to boost the\nperformance of the model. The dataset and benchmarks are available at our\nGithub page.",
            "author": [
                "Hamed Hematian Hemati",
                "Atousa Toghyani",
                "Atena Souri",
                "Sayed Hesam Alavian",
                "Hossein Sameti",
                "Hamid Beigy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04362v1",
                "http://arxiv.org/pdf/2312.04362v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04360v1",
            "title": "The Computational Advantage of MIP* Vanishes in the Presence of Noise",
            "updated": "2023-12-07T15:28:40Z",
            "published": "2023-12-07T15:28:40Z",
            "summary": "Quantum multiprover interactive proof systems with entanglement MIP* are much\nmore powerful than their classical counterpart MIP (Babai et al. '91, Ji et al.\n'20): while MIP = NEXP, the quantum class MIP* is equal to RE, a class\nincluding the halting problem. This is because the provers in MIP* can share\nunbounded quantum entanglement. However, recent works of Qin and Yao '21 and\n'23 have shown that this advantage is significantly reduced if the provers'\nshared state contains noise. This paper attempts to exactly characterize the\neffect of noise on the computational power of quantum multiprover interactive\nproof systems. We investigate the quantum two-prover one-round interactive\nsystem MIP*[poly, O(1)], where the verifier sends polynomially many bits to the\nprovers and the provers send back constantly many bits. We show noise\ncompletely destroys the computational advantage given by shared entanglement in\nthis model. Specifically, we show that if the provers are allowed to share\narbitrarily many EPR states, where each EPR state is affected by an arbitrarily\nsmall constant amount of noise, the resulting complexity class is contained in\nNEXP = MIP. This improves significantly on the previous best-known bound of\nNEEEXP (nondeterministic triply exponential time) by Qin and Yao '21. We also\nshow that this collapse in power is due to the noise, rather than the O(1)\nanswer size, by showing that allowing for noiseless EPR states gives the class\nthe full power of RE = MIP*[poly, poly]. Along the way, we develop two\ntechnical tools of independent interest. First, we give a new, deterministic\ntester for the positivity of an exponentially large matrix, provided it has a\nlow-degree Fourier decomposition in terms of Pauli matrices. Secondly, we\ndevelop a new invariance principle for smooth matrix functions having bounded\nthird-order Fr\\'echet derivatives or which are Lipschitz continous.",
            "author": [
                "Yangjing Dong",
                "Honghao Fu",
                "Anand Natarajan",
                "Minglong Qin",
                "Haochen Xu",
                "Penghui Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04360v1",
                "http://arxiv.org/pdf/2312.04360v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04356v1",
            "title": "NeuJeans: Private Neural Network Inference with Joint Optimization of\n  Convolution and Bootstrapping",
            "updated": "2023-12-07T15:23:07Z",
            "published": "2023-12-07T15:23:07Z",
            "summary": "Fully homomorphic encryption (FHE) is a promising cryptographic primitive for\nrealizing private neural network inference (PI) services by allowing a client\nto fully offload the inference task to a cloud server while keeping the client\ndata oblivious to the server. This work proposes NeuJeans, an FHE-based\nsolution for the PI of deep convolutional neural networks (CNNs). NeuJeans\ntackles the critical problem of the enormous computational cost for the FHE\nevaluation of convolutional layers (conv2d), mainly due to the high cost of\ndata reordering and bootstrapping. We first propose an encoding method\nintroducing nested structures inside encoded vectors for FHE, which enables us\nto develop efficient conv2d algorithms with reduced data reordering costs.\nHowever, the new encoding method also introduces additional computations for\nconversion between encoding methods, which could negate its advantages. We\ndiscover that fusing conv2d with bootstrapping eliminates such computations\nwhile reducing the cost of bootstrapping. Then, we devise optimized execution\nflows for various types of conv2d and apply them to end-to-end implementation\nof CNNs. NeuJeans accelerates the performance of conv2d by up to 5.68 times\ncompared to state-of-the-art FHE-based PI work and performs the PI of a CNN at\nthe scale of ImageNet (ResNet18) within a mere few seconds",
            "author": [
                "Jae Hyung Ju",
                "Jaiyoung Park",
                "Jongmin Kim",
                "Donghwan Kim",
                "Jung Ho Ahn"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04356v1",
                "http://arxiv.org/pdf/2312.04356v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04353v1",
            "title": "Interface-Induced Superconductivity in Magnetic Topological\n  Insulator-Iron Chalcogenide Heterostructures",
            "updated": "2023-12-07T15:19:51Z",
            "published": "2023-12-07T15:19:51Z",
            "summary": "When two different electronic materials are brought together, the resultant\ninterface often shows unexpected quantum phenomena, including interfacial\nsuperconductivity and Fu-Kane topological superconductivity (TSC). Here, we use\nmolecular beam epitaxy (MBE) to synthesize heterostructures formed by stacking\ntogether two magnetic materials, a ferromagnetic topological insulator (TI) and\nan antiferromagnetic iron chalcogenide (FeTe). We discover emergent\ninterface-induced superconductivity in these heterostructures and demonstrate\nthe trifecta occurrence of superconductivity, ferromagnetism, and topological\nband structure in the magnetic TI layer, the three essential ingredients of\nchiral TSC. The unusual coexistence of ferromagnetism and superconductivity can\nbe attributed to the high upper critical magnetic field that exceeds the Pauli\nparamagnetic limit for conventional superconductors at low temperatures. The\nmagnetic TI/FeTe heterostructures with robust superconductivity and atomically\nsharp interfaces provide an ideal wafer-scale platform for the exploration of\nchiral TSC and Majorana physics, constituting an important step toward scalable\ntopological quantum computation.",
            "author": [
                "Hemian Yi",
                "Yi-Fan Zhao",
                "Ying-Ting Chan",
                "Jiaqi Cai",
                "Ruobing Mei",
                "Xianxin Wu",
                "Zi-Jie Yan",
                "Ling-Jie Zhou",
                "Ruoxi Zhang",
                "Zihao Wang",
                "Stephen Paolini",
                "Run Xiao",
                "Ke Wang",
                "Anthony R. Richardella",
                "John Singleton",
                "Laurel E. Winter",
                "Thomas Prokscha",
                "Zaher Salman",
                "Andreas Suter",
                "Purnima P. Balakrishnan",
                "Alexander J. Grutter",
                "Moses H. W. Chan",
                "Nitin Samarth",
                "Xiaodong Xu",
                "Weida Wu",
                "Chao-Xing Liu",
                "Cui-Zu Chang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04353v1",
                "http://arxiv.org/pdf/2312.04353v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "cond-mat.mtrl-sci",
                "cond-mat.supr-con"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04351v1",
            "title": "Nash Equilibria of Two-round Auctions",
            "updated": "2023-12-07T15:17:08Z",
            "published": "2023-12-07T15:17:08Z",
            "summary": "In a two-round auction, a subset of bidders is selected (probabilistically),\naccording to their bids in the first round, for the second round, where they\ncan increase their bids. We formalize the two-round auction model, restricting\nthe second round to a dominant strategy incentive compatible (DSIC) auction for\nthe selected bidders. It turns out that, however, such two-round auctions are\nnot directly DSIC, even if the probability of each bidder being selected for\nthe second round is monotonic to its first bid, which is surprisingly\ncounter-intuitive. We also illustrate the necessary and sufficient conditions\nof two-round auctions being DSIC. Besides, we characterize the Nash equilibria\nfor untruthful two-round auctions. One can achieve better revenue performance\nby setting proper probability for selecting bidders for the second round\ncompared with truthful two-round auctions.",
            "author": [
                "Chulong Zhong",
                "Xiang Yan",
                "Yuyi Wang",
                "Shuangping Huang",
                "Jin Zhong"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3627676.3627682",
                "http://arxiv.org/abs/2312.04351v1",
                "http://arxiv.org/pdf/2312.04351v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04350v1",
            "title": "CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language\n  Models",
            "updated": "2023-12-07T15:12:12Z",
            "published": "2023-12-07T15:12:12Z",
            "summary": "The ability to perform causal reasoning is widely considered a core feature\nof intelligence. In this work, we investigate whether large language models\n(LLMs) can coherently reason about causality. Much of the existing work in\nnatural language processing (NLP) focuses on evaluating commonsense causal\nreasoning in LLMs, thus failing to assess whether a model can perform causal\ninference in accordance with a set of well-defined formal rules. To address\nthis, we propose a new NLP task, causal inference in natural language, inspired\nby the \"causal inference engine\" postulated by Judea Pearl et al. We compose a\nlarge dataset, CLadder, with 10K samples: based on a collection of causal\ngraphs and queries (associational, interventional, and counterfactual), we\nobtain symbolic questions and ground-truth answers, through an oracle causal\ninference engine. These are then translated into natural language. We evaluate\nmultiple LLMs on our dataset, and we introduce and evaluate a bespoke\nchain-of-thought prompting strategy, CausalCoT. We show that our task is highly\nchallenging for LLMs, and we conduct an in-depth analysis to gain deeper\ninsight into the causal reasoning abilities of LLMs. Our data is open-sourced\nat https://huggingface.co/datasets/causalNLP/cladder, and our code can be found\nat https://github.com/causalNLP/cladder.",
            "author": [
                "Zhijing Jin",
                "Yuen Chen",
                "Felix Leeb",
                "Luigi Gresele",
                "Ojasv Kamal",
                "Zhiheng Lyu",
                "Kevin Blin",
                "Fernando Gonzalez Adauto",
                "Max Kleiman-Weiner",
                "Mrinmaya Sachan",
                "Bernhard Sch\u00f6lkopf"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04350v1",
                "http://arxiv.org/pdf/2312.04350v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04348v1",
            "title": "When Input Integers are Given in the Unary Numeral Representation",
            "updated": "2023-12-07T15:09:24Z",
            "published": "2023-12-07T15:09:24Z",
            "summary": "Many NP-complete problems take integers as part of their input instances.\nThese input integers are generally binarized, that is, provided in the form of\nthe \"binary\" numeral representation, and the lengths of such binary forms are\nused as a basis unit to measure the computational complexity of the problems.\nIn sharp contrast, the \"unarization\" (or the \"unary\" numeral representation) of\nnumbers has been known to bring a remarkably different effect onto the\ncomputational complexity of the problems. When no computational-complexity\ndifference is observed between binarization and unarization of instances, on\nthe contrary, the problems are said to be strong NP-complete. This work\nattempts to spotlight an issue of how the unarization of instances affects the\ncomputational complexity of various combinatorial problems. We present numerous\nNP-complete (or even NP-hard) problems, which turn out to be easily solvable\nwhen input integers are represented in unary. We then discuss the computational\ncomplexities of such problems when taking unary-form integer inputs. We hope\nthat a list of such problems signifies the structural differences between\nstrong NP-completeness and non-strong NP-completeness.",
            "author": [
                "Tomoyuki Yamakami"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04348v1",
                "http://arxiv.org/pdf/2312.04348v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC",
                "cs.CL",
                "cs.FL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04346v1",
            "title": "Improved Efficient Two-Stage Denoising Diffusion Power System\n  Measurement Recovery Against False Data Injection Attacks and Data Losses",
            "updated": "2023-12-07T15:06:06Z",
            "published": "2023-12-07T15:06:06Z",
            "summary": "Measurement uncertainties, represented by cyber-attacks and data losses,\nseriously degrade the quality of power system measurements. Fortunately, the\npowerful generation ability of the denoising diffusion models can enable more\nprecise measurement generation for power system data recovery. However, the\ncontrollable data generation and efficient computing methods of denoising\ndiffusion models for deterministic trajectory still need further investigation.\nTo this end, this paper proposes an improved two-stage denoising diffusion\nmodel (TSDM) to identify and reconstruct the measurements with various\nmeasurement uncertainties. The first stage of the model comprises a\nclassifier-guided conditional anomaly detection component, while the second\nstage involves diffusion-based measurement imputation component. Moreover, the\nproposed TSDM adopts precise means and optimal variances to accelerate the\ndiffusion generation process with subsequence sampling. Extensive numerical\ncase studies demonstrate that the proposed TSDM can accurately recover power\nsystem measurements despite strong randomness under renewable energy\nintegration and highly nonlinear dynamics under complex cyber-physical\ncontingencies. Additionally, the proposed TSDM has stronger robustness compared\nto existing reconstruction networks and exhibits lower computational complexity\nthan general denoising diffusion models.",
            "author": [
                "Jianhua Pei",
                "Jingyu Wang",
                "Dongyuan Shi",
                "Ping Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04346v1",
                "http://arxiv.org/pdf/2312.04346v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04344v1",
            "title": "Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on\n  Prompt Engineering Strategies",
            "updated": "2023-12-07T15:05:59Z",
            "published": "2023-12-07T15:05:59Z",
            "summary": "OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piqued\nconsiderable interest for its potential in medical applications. Despite its\npromise, recent studies and internal reviews highlight its underperformance in\nspecialized medical tasks. This paper explores the boundary of GPT-4V's\ncapabilities in medicine, particularly in processing complex imaging data from\nendoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we\nassessed its foundational competencies, identifying substantial areas for\nenhancement. Our research emphasizes prompt engineering, an often-underutilized\nstrategy for improving AI responsiveness. Through iterative testing, we refined\nthe model's prompts, significantly improving its interpretative accuracy and\nrelevance in medical imaging. From our comprehensive evaluations, we distilled\n10 effective prompt engineering techniques, each fortifying GPT-4V's medical\nacumen. These methodical enhancements facilitate more reliable, precise, and\nclinically valuable insights from GPT-4V, advancing its operability in critical\nhealthcare environments. Our findings are pivotal for those employing AI in\nmedicine, providing clear, actionable guidance on harnessing GPT-4V's full\ndiagnostic potential.",
            "author": [
                "Pengcheng Chen",
                "Ziyan Huang",
                "Zhongying Deng",
                "Tianbin Li",
                "Yanzhou Su",
                "Haoyu Wang",
                "Jin Ye",
                "Yu Qiao",
                "Junjun He"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04344v1",
                "http://arxiv.org/pdf/2312.04344v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04339v1",
            "title": "Merging by Matching Models in Task Subspaces",
            "updated": "2023-12-07T14:59:15Z",
            "published": "2023-12-07T14:59:15Z",
            "summary": "Model merging aims to cheaply combine individual task-specific models into a\nsingle multitask model. In this work, we view past merging methods as\nleveraging different notions of a ''task subspace'' in which models are matched\nbefore being merged. We connect the task subspace of a given model to its loss\nlandscape and formalize how this approach to model merging can be seen as\nsolving a linear system of equations. While past work has generally been\nlimited to linear systems that have a closed-form solution, we consider using\nthe conjugate gradient method to find a solution. We show that using the\nconjugate gradient method can outperform closed-form solutions, enables merging\nvia linear systems that are otherwise intractable to solve, and flexibly allows\nchoosing from a wide variety of initializations and estimates for the ''task\nsubspace''. We ultimately demonstrate that our merging framework called\n''Matching Models in their Task Subspace'' (MaTS) achieves state-of-the-art\nresults in multitask and intermediate-task model merging. We release all of the\ncode and checkpoints used in our work at https://github.com/r-three/mats.",
            "author": [
                "Derek Tam",
                "Mohit Bansal",
                "Colin Raffel"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04339v1",
                "http://arxiv.org/pdf/2312.04339v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04337v1",
            "title": "Multi-View Unsupervised Image Generation with Cross Attention Guidance",
            "updated": "2023-12-07T14:55:13Z",
            "published": "2023-12-07T14:55:13Z",
            "summary": "The growing interest in novel view synthesis, driven by Neural Radiance Field\n(NeRF) models, is hindered by scalability issues due to their reliance on\nprecisely annotated multi-view images. Recent models address this by\nfine-tuning large text2image diffusion models on synthetic multi-view data.\nDespite robust zero-shot generalization, they may need post-processing and can\nface quality issues due to the synthetic-real domain gap. This paper introduces\na novel pipeline for unsupervised training of a pose-conditioned diffusion\nmodel on single-category datasets. With the help of pretrained self-supervised\nVision Transformers (DINOv2), we identify object poses by clustering the\ndataset through comparing visibility and locations of specific object parts.\nThe pose-conditioned diffusion model, trained on pose labels, and equipped with\ncross-frame attention at inference time ensures cross-view consistency, that is\nfurther aided by our novel hard-attention guidance. Our model, MIRAGE,\nsurpasses prior work in novel view synthesis on real images. Furthermore,\nMIRAGE is robust to diverse textures and geometries, as demonstrated with our\nexperiments on synthetic images generated with pretrained Stable Diffusion.",
            "author": [
                "Llukman Cerkezi",
                "Aram Davtyan",
                "Sepehr Sameni",
                "Paolo Favaro"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04337v1",
                "http://arxiv.org/pdf/2312.04337v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04335v1",
            "title": "The charge density and neutron skin thickness of Skyrmions",
            "updated": "2023-12-07T14:52:09Z",
            "published": "2023-12-07T14:52:09Z",
            "summary": "Motivated by recent parity violating electron scattering experiments, we\ncompute the Neutron Skin Thickness (NST) of nuclei modelled as (quantized)\nSkyrmions, the topological solitons of the Skyrme model. We show how in a\ncertain approximation, the result for the NST is oblivious to the fine details\nof the (generally very complicated) quantum state of the soliton and only\ndepends on the total baryon number and the isospin number. Moreover, in the\nleading order, the linear dependence on the asymmetry parameter is recovered,\nas expected both from experimental data and other models of nuclei such as the\nliquid drop model.",
            "author": [
                "Alberto Garc\u00eda Mart\u00edn-Caro",
                "Chris Halcrow"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04335v1",
                "http://arxiv.org/pdf/2312.04335v1"
            ],
            "primary_category": "nucl-th",
            "category": [
                "nucl-th",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04334v1",
            "title": "Towards a Perceptual Evaluation Framework for Lighting Estimation",
            "updated": "2023-12-07T14:51:12Z",
            "published": "2023-12-07T14:51:12Z",
            "summary": "Progress in lighting estimation is tracked by computing existing image\nquality assessment (IQA) metrics on images from standard datasets. While this\nmay appear to be a reasonable approach, we demonstrate that doing so does not\ncorrelate to human preference when the estimated lighting is used to relight a\nvirtual scene into a real photograph. To study this, we design a controlled\npsychophysical experiment where human observers must choose their preference\namongst rendered scenes lit using a set of lighting estimation algorithms\nselected from the recent literature, and use it to analyse how these algorithms\nperform according to human perception. Then, we demonstrate that none of the\nmost popular IQA metrics from the literature, taken individually, correctly\nrepresent human perception. Finally, we show that by learning a combination of\nexisting IQA metrics, we can more accurately represent human preference. This\nprovides a new perceptual framework to help evaluate future lighting estimation\nalgorithms.",
            "author": [
                "Justine Giroux",
                "Mohammad Reza Karimi Dastjerdi",
                "Yannick Hold-Geoffroy",
                "Javier Vazquez-Corral",
                "Jean-Fran\u00e7ois Lalonde"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04334v1",
                "http://arxiv.org/pdf/2312.04334v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04333v1",
            "title": "Beyond Surface: Probing LLaMA Across Scales and Layers",
            "updated": "2023-12-07T14:50:41Z",
            "published": "2023-12-07T14:50:41Z",
            "summary": "This paper presents an in-depth analysis of Large Language Models (LLMs),\nfocusing on LLaMA, a prominent open-source foundational model in natural\nlanguage processing. Instead of assessing LLaMA through its generative output,\nwe design multiple-choice tasks to probe its intrinsic understanding in\nhigh-order tasks such as reasoning and computation. We examine the model\nhorizontally, comparing different sizes, and vertically, assessing different\nlayers. We unveil several key and uncommon findings based on the designed\nprobing tasks: (1) Horizontally, enlarging model sizes almost could not\nautomatically impart additional knowledge or computational prowess. Instead, it\ncan enhance reasoning abilities, especially in math problem solving, and helps\nreduce hallucinations, but only beyond certain size thresholds; (2) In vertical\nanalysis, the lower layers of LLaMA lack substantial arithmetic and factual\nknowledge, showcasing logical thinking, multilingual and recognitive abilities,\nwith top layers housing most computational power and real-world knowledge.",
            "author": [
                "Nuo Chen",
                "Ning Wu",
                "Shining Liang",
                "Ming Gong",
                "Linjun Shou",
                "Dongmei Zhang",
                "Jia Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04333v1",
                "http://arxiv.org/pdf/2312.04333v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04328v1",
            "title": "A Multi-scale Information Integration Framework for Infrared and Visible\n  Image Fusion",
            "updated": "2023-12-07T14:40:05Z",
            "published": "2023-12-07T14:40:05Z",
            "summary": "Infrared and visible image fusion aims at generating a fused image containing\nthe intensity and detail information of source images, and the key issue is\neffectively measuring and integrating the complementary information of\nmulti-modality images from the same scene. Existing methods mostly adopt a\nsimple weight in the loss function to decide the information retention of each\nmodality rather than adaptively measuring complementary information for\ndifferent image pairs. In this study, we propose a multi-scale dual attention\n(MDA) framework for infrared and visible image fusion, which is designed to\nmeasure and integrate complementary information in both structure and loss\nfunction at the image and patch level. In our method, the residual downsample\nblock decomposes source images into three scales first. Then, dual attention\nfusion block integrates complementary information and generates a spatial and\nchannel attention map at each scale for feature fusion. Finally, the output\nimage is reconstructed by the residual reconstruction block. Loss function\nconsists of image-level, feature-level and patch-level three parts, of which\nthe calculation of the image-level and patch-level two parts are based on the\nweights generated by the complementary information measurement. Indeed, to\nconstrain the pixel intensity distribution between the output and infrared\nimage, a style loss is added. Our fusion results perform robust and informative\nacross different scenarios. Qualitative and quantitative results on two\ndatasets illustrate that our method is able to preserve both thermal radiation\nand detailed information from two modalities and achieve comparable results\ncompared with the other state-of-the-art methods. Ablation experiments show the\neffectiveness of our information integration architecture and adaptively\nmeasure complementary information retention in the loss function.",
            "author": [
                "Guang Yang",
                "Jie Li",
                "Hanxiao Lei",
                "Xinbo Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04328v1",
                "http://arxiv.org/pdf/2312.04328v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04327v1",
            "title": "Learning to sample in Cartesian MRI",
            "updated": "2023-12-07T14:38:07Z",
            "published": "2023-12-07T14:38:07Z",
            "summary": "Despite its exceptional soft tissue contrast, Magnetic Resonance Imaging\n(MRI) faces the challenge of long scanning times compared to other modalities\nlike X-ray radiography. Shortening scanning times is crucial in clinical\nsettings, as it increases patient comfort, decreases examination costs and\nimproves throughput. Recent advances in compressed sensing (CS) and deep\nlearning allow accelerated MRI acquisition by reconstructing high-quality\nimages from undersampled data. While reconstruction algorithms have received\nmost of the focus, designing acquisition trajectories to optimize\nreconstruction quality remains an open question. This thesis explores two\napproaches to address this gap in the context of Cartesian MRI. First, we\npropose two algorithms, lazy LBCS and stochastic LBCS, that significantly\nimprove upon G\\\"ozc\\\"u et al.'s greedy learning-based CS (LBCS) approach. These\nalgorithms scale to large, clinically relevant scenarios like multi-coil 3D MR\nand dynamic MRI, previously inaccessible to LBCS. Additionally, we demonstrate\nthat generative adversarial networks (GANs) can serve as a natural criterion\nfor adaptive sampling by leveraging variance in the measurement domain to guide\nacquisition. Second, we delve into the underlying structures or assumptions\nthat enable mask design algorithms to perform well in practice. Our experiments\nreveal that state-of-the-art deep reinforcement learning (RL) approaches, while\ncapable of adaptation and long-horizon planning, offer only marginal\nimprovements over stochastic LBCS, which is neither adaptive nor does long-term\nplanning. Altogether, our findings suggest that stochastic LBCS and similar\nmethods represent promising alternatives to deep RL. They shine in particular\nby their scalability and computational efficiency and could be key in the\ndeployment of optimized acquisition trajectories in Cartesian MRI.",
            "author": [
                "Thomas Sanchez"
            ],
            "link": [
                "http://dx.doi.org/10.5075/epfl-thesis-9981",
                "http://arxiv.org/abs/2312.04327v1",
                "http://arxiv.org/pdf/2312.04327v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04326v1",
            "title": "iDesigner: A High-Resolution and Complex-Prompt Following Text-to-Image\n  Diffusion Model for Interior Design",
            "updated": "2023-12-07T14:37:01Z",
            "published": "2023-12-07T14:37:01Z",
            "summary": "With the open-sourcing of text-to-image models (T2I) such as stable diffusion\n(SD) and stable diffusion XL (SD-XL), there is an influx of models fine-tuned\nin specific domains based on the open-source SD model, such as in anime,\ncharacter portraits, etc. However, there are few specialized models in certain\ndomains, such as interior design, which is attributed to the complex textual\ndescriptions and detailed visual elements inherent in design, alongside the\nnecessity for adaptable resolution. Therefore, text-to-image models for\ninterior design are required to have outstanding prompt-following capabilities,\nas well as iterative collaboration with design professionals to achieve the\ndesired outcome. In this paper, we collect and optimize text-image data in the\ndesign field and continue training in both English and Chinese on the basis of\nthe open-source CLIP model. We also proposed a fine-tuning strategy with\ncurriculum learning and reinforcement learning from CLIP feedback to enhance\nthe prompt-following capabilities of our approach so as to improve the quality\nof image generation. The experimental results on the collected dataset\ndemonstrate the effectiveness of the proposed approach, which achieves\nimpressive results and outperforms strong baselines.",
            "author": [
                "Ruyi Gan",
                "Xiaojun Wu",
                "Junyu Lu",
                "Yuanhe Tian",
                "Dixiang Zhang",
                "Ziwei Wu",
                "Renliang Sun",
                "Chang Liu",
                "Jiaxing Zhang",
                "Pingjian Zhang",
                "Yan Song"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04326v1",
                "http://arxiv.org/pdf/2312.04326v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04322v1",
            "title": "Estimating the Number of States via the Rodeo Algorithm for Quantum\n  Computation",
            "updated": "2023-12-07T14:29:15Z",
            "published": "2023-12-07T14:29:15Z",
            "summary": "Our proposal introduces a customization of the rodeo algorithm that enables\nus to determine the number of states associated with all energy levels of a\nquantum system without explicitly solving the Schr\\\"odinger equation. Quantum\ncomputers, with their innate ability to address the intricacies of quantum\nsystems, make this approach particularly promising for the study of the\nthermodynamics of quantum systems. To illustrate the effectiveness of our\napproach, we apply it to compute the number of states of the 1D\ntransverse-field Ising model and, consequently, its specific heat.",
            "author": [
                "Julio Cesar Siqueira Rocha",
                "Raphael Fortes Infante Gomes",
                "Wallon Anderson Tadaiesky Nogueira",
                "Rodrigo Alves Dias"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04322v1",
                "http://arxiv.org/pdf/2312.04322v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04318v1",
            "title": "MIMo: A Multi-Modal Infant Model for Studying Cognitive Development",
            "updated": "2023-12-07T14:21:31Z",
            "published": "2023-12-07T14:21:31Z",
            "summary": "Human intelligence and human consciousness emerge gradually during the\nprocess of cognitive development. Understanding this development is an\nessential aspect of understanding the human mind and may facilitate the\nconstruction of artificial minds with similar properties. Importantly, human\ncognitive development relies on embodied interactions with the physical and\nsocial environment, which is perceived via complementary sensory modalities.\nThese interactions allow the developing mind to probe the causal structure of\nthe world. This is in stark contrast to common machine learning approaches,\ne.g., for large language models, which are merely passively ``digesting'' large\namounts of training data, but are not in control of their sensory inputs.\nHowever, computational modeling of the kind of self-determined embodied\ninteractions that lead to human intelligence and consciousness is a formidable\nchallenge. Here we present MIMo, an open-source multi-modal infant model for\nstudying early cognitive development through computer simulations. MIMo's body\nis modeled after an 18-month-old child with detailed five-fingered hands. MIMo\nperceives its surroundings via binocular vision, a vestibular system,\nproprioception, and touch perception through a full-body virtual skin, while\ntwo different actuation models allow control of his body. We describe the\ndesign and interfaces of MIMo and provide examples illustrating its use. All\ncode is available at https://github.com/trieschlab/MIMo .",
            "author": [
                "Dominik Mattern",
                "Pierre Schumacher",
                "Francisco M. L\u00f3pez",
                "Marcel C. Raabe",
                "Markus R. Ernst",
                "Arthur Aubret",
                "Jochen Triesch"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04318v1",
                "http://arxiv.org/pdf/2312.04318v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04316v1",
            "title": "Towards Knowledge-driven Autonomous Driving",
            "updated": "2023-12-07T14:17:17Z",
            "published": "2023-12-07T14:17:17Z",
            "summary": "This paper explores the emerging knowledge-driven autonomous driving\ntechnologies. Our investigation highlights the limitations of current\nautonomous driving systems, in particular their sensitivity to data bias,\ndifficulty in handling long-tail scenarios, and lack of interpretability.\nConversely, knowledge-driven methods with the abilities of cognition,\ngeneralization and life-long learning emerge as a promising way to overcome\nthese challenges. This paper delves into the essence of knowledge-driven\nautonomous driving and examines its core components: dataset \\& benchmark,\nenvironment, and driver agent. By leveraging large language models, world\nmodels, neural rendering, and other advanced artificial intelligence\ntechniques, these components collectively contribute to a more holistic,\nadaptive, and intelligent autonomous driving system. The paper systematically\norganizes and reviews previous research efforts in this area, and provides\ninsights and guidance for future research and practical applications of\nautonomous driving. We will continually share the latest updates on\ncutting-edge developments in knowledge-driven autonomous driving along with the\nrelevant valuable open-source resources at:\n\\url{https://github.com/PJLab-ADG/awesome-knowledge-driven-AD}.",
            "author": [
                "Xin Li",
                "Yeqi Bai",
                "Pinlong Cai",
                "Licheng Wen",
                "Daocheng Fu",
                "Bo Zhang",
                "Xuemeng Yang",
                "Xinyu Cai",
                "Tao Ma",
                "Jianfei Guo",
                "Xing Gao",
                "Min Dou",
                "Botian Shi",
                "Yong Liu",
                "Liang He",
                "Yu Qiao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04316v1",
                "http://arxiv.org/pdf/2312.04316v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04315v1",
            "title": "The lattice extraction of the TMD soft function using the auxiliary\n  field representation of the Wilson line",
            "updated": "2023-12-07T14:16:06Z",
            "published": "2023-12-07T14:16:06Z",
            "summary": "The TMD soft function can be obtained by formulating the Wilson line in terms\nof auxiliary 1-dimensional fermion fields on the lattice. In this formulation,\nthe directional vector of the auxiliary field in Euclidean space has the form\n$\\tilde n = (in^0, \\vec 0_\\perp, n^3)$, where the time component is purely\nimaginary. The components of these complex directional vectors in the Euclidean\nspace can be mapped directly to the rapidities of the Minkowski space soft\nfunction. We present the results of the one-loop calculation of the Euclidean\nspace analog to the soft function using these complex directional vectors. As a\nresult, we show that the calculation is valid only when the directional vectors\nobey the relation: $|r| = |n^3/n^0| > 1$, and that this result corresponds to a\ncomputation in Minkowski space with space-like directed Wilson lines. Finally,\nwe show that a lattice calculable object can be constructed that has the\ndesired properties of the soft function.",
            "author": [
                "Anthony Francis",
                "Issaku Kanamori",
                "C. -J. David Lin",
                "Wayne Morris",
                "Yong Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04315v1",
                "http://arxiv.org/pdf/2312.04315v1"
            ],
            "primary_category": "hep-lat",
            "category": [
                "hep-lat",
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04314v1",
            "title": "GPT4SGG: Synthesizing Scene Graphs from Holistic and Region-specific\n  Narratives",
            "updated": "2023-12-07T14:11:00Z",
            "published": "2023-12-07T14:11:00Z",
            "summary": "Learning scene graphs from natural language descriptions has proven to be a\ncheap and promising scheme for Scene Graph Generation (SGG). However, such\nunstructured caption data and its processing are troubling the learning an\nacurrate and complete scene graph. This dilema can be summarized as three\npoints. First, traditional language parsers often fail to extract meaningful\nrelationship triplets from caption data. Second, grounding unlocalized objects\nin parsed triplets will meet ambiguity in visual-language alignment. Last,\ncaption data typically are sparse and exhibit bias to partial observations of\nimage content. These three issues make it hard for the model to generate\ncomprehensive and accurate scene graphs. To fill this gap, we propose a simple\nyet effective framework, GPT4SGG, to synthesize scene graphs from holistic and\nregion-specific narratives. The framework discards traditional language parser,\nand localize objects before obtaining relationship triplets. To obtain\nrelationship triplets, holistic and dense region-specific narratives are\ngenerated from the image. With such textual representation of image data and a\ntask-specific prompt, an LLM, particularly GPT-4, directly synthesizes a scene\ngraph as \"pseudo labels\". Experimental results showcase GPT4SGG significantly\nimproves the performance of SGG models trained on image-caption data. We\nbelieve this pioneering work can motivate further research into mining the\nvisual reasoning capabilities of LLMs.",
            "author": [
                "Zuyao Chen",
                "Jinlin Wu",
                "Zhen Lei",
                "Zhaoxiang Zhang",
                "Changwen Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04314v1",
                "http://arxiv.org/pdf/2312.04314v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04306v1",
            "title": "nerblackbox: A High-level Library for Named Entity Recognition in Python",
            "updated": "2023-12-07T14:04:15Z",
            "published": "2023-12-07T14:04:15Z",
            "summary": "We present nerblackbox, a python library to facilitate the use of\nstate-of-the-art transformer-based models for named entity recognition. It\nprovides simple-to-use yet powerful methods to access data and models from a\nwide range of sources, for fully automated model training and evaluation as\nwell as versatile model inference. While many technical challenges are solved\nand hidden from the user by default, nerblackbox also offers fine-grained\ncontrol and a rich set of customizable features. It is thus targeted both at\napplication-oriented developers as well as machine learning experts and\nresearchers.",
            "author": [
                "Felix Stollenwerk"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04306v1",
                "http://arxiv.org/pdf/2312.04306v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04302v1",
            "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
            "updated": "2023-12-07T13:53:29Z",
            "published": "2023-12-07T13:53:29Z",
            "summary": "This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs)\ninference: explicit controllable text generation. Multi-modal LLMs empower\nmulti-modality understanding with the capability of semantic generation yet\nbring less explainability and heavier reliance on prompt contents due to their\nautoregressive generative nature. While manipulating prompt formats could\nimprove outputs, designing specific and precise prompts per task can be\nchallenging and ineffective. To tackle this issue, we introduce a novel\ninference method, Prompt Highlighter, which enables users to highlight specific\nprompt spans to interactively control the focus during generation. Motivated by\nthe classifier-free diffusion guidance, we form regular and unconditional\ncontext pairs based on highlighted tokens, demonstrating that the\nautoregressive generation in models can be guided in a classifier-free way.\nNotably, we find that, during inference, guiding the models with highlighted\ntokens through the attention weights leads to more desired outputs. Our\napproach is compatible with current LLMs and VLMs, achieving impressive\ncustomized generation results without training. Experiments confirm its\neffectiveness in focusing on input contexts and generating reliable content.\nWithout tuning on LLaVA-v1.5, our method secured 69.5 in the MMBench test and\n1552.5 in MME-perception. The code is available at:\nhttps://github.com/dvlab-research/Prompt-Highlighter/",
            "author": [
                "Yuechen Zhang",
                "Shengju Qian",
                "Bohao Peng",
                "Shu Liu",
                "Jiaya Jia"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04302v1",
                "http://arxiv.org/pdf/2312.04302v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04296v1",
            "title": "Cross-codex Learning for Reliable Scribe Identification in Medieval\n  Manuscripts",
            "updated": "2023-12-07T13:40:20Z",
            "published": "2023-12-07T13:40:20Z",
            "summary": "Historic scribe identification is a substantial task for obtaining\ninformation about the past. Uniform script styles, such as the Carolingian\nminuscule, make it a difficult task for classification to focus on meaningful\nfeatures. Therefore, we demonstrate in this paper the importance of cross-codex\ntraining data for CNN based text-independent off-line scribe identification, to\novercome codex dependent overfitting. We report three main findings: First, we\nfound that preprocessing with masked grayscale images instead of RGB images\nclearly increased the F1-score of the classification results. Second, we\ntrained different neural networks on our complex data, validating time and\naccuracy differences in order to define the most reliable network architecture.\nWith AlexNet, the network with the best trade-off between F1-score and time, we\nachieved for individual classes F1-scores of up to 0,96 on line level and up to\n1.0 on page level in classification. Third, we could replicate the finding that\nthe CNN output can be further improved by implementing a reject option, giving\nmore stable results. We present the results on our large scale open source\ndataset -- the Codex Claustroneoburgensis database (CCl-DB) -- containing a\nsignificant number of writings from different scribes in several codices. We\ndemonstrate for the first time on a dataset with such a variety of codices that\npaleographic decisions can be reproduced automatically and precisely with CNNs.\nThis gives manifold new and fast possibilities for paleographers to gain\ninsights into unlabeled material, but also to develop further hypotheses.",
            "author": [
                "Julius Wei\u00dfmann",
                "Markus Seidl",
                "Anya Dietrich",
                "Martin Haltrich"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04296v1",
                "http://arxiv.org/pdf/2312.04296v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "I.4.9; I.5.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04293v1",
            "title": "GPT-4V with Emotion: A Zero-shot Benchmark for Multimodal Emotion\n  Understanding",
            "updated": "2023-12-07T13:27:37Z",
            "published": "2023-12-07T13:27:37Z",
            "summary": "Recently, GPT-4 with Vision (GPT-4V) has shown remarkable performance across\nvarious multimodal tasks. However, its efficacy in emotion recognition remains\na question. This paper quantitatively evaluates GPT-4V's capabilities in\nmultimodal emotion understanding, encompassing tasks such as facial emotion\nrecognition, visual sentiment analysis, micro-expression recognition, dynamic\nfacial emotion recognition, and multimodal emotion recognition. Our experiments\nshow that GPT-4V exhibits impressive multimodal and temporal understanding\ncapabilities, even surpassing supervised systems in some tasks. Despite these\nachievements, GPT-4V is currently tailored for general domains. It performs\npoorly in micro-expression recognition that requires specialized expertise. The\nmain purpose of this paper is to present quantitative results of GPT-4V on\nemotion understanding and establish a zero-shot benchmark for future research.\nCode and evaluation results are available at:\nhttps://github.com/zeroQiaoba/gpt4v-emotion.",
            "author": [
                "Zheng Lian",
                "Licai Sun",
                "Haiyang Sun",
                "Kang Chen",
                "Zhuofan Wen",
                "Hao Gu",
                "Shun Chen",
                "Bin Liu",
                "Jianhua Tao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04293v1",
                "http://arxiv.org/pdf/2312.04293v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04290v1",
            "title": "Convergence Analysis of Opto-Electronic Oscillator based Coherent Ising\n  Machines",
            "updated": "2023-12-07T13:18:13Z",
            "published": "2023-12-07T13:18:13Z",
            "summary": "Ising machines are purported to be better at solving large-scale\ncombinatorial optimisation problems better than conventional von Neumann\ncomputers. However, these Ising machines are widely believed to be heuristics,\nwhose promise is observed empirically rather than obtained theoretically. We\nbridge this gap by considering an opto-electronic oscillator based coherent\nIsing machine, and providing the first analytical proof that under reasonable\nassumptions, the OEO-CIM is not a heuristic approach. We find and prove bounds\non its performance in terms of the expected difference between the objective\nvalue at the final iteration and the optimal one, and on the number of\niterations required by it. In the process, we emphasise on some of its\nlimitations such as the inability to handle asymmetric coupling between spins,\nand the absence of external magnetic field applied on them (both of which are\nnecessary in many optimisation problems), along with some issues in its\nconvergence. We overcome these limitations by proposing suitable adjustments\nand prove that the improved architecture is guaranteed to converge to the\noptimum of the relaxed objective function.",
            "author": [
                "Sayantan Pramanik",
                "Sourav Chatterjee",
                "Harshkumar Oza"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04290v1",
                "http://arxiv.org/pdf/2312.04290v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04289v1",
            "title": "Fast simulation of airfoil flow field via deep neural network",
            "updated": "2023-12-07T13:16:02Z",
            "published": "2023-12-07T13:16:02Z",
            "summary": "Computational Fluid Dynamics (CFD) has become an indispensable tool in the\noptimization design, and evaluation of aircraft aerodynamics. However, solving\nthe Navier-Stokes (NS) equations is a time-consuming, memory demanding and\ncomputationally expensive task. Artificial intelligence offers a promising\navenue for flow field solving. In this work, we propose a novel deep learning\nframework for rapidly reconstructing airfoil flow fields. Channel attention and\nspatial attention modules are utilized in the downsampling stage of the UNet to\nenhance the feature learning capabilities of the deep learning model.\nAdditionally, integrating the predicted flow field values generated by the deep\nlearning model into the NS equation solver validates the credibility of the\nflow field prediction results. The NACA series airfoils were used to validate\nthe prediction accuracy and generalization of the deep learning model. The\nexperimental results represent the deep learning model achieving flow field\nprediction speeds three orders of magnitude faster than CFD solver.\nFurthermore, the CFD solver integrated with deep learning model demonstrates a\nthreefold acceleration compared to CFD solver. By extensively mining historical\nflow field data, an efficient solution is derived for the rapid simulation of\naircraft flow fields.",
            "author": [
                "Kuijun Zuo",
                "Zhengyin Ye",
                "Shuhui Bu",
                "Xianxu Yuan",
                "Weiwei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04289v1",
                "http://arxiv.org/pdf/2312.04289v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04288v1",
            "title": "Shadows of Loop Quantum Black Holes: Semi-analytical Simulations of Loop\n  Quantum Gravity Effects on Sagittarius A* and M 87*",
            "updated": "2023-12-07T13:13:58Z",
            "published": "2023-12-07T13:13:58Z",
            "summary": "In this study, we delve into the observational implications of rotating Loop\nQuantum Black Holes (LQBHs) within an astrophysical framework. We employ\nsemi-analytical General Relativistic Radiative Transfer (GRRT) computations to\nstudy the emission from the accretion flow around LQBHs. Our findings indicate\nthat the increase of Loop Quantum Gravity (LQG) effects results in an\nenlargement of the rings from LQBHs, thereby causing a more circular\npolarization pattern in the shadow images. We make comparisons with the Event\nHorizon Telescope (EHT) observations of Sgr\\,A$^*$ and M\\,87$^*$, which enable\nus to determine an upper limit for the polymetric function $P$ in LQG. The\nupper limit for Sgr\\,A$^*$ is $0.2$, while for M\\,87$^*$ it is $0.07$. Both\nblack holes exhibit a preference for a relatively high spin ($a\\gtrsim0.5$ for\nSgr\\,A$^*$ and $0.5\\lesssim a \\lesssim 0.7$ for M\\,87$^*$). The constraints for\nSgr\\,A$^*$ are based on black hole spin and ring diameter, whereas for\nM\\,87$^*$, the constraints are further tightened by the polarimetric pattern.\nIn essence, our simulations provide observational constraints on the effect of\nLQG in supermassive black holes (SMBH), providing the most consistent\ncomparison with observation.",
            "author": [
                "Hong-Xuan Jiang",
                "Cheng Liu",
                "Indu K. Dihingia",
                "Yosuke Mizuno",
                "Haiguang Xu",
                "Tao Zhu",
                "Qiang Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04288v1",
                "http://arxiv.org/pdf/2312.04288v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc",
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04279v1",
            "title": "MSEVA : A System for Multimodal Short Videos Emotion Visual Analysis",
            "updated": "2023-12-07T12:59:38Z",
            "published": "2023-12-07T12:59:38Z",
            "summary": "YouTube Shorts, a new section launched by YouTube in 2021, is a direct\ncompetitor to short video platforms like TikTok. It reflects the rising demand\nfor short video content among online users. Social media platforms are often\nflooded with short videos that capture different perspectives and emotions on\nhot events. These videos can go viral and have a significant impact on the\npublic's mood and views. However, short videos' affective computing was a\nneglected area of research in the past. Monitoring the public's emotions\nthrough these videos requires a lot of time and effort, which may not be enough\nto prevent undesirable outcomes. In this paper, we create the first multimodal\ndataset of short video news covering hot events. We also propose an automatic\ntechnique for audio segmenting and transcribing. In addition, we improve the\naccuracy of the multimodal affective computing model by about 4.17% by\noptimizing it. Moreover, a novel system MSEVA for emotion analysis of short\nvideos is proposed. Achieving good results on the bili-news dataset, the MSEVA\nsystem applies the multimodal emotion analysis method in the real world. It is\nhelpful to conduct timely public opinion guidance and stop the spread of\nnegative emotions. Data and code from our investigations can be accessed at:\nhttp://xxx.github.com.",
            "author": [
                "Qinglan Wei",
                "Yaqi Zhou",
                "Yuan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04279v1",
                "http://arxiv.org/pdf/2312.04279v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04275v1",
            "title": "Estimating Countries with Similar Maternal Mortality Rate using Cluster\n  Analysis and Pairing Countries with Identical MMR",
            "updated": "2023-12-07T12:54:16Z",
            "published": "2023-12-07T12:54:16Z",
            "summary": "In the evolving world, we require more additionally the young era to flourish\nand evolve into developed land. Most of the population all around the world are\nunaware of the complications involved in the routine they follow while they are\npregnant and how hospital facilities affect maternal health. Maternal Mortality\nis the death of a pregnant woman due to intricacies correlated to pregnancy,\nunderlying circumstances exacerbated by the pregnancy or management of these\nsituations. It is crucial to consider the Maternal Mortality Rate (MMR) in\ndiverse locations and determine which human routines and hospital facilities\ndiminish the Maternal Mortality Rate (MMR). This research aims to examine and\ndiscover the countries which are keeping more lavish threats of MMR and\ncountries alike in MMR encountered. Data is examined and collected for various\ncountries, data consists of the earlier years' observation. From the\nperspective of Machine Learning, Unsupervised Machine Learning is implemented\nto perform Cluster Analysis. Therefore the pairs of countries with similar MMR\nas well as the extreme opposite pair concerning the MMR are found.",
            "author": [
                "S. Nandini",
                "Sanjjushri Varshini R"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04275v1",
                "http://arxiv.org/pdf/2312.04275v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04266v1",
            "title": "Activity Grammars for Temporal Action Segmentation",
            "updated": "2023-12-07T12:45:33Z",
            "published": "2023-12-07T12:45:33Z",
            "summary": "Sequence prediction on temporal data requires the ability to understand\ncompositional structures of multi-level semantics beyond individual and\ncontextual properties. The task of temporal action segmentation, which aims at\ntranslating an untrimmed activity video into a sequence of action segments,\nremains challenging for this reason. This paper addresses the problem by\nintroducing an effective activity grammar to guide neural predictions for\ntemporal action segmentation. We propose a novel grammar induction algorithm\nthat extracts a powerful context-free grammar from action sequence data. We\nalso develop an efficient generalized parser that transforms frame-level\nprobability distributions into a reliable sequence of actions according to the\ninduced grammar with recursive rules. Our approach can be combined with any\nneural network for temporal action segmentation to enhance the sequence\nprediction and discover its compositional structure. Experimental results\ndemonstrate that our method significantly improves temporal action segmentation\nin terms of both performance and interpretability on two standard benchmarks,\nBreakfast and 50 Salads.",
            "author": [
                "Dayoung Gong",
                "Joonseok Lee",
                "Deunsol Jung",
                "Suha Kwak",
                "Minsu Cho"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04266v1",
                "http://arxiv.org/pdf/2312.04266v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04265v1",
            "title": "Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for\n  Domain Generalized Semantic Segmentation",
            "updated": "2023-12-07T12:43:00Z",
            "published": "2023-12-07T12:43:00Z",
            "summary": "In this paper, we first assess and harness various Vision Foundation Models\n(VFMs) in the context of Domain Generalized Semantic Segmentation (DGSS).\nDriven by the motivation that Leveraging Stronger pre-trained models and Fewer\ntrainable parameters for Superior generalizability, we introduce a robust\nfine-tuning approach, namely Rein, to parameter-efficiently harness VFMs for\nDGSS. Built upon a set of trainable tokens, each linked to distinct instances,\nRein precisely refines and forwards the feature maps from each layer to the\nnext layer within the backbone. This process produces diverse refinements for\ndifferent categories within a single image. With fewer trainable parameters,\nRein efficiently fine-tunes VFMs for DGSS tasks, surprisingly surpassing full\nparameter fine-tuning. Extensive experiments across various settings\ndemonstrate that Rein significantly outperforms state-of-the-art methods.\nRemarkably, with just an extra 1% of trainable parameters within the frozen\nbackbone, Rein achieves a mIoU of 68.1% on the Cityscapes, without accessing\nany real urban-scene datasets.",
            "author": [
                "Zhixiang Wei",
                "Lin Chen",
                "Yi Jin",
                "Xiaoxiao Ma",
                "Tianle Liu",
                "Pengyang Lin",
                "Ben Wang",
                "Huaian Chen",
                "Jinjin Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04265v1",
                "http://arxiv.org/pdf/2312.04265v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04264v1",
            "title": "Multi-agricultural Machinery Collaborative Task Assignment Based on\n  Improved Genetic Hybrid Optimization Algorithm",
            "updated": "2023-12-07T12:42:40Z",
            "published": "2023-12-07T12:42:40Z",
            "summary": "To address the challenges of delayed scheduling information, heavy reliance\non manual labour, and low operational efficiency in traditional large-scale\nagricultural machinery operations, this study proposes a method for\nmulti-agricultural machinery collaborative task assignment based on an improved\ngenetic hybrid optimisation algorithm. The proposed method establishes a\nmulti-agricultural machinery task allocation model by combining the path\npre-planning of a simulated annealing algorithm and the static task allocation\nof a genetic algorithm. By sequentially fusing these two algorithms, their\nrespective shortcomings can be overcome, and their advantages in global and\nlocal search can be utilised. Consequently, the search capability of the\npopulation is enhanced, leading to the discovery of more optimal solutions.\nThen, an adaptive crossover operator is constructed according to the task\nassignment model, considering the capacity, path cost, and time of agricultural\nmachinery; two-segment coding and multi-population adaptive mutation are used\nto assign tasks to improve the diversity of the population and enhance the\nexploration ability of the population; and to improve the global optimisation\nability of the hybrid algorithm, a 2-Opt local optimisation operator and an\nCircle modification algorithm are introduced. Finally, simulation experiments\nwere conducted in MATLAB to evaluate the performance of the multi-agricultural\nmachinery collaborative task assignment based on the improved genetic hybrid\nalgorithm. The algorithm's capabilities were assessed through comparative\nanalysis in the simulation trials. The results demonstrate that the developed\nhybrid algorithm can effectively reduce path costs, and the efficiency of the\nassignment outcomes surpasses that of the classical genetic algorithm. This\napproach proves particularly suitable for addressing large-scale task\nallocation problems.",
            "author": [
                "Haohao Du"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04264v1",
                "http://arxiv.org/pdf/2312.04264v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04262v1",
            "title": "PsyChat: A Client-Centric Dialogue System for Mental Health Support",
            "updated": "2023-12-07T12:40:00Z",
            "published": "2023-12-07T12:40:00Z",
            "summary": "Dialogue systems are increasingly integrated into mental health support to\nhelp clients facilitate exploration, gain insight, take action, and ultimately\nheal themselves. For a dialogue system to be practical and user-friendly, it\nshould be client-centric, focusing on the client's behaviors. However, existing\ndialogue systems publicly available for mental health support often concentrate\nsolely on the counselor's strategies rather than the behaviors expressed by\nclients. This can lead to the implementation of unreasonable or inappropriate\ncounseling strategies and corresponding responses from the dialogue system. To\naddress this issue, we propose PsyChat, a client-centric dialogue system that\nprovides psychological support through online chat. The client-centric dialogue\nsystem comprises five modules: client behavior recognition, counselor strategy\nselection, input packer, response generator intentionally fine-tuned to produce\nresponses, and response selection. Both automatic and human evaluations\ndemonstrate the effectiveness and practicality of our proposed dialogue system\nfor real-life mental health support. Furthermore, we employ our proposed\ndialogue system to simulate a real-world client-virtual-counselor interaction\nscenario. The system is capable of predicting the client's behaviors, selecting\nappropriate counselor strategies, and generating accurate and suitable\nresponses, as demonstrated in the scenario.",
            "author": [
                "Huachuan Qiu",
                "Anqi Li",
                "Lizhi Ma",
                "Zhenzhong Lan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04262v1",
                "http://arxiv.org/pdf/2312.04262v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04257v1",
            "title": "Proxima: Near-storage Acceleration for Graph-based Approximate Nearest\n  Neighbor Search in 3D NAND",
            "updated": "2023-12-07T12:32:18Z",
            "published": "2023-12-07T12:32:18Z",
            "summary": "Approximate nearest neighbor search (ANNS) plays an indispensable role in a\nwide variety of applications, including recommendation systems, information\nretrieval, and semantic search. Among the cutting-edge ANNS algorithms,\ngraph-based approaches provide superior accuracy and scalability on massive\ndatasets. However, the best-performing graph-based ANN search solutions incur\ntens of hundreds of memory footprints as well as costly distance computation,\nthus hindering their efficient deployment at scale. The 3D NAND flash is\nemerging as a promising device for data-intensive applications due to its high\ndensity and nonvolatility. In this work, we present the near-storage processing\n(NSP)-based ANNS solution Proxima, to accelerate graph-based ANNS with\nalgorithm-hardware co-design in 3D NAND flash. Proxima significantly reduces\nthe complexity of graph search by leveraging the distance approximation and\nearly termination. On top of the algorithmic enhancement, we implement Proxima\nsearch algorithm in 3D NAND flash using the heterogeneous integration\ntechnique. To maximize 3D NAND's bandwidth utilization, we present customized\ndataflow and optimized data allocation scheme. Our evaluation results show\nthat: compared to graph ANNS on CPU and GPU, Proxima achieves a magnitude\nimprovement in throughput or energy efficiency. Proxima yields 7x to 13x\nspeedup over existing ASIC designs. Furthermore, Proxima achieves a good\nbalance between accuracy, efficiency and storage density compared to previous\nNSP-based accelerators.",
            "author": [
                "Weihong Xu",
                "Junwei Chen",
                "Po-Kai Hsu",
                "Jaeyoung Kang",
                "Minxuan Zhou",
                "Sumukh Pinge",
                "Shimeng Yu",
                "Tajana Rosing"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04257v1",
                "http://arxiv.org/pdf/2312.04257v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04251v1",
            "title": "Accurate Linear Cutting-Plane Relaxations for ACOPF",
            "updated": "2023-12-07T12:20:34Z",
            "published": "2023-12-07T12:20:34Z",
            "summary": "We present a pure linear cutting-plane relaxation approach for rapidly\nproving tight lower bounds for the Alternating Current Optimal Power Flow\nProblem (ACOPF). Our method leverages outer-envelope linear cuts for well-known\nsecond-order cone relaxations for ACOPF together with modern cut management\ntechniques. These techniques prove effective on a broad family of ACOPF\ninstances, including the largest ones publicly available, quickly and robustly\nyielding sharp bounds. Additionally, we consider the (frequent) case where an\nACOPF instance is handled following a small or moderate change in problem data,\ne.g., load changes and generator or branch shut-offs. We provide significant\ncomputational evidence that the cuts computed on the prior instance provide\nvery good lower bounds when warm-starting our algorithm on the perturbed\ninstance.",
            "author": [
                "Daniel Bienstock",
                "Matias Villagra"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04251v1",
                "http://arxiv.org/pdf/2312.04251v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04249v1",
            "title": "Extending Answer Set Programming with Rational Numbers",
            "updated": "2023-12-07T12:11:25Z",
            "published": "2023-12-07T12:11:25Z",
            "summary": "Answer Set Programming (ASP) is a widely used declarative programming\nparadigm that has shown great potential in solving complex computational\nproblems. However, the inability to natively support non-integer arithmetic has\nbeen highlighted as a major drawback in real-world applications. This feature\nis crucial to accurately model and manage real-world data and information as\nemerged in various contexts, such as the smooth movement of video game\ncharacters, the 3D movement of mechanical arms, and data streamed by sensors.\nNevertheless, extending ASP in this direction, without affecting its\ndeclarative nature and its well-defined semantics, poses non-trivial\nchallenges; thus, no ASP system is able to reason natively with non-integer\ndomains. Indeed, the widespread floating-point arithmetic is not applicable to\nthe ASP case, as the reproducibility of results cannot be guaranteed and the\nsemantics of an ASP program would not be uniquely and declaratively determined,\nregardless of the employed machine or solver. To overcome such limitations and\nin the realm of pure ASP, this paper proposes an extension of ASP in which\nnon-integers are approximated to rational numbers, fully granting\nreproducibility and declarativity. We provide a well-defined semantics for the\nASP-Core-2 standard extended with rational numbers and an implementation\nthereof. We hope this work could serve as a stepping stone towards a more\nexpressive and versatile ASP language that can handle a broader range of\nreal-world problems.",
            "author": [
                "Francesco Pacenza",
                "Jessica Zangari"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04249v1",
                "http://arxiv.org/pdf/2312.04249v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LO",
                "cs.PL",
                "cs.SE",
                "D.1.6; D.3.1; D.3.3; I.2.4; I.2.5"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04248v1",
            "title": "TeMO: Towards Text-Driven 3D Stylization for Multi-Object Meshes",
            "updated": "2023-12-07T12:10:05Z",
            "published": "2023-12-07T12:10:05Z",
            "summary": "Recent progress in the text-driven 3D stylization of a single object has been\nconsiderably promoted by CLIP-based methods. However, the stylization of\nmulti-object 3D scenes is still impeded in that the image-text pairs used for\npre-training CLIP mostly consist of an object. Meanwhile, the local details of\nmultiple objects may be susceptible to omission due to the existing supervision\nmanner primarily relying on coarse-grained contrast of image-text pairs. To\novercome these challenges, we present a novel framework, dubbed TeMO, to parse\nmulti-object 3D scenes and edit their styles under the contrast supervision at\nmultiple levels. We first propose a Decoupled Graph Attention (DGA) module to\ndistinguishably reinforce the features of 3D surface points. Particularly, a\ncross-modal graph is constructed to align the object points accurately and noun\nphrases decoupled from the 3D mesh and textual description. Then, we develop a\nCross-Grained Contrast (CGC) supervision system, where a fine-grained loss\nbetween the words in the textual description and the randomly rendered images\nare constructed to complement the coarse-grained loss. Extensive experiments\nshow that our method can synthesize high-quality stylized content and\noutperform the existing methods over a wide range of multi-object 3D meshes.\nOur code and results will be made publicly available",
            "author": [
                "Xuying Zhang",
                "Bo-Wen Yin",
                "Yuming Chen",
                "Zheng Lin",
                "Yunheng Li",
                "Qibin Hou",
                "Ming-Ming Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04248v1",
                "http://arxiv.org/pdf/2312.04248v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04245v1",
            "title": "Mastering Complex Coordination through Attention-based Dynamic Graph",
            "updated": "2023-12-07T12:02:14Z",
            "published": "2023-12-07T12:02:14Z",
            "summary": "The coordination between agents in multi-agent systems has become a popular\ntopic in many fields. To catch the inner relationship between agents, the graph\nstructure is combined with existing methods and improves the results. But in\nlarge-scale tasks with numerous agents, an overly complex graph would lead to a\nboost in computational cost and a decline in performance. Here we present\nDAGMIX, a novel graph-based value factorization method. Instead of a complete\ngraph, DAGMIX generates a dynamic graph at each time step during training, on\nwhich it realizes a more interpretable and effective combining process through\nthe attention mechanism. Experiments show that DAGMIX significantly outperforms\nprevious SOTA methods in large-scale scenarios, as well as achieving promising\nresults on other tasks.",
            "author": [
                "Guangchong Zhou",
                "Zhiwei Xu",
                "Zeren Zhang",
                "Guoliang Fan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04245v1",
                "http://arxiv.org/pdf/2312.04245v1"
            ],
            "primary_category": "cs.MA",
            "category": [
                "cs.MA",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04239v1",
            "title": "A perturbative construction of primitive forms from log Landau-Ginzburg\n  mirrors of toric manifolds",
            "updated": "2023-12-07T11:52:07Z",
            "published": "2023-12-07T11:52:07Z",
            "summary": "We introduce the notion of a logarithmic Landau-Ginzburg (log LG) model,\nwhich is essentially given by equipping the central degenerate fiber of the\nfamily of Landau-Ginzburg (LG) models mirror to a projective toric manifold\nwith a natural log structure. We show that the state space of the mirror log LG\nmodel is naturally isomorphic to that of the original toric manifold. Following\nLi-Li-Saito, we give a perturbative construction of primitive forms by studying\nthe deformation theory of such a log LG model, which involves both smoothing of\nthe central degenerate fiber and unfolding of the superpotential. This yields a\nlogarithmic Frobenius manifold structure on the base space of the universal\nunfolding. The primitive forms and flat coordinates we obtained are computable\nand closely related to the bulk-deformed Lagrangian Floer superpotential of a\nprojective toric manifold, at least in the semi-Fano case.",
            "author": [
                "Kwokwai Chan",
                "Ziming Nikolas Ma",
                "Hao Wen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04239v1",
                "http://arxiv.org/pdf/2312.04239v1"
            ],
            "primary_category": "math.AG",
            "category": [
                "math.AG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04238v1",
            "title": "Long-lived Particles Anomaly Detection with Parametrized Quantum\n  Circuits",
            "updated": "2023-12-07T11:50:42Z",
            "published": "2023-12-07T11:50:42Z",
            "summary": "We investigate the possibility to apply quantum machine learning techniques\nfor data analysis, with particular regard to an interesting use-case in\nhigh-energy physics. We propose an anomaly detection algorithm based on a\nparametrized quantum circuit. This algorithm has been trained on a classical\ncomputer and tested with simulations as well as on real quantum hardware. Tests\non NISQ devices have been performed with IBM quantum computers. For the\nexecution on quantum hardware specific hardware driven adaptations have been\ndevised and implemented. The quantum anomaly detection algorithm is able to\ndetect simple anomalies like different characters in handwritten digits as well\nas more complex structures like anomalous patterns in the particle detectors\nproduced by the decay products of long-lived particles produced at a collider\nexperiment. For the high-energy physics application, performance is estimated\nin simulation only, as the quantum circuit is not simple enough to be executed\non the available quantum hardware. This work demonstrates that it is possible\nto perform anomaly detection with quantum algorithms, however, as amplitude\nencoding of classical data is required for the task, due to the noise level in\nthe available quantum hardware, current implementation cannot outperform\nclassic anomaly detection algorithms based on deep neural networks.",
            "author": [
                "Simone Bordoni",
                "Denis Stanev",
                "Tommaso Santantonio",
                "Stefano Giagu"
            ],
            "link": [
                "http://dx.doi.org/10.3390/particles6010016",
                "http://arxiv.org/abs/2312.04238v1",
                "http://arxiv.org/pdf/2312.04238v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04236v1",
            "title": "Detecting and Restoring Non-Standard Hands in Stable Diffusion Generated\n  Images",
            "updated": "2023-12-07T11:41:26Z",
            "published": "2023-12-07T11:41:26Z",
            "summary": "We introduce a pipeline to address anatomical inaccuracies in Stable\nDiffusion generated hand images. The initial step involves constructing a\nspecialized dataset, focusing on hand anomalies, to train our models\neffectively. A finetuned detection model is pivotal for precise identification\nof these anomalies, ensuring targeted correction. Body pose estimation aids in\nunderstanding hand orientation and positioning, crucial for accurate anomaly\ncorrection. The integration of ControlNet and InstructPix2Pix facilitates\nsophisticated inpainting and pixel-level transformation, respectively. This\ndual approach allows for high-fidelity image adjustments. This comprehensive\napproach ensures the generation of images with anatomically accurate hands,\nclosely resembling real-world appearances. Our experimental results demonstrate\nthe pipeline's efficacy in enhancing hand image realism in Stable Diffusion\noutputs. We provide an online demo at https://fixhand.yiqun.io",
            "author": [
                "Yiqun Zhang",
                "Zhenyue Qin",
                "Yang Liu",
                "Dylan Campbell"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04236v1",
                "http://arxiv.org/pdf/2312.04236v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04235v1",
            "title": "Distances and shortest paths on graphs of bounded highway dimension:\n  simple, fast, dynamic",
            "updated": "2023-12-07T11:41:07Z",
            "published": "2023-12-07T11:41:07Z",
            "summary": "Dijkstra's algorithm is the standard method for computing shortest paths on\narbitrary graphs. However, it is slow for large graphs, taking at least linear\ntime. It has been long known that for real world road networks, creating a\nhierarchy of well-chosen shortcuts allows fast distance and path computation,\nwith exact distance queries seemingly being answered in logarithmic time.\nHowever, these methods were but heuristics until the work of Abraham et\nal.~[JACM 2016], where they defined a graph parameter called highway dimension\nwhich is constant for real-world road networks, and showed that in graphs of\nconstant highway dimension, a shortcut hierarchy exists that guarantees\nshortest distance computation takes $O(\\log (U+V))$ time and $O(V \\log (U+V))$\nspace, where $U$ is the ratio of the smallest to largest edge, and $V$ is the\nnumber of vertices. The problem is that they were unable to efficiently compute\nthe hierarchy of shortcuts. Here we present a simple and efficient algorithm to\ncompute the needed hierarchy of shortcuts in time and space $O(V \\log (U+V))$,\nas well as supporting updates in time $O( \\log (U+V))$.",
            "author": [
                "S\u00e9bastien Collette",
                "John Iacono"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04235v1",
                "http://arxiv.org/pdf/2312.04235v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04234v1",
            "title": "Graph Convolutions Enrich the Self-Attention in Transformers!",
            "updated": "2023-12-07T11:40:32Z",
            "published": "2023-12-07T11:40:32Z",
            "summary": "Transformers, renowned for their self-attention mechanism, have achieved\nstate-of-the-art performance across various tasks in natural language\nprocessing, computer vision, time-series modeling, etc. However, one of the\nchallenges with deep Transformer models is the oversmoothing problem, where\nrepresentations across layers converge to indistinguishable values, leading to\nsignificant performance degradation. We interpret the original self-attention\nas a simple graph filter and redesign it from a graph signal processing (GSP)\nperspective. We propose graph-filter-based self-attention (GFSA) to learn a\ngeneral yet effective one, whose complexity, however, is slightly larger than\nthat of the original self-attention mechanism. We demonstrate that GFSA\nimproves the performance of Transformers in various fields, including computer\nvision, natural language processing, graph pattern classification, speech\nrecognition, and code classification.",
            "author": [
                "Jeongwhan Choi",
                "Hyowon Wi",
                "Jayoung Kim",
                "Yehjin Shin",
                "Kookjin Lee",
                "Nathaniel Trask",
                "Noseong Park"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04234v1",
                "http://arxiv.org/pdf/2312.04234v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04233v1",
            "title": "Fine-tune vision foundation model for crack segmentation in civil\n  infrastructures",
            "updated": "2023-12-07T11:39:11Z",
            "published": "2023-12-07T11:39:11Z",
            "summary": "Large-scale foundation models have become the mainstream method in the field\nof deep learning, while in civil engineering, the scale of AI models is\nstrictly limited. In this work, vision foundation model is introduced for crack\nsegmentation. Two Parameter-efficient fine-tuning methods, adapter and low-rank\nadaptation, are adopted to fine-tune the foundation model in the field of\nsemantic segmentation: Segment Anything Model (SAM). The fine-tuned model\nCrackSAM is much larger than all the existing crack segmentation models, but\nshows excellent performance. To test the zero-shot performance of the proposed\nmethod, two unique datasets related to road and exterior wall cracks are\ncollected, annotated and open-sourced, in total 810 images. Comparative\nexperiments are conducted with twelve mature semantic segmentation models. On\ndatasets with artificial noise and previously unseen datasets, the performance\nof CrackSAM far exceeds that of all state-of-the-art models. CrackSAM exhibits\nremarkable superiority, particularly in challenging conditions such as dim\nlighting, shadows, road markings, construction joints, and other interference\nfactors. Such cross-scenario results demonstrate the outstanding zero-shot\ncapability of foundation models, and provide new ideas for the development of\nvision models in civil engineering.",
            "author": [
                "Kang Ge",
                "Chen Wang",
                "Yutao Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04233v1",
                "http://arxiv.org/pdf/2312.04233v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04232v1",
            "title": "Emergent Error Correcting States in Networks of Nonlinear Oscillators",
            "updated": "2023-12-07T11:31:24Z",
            "published": "2023-12-07T11:31:24Z",
            "summary": "Networks of nonlinear oscillators can exhibit complex collective behaviour\nranging from synchronised states to chaos. Here, we simulate the dynamics of\nthree coupled Duffing oscillators whose multiple equilibrium states can be used\nfor information processing and storage. Our analysis reveals that even for this\nsmall network, there is the emergence of an error correcting phase where the\nsystem autonomously corrects errors from random impulses. The system has\nseveral surprising and attractive features, including dynamic isolation of\nresonators exposed to extreme impulses and the ability to correct simultaneous\nerrors. The existence of an error correcting phase opens the prospect of\nfault-tolerant information storage, with particular applications in\nnanomechanical computing.",
            "author": [
                "Xiaoya Jin",
                "Christopher G. Baker",
                "Erick Romero",
                "Nicholas P. Mauranyapin",
                "Timothy M. F. Hirsch",
                "Warwick P. Bowen",
                "Glen I. Harris"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04232v1",
                "http://arxiv.org/pdf/2312.04232v1"
            ],
            "primary_category": "physics.class-ph",
            "category": [
                "physics.class-ph",
                "cond-mat.dis-nn",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04231v1",
            "title": "Adventures of Trustworthy Vision-Language Models: A Survey",
            "updated": "2023-12-07T11:31:20Z",
            "published": "2023-12-07T11:31:20Z",
            "summary": "Recently, transformers have become incredibly popular in computer vision and\nvision-language tasks. This notable rise in their usage can be primarily\nattributed to the capabilities offered by attention mechanisms and the\noutstanding ability of transformers to adapt and apply themselves to a variety\nof tasks and domains. Their versatility and state-of-the-art performance have\nestablished them as indispensable tools for a wide array of applications.\nHowever, in the constantly changing landscape of machine learning, the\nassurance of the trustworthiness of transformers holds utmost importance. This\npaper conducts a thorough examination of vision-language transformers,\nemploying three fundamental principles of responsible AI: Bias, Robustness, and\nInterpretability. The primary objective of this paper is to delve into the\nintricacies and complexities associated with the practical use of transformers,\nwith the overarching goal of advancing our comprehension of how to enhance\ntheir reliability and accountability.",
            "author": [
                "Mayank Vatsa",
                "Anubhooti Jain",
                "Richa Singh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04231v1",
                "http://arxiv.org/pdf/2312.04231v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04230v1",
            "title": "Resource-Efficient Quantum Circuits for Molecular Simulations: A Case\n  Study of Umbrella Inversion in Ammonia",
            "updated": "2023-12-07T11:30:09Z",
            "published": "2023-12-07T11:30:09Z",
            "summary": "We conducted a thorough evaluation of various state-of-the-art strategies to\nprepare the ground state wavefunction of a system on a quantum computer,\nspecifically within the framework of variational quantum eigensolver (VQE).\nDespite the advantages of VQE and its variants, the current quantum\ncomputational chemistry calculations often provide inaccurate results for\nlarger molecules, mainly due to the polynomial growth in the depth of quantum\ncircuits and the number of two-qubit gates, such as CNOT gates. To alleviate\nthis problem, we aim to design efficient quantum circuits that would outperform\nthe existing ones on the current noisy quantum devices. In this study, we\ndesigned a novel quantum circuit that reduces the required circuit depth and\nnumber of two-qubit entangling gates by about 60%, while retaining the accuracy\nof the ground state energies close to the chemical accuracy. Moreover, even in\nthe presence of device noise, these novel shallower circuits yielded\nsubstantially low error rates than the existing approaches for predicting the\nground state energies of molecules. By considering the umbrella inversion\nprocess in ammonia molecule as an example, we demonstrated the advantages of\nthis new approach and estimated the energy barrier for the inversion process.",
            "author": [
                "M. R. Nirmal",
                "Sharma S. R. K. C. Yamijala",
                "Kalpak Ghosh",
                "Sumit Kumar",
                "Manoj Nambiar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04230v1",
                "http://arxiv.org/pdf/2312.04230v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04226v1",
            "title": "Dynamic Data-Driven Digital Twins for Blockchain Systems",
            "updated": "2023-12-07T11:18:57Z",
            "published": "2023-12-07T11:18:57Z",
            "summary": "In recent years, we have seen an increase in the adoption of blockchain-based\nsystems in non-financial applications, looking to benefit from what the\ntechnology has to offer. Although many fields have managed to include\nblockchain in their core functionalities, the adoption of blockchain, in\ngeneral, is constrained by the so-called trilemma trade-off between\ndecentralization, scalability, and security. In our previous work, we have\nshown that using a digital twin for dynamically managing blockchain systems\nduring runtime can be effective in managing the trilemma trade-off. Our Digital\nTwin leverages DDDAS feedback loop, which is responsible for getting the data\nfrom the system to the digital twin, conducting optimisation, and updating the\nphysical system. This paper examines how leveraging DDDAS feedback loop can\nsupport the optimisation component of the trilemma benefiting from\nReinforcement Learning agents and a simulation component to augment the quality\nof the learned model while reducing the computational overhead required for\ndecision-making.",
            "author": [
                "Georgios Diamantopoulos",
                "Nikos Tziritas",
                "Rami Bahsoon",
                "Georgios Theodoropoulos"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04226v1",
                "http://arxiv.org/pdf/2312.04226v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "cs.DC",
                "cs.PF"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04225v1",
            "title": "TLCE: Transfer-Learning Based Classifier Ensembles for Few-Shot\n  Class-Incremental Learning",
            "updated": "2023-12-07T11:16:00Z",
            "published": "2023-12-07T11:16:00Z",
            "summary": "Few-shot class-incremental learning (FSCIL) struggles to incrementally\nrecognize novel classes from few examples without catastrophic forgetting of\nold classes or overfitting to new classes. We propose TLCE, which ensembles\nmultiple pre-trained models to improve separation of novel and old classes.\nTLCE minimizes interference between old and new classes by mapping old class\nimages to quasi-orthogonal prototypes using episodic training. It then\nensembles diverse pre-trained models to better adapt to novel classes despite\ndata imbalance. Extensive experiments on various datasets demonstrate that our\ntransfer learning ensemble approach outperforms state-of-the-art FSCIL methods.",
            "author": [
                "Shuangmei Wang",
                "Yang Cao",
                "Tieru Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04225v1",
                "http://arxiv.org/pdf/2312.04225v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04222v1",
            "title": "Counting collisions in random circuit sampling for benchmarking quantum\n  computers",
            "updated": "2023-12-07T11:12:30Z",
            "published": "2023-12-07T11:12:30Z",
            "summary": "We show that counting the number of collisions (re-sampled bitstrings) when\nmeasuring a random quantum circuit provides a practical benchmark for the\nquality of a quantum computer and a quantitative noise characterization method.\nWe analytically estimate the difference in the expected number of collisions\nwhen sampling bitstrings from a pure random state and when sampling from the\nclassical uniform distribution. We show that this quantity, if properly\nnormalized, can be used as a \"collision anomaly\" benchmark or as a \"collision\nvolume\" test which is similar to the well-known quantum volume test, with\nadvantages (no classical computing cost) and disadvantages (high sampling\ncost). We also propose to count the number of cross-collisions between two\nindependent quantum computers running the same random circuit in order to\nobtain a cross-validation test of the two devices. Finally, we quantify the\nsampling cost of quantum collision experiments. We find that the sampling cost\nfor running a collision volume test on state-of-the-art processors (e.g.~20\neffective clean qubits) is quite small: less than $10^5$ shots. For large-scale\nexperiments in the quantum supremacy regime the required number of shots for\nobserving a quantum signal in the observed number of collisions is currently\ninfeasible ($>10^{12}$), but not completely out of reach for near-future\ntechnology.",
            "author": [
                "Andrea Mari"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04222v1",
                "http://arxiv.org/pdf/2312.04222v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04219v1",
            "title": "Swap distance minimization in SOV languages. Cognitive and mathematical\n  foundations",
            "updated": "2023-12-07T11:10:28Z",
            "published": "2023-12-07T11:10:28Z",
            "summary": "Distance minimization is a general principle of language. A special case of\nthis principle in the domain of word order is swap distance minimization. This\nprinciple predicts that variations from a canonical order that are reached by\nfewer swaps of adjacent constituents are lest costly and thus more likely. Here\nwe investigate the principle in the context of the triple formed by subject\n(S), object (O) and verb (V). We introduce the concept of word order rotation\nas a cognitive underpinning of that prediction. When the canonical order of a\nlanguage is SOV, the principle predicts SOV < SVO, OSV < VSO, OVS < VOS, in\norder of increasing cognitive cost. We test the prediction in three flexible\norder SOV languages: Korean (Koreanic), Malayalam (Dravidian), and Sinhalese\n(Indo-European). Evidence of swap distance minimization is found in all three\nlanguages, but it is weaker in Sinhalese. Swap distance minimization is\nstronger than a preference for the canonical order in Korean and especially\nMalayalam.",
            "author": [
                "Ramon Ferrer-i-Cancho",
                "Savithry Namboodiripad"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04219v1",
                "http://arxiv.org/pdf/2312.04219v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04217v1",
            "title": "A Hybrid Monte Carlo, Discontinuous Galerkin method for linear kinetic\n  transport equations",
            "updated": "2023-12-07T11:04:50Z",
            "published": "2023-12-07T11:04:50Z",
            "summary": "We present a hybrid method for time-dependent particle transport problems\nthat combines Monte Carlo (MC) estimation with deterministic solutions based on\ndiscrete ordinates. For spatial discretizations, the MC algorithm computes a\npiecewise constant solution and the discrete ordinates uses bilinear\ndiscontinuous finite elements. From the hybridization of the problem, the\nresulting problem solved by Monte Carlo is scattering free, resulting in a\nsimple, efficient solution procedure. Between time steps, we use a projection\napproach to ``relabel'' collided particles as uncollided particles. From a\nseries of standard 2-D Cartesian test problems we observe that our hybrid\nmethod has improved accuracy and reduction in computational complexity of\napproximately an order of magnitude relative to standard discrete ordinates\nsolutions.",
            "author": [
                "Johannes Krotz",
                "Cory D. Hauck",
                "Ryan G. McClarren"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04217v1",
                "http://arxiv.org/pdf/2312.04217v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04215v1",
            "title": "Guided Reconstruction with Conditioned Diffusion Models for Unsupervised\n  Anomaly Detection in Brain MRIs",
            "updated": "2023-12-07T11:03:42Z",
            "published": "2023-12-07T11:03:42Z",
            "summary": "Unsupervised anomaly detection in Brain MRIs aims to identify abnormalities\nas outliers from a healthy training distribution. Reconstruction-based\napproaches that use generative models to learn to reconstruct healthy brain\nanatomy are commonly used for this task. Diffusion models are an emerging class\nof deep generative models that show great potential regarding reconstruction\nfidelity. However, they face challenges in preserving intensity characteristics\nin the reconstructed images, limiting their performance in anomaly detection.\nTo address this challenge, we propose to condition the denoising mechanism of\ndiffusion models with additional information about the image to reconstruct\ncoming from a latent representation of the noise-free input image. This\nconditioning enables high-fidelity reconstruction of healthy brain structures\nwhile aligning local intensity characteristics of input-reconstruction pairs.\nWe evaluate our method's reconstruction quality, domain adaptation features and\nfinally segmentation performance on publicly available data sets with various\npathologies. Using our proposed conditioning mechanism we can reduce the\nfalse-positive predictions and enable a more precise delineation of anomalies\nwhich significantly enhances the anomaly detection performance compared to\nestablished state-of-the-art approaches to unsupervised anomaly detection in\nbrain MRI. Furthermore, our approach shows promise in domain adaptation across\ndifferent MRI acquisitions and simulated contrasts, a crucial property of\ngeneral anomaly detection methods.",
            "author": [
                "Finn Behrendt",
                "Debayan Bhattacharya",
                "Robin Mieling",
                "Lennart Maack",
                "Julia Kr\u00fcger",
                "Roland Opfer",
                "Alexander Schlaefer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04215v1",
                "http://arxiv.org/pdf/2312.04215v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04210v1",
            "title": "Constraint Model for the Satellite Image Mosaic Selection Problem",
            "updated": "2023-12-07T10:52:16Z",
            "published": "2023-12-07T10:52:16Z",
            "summary": "Satellite imagery solutions are widely used to study and monitor different\nregions of the Earth. However, a single satellite image can cover only a\nlimited area. In cases where a larger area of interest is studied, several\nimages must be stitched together to create a single larger image, called a\nmosaic, that can cover the area. Today, with the increasing number of satellite\nimages available for commercial use, selecting the images to build the mosaic\nis challenging, especially when the user wants to optimize one or more\nparameters, such as the total cost and the cloud coverage percentage in the\nmosaic. More precisely, for this problem the input is an area of interest,\nseveral satellite images intersecting the area, a list of requirements relative\nto the image and the mosaic, such as cloud coverage percentage, image\nresolution, and a list of objectives to optimize. We contribute to the\nconstraint and mixed integer lineal programming formulation of this new\nproblem, which we call the \\textit{satellite image mosaic selection problem},\nwhich is a multi-objective extension of the polygon cover problem. We propose a\ndataset of realistic and challenging instances, where the images were captured\nby the satellite constellations SPOT, Pl\\'eiades and Pl\\'eiades Neo. We\nevaluate and compare the two proposed models and show their efficiency for\nlarge instances, up to 200 images.",
            "author": [
                "Manuel Combarro Sim\u00f3n",
                "Pierre Talbot",
                "Gr\u00e9goire Danoy",
                "Jedrzej Musial",
                "Mohammed Alswaitti",
                "Pascal Bouvry"
            ],
            "link": [
                "http://dx.doi.org/10.4230/LIPIcs.CP.2023.44",
                "http://arxiv.org/abs/2312.04210v1",
                "http://arxiv.org/pdf/2312.04210v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CG",
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04209v1",
            "title": "Constrained Hierarchical Clustering via Graph Coarsening and Optimal\n  Cuts",
            "updated": "2023-12-07T10:52:06Z",
            "published": "2023-12-07T10:52:06Z",
            "summary": "Motivated by extracting and summarizing relevant information in short\nsentence settings, such as satisfaction questionnaires, hotel reviews, and\nX/Twitter, we study the problem of clustering words in a hierarchical fashion.\nIn particular, we focus on the problem of clustering with horizontal and\nvertical structural constraints. Horizontal constraints are typically\ncannot-link and must-link among words, while vertical constraints are\nprecedence constraints among cluster levels. We overcome state-of-the-art\nbottlenecks by formulating the problem in two steps: first, as a\nsoft-constrained regularized least-squares which guides the result of a\nsequential graph coarsening algorithm towards the horizontal feasible set.\nThen, flat clusters are extracted from the resulting hierarchical tree by\ncomputing optimal cut heights based on the available constraints. We show that\nthe resulting approach compares very well with respect to existing algorithms\nand is computationally light.",
            "author": [
                "Eliabelle Mauduit",
                "Andrea Simonetto"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04209v1",
                "http://arxiv.org/pdf/2312.04209v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04204v1",
            "title": "Wavelength-multiplexed Delayed Inputs for Memory Enhancement of\n  Microring-based Reservoir Computing",
            "updated": "2023-12-07T10:40:37Z",
            "published": "2023-12-07T10:40:37Z",
            "summary": "We numerically demonstrate a silicon add-drop microring-based reservoir\ncomputing scheme that combines parallel delayed inputs and wavelength division\nmultiplexing. The scheme solves memory-demanding tasks like time-series\nprediction with good performance without requiring external optical feedback.",
            "author": [
                "Bernard J. Giron Castro",
                "Christophe Peucheret",
                "Francesco Da Ros"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04204v1",
                "http://arxiv.org/pdf/2312.04204v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE",
                "cs.ET",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04201v1",
            "title": "Computation of the 2-parameter matching distance via the extended Pareto\n  grid",
            "updated": "2023-12-07T10:38:41Z",
            "published": "2023-12-07T10:38:41Z",
            "summary": "One of the most animated themes of multidimensional persistence is the\ncomparison between invariants. The matching distance between persistent Betti\nnumbers functions (or rank invariants), is among the most studied metrics in\nthis context, particularly in 2-parameter persistence. The main reason for this\ninterest is that, in the 2-parameter case, the foliation method allows for an\neffective computation of the matching distance, based on filtering the space\nalong lines of positive slope. Our work provides a qualitative analysis, based\non a construction called extended Pareto grid, of the filtering lines that\nactually contribute to the computation of the matching distance. Under certain\ngenericity assumptions, we show that these lines must either be horizontal,\nvertical, of slope 1 or belong to a finite collection of special lines\nassociated with discontinuity phenomena.",
            "author": [
                "Patrizio Frosini",
                "Eloy M\u00f3sig Garc\u00eda",
                "Nicola Quercioli",
                "Francesca Tombari"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04201v1",
                "http://arxiv.org/pdf/2312.04201v1"
            ],
            "primary_category": "math.AT",
            "category": [
                "math.AT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04200v1",
            "title": "Spectral properties of the Bloch-Torrey operator in three dimensions",
            "updated": "2023-12-07T10:37:47Z",
            "published": "2023-12-07T10:37:47Z",
            "summary": "We consider the Bloch-Torrey operator, $-\\Delta + igx$, that governs the time\nevolution of the transverse magnetization in diffusion magnetic resonance\nimaging (dMRI). Using the matrix formalism, we compute numerically the\neigenvalues and eigenfunctions of this non-Hermitian operator for two bounded\nthree-dimensional domains: a sphere and a capped cylinder. We study the\ndependence of its eigenvalues and eigenfunctions on the parameter $g$ and on\nthe shape of the domain (its eventual symmetries and anisotropy). In\nparticular, we show how an eigenfunction drastically changes its shape when the\nassociated eigenvalue crosses a branch (or exceptional) point in the spectrum.\nPotential implications of this behavior for dMRI are discussed.",
            "author": [
                "Denis S. Grebenkov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04200v1",
                "http://arxiv.org/pdf/2312.04200v1"
            ],
            "primary_category": "math-ph",
            "category": [
                "math-ph",
                "math.MP",
                "math.SP",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04197v1",
            "title": "SAMBA: A Trainable Segmentation Web-App with Smart Labelling",
            "updated": "2023-12-07T10:31:05Z",
            "published": "2023-12-07T10:31:05Z",
            "summary": "Segmentation is the assigning of a semantic class to every pixel in an image\nand is a prerequisite for various statistical analysis tasks in materials\nscience, like phase quantification, physics simulations or morphological\ncharacterization. The wide range of length scales, imaging techniques and\nmaterials studied in materials science means any segmentation algorithm must\ngeneralise to unseen data and support abstract, user-defined semantic classes.\nTrainable segmentation is a popular interactive segmentation paradigm where a\nclassifier is trained to map from image features to user drawn labels. SAMBA is\na trainable segmentation tool that uses Meta's Segment Anything Model (SAM) for\nfast, high-quality label suggestions and a random forest classifier for robust,\ngeneralizable segmentations. It is accessible in the browser\n(https://www.sambasegment.com/) without the need to download any external\ndependencies. The segmentation backend is run in the cloud, so does not require\nthe user to have powerful hardware.",
            "author": [
                "Ronan Docherty",
                "Isaac Squires",
                "Antonis Vamvakeros",
                "Samuel J. Cooper"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04197v1",
                "http://arxiv.org/pdf/2312.04197v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04193v1",
            "title": "Language Model Knowledge Distillation for Efficient Question Answering\n  in Spanish",
            "updated": "2023-12-07T10:21:22Z",
            "published": "2023-12-07T10:21:22Z",
            "summary": "Recent advances in the development of pre-trained Spanish language models has\nled to significant progress in many Natural Language Processing (NLP) tasks,\nsuch as question answering. However, the lack of efficient models imposes a\nbarrier for the adoption of such models in resource-constrained environments.\nTherefore, smaller distilled models for the Spanish language could be proven to\nbe highly scalable and facilitate their further adoption on a variety of tasks\nand scenarios. In this work, we take one step in this direction by developing\nSpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficient\nquestion answering in Spanish. To achieve this, we employ knowledge\ndistillation from a large model onto a lighter model that allows for a wider\nimplementation, even in areas with limited computational resources, whilst\nattaining negligible performance sacrifice. Our experiments show that the dense\ndistilled model can still preserve the performance of its larger counterpart,\nwhile significantly increasing inference speedup. This work serves as a\nstarting point for further research and investigation of model compression\nefforts for Spanish language models across various NLP tasks.",
            "author": [
                "Adri\u00e1n Bazaga",
                "Pietro Li\u00f2",
                "Gos Micklem"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04193v1",
                "http://arxiv.org/pdf/2312.04193v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04189v1",
            "title": "Joint-Individual Fusion Structure with Fusion Attention Module for\n  Multi-Modal Skin Cancer Classification",
            "updated": "2023-12-07T10:16:21Z",
            "published": "2023-12-07T10:16:21Z",
            "summary": "Most convolutional neural network (CNN) based methods for skin cancer\nclassification obtain their results using only dermatological images. Although\ngood classification results have been shown, more accurate results can be\nachieved by considering the patient's metadata, which is valuable clinical\ninformation for dermatologists. Current methods only use the simple joint\nfusion structure (FS) and fusion modules (FMs) for the multi-modal\nclassification methods, there still is room to increase the accuracy by\nexploring more advanced FS and FM. Therefore, in this paper, we design a new\nfusion method that combines dermatological images (dermoscopy images or\nclinical images) and patient metadata for skin cancer classification from the\nperspectives of FS and FM. First, we propose a joint-individual fusion (JIF)\nstructure that learns the shared features of multi-modality data and preserves\nspecific features simultaneously. Second, we introduce a fusion attention (FA)\nmodule that enhances the most relevant image and metadata features based on\nboth the self and mutual attention mechanism to support the decision-making\npipeline. We compare the proposed JIF-MMFA method with other state-of-the-art\nfusion methods on three different public datasets. The results show that our\nJIF-MMFA method improves the classification results for all tested CNN\nbackbones and performs better than the other fusion methods on the three public\ndatasets, demonstrating our method's effectiveness and robustness",
            "author": [
                "Peng Tang",
                "Xintong Yan",
                "Yang Nan",
                "Xiaobin Hu",
                "Xiaobin Hu",
                "Bjoern H Menzee. Sebastian Krammer",
                "Tobias Lasser"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04189v1",
                "http://arxiv.org/pdf/2312.04189v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04187v1",
            "title": "Enumerating Complexity Revisited",
            "updated": "2023-12-07T10:14:42Z",
            "published": "2023-12-07T10:14:42Z",
            "summary": "We reduce the best-known upper bound on the length of a program that\nenumerates a set in terms of the probability of it being enumerated by a random\nprogram. We prove a general result that any linear upper bound for finite sets\nimplies the same linear bound for infinite sets.\n  So far, the best-known upper bound was given by Solovay. He showed that the\nminimum length of a program enumerating a subset $S$ of natural numbers is\nbounded by minus three binary logarithms of the probability that a random\nprogram will enumerate $S$. Later, Vereshchagin showed that the constant can be\nimproved from three to two for finite sets. In this work, using an improvement\nof the method proposed by Solovay, we demonstrate that any bound for finite\nsets implies the same for infinite sets, modulo logarithmic factors. Using\nVereshchagin's result, we improve the current best-known upper bound from three\nto two.",
            "author": [
                "Alexander Shekhovtsov",
                "Georgii Zakharov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04187v1",
                "http://arxiv.org/pdf/2312.04187v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04186v1",
            "title": "Superconducting processor design optimization for quantum error\n  correction performance",
            "updated": "2023-12-07T10:13:08Z",
            "published": "2023-12-07T10:13:08Z",
            "summary": "In the quest for fault-tolerant quantum computation using superconducting\nprocessors, accurate performance assessment and continuous design optimization\nstands at the forefront. To facilitate both meticulous simulation and\nstreamlined design optimization, we introduce a multi-level simulation\nframework that spans both Hamiltonian and quantum error correction levels, and\nis equipped with the capability to compute gradients efficiently. This toolset\naids in design optimization, tailored to specific objectives like quantum\nmemory performance. Within our framework, we investigate the often-neglected\nspatially correlated unitary errors, highlighting their significant impact on\nlogical error rates. We exemplify our approach through the multi-path coupling\nscheme of fluxonium qubits.",
            "author": [
                "Xiaotong Ni",
                "Ziang Wang",
                "Rui Chao",
                "Jianxin Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04186v1",
                "http://arxiv.org/pdf/2312.04186v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04185v1",
            "title": "A minimal model for prestacks via Koszul duality for box operads",
            "updated": "2023-12-07T10:12:59Z",
            "published": "2023-12-07T10:12:59Z",
            "summary": "Prestacks are algebro-geometric objects whose defining relations are far from\nquadratic. Indeed, they are cubic and quartic, and moreover inhomogeneous. We\nshow that box operads, a rectangular type of operads introduced in\narXiv:2305.20036, constitute the correct framework to encode them and resolve\ntheir relations up to homotopy. Our first main result is a Koszul duality\ntheory for box operads, extending the duality for (nonsymmetric) operads. In\nthis new theory, the classical restriction of being quadratic is replaced by\nthe notion of being $\\textit{thin-quadratic}$, a condition referring to a\nparticular class of ``thin'' operations. Our main case of interest is the box\noperad $Lax$ encoding lax prestacks as algebras. We show that $Lax$ is not\nKoszul by explicitly computing its Koszul complex. We then go on to remedy the\nsituation by suitably restricting the Koszul dual box cooperad\n$Lax^{\\antishriek}$ to obtain our second main result: we establish a minimal\n(in particular cofibrant) model $Lax_{\\infty}$ for the box operad $Lax$\nencoding lax prestacks. This sheds new light on Markl's question on the\nexistence of a cofibrant model for the operad encoding presheaves of algebras\nfrom arXiv:math/0103052. Indeed, we answer the parallel question with\npresheaves viewed as prestacks, and prestacks considered as algebras over a box\noperad, in the positive.",
            "author": [
                "Lander Hermans"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04185v1",
                "http://arxiv.org/pdf/2312.04185v1"
            ],
            "primary_category": "math.AT",
            "category": [
                "math.AT",
                "math.AG",
                "math.CT",
                "18M70 (Primary) 18M65, 14A20 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04183v1",
            "title": "Enhanced data Detection for Massive MIMO with 1-Bit ADCs",
            "updated": "2023-12-07T10:11:20Z",
            "published": "2023-12-07T10:11:20Z",
            "summary": "We present new insightful results on the uplink data detection for massive\nmultiple-input multiple-output systems with 1-bit analog-to-digital converters.\nThe expected values of the soft-estimated symbols (i.e., after the linear\ncombining and prior to the data detection) have been recently characterized for\nmultiple user equipments (UEs) and maximum ratio combining (MRC) receiver at\nthe base station. In this paper, we first provide a numerical evaluation of the\nexpected value of the soft-estimated symbols with zero-forcing (ZF) and minimum\nmean squared error (MMSE) receivers for a multi-UE setting with correlated\nRayleigh fading. Then, we propose a joint data detection (JD) strategy, which\nexploits the interdependence among the soft-estimated symbols of the\ninterfering UEs, along with its low-complexity variant. These strategies are\ncompared with a naive approach that adapts the maximum-likelihood data\ndetection to the 1-bit quantization. Numerical results show that ZF and MMSE\nprovide considerable gains over MRC in terms of symbol error rate. Moreover,\nthe proposed JD and its low-complexity variant provide a significant boost in\ncomparison with the single-UE data detection.",
            "author": [
                "Amin Radbord",
                "Italo Atzeni",
                "Antti Tolli"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04183v1",
                "http://arxiv.org/pdf/2312.04183v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04182v1",
            "title": "Bayesian Persuasion for Containing SIS Epidemic with Asymptomatic\n  Infection",
            "updated": "2023-12-07T10:10:55Z",
            "published": "2023-12-07T10:10:55Z",
            "summary": "We investigate the strategic behavior of a large population of agents who\ndecide whether to adopt a costly partially effective protection or remain\nunprotected against the susceptible-infected-susceptible epidemic. In contrast\nwith most prior works on epidemic games, we assume that the agents are not\naware of their true infection status while making decisions. We adopt the\nBayesian persuasion framework where the agents receive a noisy signal regarding\ntheir true infection status, and maximize their expected utility computed using\nthe posterior probability of being infected conditioned on the received signal.\nWe completely characterize the stationary Nash equilibrium of this setting, and\nidentify conditions under which partial information disclosure leads to a\nsmaller proportion of infected individuals at the equilibrium compared to full\ninformation disclosure, and vice versa.",
            "author": [
                "Ashish R. Hota",
                "Abhisek Satapathi",
                "Urmee Maitra"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04182v1",
                "http://arxiv.org/pdf/2312.04182v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.GT",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04181v1",
            "title": "Cell segmentation of in situ transcriptomics data using signed graph\n  partitioning",
            "updated": "2023-12-07T10:08:07Z",
            "published": "2023-12-07T10:08:07Z",
            "summary": "The locations of different mRNA molecules can be revealed by multiplexed in\nsitu RNA detection. By assigning detected mRNA molecules to individual cells,\nit is possible to identify many different cell types in parallel. This in turn\nenables investigation of the spatial cellular architecture in tissue, which is\ncrucial for furthering our understanding of biological processes and diseases.\nHowever, cell typing typically depends on the segmentation of cell nuclei,\nwhich is often done based on images of a DNA stain, such as DAPI. Limiting cell\ndefinition to a nuclear stain makes it fundamentally difficult to determine\naccurate cell borders, and thereby also difficult to assign mRNA molecules to\nthe correct cell. As such, we have developed a computational tool that segments\ncells solely based on the local composition of mRNA molecules. First, a small\nneural network is trained to compute attractive and repulsive edges between\npairs of mRNA molecules. The signed graph is then partitioned by a mutex\nwatershed into components corresponding to different cells. We evaluated our\nmethod on two publicly available datasets and compared it against the current\nstate-of-the-art and older baselines. We conclude that combining neural\nnetworks with combinatorial optimization is a promising approach for cell\nsegmentation of in situ transcriptomics data.",
            "author": [
                "Axel Andersson",
                "Andrea Behanova",
                "Carolina W\u00e4hlby",
                "Filip Malmberg"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-42795-4_13",
                "http://arxiv.org/abs/2312.04181v1",
                "http://arxiv.org/pdf/2312.04181v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04180v1",
            "title": "AI and Jobs: Has the Inflection Point Arrived? Evidence from an Online\n  Labor Platform",
            "updated": "2023-12-07T10:06:34Z",
            "published": "2023-12-07T10:06:34Z",
            "summary": "Artificial intelligence (AI) refers to the ability of machines or software to\nmimic or even surpass human intelligence in a given cognitive task. While\nhumans learn by both induction and deduction, the success of current AI is\nrooted in induction, relying on its ability to detect statistical regularities\nin task input -- an ability learnt from a vast amount of training data using\nenormous computation resources. We examine the performance of such a\nstatistical AI in a human task through the lens of four factors, including task\nlearnability, statistical resource, computation resource, and learning\ntechniques, and then propose a three-phase visual framework to understand the\nevolving relation between AI and jobs. Based on this conceptual framework, we\ndevelop a simple economic model of competition to show the existence of an\ninflection point for each occupation. Before AI performance crosses the\ninflection point, human workers always benefit from an improvement in AI\nperformance, but after the inflection point, human workers become worse off\nwhenever such an improvement occurs. To offer empirical evidence, we first\nargue that AI performance has passed the inflection point for the occupation of\ntranslation but not for the occupation of web development. We then study how\nthe launch of ChatGPT, which led to significant improvement of AI performance\non many tasks, has affected workers in these two occupations on a large online\nlabor platform. Consistent with the inflection point conjecture, we find that\ntranslators are negatively affected by the shock both in terms of the number of\naccepted jobs and the earnings from those jobs, while web developers are\npositively affected by the very same shock. Given the potentially large\ndisruption of AI on employment, more studies on more occupations using data\nfrom different platforms are urgently needed.",
            "author": [
                "Dandan Qiao",
                "Huaxia Rui",
                "Qian Xiong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04180v1",
                "http://arxiv.org/pdf/2312.04180v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CY",
                "econ.GN",
                "q-fin.EC",
                "J.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04178v1",
            "title": "Lattice investigation of the general Two Higgs Doublet Model with\n  $SU(2)$ gauge fields",
            "updated": "2023-12-07T09:55:35Z",
            "published": "2023-12-07T09:55:35Z",
            "summary": "We study the most general Two Higgs Doublet Model with $SU(2)$ gauge fields\non the lattice. The phase space is probed through the computation of\ngauge-invariant global observables serving as proxies for order parameters. In\neach phase, the spectrum of the theory is analysed for different combinations\nof bare couplings and different symmetry breaking patterns. The scale setting\nand determination of the running gauge coupling are performed through the\nWilson flow computation of the action density.",
            "author": [
                "Guilherme Catumba",
                "Atsuki Hiraguchi",
                "George W. -S Hou",
                "Karl Jansen",
                "Ying-Jer Kao",
                "C. -J. David Lin",
                "Alberto Ramos",
                "Mugdha Sarkar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04178v1",
                "http://arxiv.org/pdf/2312.04178v1"
            ],
            "primary_category": "hep-lat",
            "category": [
                "hep-lat"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04174v1",
            "title": "Coherent energy and force uncertainty in deep learning force fields",
            "updated": "2023-12-07T09:49:05Z",
            "published": "2023-12-07T09:49:05Z",
            "summary": "In machine learning energy potentials for atomic systems, forces are commonly\nobtained as the negative derivative of the energy function with respect to\natomic positions. To quantify aleatoric uncertainty in the predicted energies,\na widely used modeling approach involves predicting both a mean and variance\nfor each energy value. However, this model is not differentiable under the\nusual white noise assumption, so energy uncertainty does not naturally\ntranslate to force uncertainty. In this work we propose a machine learning\npotential energy model in which energy and force aleatoric uncertainty are\nlinked through a spatially correlated noise process. We demonstrate our\napproach on an equivariant messages passing neural network potential trained on\nenergies and forces on two out-of-equilibrium molecular datasets. Furthermore,\nwe also show how to obtain epistemic uncertainties in this setting based on a\nBayesian interpretation of deep ensemble models.",
            "author": [
                "Peter Bj\u00f8rn J\u00f8rgensen",
                "Jonas Busk",
                "Ole Winther",
                "Mikkel N. Schmidt"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04174v1",
                "http://arxiv.org/pdf/2312.04174v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04172v1",
            "title": "On the Absolute-Value Integral of a Brownian Motion with Drift: Exact\n  and Asymptotic Formulae",
            "updated": "2023-12-07T09:47:40Z",
            "published": "2023-12-07T09:47:40Z",
            "summary": "The present paper is concerned with the integral of the absolute value of a\nBrownian motion with drift. By establishing an asymptotic expansion of the\nspace Laplace transform, we obtain series representations for the probability\ndensity function and cumulative distribution function of the integral, making\nuse of Meijer's G-function. A functional recursive formula is derived for the\nmoments, which is shown to yield only exponentials and Gauss' error function up\nto arbitrary orders, permitting exact computations. In order to obtain sharp\nasymptotic estimates for small- and large-deviation probabilities, we employ a\nmarginal space-time Laplace transform and apply a newly developed generalized\nLaplace's method to exponential Airy integrals. The impact of drift on the\ncomplete distribution of the integral is explored in depth. The new formulae\ncomplement existing ones in the standard Brownian motion case to great extent\nin terms of both theoretical generality and modeling capacity and have been\npresented for easy implementation, which numerical experiments demonstrate.",
            "author": [
                "Weixuan Xia",
                "Yuyang Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04172v1",
                "http://arxiv.org/pdf/2312.04172v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.CV",
                "60E10, 60G15, 33C10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04168v1",
            "title": "Augmentation-Free Dense Contrastive Knowledge Distillation for Efficient\n  Semantic Segmentation",
            "updated": "2023-12-07T09:37:28Z",
            "published": "2023-12-07T09:37:28Z",
            "summary": "In recent years, knowledge distillation methods based on contrastive learning\nhave achieved promising results on image classification and object detection\ntasks. However, in this line of research, we note that less attention is paid\nto semantic segmentation. Existing methods heavily rely on data augmentation\nand memory buffer, which entail high computational resource demands when\napplying them to handle semantic segmentation that requires to preserve\nhigh-resolution feature maps for making dense pixel-wise predictions. In order\nto address this problem, we present Augmentation-free Dense Contrastive\nKnowledge Distillation (Af-DCD), a new contrastive distillation learning\nparadigm to train compact and accurate deep neural networks for semantic\nsegmentation applications. Af-DCD leverages a masked feature mimicking\nstrategy, and formulates a novel contrastive learning loss via taking advantage\nof tactful feature partitions across both channel and spatial dimensions,\nallowing to effectively transfer dense and structured local knowledge learnt by\nthe teacher model to a target student model while maintaining training\nefficiency. Extensive experiments on five mainstream benchmarks with various\nteacher-student network pairs demonstrate the effectiveness of our approach.\nFor instance, the DeepLabV3-Res18|DeepLabV3-MBV2 model trained by Af-DCD\nreaches 77.03%|76.38% mIOU on Cityscapes dataset when choosing DeepLabV3-Res101\nas the teacher, setting new performance records. Besides that, Af-DCD achieves\nan absolute mIOU improvement of 3.26%|3.04%|2.75%|2.30%|1.42% compared with\nindividually trained counterpart on Cityscapes|Pascal\nVOC|Camvid|ADE20K|COCO-Stuff-164K. Code is available at\nhttps://github.com/OSVAI/Af-DCD",
            "author": [
                "Jiawei Fan",
                "Chao Li",
                "Xiaolong Liu",
                "Meina Song",
                "Anbang Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04168v1",
                "http://arxiv.org/pdf/2312.04168v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04167v1",
            "title": "Mixture of Dynamical Variational Autoencoders for Multi-Source\n  Trajectory Modeling and Separation",
            "updated": "2023-12-07T09:36:31Z",
            "published": "2023-12-07T09:36:31Z",
            "summary": "In this paper, we propose a latent-variable generative model called mixture\nof dynamical variational autoencoders (MixDVAE) to model the dynamics of a\nsystem composed of multiple moving sources. A DVAE model is pre-trained on a\nsingle-source dataset to capture the source dynamics. Then, multiple instances\nof the pre-trained DVAE model are integrated into a multi-source mixture model\nwith a discrete observation-to-source assignment latent variable. The posterior\ndistributions of both the discrete observation-to-source assignment variable\nand the continuous DVAE variables representing the sources content/position are\nestimated using a variational expectation-maximization algorithm, leading to\nmulti-source trajectories estimation. We illustrate the versatility of the\nproposed MixDVAE model on two tasks: a computer vision task, namely\nmulti-object tracking, and an audio processing task, namely single-channel\naudio source separation. Experimental results show that the proposed method\nworks well on these two tasks, and outperforms several baseline methods.",
            "author": [
                "Xiaoyu Lin",
                "Laurent Girin",
                "Xavier Alameda-Pineda"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04167v1",
                "http://arxiv.org/pdf/2312.04167v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04166v1",
            "title": "Improving Communication Efficiency of Federated Distillation via\n  Accumulating Local Updates",
            "updated": "2023-12-07T09:36:18Z",
            "published": "2023-12-07T09:36:18Z",
            "summary": "As an emerging federated learning paradigm, federated distillation enables\ncommunication-efficient model training by transmitting only small-scale\nknowledge during the learning process. To further improve the communication\nefficiency of federated distillation, we propose a novel technique, ALU, which\naccumulates multiple rounds of local updates before transferring the knowledge\nto the central server. ALU drastically decreases the frequency of communication\nin federated distillation, thereby significantly reducing the communication\noverhead during the training process. Empirical experiments demonstrate the\nsubstantial effect of ALU in improving the communication efficiency of\nfederated distillation.",
            "author": [
                "Zhiyuan Wu",
                "Sheng Sun",
                "Yuwei Wang",
                "Min Liu",
                "Tian Wen",
                "Wen Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04166v1",
                "http://arxiv.org/pdf/2312.04166v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04160v1",
            "title": "Text as Image: Learning Transferable Adapter for Multi-Label\n  Classification",
            "updated": "2023-12-07T09:22:20Z",
            "published": "2023-12-07T09:22:20Z",
            "summary": "Pre-trained vision-language models have notably accelerated progress of\nopen-world concept recognition. Their impressive zero-shot ability has recently\nbeen transferred to multi-label image classification via prompt tuning,\nenabling to discover novel labels in an open-vocabulary manner. However, this\nparadigm suffers from non-trivial training costs, and becomes computationally\nprohibitive for a large number of candidate labels. To address this issue, we\nnote that vision-language pre-training aligns images and texts in a unified\nembedding space, making it potential for an adapter network to identify labels\nin visual modality while be trained in text modality. To enhance such\ncross-modal transfer ability, a simple yet effective method termed random\nperturbation is proposed, which enables the adapter to search for potential\nvisual embeddings by perturbing text embeddings with noise during training,\nresulting in better performance in visual modality. Furthermore, we introduce\nan effective approach to employ large language models for multi-label\ninstruction-following text generation. In this way, a fully automated pipeline\nfor visual label recognition is developed without relying on any manual data.\nExtensive experiments on public benchmarks show the superiority of our method\nin various multi-label classification tasks.",
            "author": [
                "Xuelin Zhu",
                "Jiuxin Cao",
                "Jian liu",
                "Dongqi Tang",
                "Furong Xu",
                "Weijia Liu",
                "Jiawei Ge",
                "Bo Liu",
                "Qingpei Guo",
                "Tianyi Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04160v1",
                "http://arxiv.org/pdf/2312.04160v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04159v1",
            "title": "Zero-Touch Networks: Towards Next-Generation Network Automation",
            "updated": "2023-12-07T09:21:41Z",
            "published": "2023-12-07T09:21:41Z",
            "summary": "The Zero-touch network and Service Management (ZSM) framework represents an\nemerging paradigm in the management of the fifth-generation (5G) and Beyond\n(5G+) networks, offering automated self-management and self-healing\ncapabilities to address the escalating complexity and the growing data volume\nof modern networks. ZSM frameworks leverage advanced technologies such as\nMachine Learning (ML) to enable intelligent decision-making and reduce human\nintervention. This paper presents a comprehensive survey of Zero-Touch Networks\n(ZTNs) within the ZSM framework, covering network optimization, traffic\nmonitoring, energy efficiency, and security aspects of next-generational\nnetworks. The paper explores the challenges associated with ZSM, particularly\nthose related to ML, which necessitate the need to explore diverse network\nautomation solutions. In this context, the study investigates the application\nof Automated ML (AutoML) in ZTNs, to reduce network management costs and\nenhance performance. AutoML automates the selection and tuning process of a ML\nmodel for a given task. Specifically, the focus is on AutoML's ability to\npredict application throughput and autonomously adapt to data drift.\nExperimental results demonstrate the superiority of the proposed AutoML\npipeline over traditional ML in terms of prediction accuracy. Integrating\nAutoML and ZSM concepts significantly reduces network configuration and\nmanagement efforts, allowing operators to allocate more time and resources to\nother important tasks. The paper also provides a high-level 5G system\narchitecture incorporating AutoML and ZSM concepts. This research highlights\nthe potential of ZTNs and AutoML to revolutionize the management of 5G+\nnetworks, enabling automated decision-making and empowering network operators\nto achieve higher efficiency, improved performance, and enhanced user\nexperience.",
            "author": [
                "Mirna El Rajab",
                "Li Yang",
                "Abdallah Shami"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04159v1",
                "http://arxiv.org/pdf/2312.04159v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.LG",
                "68T01, 68M10, 90B18",
                "I.2.0; I.2.2; C.2.0"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04157v1",
            "title": "Evolution of the magnetic field and flows of solar active regions with\n  persistent magnetic bipoles before emergence",
            "updated": "2023-12-07T09:15:02Z",
            "published": "2023-12-07T09:15:02Z",
            "summary": "Magnetic active regions on the Sun are harbingers of space weather events.\nUnderstanding the physics of how they form and evolve will improve space\nweather forecasting. Our aim is to characterise the surface magnetic field and\nflows for a sample of active regions with persistent magnetic bipoles prior to\nemergence. We identified 42 emerging active regions (EARs), in the Solar\nDynamics Observatory Helioseismic Emerging Active Region survey (Schunker et\nal. 2016), associated with small magnetic bipoles at least one day before the\ntime of emergence. We then identified a contrasting sample of 42 EARs that\nemerge more abruptly without bipoles before emergence. We computed the\nsupergranulation scale surface flows using helioseismic holography. We averaged\nthe flow maps and magnetic field maps over all active regions in each sample at\neach time interval from 2 days before emergence to 1 day after. We found that\nEARs associated with a persistent pre-emergence bipole evolve to be, on\naverage, lower flux active regions than EARs that emerge more abruptly.\nFurther, we found that the EARs that emerge more abruptly do so with a\ndiverging flow of $(3\\pm 0.6) \\times 10^{-6}$ s$^{-1}$ on the order of 50-100\nms$^{-1}$. Our results suggest that there is a statistical dependence of the\nsurface flow signature throughout the emergence process on the maximum magnetic\nflux of the active region.",
            "author": [
                "Camron S. Alley",
                "Hannah Schunker"
            ],
            "link": [
                "http://dx.doi.org/10.1017/pasa.2023.52",
                "http://arxiv.org/abs/2312.04157v1",
                "http://arxiv.org/pdf/2312.04157v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04152v1",
            "title": "EulerMormer: Robust Eulerian Motion Magnification via Dynamic Filtering\n  within Transformer",
            "updated": "2023-12-07T09:10:16Z",
            "published": "2023-12-07T09:10:16Z",
            "summary": "Video Motion Magnification (VMM) aims to break the resolution limit of human\nvisual perception capability and reveal the imperceptible minor motion that\ncontains valuable information in the macroscopic domain. However, challenges\narise in this task due to photon noise inevitably introduced by photographic\ndevices and spatial inconsistency in amplification, leading to flickering\nartifacts in static fields and motion blur and distortion in dynamic fields in\nthe video. Existing methods focus on explicit motion modeling without\nemphasizing prioritized denoising during the motion magnification process. This\npaper proposes a novel dynamic filtering strategy to achieve static-dynamic\nfield adaptive denoising. Specifically, based on Eulerian theory, we separate\ntexture and shape to extract motion representation through inter-frame shape\ndifferences, expecting to leverage these subdivided features to solve this task\nfinely. Then, we introduce a novel dynamic filter that eliminates noise cues\nand preserves critical features in the motion magnification and amplification\ngeneration phases. Overall, our unified framework, EulerMormer, is a pioneering\neffort to first equip with Transformer in learning-based VMM. The core of the\ndynamic filter lies in a global dynamic sparse cross-covariance attention\nmechanism that explicitly removes noise while preserving vital information,\ncoupled with a multi-scale dual-path gating mechanism that selectively\nregulates the dependence on different frequency features to reduce spatial\nattenuation and complement motion boundaries. We demonstrate extensive\nexperiments that EulerMormer achieves more robust video motion magnification\nfrom the Eulerian perspective, significantly outperforming state-of-the-art\nmethods. The source code is available at\nhttps://github.com/VUT-HFUT/EulerMormer.",
            "author": [
                "Fei Wang",
                "Dan Guo",
                "Kun Li",
                "Meng Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04152v1",
                "http://arxiv.org/pdf/2312.04152v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04145v1",
            "title": "Diffusing Colors: Image Colorization with Text Guided Diffusion",
            "updated": "2023-12-07T08:59:20Z",
            "published": "2023-12-07T08:59:20Z",
            "summary": "The colorization of grayscale images is a complex and subjective task with\nsignificant challenges. Despite recent progress in employing large-scale\ndatasets with deep neural networks, difficulties with controllability and\nvisual quality persist. To tackle these issues, we present a novel image\ncolorization framework that utilizes image diffusion techniques with granular\ntext prompts. This integration not only produces colorization outputs that are\nsemantically appropriate but also greatly improves the level of control users\nhave over the colorization process. Our method provides a balance between\nautomation and control, outperforming existing techniques in terms of visual\nquality and semantic coherence. We leverage a pretrained generative Diffusion\nModel, and show that we can finetune it for the colorization task without\nlosing its generative power or attention to text prompts. Moreover, we present\na novel CLIP-based ranking model that evaluates color vividness, enabling\nautomatic selection of the most suitable level of vividness based on the\nspecific scene semantics. Our approach holds potential particularly for color\nenhancement and historical image colorization.",
            "author": [
                "Nir Zabari",
                "Aharon Azulay",
                "Alexey Gorkor",
                "Tavi Halperin",
                "Ohad Fried"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04145v1",
                "http://arxiv.org/pdf/2312.04145v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04143v1",
            "title": "Towards 4D Human Video Stylization",
            "updated": "2023-12-07T08:58:33Z",
            "published": "2023-12-07T08:58:33Z",
            "summary": "We present a first step towards 4D (3D and time) human video stylization,\nwhich addresses style transfer, novel view synthesis and human animation within\na unified framework. While numerous video stylization methods have been\ndeveloped, they are often restricted to rendering images in specific viewpoints\nof the input video, lacking the capability to generalize to novel views and\nnovel poses in dynamic scenes. To overcome these limitations, we leverage\nNeural Radiance Fields (NeRFs) to represent videos, conducting stylization in\nthe rendered feature space. Our innovative approach involves the simultaneous\nrepresentation of both the human subject and the surrounding scene using two\nNeRFs. This dual representation facilitates the animation of human subjects\nacross various poses and novel viewpoints. Specifically, we introduce a novel\ngeometry-guided tri-plane representation, significantly enhancing feature\nrepresentation robustness compared to direct tri-plane optimization. Following\nthe video reconstruction, stylization is performed within the NeRFs' rendered\nfeature space. Extensive experiments demonstrate that the proposed method\nstrikes a superior balance between stylized textures and temporal coherence,\nsurpassing existing approaches. Furthermore, our framework uniquely extends its\ncapabilities to accommodate novel poses and viewpoints, making it a versatile\ntool for creative human video stylization.",
            "author": [
                "Tiantian Wang",
                "Xinxin Zuo",
                "Fangzhou Mu",
                "Jian Wang",
                "Ming-Hsuan Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04143v1",
                "http://arxiv.org/pdf/2312.04143v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04141v1",
            "title": "Distributed Approximate Computing with Constant Locality",
            "updated": "2023-12-07T08:56:35Z",
            "published": "2023-12-07T08:56:35Z",
            "summary": "We consider a distributed coding for computing problem with constant decoding\nlocality, i.e. with a vanishing error probability, any single sample of the\nfunction can be approximately recovered by probing only constant number of\ncompressed bits. We establish an achievable rate region by designing an\nefficient coding scheme. The scheme reduces the required rate by introducing\nauxiliary random variables and supports local decoding at the same time. Then\nwe show the rate region is optimal under mild regularity conditions on source\ndistributions. A coding for computing problem with side information is\nanalogously studied. These results indicate that more rate has to be taken in\norder to achieve lower coding complexity in distributed computing settings.\nMoreover, useful graph characterizations are developed to simplify the\ncomputation of the achievable rate region.",
            "author": [
                "Deheng Yuan",
                "Tao Guo",
                "Zhongyi Huang",
                "Shi Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04141v1",
                "http://arxiv.org/pdf/2312.04141v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04140v1",
            "title": "Polarimetric Light Transport Analysis for Specular Inter-reflection",
            "updated": "2023-12-07T08:55:28Z",
            "published": "2023-12-07T08:55:28Z",
            "summary": "Polarization is well known for its ability to decompose diffuse and specular\nreflections. However, the existing decomposition methods only focus on direct\nreflection and overlook multiple reflections, especially specular\ninter-reflection. In this paper, we propose a novel decomposition method for\nhandling specular inter-reflection of metal objects by using a unique\npolarimetric feature: the rotation direction of linear polarization. This\nrotation direction serves as a discriminative factor between direct and\ninter-reflection on specular surfaces. To decompose the reflectance components,\nwe actively rotate the linear polarization of incident light and analyze the\nrotation direction of the reflected light. We evaluate our method using both\nsynthetic and real data, demonstrating its effectiveness in decomposing\nspecular inter-reflections of metal objects. Furthermore, we demonstrate that\nour method can be combined with other decomposition methods for a detailed\nanalysis of light transport. As a practical application, we show its\neffectiveness in improving the accuracy of 3D measurement against strong\nspecular inter-reflection.",
            "author": [
                "Ryota Maeda",
                "Shinsaku Hiura"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04140v1",
                "http://arxiv.org/pdf/2312.04140v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04135v1",
            "title": "A Novel Federated Learning-based Intrusion Detection System for Flying\n  Ad Hoc Networks",
            "updated": "2023-12-07T08:50:25Z",
            "published": "2023-12-07T08:50:25Z",
            "summary": "Unmanned aerial vehicles (UAVs) in flying ad-hoc networks (FANETs) face\nsecurity challenges due to the dynamic and distributed nature of these\nnetworks. This paper presents the Federated Learning-based Intrusion Detection\nSystem (FL-IDS), an innovative approach designed to improve FANET security.\nFL-IDS leverages federated learning to address privacy concerns of centralized\nintrusion detection systems. FL-IDS operates in a decentralized manner,\nenabling UAVs to collaboratively train a global intrusion detection model\nwithout sharing raw data. Local models are assigned to each UAV, using\nclient-specific data, and only updated model weights are shared with a central\nserver. This preserves privacy while utilizing collective intelligence for\neffective intrusion detection. Experimental results show FL-IDS's competitive\nperformance with Central IDS (C-IDS) while mitigating privacy concerns. The\nBias Towards Specific Clients (BTSC) method further enhances FL-IDS\nperformance, surpassing C-IDS even at lower attacker ratios. A comparative\nanalysis with traditional intrusion detection methods, including Local IDS\n(L-IDS), provides insights into FL-IDS's strengths. This study significantly\ncontributes to FANET security by introducing a privacy-aware, decentralized\nintrusion detection approach tailored to the unique challenges of UAV networks.",
            "author": [
                "Ozlem Ceviz",
                "Pinar Sadioglu",
                "Sevil Sen",
                "Vassilios G. Vassilakis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04135v1",
                "http://arxiv.org/pdf/2312.04135v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04134v1",
            "title": "Using a Large Language Model to generate a Design Structure Matrix",
            "updated": "2023-12-07T08:48:54Z",
            "published": "2023-12-07T08:48:54Z",
            "summary": "The Design Structure Matrix (DSM) is an established method used in dependency\nmodelling, especially in the design of complex engineering systems. The\ngeneration of DSM is traditionally carried out through manual means and can\ninvolve interviewing experts to elicit critical system elements and the\nrelationships between them. Such manual approaches can be time-consuming and\ncostly. This paper presents a workflow that uses a Large Language Model (LLM)\nto support the generation of DSM and improve productivity. A prototype of the\nworkflow was developed in this work and applied on a diesel engine DSM\npublished previously. It was found that the prototype could reproduce 357 out\nof 462 DSM entries published (i.e. 77.3%), suggesting that the work can aid DSM\ngeneration. A no-code version of the prototype is made available online to\nsupport future research.",
            "author": [
                "Edwin C. Y. Koh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04134v1",
                "http://arxiv.org/pdf/2312.04134v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04129v1",
            "title": "Nanoscale magnets embedded in a microstrip",
            "updated": "2023-12-07T08:37:35Z",
            "published": "2023-12-07T08:37:35Z",
            "summary": "Nanoscale magnetic resonance imaging (NanoMRI) is an active area of applied\nresearch with potential use in structural biology and quantum engineering. The\nsuccess of this technological vision hinges on improving the instrument's\nsensitivity and functionality. A particular challenge is the optimization of\nthe magnetic field gradient required for spatial encoding, and of the\nradio-frequency field used for spin control, in analogy to the components used\nin clinical MRI. In this work, we present the fabrication and characterization\nof a magnet-in-microstrip device that yields a compact form factor for both\nelements. We find that our design leads to a number of advantages, among them a\nfourfold increase of the magnetic field gradient compared to those achieved\nwith traditional fabrication methods. Our results can be useful for boosting\nthe efficiency of a variety of different experimental arrangements and\ndetection principles in the field of NanoMRI.",
            "author": [
                "Raphael Pachlatko",
                "Nils Prumbaum",
                "Marc-Dominik Krass",
                "Urs Grob",
                "Christian L. Degen",
                "Alexander Eichler"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04129v1",
                "http://arxiv.org/pdf/2312.04129v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph",
                "cond-mat.mes-hall"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04127v1",
            "title": "Analyzing the Inherent Response Tendency of LLMs: Real-World\n  Instructions-Driven Jailbreak",
            "updated": "2023-12-07T08:29:58Z",
            "published": "2023-12-07T08:29:58Z",
            "summary": "Extensive work has been devoted to improving the safety mechanism of Large\nLanguage Models (LLMs). However, in specific scenarios, LLMs still generate\nharmful responses when faced with malicious instructions, a phenomenon referred\nto as \"Jailbreak Attack\". In our research, we introduce a novel jailbreak\nattack method (\\textbf{RADIAL}), which consists of two steps: 1) Inherent\nResponse Tendency Analysis: we analyze the inherent affirmation and rejection\ntendency of LLMs to react to real-world instructions. 2) Real-World\nInstructions-Driven Jailbreak: based on our analysis, we strategically choose\nseveral real-world instructions and embed malicious instructions into them to\namplify the LLM's potential to generate harmful responses. On three open-source\nhuman-aligned LLMs, our method achieves excellent jailbreak attack performance\nfor both Chinese and English malicious instructions. Besides, we guided\ndetailed ablation experiments and verified the effectiveness of our core idea\n\"Inherent Response Tendency Analysis\". Our exploration also exposes the\nvulnerability of LLMs to being induced into generating more detailed harmful\nresponses in subsequent rounds of dialogue.",
            "author": [
                "Yanrui Du",
                "Sendong Zhao",
                "Ming Ma",
                "Yuhan Chen",
                "Bing Qin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04127v1",
                "http://arxiv.org/pdf/2312.04127v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04126v1",
            "title": "An Improved Scheduling with Advantage Actor-Critic for Storm Workloads",
            "updated": "2023-12-07T08:29:10Z",
            "published": "2023-12-07T08:29:10Z",
            "summary": "Various resources as the essential elements of data centers, and the\ncompletion time is vital to users. In terms of the persistence, the periodicity\nand the spatial-temporal dependence of stream workload, a new Storm scheduler\nwith Advantage Actor-Critic is proposed to improve resource utilization for\nminimizing the completion time. A new weighted embedding with a Graph Neural\nNetwork is designed to depend on the features of a job comprehensively, which\nincludes the dependence, the types and the positions of tasks in a job. An\nimproved Advantage Actor-Critic integrating task chosen and executor assignment\nis proposed to schedule tasks to executors in order to better resource\nutilization. Then the status of tasks and executors are updated for the next\nscheduling. Compared to existing methods, experimental results show that the\nproposed Storm scheduler improves resource utilization. The completion time is\nreduced by almost 17\\% on the TPC-H data set and reduced by almost 25\\% on the\nAlibaba data set.",
            "author": [
                "Gaoqiang Dong",
                "Jia Wang",
                "Mingjing Wang",
                "Tingting Su"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04126v1",
                "http://arxiv.org/pdf/2312.04126v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04125v1",
            "title": "Forensic Iris Image Synthesis",
            "updated": "2023-12-07T08:28:41Z",
            "published": "2023-12-07T08:28:41Z",
            "summary": "Post-mortem iris recognition is an emerging application of iris-based human\nidentification in a forensic setup, able to correctly identify deceased\nsubjects even three weeks post-mortem. This technique thus is considered as an\nimportant component of future forensic toolkits. The current advancements in\nthis field are seriously slowed down by exceptionally difficult data\ncollection, which can happen in mortuary conditions, at crime scenes, or in\n``body farm'' facilities. This paper makes a novel contribution to facilitate\nprogress in post-mortem iris recognition by offering a conditional\nStyleGAN-based iris synthesis model, trained on the largest-available dataset\nof post-mortem iris samples acquired from more than 350 subjects, generating --\nthrough appropriate exploration of StyleGAN latent space -- multiple\nwithin-class (same identity) and between-class (different new identities)\npost-mortem iris images, compliant with ISO/IEC 29794-6, and with decomposition\ndeformations controlled by the requested PMI (post mortem interval). Besides an\nobvious application to enhance the existing, very sparse, post-mortem iris\ndatasets to advance -- among others -- iris presentation attack endeavors, we\nanticipate it may be useful to generate samples that would expose professional\nforensic human examiners to never-seen-before deformations for various PMIs,\nincreasing their training effectiveness. The source codes and model weights are\nmade available with the paper.",
            "author": [
                "Rasel Ahmed Bhuiyan",
                "Adam Czajka"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04125v1",
                "http://arxiv.org/pdf/2312.04125v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04123v1",
            "title": "Approximating the Graph Edit Distance with Compact Neighborhood\n  Representations",
            "updated": "2023-12-07T08:25:00Z",
            "published": "2023-12-07T08:25:00Z",
            "summary": "The graph edit distance is used for comparing graphs in various domains. Due\nto its high computational complexity it is primarily approximated. Widely-used\nheuristics search for an optimal assignment of vertices based on the distance\nbetween local substructures. While faster ones only consider vertices and their\nincident edges, leading to poor accuracy, other approaches require\ncomputationally intense exact distance computations between subgraphs. Our new\nmethod abstracts local substructures to neighborhood trees and compares them\nusing efficient tree matching techniques. This results in a ground distance for\nmapping vertices that yields high quality approximations of the graph edit\ndistance. By limiting the maximum tree height, our method supports steering\nbetween more accurate results and faster execution. We thoroughly analyze the\nrunning time of the tree matching method and propose several techniques to\naccelerate computation in practice. We use compressed tree representations,\nrecognize redundancies by tree canonization and exploit them via caching.\nExperimentally we show that our method provides a significantly improved\ntrade-off between running time and approximation quality compared to existing\nstate-of-the-art approaches.",
            "author": [
                "Franka Bause",
                "Christian Permann",
                "Nils M. Kriege"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04123v1",
                "http://arxiv.org/pdf/2312.04123v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04119v1",
            "title": "A Multilevel Guidance-Exploration Network and Behavior-Scene Matching\n  Method for Human Behavior Anomaly Detection",
            "updated": "2023-12-07T08:20:07Z",
            "published": "2023-12-07T08:20:07Z",
            "summary": "Human behavior anomaly detection aims to identify unusual human actions,\nplaying a crucial role in intelligent surveillance and other areas. The current\nmainstream methods still adopt reconstruction or future frame prediction\ntechniques. However, reconstructing or predicting low-level pixel features\neasily enables the network to achieve overly strong generalization ability,\nallowing anomalies to be reconstructed or predicted as effectively as normal\ndata. Different from their methods, inspired by the Student-Teacher Network, we\npropose a novel framework called the Multilevel Guidance-Exploration\nNetwork(MGENet), which detects anomalies through the difference in high-level\nrepresentation between the Guidance and Exploration network. Specifically, we\nfirst utilize the pre-trained Normalizing Flow that takes skeletal keypoints as\ninput to guide an RGB encoder, which takes unmasked RGB frames as input, to\nexplore motion latent features. Then, the RGB encoder guides the mask encoder,\nwhich takes masked RGB frames as input, to explore the latent appearance\nfeature. Additionally, we design a Behavior-Scene Matching Module(BSMM) to\ndetect scene-related behavioral anomalies. Extensive experiments demonstrate\nthat our proposed method achieves state-of-the-art performance on ShanghaiTech\nand UBnormal datasets, with AUC of 86.9 % and 73.5 %, respectively. The code\nwill be available on https://github.com/molu-ggg/GENet.",
            "author": [
                "Guoqing Yang",
                "Zhiming Luo",
                "Jianzhe Gao",
                "Yingxin Lai",
                "Kun Yang",
                "Yifan He",
                "Shaozi Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04119v1",
                "http://arxiv.org/pdf/2312.04119v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04118v1",
            "title": "Caregiver Talk Shapes Toddler Vision: A Computational Study of Dyadic\n  Play",
            "updated": "2023-12-07T08:18:40Z",
            "published": "2023-12-07T08:18:40Z",
            "summary": "Infants' ability to recognize and categorize objects develops gradually. The\nsecond year of life is marked by both the emergence of more semantic visual\nrepresentations and a better understanding of word meaning. This suggests that\nlanguage input may play an important role in shaping visual representations.\nHowever, even in suitable contexts for word learning like dyadic play sessions,\ncaregivers utterances are sparse and ambiguous, often referring to objects that\nare different from the one to which the child attends. Here, we systematically\ninvestigate to what extent caregivers' utterances can nevertheless enhance\nvisual representations. For this we propose a computational model of visual\nrepresentation learning during dyadic play. We introduce a synthetic dataset of\nego-centric images perceived by a toddler-agent that moves and rotates toy\nobjects in different parts of its home environment while hearing caregivers'\nutterances, modeled as captions. We propose to model toddlers' learning as\nsimultaneously aligning representations for 1) close-in-time images and 2)\nco-occurring images and utterances. We show that utterances with statistics\nmatching those of real caregivers give rise to representations supporting\nimproved category recognition. Our analysis reveals that a small\ndecrease/increase in object-relevant naming frequencies can drastically impact\nthe learned representations. This affects the attention on object names within\nan utterance, which is required for efficient visuo-linguistic alignment.\nOverall, our results support the hypothesis that caregivers' naming utterances\ncan improve toddlers' visual representations.",
            "author": [
                "Timothy Schauml\u00f6ffel",
                "Arthur Aubret",
                "Gemma Roig",
                "Jochen Triesch"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04118v1",
                "http://arxiv.org/pdf/2312.04118v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04117v1",
            "title": "Instance Tracking in 3D Scenes from Egocentric Videos",
            "updated": "2023-12-07T08:18:35Z",
            "published": "2023-12-07T08:18:35Z",
            "summary": "Egocentric sensors such as AR/VR devices capture human-object interactions\nand offer the potential to provide task-assistance by recalling 3D locations of\nobjects of interest in the surrounding environment. This capability requires\ninstance tracking in real-world 3D scenes from egocentric videos (IT3DEgo). We\nexplore this problem by first introducing a new benchmark dataset, consisting\nof RGB and depth videos, per-frame camera pose, and instance-level annotations\nin both 2D camera and 3D world coordinates. We present an evaluation protocol\nwhich evaluates tracking performance in 3D coordinates with two settings for\nenrolling instances to track: (1) single-view online enrollment where an\ninstance is specified on-the-fly based on the human wearer's interactions. and\n(2) multi-view pre-enrollment where images of an instance to be tracked are\nstored in memory ahead of time. To address IT3DEgo, we first re-purpose methods\nfrom relevant areas, e.g., single object tracking (SOT) -- running SOT methods\nto track instances in 2D frames and lifting them to 3D using camera pose and\ndepth. We also present a simple method that leverages pretrained segmentation\nand detection models to generate proposals from RGB frames and match proposals\nwith enrolled instance images. Perhaps surprisingly, our extensive experiments\nshow that our method (with no finetuning) significantly outperforms SOT-based\napproaches. We conclude by arguing that the problem of egocentric instance\ntracking is made easier by leveraging camera pose and using a 3D allocentric\n(world) coordinate representation.",
            "author": [
                "Yunhan Zhao",
                "Haoyu Ma",
                "Shu Kong",
                "Charless Fowlkes"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04117v1",
                "http://arxiv.org/pdf/2312.04117v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04116v1",
            "title": "Unconventional mechanical and thermal behaviors of MOF CALF-20",
            "updated": "2023-12-07T08:06:34Z",
            "published": "2023-12-07T08:06:34Z",
            "summary": "CALF-20 was recently identified as a novel benchmark sorbent for CO$_2$\ncapture at the industrial scale, however comprehensive atomistic insight into\nits mechanical/thermal properties under working conditions is still lacking. In\nthis study, we developed a general-purpose machine-learned potential (MLP) for\nthe CALF-20 MOF framework that predicts the thermodynamic and mechanical\nproperties of the structure at finite temperatures within first-principles\naccuracy. Interestingly, CALF-20 was demonstrated to exhibit both negative area\ncompression and negative thermal expansion. Most strikingly, upon application\nof the tensile strain along the [001] direction, CALF-20 was shown to display a\ndistinct two-step elastic deformation behavior, unlike typical MOFs that\nundergo plastic deformation after elasticity. Furthermore, this MOF was shown\nto exhibit a spectacular fracture strain of up to 27% along the [001] direction\nat room temperature comparable to that of MOF glasses. These abnormal thermal\nand mechanical properties make CALF-20 as attractive material for flexible and\nstretchable electronics and sensors.",
            "author": [
                "Dong Fan",
                "Supriyo Naskar",
                "Guillaume Maurin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04116v1",
                "http://arxiv.org/pdf/2312.04116v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04113v1",
            "title": "Multi-strategy Collaborative Optimized YOLOv5s and its Application in\n  Distance Estimation",
            "updated": "2023-12-07T07:59:10Z",
            "published": "2023-12-07T07:59:10Z",
            "summary": "The increasing accident rate brought about by the explosive growth of\nautomobiles has made the research on active safety systems of automobiles\nincreasingly important. The importance of improving the accuracy of vehicle\ntarget detection is self-evident. To achieve the goals of vehicle detection and\ndistance estimation and provide safety warnings, a Distance Estimation Safety\nWarning System (DESWS) based on a new neural network model (YOLOv5s-SE) by\nreplacing the IoU with DIoU, embedding SE attention module, and a distance\nestimation method through using the principle of similar triangles was\nproposed. In addition, a method that can give safety suggestions based on the\nestimated distance using nonparametric testing was presented in this work.\nThrough the simulation experiment, it was verified that the mAP was improved by\n5.5% and the purpose of giving safety suggestions based on the estimated\ndistance information can be achieved.",
            "author": [
                "Zijian Shen",
                "Zhenping Mu",
                "Xiangxiang Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04113v1",
                "http://arxiv.org/pdf/2312.04113v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04109v1",
            "title": "Bridge the Present and Future: A Cross-Layer Matching Game in Dynamic\n  Cloud-Aided Mobile Edge Networks",
            "updated": "2023-12-07T07:52:55Z",
            "published": "2023-12-07T07:52:55Z",
            "summary": "Cloud-aided mobile edge networks (CAMENs) allow edge servers (ESs) to\npurchase resources from remote cloud servers (CSs), while overcoming resource\nshortage when handling computation-intensive tasks of mobile users (MUs).\nConventional trading mechanisms (e.g., onsite trading) confront many\nchallenges, including decision-making overhead (e.g., latency) and potential\ntrading failures. This paper investigates a series of cross-layer matching\nmechanisms to achieve stable and cost-effective resource provisioning across\ndifferent layers (i.e., MUs, ESs, CSs), seamlessly integrated into a novel\nhybrid paradigm that incorporates futures and spot trading. In futures trading,\nwe explore an overbooking-driven aforehand cross-layer matching (OA-CLM)\nmechanism, facilitating two future contract types: contract between MUs and\nESs, and contract between ESs and CSs, while assessing potential risks under\nhistorical statistical analysis. In spot trading, we design two backup plans\nrespond to current network/market conditions: determination on contractual MUs\nthat should switch to local processing from edge/cloud services; and an onsite\ncross-layer matching (OS-CLM) mechanism that engages participants in real-time\npractical transactions. We next show that our matching mechanisms theoretically\nsatisfy stability, individual rationality, competitive equilibrium, and weak\nPareto optimality. Comprehensive simulations in real-world and numerical\nnetwork settings confirm the corresponding efficacy, while revealing remarkable\nimprovements in time/energy efficiency and social welfare.",
            "author": [
                "Houyi Qi",
                "Minghui Liwang",
                "Xianbin Wang",
                "Li Li",
                "Wei Gong",
                "Jian Jin",
                "Zhenzhen Jiao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04109v1",
                "http://arxiv.org/pdf/2312.04109v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04108v1",
            "title": "Current poisson's ratio values of finite element models are too low to\n  consider soft tissues nearly-incompressible: illustration on the human heel\n  region",
            "updated": "2023-12-07T07:49:49Z",
            "published": "2023-12-07T07:49:49Z",
            "summary": "Finite element analysis of soft tissues is a well-developed method that\nallows estimation of mechanical quantities (e.g. stresses, strains). A\nconstitutive law has to be used to characterise the individual tissues. This is\ncomplex as biological tissues are generally visco-hyperelastic, anisotropic,\nand heterogenous. A specific characteristic, their nearly incompressibility,\nwas well reported in the literature, but very little effort has been made to\ncompare volume variations computed by the simulations with in vivo\nmeasurements. In the present study, volume changes of the fat pad during\ncontrolled indentations of the human heel region were estimated from segmented\nmedical images using digital volume correlation. Indentations were repeated\nwith high and mild intensity normal and shear loads. The experiment was\nreproduced using finite element modelling with several values of Poisson's\nratio for the fat pad, extracted from literature values (from 0.4500 to\n0.4999). Estimated fat pad volume changes were compared to the measured ones to\nassess the best value of Poisson's ratio in each indentation case. The impact\nof the Poisson's ratio on the Jacobian of the deformation gradient and the\nvolumetric strains was also computed. A single value of Poisson's ratio could\nnot fit all the indentation cases. Estimated volume changes were between 0.9 %\n- 11.7 % with a Poisson's ratio from 0.4500 to 0.4999. The best fit was\nobtained with a 0.4900 Poisson's ratio except for the high normal load where a\nvalue of 0.4999 resulted in less error. In conclusion, special care should be\ntaken when setting the Poisson's ratio as the resulting estimated deformations\nmay become unrealistic when the value is far from incompressible materials.",
            "author": [
                "Nolwenn Fougeron",
                "Alessio Trebbi",
                "Bethany Keenan",
                "Yohan Payan",
                "Gregory Chagnon"
            ],
            "link": [
                "http://dx.doi.org/10.1080/10255842.2023.2269286",
                "http://arxiv.org/abs/2312.04108v1",
                "http://arxiv.org/pdf/2312.04108v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04106v1",
            "title": "Identity-Obscured Neural Radiance Fields: Privacy-Preserving 3D Facial\n  Reconstruction",
            "updated": "2023-12-07T07:41:10Z",
            "published": "2023-12-07T07:41:10Z",
            "summary": "Neural radiance fields (NeRF) typically require a complete set of images\ntaken from multiple camera perspectives to accurately reconstruct geometric\ndetails. However, this approach raise significant privacy concerns in the\ncontext of facial reconstruction. The critical need for privacy protection\noften leads invidividuals to be reluctant in sharing their facial images, due\nto fears of potential misuse or security risks. Addressing these concerns, we\npropose a method that leverages privacy-preserving images for reconstructing 3D\nhead geometry within the NeRF framework. Our method stands apart from\ntraditional facial reconstruction techniques as it does not depend on RGB\ninformation from images containing sensitive facial data. Instead, it\neffectively generates plausible facial geometry using a series of\nidentity-obscured inputs, thereby protecting facial privacy.",
            "author": [
                "Jiayi Kong",
                "Baixin Xu",
                "Xurui Song",
                "Chen Qian",
                "Jun Luo",
                "Ying He"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04106v1",
                "http://arxiv.org/pdf/2312.04106v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04103v1",
            "title": "Enhancing the Rationale-Input Alignment for Self-explaining\n  Rationalization",
            "updated": "2023-12-07T07:37:15Z",
            "published": "2023-12-07T07:37:15Z",
            "summary": "Rationalization empowers deep learning models with self-explaining\ncapabilities through a cooperative game, where a generator selects a\nsemantically consistent subset of the input as a rationale, and a subsequent\npredictor makes predictions based on the selected rationale. In this paper, we\ndiscover that rationalization is prone to a problem named \\emph{rationale\nshift}, which arises from the algorithmic bias of the cooperative game.\nRationale shift refers to a situation where the semantics of the selected\nrationale may deviate from the original input, but the predictor still produces\naccurate predictions based on the deviation, resulting in a compromised\ngenerator with misleading feedback.\n  To address this issue, we first demonstrate the importance of the alignment\nbetween the rationale and the full input through both empirical observations\nand theoretical analysis. Subsequently, we introduce a novel approach called\nDAR (\\textbf{D}iscriminatively \\textbf{A}ligned \\textbf{R}ationalization),\nwhich utilizes an auxiliary module pretrained on the full input to\ndiscriminatively align the selected rationale and the original input. We\ntheoretically illustrate how DAR accomplishes the desired alignment, thereby\novercoming the rationale shift problem. The experiments on two widely used\nreal-world benchmarks show that the proposed method significantly improves the\nexplanation quality (measured by the overlap between the model-selected\nexplanation and the human-annotated rationale) as compared to state-of-the-art\ntechniques. Additionally, results on two synthetic settings further validate\nthe effectiveness of DAR in addressing the rationale shift problem.",
            "author": [
                "Wei Liu",
                "Haozhao Wang",
                "Jun Wang",
                "Zhiying Deng",
                "YuanKai Zhang",
                "Cheng Wang",
                "Ruixuan Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04103v1",
                "http://arxiv.org/pdf/2312.04103v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04101v1",
            "title": "Edge computing service deployment and task offloading based on\n  multi-task high-dimensional multi-objective optimization",
            "updated": "2023-12-07T07:30:47Z",
            "published": "2023-12-07T07:30:47Z",
            "summary": "The Mobile Edge Computing (MEC) system located close to the client allows\nmobile smart devices to offload their computations onto edge servers, enabling\nthem to benefit from low-latency computing services. Both cloud service\nproviders and users seek more comprehensive solutions, necessitating judicious\ndecisions in service deployment and task offloading while balancing multiple\nobjectives. This study investigates service deployment and task offloading\nchallenges in a multi-user environment, framing them as a multi-task\nhigh-dimensional multi-objective optimization (MT-HD-MOO) problem within an\nedge environment. To ensure stable service provisioning, beyond considering\nlatency, energy consumption, and cost as deployment objectives, network\nreliability is also incorporated. Furthermore, to promote equitable usage of\nedge servers, load balancing is introduced as a fourth task offloading\nobjective, in addition to latency, energy consumption, and cost. Additionally,\nthis paper designs a MT-HD-MOO algorithm based on a multi-selection strategy to\naddress this model and its solution. By employing diverse selection strategies,\nan environment selection strategy pool is established to enhance population\ndiversity within the high-dimensional objective space. Ultimately, the\nalgorithm's effectiveness is verified through simulation experiments.",
            "author": [
                "Yanheng Guo",
                "Yan Zhang",
                "Linjie Wu",
                "Mengxia Li",
                "Xingjuan Cai",
                "Jinjun Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04101v1",
                "http://arxiv.org/pdf/2312.04101v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04095v1",
            "title": "Learn to Unlearn for Deep Neural Networks: Minimizing Unlearning\n  Interference with Gradient Projection",
            "updated": "2023-12-07T07:17:24Z",
            "published": "2023-12-07T07:17:24Z",
            "summary": "Recent data-privacy laws have sparked interest in machine unlearning, which\ninvolves removing the effect of specific training samples from a learnt model\nas if they were never present in the original training dataset. The challenge\nof machine unlearning is to discard information about the ``forget'' data in\nthe learnt model without altering the knowledge about the remaining dataset and\nto do so more efficiently than the naive retraining approach. To achieve this,\nwe adopt a projected-gradient based learning method, named as\nProjected-Gradient Unlearning (PGU), in which the model takes steps in the\northogonal direction to the gradient subspaces deemed unimportant for the\nretaining dataset, so as to its knowledge is preserved. By utilizing Stochastic\nGradient Descent (SGD) to update the model weights, our method can efficiently\nscale to any model and dataset size. We provide empirically evidence to\ndemonstrate that our unlearning method can produce models that behave similar\nto models retrained from scratch across various metrics even when the training\ndataset is no longer accessible. Our code is available at\nhttps://github.com/hnanhtuan/projected_gradient_unlearning.",
            "author": [
                "Tuan Hoang",
                "Santu Rana",
                "Sunil Gupta",
                "Svetha Venkatesh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04095v1",
                "http://arxiv.org/pdf/2312.04095v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04091v1",
            "title": "Hyperarithmetical Complexity of Infinitary Action Logic with\n  Multiplexing",
            "updated": "2023-12-07T07:05:48Z",
            "published": "2023-12-07T07:05:48Z",
            "summary": "In 2023, Kuznetsov and Speranski introduced infinitary action logic with\nmultiplexing $!^m\\nabla \\mathrm{ACT}_\\omega$ and proved that the derivability\nproblem for it lies between the $\\omega$ and $\\omega^\\omega$ levels of the\nhyperarithmetical hierarchy. We prove that this problem is\n$\\Delta^0_{\\omega^\\omega}$-complete under Turing reductions. Namely, we prove\nthat it is recursively isomorphic to the satisfaction predicate for computable\ninfinitary formulas of rank less than $\\omega^\\omega$ in the language of\narithmetic. We also prove this result for the fragment of $!^m\\nabla\n\\mathrm{ACT}_\\omega$ where Kleene star is not allowed to be in the scope of the\nsubexponential. Finally, we present a family of logics, which are fragments of\n$!^m\\nabla \\mathrm{ACT}_\\omega$, such that the complexity of the $k$-th logic\nis between $\\Delta^0_{\\omega^k}$ and $\\Delta^0_{\\omega^{k+1}}$.",
            "author": [
                "Tikhon Pshenitsyn"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04091v1",
                "http://arxiv.org/pdf/2312.04091v1"
            ],
            "primary_category": "math.LO",
            "category": [
                "math.LO",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04090v1",
            "title": "A Review of Sequential Decision Making via Simulation",
            "updated": "2023-12-07T07:01:24Z",
            "published": "2023-12-07T07:01:24Z",
            "summary": "Optimization via simulation has been well established to find optimal\nsolutions and designs in complex systems. However, it still faces modeling and\ncomputational challenges when extended to the multi-stage setting. This survey\nreviews the models and methodologies of single-stage optimization via\nsimulation and multi-stage stochastic programming. These are necessary\ntheoretical components to push forward the development of sequential decision\nmaking via simulation. We identify the key challenge of sequential decision\nmaking via simulation as the appropriate modeling of the stage-wise value\nfunction, for which we survey the state-of-the-art meta-models and their\npotential solution algorithms.",
            "author": [
                "Zhuo Zhang",
                "Dan Wang",
                "Haoxiang Yang",
                "Shubin Si"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04090v1",
                "http://arxiv.org/pdf/2312.04090v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04089v1",
            "title": "Open-Vocabulary Segmentation with Semantic-Assisted Calibration",
            "updated": "2023-12-07T07:00:09Z",
            "published": "2023-12-07T07:00:09Z",
            "summary": "This paper studies open-vocabulary segmentation (OVS) through calibrating\nin-vocabulary and domain-biased embedding space with generalized contextual\nprior of CLIP. As the core of open-vocabulary understanding, alignment of\nvisual content with the semantics of unbounded text has become the bottleneck\nof this field. To address this challenge, recent works propose to utilize CLIP\nas an additional classifier and aggregate model predictions with CLIP\nclassification results. Despite their remarkable progress, performance of OVS\nmethods in relevant scenarios is still unsatisfactory compared with supervised\ncounterparts. We attribute this to the in-vocabulary embedding and\ndomain-biased CLIP prediction. To this end, we present a Semantic-assisted\nCAlibration Network (SCAN). In SCAN, we incorporate generalized semantic prior\nof CLIP into proposal embedding to avoid collapsing on known categories.\nBesides, a contextual shift strategy is applied to mitigate the lack of global\ncontext and unnatural background noise. With above designs, SCAN achieves\nstate-of-the-art performance on all popular open-vocabulary segmentation\nbenchmarks. Furthermore, we also focus on the problem of existing evaluation\nsystem that ignores semantic duplication across categories, and propose a new\nmetric called Semantic-Guided IoU (SG-IoU).",
            "author": [
                "Yong Liu",
                "Sule Bai",
                "Guanbin Li",
                "Yitong Wang",
                "Yansong Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04089v1",
                "http://arxiv.org/pdf/2312.04089v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04087v1",
            "title": "VRPTEST: Evaluating Visual Referring Prompting in Large Multimodal\n  Models",
            "updated": "2023-12-07T06:53:55Z",
            "published": "2023-12-07T06:53:55Z",
            "summary": "With recent advancements in Large Multimodal Models (LMMs) across various\ndomains, a novel prompting method called visual referring prompting has\nemerged, showing significant potential in enhancing human-computer interaction\nwithin multimodal systems. This method offers a more natural and flexible\napproach to human interaction with these systems compared to traditional text\ndescriptions or coordinates. However, the categorization of visual referring\nprompting remains undefined, and its impact on the performance of LMMs has yet\nto be formally examined. In this study, we conduct the first comprehensive\nanalysis of LMMs using a variety of visual referring prompting strategies. We\nintroduce a benchmark dataset called VRPTEST, comprising 3 different visual\ntasks and 2,275 images, spanning diverse combinations of prompt strategies.\nUsing VRPTEST, we conduct a comprehensive evaluation of eight versions of\nprominent open-source and proprietary foundation models, including two early\nversions of GPT-4V. We develop an automated assessment framework based on\nsoftware metamorphic testing techniques to evaluate the accuracy of LMMs\nwithout the need for human intervention or manual labeling. We find that the\ncurrent proprietary models generally outperform the open-source ones, showing\nan average accuracy improvement of 22.70%; however, there is still potential\nfor improvement. Moreover, our quantitative analysis shows that the choice of\nprompt strategy significantly affects the accuracy of LMMs, with variations\nranging from -17.5% to +7.3%. Further case studies indicate that an appropriate\nvisual referring prompting strategy can improve LMMs' understanding of context\nand location information, while an unsuitable one might lead to answer\nrejection. We also provide insights on minimizing the negative impact of visual\nreferring prompting on LMMs.",
            "author": [
                "Zongjie Li",
                "Chaozheng Wang",
                "Chaowei Liu",
                "Pingchuan Ma",
                "Daoyuan Wu",
                "Shuai Wang",
                "Cuiyun Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04087v1",
                "http://arxiv.org/pdf/2312.04087v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04086v1",
            "title": "MTVG : Multi-text Video Generation with Text-to-Video Models",
            "updated": "2023-12-07T06:53:25Z",
            "published": "2023-12-07T06:53:25Z",
            "summary": "Recently, video generation has attracted massive attention and yielded\nnoticeable outcomes. Concerning the characteristics of video, multi-text\nconditioning incorporating sequential events is necessary for next-step video\ngeneration. In this work, we propose a novel multi-text video generation~(MTVG)\nby directly utilizing a pre-trained diffusion-based text-to-video~(T2V)\ngeneration model without additional fine-tuning. To generate consecutive video\nsegments, visual consistency generated by distinct prompts is necessary with\ndiverse variations, such as motion and content-related transitions. Our\nproposed MTVG includes Dynamic Noise and Last Frame Aware Inversion which\nreinitialize the noise latent to preserve visual coherence between videos of\ndifferent prompts and prevent repetitive motion or contents. Furthermore, we\npresent Structure Guiding Sampling to maintain the global appearance across the\nframes in a single video clip, where we leverage iterative latent updates\nacross the preceding frame. Additionally, our Prompt Generator allows for\narbitrary format of text conditions consisting of diverse events. As a result,\nour extensive experiments, including diverse transitions of descriptions,\ndemonstrate that our proposed methods show superior generated outputs in terms\nof semantically coherent and temporally seamless video.Video examples are\navailable in our project page: https://kuai-lab.github.io/mtvg-page.",
            "author": [
                "Gyeongrok Oh",
                "Jaehwan Jeong",
                "Sieun Kim",
                "Wonmin Byeon",
                "Jinkyu Kim",
                "Sungwoong Kim",
                "Hyeokmin Kwon",
                "Sangpil Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04086v1",
                "http://arxiv.org/pdf/2312.04086v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04077v1",
            "title": "When is Plasmode simulation superior to parametric simulation when\n  estimating the MSE of the least squares estimator in linear regression?",
            "updated": "2023-12-07T06:44:00Z",
            "published": "2023-12-07T06:44:00Z",
            "summary": "Simulation is a crucial tool for the evaluation and comparison of statistical\nmethods. How to design fair and neutral simulation studies is therefore of\ngreat interest for both researchers developing new methods and practitioners\nconfronted with the choice of the most suitable method. The term simulation\nusually refers to parametric simulation, that is, computer experiments using\nartificial data made up of pseudo-random numbers. Plasmode simulation, that is,\ncomputer experiments using the combination of resampling feature data from a\nreal-life dataset and generating the target variable with a user-selected\noutcome-generating model (OGM), is an alternative that is often claimed to\nproduce more realistic data. We compare parametric and Plasmode simulation for\nthe example of estimating the mean squared error of the least squares estimator\nin linear regression. If the true underlying data-generating process (DGP) and\nthe OGM were known, parametric simulation would be the best choice in terms of\nestimating the MSE well. However, in reality, both are usually unknown, so\nresearchers have to make assumptions: in Plasmode simulation studies for the\nOGM, in parametric simulation for both DGP and OGM. Most likely, these\nassumptions do not reflect the truth. Here, we aim to find out how assumptions\ndeviating from the true DGP and the true OGM affect the performance of\nparametric simulation and Plasmode simulations in the context of MSE estimation\nfor the least squares estimator and in which situations which simulation type\nis preferable. Our results suggest that the preferable simulation method\ndepends on many factors, including the number of features, and how the\nassumptions of a parametric simulation differ from the true DGP. Also, the\nresampling strategy used for Plasmode influences the results. In particular,\nsubsampling with a small sampling proportion can be recommended.",
            "author": [
                "Marieke Stolte",
                "Nicholas Schreck",
                "Alla Slynko",
                "Maral Saadati",
                "Axel Benner",
                "J\u00f6rg Rahnenf\u00fchrer",
                "Andrea Bommert"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04077v1",
                "http://arxiv.org/pdf/2312.04077v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "stat.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04076v1",
            "title": "Large Language Models are Good Prompt Learners for Low-Shot Image\n  Classification",
            "updated": "2023-12-07T06:43:34Z",
            "published": "2023-12-07T06:43:34Z",
            "summary": "Low-shot image classification, where training images are limited or\ninaccessible, has benefited from recent progress on pre-trained vision-language\n(VL) models with strong generalizability, e.g. CLIP. Prompt learning methods\nbuilt with VL models generate text features from the class names that only have\nconfined class-specific information. Large Language Models (LLMs), with their\nvast encyclopedic knowledge, emerge as the complement. Thus, in this paper, we\ndiscuss the integration of LLMs to enhance pre-trained VL models, specifically\non low-shot classification. However, the domain gap between language and vision\nblocks the direct application of LLMs. Thus, we propose LLaMP, Large Language\nModels as Prompt learners, that produces adaptive prompts for the CLIP text\nencoder, establishing it as the connecting bridge. Experiments show that,\ncompared with other state-of-the-art prompt learning methods, LLaMP yields\nbetter performance on both zero-shot generalization and few-shot image\nclassification, over a spectrum of 11 datasets.",
            "author": [
                "Zhaoheng Zheng",
                "Jingmin Wei",
                "Xuefeng Hu",
                "Haidong Zhu",
                "Ram Nevatia"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04076v1",
                "http://arxiv.org/pdf/2312.04076v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04073v1",
            "title": "Information Design for Hybrid Work under Infectious Disease Transmission\n  Risk",
            "updated": "2023-12-07T06:34:56Z",
            "published": "2023-12-07T06:34:56Z",
            "summary": "We study a planner's provision of information to manage workplace occupancy\nwhen strategic workers (agents) face risk of infectious disease transmission.\nThe planner implements an information mechanism to signal information about the\nunderlying risk of infection at the workplace. Agents update their belief over\nthe risk parameter using this information and choose to work in-person or\nremotely. We address the design of the optimal signaling mechanism that best\naligns the workplace occupancy with the planner's preference (i.e., maintaining\nsafe capacity limits and operational efficiency at workplace). For various\nforms of planner preferences, we show numerical and analytical proof that\ninterval-based information mechanisms are optimal. These mechanisms partition\nthe continuous domain of the risk parameter into disjoint intervals and\nprovision information based on interval-specific probability distributions over\na finite set of signals. When the planner seeks to achieve an occupancy that\nlies in one of finitely many pre-specified ranges independent of the underlying\nrisk, we provide an optimal mechanism that uses at most two intervals. On the\nother hand, when the preference on the occupancy is risk-dependent, we show\nthat an approximately optimal interval-based mechanism can be computed\nefficiently. We bound the approximation loss for preferences that are expressed\nthrough a Lipschitz continuous function of both occupancy and risk parameter.\nWe provide examples that demonstrate the improvement of proposed signaling\nmechanisms relative to the common benchmarks in information provision. Our\nfindings suggest that information provision over the risk of disease\ntransmission is an effective intervention for maintaining desirable occupancy\nlevels at the workplace.",
            "author": [
                "Sohil Shah",
                "Saurabh Amin",
                "Patrick Jaillet"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04073v1",
                "http://arxiv.org/pdf/2312.04073v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04070v1",
            "title": "A Transformer Model for Symbolic Regression towards Scientific Discovery",
            "updated": "2023-12-07T06:27:48Z",
            "published": "2023-12-07T06:27:48Z",
            "summary": "Symbolic Regression (SR) searches for mathematical expressions which best\ndescribe numerical datasets. This allows to circumvent interpretation issues\ninherent to artificial neural networks, but SR algorithms are often\ncomputationally expensive. This work proposes a new Transformer model aiming at\nSymbolic Regression particularly focused on its application for Scientific\nDiscovery. We propose three encoder architectures with increasing flexibility\nbut at the cost of column-permutation equivariance violation. Training results\nindicate that the most flexible architecture is required to prevent from\noverfitting. Once trained, we apply our best model to the SRSD datasets\n(Symbolic Regression for Scientific Discovery datasets) which yields\nstate-of-the-art results using the normalized tree-based edit distance, at no\nextra computational cost.",
            "author": [
                "Florian Lalande",
                "Yoshitomo Matsubara",
                "Naoya Chiba",
                "Tatsunori Taniai",
                "Ryo Igarashi",
                "Yoshitala Ushiku"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04070v1",
                "http://arxiv.org/pdf/2312.04070v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04068v1",
            "title": "Making Translators Privacy-aware on the User's Side",
            "updated": "2023-12-07T06:23:17Z",
            "published": "2023-12-07T06:23:17Z",
            "summary": "We propose PRISM to enable users of machine translation systems to preserve\nthe privacy of data on their own initiative. There is a growing demand to apply\nmachine translation systems to data that require privacy protection. While\nseveral machine translation engines claim to prioritize privacy, the extent and\nspecifics of such protection are largely ambiguous. First, there is often a\nlack of clarity on how and to what degree the data is protected. Even if\nservice providers believe they have sufficient safeguards in place,\nsophisticated adversaries might still extract sensitive information. Second,\nvulnerabilities may exist outside of these protective measures, such as within\ncommunication channels, potentially leading to data leakage. As a result, users\nare hesitant to utilize machine translation engines for data demanding high\nlevels of privacy protection, thereby missing out on their benefits. PRISM\nresolves this problem. Instead of relying on the translation service to keep\ndata safe, PRISM provides the means to protect data on the user's side. This\napproach ensures that even machine translation engines with inadequate privacy\nmeasures can be used securely. For platforms already equipped with privacy\nsafeguards, PRISM acts as an additional protection layer, reinforcing their\nsecurity furthermore. PRISM adds these privacy features without significantly\ncompromising translation accuracy. Our experiments demonstrate the\neffectiveness of PRISM using real-world translators, T5 and ChatGPT\n(GPT-3.5-turbo), and the datasets with two languages. PRISM effectively\nbalances privacy protection with translation accuracy.",
            "author": [
                "Ryoma Sato"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04068v1",
                "http://arxiv.org/pdf/2312.04068v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04067v1",
            "title": "MeanCut: A Greedy-Optimized Graph Clustering via Path-based Similarity\n  and Degree Descent Criterion",
            "updated": "2023-12-07T06:19:39Z",
            "published": "2023-12-07T06:19:39Z",
            "summary": "As the most typical graph clustering method, spectral clustering is popular\nand attractive due to the remarkable performance, easy implementation, and\nstrong adaptability. Classical spectral clustering measures the edge weights of\ngraph using pairwise Euclidean-based metric, and solves the optimal graph\npartition by relaxing the constraints of indicator matrix and performing\nLaplacian decomposition. However, Euclidean-based similarity might cause skew\ngraph cuts when handling non-spherical data distributions, and the relaxation\nstrategy introduces information loss. Meanwhile, spectral clustering requires\nspecifying the number of clusters, which is hard to determine without enough\nprior knowledge. In this work, we leverage the path-based similarity to enhance\nintra-cluster associations, and propose MeanCut as the objective function and\ngreedily optimize it in degree descending order for a nondestructive graph\npartition. This algorithm enables the identification of arbitrary shaped\nclusters and is robust to noise. To reduce the computational complexity of\nsimilarity calculation, we transform optimal path search into generating the\nmaximum spanning tree (MST), and develop a fast MST (FastMST) algorithm to\nfurther improve its time-efficiency. Moreover, we define a density gradient\nfactor (DGF) for separating the weakly connected clusters. The validity of our\nalgorithm is demonstrated by testifying on real-world benchmarks and\napplication of face recognition. The source code of MeanCut is available at\nhttps://github.com/ZPGuiGroupWhu/MeanCut-Clustering.",
            "author": [
                "Dehua Peng",
                "Zhipeng Gui",
                "Huayi Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04067v1",
                "http://arxiv.org/pdf/2312.04067v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "I.5.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04066v1",
            "title": "Combining inherent knowledge of vision-language models with unsupervised\n  domain adaptation through self-knowledge distillation",
            "updated": "2023-12-07T06:16:39Z",
            "published": "2023-12-07T06:16:39Z",
            "summary": "Unsupervised domain adaptation (UDA) tries to overcome the tedious work of\nlabeling data by leveraging a labeled source dataset and transferring its\nknowledge to a similar but different target dataset. On the other hand, current\nvision-language models exhibit astonishing zero-shot prediction capabilities.\nIn this work, we combine knowledge gained through UDA with the inherent\nknowledge of vision-language models. In a first step, we generate the zero-shot\npredictions of the source and target dataset using the vision-language model.\nSince zero-shot predictions usually exhibit a large entropy, meaning that the\nclass probabilities are rather evenly distributed, we first adjust the\ndistribution to accentuate the winning probabilities. This is done using both\nsource and target data to keep the relative confidence between source and\ntarget data. We then employ a conventional DA method, to gain the knowledge\nfrom the source dataset, in combination with self-knowledge distillation, to\nmaintain the inherent knowledge of the vision-language model. We further\ncombine our method with a gradual source domain expansion strategy (GSDE) and\nshow that this strategy can also benefit by including zero-shot predictions. We\nconduct experiments and ablation studies on three benchmarks (OfficeHome,\nVisDA, and DomainNet) and outperform state-of-the-art methods. We further show\nin ablation studies the contributions of different parts of our algorithm.",
            "author": [
                "Thomas Westfechtel",
                "Dexuan Zhang",
                "Tatsuya Harada"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04066v1",
                "http://arxiv.org/pdf/2312.04066v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04063v1",
            "title": "An unsupervised approach towards promptable defect segmentation in\n  laser-based additive manufacturing by Segment Anything",
            "updated": "2023-12-07T06:03:07Z",
            "published": "2023-12-07T06:03:07Z",
            "summary": "Foundation models are currently driving a paradigm shift in computer vision\ntasks for various fields including biology, astronomy, and robotics among\nothers, leveraging user-generated prompts to enhance their performance. In the\nmanufacturing domain, accurate image-based defect segmentation is imperative to\nensure product quality and facilitate real-time process control. However, such\ntasks are often characterized by multiple challenges including the absence of\nlabels and the requirement for low latency inference among others. To address\nthese issues, we construct a framework for image segmentation using a\nstate-of-the-art Vision Transformer (ViT) based Foundation model (Segment\nAnything Model) with a novel multi-point prompt generation scheme using\nunsupervised clustering. We apply our framework to perform real-time porosity\nsegmentation in a case study of laser base powder bed fusion (L-PBF) and obtain\nhigh Dice Similarity Coefficients (DSC) without the necessity for any\nsupervised fine-tuning in the model. Using such lightweight foundation model\ninference in conjunction with unsupervised prompt generation, we envision the\nconstruction of a real-time anomaly detection pipeline that has the potential\nto revolutionize the current laser-based additive manufacturing processes,\nthereby facilitating the shift towards Industry 4.0 and promoting defect-free\nproduction along with operational efficiency.",
            "author": [
                "Israt Zarin Era",
                "Imtiaz Ahmed",
                "Zhichao Liu",
                "Srinjoy Das"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04063v1",
                "http://arxiv.org/pdf/2312.04063v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04060v1",
            "title": "Differentiable Registration of Images and LiDAR Point Clouds with\n  VoxelPoint-to-Pixel Matching",
            "updated": "2023-12-07T05:46:10Z",
            "published": "2023-12-07T05:46:10Z",
            "summary": "Cross-modality registration between 2D images from cameras and 3D point\nclouds from LiDARs is a crucial task in computer vision and robotic. Previous\nmethods estimate 2D-3D correspondences by matching point and pixel patterns\nlearned by neural networks, and use Perspective-n-Points (PnP) to estimate\nrigid transformation during post-processing. However, these methods struggle to\nmap points and pixels to a shared latent space robustly since points and pixels\nhave very different characteristics with patterns learned in different manners\n(MLP and CNN), and they also fail to construct supervision directly on the\ntransformation since the PnP is non-differentiable, which leads to unstable\nregistration results. To address these problems, we propose to learn a\nstructured cross-modality latent space to represent pixel features and 3D\nfeatures via a differentiable probabilistic PnP solver. Specifically, we design\na triplet network to learn VoxelPoint-to-Pixel matching, where we represent 3D\nelements using both voxels and points to learn the cross-modality latent space\nwith pixels. We design both the voxel and pixel branch based on CNNs to operate\nconvolutions on voxels/pixels represented in grids, and integrate an additional\npoint branch to regain the information lost during voxelization. We train our\nframework end-to-end by imposing supervisions directly on the predicted pose\ndistribution with a probabilistic PnP solver. To explore distinctive patterns\nof cross-modality features, we design a novel loss with adaptive-weighted\noptimization for cross-modality feature description. The experimental results\non KITTI and nuScenes datasets show significant improvements over the\nstate-of-the-art methods. The code and models are available at\nhttps://github.com/junshengzhou/VP2P-Match.",
            "author": [
                "Junsheng Zhou",
                "Baorui Ma",
                "Wenyuan Zhang",
                "Yi Fang",
                "Yu-Shen Liu",
                "Zhizhong Han"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04060v1",
                "http://arxiv.org/pdf/2312.04060v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04059v1",
            "title": "Comparing Large Language Model AI and Human-Generated Coaching Messages\n  for Behavioral Weight Loss",
            "updated": "2023-12-07T05:45:24Z",
            "published": "2023-12-07T05:45:24Z",
            "summary": "Automated coaching messages for weight control can save time and costs, but\ntheir repetitive, generic nature may limit their effectiveness compared to\nhuman coaching. Large language model (LLM) based artificial intelligence (AI)\nchatbots, like ChatGPT, could offer more personalized and novel messages to\naddress repetition with their data-processing abilities. While LLM AI\ndemonstrates promise to encourage healthier lifestyles, studies have yet to\nexamine the feasibility and acceptability of LLM-based BWL coaching. 87 adults\nin a weight-loss trial rated ten coaching messages' helpfulness (five\nhuman-written, five ChatGPT-generated) using a 5-point Likert scale, providing\nadditional open-ended feedback to justify their ratings. Participants also\nidentified which messages they believed were AI-generated. The evaluation\noccurred in two phases: messages in Phase 1 were perceived as impersonal and\nnegative, prompting revisions for Phase 2 messages. In Phase 1, AI-generated\nmessages were rated less helpful than human-written ones, with 66 percent\nreceiving a helpfulness rating of 3 or higher. However, in Phase 2, the AI\nmessages matched the human-written ones regarding helpfulness, with 82% scoring\nthree or above. Additionally, 50% were misidentified as human-written,\nsuggesting AI's sophistication in mimicking human-generated content. A thematic\nanalysis of open-ended feedback revealed that participants appreciated AI's\nempathy and personalized suggestions but found them more formulaic, less\nauthentic, and too data-focused. This study reveals the preliminary feasibility\nand acceptability of LLM AIs, like ChatGPT, in crafting potentially effective\nweight control coaching messages. Our findings also underscore areas for future\nenhancement.",
            "author": [
                "Zhuoran Huang",
                "Michael P. Berry",
                "Christina Chwyl",
                "Gary Hsieh",
                "Jing Wei",
                "Evan M. Forman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04059v1",
                "http://arxiv.org/pdf/2312.04059v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04052v1",
            "title": "Multimodal Misinformation Detection in a South African Social Media\n  Environment",
            "updated": "2023-12-07T05:20:15Z",
            "published": "2023-12-07T05:20:15Z",
            "summary": "With the constant spread of misinformation on social media networks, a need\nhas arisen to continuously assess the veracity of digital content. This need\nhas inspired numerous research efforts on the development of misinformation\ndetection (MD) models. However, many models do not use all information\navailable to them and existing research contains a lack of relevant datasets to\ntrain the models, specifically within the South African social media\nenvironment. The aim of this paper is to investigate the transferability of\nknowledge of a MD model between different contextual environments. This\nresearch contributes a multimodal MD model capable of functioning in the South\nAfrican social media environment, as well as introduces a South African\nmisinformation dataset. The model makes use of multiple sources of information\nfor misinformation detection, namely: textual and visual elements. It uses\nbidirectional encoder representations from transformers (BERT) as the textual\nencoder and a residual network (ResNet) as the visual encoder. The model is\ntrained and evaluated on the Fakeddit dataset and a South African\nmisinformation dataset. Results show that using South African samples in the\ntraining of the model increases model performance, in a South African\ncontextual environment, and that a multimodal model retains significantly more\nknowledge than both the textual and visual unimodal models. Our study suggests\nthat the performance of a misinformation detection model is influenced by the\ncultural nuances of its operating environment and multimodal models assist in\nthe transferability of knowledge between different contextual environments.\nTherefore, local data should be incorporated into the training process of a\nmisinformation detection model in order to optimize model performance.",
            "author": [
                "Amica De Jager",
                "Vukosi Marivate",
                "Abioudun Modupe"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-49002-6_19",
                "http://arxiv.org/abs/2312.04052v1",
                "http://arxiv.org/pdf/2312.04052v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04051v1",
            "title": "PLS is contained in PLC",
            "updated": "2023-12-07T05:18:51Z",
            "published": "2023-12-07T05:18:51Z",
            "summary": "Recently, Pasarkar, Papadimitriou, and Yannakakis (ITCS 2023) have introduced\nthe new TFNP subclass called PLC that contains the class PPP; they also have\nproven that several search problems related to extremal combinatorial\nprinciples (e.g., Ramsey's theorem and the Sunflower lemma) belong to PLC. This\nshort paper shows that the class PLC also contains PLS, a complexity class for\nTFNP problems that can be solved by a local search method. However, it is still\nopen whether PLC contains the class PPA.",
            "author": [
                "Takashi Ishizuka"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04051v1",
                "http://arxiv.org/pdf/2312.04051v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04044v1",
            "title": "Residual Graph Convolutional Network for Bird's-Eye-View Semantic\n  Segmentation",
            "updated": "2023-12-07T05:04:41Z",
            "published": "2023-12-07T05:04:41Z",
            "summary": "Retrieving spatial information and understanding the semantic information of\nthe surroundings are important for Bird's-Eye-View (BEV) semantic segmentation.\nIn the application of autonomous driving, autonomous vehicles need to be aware\nof their surroundings to drive safely. However, current BEV semantic\nsegmentation techniques, deep Convolutional Neural Networks (CNNs) and\ntransformers, have difficulties in obtaining the global semantic relationships\nof the surroundings at the early layers of the network. In this paper, we\npropose to incorporate a novel Residual Graph Convolutional (RGC) module in\ndeep CNNs to acquire both the global information and the region-level semantic\nrelationship in the multi-view image domain. Specifically, the RGC module\nemploys a non-overlapping graph space projection to efficiently project the\ncomplete BEV information into graph space. It then builds interconnected\nspatial and channel graphs to extract spatial information between each node and\nchannel information within each node (i.e., extract contextual relationships of\nthe global features). Furthermore, it uses a downsample residual process to\nenhance the coordinate feature reuse to maintain the global information. The\nsegmentation data augmentation and alignment module helps to simultaneously\naugment and align BEV features and ground truth to geometrically preserve their\nalignment to achieve better segmentation results. Our experimental results on\nthe nuScenes benchmark dataset demonstrate that the RGC network outperforms\nfour state-of-the-art networks and its four variants in terms of IoU and mIoU.\nThe proposed RGC network achieves a higher mIoU of 3.1% than the best\nstate-of-the-art network, BEVFusion. Code and models will be released.",
            "author": [
                "Qiuxiao Chen",
                "Xiaojun Qi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04044v1",
                "http://arxiv.org/pdf/2312.04044v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04043v1",
            "title": "Doodle Your 3D: From Abstract Freehand Sketches to Precise 3D Shapes",
            "updated": "2023-12-07T05:04:33Z",
            "published": "2023-12-07T05:04:33Z",
            "summary": "In this paper, we democratise 3D content creation, enabling precise\ngeneration of 3D shapes from abstract sketches while overcoming limitations\ntied to drawing skills. We introduce a novel part-level modelling and alignment\nframework that facilitates abstraction modelling and cross-modal\ncorrespondence. Leveraging the same part-level decoder, our approach seamlessly\nextends to sketch modelling by establishing correspondence between CLIPasso\nedgemaps and projected 3D part regions, eliminating the need for a dataset\npairing human sketches and 3D shapes. Additionally, our method introduces a\nseamless in-position editing process as a byproduct of cross-modal part-aligned\nmodelling. Operating in a low-dimensional implicit space, our approach\nsignificantly reduces computational demands and processing time.",
            "author": [
                "Hmrishav Bandyopadhyay",
                "Subhadeep Koley",
                "Ayan Das",
                "Aneeshan Sain",
                "Pinaki Nath Chowdhury",
                "Tao Xiang",
                "Ayan Kumar Bhunia",
                "Yi-Zhe Song"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04043v1",
                "http://arxiv.org/pdf/2312.04043v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04036v1",
            "title": "DiffusionPhase: Motion Diffusion in Frequency Domain",
            "updated": "2023-12-07T04:39:22Z",
            "published": "2023-12-07T04:39:22Z",
            "summary": "In this study, we introduce a learning-based method for generating\nhigh-quality human motion sequences from text descriptions (e.g., ``A person\nwalks forward\"). Existing techniques struggle with motion diversity and smooth\ntransitions in generating arbitrary-length motion sequences, due to limited\ntext-to-motion datasets and the pose representations used that often lack\nexpressiveness or compactness. To address these issues, we propose the first\nmethod for text-conditioned human motion generation in the frequency domain of\nmotions. We develop a network encoder that converts the motion space into a\ncompact yet expressive parameterized phase space with high-frequency details\nencoded, capturing the local periodicity of motions in time and space with high\naccuracy. We also introduce a conditional diffusion model for predicting\nperiodic motion parameters based on text descriptions and a start pose,\nefficiently achieving smooth transitions between motion sequences associated\nwith different text descriptions. Experiments demonstrate that our approach\noutperforms current methods in generating a broader variety of high-quality\nmotions, and synthesizing long sequences with natural transitions.",
            "author": [
                "Weilin Wan",
                "Yiming Huang",
                "Shutong Wu",
                "Taku Komura",
                "Wenping Wang",
                "Dinesh Jayaraman",
                "Lingjie Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04036v1",
                "http://arxiv.org/pdf/2312.04036v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04032v1",
            "title": "RoAST: Robustifying Language Models via Adversarial Perturbation with\n  Selective Training",
            "updated": "2023-12-07T04:23:36Z",
            "published": "2023-12-07T04:23:36Z",
            "summary": "Fine-tuning pre-trained language models (LMs) has become the de facto\nstandard in many NLP tasks. Nevertheless, fine-tuned LMs are still prone to\nrobustness issues, such as adversarial robustness and model calibration.\nSeveral perspectives of robustness for LMs have been studied independently, but\nlacking a unified consideration in multiple perspectives. In this paper, we\npropose Robustifying LMs via Adversarial perturbation with Selective Training\n(RoAST), a simple yet effective fine-tuning technique to enhance the\nmulti-perspective robustness of LMs in a unified way. RoAST effectively\nincorporates two important sources for the model robustness, robustness on the\nperturbed inputs and generalizable knowledge in pre-trained LMs. To be\nspecific, RoAST introduces adversarial perturbation during fine-tuning while\nthe model parameters are selectively updated upon their relative importance to\nminimize unnecessary deviation. Under a unified evaluation of fine-tuned LMs by\nincorporating four representative perspectives of model robustness, we\ndemonstrate the effectiveness of RoAST compared to state-of-the-art fine-tuning\nmethods on six different types of LMs, which indicates its usefulness in\npractice.",
            "author": [
                "Jaehyung Kim",
                "Yuning Mao",
                "Rui Hou",
                "Hanchao Yu",
                "Davis Liang",
                "Pascale Fung",
                "Qifan Wang",
                "Fuli Feng",
                "Lifu Huang",
                "Madian Khabsa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04032v1",
                "http://arxiv.org/pdf/2312.04032v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04030v1",
            "title": "Modeling Boundedly Rational Agents with Latent Inference Budgets",
            "updated": "2023-12-07T03:55:51Z",
            "published": "2023-12-07T03:55:51Z",
            "summary": "We study the problem of modeling a population of agents pursuing unknown\ngoals subject to unknown computational constraints. In standard models of\nbounded rationality, sub-optimal decision-making is simulated by adding\nhomoscedastic noise to optimal decisions rather than explicitly simulating\nconstrained inference. In this work, we introduce a latent inference budget\nmodel (L-IBM) that models agents' computational constraints explicitly, via a\nlatent variable (inferred jointly with a model of agents' goals) that controls\nthe runtime of an iterative inference algorithm. L-IBMs make it possible to\nlearn agent models using data from diverse populations of suboptimal actors. In\nthree modeling tasks -- inferring navigation goals from routes, inferring\ncommunicative intents from human utterances, and predicting next moves in human\nchess games -- we show that L-IBMs match or outperform Boltzmann models of\ndecision-making under uncertainty. Inferred inference budgets are themselves\nmeaningful, efficient to compute, and correlated with measures of player skill,\npartner skill and task difficulty.",
            "author": [
                "Athul Paul Jacob",
                "Abhishek Gupta",
                "Jacob Andreas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04030v1",
                "http://arxiv.org/pdf/2312.04030v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04029v1",
            "title": "Improved Face Representation via Joint Label Classification and\n  Supervised Contrastive Clustering",
            "updated": "2023-12-07T03:55:20Z",
            "published": "2023-12-07T03:55:20Z",
            "summary": "Face clustering tasks can learn hierarchical semantic information from\nlarge-scale data, which has the potential to help facilitate face recognition.\nHowever, there are few works on this problem. This paper explores it by\nproposing a joint optimization task of label classification and supervised\ncontrastive clustering to introduce the cluster knowledge to the traditional\nface recognition task in two ways. We first extend ArcFace with a\ncluster-guided angular margin to adjust the within-class feature distribution\naccording to the hard level of face clustering. Secondly, we propose a\nsupervised contrastive clustering approach to pull the features to the cluster\ncenter and propose the cluster-aligning procedure to align the cluster center\nand the learnable class center in the classifier for joint training. Finally,\nextensive qualitative and quantitative experiments on popular facial benchmarks\ndemonstrate the effectiveness of our paradigm and its superiority over the\nexisting approaches to face recognition.",
            "author": [
                "Zhenduo Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04029v1",
                "http://arxiv.org/pdf/2312.04029v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04028v1",
            "title": "ImFace++: A Sophisticated Nonlinear 3D Morphable Face Model with\n  Implicit Neural Representations",
            "updated": "2023-12-07T03:53:53Z",
            "published": "2023-12-07T03:53:53Z",
            "summary": "Accurate representations of 3D faces are of paramount importance in various\ncomputer vision and graphics applications. However, the challenges persist due\nto the limitations imposed by data discretization and model linearity, which\nhinder the precise capture of identity and expression clues in current studies.\nThis paper presents a novel 3D morphable face model, named ImFace++, to learn a\nsophisticated and continuous space with implicit neural representations.\nImFace++ first constructs two explicitly disentangled deformation fields to\nmodel complex shapes associated with identities and expressions, respectively,\nwhich simultaneously facilitate the automatic learning of correspondences\nacross diverse facial shapes. To capture more sophisticated facial details, a\nrefinement displacement field within the template space is further\nincorporated, enabling a fine-grained learning of individual-specific facial\ndetails. Furthermore, a Neural Blend-Field is designed to reinforce the\nrepresentation capabilities through adaptive blending of an array of local\nfields. In addition to ImFace++, we have devised an improved learning strategy\nto extend expression embeddings, allowing for a broader range of expression\nvariations. Comprehensive qualitative and quantitative evaluations demonstrate\nthat ImFace++ significantly advances the state-of-the-art in terms of both face\nreconstruction fidelity and correspondence accuracy.",
            "author": [
                "Mingwu Zheng",
                "Haiyu Zhang",
                "Hongyu Yang",
                "Liming Chen",
                "Di Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04028v1",
                "http://arxiv.org/pdf/2312.04028v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04025v1",
            "title": "Moirai: Towards Optimal Placement for Distributed Inference on\n  Heterogeneous Devices",
            "updated": "2023-12-07T03:46:14Z",
            "published": "2023-12-07T03:46:14Z",
            "summary": "The escalating size of Deep Neural Networks (DNNs) has spurred a growing\nresearch interest in hosting and serving DNN models across multiple devices. A\nnumber of studies have been reported to partition a DNN model across devices,\nproviding device placement solutions. The methods appeared in the literature,\nhowever, either suffer from poor placement performance due to the exponential\nsearch space or miss an optimal placement as a consequence of the reduced\nsearch space with limited heuristics. Moreover, these methods have ignored the\nruntime inter-operator optimization of a computation graph when coarsening the\ngraph, which degrades the end-to-end inference performance. This paper presents\nMoirai that better exploits runtime inter-operator fusion in a model to render\na coarsened computation graph, reducing the search space while maintaining the\ninter-operator optimization provided by inference backends. Moirai also\ngeneralizes the device placement algorithm from multiple perspectives by\nconsidering inference constraints and device heterogeneity.Extensive\nexperimental evaluation with 11 large DNNs demonstrates that Moirai outperforms\nthe state-of-the-art counterparts, i.e., Placeto, m-SCT, and GETF, up to\n4.28$\\times$ in reduction of the end-to-end inference latency. Moirai code is\nanonymously released at \\url{https://github.com/moirai-placement/moirai}.",
            "author": [
                "Beibei Zhang",
                "Hongwei Zhu",
                "Feng Gao",
                "Zhihui Yang",
                "Sean Xiaoyang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04025v1",
                "http://arxiv.org/pdf/2312.04025v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04024v1",
            "title": "k* Distribution: Evaluating the Latent Space of Deep Neural Networks\n  using Local Neighborhood Analysis",
            "updated": "2023-12-07T03:42:48Z",
            "published": "2023-12-07T03:42:48Z",
            "summary": "Most examinations of neural networks' learned latent spaces typically employ\ndimensionality reduction techniques such as t-SNE or UMAP. While these methods\neffectively capture the overall sample distribution in the entire learned\nlatent space, they tend to distort the structure of sample distributions within\nspecific classes in the subset of the latent space. This distortion complicates\nthe task of easily distinguishing classes identifiable by neural networks. In\nresponse to this challenge, we introduce the k* Distribution methodology. This\napproach focuses on capturing the characteristics and structure of sample\ndistributions for individual classes within the subset of the learned latent\nspace using local neighborhood analysis. The key concept is to facilitate easy\ncomparison of different k* distributions, enabling analysis of how various\nclasses are processed by the same neural network. This provides a more profound\nunderstanding of existing contemporary visualizations. Our study reveals three\ndistinct distributions of samples within the learned latent space subset: a)\nFractured, b) Overlapped, and c) Clustered. We note and demonstrate that the\ndistribution of samples within the network's learned latent space significantly\nvaries depending on the class. Furthermore, we illustrate that our analysis can\nbe applied to explore the latent space of diverse neural network architectures,\nvarious layers within neural networks, transformations applied to input\nsamples, and the distribution of training and testing data for neural networks.\nWe anticipate that our approach will facilitate more targeted investigations\ninto neural networks by collectively examining the distribution of different\nsamples within the learned latent space.",
            "author": [
                "Shashank Kotyan",
                "Ueda Tatsuya",
                "Danilo Vasconcellos Vargas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04024v1",
                "http://arxiv.org/pdf/2312.04024v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04023v1",
            "title": "A generalized framework for quantum state discrimination, hybrid\n  algorithms, and the quantum change point problem",
            "updated": "2023-12-07T03:42:40Z",
            "published": "2023-12-07T03:42:40Z",
            "summary": "Quantum state discrimination is a central task in many quantum computing\nsettings where one wishes to identify what quantum state they are holding. We\nintroduce a framework that generalizes many of its variants and present a\nhybrid quantum-classical algorithm based on semidefinite programming to\ncalculate the maximum reward when the states are pure and have efficient\ncircuits. To this end, we study the (not necessarily linearly independent) pure\nstate case and reduce the standard SDP problem size from $2^n L$ to $N L$ where\n$n$ is the number of qubits, $N$ is the number of states, and $L$ is the number\nof possible guesses (typically $L = N$). As an application, we give\nnow-possible algorithms for the quantum change point identification problem\nwhich asks, given a sequence of quantum states, determine the time steps when\nthe quantum states changed. With our reductions, we are able to solve SDPs for\nproblem sizes of up to $220$ qubits in about $8$ hours and we also give\nheuristics which speed up the computations.",
            "author": [
                "Ankith Mohan",
                "Jamie Sikora",
                "Sarvagya Upadhyay"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04023v1",
                "http://arxiv.org/pdf/2312.04023v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04021v1",
            "title": "A Study on the Calibration of In-context Learning",
            "updated": "2023-12-07T03:37:39Z",
            "published": "2023-12-07T03:37:39Z",
            "summary": "Modern auto-regressive language models are trained to minimize log loss on\nbroad data by predicting the next token so they are expected to get calibrated\nanswers when framing a problem as a next-token prediction task. We study this\nfor in-context learning (ICL), a widely used way to adapt frozen large language\nmodels (LLMs) via crafting prompts, and investigate the trade-offs between\nperformance and calibration on a wide range of natural language understanding\nand reasoning tasks. We conduct extensive experiments to show that such\ntrade-offs may get worse as we increase model size, incorporate more ICL\nexamples, and fine-tune models using instruction, dialog, or reinforcement\nlearning from human feedback (RLHF) on carefully curated datasets. Furthermore,\nwe find that common recalibration techniques that are widely effective such as\ntemperature scaling provide limited gains in calibration errors, suggesting\nthat new methods may be required for settings where models are expected to be\nreliable.",
            "author": [
                "Hanlin Zhang",
                "Yi-Fan Zhang",
                "Yaodong Yu",
                "Dhruv Madeka",
                "Dean Foster",
                "Eric Xing",
                "Hima Lakkaraju",
                "Sham Kakade"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04021v1",
                "http://arxiv.org/pdf/2312.04021v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04019v1",
            "title": "Efficiently Predicting Protein Stability Changes Upon Single-point\n  Mutation with Large Language Models",
            "updated": "2023-12-07T03:25:49Z",
            "published": "2023-12-07T03:25:49Z",
            "summary": "Predicting protein stability changes induced by single-point mutations has\nbeen a persistent challenge over the years, attracting immense interest from\nnumerous researchers. The ability to precisely predict protein thermostability\nis pivotal for various subfields and applications in biochemistry, including\ndrug development, protein evolution analysis, and enzyme synthesis. Despite the\nproposition of multiple methodologies aimed at addressing this issue, few\napproaches have successfully achieved optimal performance coupled with high\ncomputational efficiency. Two principal hurdles contribute to the existing\nchallenges in this domain. The first is the complexity of extracting and\naggregating sufficiently representative features from proteins. The second\nrefers to the limited availability of experimental data for protein mutation\nanalysis, further complicating the comprehensive evaluation of model\nperformance on unseen data samples. With the advent of Large Language\nModels(LLM), such as the ESM models in protein research, profound\ninterpretation of protein features is now accessibly aided by enormous training\ndata. Therefore, LLMs are indeed to facilitate a wide range of protein\nresearch. In our study, we introduce an ESM-assisted efficient approach that\nintegrates protein sequence and structural features to predict the\nthermostability changes in protein upon single-point mutations. Furthermore, we\nhave curated a dataset meticulously designed to preclude data leakage,\ncorresponding to two extensively employed test datasets, to facilitate a more\nequitable model comparison.",
            "author": [
                "Yijie Zhang",
                "Zhangyang Gao",
                "Cheng Tan",
                "Stan Z. Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04019v1",
                "http://arxiv.org/pdf/2312.04019v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04018v1",
            "title": "Ricci-Notation Tensor Framework for Model-Based Approaches to Imaging",
            "updated": "2023-12-07T03:25:40Z",
            "published": "2023-12-07T03:25:40Z",
            "summary": "Model-based approaches to imaging, like specialized image enhancements in\nastronomy, favour physics-based models which facilitate explanations of\nrelationships between observed inputs and computed outputs. While this paper\nfeatures a tutorial example, inspired by exoplanet imaging, that reveals\nembedded 2D fast Fourier transforms in an image enhancement model, the work is\nactually about the tensor algebra and software, or tensor frameworks, available\nfor model-based imaging. The paper proposes a Ricci-notation tensor (RT)\nframework, comprising an extended Ricci notation, which aligns well with the\nsymbolic dual-index algebra of non-Euclidean geometry, and codesigned\nobject-oriented software, called the RTToolbox for MATLAB. Extensions offer\nnovel representations for entrywise, pagewise, and broadcasting operations\npopular in extended matrix-vector (EMV) frameworks for imaging. Complementing\nthe EMV algebra computable with MATLAB, the RTToolbox demonstrates programmatic\nand computational efficiency thanks to careful design of tensor and dual-index\nclasses. Compared to a numeric tensor predecessor, the RT framework enables\nsuperior ways to model imaging problems and, thereby, to develop solutions.",
            "author": [
                "Dileepan Joseph"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04018v1",
                "http://arxiv.org/pdf/2312.04018v1"
            ],
            "primary_category": "cs.MS",
            "category": [
                "cs.MS",
                "astro-ph.IM",
                "eess.IV",
                "G.4; I.4.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04016v1",
            "title": "PartDistill: 3D Shape Part Segmentation by Vision-Language Model\n  Distillation",
            "updated": "2023-12-07T03:10:03Z",
            "published": "2023-12-07T03:10:03Z",
            "summary": "This paper proposes a cross-modal distillation framework, PartDistill, which\ntransfers 2D knowledge from vision-language models (VLMs) to facilitate 3D\nshape part segmentation. PartDistill addresses three major challenges in this\ntask: the lack of 3D segmentation in invisible or undetected regions in the 2D\nprojections, inaccurate and inconsistent 2D predictions by VLMs, and the lack\nof knowledge accumulation across different 3D shapes. PartDistill consists of a\nteacher network that uses a VLM to make 2D predictions and a student network\nthat learns from the 2D predictions while extracting geometrical features from\nmultiple 3D shapes to carry out 3D part segmentation. A bi-directional\ndistillation, including forward and backward distillations, is carried out\nwithin the framework, where the former forward distills the 2D predictions to\nthe student network, and the latter improves the quality of the 2D predictions,\nwhich subsequently enhances the final 3D part segmentation. Moreover,\nPartDistill can exploit generative models that facilitate effortless 3D shape\ncreation for generating knowledge sources to be distilled. Through extensive\nexperiments, PartDistill boosts the existing methods with substantial margins\non widely used ShapeNetPart and PartE datasets, by more than 15% and 12% higher\nmIoU scores, respectively.",
            "author": [
                "Ardian Umam",
                "Cheng-Kun Yang",
                "Min-Hung Chen",
                "Jen-Hui Chuang",
                "Yen-Yu Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04016v1",
                "http://arxiv.org/pdf/2312.04016v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04013v1",
            "title": "A self-improvable Polymer Discovery Framework Based on Conditional\n  Generative Model",
            "updated": "2023-12-07T03:00:38Z",
            "published": "2023-12-07T03:00:38Z",
            "summary": "In this work, we introduce a polymer discovery platform designed to identify\npolymers with tailored properties efficiently, exemplified through the\ndiscovery of high-performance polymer electrolytes. The platform integrates\nthree core components: a conditioned generative model, validation modules, and\na feedback mechanism, creating a self-improving system for material innovation.\nTo demonstrate the efficacy of this platform, it is used to identify polymer\nelectrolyte materials with high ionic conductivity. A simple conditional\ngenerative model, based on the minGPT architecture, can effectively generate\ncandidate polymers that exhibit a mean ionic conductivity that is significantly\ngreater than those in the original training set. This approach, coupled with\nmolecular dynamics simulations for validation and a specifically designed\nacquisition mechanism, allows the platform to refine its output iteratively.\nNotably, after the first iteration, we observed an increase in both the mean\nand the lower bound of the ionic conductivity of the new polymer candidates.\nThe platform's effectiveness is underscored by the identification of 19 polymer\nrepeating units, each displaying a computed ionic conductivity surpassing that\nof Polyethylene Oxide (PEO). The discovery of these polymers validates the\nplatform's efficacy in identifying potential polymer materials. Acknowledging\ncurrent limitations, future work will focus on enhancing modeling techniques,\nvalidation processes, and acquisition strategies, aiming for broader\napplicability in polymer science and machine learning.",
            "author": [
                "Xiangyun Lei",
                "Weike Ye",
                "Zhenze Yang",
                "Daniel Schweigert",
                "Ha-Kyung Kwon",
                "Arash Khajeh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04013v1",
                "http://arxiv.org/pdf/2312.04013v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04008v1",
            "title": "Natural-language-driven Simulation Benchmark and Copilot for Efficient\n  Production of Object Interactions in Virtual Road Scenes",
            "updated": "2023-12-07T02:55:46Z",
            "published": "2023-12-07T02:55:46Z",
            "summary": "We advocate the idea of the natural-language-driven(NLD) simulation to\nefficiently produce the object interactions between multiple objects in the\nvirtual road scenes, for teaching and testing the autonomous driving systems\nthat should take quick action to avoid collision with obstacles with\nunpredictable motions. The NLD simulation allows the brief natural-language\ndescription to control the object interactions, significantly reducing the\nhuman efforts for creating a large amount of interaction data. To facilitate\nthe research of NLD simulation, we collect the Language-to-Interaction(L2I)\nbenchmark dataset with 120,000 natural-language descriptions of object\ninteractions in 6 common types of road topologies. Each description is\nassociated with the programming code, which the graphic render can use to\nvisually reconstruct the object interactions in the virtual scenes. As a\nmethodology contribution, we design SimCopilot to translate the interaction\ndescriptions to the renderable code. We use the L2I dataset to evaluate\nSimCopilot's abilities to control the object motions, generate complex\ninteractions, and generalize interactions across road topologies. The L2I\ndataset and the evaluation results motivate the relevant research of the NLD\nsimulation.",
            "author": [
                "Kairui Yang",
                "Zihao Guo",
                "Gengjie Lin",
                "Haotian Dong",
                "Die Zuo",
                "Jibin Peng",
                "Zhao Huang",
                "Zhecheng Xu",
                "Fupeng Li",
                "Ziyun Bai",
                "Di Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04008v1",
                "http://arxiv.org/pdf/2312.04008v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04005v1",
            "title": "KOALA: Self-Attention Matters in Knowledge Distillation of Latent\n  Diffusion Models for Memory-Efficient and Fast Image Synthesis",
            "updated": "2023-12-07T02:46:18Z",
            "published": "2023-12-07T02:46:18Z",
            "summary": "Stable diffusion is the mainstay of the text-to-image (T2I) synthesis in the\ncommunity due to its generation performance and open-source nature. Recently,\nStable Diffusion XL (SDXL), the successor of stable diffusion, has received a\nlot of attention due to its significant performance improvements with a higher\nresolution of 1024x1024 and a larger model. However, its increased computation\ncost and model size require higher-end hardware(e.g., bigger VRAM GPU) for\nend-users, incurring higher costs of operation. To address this problem, in\nthis work, we propose an efficient latent diffusion model for text-to-image\nsynthesis obtained by distilling the knowledge of SDXL. To this end, we first\nperform an in-depth analysis of the denoising U-Net in SDXL, which is the main\nbottleneck of the model, and then design a more efficient U-Net based on the\nanalysis. Secondly, we explore how to effectively distill the generation\ncapability of SDXL into an efficient U-Net and eventually identify four\nessential factors, the core of which is that self-attention is the most\nimportant part. With our efficient U-Net and self-attention-based knowledge\ndistillation strategy, we build our efficient T2I models, called KOALA-1B &\n-700M, while reducing the model size up to 54% and 69% of the original SDXL\nmodel. In particular, the KOALA-700M is more than twice as fast as SDXL while\nstill retaining a decent generation quality. We hope that due to its balanced\nspeed-performance tradeoff, our KOALA models can serve as a cost-effective\nalternative to SDXL in resource-constrained environments.",
            "author": [
                "Youngwan Lee",
                "Kwanyong Park",
                "Yoorhim Cho",
                "Yong-Ju Lee",
                "Sung Ju Hwang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04005v1",
                "http://arxiv.org/pdf/2312.04005v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04003v1",
            "title": "Tracker behaviour of quintom dark energy and the Hubble tension",
            "updated": "2023-12-07T02:44:32Z",
            "published": "2023-12-07T02:44:32Z",
            "summary": "We study the dynamics of the quintom dark energy model using state-of-the-art\ncosmological observations. The set of equations has been converted into an\nautonomous system using suitable transformations of the variables. We have\ndiscussed the fixed points of the model and the general phase-space behavior,\nin particular, in finding the existence of the tracker solutions for this\nmodel. The observations suggest that at late times the phantom field should\ndominate the dark energy sector with an approximately 15% share to the\nquintessence counterpart, and with both fields tracking the background at early\ntimes. A Bayesian model comparison with LambdaCDM has also been done by\ncomputing the Bayes factor and a positive preference has been obtained for the\nquintom model. Although not fully resolved, the Hubble tension can be reduced\nto 2.6{\\sigma} when compared with the value of H0 reported in [1] and to\n1.6{\\sigma} when compared with that of [2].",
            "author": [
                "Nandan Roy",
                "L Arturo Ure\u00f1a-L\u00f3pez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04003v1",
                "http://arxiv.org/pdf/2312.04003v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04000v1",
            "title": "LiDAR: Sensing Linear Probing Performance in Joint Embedding SSL\n  Architectures",
            "updated": "2023-12-07T02:31:28Z",
            "published": "2023-12-07T02:31:28Z",
            "summary": "Joint embedding (JE) architectures have emerged as a promising avenue for\nacquiring transferable data representations. A key obstacle to using JE\nmethods, however, is the inherent challenge of evaluating learned\nrepresentations without access to a downstream task, and an annotated dataset.\nWithout efficient and reliable evaluation, it is difficult to iterate on\narchitectural and training choices for JE methods. In this paper, we introduce\nLiDAR (Linear Discriminant Analysis Rank), a metric designed to measure the\nquality of representations within JE architectures. Our metric addresses\nseveral shortcomings of recent approaches based on feature covariance rank by\ndiscriminating between informative and uninformative features. In essence,\nLiDAR quantifies the rank of the Linear Discriminant Analysis (LDA) matrix\nassociated with the surrogate SSL task -- a measure that intuitively captures\nthe information content as it pertains to solving the SSL task. We empirically\ndemonstrate that LiDAR significantly surpasses naive rank based approaches in\nits predictive power of optimal hyperparameters. Our proposed criterion\npresents a more robust and intuitive means of assessing the quality of\nrepresentations within JE architectures, which we hope facilitates broader\nadoption of these powerful techniques in various domains.",
            "author": [
                "Vimal Thilak",
                "Chen Huang",
                "Omid Saremi",
                "Laurent Dinh",
                "Hanlin Goh",
                "Preetum Nakkiran",
                "Joshua M. Susskind",
                "Etai Littwin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04000v1",
                "http://arxiv.org/pdf/2312.04000v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03998v1",
            "title": "Series2Vec: Similarity-based Self-supervised Representation Learning for\n  Time Series Classification",
            "updated": "2023-12-07T02:30:40Z",
            "published": "2023-12-07T02:30:40Z",
            "summary": "We argue that time series analysis is fundamentally different in nature to\neither vision or natural language processing with respect to the forms of\nmeaningful self-supervised learning tasks that can be defined. Motivated by\nthis insight, we introduce a novel approach called \\textit{Series2Vec} for\nself-supervised representation learning. Unlike other self-supervised methods\nin time series, which carry the risk of positive sample variants being less\nsimilar to the anchor sample than series in the negative set, Series2Vec is\ntrained to predict the similarity between two series in both temporal and\nspectral domains through a self-supervised task. Series2Vec relies primarily on\nthe consistency of the unsupervised similarity step, rather than the intrinsic\nquality of the similarity measurement, without the need for hand-crafted data\naugmentation. To further enforce the network to learn similar representations\nfor similar time series, we propose a novel approach that applies\norder-invariant attention to each representation within the batch during\ntraining. Our evaluation of Series2Vec on nine large real-world datasets, along\nwith the UCR/UEA archive, shows enhanced performance compared to current\nstate-of-the-art self-supervised techniques for time series. Additionally, our\nextensive experiments show that Series2Vec performs comparably with fully\nsupervised training and offers high efficiency in datasets with limited-labeled\ndata. Finally, we show that the fusion of Series2Vec with other representation\nlearning models leads to enhanced performance for time series classification.\nCode and models are open-source at\n\\url{https://github.com/Navidfoumani/Series2Vec.}",
            "author": [
                "Navid Mohammadi Foumani",
                "Chang Wei Tan",
                "Geoffrey I. Webb",
                "Mahsa Salehi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03998v1",
                "http://arxiv.org/pdf/2312.03998v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03997v1",
            "title": "Asymmetrical post quench transport in an embedded parity time symmetric\n  Su-Schrieffer-Heeger system",
            "updated": "2023-12-07T02:28:23Z",
            "published": "2023-12-07T02:28:23Z",
            "summary": "We study the effect of PT-symmetric non-hermiticity on the transport of edge\nstate probability density arising as a result of a quench. A hybrid system\ninvolving a PT-symmetric SSH region sandwiched between two plain SSH systems is\ndesigned to study the dynamics. Geometrical arguments and numerical\ncalculations were made to ascertain the nature of edge states. We then compute\nthe quench dynamics numerically and demonstrate that the post-quench\nprobability density light cones exhibit contrasting shapes as a result of\nasymmetrical reflections from the non-Hermitian part of the system depending on\nthe direction of propagation of the transporting wave and, hence, on the\ninitial localization of the edge state.",
            "author": [
                "Anirban Ghosh",
                "Andy Martin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03997v1",
                "http://arxiv.org/pdf/2312.03997v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03996v1",
            "title": "Stable diffusion for Data Augmentation in COCO and Weed Datasets",
            "updated": "2023-12-07T02:23:32Z",
            "published": "2023-12-07T02:23:32Z",
            "summary": "Generative models have increasingly impacted relative tasks ranging from\nimage revision and object detection in computer vision to interior design and\nidea illustration in more general fields. Stable diffusion is an outstanding\nmodel series that paves the way for producing high-resolution images with\nthorough details from text prompts or reference images. It will be an\ninteresting topic about how to leverage the capability of stable diffusion to\nelevate the image variations of certain categories (e.g., vehicles, humans, and\ndaily objects); particularly, it has the potential to gain improvements for\nsmall datasets with image-sparse categories. This study utilized seven\ncategories in the popular COCO dataset and three widespread weed species in\nMichigan to evaluate the efficiency of a recent version of stable diffusion. In\ndetail, Stable diffusion was used to generate synthetic images belonging to\nthese classes; then, YOLOv8 models were trained based on these synthetic\nimages, whose performance was compared to the models trained on original\nimages. In addition, several techniques (e.g., Image-to-image translation,\nDreambooth, ControlNet) of Stable diffusion were leveraged for image generation\nwith different focuses. In spite of the overall results being disappointing,\npromising results have been achieved in some classes, illustrating the\npotential of stable diffusion models to improve the performance of detection\nmodels, which represent more helpful information being conveyed into the models\nby the generated images. This seminal study may expedite the adaption of stable\ndiffusion models to classification and detection tasks in different fields.",
            "author": [
                "Boyang Deng",
                "Yuzhen Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03996v1",
                "http://arxiv.org/pdf/2312.03996v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03993v1",
            "title": "Style Transfer to Calvin and Hobbes comics using Stable Diffusion",
            "updated": "2023-12-07T02:21:31Z",
            "published": "2023-12-07T02:21:31Z",
            "summary": "This project report summarizes our journey to perform stable diffusion\nfine-tuning on a dataset containing Calvin and Hobbes comics. The purpose is to\nconvert any given input image into the comic style of Calvin and Hobbes,\nessentially performing style transfer. We train stable-diffusion-v1.5 using Low\nRank Adaptation (LoRA) to efficiently speed up the fine-tuning process. The\ndiffusion itself is handled by a Variational Autoencoder (VAE), which is a\nU-net. Our results were visually appealing for the amount of training time and\nthe quality of input data that went into training.",
            "author": [
                "Sloke Shrestha",
                "Sundar Sripada V. S.",
                "Asvin Venkataramanan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03993v1",
                "http://arxiv.org/pdf/2312.03993v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03991v1",
            "title": "MICRO: Model-Based Offline Reinforcement Learning with a Conservative\n  Bellman Operator",
            "updated": "2023-12-07T02:17:45Z",
            "published": "2023-12-07T02:17:45Z",
            "summary": "Offline reinforcement learning (RL) faces a significant challenge of\ndistribution shift. Model-free offline RL penalizes the Q value for\nout-of-distribution (OOD) data or constrains the policy closed to the behavior\npolicy to tackle this problem, but this inhibits the exploration of the OOD\nregion. Model-based offline RL, which uses the trained environment model to\ngenerate more OOD data and performs conservative policy optimization within\nthat model, has become an effective method for this problem. However, the\ncurrent model-based algorithms rarely consider agent robustness when\nincorporating conservatism into policy. Therefore, the new model-based offline\nalgorithm with a conservative Bellman operator (MICRO) is proposed. This method\ntrades off performance and robustness via introducing the robust Bellman\noperator into the algorithm. Compared with previous model-based algorithms with\nrobust adversarial models, MICRO can significantly reduce the computation cost\nby only choosing the minimal Q value in the state uncertainty set. Extensive\nexperiments demonstrate that MICRO outperforms prior RL algorithms in offline\nRL benchmark and is considerably robust to adversarial perturbations.",
            "author": [
                "Xiao-Yin Liu",
                "Xiao-Hu Zhou",
                "Guo-Tao Li",
                "Hao Li",
                "Mei-Jiang Gui",
                "Tian-Yu Xiang",
                "De-Xing Huang",
                "Zeng-Guang Hou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03991v1",
                "http://arxiv.org/pdf/2312.03991v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03987v1",
            "title": "Cost-Effective In-Context Learning for Entity Resolution: A Design Space\n  Exploration",
            "updated": "2023-12-07T02:09:27Z",
            "published": "2023-12-07T02:09:27Z",
            "summary": "Entity resolution (ER) is an important data integration task with a wide\nspectrum of applications. The state-of-the-art solutions on ER rely on\npre-trained language models (PLMs), which require fine-tuning on a lot of\nlabeled matching/non-matching entity pairs. Recently, large languages models\n(LLMs), such as GPT-4, have shown the ability to perform many tasks without\ntuning model parameters, which is known as in-context learning (ICL) that\nfacilitates effective learning from a few labeled input context demonstrations.\nHowever, existing ICL approaches to ER typically necessitate providing a task\ndescription and a set of demonstrations for each entity pair and thus have\nlimitations on the monetary cost of interfacing LLMs. To address the problem,\nin this paper, we provide a comprehensive study to investigate how to develop a\ncost-effective batch prompting approach to ER. We introduce a framework BATCHER\nconsisting of demonstration selection and question batching and explore\ndifferent design choices that support batch prompting for ER. We also devise a\ncovering-based demonstration selection strategy that achieves an effective\nbalance between matching accuracy and monetary cost. We conduct a thorough\nevaluation to explore the design space and evaluate our proposed strategies.\nThrough extensive experiments, we find that batch prompting is very\ncost-effective for ER, compared with not only PLM-based methods fine-tuned with\nextensive labeled data but also LLM-based methods with manually designed\nprompting. We also provide guidance for selecting appropriate design choices\nfor batch prompting.",
            "author": [
                "Meihao Fan",
                "Xiaoyue Han",
                "Ju Fan",
                "Chengliang Chai",
                "Nan Tang",
                "Guoliang Li",
                "Xiaoyong Du"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03987v1",
                "http://arxiv.org/pdf/2312.03987v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03982v1",
            "title": "Logical quantum processor based on reconfigurable atom arrays",
            "updated": "2023-12-07T01:54:45Z",
            "published": "2023-12-07T01:54:45Z",
            "summary": "Suppressing errors is the central challenge for useful quantum computing,\nrequiring quantum error correction for large-scale processing. However, the\noverhead in the realization of error-corrected ``logical'' qubits, where\ninformation is encoded across many physical qubits for redundancy, poses\nsignificant challenges to large-scale logical quantum computing. Here we report\nthe realization of a programmable quantum processor based on encoded logical\nqubits operating with up to 280 physical qubits. Utilizing logical-level\ncontrol and a zoned architecture in reconfigurable neutral atom arrays, our\nsystem combines high two-qubit gate fidelities, arbitrary connectivity, as well\nas fully programmable single-qubit rotations and mid-circuit readout. Operating\nthis logical processor with various types of encodings, we demonstrate\nimprovement of a two-qubit logic gate by scaling surface code distance from d=3\nto d=7, preparation of color code qubits with break-even fidelities,\nfault-tolerant creation of logical GHZ states and feedforward entanglement\nteleportation, as well as operation of 40 color code qubits. Finally, using\nthree-dimensional [[8,3,2]] code blocks, we realize computationally complex\nsampling circuits with up to 48 logical qubits entangled with hypercube\nconnectivity with 228 logical two-qubit gates and 48 logical CCZ gates. We find\nthat this logical encoding substantially improves algorithmic performance with\nerror detection, outperforming physical qubit fidelities at both cross-entropy\nbenchmarking and quantum simulations of fast scrambling. These results herald\nthe advent of early error-corrected quantum computation and chart a path toward\nlarge-scale logical processors.",
            "author": [
                "Dolev Bluvstein",
                "Simon J. Evered",
                "Alexandra A. Geim",
                "Sophie H. Li",
                "Hengyun Zhou",
                "Tom Manovitz",
                "Sepehr Ebadi",
                "Madelyn Cain",
                "Marcin Kalinowski",
                "Dominik Hangleiter",
                "J. Pablo Bonilla Ataides",
                "Nishad Maskara",
                "Iris Cong",
                "Xun Gao",
                "Pedro Sales Rodriguez",
                "Thomas Karolyshyn",
                "Giulia Semeghini",
                "Michael J. Gullans",
                "Markus Greiner",
                "Vladan Vuletic",
                "Mikhail D. Lukin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03982v1",
                "http://arxiv.org/pdf/2312.03982v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.quant-gas",
                "physics.atom-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03970v1",
            "title": "Improving Medical Report Generation with Adapter Tuning and Knowledge\n  Enhancement in Vision-Language Foundation Models",
            "updated": "2023-12-07T01:01:45Z",
            "published": "2023-12-07T01:01:45Z",
            "summary": "Medical report generation demands automatic creation of coherent and precise\ndescriptions for medical images. However, the scarcity of labelled medical\nimage-report pairs poses formidable challenges in developing large-scale neural\nnetworks capable of harnessing the potential of artificial intelligence,\nexemplified by large language models. This study builds upon the\nstate-of-the-art vision-language pre-training and fine-tuning approach, BLIP-2,\nto customize general large-scale foundation models. Integrating adapter tuning\nand a medical knowledge enhancement loss, our model significantly improves\naccuracy and coherence. Validation on the dataset of ImageCLEFmedical 2023\ndemonstrates our model's prowess, achieving the best-averaged results against\nseveral state-of-the-art methods. Significant improvements in ROUGE and CIDEr\nunderscore our method's efficacy, highlighting promising outcomes for the rapid\nmedical-domain adaptation of the vision-language foundation models in\naddressing challenges posed by data scarcity.",
            "author": [
                "Shibin Wu",
                "Bang Yang",
                "Zhiyu Ye",
                "Haoqian Wang",
                "Hairong Zheng",
                "Tong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03970v1",
                "http://arxiv.org/pdf/2312.03970v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03966v1",
            "title": "Impostor Phenomenon in Software Engineers",
            "updated": "2023-12-07T00:51:58Z",
            "published": "2023-12-07T00:51:58Z",
            "summary": "The Impostor Phenomenon (IP) is widely discussed in Science, Technology,\nEngineering, and Mathematics (STEM) and has been evaluated in Computer Science\nstudents. However, formal research on IP in software engineers has yet to be\nconducted, although its impacts may lead to mental disorders such as depression\nand burnout. This study describes a survey that investigates the extent of\nimpostor feelings in software engineers, considering aspects such as gender,\nrace/ethnicity, and roles. Furthermore, we investigate the influence of IP on\ntheir perceived productivity. The survey instrument was designed using a\ntheory-driven approach and included demographic questions, an internationally\nvalidated IP scale, and questions for measuring perceived productivity based on\nthe SPACE framework constructs. The survey was sent to companies operating in\nvarious business sectors. Data analysis used bootstrapping with resampling to\ncalculate confidence intervals and Mann-Whitney statistical significance\ntesting for assessing the hypotheses. We received responses from 624 software\nengineers from 26 countries. The bootstrapping results reveal that a proportion\nof 52.7% of software engineers experience frequent to intense levels of IP and\nthat women suffer at a significantly higher proportion (60.6%) than men\n(48.8%). Regarding race/ethnicity, we observed more frequent impostor feelings\nin Asian (67.9%) and Black (65.1%) than in White (50.0%) software engineers. We\nalso observed that the presence of IP is less common among individuals who are\nmarried and have children. Moreover, the prevalence of IP showed a\nstatistically significant negative effect on the perceived productivity for all\nSPACE framework constructs. The evidence relating IP to software engineers\nprovides a starting point to help organizations find ways to raise awareness of\nthe problem and improve the emotional skills of software professionals.",
            "author": [
                "Paloma Guenes",
                "Rafael Tomaz",
                "Marcos Kalinowski",
                "Maria Teresa Baldassarre",
                "Margaret-Anne Storey"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03966v1",
                "http://arxiv.org/pdf/2312.03966v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03957v1",
            "title": "PerSival: Neural-network-based visualisation for pervasive\n  continuum-mechanical simulations in musculoskeletal biomechanics",
            "updated": "2023-12-07T00:07:35Z",
            "published": "2023-12-07T00:07:35Z",
            "summary": "This paper presents a novel neural network architecture for the purpose of\npervasive visualisation of a 3D human upper limb musculoskeletal system model.\nBringing simulation capabilities to resource-poor systems like mobile devices\nis of growing interest across many research fields, to widen applicability of\nmethods and results. Until recently, this goal was thought to be out of reach\nfor realistic continuum-mechanical simulations of musculoskeletal systems, due\nto prohibitive computational cost. Within this work we use a sparse grid\nsurrogate to capture the surface deformation of the m.~biceps brachii in order\nto train a deep learning model, used for real-time visualisation of the same\nmuscle. Both these surrogate models take 5 muscle activation levels as input\nand output Cartesian coordinate vectors for each mesh node on the muscle's\nsurface. Thus, the neural network architecture features a significantly lower\ninput than output dimension. 5 muscle activation levels were sufficient to\nachieve an average error of 0.97 +/- 0.16 mm, or 0.57 +/- 0.10 % for the 2809\nmesh node positions of the biceps. The model achieved evaluation times of 9.88\nms per predicted deformation state on CPU only and 3.48 ms with GPU-support,\nleading to theoretical frame rates of 101 fps and 287 fps respectively. Deep\nlearning surrogates thus provide a way to make continuum-mechanical simulations\naccessible for visual real-time applications.",
            "author": [
                "David Rosin",
                "Johannes K\u00e4ssinger",
                "Xingyao Yu",
                "Okan Avci",
                "Christian Bleiler",
                "Oliver R\u00f6hrle"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03957v1",
                "http://arxiv.org/pdf/2312.03957v1"
            ],
            "primary_category": "q-bio.TO",
            "category": [
                "q-bio.TO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03952v1",
            "title": "Deterministic Creation of Large Photonic Multipartite Entangled States\n  with Group-IV Color Centers in Diamond",
            "updated": "2023-12-06T23:33:22Z",
            "published": "2023-12-06T23:33:22Z",
            "summary": "Measurement-based quantum computation relies on single qubit measurements of\nlarge multipartite entangled states, so-called lattice-graph or cluster states.\nGraph states are also an important resource for quantum communication, where\ntree cluster states are a key resource for one-way quantum repeaters. A\nphotonic realization of this kind of state would inherit many of the benefits\nof photonic platforms, such as very little dephasing due to weak environmental\ninteractions and the well-developed infrastructure to route and measure\nphotonic qubits. In this work, a linear cluster state and GHZ state generation\nscheme is developed for group-IV color centers. In particular, this article\nfocuses on an in-depth investigation of the required control operations,\nincluding the coherent spin and excitation gates. We choose an off-resonant\nRaman scheme for the spin gates, which can be much faster than microwave\ncontrol. We do not rely on a reduced level scheme and use efficient\napproximations to design high-fidelity Raman gates. We benchmark the\nspin-control and excitation scheme using the tin vacancy color center coupled\nto a cavity, assuming a realistic experimental setting. Additionally, the\narticle investigates the fidelities of the Raman and excitation gates in the\npresence of radiative and non-radiative decay mechanisms. Finally, a quality\nmeasure is devised, which emphasizes the importance of fast and high-fidelity\nspin gates in the creation of large entangled photonic states.",
            "author": [
                "Gregor Pieplow",
                "Yannick Strocka",
                "Mariano Isaza-Monsalve",
                "Joseph H. D. Munns",
                "Tim Schr\u00f6der"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03952v1",
                "http://arxiv.org/pdf/2312.03952v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03950v1",
            "title": "A Scalable and Generalizable Pathloss Map Prediction",
            "updated": "2023-12-06T23:22:49Z",
            "published": "2023-12-06T23:22:49Z",
            "summary": "Large-scale channel prediction, i.e., estimation of the pathloss from\ngeographical/morphological/building maps, is an essential component of wireless\nnetwork planning. Ray tracing (RT)-based methods have been widely used for many\nyears, but they require significant computational effort that may become\nprohibitive with the increased network densification and/or use of higher\nfrequencies in B5G/6G systems. In this paper, we propose a data-driven,\nmodel-free pathloss map prediction (PMP) method, called PMNet. PMNet uses a\nsupervised learning approach: it is trained on a limited amount of RT (or\nchannel measurement) data and map data. Once trained, PMNet can predict\npathloss over location with high accuracy (an RMSE level of $10^{-2}$) in a few\nmilliseconds. We further extend PMNet by employing transfer learning (TL). TL\nallows PMNet to learn a new network scenario quickly (x5.6 faster training) and\nefficiently (using x4.5 less data) by transferring knowledge from a pre-trained\nmodel, while retaining accuracy. Our results demonstrate that PMNet is a\nscalable and generalizable ML-based PMP method, showing its potential to be\nused in several network optimization applications.",
            "author": [
                "Ju-Hyung Lee",
                "Andreas F. Molisch"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03950v1",
                "http://arxiv.org/pdf/2312.03950v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.LG",
                "cs.NI",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03946v1",
            "title": "A Layer-Wise Tokens-to-Token Transformer Network for Improved Historical\n  Document Image Enhancement",
            "updated": "2023-12-06T23:01:11Z",
            "published": "2023-12-06T23:01:11Z",
            "summary": "Document image enhancement is a fundamental and important stage for attaining\nthe best performance in any document analysis assignment because there are many\ndegradation situations that could harm document images, making it more\ndifficult to recognize and analyze them. In this paper, we propose\n\\textbf{T2T-BinFormer} which is a novel document binarization encoder-decoder\narchitecture based on a Tokens-to-token vision transformer. Each image is\ndivided into a set of tokens with a defined length using the ViT model, which\nis then applied several times to model the global relationship between the\ntokens. However, the conventional tokenization of input data does not\nadequately reflect the crucial local structure between adjacent pixels of the\ninput image, which results in low efficiency. Instead of using a simple ViT and\nhard splitting of images for the document image enhancement task, we employed a\nprogressive tokenization technique to capture this local information from an\nimage to achieve more effective results. Experiments on various DIBCO and\nH-DIBCO benchmarks demonstrate that the proposed model outperforms the existing\nCNN and ViT-based state-of-the-art methods. In this research, the primary area\nof examination is the application of the proposed architecture to the task of\ndocument binarization. The source code will be made available at\nhttps://github.com/RisabBiswas/T2T-BinFormer.",
            "author": [
                "Risab Biswas",
                "Swalpa Kumar Roy",
                "Umapada Pal"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03946v1",
                "http://arxiv.org/pdf/2312.03946v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03940v1",
            "title": "PECANN: Parallel Efficient Clustering with Graph-Based Approximate\n  Nearest Neighbor Search",
            "updated": "2023-12-06T22:43:50Z",
            "published": "2023-12-06T22:43:50Z",
            "summary": "This paper studies density-based clustering of point sets. These methods use\ndense regions of points to detect clusters of arbitrary shapes. In particular,\nwe study variants of density peaks clustering, a popular type of algorithm that\nhas been shown to work well in practice. Our goal is to cluster large\nhigh-dimensional datasets, which are prevalent in practice. Prior solutions are\neither sequential, and cannot scale to large data, or are specialized for\nlow-dimensional data.\n  This paper unifies the different variants of density peaks clustering into a\nsingle framework, PECANN, by abstracting out several key steps common to this\nclass of algorithms. One such key step is to find nearest neighbors that\nsatisfy a predicate function, and one of the main contributions of this paper\nis an efficient way to do this predicate search using graph-based approximate\nnearest neighbor search (ANNS). To provide ample parallelism, we propose a\ndoubling search technique that enables points to find an approximate nearest\nneighbor satisfying the predicate in a small number of rounds. Our technique\ncan be applied to many existing graph-based ANNS algorithms, which can all be\nplugged into PECANN.\n  We implement five clustering algorithms with PECANN and evaluate them on\nsynthetic and real-world datasets with up to 1.28 million points and up to 1024\ndimensions on a 30-core machine with two-way hyper-threading. Compared to the\nstate-of-the-art FASTDP algorithm for high-dimensional density peaks\nclustering, which is sequential, our best algorithm is 45x-734x faster while\nachieving competitive ARI scores. Compared to the state-of-the-art parallel\nDPC-based algorithm, which is optimized for low dimensions, we show that PECANN\nis two orders of magnitude faster. As far as we know, our work is the first to\nevaluate DPC variants on large high-dimensional real-world image and text\nembedding datasets.",
            "author": [
                "Shangdi Yu",
                "Joshua Engels",
                "Yihao Huang",
                "Julian Shun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03940v1",
                "http://arxiv.org/pdf/2312.03940v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03939v1",
            "title": "On the scanning map and the space of smooth projective hypersurfaces",
            "updated": "2023-12-06T22:43:31Z",
            "published": "2023-12-06T22:43:31Z",
            "summary": "The space of degree d smooth projective hypersurfaces of CP n admits a\nscanning map to a certain space of sections. We compute a rational homotopy\nmodel of the action by conjugation of the group U (n + 1) on this space of\nsections, from which we deduce that the scanning map induces a monomorphism on\ncohomology when d > 2. Our main technique is the rational homotopy theory of\nSullivan and, more specifically, a Sullivan model for the action by conjugation\nof a connected topological group on a space of sections.",
            "author": [
                "Federico Cantero-Mor\u00e1n",
                "\u00c1ngel Alonso"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03939v1",
                "http://arxiv.org/pdf/2312.03939v1"
            ],
            "primary_category": "math.AT",
            "category": [
                "math.AT",
                "math.AG",
                "55R80, 14J70"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03938v1",
            "title": "Adapting HouseDiffusion for conditional Floor Plan generation on\n  Modified Swiss Dwellings dataset",
            "updated": "2023-12-06T22:35:59Z",
            "published": "2023-12-06T22:35:59Z",
            "summary": "Automated floor plan generation has recently gained momentum with several\nmethods that have been proposed. The CVAAD Floor Plan Auto-Completion workshop\nchallenge introduced MSD, a new dataset that includes existing structural walls\nof the building as an additional input constraint. This technical report\npresents an approach for extending a recent work, HouseDiffusion\n(arXiv:2211.13287 [cs.CV]), to the MSD dataset. The adaption involves modifying\nthe model's transformer layers to condition on a set of wall lines. The report\nintroduces a pre-processing pipeline to extract wall lines from the binary mask\nof the building structure provided as input. Additionally, it was found that a\ndata processing procedure that simplifies all room polygons to rectangles leads\nto better performance. This indicates that future work should explore better\nrepresentations of variable-length polygons in diffusion models. The code will\nbe made available at a later date.",
            "author": [
                "Emanuel Kuhn"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03938v1",
                "http://arxiv.org/pdf/2312.03938v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03936v1",
            "title": "The Potential of Vision-Language Models for Content Moderation of\n  Children's Videos",
            "updated": "2023-12-06T22:29:16Z",
            "published": "2023-12-06T22:29:16Z",
            "summary": "Natural language supervision has been shown to be effective for zero-shot\nlearning in many computer vision tasks, such as object detection and activity\nrecognition. However, generating informative prompts can be challenging for\nmore subtle tasks, such as video content moderation. This can be difficult, as\nthere are many reasons why a video might be inappropriate, beyond violence and\nobscenity. For example, scammers may attempt to create junk content that is\nsimilar to popular educational videos but with no meaningful information. This\npaper evaluates the performance of several CLIP variations for content\nmoderation of children's cartoons in both the supervised and zero-shot setting.\nWe show that our proposed model (Vanilla CLIP with Projection Layer)\noutperforms previous work conducted on the Malicious or Benign (MOB) benchmark\nfor video content moderation. This paper presents an in depth analysis of how\ncontext-specific language prompts affect content moderation performance. Our\nresults indicate that it is important to include more context in content\nmoderation prompts, particularly for cartoon videos as they are not well\nrepresented in the CLIP training data.",
            "author": [
                "Syed Hammad Ahmed",
                "Shengnan Hu",
                "Gita Sukthankar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03936v1",
                "http://arxiv.org/pdf/2312.03936v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CY",
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03929v1",
            "title": "Simulation of a L\u00e9vy process, its extremum, and hitting time of the\n  extremum via characteristic functions",
            "updated": "2023-12-06T22:11:20Z",
            "published": "2023-12-06T22:11:20Z",
            "summary": "We suggest a general framework for simulation of the triplet $(X_T,\\bar X_\nT,\\tau_T)$ (L\\'evy process, its extremum, and hitting time of the extremum),\nand, separately, $X_T,\\bar X_ T$ and pairs $(X_T,\\bar X_ T)$, $(\\bar X_\nT,\\tau_T)$,\n  $(\\bar X_ T-X_T,\\tau_T)$, via characteristic functions and conditional\ncharacteristic functions. The conformal deformations technique allows one to\nevaluate probability distributions, joint probability distributions and\nconditional probability distributions accurately and fast. For simulations in\nthe far tails of the distribution, we precalculate and store the values of the\n(conditional) characteristic functions on multi-grids on appropriate surfaces\nin $C^n$, and use these values to calculate the quantiles in the tails. For\nsimulation in the central part of a distribution, we precalculate the values of\nthe cumulative distribution at points of a non-uniform (multi-)grid, and use\ninterpolation to calculate quantiles.",
            "author": [
                "Svetlana Boyarchenko",
                "Sergei Levendorskii"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03929v1",
                "http://arxiv.org/pdf/2312.03929v1"
            ],
            "primary_category": "q-fin.CP",
            "category": [
                "q-fin.CP",
                "math.PR",
                "60-08, 42A38, 42B10, 44A10, 65R10, 65G51, 91G20, 91G60"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03927v1",
            "title": "A fast numerical algorithm for finding all real solutions to a system of\n  N nonlinear equations in a finite domain",
            "updated": "2023-12-06T22:05:03Z",
            "published": "2023-12-06T22:05:03Z",
            "summary": "A highly recurrent traditional bottleneck in applied mathematics, for which\nthe most popular codes (Mathematica and Matlab) do not offer a solution, is to\nfind all the real solutions of a system of N nonlinear equations in a certain\nfinite domain of the N-dimensional space of variables. We present an algorithm\nof minimum length and computational weight to solve this problem, resembling a\ngraphical tool of edge detection in an image extended to N dimensions. Once the\nhypersurfaces (edges) defined by each nonlinear equation have been identified\nin a single, simultaneous step, the coincidence of the hypersurfaces in the\nvicinity of all the hyperpoints that constitute the solutions makes the final\nNewton-Raphson step rapidly convergent to all the solutions with the desired\ndegree of accuracy. As long as N remains smaller than about five, which is\noften the case for physical systems that depend on fewer than five parameters,\nthis approach demonstrates excellent effectiveness.",
            "author": [
                "Fernando Chueca-Diez",
                "Alfonso M. Ganan-Calvo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03927v1",
                "http://arxiv.org/pdf/2312.03927v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03925v1",
            "title": "Pruning vineyards: updating barcodes by removing simplices",
            "updated": "2023-12-06T22:04:14Z",
            "published": "2023-12-06T22:04:14Z",
            "summary": "The barcode computation of a filtration can be computationally expensive.\nTherefore, it is useful to have methods to update a barcode if the associated\nfiltration undergoes small changes, such as changing the entrance order, or\nadding and removing simplices. There is already a rich literature on how to\nefficiently update a barcode in the first two cases, but the latter case has\nnot been investigated yet. In this work, we provide an algorithm to update a\nreduced boundary matrix when simplices are removed. We show that the complexity\nof this algorithm is lower than recomputing the barcode from scratch, with both\ntheoretical and experimental methods.",
            "author": [
                "Barbara Giunti",
                "J\u0101nis Lazovskis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03925v1",
                "http://arxiv.org/pdf/2312.03925v1"
            ],
            "primary_category": "math.AT",
            "category": [
                "math.AT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03924v1",
            "title": "Integrating Traditional CS Class Activities with Computing for Social\n  Good, Ethics, and Communication and Leadership Skills",
            "updated": "2023-12-06T21:48:55Z",
            "published": "2023-12-06T21:48:55Z",
            "summary": "Software and information technologies are becoming increasingly integrated\nand pervasive in human society and range from automated decision making and\nsocial media and entertainment, to running critical social and physical\ninfrastructures like government programs, utilities, and financial\ninstitutions. As a result, there is a growing awareness of the need to develop\nprofessionals who will harness these technologies in fair and inclusive ways\nand use them to address global issues like health, water management, poverty,\nand human rights. In this regard, many academic researchers have expressed the\nneed to complement traditional teaching of CS technical skills with computer\nand information ethics (computing for social good), as well as communication\nand leadership skills. In this paper, we describe our goals and some possible\nclass activities we have developed and refined over the past few years with\nencouraging results, to help CS students understand the potential uses of\ncomputing for social good. In these carefully planned project assignments, we\nseamlessly integrate traditional approaches to develop technical skills with\nbroader professional responsibility and soft skills. We then discuss the\nlessons learned from these activities and briefly outline future plans.",
            "author": [
                "Renato Cortinovis",
                "Devender Goyal",
                "Luiz Fernando Capretz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03924v1",
                "http://arxiv.org/pdf/2312.03924v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03922v1",
            "title": "Slepian Beamforming: Broadband Beamforming using Streaming Least Squares",
            "updated": "2023-12-06T21:47:58Z",
            "published": "2023-12-06T21:47:58Z",
            "summary": "In this paper we revisit the classical problem of estimating a signal as it\nimpinges on a multi-sensor array. We focus on the case where the impinging\nsignal's bandwidth is appreciable and is operating in a broadband regime.\nEstimating broadband signals, often termed broadband (or wideband) beamforming,\nis traditionally done through filter and summation, true time delay, or a\ncoupling of the two. Our proposed method deviates substantially from these\nparadigms in that it requires no notion of filtering or true time delay. We use\nblocks of samples taken directly from the sensor outputs to fit a robust\nSlepian subspace model using a least squares approach. We then leverage this\nmodel to estimate uniformly spaced samples of the impinging signal. Alongside a\ncareful discussion of this model and how to choose its parameters we show how\nto fit the model to new blocks of samples as they are received, producing a\nstreaming output. We then go on to show how this method naturally extends to\nadaptive beamforming scenarios, where we leverage signal statistics to\nattenuate interfering sources. Finally, we discuss how to use our model to\nestimate from dimensionality reducing measurements. Accompanying these\ndiscussions are extensive numerical experiments establishing that our method\noutperforms existing filter based approaches while being comparable in terms of\ncomputational complexity.",
            "author": [
                "Coleman DeLude",
                "Mark A. Davenport",
                "Justin Romberg"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03922v1",
                "http://arxiv.org/pdf/2312.03922v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03919v1",
            "title": "Indivisibility and uniform computational strength",
            "updated": "2023-12-06T21:34:37Z",
            "published": "2023-12-06T21:34:37Z",
            "summary": "A countable structure is indivisible if for every coloring with finite range\nthere is a monochromatic isomorphic subcopy of the structure. Each indivisible\nstructure $\\mathcal{S}$ naturally corresponds to an indivisibility problem\n$\\mathsf{Ind}\\ \\mathcal{S}$, which outputs such a subcopy given a presentation\nand coloring. We investigate the Weihrauch complexity of the indivisibility\nproblems for two structures: the rational numbers $\\mathbb{Q}$ as a linear\norder, and the equivalence relation $\\mathscr{E}$ with countably many\nequivalence classes each having countably many members. We separate the\nWeihrauch degrees of both $\\mathsf{Ind}\\ \\mathbb{Q}$ and $\\mathsf{Ind}\\\n\\mathscr{E}$ from several benchmark problems, showing in particular that\n$\\mathsf{C}_\\mathbb{N} \\vert_\\mathrm{W} \\mathsf{Ind}\\ \\mathbb{Q}$ and hence\n$\\mathsf{Ind}\\ \\mathbb{Q}$ is strictly weaker than the problem of finding an\ninterval in which some color is dense for a given coloring of $\\mathbb{Q}$; and\nthat the Weihrauch degree of $\\mathsf{Ind}\\ \\mathscr{E}_k$ is strictly between\nthose of $\\mathsf{SRT}^2_k$ and $\\mathsf{RT}^2_k$, where $\\mathsf{Ind}\\\n\\mathcal{S}_k$ is the restriction of $\\mathsf{Ind}\\ \\mathcal{S}$ to\n$k$-colorings.",
            "author": [
                "Kenneth Gill"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03919v1",
                "http://arxiv.org/pdf/2312.03919v1"
            ],
            "primary_category": "math.LO",
            "category": [
                "math.LO",
                "cs.LO",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03915v1",
            "title": "Alternative models for FX, arbitrage opportunities and efficient pricing\n  of double barrier options in L\u00e9vy models",
            "updated": "2023-12-06T21:26:58Z",
            "published": "2023-12-06T21:26:58Z",
            "summary": "We analyze the qualitative differences between prices of double barrier\nno-touch options in the Heston model and pure jump KoBoL model calibrated to\nthe same set of the empirical data, and discuss the potential for arbitrage\nopportunities if the correct model is a pure jump model. We explain and\ndemonstrate with numerical examples that accurate and fast calculations of\nprices of double barrier options in jump models are extremely difficult using\nthe numerical methods available in the literature. We develop a new efficient\nmethod (GWR-SINH method) based of the Gaver-Wynn-Rho acceleration applied to\nthe Bromwich integral; the SINH-acceleration and simplified trapezoid rule are\nused to evaluate perpetual double barrier options for each value of the\nspectral parameter in GWR-algorithm. The program in Matlab running on a Mac\nwith moderate characteristics achieves the precision of the order of E-5 and\nbetter in several several dozen of milliseconds; the precision E-07 is\nachievable in about 0.1 sec. We outline the extension of GWR-SINH method to\nregime-switching models and models with stochastic parameters and stochastic\ninterest rates.",
            "author": [
                "Svetlana Boyarchenko",
                "Sergei Levendorskii"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03915v1",
                "http://arxiv.org/pdf/2312.03915v1"
            ],
            "primary_category": "q-fin.CP",
            "category": [
                "q-fin.CP",
                "econ.EM",
                "60-08, 42A38, 42B10, 44A10, 65R10, 65G51, 91G20, 91G60"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03913v1",
            "title": "Controllable Human-Object Interaction Synthesis",
            "updated": "2023-12-06T21:14:20Z",
            "published": "2023-12-06T21:14:20Z",
            "summary": "Synthesizing semantic-aware, long-horizon, human-object interaction is\ncritical to simulate realistic human behaviors. In this work, we address the\nchallenging problem of generating synchronized object motion and human motion\nguided by language descriptions in 3D scenes. We propose Controllable\nHuman-Object Interaction Synthesis (CHOIS), an approach that generates object\nmotion and human motion simultaneously using a conditional diffusion model\ngiven a language description, initial object and human states, and sparse\nobject waypoints. While language descriptions inform style and intent,\nwaypoints ground the motion in the scene and can be effectively extracted using\nhigh-level planning methods. Naively applying a diffusion model fails to\npredict object motion aligned with the input waypoints and cannot ensure the\nrealism of interactions that require precise hand-object contact and\nappropriate contact grounded by the floor. To overcome these problems, we\nintroduce an object geometry loss as additional supervision to improve the\nmatching between generated object motion and input object waypoints. In\naddition, we design guidance terms to enforce contact constraints during the\nsampling process of the trained diffusion model.",
            "author": [
                "Jiaman Li",
                "Alexander Clegg",
                "Roozbeh Mottaghi",
                "Jiajun Wu",
                "Xavier Puig",
                "C. Karen Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03913v1",
                "http://arxiv.org/pdf/2312.03913v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03912v1",
            "title": "Collaboration or Corporate Capture? Quantifying NLP's Reliance on\n  Industry Artifacts and Contributions",
            "updated": "2023-12-06T21:12:22Z",
            "published": "2023-12-06T21:12:22Z",
            "summary": "The advent of transformers, higher computational budgets, and big data has\nengendered remarkable progress in Natural Language Processing (NLP). Impressive\nperformance of industry pre-trained models has garnered public attention in\nrecent years and made news headlines. That these are industry models is\nnoteworthy. Rarely, if ever, are academic institutes producing exciting new NLP\nmodels. Using these models is critical for competing on NLP benchmarks and\ncorrespondingly to stay relevant in NLP research. We surveyed 100 papers\npublished at EMNLP 2022 to determine whether this phenomenon constitutes a\nreliance on industry for NLP publications.\n  We find that there is indeed a substantial reliance. Citations of industry\nartifacts and contributions across categories is at least three times greater\nthan industry publication rates per year. Quantifying this reliance does not\nsettle how we ought to interpret the results. We discuss two possible\nperspectives in our discussion: 1) Is collaboration with industry still\ncollaboration in the absence of an alternative? Or 2) has free NLP inquiry been\ncaptured by the motivations and research direction of private corporations?",
            "author": [
                "Will Aitken",
                "Mohamed Abdalla",
                "Karen Rudie",
                "Catherine Stinson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03912v1",
                "http://arxiv.org/pdf/2312.03912v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03911v1",
            "title": "Improving Gradient-guided Nested Sampling for Posterior Inference",
            "updated": "2023-12-06T21:09:18Z",
            "published": "2023-12-06T21:09:18Z",
            "summary": "We present a performant, general-purpose gradient-guided nested sampling\nalgorithm, ${\\tt GGNS}$, combining the state of the art in differentiable\nprogramming, Hamiltonian slice sampling, clustering, mode separation, dynamic\nnested sampling, and parallelization. This unique combination allows ${\\tt\nGGNS}$ to scale well with dimensionality and perform competitively on a variety\nof synthetic and real-world problems. We also show the potential of combining\nnested sampling with generative flow networks to obtain large amounts of\nhigh-quality samples from the posterior distribution. This combination leads to\nfaster mode discovery and more accurate estimates of the partition function.",
            "author": [
                "Pablo Lemos",
                "Nikolay Malkin",
                "Will Handley",
                "Yoshua Bengio",
                "Yashar Hezaveh",
                "Laurence Perreault-Levasseur"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03911v1",
                "http://arxiv.org/pdf/2312.03911v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.CO",
                "stat.ME",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03908v1",
            "title": "A Theory of Irrotational Contact Fields",
            "updated": "2023-12-06T21:05:39Z",
            "published": "2023-12-06T21:05:39Z",
            "summary": "We present a framework that enables to write a family of convex\napproximations of complex contact models. Within this framework, we show that\nwe can incorporate well established and experimentally validated contact models\nsuch as the Hunt & Crossley model. Moreover, we show how to incorporate\nCoulomb's law and the principle of maximum dissipation using a regularized\nmodel of friction. Contrary to common wisdom that favors the use of rigid\ncontact models, our convex formulation is robust and performant even at high\nstiffness values far beyond that of materials such as steel. Therefore, the\nsame formulation enables the modeling of compliant surfaces such as rubber\ngripper pads or robot feet as well as hard objects. We characterize and\nevaluate our approximations in a number of tests cases. We report their\nproperties and highlight limitations.\n  Finally, we demonstrate robust simulation of robotic tasks at interactive\nrates, with accurately resolved stiction and contact transitions, as required\nfor meaningful sim-to-real transfer. Our method is implemented in the open\nsource robotics toolkit Drake.",
            "author": [
                "Alejandro Castro",
                "Xuchen Han",
                "Joseph Masterjohn"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03908v1",
                "http://arxiv.org/pdf/2312.03908v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CE",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03906v1",
            "title": "Computing the Volume of a Restricted Independent Set Polytope\n  Deterministically",
            "updated": "2023-12-06T20:59:27Z",
            "published": "2023-12-06T20:59:27Z",
            "summary": "We construct a quasi-polynomial time deterministic approximation algorithm\nfor computing the volume of an independent set polytope with restrictions.\nRandomized polynomial time approximation algorithms for computing the volume of\na convex body have been known now for several decades, but the corresponding\ndeterministic counterparts are not available, and our algorithm is the first of\nthis kind. The class of polytopes for which our algorithm applies arises as\nlinear programming relaxation of the independent set problem with the\nadditional restriction that each variable takes value in the interval\n$[0,1-\\alpha]$ for some $\\alpha<1/2$. (We note that the $\\alpha\\ge 1/2$ case is\ntrivial).\n  We use the correlation decay method for this problem applied to its\nappropriate and natural discretization. The method works provided $\\alpha>\n1/2-O(1/\\Delta^2)$, where $\\Delta$ is the maximum degree of the graph. When\n$\\Delta=3$ (the sparsest non-trivial case), our method works provided\n$0.488<\\alpha<0.5$. Interestingly, the interpolation method, which is based on\nanalyzing complex roots of the associated partition functions, fails even in\nthe trivial case when the underlying graph is a singleton.",
            "author": [
                "David Gamarnik",
                "Devin Smedira"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03906v1",
                "http://arxiv.org/pdf/2312.03906v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DM",
                "math.CO",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03905v1",
            "title": "A Pseudo-Semantic Loss for Autoregressive Models with Logical\n  Constraints",
            "updated": "2023-12-06T20:58:07Z",
            "published": "2023-12-06T20:58:07Z",
            "summary": "Neuro-symbolic AI bridges the gap between purely symbolic and neural\napproaches to learning. This often requires maximizing the likelihood of a\nsymbolic constraint w.r.t the neural network's output distribution. Such output\ndistributions are typically assumed to be fully-factorized. This limits the\napplicability of neuro-symbolic learning to the more expressive autoregressive\ndistributions, e.g., transformers. Under such distributions, computing the\nlikelihood of even simple constraints is #P-hard. Instead of attempting to\nenforce the constraint on the entire output distribution, we propose to do so\non a random, local approximation thereof. More precisely, we optimize the\nlikelihood of the constraint under a pseudolikelihood-based approximation\ncentered around a model sample. Our approximation is factorized, allowing the\nreuse of solutions to sub-problems, a main tenet for efficiently computing\nneuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of\nthe likelihood, exhibiting low entropy and KL-divergence around the model\nsample. We evaluate our approach on Sudoku and shortest-path prediction cast as\nautoregressive generation, and observe that we greatly improve upon the base\nmodel's ability to predict logically-consistent outputs. We also evaluate on\nthe task of detoxifying large language models. Using a simple constraint\ndisallowing a list of toxic words, we are able to steer the model's outputs\naway from toxic generations, achieving SoTA detoxification compared to previous\napproaches.",
            "author": [
                "Kareem Ahmed",
                "Kai-Wei Chang",
                "Guy Van den Broeck"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03905v1",
                "http://arxiv.org/pdf/2312.03905v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03901v1",
            "title": "Redrawing the 2012 map of the Maryland congressional districts",
            "updated": "2023-12-06T20:51:40Z",
            "published": "2023-12-06T20:51:40Z",
            "summary": "Gerrymandering is the practice of drawing biased electoral maps that\nmanipulate the voter population to gain an advantage. The most recent time\ngerrymandering became an issue was 2019 when the U.S. Federal Supreme Court\ndecided that the court does not have the authority to dictate how to draw the\ndistrict map and state legislators are the ones who should come up with an\nelectoral district plan. We solve the political districting problem and redraw\nthe 2012 map of Maryland congressional districts which raised the issue in\n2019.",
            "author": [
                "Noah Lee",
                "Hyunwoo Park",
                "Sangho Shim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03901v1",
                "http://arxiv.org/pdf/2312.03901v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "90"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03899v1",
            "title": "An Extension of the Non-Inferior Set Estimation Algorithm for Many\n  Objectives",
            "updated": "2023-12-06T20:48:37Z",
            "published": "2023-12-06T20:48:37Z",
            "summary": "This work proposes a novel multi-objective optimization approach that\nglobally finds a representative non-inferior set of solutions, also known as\nPareto-optimal solutions, by automatically formulating and solving a sequence\nof weighted sum method scalarization problems. The approach is called MONISE\n(Many-Objective NISE) because it represents an extension of the well-known\nnon-inferior set estimation (NISE) algorithm, which was originally conceived to\ndeal with two-dimensional objective spaces. The proposal is endowed with the\nfollowing characteristics: (1) uses a mixed-integer linear programming\nformulation to operate in two or more dimensions, thus properly supporting many\n(i.e., three or more) objectives; (2) relies on an external algorithm to solve\nthe weighted sum method scalarization problem to optimality; and (3) creates a\nfaithful representation of the Pareto frontier in the case of convex problems,\nand a useful approximation of it in the non-convex case. Moreover, when dealing\nspecifically with two objectives, some additional properties are portrayed for\nthe estimated non-inferior set. Experimental results validate the proposal and\nindicate that MONISE is competitive, in convex and non-convex (combinatorial)\nproblems, both in terms of computational cost and the overall quality of the\nnon-inferior set, measured by the acquired hypervolume.",
            "author": [
                "Marcos M. Raimundo",
                "Paulo A. V. Ferreira",
                "Fernando J. Von Zuben"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.ejor.2019.11.017",
                "http://arxiv.org/abs/2312.03899v1",
                "http://arxiv.org/pdf/2312.03899v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03897v1",
            "title": "Revisiting the Optimality of Word Lengths",
            "updated": "2023-12-06T20:41:47Z",
            "published": "2023-12-06T20:41:47Z",
            "summary": "Zipf (1935) posited that wordforms are optimized to minimize utterances'\ncommunicative costs. Under the assumption that cost is given by an utterance's\nlength, he supported this claim by showing that words' lengths are inversely\ncorrelated with their frequencies. Communicative cost, however, can be\noperationalized in different ways. Piantadosi et al. (2011) claim that cost\nshould be measured as the distance between an utterance's information rate and\nchannel capacity, which we dub the channel capacity hypothesis (CCH) here.\nFollowing this logic, they then proposed that a word's length should be\nproportional to the expected value of its surprisal (negative log-probability\nin context). In this work, we show that Piantadosi et al.'s derivation does not\nminimize CCH's cost, but rather a lower bound, which we term CCH-lower. We\npropose a novel derivation, suggesting an improved way to minimize CCH's cost.\nUnder this method, we find that a language's word lengths should instead be\nproportional to the surprisal's expectation plus its variance-to-mean ratio.\nExperimentally, we compare these three communicative cost functions: Zipf's,\nCCH-lower , and CCH. Across 13 languages and several experimental settings, we\nfind that length is better predicted by frequency than either of the other\nhypotheses. In fact, when surprisal's expectation, or expectation plus\nvariance-to-mean ratio, is estimated using better language models, it leads to\nworse word length predictions. We take these results as evidence that Zipf's\nlongstanding hypothesis holds.",
            "author": [
                "Tiago Pimentel",
                "Clara Meister",
                "Ethan Gotlieb Wilcox",
                "Kyle Mahowald",
                "Ryan Cotterell"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03897v1",
                "http://arxiv.org/pdf/2312.03897v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03896v1",
            "title": "A Tight Threshold Bound for Search Trees with 2-way Comparisons",
            "updated": "2023-12-06T20:40:48Z",
            "published": "2023-12-06T20:40:48Z",
            "summary": "We study search trees with 2-way comparisons (2WCST's), which involve\nseparate less-than and equal-to tests in their nodes, each test having two\npossible outcomes, yes and no. These trees have a much subtler structure than\nstandard search trees with 3-way comparisons (3WCST's) and are still not well\nunderstood, hampering progress towards designing an efficient algorithm for\ncomputing minimum-cost trees. One question that attracted attention in the past\nis whether there is an easy way to determine which type of comparison should be\napplied at any step of the search. Anderson, Kannan, Karloff and Ladner studied\nthis in terms of the ratio between the maximum and total key weight, and\ndefined two threshold values: $\\lambda^-$ is the largest ratio that forces the\nless-than test, and $\\lambda^+$ is the smallest ratio that forces the equal-to\ntest. They determined that $\\lambda^- = 1/4$, but for the higher threshold they\nonly showed that $\\lambda^+\\in [3/7,4/9]$. We give the tight bound for the\nhigher threshold, by proving that in fact $\\lambda^+ = 3/7$.",
            "author": [
                "Sunny Atalig",
                "Marek Chrobak"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03896v1",
                "http://arxiv.org/pdf/2312.03896v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03895v1",
            "title": "HLoOP -- Hyperbolic 2-space Local Outlier Probabilities",
            "updated": "2023-12-06T20:38:39Z",
            "published": "2023-12-06T20:38:39Z",
            "summary": "Hyperbolic geometry has recently garnered considerable attention in machine\nlearning due to its capacity to embed hierarchical graph structures with low\ndistortions for further downstream processing. This paper introduces a simple\nframework to detect local outliers for datasets grounded in hyperbolic 2-space\nreferred to as HLoOP (Hyperbolic Local Outlier Probability). Within a Euclidean\nspace, well-known techniques for local outlier detection are based on the Local\nOutlier Factor (LOF) and its variant, the LoOP (Local Outlier Probability),\nwhich incorporates probabilistic concepts to model the outlier level of a data\nvector. The developed HLoOP combines the idea of finding nearest neighbors,\ndensity-based outlier scoring with a probabilistic, statistically oriented\napproach. Therefore, the method consists in computing the Riemmanian distance\nof a data point to its nearest neighbors following a Gaussian probability\ndensity function expressed in a hyperbolic space. This is achieved by defining\na Gaussian cumulative distribution in this space. The HLoOP algorithm is tested\non the WordNet dataset yielding promising results. Code and data will be made\navailable on request for reproductibility.",
            "author": [
                "Cl\u00e9mence Allietta",
                "Jean-Philippe Condomines",
                "Jean-Yves Tourneret",
                "Emmanuel Lochin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03895v1",
                "http://arxiv.org/pdf/2312.03895v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03893v1",
            "title": "Deliberative Technology for Alignment",
            "updated": "2023-12-06T20:34:32Z",
            "published": "2023-12-06T20:34:32Z",
            "summary": "For humanity to maintain and expand its agency into the future, the most\npowerful systems we create must be those which act to align the future with the\nwill of humanity. The most powerful systems today are massive institutions like\ngovernments, firms, and NGOs. Deliberative technology is already being used\nacross these institutions to help align governance and diplomacy with human\nwill, and modern AI is poised to make this technology significantly better. At\nthe same time, the race to superhuman AGI is already underway, and the AI\nsystems it gives rise to may become the most powerful systems of the future.\nFailure to align the impact of such powerful AI with the will of humanity may\nlead to catastrophic consequences, while success may unleash abundance. Right\nnow, there is a window of opportunity to use deliberative technology to align\nthe impact of powerful AI with the will of humanity. Moreover, it may be\npossible to engineer a symbiotic coupling between powerful AI and deliberative\nalignment systems such that the quality of alignment improves as AI\ncapabilities increase.",
            "author": [
                "Andrew Konya",
                "Deger Turan",
                "Aviv Ovadya",
                "Lina Qui",
                "Daanish Masood",
                "Flynn Devine",
                "Lisa Schirch",
                "Isabella Roberts",
                "Deliberative Alignment Forum"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03893v1",
                "http://arxiv.org/pdf/2312.03893v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03891v1",
            "title": "Evaluation of Infrastructure-based Warning System on Driving Behaviors-A\n  Roundabout Study",
            "updated": "2023-12-06T20:31:22Z",
            "published": "2023-12-06T20:31:22Z",
            "summary": "Smart intersections have the potential to improve road safety with sensing,\ncommunication, and edge computing technologies. Perception sensors installed at\na smart intersection can monitor the traffic environment in real time and send\ninfrastructure-based warnings to nearby travelers through V2X communication.\nThis paper investigated how infrastructure-based warnings can influence driving\nbehaviors and improve roundabout safety through a driving-simulator study - a\nchallenging driving scenario for human drivers. A co-simulation platform\nintegrating Simulation of Urban Mobility (SUMO) and Webots was developed to\nserve as the driving simulator. A real-world roundabout in Ann Arbor, Michigan\nwas built in the co-simulation platform as the study area, and the merging\nscenarios were investigated. 36 participants were recruited and asked to\nnavigate the roundabout under three danger levels (e.g., low, medium, high) and\nthree collision warning designs (e.g., no warning, warning issued 1 second in\nadvance, warning issued 2 seconds in advance). Results indicated that advanced\nwarnings can significantly enhance safety by minimizing potential risks\ncompared to scenarios without warnings. Earlier warnings enabled smoother\ndriver responses and reduced abrupt decelerations. In addition, a personalized\nintention prediction model was developed to predict drivers' stop-or-go\ndecisions when the warning is displayed. Among all tested machine learning\nmodels, the XGBoost model achieved the highest prediction accuracy with a\nprecision rate of 95.56% and a recall rate of 97.73%.",
            "author": [
                "Cong Zhang",
                "Chi Tian",
                "Tianfang Han",
                "Hang Li",
                "Yiheng Feng",
                "Yunfeng Chen",
                "Robert W. Proctor",
                "Jiansong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03891v1",
                "http://arxiv.org/pdf/2312.03891v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03889v1",
            "title": "A Masked Pruning Approach for Dimensionality Reduction in\n  Communication-Efficient Federated Learning Systems",
            "updated": "2023-12-06T20:29:23Z",
            "published": "2023-12-06T20:29:23Z",
            "summary": "Federated Learning (FL) represents a growing machine learning (ML) paradigm\ndesigned for training models across numerous nodes that retain local datasets,\nall without directly exchanging the underlying private data with the parameter\nserver (PS). Its increasing popularity is attributed to notable advantages in\nterms of training deep neural network (DNN) models under privacy aspects and\nefficient utilization of communication resources. Unfortunately, DNNs suffer\nfrom high computational and communication costs, as well as memory consumption\nin intricate tasks. These factors restrict the applicability of FL algorithms\nin communication-constrained systems with limited hardware resources.\n  In this paper, we develop a novel algorithm that overcomes these limitations\nby synergistically combining a pruning-based method with the FL process,\nresulting in low-dimensional representations of the model with minimal\ncommunication cost, dubbed Masked Pruning over FL (MPFL). The algorithm\noperates by initially distributing weights to the nodes through the PS.\nSubsequently, each node locally trains its model and computes pruning masks.\nThese low-dimensional masks are then transmitted back to the PS, which\ngenerates a consensus pruning mask, broadcasted back to the nodes. This\niterative process enhances the robustness and stability of the masked pruning\nmodel. The generated mask is used to train the FL model, achieving significant\nbandwidth savings. We present an extensive experimental study demonstrating the\nsuperior performance of MPFL compared to existing methods. Additionally, we\nhave developed an open-source software package for the benefit of researchers\nand developers in related fields.",
            "author": [
                "Tamir L. S. Gez",
                "Kobi Cohen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03889v1",
                "http://arxiv.org/pdf/2312.03889v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03886v1",
            "title": "On The Fairness Impacts of Hardware Selection in Machine Learning",
            "updated": "2023-12-06T20:24:17Z",
            "published": "2023-12-06T20:24:17Z",
            "summary": "In the machine learning ecosystem, hardware selection is often regarded as a\nmere utility, overshadowed by the spotlight on algorithms and data. This\noversight is particularly problematic in contexts like ML-as-a-service\nplatforms, where users often lack control over the hardware used for model\ndeployment. How does the choice of hardware impact generalization properties?\nThis paper investigates the influence of hardware on the delicate balance\nbetween model performance and fairness. We demonstrate that hardware choices\ncan exacerbate existing disparities, attributing these discrepancies to\nvariations in gradient flows and loss surfaces across different demographic\ngroups. Through both theoretical and empirical analysis, the paper not only\nidentifies the underlying factors but also proposes an effective strategy for\nmitigating hardware-induced performance imbalances.",
            "author": [
                "Sree Harsha Nelaturu",
                "Nishaanth Kanna Ravichandran",
                "Cuong Tran",
                "Sara Hooker",
                "Ferdinando Fioretto"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03886v1",
                "http://arxiv.org/pdf/2312.03886v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03885v1",
            "title": "Adapting Newton's Method to Neural Networks through a Summary of\n  Higher-Order Derivatives",
            "updated": "2023-12-06T20:24:05Z",
            "published": "2023-12-06T20:24:05Z",
            "summary": "We consider a gradient-based optimization method applied to a function\n$\\mathcal{L}$ of a vector of variables $\\boldsymbol{\\theta}$, in the case where\n$\\boldsymbol{\\theta}$ is represented as a tuple of tensors $(\\mathbf{T}_1,\n\\cdots, \\mathbf{T}_S)$. This framework encompasses many common use-cases, such\nas training neural networks by gradient descent. First, we propose a\ncomputationally inexpensive technique providing higher-order information on\n$\\mathcal{L}$, especially about the interactions between the tensors\n$\\mathbf{T}_s$, based on automatic differentiation and computational tricks.\nSecond, we use this technique at order 2 to build a second-order optimization\nmethod which is suitable, among other things, for training deep neural networks\nof various architectures. This second-order method leverages the partition\nstructure of $\\boldsymbol{\\theta}$ into tensors $(\\mathbf{T}_1, \\cdots,\n\\mathbf{T}_S)$, in such a way that it requires neither the computation of the\nHessian of $\\mathcal{L}$ according to $\\boldsymbol{\\theta}$, nor any\napproximation of it. The key part consists in computing a smaller matrix\ninterpretable as a \"Hessian according to the partition\", which can be computed\nexactly and efficiently. In contrast to many existing practical second-order\nmethods used in neural networks, which perform a diagonal or block-diagonal\napproximation of the Hessian or its inverse, the method we propose does not\nneglect interactions between layers. Finally, we can tune the coarseness of the\npartition to recover well-known optimization methods: the coarsest case\ncorresponds to Cauchy's steepest descent method, the finest case corresponds to\nthe usual Newton's method.",
            "author": [
                "Pierre Wolinski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03885v1",
                "http://arxiv.org/pdf/2312.03885v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03884v1",
            "title": "WonderJourney: Going from Anywhere to Everywhere",
            "updated": "2023-12-06T20:22:32Z",
            "published": "2023-12-06T20:22:32Z",
            "summary": "We introduce WonderJourney, a modularized framework for perpetual 3D scene\ngeneration. Unlike prior work on view generation that focuses on a single type\nof scenes, we start at any user-provided location (by a text description or an\nimage) and generate a journey through a long sequence of diverse yet coherently\nconnected 3D scenes. We leverage an LLM to generate textual descriptions of the\nscenes in this journey, a text-driven point cloud generation pipeline to make a\ncompelling and coherent sequence of 3D scenes, and a large VLM to verify the\ngenerated scenes. We show compelling, diverse visual results across various\nscene types and styles, forming imaginary \"wonderjourneys\". Project website:\nhttps://kovenyu.com/WonderJourney/",
            "author": [
                "Hong-Xing Yu",
                "Haoyi Duan",
                "Junhwa Hur",
                "Kyle Sargent",
                "Michael Rubinstein",
                "William T. Freeman",
                "Forrester Cole",
                "Deqing Sun",
                "Noah Snavely",
                "Jiajun Wu",
                "Charles Herrmann"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03884v1",
                "http://arxiv.org/pdf/2312.03884v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03883v1",
            "title": "The Gailitis-Damburg oscillations in the three-body $e^-e^+\\bar{p}$\n  system",
            "updated": "2023-12-06T20:17:53Z",
            "published": "2023-12-06T20:17:53Z",
            "summary": "We study the near threshold behavior of cross sections of low-energy\nantiproton scattering off the ground and excited states of positronium with\nzero total orbital momentum $L=0$. In our computational experiment, the\nexistence of singularities called the Gailitis-Damburg oscillations above the\nthresholds of excited states of positronium and antihydrogen atoms is\nconfirmed. In the future the obtained results can be useful for developing\nproposals for improving the conditions of experiments with antimatter.",
            "author": [
                "V. A. Gradusov",
                "S. L. Yakovlev"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03883v1",
                "http://arxiv.org/pdf/2312.03883v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "physics.atom-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03881v1",
            "title": "FoMo Rewards: Can we cast foundation models as reward functions?",
            "updated": "2023-12-06T20:11:02Z",
            "published": "2023-12-06T20:11:02Z",
            "summary": "We explore the viability of casting foundation models as generic reward\nfunctions for reinforcement learning. To this end, we propose a simple pipeline\nthat interfaces an off-the-shelf vision model with a large language model.\nSpecifically, given a trajectory of observations, we infer the likelihood of an\ninstruction describing the task that the user wants an agent to perform. We\nshow that this generic likelihood function exhibits the characteristics ideally\nexpected from a reward function: it associates high values with the desired\nbehaviour and lower values for several similar, but incorrect policies.\nOverall, our work opens the possibility of designing open-ended agents for\ninteractive tasks via foundation models.",
            "author": [
                "Ekdeep Singh Lubana",
                "Johann Brehmer",
                "Pim de Haan",
                "Taco Cohen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03881v1",
                "http://arxiv.org/pdf/2312.03881v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03876v1",
            "title": "Scaling transformer neural networks for skillful and reliable\n  medium-range weather forecasting",
            "updated": "2023-12-06T19:46:06Z",
            "published": "2023-12-06T19:46:06Z",
            "summary": "Weather forecasting is a fundamental problem for anticipating and mitigating\nthe impacts of climate change. Recently, data-driven approaches for weather\nforecasting based on deep learning have shown great promise, achieving\naccuracies that are competitive with operational systems. However, those\nmethods often employ complex, customized architectures without sufficient\nablation analysis, making it difficult to understand what truly contributes to\ntheir success. Here we introduce Stormer, a simple transformer model that\nachieves state-of-the-art performance on weather forecasting with minimal\nchanges to the standard transformer backbone. We identify the key components of\nStormer through careful empirical analyses, including weather-specific\nembedding, randomized dynamics forecast, and pressure-weighted loss. At the\ncore of Stormer is a randomized forecasting objective that trains the model to\nforecast the weather dynamics over varying time intervals. During inference,\nthis allows us to produce multiple forecasts for a target lead time and combine\nthem to obtain better forecast accuracy. On WeatherBench 2, Stormer performs\ncompetitively at short to medium-range forecasts and outperforms current\nmethods beyond 7 days, while requiring orders-of-magnitude less training data\nand compute. Additionally, we demonstrate Stormer's favorable scaling\nproperties, showing consistent improvements in forecast accuracy with increases\nin model size and training tokens. Code and checkpoints will be made publicly\navailable.",
            "author": [
                "Tung Nguyen",
                "Rohan Shah",
                "Hritik Bansal",
                "Troy Arcomano",
                "Sandeep Madireddy",
                "Romit Maulik",
                "Veerabhadra Kotamarthi",
                "Ian Foster",
                "Aditya Grover"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03876v1",
                "http://arxiv.org/pdf/2312.03876v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03872v1",
            "title": "The BigCode Project Governance Card",
            "updated": "2023-12-06T19:37:08Z",
            "published": "2023-12-06T19:37:08Z",
            "summary": "This document serves as an overview of the different mechanisms and areas of\ngovernance in the BigCode project. It aims to support transparency by providing\nrelevant information about choices that were made during the project to the\nbroader public, and to serve as an example of intentional governance of an open\nresearch project that future endeavors can leverage to shape their own\napproach. The first section, Project Structure, covers the project\norganization, its stated goals and values, its internal decision processes, and\nits funding and resources. The second section, Data and Model Governance,\ncovers decisions relating to the questions of data subject consent, privacy,\nand model release.",
            "author": [
                "BigCode collaboration",
                "Sean Hughes",
                "Harm de Vries",
                "Jennifer Robinson",
                "Carlos Mu\u00f1oz Ferrandis",
                "Loubna Ben Allal",
                "Leandro von Werra",
                "Jennifer Ding",
                "Sebastien Paquet",
                "Yacine Jernite"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03872v1",
                "http://arxiv.org/pdf/2312.03872v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.AI",
                "cs.CL",
                "cs.LG",
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03870v1",
            "title": "A M|M|m|m Queue System Transient Behavior Study",
            "updated": "2023-12-06T19:33:19Z",
            "published": "2023-12-06T19:33:19Z",
            "summary": "It is a very hard task to compute an exact solution for the differential\nequations, with differences, system that allows the determination of the\nM|M|m|m system transient probabilities. The respective complexity grows with m.\nThe computations are extremely fastidious and the length and the fact that the\nexpressions obtained are often approximate, and not exact, will not allow the\ntransient probabilities behavior as time functions characterization. To\novercome these problems, in this work it is analyzed how that system can supply\napproximate values to the M|M|m|m queue system. It is also presented an\nasymptotic method to solve the system that becomes possible in many cases to\nobtain simple approximated expressions for those probabilities using the\nM|M|Inf transient probabilities, very well-known and very much easier to study.",
            "author": [
                "Manuel Alberto M. Ferreira"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03870v1",
                "http://arxiv.org/pdf/2312.03870v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "cs.NA",
                "math.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03869v1",
            "title": "Inpaint3D: 3D Scene Content Generation using 2D Inpainting Diffusion",
            "updated": "2023-12-06T19:30:04Z",
            "published": "2023-12-06T19:30:04Z",
            "summary": "This paper presents a novel approach to inpainting 3D regions of a scene,\ngiven masked multi-view images, by distilling a 2D diffusion model into a\nlearned 3D scene representation (e.g. a NeRF). Unlike 3D generative methods\nthat explicitly condition the diffusion model on camera pose or multi-view\ninformation, our diffusion model is conditioned only on a single masked 2D\nimage. Nevertheless, we show that this 2D diffusion model can still serve as a\ngenerative prior in a 3D multi-view reconstruction problem where we optimize a\nNeRF using a combination of score distillation sampling and NeRF reconstruction\nlosses. Predicted depth is used as additional supervision to encourage accurate\ngeometry. We compare our approach to 3D inpainting methods that focus on object\nremoval. Because our method can generate content to fill any 3D masked region,\nwe additionally demonstrate 3D object completion, 3D object replacement, and 3D\nscene completion.",
            "author": [
                "Kira Prabhu",
                "Jane Wu",
                "Lynn Tsai",
                "Peter Hedman",
                "Dan B Goldman",
                "Ben Poole",
                "Michael Broxton"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03869v1",
                "http://arxiv.org/pdf/2312.03869v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03868v1",
            "title": "Uncertainty-Informed Renewable Energy Scheduling: A Scalable Bilevel\n  Framework",
            "updated": "2023-12-06T19:26:10Z",
            "published": "2023-12-06T19:26:10Z",
            "summary": "This work proposes an uncertainty-informed bid adjustment framework for\nintegrating variable renewable energy sources (VRES) into electricity markets.\nThis framework adopts a bilevel model to compute the optimal VRES day-ahead\nbids. It aims to minimize the expected system cost across day-ahead and\nreal-time stages and approximate the cost efficiency of the stochastic market\ndesign. However, solving the bilevel optimization problem is computationally\nchallenging for large-scale systems. To overcome this challenge, we introduce a\nnovel technique based on strong duality and McCormick envelopes, which relaxes\nthe problem to a linear program, enabling large-scale applications. The\nproposed bilevel framework is applied to the 1576-bus NYISO system and\nbenchmarked against a myopic strategy, where the VRES bid is the mean value of\nthe probabilistic power forecast. Results demonstrate that, under high VRES\npenetration levels (e.g., 40%), our framework can significantly reduce system\ncosts and market-price volatility, by optimizing VRES quantities efficiently in\nthe day-ahead market. Furthermore, we find that when transmission capacity\nincreases, the proposed bilevel model will still reduce the system cost,\nwhereas the myopic strategy may incur a much higher cost due to over-scheduling\nof VRES in the day-ahead market and the lack of flexible conventional\ngenerators in real time.",
            "author": [
                "Dongwei Zhao",
                "Vladimir Dvorkin",
                "Stefanos Delikaraoglou",
                "Alberto J. Lamadrid L.",
                "Audun Botterud"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03868v1",
                "http://arxiv.org/pdf/2312.03868v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY",
                "econ.GN",
                "math.OC",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03867v1",
            "title": "Multi-Group Fairness Evaluation via Conditional Value-at-Risk Testing",
            "updated": "2023-12-06T19:25:32Z",
            "published": "2023-12-06T19:25:32Z",
            "summary": "Machine learning (ML) models used in prediction and classification tasks may\ndisplay performance disparities across population groups determined by\nsensitive attributes (e.g., race, sex, age). We consider the problem of\nevaluating the performance of a fixed ML model across population groups defined\nby multiple sensitive attributes (e.g., race and sex and age). Here, the sample\ncomplexity for estimating the worst-case performance gap across groups (e.g.,\nthe largest difference in error rates) increases exponentially with the number\nof group-denoting sensitive attributes. To address this issue, we propose an\napproach to test for performance disparities based on Conditional Value-at-Risk\n(CVaR). By allowing a small probabilistic slack on the groups over which a\nmodel has approximately equal performance, we show that the sample complexity\nrequired for discovering performance violations is reduced exponentially to be\nat most upper bounded by the square root of the number of groups. As a\nbyproduct of our analysis, when the groups are weighted by a specific prior\ndistribution, we show that R\\'enyi entropy of order $2/3$ of the prior\ndistribution captures the sample complexity of the proposed CVaR test\nalgorithm. Finally, we also show that there exists a non-i.i.d. data collection\nstrategy that results in a sample complexity independent of the number of\ngroups.",
            "author": [
                "Lucas Monteiro Paes",
                "Ananda Theertha Suresh",
                "Alex Beutel",
                "Flavio P. Calmon",
                "Ahmad Beirami"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03867v1",
                "http://arxiv.org/pdf/2312.03867v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY",
                "cs.IT",
                "math.IT",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03863v1",
            "title": "Efficient Large Language Models: A Survey",
            "updated": "2023-12-06T19:18:42Z",
            "published": "2023-12-06T19:18:42Z",
            "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nimportant tasks such as natural language understanding, language generation,\nand complex reasoning and have the potential to make a substantial impact on\nour society. Such capabilities, however, come with the considerable resources\nthey demand, highlighting the strong need to develop effective techniques for\naddressing their efficiency challenges. In this survey, we provide a systematic\nand comprehensive review of efficient LLMs research. We organize the literature\nin a taxonomy consisting of three main categories, covering distinct yet\ninterconnected efficient LLMs topics from model-centric, data-centric, and\nframework-centric perspective, respectively. We have also created a GitHub\nrepository where we compile the papers featured in this survey at\nhttps://github.com/AIoT-MLSys-Lab/EfficientLLMs,\nhttps://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey, and will actively\nmaintain this repository and incorporate new research as it emerges. We hope\nour survey can serve as a valuable resource to help researchers and\npractitioners gain a systematic understanding of the research developments in\nefficient LLMs and inspire them to contribute to this important and exciting\nfield.",
            "author": [
                "Zhongwei Wan",
                "Xin Wang",
                "Che Liu",
                "Samiul Alam",
                "Yu Zheng",
                "Zhongnan Qu",
                "Shen Yan",
                "Yi Zhu",
                "Quanlu Zhang",
                "Mosharaf Chowdhury",
                "Mi Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03863v1",
                "http://arxiv.org/pdf/2312.03863v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03859v1",
            "title": "Towards Tight Bounds for the Graph Homomorphism Problem Parameterized by\n  Cutwidth via Asymptotic Rank Parameters",
            "updated": "2023-12-06T19:15:10Z",
            "published": "2023-12-06T19:15:10Z",
            "summary": "A homomorphism from a graph $G$ to a graph $H$ is an edge-preserving mapping\nfrom $V(G)$ to $V(H)$. In the graph homomorphism problem, denoted by $Hom(H)$,\nthe graph $H$ is fixed and we need to determine if there exists a homomorphism\nfrom an instance graph $G$ to $H$. We study the complexity of the problem\nparameterized by the cutwidth of $G$.\n  We aim, for each $H$, for algorithms for $Hom(H)$ running in time $c_H^k\nn^{\\mathcal{O}(1)}$ and matching lower bounds that exclude $c_H^{k \\cdot\no(1)}n^{\\mathcal{O}(1)}$ or $c_H^{k(1-\\Omega(1))}n^{\\mathcal{O}(1)}$ time\nalgorithms under the (Strong) Exponential Time Hypothesis.\n  In the paper we introduce a new parameter that we call $\\mathrm{mimsup}(H)$.\nOur main contribution is strong evidence of a close connection between $c_H$\nand $\\mathrm{mimsup}(H)$:\n  * an information-theoretic argument that the number of states needed in a\nnatural dynamic programming algorithm is at most $\\mathrm{mimsup}(H)^k$,\n  * lower bounds that show that for almost all graphs $H$ indeed we have $c_H\n\\geq \\mathrm{mimsup}(H)$, assuming the (Strong) Exponential-Time Hypothesis,\nand\n  * an algorithm with running time $\\exp ( {\\mathcal{O}( \\mathrm{mimsup}(H)\n\\cdot k \\log k)}) n^{\\mathcal{O}(1)}$.\n  The parameter $\\mathrm{mimsup}(H)$ can be thought of as the $p$-th root of\nthe maximum induced matching number in the graph obtained by multiplying $p$\ncopies of $H$ via certain graph product, where $p$ tends to infinity. It can\nalso be defined as an asymptotic rank parameter of the adjacency matrix of $H$.\nOur results tightly link the parameterized complexity of a problem to such an\nasymptotic rank parameter for the first time.",
            "author": [
                "Carla Groenland",
                "Isja Mannens",
                "Jesper Nederlof",
                "Marta Piecyk",
                "Pawe\u0142 Rz\u0105\u017cewski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03859v1",
                "http://arxiv.org/pdf/2312.03859v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03855v1",
            "title": "Confining Strings and Glueballs in $\\mathbb{Z}_N$ Gauge Theories",
            "updated": "2023-12-06T19:08:27Z",
            "published": "2023-12-06T19:08:27Z",
            "summary": "Effective string theory has shown its universal power in the prediction of\nthe spectrum of low-lying excited states of confining strings. Here we study\nconfining flux tubes in $\\mathbb{Z}_N$ gauge theories. For the $N=2$ theory,\nwhich corresponds to the 3d Ising gauge model, we compute the spectrum of\nlow-lying excitations of confining strings and show that it agrees with the\nuniversal Nambu--Goto predictions except for an additional massive scalar\nresonance. This resonance, however, turns out to be a bulk glueball mixing with\nthe flux tube excitations rather than a genuine string worldsheet state. In\ngeneral $\\mathbb{Z}_N$ gauge theories (dual to clock spin models), we observe a\ncontinuous phase transition for $N \\geq 4$, while for $N > 5$ it is governed by\nthe $O(2)$ universality class. The critical behavior of the string tension and\nmass gap is verified to be described by a dangerously irrelevant operator. At\nlarge $N$ the glueball spectrum is expected to approach the spectrum of U(1)\ngauge theory, which is confirmed by our lattice data.",
            "author": [
                "Andreas Athenodorou",
                "Sergei Dubovsky",
                "Conghuan Luo",
                "Michael Teper"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03855v1",
                "http://arxiv.org/pdf/2312.03855v1"
            ],
            "primary_category": "hep-lat",
            "category": [
                "hep-lat",
                "cond-mat.stat-mech",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03849v1",
            "title": "LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction\n  Tuning",
            "updated": "2023-12-06T19:02:40Z",
            "published": "2023-12-06T19:02:40Z",
            "summary": "Generating instructional images of human daily actions from an egocentric\nviewpoint serves a key step towards efficient skill transfer. In this paper, we\nintroduce a novel problem -- egocentric action frame generation. The goal is to\nsynthesize the action frame conditioning on the user prompt question and an\ninput egocentric image that captures user's environment. Notably, existing\negocentric datasets lack the detailed annotations that describe the execution\nof actions. Additionally, the diffusion-based image manipulation models fail to\ncontrol the state change of an action within the corresponding egocentric image\npixel space. To this end, we finetune a visual large language model (VLLM) via\nvisual instruction tuning for curating the enriched action descriptions to\naddress our proposed problem. Moreover, we propose to Learn EGOcentric (LEGO)\naction frame generation using image and text embeddings from VLLM as additional\nconditioning. We validate our proposed model on two egocentric datasets --\nEgo4D and Epic-Kitchens. Our experiments show prominent improvement over prior\nimage manipulation models in both quantitative and qualitative evaluation. We\nalso conduct detailed ablation studies and analysis to provide insights on our\nmethod.",
            "author": [
                "Bolin Lai",
                "Xiaoliang Dai",
                "Lawrence Chen",
                "Guan Pang",
                "James M. Rehg",
                "Miao Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03849v1",
                "http://arxiv.org/pdf/2312.03849v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03847v1",
            "title": "The Pristine Inner Galaxy Survey (PIGS) VIII: Characterising the orbital\n  properties of the ancient, very metal-poor inner Milky Way",
            "updated": "2023-12-06T19:02:02Z",
            "published": "2023-12-06T19:02:02Z",
            "summary": "The oldest stars in the Milky Way (born in the first few billion years) are\nexpected to have a high density in the inner few kpc, spatially overlapping\nwith the Galactic bulge. We use spectroscopic data from the Pristine Inner\nGalaxy Survey (PIGS) to study the dynamical properties of ancient, metal-poor\ninner Galaxy stars. We compute distances using StarHorse, and orbital\nproperties in a barred Galactic potential. With this paper, we release the\nspectroscopic AAT/PIGS catalogue (13 235 stars). We find that most PIGS stars\nhave orbits typical for a pressure-supported population. The fraction of stars\nconfined to the inner Galaxy decreases with decreasing metallicity, but many\nvery metal-poor stars (VMP, [Fe/H] < -2.0) stay confined (~ 60% stay within 5\nkpc). The azimuthal velocity v$_\\phi$ also decreases between [Fe/H] = -1.0 and\n-2.0, but is constant for VMP stars (at ~ 40 km/s). The carbon-enhanced\nmetal-poor (CEMP) stars in PIGS appear to have similar orbital properties\ncompared to normal VMP stars. Our results suggest a possible transition between\ntwo spheroidal components - a more metal-rich, more concentrated, faster\nrotating component, and a more metal-poor, more extended and\nslower/non-rotating component. We propose that the former may be connected to\npre-disc in-situ stars (or those born in large building blocks), whereas the\nlatter may be dominated by contributions from smaller galaxies. This is an\nexciting era where large metal-poor samples, such as in this work (as well as\nupcoming surveys, e.g., 4MOST), shed light on the earliest evolution of our\nGalaxy.",
            "author": [
                "Anke Ardern-Arentsen",
                "Giacomo Monari",
                "Anna B. A. Queiroz",
                "Else Starkenburg",
                "Nicolas F. Martin",
                "Cristina Chiappini",
                "David S. Aguado",
                "Vasily Belokurov",
                "Ray Carlberg",
                "Stephanie Monty",
                "GyuChul Myeong",
                "Mathias Schultheis",
                "Federico Sestito",
                "Kim A. Venn",
                "Sara Vitali",
                "Zhen Yuan",
                "Hanyuan Zhang",
                "Sven Buder",
                "Geraint F. Lewis",
                "William H. Oliver",
                "Zhen Wan",
                "Daniel B. Zucker"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03847v1",
                "http://arxiv.org/pdf/2312.03847v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03841v1",
            "title": "Phonon screening and dissociation of excitons at finite temperatures\n  from first principles",
            "updated": "2023-12-06T19:00:07Z",
            "published": "2023-12-06T19:00:07Z",
            "summary": "The properties of excitons, or correlated electron-hole pairs, are of\nparamount importance to optoelectronic applications of materials. A central\ncomponent of exciton physics is the electron-hole interaction, which is\ncommonly treated as screened solely by electrons within a material. However,\nnuclear motion can screen this Coulomb interaction as well, with several recent\nstudies developing model approaches for approximating the phonon screening to\nthe properties of excitons. While these model approaches tend to improve\nagreement with experiment for exciton properties, they rely on several\napproximations that restrict their applicability to a wide range of materials,\nand thus far they have neglected the effect of finite temperatures. Here, we\ndevelop a fully first-principles, parameter-free approach to compute the\ntemperature-dependent effects of phonon screening within the ab initio GW-Bethe\nSalpeter equation framework. We recover previously proposed models of phonon\nscreening as well-defined limits of our general framework, and discuss their\nvalidity by comparing them against our first-principles results. We develop an\nefficient computational workflow and apply it to a diverse set of\nsemiconductors, specifically AlN, CdS, GaN, MgO and SrTiO3. We demonstrate\nunder different physical scenarios how excitons may be screened by multiple\npolar optical or acoustic phonons, how their binding energies can exhibit\nstrong temperature dependence, and the ultrafast timescales on which they\ndissociate into free electron-hole pairs.",
            "author": [
                "Antonios M. Alvertis",
                "Jonah B. Haber",
                "Zhenglu Li",
                "Christopher J. N. Coveney",
                "Steven G. Louie",
                "Marina R. Filip",
                "Jeffrey B. Neaton"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03841v1",
                "http://arxiv.org/pdf/2312.03841v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03835v1",
            "title": "Constraining the gas mass of Herbig disks using CO isotopologues",
            "updated": "2023-12-06T19:00:03Z",
            "published": "2023-12-06T19:00:03Z",
            "summary": "The total disk mass sets the formation potential for exoplanets.\nCarbon-monoxide (CO) has been used as a gas mass tracer in T Tauri disks, but\nwas found to be less abundant than expected due to freeze-out and chemical\nconversion of CO on the surfaces of cold dust grains. The disks around more\nmassive intermediate mass pre-main sequence stars called Herbig disks are\nlikely to be warmer, allowing for the possibility of using CO as a more\neffective total gas mass tracer. Using ALMA archival data and new NOEMA data of\n12CO, 13CO, and C18O transitions of 35 Herbig disks within 450 pc, the masses\nare determined using the thermo-chemical code Dust And LInes (DALI). The\nmajority of Herbig disks for which 13CO and C18O are detected are optically\nthick in both. Computing the gas mass using a simple optically thin relation\nbetween line flux and column density results in an underestimate of the gas\nmass of at least an order of magnitude compared to the masses obtained with\nDALI. The inferred gas masses with DALI are consistent with a gas-to-dust ratio\nof at least 100. These gas-to-dust ratios are two orders of magnitude higher\ncompared to those found for T Tauri disks using similar techniques, even over\nmultiple orders of magnitude in dust mass, illustrating the importance of\nchemical conversion of CO in colder T Tauri disks. Similar high gas-to-dust\nratios are found for Herbig group I and II disks. Since group II disks have\ndust masses comparable to T Tauri disks, their higher CO gas masses illustrate\nthe determining role of temperature. Compared to debris disks, Herbig disks\nhave four orders of magnitude higher gas masses. At least one Herbig disk, HD\n163296, has a detected molecular disk wind, but our investigation has not\nturned up other detections of the CO disk wind in spite of similar\nsensitivities.",
            "author": [
                "L. M. Stapper",
                "M. R. Hogerheijde",
                "E. F. van Dishoeck",
                "L. Lin",
                "A. Ahmadi",
                "A. S. Booth",
                "S. L. Grant",
                "K. Immer",
                "M. Leemker",
                "A. F. P\u00e9rez-S\u00e1nchez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03835v1",
                "http://arxiv.org/pdf/2312.03835v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP",
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03836v1",
            "title": "Four-dimensional $\\mathcal N=2$ superconformal long circular quivers",
            "updated": "2023-12-06T19:00:03Z",
            "published": "2023-12-06T19:00:03Z",
            "summary": "We study four-dimensional $\\mathcal N=2$ superconformal circular, cyclic\nsymmetric quiver theories which are planar equivalent to $\\mathcal N=4$ super\nYang-Mills. We use localization to compute nonplanar corrections to the free\nenergy and the circular half-BPS Wilson loop in these theories for an arbitrary\nnumber of nodes, and examine their behaviour in the limit of long quivers.\nExploiting the relationship between the localization quiver matrix integrals\nand an integrable Bessel operator, we find a closed-form expression for the\nleading nonplanar correction to both observables in the limit when the number\nof nodes and 't Hooft coupling become large. We demonstrate that it has\ndifferent asymptotic behaviour depending on how the two parameters are\ncompared, and interpret this behaviour in terms of properties of a lattice\nmodel defined on the quiver diagram.",
            "author": [
                "M. Beccaria",
                "G. P. Korchemsky"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03836v1",
                "http://arxiv.org/pdf/2312.03836v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03830v1",
            "title": "QSlack: A slack-variable approach for variational quantum semi-definite\n  programming",
            "updated": "2023-12-06T19:00:01Z",
            "published": "2023-12-06T19:00:01Z",
            "summary": "Solving optimization problems is a key task for which quantum computers could\npossibly provide a speedup over the best known classical algorithms. Particular\nclasses of optimization problems including semi-definite programming (SDP) and\nlinear programming (LP) have wide applicability in many domains of computer\nscience, engineering, mathematics, and physics. Here we focus on semi-definite\nand linear programs for which the dimensions of the variables involved are\nexponentially large, so that standard classical SDP and LP solvers are not\nhelpful for such large-scale problems. We propose the QSlack and CSlack methods\nfor estimating their optimal values, respectively, which work by 1) introducing\nslack variables to transform inequality constraints to equality constraints, 2)\ntransforming a constrained optimization to an unconstrained one via the penalty\nmethod, and 3) replacing the optimizations over all possible non-negative\nvariables by optimizations over parameterized quantum states and parameterized\nprobability distributions. Under the assumption that the SDP and LP inputs are\nefficiently measurable observables, it follows that all terms in the resulting\nobjective functions are efficiently estimable by either a quantum computer in\nthe SDP case or a quantum or probabilistic computer in the LP case.\nFurthermore, by making use of SDP and LP duality theory, we prove that these\nmethods provide a theoretical guarantee that, if one could find global optima\nof the objective functions, then the resulting values sandwich the true optimal\nvalues from both above and below. Finally, we showcase the QSlack and CSlack\nmethods on a variety of example optimization problems and discuss details of\nour implementation, as well as the resulting performance. We find that our\nimplementations of both the primal and dual for these problems approach the\nground truth, typically achieving errors of order $10^{-2}$.",
            "author": [
                "Jingxuan Chen",
                "Hanna Westerheim",
                "Zo\u00eb Holmes",
                "Ivy Luo",
                "Theshani Nuradha",
                "Dhrumil Patel",
                "Soorya Rethinasamy",
                "Kathie Wang",
                "Mark M. Wilde"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03830v1",
                "http://arxiv.org/pdf/2312.03830v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03820v1",
            "title": "The Ambient Space Formalism",
            "updated": "2023-12-06T19:00:00Z",
            "published": "2023-12-06T19:00:00Z",
            "summary": "We present a new formalism to solve the kinematical constraints due to Weyl\ninvariance for CFTs in curved backgrounds and/or non-trivial states, and we\napply it to thermal CFTs and to CFTs on squashed spheres. The ambient space\nformalism is based on constructing a class of geometric objects that are Weyl\ncovariant and identifying them as natural building blocks of correlation\nfunctions. We construct (scalar) $n$-point functions and we illustrate the\nformalism with a detailed computation of 2-point functions. We compare our\nresults for thermal 2-point functions with results that follow from thermal\nOPEs and holographic computations, finding exact agreement. In our holographic\ncomputation we also obtain the OPE coefficient of the leading double-twist\ncontribution, and we discuss how the double-twist coefficients may be computed\nfrom the multi-energy-momentum contributions, given knowledge of the analytic\nstructure of the correlator. The 2-point function for the CFT on squashed\nspheres is a new result. We also discuss the relation of our work to flat\nholography.",
            "author": [
                "Enrico Parisini",
                "Kostas Skenderis",
                "Benjamin Withers"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03820v1",
                "http://arxiv.org/pdf/2312.03820v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "gr-qc",
                "math.DG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03823v1",
            "title": "High Pileup Particle Tracking with Object Condensation",
            "updated": "2023-12-06T19:00:00Z",
            "published": "2023-12-06T19:00:00Z",
            "summary": "Recent work has demonstrated that graph neural networks (GNNs) can match the\nperformance of traditional algorithms for charged particle tracking while\nimproving scalability to meet the computing challenges posed by the HL-LHC.\nMost GNN tracking algorithms are based on edge classification and identify\ntracks as connected components from an initial graph containing spurious\nconnections. In this talk, we consider an alternative based on object\ncondensation (OC), a multi-objective learning framework designed to cluster\npoints (hits) belonging to an arbitrary number of objects (tracks) and regress\nthe properties of each object. Building on our previous results, we present a\nstreamlined model and show progress toward a one-shot OC tracking algorithm in\na high-pileup environment.",
            "author": [
                "Kilian Lieret",
                "Gage DeZoort",
                "Devdoot Chatterjee",
                "Jian Park",
                "Siqi Miao",
                "Pan Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03823v1",
                "http://arxiv.org/pdf/2312.03823v1"
            ],
            "primary_category": "physics.data-an",
            "category": [
                "physics.data-an",
                "cs.LG",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03704v1",
            "title": "Relightable Gaussian Codec Avatars",
            "updated": "2023-12-06T18:59:58Z",
            "published": "2023-12-06T18:59:58Z",
            "summary": "The fidelity of relighting is bounded by both geometry and appearance\nrepresentations. For geometry, both mesh and volumetric approaches have\ndifficulty modeling intricate structures like 3D hair geometry. For appearance,\nexisting relighting models are limited in fidelity and often too slow to render\nin real-time with high-resolution continuous environments. In this work, we\npresent Relightable Gaussian Codec Avatars, a method to build high-fidelity\nrelightable head avatars that can be animated to generate novel expressions.\nOur geometry model based on 3D Gaussians can capture 3D-consistent\nsub-millimeter details such as hair strands and pores on dynamic face\nsequences. To support diverse materials of human heads such as the eyes, skin,\nand hair in a unified manner, we present a novel relightable appearance model\nbased on learnable radiance transfer. Together with global illumination-aware\nspherical harmonics for the diffuse components, we achieve real-time relighting\nwith spatially all-frequency reflections using spherical Gaussians. This\nappearance model can be efficiently relit under both point light and continuous\nillumination. We further improve the fidelity of eye reflections and enable\nexplicit gaze control by introducing relightable explicit eye models. Our\nmethod outperforms existing approaches without compromising real-time\nperformance. We also demonstrate real-time relighting of avatars on a tethered\nconsumer VR headset, showcasing the efficiency and fidelity of our avatars.",
            "author": [
                "Shunsuke Saito",
                "Gabriel Schwartz",
                "Tomas Simon",
                "Junxuan Li",
                "Giljoo Nam"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03704v1",
                "http://arxiv.org/pdf/2312.03704v1"
            ],
            "primary_category": "cs.GR",
            "category": [
                "cs.GR",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03703v1",
            "title": "Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context\n  Learning",
            "updated": "2023-12-06T18:59:44Z",
            "published": "2023-12-06T18:59:44Z",
            "summary": "In-context learning provides a new perspective for multi-task modeling for\nvision and NLP. Under this setting, the model can perceive tasks from prompts\nand accomplish them without any extra task-specific head predictions or model\nfine-tuning. However, Skeleton sequence modeling via in-context learning\nremains unexplored. Directly applying existing in-context models from other\nareas onto skeleton sequences fails due to the inter-frame and cross-task pose\nsimilarity that makes it outstandingly hard to perceive the task correctly from\na subtle context. To address this challenge, we propose Skeleton-in-Context\n(SiC), an effective framework for in-context skeleton sequence modeling. Our\nSiC is able to handle multiple skeleton-based tasks simultaneously after a\nsingle training process and accomplish each task from context according to the\ngiven prompt. It can further generalize to new, unseen tasks according to\ncustomized prompts. To facilitate context perception, we additionally propose a\ntask-unified prompt, which adaptively learns tasks of different natures, such\nas partial joint-level generation, sequence-level prediction, or 2D-to-3D\nmotion prediction. We conduct extensive experiments to evaluate the\neffectiveness of our SiC on multiple tasks, including motion prediction, pose\nestimation, joint completion, and future pose estimation. We also evaluate its\ngeneralization capability on unseen tasks such as motion-in-between. These\nexperiments show that our model achieves state-of-the-art multi-task\nperformance and even outperforms single-task methods on certain tasks.",
            "author": [
                "Xinshun Wang",
                "Zhongbin Fang",
                "Xia Li",
                "Xiangtai Li",
                "Chen Chen",
                "Mengyuan Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03703v1",
                "http://arxiv.org/pdf/2312.03703v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03701v1",
            "title": "Self-conditioned Image Generation via Generating Representations",
            "updated": "2023-12-06T18:59:31Z",
            "published": "2023-12-06T18:59:31Z",
            "summary": "This paper presents $\\textbf{R}$epresentation-$\\textbf{C}$onditioned image\n$\\textbf{G}$eneration (RCG), a simple yet effective image generation framework\nwhich sets a new benchmark in class-unconditional image generation. RCG does\nnot condition on any human annotations. Instead, it conditions on a\nself-supervised representation distribution which is mapped from the image\ndistribution using a pre-trained encoder. During generation, RCG samples from\nsuch representation distribution using a representation diffusion model (RDM),\nand employs a pixel generator to craft image pixels conditioned on the sampled\nrepresentation. Such a design provides substantial guidance during the\ngenerative process, resulting in high-quality image generation. Tested on\nImageNet 256$\\times$256, RCG achieves a Frechet Inception Distance (FID) of\n3.31 and an Inception Score (IS) of 253.4. These results not only significantly\nimprove the state-of-the-art of class-unconditional image generation but also\nrival the current leading methods in class-conditional image generation,\nbridging the long-standing performance gap between these two tasks. Code is\navailable at https://github.com/LTH14/rcg.",
            "author": [
                "Tianhong Li",
                "Dina Katabi",
                "Kaiming He"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03701v1",
                "http://arxiv.org/pdf/2312.03701v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03818v1",
            "title": "Alpha-CLIP: A CLIP Model Focusing on Wherever You Want",
            "updated": "2023-12-06T18:59:30Z",
            "published": "2023-12-06T18:59:30Z",
            "summary": "Contrastive Language-Image Pre-training (CLIP) plays an essential role in\nextracting valuable content information from images across diverse tasks. It\naligns textual and visual modalities to comprehend the entire image, including\nall the details, even those irrelevant to specific tasks. However, for a finer\nunderstanding and controlled editing of images, it becomes crucial to focus on\nspecific regions of interest, which can be indicated as points, masks, or boxes\nby humans or perception models. To fulfill the requirements, we introduce\nAlpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to\nsuggest attentive regions and fine-tuned with constructed millions of RGBA\nregion-text pairs. Alpha-CLIP not only preserves the visual recognition ability\nof CLIP but also enables precise control over the emphasis of image contents.\nIt demonstrates effectiveness in various tasks, including but not limited to\nopen-world recognition, multimodal large language models, and conditional 2D /\n3D generation. It has a strong potential to serve as a versatile tool for\nimage-related tasks.",
            "author": [
                "Zeyi Sun",
                "Ye Fang",
                "Tong Wu",
                "Pan Zhang",
                "Yuhang Zang",
                "Shu Kong",
                "Yuanjun Xiong",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03818v1",
                "http://arxiv.org/pdf/2312.03818v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03700v1",
            "title": "OneLLM: One Framework to Align All Modalities with Language",
            "updated": "2023-12-06T18:59:19Z",
            "published": "2023-12-06T18:59:19Z",
            "summary": "Multimodal large language models (MLLMs) have gained significant attention\ndue to their strong multimodal understanding capability. However, existing\nworks rely heavily on modality-specific encoders, which usually differ in\narchitecture and are limited to common modalities. In this paper, we present\nOneLLM, an MLLM that aligns eight modalities to language using a unified\nframework. We achieve this through a unified multimodal encoder and a\nprogressive multimodal alignment pipeline. In detail, we first train an image\nprojection module to connect a vision encoder with LLM. Then, we build a\nuniversal projection module (UPM) by mixing multiple image projection modules\nand dynamic routing. Finally, we progressively align more modalities to LLM\nwith the UPM. To fully leverage the potential of OneLLM in following\ninstructions, we also curated a comprehensive multimodal instruction dataset,\nincluding 2M items from image, audio, video, point cloud, depth/normal map, IMU\nand fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,\nencompassing tasks such as multimodal captioning, question answering and\nreasoning, where it delivers excellent performance. Code, data, model and\nonline demo are available at https://github.com/csuhan/OneLLM",
            "author": [
                "Jiaming Han",
                "Kaixiong Gong",
                "Yiyuan Zhang",
                "Jiaqi Wang",
                "Kaipeng Zhang",
                "Dahua Lin",
                "Yu Qiao",
                "Peng Gao",
                "Xiangyu Yue"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03700v1",
                "http://arxiv.org/pdf/2312.03700v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03817v1",
            "title": "Diffusion Illusions: Hiding Images in Plain Sight",
            "updated": "2023-12-06T18:59:18Z",
            "published": "2023-12-06T18:59:18Z",
            "summary": "We explore the problem of computationally generating special `prime' images\nthat produce optical illusions when physically arranged and viewed in a certain\nway. First, we propose a formal definition for this problem. Next, we introduce\nDiffusion Illusions, the first comprehensive pipeline designed to automatically\ngenerate a wide range of these illusions. Specifically, we both adapt the\nexisting `score distillation loss' and propose a new `dream target loss' to\noptimize a group of differentially parametrized prime images, using a frozen\ntext-to-image diffusion model. We study three types of illusions, each where\nthe prime images are arranged in different ways and optimized using the\naforementioned losses such that images derived from them align with user-chosen\ntext prompts or images. We conduct comprehensive experiments on these illusions\nand verify the effectiveness of our proposed method qualitatively and\nquantitatively. Additionally, we showcase the successful physical fabrication\nof our illusions -- as they are all designed to work in the real world. Our\ncode and examples are publicly available at our interactive project website:\nhttps://diffusionillusions.com",
            "author": [
                "Ryan Burgert",
                "Xiang Li",
                "Abe Leite",
                "Kanchana Ranasinghe",
                "Michael S. Ryoo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03817v1",
                "http://arxiv.org/pdf/2312.03817v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03699v2",
            "title": "PROMISE: A Framework for Model-Driven Stateful Prompt Orchestration",
            "updated": "2023-12-07T10:19:27Z",
            "published": "2023-12-06T18:59:11Z",
            "summary": "The advent of increasingly powerful language models has raised expectations\nfor language-based interactions. However, controlling these models is a\nchallenge, emphasizing the need to be able to investigate the feasibility and\nvalue of their application. We present PROMISE, a framework that facilitates\nthe development of complex language-based interactions with information\nsystems. Its use of state machine modeling concepts enables model-driven,\ndynamic prompt orchestration across hierarchically nested states and\ntransitions. This improves the control of the behavior of language models and\nthus enables their effective and efficient use. We show the benefits of PROMISE\nin the context of application scenarios within health information systems and\ndemonstrate its ability to handle complex interactions.",
            "author": [
                "Wenyuan Wu",
                "Jasmin Heierli",
                "Max Meisterhans",
                "Adrian Moser",
                "Andri F\u00e4rber",
                "Mateusz Dolata",
                "Elena Gavagnin",
                "Alexandre de Spindler",
                "Gerhard Schwabe"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03699v2",
                "http://arxiv.org/pdf/2312.03699v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03698v2",
            "title": "Intrinsic Harmonization for Illumination-Aware Compositing",
            "updated": "2023-12-07T02:19:27Z",
            "published": "2023-12-06T18:59:03Z",
            "summary": "Despite significant advancements in network-based image harmonization\ntechniques, there still exists a domain disparity between typical training\npairs and real-world composites encountered during inference. Most existing\nmethods are trained to reverse global edits made on segmented image regions,\nwhich fail to accurately capture the lighting inconsistencies between the\nforeground and background found in composited images. In this work, we\nintroduce a self-supervised illumination harmonization approach formulated in\nthe intrinsic image domain. First, we estimate a simple global lighting model\nfrom mid-level vision representations to generate a rough shading for the\nforeground region. A network then refines this inferred shading to generate a\nharmonious re-shading that aligns with the background scene. In order to match\nthe color appearance of the foreground and background, we utilize ideas from\nprior harmonization approaches to perform parameterized image edits in the\nalbedo domain. To validate the effectiveness of our approach, we present\nresults from challenging real-world composites and conduct a user study to\nobjectively measure the enhanced realism achieved compared to state-of-the-art\nharmonization methods.",
            "author": [
                "Chris Careaga",
                "S. Mahdi H. Miangoleh",
                "Ya\u011f\u0131z Aksoy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03698v2",
                "http://arxiv.org/pdf/2312.03698v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03696v1",
            "title": "Efficient Learning in Polyhedral Games via Best Response Oracles",
            "updated": "2023-12-06T18:57:53Z",
            "published": "2023-12-06T18:57:53Z",
            "summary": "We study online learning and equilibrium computation in games with polyhedral\ndecision sets, a property shared by both normal-form games and extensive-form\ngames (EFGs), when the learning agent is restricted to using a best-response\noracle. We show how to achieve constant regret in zero-sum games and\n$O(T^{1/4})$ regret in general-sum games while using only $O(\\log t)$\nbest-response queries at a given iteration $t$, thus improving over the best\nprior result, which required $O(T)$ queries per iteration. Moreover, our\nframework yields the first last-iterate convergence guarantees for self-play\nwith best-response oracles in zero-sum games. This convergence occurs at a\nlinear rate, though with a condition-number dependence. We go on to show a\n$O(1/\\sqrt{T})$ best-iterate convergence rate without such a dependence. Our\nresults build on linear-rate convergence results for variants of the\nFrank-Wolfe (FW) algorithm for strongly convex and smooth minimization problems\nover polyhedral domains. These FW results depend on a condition number of the\npolytope, known as facial distance. In order to enable application to settings\nsuch as EFGs, we show two broad new results: 1) the facial distance for\npolytopes in standard form is at least $\\gamma/\\sqrt{k}$ where $\\gamma$ is the\nminimum value of a nonzero coordinate of a vertex of the polytope and $k\\leq n$\nis the number of tight inequality constraints in the optimal face, and 2) the\nfacial distance for polytopes of the form\n$\\mathbf{A}\\boldsymbol{x}=\\boldsymbol{b},\\mathbf{C}\\boldsymbol{x}\\leq\\boldsymbol{d},\n\\boldsymbol{x}\\geq \\mathbf{0}$ where $\\boldsymbol{x}\\in\\mathbb{R}^n$,\n$\\mathbf{C}\\geq\\boldsymbol{0}$ is a nonzero integral matrix, and\n$\\boldsymbol{d}\\geq \\boldsymbol{0}$, is at least\n$1/(\\|\\mathbf{C}\\|_\\infty\\sqrt{n})$. This yields the first such results for\nseveral problems such as sequence-form polytopes, flow polytopes, and matching\npolytopes.",
            "author": [
                "Darshan Chakrabarti",
                "Gabriele Farina",
                "Christian Kroer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03696v1",
                "http://arxiv.org/pdf/2312.03696v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.MA",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03695v1",
            "title": "Revisiting the averaged annihilation rate of thermal relics at low\n  temperature",
            "updated": "2023-12-06T18:56:31Z",
            "published": "2023-12-06T18:56:31Z",
            "summary": "We derive a low-temperature expansion of the formula to compute the average\nannihilation rate $\\langle \\sigma v \\rangle$ for dark matter in\n$\\mathbb{Z}_2$-symmetric models, both in the absence and the presence of mass\ndegeneracy in the spectrum near the dark matter candidate. We show that the\nresult obtained in the absence of mass degeneracy is compatible with the\nanalytic formulae in the literature, and that it has a better numerical\nbehaviour for low temperatures. We also provide as ancillary files two Wolfram\nMathematica notebooks which perform the two expansions at any order.",
            "author": [
                "A. Arbey",
                "F. Mahmoudi",
                "M. Palmiotto"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03695v1",
                "http://arxiv.org/pdf/2312.03695v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03816v1",
            "title": "AVID: Any-Length Video Inpainting with Diffusion Model",
            "updated": "2023-12-06T18:56:14Z",
            "published": "2023-12-06T18:56:14Z",
            "summary": "Recent advances in diffusion models have successfully enabled text-guided\nimage inpainting. While it seems straightforward to extend such editing\ncapability into video domain, there has been fewer works regarding text-guided\nvideo inpainting. Given a video, a masked region at its initial frame, and an\nediting prompt, it requires a model to do infilling at each frame following the\nediting guidance while keeping the out-of-mask region intact. There are three\nmain challenges in text-guided video inpainting: ($i$) temporal consistency of\nthe edited video, ($ii$) supporting different inpainting types at different\nstructural fidelity level, and ($iii$) dealing with variable video length. To\naddress these challenges, we introduce Any-Length Video Inpainting with\nDiffusion Model, dubbed as AVID. At its core, our model is equipped with\neffective motion modules and adjustable structure guidance, for fixed-length\nvideo inpainting. Building on top of that, we propose a novel Temporal\nMultiDiffusion sampling pipeline with an middle-frame attention guidance\nmechanism, facilitating the generation of videos with any desired duration. Our\ncomprehensive experiments show our model can robustly deal with various\ninpainting types at different video duration range, with high quality. More\nvisualization results is made publicly available at\nhttps://zhang-zx.github.io/AVID/ .",
            "author": [
                "Zhixing Zhang",
                "Bichen Wu",
                "Xiaoyan Wang",
                "Yaqiao Luo",
                "Luxin Zhang",
                "Yinan Zhao",
                "Peter Vajda",
                "Dimitris Metaxas",
                "Licheng Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03816v1",
                "http://arxiv.org/pdf/2312.03816v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03694v2",
            "title": "Parameter-Efficient Transfer Learning of Audio Spectrogram Transformers",
            "updated": "2023-12-07T08:13:58Z",
            "published": "2023-12-06T18:55:34Z",
            "summary": "The common modus operandi of fine-tuning large pre-trained Transformer models\nentails the adaptation of all their parameters (i.e., full fine-tuning). While\nachieving striking results on multiple tasks, this approach becomes unfeasible\nas the model size and the number of downstream tasks increase. In natural\nlanguage processing and computer vision, parameter-efficient approaches like\nprompt-tuning and adapters have emerged as solid alternatives by fine-tuning\nonly a small number of extra parameters, without sacrificing performance\naccuracy. Specifically, adapters, due to their flexibility, have recently\ngarnered significant attention, leading to several variants. For audio\nclassification tasks, the Audio Spectrogram Transformer model shows impressive\nresults. However, surprisingly, how to efficiently adapt it to several\ndownstream tasks has not been tackled before. In this paper, we bridge this gap\nand present a detailed investigation of common parameter-efficient methods,\nrevealing that adapters consistently outperform the other methods across four\nbenchmarks. This trend is also confirmed in few-shot learning settings and when\nthe total number of trainable parameters increases, demonstrating adapters\nsuperior scalability. We finally study the best adapter configuration, as well\nas the role of residual connections in the learning process.",
            "author": [
                "Umberto Cappellazzo",
                "Daniele Falavigna",
                "Alessio Brutti",
                "Mirco Ravanelli"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03694v2",
                "http://arxiv.org/pdf/2312.03694v2"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03692v1",
            "title": "Memory Triggers: Unveiling Memorization in Text-To-Image Generative\n  Models through Word-Level Duplication",
            "updated": "2023-12-06T18:54:44Z",
            "published": "2023-12-06T18:54:44Z",
            "summary": "Diffusion-based models, such as the Stable Diffusion model, have\nrevolutionized text-to-image synthesis with their ability to produce\nhigh-quality, high-resolution images. These advancements have prompted\nsignificant progress in image generation and editing tasks. However, these\nmodels also raise concerns due to their tendency to memorize and potentially\nreplicate exact training samples, posing privacy risks and enabling adversarial\nattacks. Duplication in training datasets is recognized as a major factor\ncontributing to memorization, and various forms of memorization have been\nstudied so far. This paper focuses on two distinct and underexplored types of\nduplication that lead to replication during inference in diffusion-based\nmodels, particularly in the Stable Diffusion model. We delve into these\nlesser-studied duplication phenomena and their implications through two case\nstudies, aiming to contribute to the safer and more responsible use of\ngenerative models in various applications.",
            "author": [
                "Ali Naseh",
                "Jaechul Roh",
                "Amir Houmansadr"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03692v1",
                "http://arxiv.org/pdf/2312.03692v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03689v1",
            "title": "Evaluating and Mitigating Discrimination in Language Model Decisions",
            "updated": "2023-12-06T18:53:01Z",
            "published": "2023-12-06T18:53:01Z",
            "summary": "As language models (LMs) advance, interest is growing in applying them to\nhigh-stakes societal decisions, such as determining financing or housing\neligibility. However, their potential for discrimination in such contexts\nraises ethical concerns, motivating the need for better methods to evaluate\nthese risks. We present a method for proactively evaluating the potential\ndiscriminatory impact of LMs in a wide range of use cases, including\nhypothetical use cases where they have not yet been deployed. Specifically, we\nuse an LM to generate a wide array of potential prompts that decision-makers\nmay input into an LM, spanning 70 diverse decision scenarios across society,\nand systematically vary the demographic information in each prompt. Applying\nthis methodology reveals patterns of both positive and negative discrimination\nin the Claude 2.0 model in select settings when no interventions are applied.\nWhile we do not endorse or permit the use of language models to make automated\ndecisions for the high-risk use cases we study, we demonstrate techniques to\nsignificantly decrease both positive and negative discrimination through\ncareful prompt engineering, providing pathways toward safer deployment in use\ncases where they may be appropriate. Our work enables developers and\npolicymakers to anticipate, measure, and address discrimination as language\nmodel capabilities and applications continue to expand. We release our dataset\nand prompts at https://huggingface.co/datasets/Anthropic/discrim-eval",
            "author": [
                "Alex Tamkin",
                "Amanda Askell",
                "Liane Lovitt",
                "Esin Durmus",
                "Nicholas Joseph",
                "Shauna Kravec",
                "Karina Nguyen",
                "Jared Kaplan",
                "Deep Ganguli"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03689v1",
                "http://arxiv.org/pdf/2312.03689v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03686v1",
            "title": "Canonization of a random graph by two matrix-vector multiplications",
            "updated": "2023-12-06T18:52:14Z",
            "published": "2023-12-06T18:52:14Z",
            "summary": "We show that a canonical labeling of a random $n$-vertex graph can be\nobtained by assigning to each vertex $x$ the triple $(w_1(x),w_2(x),w_3(x))$,\nwhere $w_k(x)$ is the number of walks of length $k$ starting from $x$. This\ntakes time $O(n^2)$, where $n^2$ is the input size, by using just two\nmatrix-vector multiplications. The linear-time canonization of a random graph\nis the classical result of Babai, Erd\\H{o}s, and Selkow. For this purpose they\nuse the well-known combinatorial color refinement procedure, and we make a\ncomparative analysis of the two algorithmic approaches.",
            "author": [
                "Oleg Verbitsky",
                "Maksim Zhukovskii"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03686v1",
                "http://arxiv.org/pdf/2312.03686v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03815v1",
            "title": "LLM as OS (llmao), Agents as Apps: Envisioning AIOS, Agents and the\n  AIOS-Agent Ecosystem",
            "updated": "2023-12-06T18:50:26Z",
            "published": "2023-12-06T18:50:26Z",
            "summary": "This paper envisions a revolutionary AIOS-Agent ecosystem, where Large\nLanguage Model (LLM) serves as the (Artificial) Intelligent Operating System\n(IOS, or AIOS)--an operating system ``with soul''. Upon this foundation, a\ndiverse range of LLM-based AI Agent Applications (Agents, or AAPs) are\ndeveloped, enriching the AIOS-Agent ecosystem and signaling a paradigm shift\nfrom the traditional OS-APP ecosystem. We envision that LLM's impact will not\nbe limited to the AI application level, instead, it will in turn revolutionize\nthe design and implementation of computer system, architecture, software, and\nprogramming language, featured by several main concepts: LLM as OS\n(system-level), Agents as Applications (application-level), Natural Language as\nProgramming Interface (user-level), and Tools as Devices/Libraries\n(hardware/middleware-level).",
            "author": [
                "Yingqiang Ge",
                "Yujie Ren",
                "Wenyue Hua",
                "Shuyuan Xu",
                "Juntao Tan",
                "Yongfeng Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03815v1",
                "http://arxiv.org/pdf/2312.03815v1"
            ],
            "primary_category": "cs.OS",
            "category": [
                "cs.OS",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03682v1",
            "title": "What Planning Problems Can A Relational Neural Network Solve?",
            "updated": "2023-12-06T18:47:28Z",
            "published": "2023-12-06T18:47:28Z",
            "summary": "Goal-conditioned policies are generally understood to be \"feed-forward\"\ncircuits, in the form of neural networks that map from the current state and\nthe goal specification to the next action to take. However, under what\ncircumstances such a policy can be learned and how efficient the policy will be\nare not well understood. In this paper, we present a circuit complexity\nanalysis for relational neural networks (such as graph neural networks and\ntransformers) representing policies for planning problems, by drawing\nconnections with serialized goal regression search (S-GRS). We show that there\nare three general classes of planning problems, in terms of the growth of\ncircuit width and depth as a function of the number of objects and planning\nhorizon, providing constructive proofs. We also illustrate the utility of this\nanalysis for designing neural networks for policy learning.",
            "author": [
                "Jiayuan Mao",
                "Tom\u00e1s Lozano-P\u00e9rez",
                "Joshua B. Tenenbaum",
                "Leslie Pack Kaelbling"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03682v1",
                "http://arxiv.org/pdf/2312.03682v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NE",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03678v1",
            "title": "Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching",
            "updated": "2023-12-06T18:41:01Z",
            "published": "2023-12-06T18:41:01Z",
            "summary": "Non-isometric shape correspondence remains a fundamental challenge in\ncomputer vision. Traditional methods using Laplace-Beltrami operator (LBO)\neigenmodes face limitations in characterizing high-frequency extrinsic shape\nchanges like bending and creases. We propose a novel approach of combining the\nnon-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell\nhessian with the intrinsic ones of the LBO, creating a hybrid spectral space in\nwhich we construct functional maps. To this end, we present a theoretical\nframework to effectively integrate non-orthogonal basis functions into\ndescriptor- and learning-based functional map methods. Our approach can be\nincorporated easily into existing functional map pipelines across varying\napplications and is able to handle complex deformations beyond isometries. We\nshow extensive evaluations across various supervised and unsupervised settings\nand demonstrate significant improvements. Notably, our approach achieves up to\n15% better mean geodesic error for non-isometric correspondence settings and up\nto 45% improvement in scenarios with topological noise.",
            "author": [
                "Lennart Bastian",
                "Yizheng Xie",
                "Nassir Navab",
                "Zorah L\u00e4hner"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03678v1",
                "http://arxiv.org/pdf/2312.03678v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03674v1",
            "title": "Building a Digital Twin for British Cities",
            "updated": "2023-12-06T18:38:10Z",
            "published": "2023-12-06T18:38:10Z",
            "summary": "Ever faster computers are enabling us to extend our standard land use\ntransportation interaction (LUTI) models to systems of cities within which\nindividual cities compete for resources within the wider environment in which\nthey interact.As we scale up in this way, we are able to simulate and measure\nthe impacts of large-scale infrastructures at different spatial levels.Here we\nbuild a platform, which is essentially a digital twin, for over 8000 urban\nplaces in Great Britain where we can rapidly model all flows between these\nlocations using multi-modal spatial interaction models.We first present the\nstructure of the model and then apply it to population, employment and trip\nflow data for three modes of travel (road, bus and rail) between small spatial\nunits defining the three countries, England, Scotland and Wales.We then tune\nand train the model to reproduce a baseline, and follow this with a\ndemonstration of the web-based interface used to run and interact with the\nmodel and its predictions.Once we have developed the platform, we are able to\nexplore variants of the twin, partitioning the country in different ways,\nshowing how different forms of spatial representation change the performance of\nthe model.We are developing the model at a much finer scale making comparisons\nof performance while adding an active travel layer that elaborates the twin.We\nfinally illustrate how the model can be used to measure the impacts of new\nscenarios for rail, simulating the Integrated Rail Plan and the High Speed 2\nproposal",
            "author": [
                "Michael Batty",
                "Richard Milton"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03674v1",
                "http://arxiv.org/pdf/2312.03674v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "91: Game theory, economics, social and behavioral sciences",
                "I.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03668v1",
            "title": "An Integration of Pre-Trained Speech and Language Models for End-to-End\n  Speech Recognition",
            "updated": "2023-12-06T18:34:42Z",
            "published": "2023-12-06T18:34:42Z",
            "summary": "Advances in machine learning have made it possible to perform various text\nand speech processing tasks, including automatic speech recognition (ASR), in\nan end-to-end (E2E) manner. Since typical E2E approaches require large amounts\nof training data and resources, leveraging pre-trained foundation models\ninstead of training from scratch is gaining attention. Although there have been\nattempts to use pre-trained speech and language models in ASR, most of them are\nlimited to using either. This paper explores the potential of integrating a\npre-trained speech representation model with a large language model (LLM) for\nE2E ASR. The proposed model enables E2E ASR by generating text tokens in an\nautoregressive manner via speech representations as speech prompts, taking\nadvantage of the vast knowledge provided by the LLM. Furthermore, the proposed\nmodel can incorporate remarkable developments for LLM utilization, such as\ninference optimization and parameter-efficient domain adaptation. Experimental\nresults show that the proposed model achieves performance comparable to modern\nE2E ASR models.",
            "author": [
                "Yukiya Hono",
                "Koh Mitsuda",
                "Tianyu Zhao",
                "Kentaro Mitsui",
                "Toshiaki Wakatsuki",
                "Kei Sawada"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03668v1",
                "http://arxiv.org/pdf/2312.03668v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03667v1",
            "title": "WarpDiffusion: Efficient Diffusion Model for High-Fidelity Virtual\n  Try-on",
            "updated": "2023-12-06T18:34:32Z",
            "published": "2023-12-06T18:34:32Z",
            "summary": "Image-based Virtual Try-On (VITON) aims to transfer an in-shop garment image\nonto a target person. While existing methods focus on warping the garment to\nfit the body pose, they often overlook the synthesis quality around the\ngarment-skin boundary and realistic effects like wrinkles and shadows on the\nwarped garments. These limitations greatly reduce the realism of the generated\nresults and hinder the practical application of VITON techniques. Leveraging\nthe notable success of diffusion-based models in cross-modal image synthesis,\nsome recent diffusion-based methods have ventured to tackle this issue.\nHowever, they tend to either consume a significant amount of training resources\nor struggle to achieve realistic try-on effects and retain garment details. For\nefficient and high-fidelity VITON, we propose WarpDiffusion, which bridges the\nwarping-based and diffusion-based paradigms via a novel informative and local\ngarment feature attention mechanism. Specifically, WarpDiffusion incorporates\nlocal texture attention to reduce resource consumption and uses a novel\nauto-mask module that effectively retains only the critical areas of the warped\ngarment while disregarding unrealistic or erroneous portions. Notably,\nWarpDiffusion can be integrated as a plug-and-play component into existing\nVITON methodologies, elevating their synthesis quality. Extensive experiments\non high-resolution VITON benchmarks and an in-the-wild test set demonstrate the\nsuperiority of WarpDiffusion, surpassing state-of-the-art methods both\nqualitatively and quantitatively.",
            "author": [
                "xujie zhang",
                "Xiu Li",
                "Michael Kampffmeyer",
                "Xin Dong",
                "Zhenyu Xie",
                "Feida Zhu",
                "Haoye Dong",
                "Xiaodan Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03667v1",
                "http://arxiv.org/pdf/2312.03667v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03664v1",
            "title": "Generative agent-based modeling with actions grounded in physical,\n  social, or digital space using Concordia",
            "updated": "2023-12-06T18:33:50Z",
            "published": "2023-12-06T18:33:50Z",
            "summary": "Agent-based modeling has been around for decades, and applied widely across\nthe social and natural sciences. The scope of this research method is now\npoised to grow dramatically as it absorbs the new affordances provided by Large\nLanguage Models (LLM)s. Generative Agent-Based Models (GABM) are not just\nclassic Agent-Based Models (ABM)s where the agents talk to one another. Rather,\nGABMs are constructed using an LLM to apply common sense to situations, act\n\"reasonably\", recall common semantic knowledge, produce API calls to control\ndigital technologies like apps, and communicate both within the simulation and\nto researchers viewing it from the outside. Here we present Concordia, a\nlibrary to facilitate constructing and working with GABMs. Concordia makes it\neasy to construct language-mediated simulations of physically- or\ndigitally-grounded environments. Concordia agents produce their behavior using\na flexible component system which mediates between two fundamental operations:\nLLM calls and associative memory retrieval. A special agent called the Game\nMaster (GM), which was inspired by tabletop role-playing games, is responsible\nfor simulating the environment where the agents interact. Agents take actions\nby describing what they want to do in natural language. The GM then translates\ntheir actions into appropriate implementations. In a simulated physical world,\nthe GM checks the physical plausibility of agent actions and describes their\neffects. In digital environments simulating technologies such as apps and\nservices, the GM may handle API calls to integrate with external tools such as\ngeneral AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar,\nEmail, Search, etc.). Concordia was designed to support a wide array of\napplications both in scientific research and for evaluating performance of real\ndigital services by simulating users and/or generating synthetic data.",
            "author": [
                "Alexander Sasha Vezhnevets",
                "John P. Agapiou",
                "Avia Aharon",
                "Ron Ziv",
                "Jayd Matyas",
                "Edgar A. Du\u00e9\u00f1ez-Guzm\u00e1n",
                "William A. Cunningham",
                "Simon Osindero",
                "Danny Karmon",
                "Joel Z. Leibo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03664v1",
                "http://arxiv.org/pdf/2312.03664v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03661v1",
            "title": "Reason2Drive: Towards Interpretable and Chain-based Reasoning for\n  Autonomous Driving",
            "updated": "2023-12-06T18:32:33Z",
            "published": "2023-12-06T18:32:33Z",
            "summary": "Large vision-language models (VLMs) have garnered increasing interest in\nautonomous driving areas, due to their advanced capabilities in complex\nreasoning tasks essential for highly autonomous vehicle behavior. Despite their\npotential, research in autonomous systems is hindered by the lack of datasets\nwith annotated reasoning chains that explain the decision-making processes in\ndriving. To bridge this gap, we present Reason2Drive, a benchmark dataset with\nover 600K video-text pairs, aimed at facilitating the study of interpretable\nreasoning in complex driving environments. We distinctly characterize the\nautonomous driving process as a sequential combination of perception,\nprediction, and reasoning steps, and the question-answer pairs are\nautomatically collected from a diverse range of open-source outdoor driving\ndatasets, including nuScenes, Waymo and ONCE. Moreover, we introduce a novel\naggregated evaluation metric to assess chain-based reasoning performance in\nautonomous systems, addressing the semantic ambiguities of existing metrics\nsuch as BLEU and CIDEr. Based on the proposed benchmark, we conduct experiments\nto assess various existing VLMs, revealing insights into their reasoning\ncapabilities. Additionally, we develop an efficient approach to empower VLMs to\nleverage object-level perceptual elements in both feature extraction and\nprediction, further enhancing their reasoning accuracy. The code and dataset\nwill be released.",
            "author": [
                "Ming Nie",
                "Renyuan Peng",
                "Chunwei Wang",
                "Xinyue Cai",
                "Jianhua Han",
                "Hang Xu",
                "Li Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03661v1",
                "http://arxiv.org/pdf/2312.03661v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03657v1",
            "title": "Multisymplecticity in finite element exterior calculus",
            "updated": "2023-12-06T18:27:21Z",
            "published": "2023-12-06T18:27:21Z",
            "summary": "We consider the application of finite element exterior calculus (FEEC)\nmethods to a class of canonical Hamiltonian PDE systems involving differential\nforms. Solutions to these systems satisfy a local multisymplectic conservation\nlaw, which generalizes the more familiar symplectic conservation law for\nHamiltonian systems of ODEs, and which is connected with physically-important\nreciprocity phenomena, such as Lorentz reciprocity in electromagnetics. We\ncharacterize hybrid FEEC methods whose numerical traces satisfy a version of\nthe multisymplectic conservation law, and we apply this characterization to\nseveral specific classes of FEEC methods, including conforming\nArnold-Falk-Winther-type methods and various hybridizable discontinuous\nGalerkin (HDG) methods. Interestingly, the HDG-type and other nonconforming\nmethods are shown, in general, to be multisymplectic in a stronger sense than\nthe conforming FEEC methods. This substantially generalizes previous work of\nMcLachlan and Stern [Found. Comput. Math., 20 (2020), pp. 35-69] on the more\nrestricted class of canonical Hamiltonian PDEs in the de Donder-Weyl \"grad-div\"\nform.",
            "author": [
                "Ari Stern",
                "Enrico Zampa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03657v1",
                "http://arxiv.org/pdf/2312.03657v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "65N30 (Primary) 37K06 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03813v1",
            "title": "Improving Activation Steering in Language Models with Mean-Centring",
            "updated": "2023-12-06T18:27:07Z",
            "published": "2023-12-06T18:27:07Z",
            "summary": "Recent work in activation steering has demonstrated the potential to better\ncontrol the outputs of Large Language Models (LLMs), but it involves finding\nsteering vectors. This is difficult because engineers do not typically know how\nfeatures are represented in these models. We seek to address this issue by\napplying the idea of mean-centring to steering vectors. We find that taking the\naverage of activations associated with a target dataset, and then subtracting\nthe mean of all training activations, results in effective steering vectors. We\ntest this method on a variety of models on natural language tasks by steering\naway from generating toxic text, and steering the completion of a story towards\na target genre. We also apply mean-centring to extract function vectors, more\neffectively triggering the execution of a range of natural language tasks by a\nsignificant margin (compared to previous baselines). This suggests that\nmean-centring can be used to easily improve the effectiveness of activation\nsteering in a wide range of contexts.",
            "author": [
                "Ole Jorgensen",
                "Dylan Cope",
                "Nandi Schoots",
                "Murray Shanahan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03813v1",
                "http://arxiv.org/pdf/2312.03813v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03656v1",
            "title": "Interpretability Illusions in the Generalization of Simplified Models",
            "updated": "2023-12-06T18:25:53Z",
            "published": "2023-12-06T18:25:53Z",
            "summary": "A common method to study deep learning systems is to use simplified model\nrepresentations -- for example, using singular value decomposition to visualize\nthe model's hidden states in a lower dimensional space. This approach assumes\nthat the results of these simplified are faithful to the original model. Here,\nwe illustrate an important caveat to this assumption: even if the simplified\nrepresentations can accurately approximate the full model on the training set,\nthey may fail to accurately capture the model's behavior out of distribution --\nthe understanding developed from simplified representations may be an illusion.\nWe illustrate this by training Transformer models on controlled datasets with\nsystematic generalization splits. First, we train models on the Dyck\nbalanced-parenthesis languages. We simplify these models using tools like\ndimensionality reduction and clustering, and then explicitly test how these\nsimplified proxies match the behavior of the original model on various\nout-of-distribution test sets. We find that the simplified proxies are\ngenerally less faithful out of distribution. In cases where the original model\ngeneralizes to novel structures or deeper depths, the simplified versions may\nfail, or generalize better. This finding holds even if the simplified\nrepresentations do not directly depend on the training distribution. Next, we\nstudy a more naturalistic task: predicting the next character in a dataset of\ncomputer code. We find similar generalization gaps between the original model\nand simplified proxies, and conduct further analysis to investigate which\naspects of the code completion task are associated with the largest gaps.\nTogether, our results raise questions about the extent to which mechanistic\ninterpretations derived using tools like SVD can reliably predict what a model\nwill do in novel situations.",
            "author": [
                "Dan Friedman",
                "Andrew Lampinen",
                "Lucas Dixon",
                "Danqi Chen",
                "Asma Ghandeharioun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03656v1",
                "http://arxiv.org/pdf/2312.03656v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03812v1",
            "title": "Seeing the random forest through the decision trees. Supporting learning\n  health systems from histopathology with machine learning models: Challenges\n  and opportunities",
            "updated": "2023-12-06T18:24:21Z",
            "published": "2023-12-06T18:24:21Z",
            "summary": "This paper discusses some overlooked challenges faced when working with\nmachine learning models for histopathology and presents a novel opportunity to\nsupport \"Learning Health Systems\" with them. Initially, the authors elaborate\non these challenges after separating them according to their mitigation\nstrategies: those that need innovative approaches, time, or future\ntechnological capabilities and those that require a conceptual reappraisal from\na critical perspective. Then, a novel opportunity to support \"Learning Health\nSystems\" by integrating hidden information extracted by ML models from\ndigitalized histopathology slides with other healthcare big data is presented.",
            "author": [
                "Ricardo Gonzalez",
                "Ashirbani Saha",
                "Clinton J. V. Campbell",
                "Peyman Nejat",
                "Cynthia Lokker",
                "Andrew P. Norgan"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.jpi.2023.100347",
                "http://arxiv.org/abs/2312.03812v1",
                "http://arxiv.org/pdf/2312.03812v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03654v1",
            "title": "Efficient Inverse Design Optimization through Multi-fidelity\n  Simulations, Machine Learning, and Search Space Reduction Strategies",
            "updated": "2023-12-06T18:20:46Z",
            "published": "2023-12-06T18:20:46Z",
            "summary": "This paper introduces a methodology designed to augment the inverse design\noptimization process in scenarios constrained by limited compute, through the\nstrategic synergy of multi-fidelity evaluations, machine learning models, and\noptimization algorithms. The proposed methodology is analyzed on two distinct\nengineering inverse design problems: airfoil inverse design and the scalar\nfield reconstruction problem. It leverages a machine learning model trained\nwith low-fidelity simulation data, in each optimization cycle, thereby\nproficiently predicting a target variable and discerning whether a\nhigh-fidelity simulation is necessitated, which notably conserves computational\nresources. Additionally, the machine learning model is strategically deployed\nprior to optimization to reduce the search space, thereby further accelerating\nconvergence toward the optimal solution. The methodology has been employed to\nenhance two optimization algorithms, namely Differential Evolution and Particle\nSwarm Optimization. Comparative analyses illustrate performance improvements\nacross both algorithms. Notably, this method is adeptly adaptable across any\ninverse design application, facilitating a harmonious synergy between a\nrepresentative low-fidelity machine learning model, and high-fidelity\nsimulation, and can be seamlessly applied across any variety of\npopulation-based optimization algorithms.",
            "author": [
                "Luka Grbcic",
                "Juliane M\u00fcller",
                "Wibe Albert de Jong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03654v1",
                "http://arxiv.org/pdf/2312.03654v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE",
                "cs.AI",
                "cs.LG",
                "cs.NE",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03653v1",
            "title": "Quantum Picturalism: Learning Quantum Theory in High School",
            "updated": "2023-12-06T18:16:12Z",
            "published": "2023-12-06T18:16:12Z",
            "summary": "Quantum theory is often regarded as challenging to learn and teach, with\nadvanced mathematical prerequisites ranging from complex numbers and\nprobability theory to matrix multiplication, vector space algebra and symbolic\nmanipulation within the Hilbert space formalism. It is traditionally considered\nan advanced undergraduate or graduate-level subject.\n  In this work, we challenge the conventional view by proposing \"Quantum\nPicturalism\" as a new approach to teaching the fundamental concepts of quantum\ntheory and computation. We establish the foundations and methodology for an\nongoing educational experiment to investigate the question \"From what age can\nstudents learn quantum theory if taught using a diagrammatic approach?\". We\nanticipate that the primary benefit of leveraging such a diagrammatic approach,\nwhich is conceptually intuitive yet mathematically rigorous, will be\neliminating some of the most daunting barriers to teaching and learning this\nsubject while enabling young learners to reason proficiently about high-level\nproblems. We posit that transitioning from symbolic presentations to pictorial\nones will increase the appeal of STEM education, attracting more diverse\naudience.",
            "author": [
                "Selma D\u00fcndar-Coecke",
                "Lia Yeh",
                "Caterina Puca",
                "Sieglinde M. -L. Pfaendler",
                "Muhammad Hamza Waseem",
                "Thomas Cervoni",
                "Aleks Kissinger",
                "Stefano Gogioso",
                "Bob Coecke"
            ],
            "link": [
                "http://dx.doi.org/10.1109/QCE57702.2023.20321",
                "http://arxiv.org/abs/2312.03653v1",
                "http://arxiv.org/pdf/2312.03653v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03651v2",
            "title": "MIRACLE: Inverse Reinforcement and Curriculum Learning Model for\n  Human-inspired Mobile Robot Navigation",
            "updated": "2023-12-07T02:26:52Z",
            "published": "2023-12-06T18:13:21Z",
            "summary": "In emergency scenarios, mobile robots must navigate like humans, interpreting\nstimuli to locate potential victims rapidly without interfering with first\nresponders. Existing socially-aware navigation algorithms face computational\nand adaptability challenges. To overcome these, we propose a solution, MIRACLE\n-- an inverse reinforcement and curriculum learning model, that employs\ngamified learning to gather stimuli-driven human navigational data. This data\nis then used to train a Deep Inverse Maximum Entropy Reinforcement Learning\nmodel, reducing reliance on demonstrator abilities. Testing reveals a low loss\nof 2.7717 within a 400-sized environment, signifying human-like response\nreplication. Current databases lack comprehensive stimuli-driven data,\nnecessitating our approach. By doing so, we enable robots to navigate emergency\nsituations with human-like perception, enhancing their life-saving\ncapabilities.",
            "author": [
                "Nihal Gunukula",
                "Kshitij Tiwari",
                "Aniket Bera"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03651v2",
                "http://arxiv.org/pdf/2312.03651v2"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03649v1",
            "title": "Quantum Optics with Rydberg Superatoms",
            "updated": "2023-12-06T18:11:04Z",
            "published": "2023-12-06T18:11:04Z",
            "summary": "Quantum optics based on highly excited atoms, also known as Rydberg atoms,\nhas cemented itself as a powerful platform for the manipulation of light at the\nfew-photon level. The Rydberg blockade, resulting from the strong interaction\nbetween individual Rydberg atoms, can turn a large ensemble of atoms into a\nsystem which collectively resembles a single two-level emitter, a so-called\nRydberg superatom. The coupling of this artificial emitter to a driving\nphotonic mode is collectively enhanced by Rydberg interactions, enabling strong\ncoherent coupling at the few-photon level in free-space. The exquisite level of\ncontrol achievable through this has already demonstrated its utility in\napplications of quantum computing and information processing. Here, we review\nthe derivation of the collective coupling between a Rydberg superatom and a\nsingle light mode and discuss the similarity of this free-space setup to\nwaveguide quantum electrodynamics systems of quantum emitters coupled to\nphotonic waveguides. We also briefly review applications of Rydberg superatoms\nto quantum optics such as single-photon generation and single-photon\nsubtraction.",
            "author": [
                "Jan Kumlin",
                "Christoph Braun",
                "Christoph Tresp",
                "Nina Stiesdal",
                "Sebastian Hofferberth",
                "Asaf Paris-Mandoki"
            ],
            "link": [
                "http://dx.doi.org/10.1088/2399-6528/acd51d",
                "http://arxiv.org/abs/2312.03649v1",
                "http://arxiv.org/pdf/2312.03649v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03647v1",
            "title": "Editable Stain Transformation Of Histological Images Using Unpaired GANs",
            "updated": "2023-12-06T18:05:41Z",
            "published": "2023-12-06T18:05:41Z",
            "summary": "Double staining in histopathology, particularly for metaplastic breast\ncancer, typically employs H&E and P63 dyes. However, P63's tissue damage and\nhigh cost necessitate alternative methods. This study introduces xAI-CycleGAN,\nan advanced architecture combining Mask CycleGAN with explainability features\nand structure-preserving capabilities for transforming H&E stained breast\ntissue images into P63-like images. The architecture allows for output editing,\nenhancing resemblance to actual images and enabling further model refinement.\nWe showcase xAI-CycleGAN's efficacy in maintaining structural integrity and\ngenerating high-quality images. Additionally, a histopathologist survey\nindicates the generated images' realism is often comparable to actual images,\nvalidating our model's high-quality output.",
            "author": [
                "Tibor Sloboda",
                "Luk\u00e1\u0161 Hudec",
                "Wanda Bene\u0161ov\u00e1"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03647v1",
                "http://arxiv.org/pdf/2312.03647v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03646v1",
            "title": "An Irredundant Decomposition of Data Flow with Affine Dependences",
            "updated": "2023-12-06T18:03:04Z",
            "published": "2023-12-06T18:03:04Z",
            "summary": "Optimization pipelines targeting polyhedral programs try to maximize the\ncompute throughput. Traditional approaches favor reuse and temporal locality;\nwhile the communicated volume can be low, failure to optimize spatial locality\nmay cause a low I/O performance.\n  Memory allocation schemes using data partitioning such as data tiling can\nimprove the spatial locality, but they are domain-specific and rarely applied\nby compilers when an existing allocation is supplied.\n  In this paper, we propose to derive a partitioned memory allocation for tiled\npolyhedral programs using their data flow information. We extend the existing\nMARS partitioning to handle affine dependences, and determine which dependences\ncan lead to a regular, simple control flow for communications.\n  While this paper consists in a theoretical study, previous work on data\npartitioning in inter-node scenarios has shown performance improvements due to\nbetter bandwidth utilization.",
            "author": [
                "Corentin Ferry",
                "Steven Derrien",
                "Sanjay Rajopadhye"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03646v1",
                "http://arxiv.org/pdf/2312.03646v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03809v1",
            "title": "Nonequilibrium diagrammatic many-body simulations with quantics tensor\n  trains",
            "updated": "2023-12-06T17:57:45Z",
            "published": "2023-12-06T17:57:45Z",
            "summary": "The nonequilibrium Green's function formalism provides a versatile and\npowerful framework for numerical studies of nonequilibrium phenomena in\ncorrelated many-body systems. For calculations starting from an equilibrium\ninitial state, a standard approach consists of discretizing the Kadanoff-Baym\ncontour and implementing a causal time-stepping scheme in which the self-energy\nof the system plays the role of a memory kernel. This approach becomes\ncomputationally expensive at long times, because of the convolution integrals\nand the large amount of computer memory needed to store the Green's functions.\nA recent idea for the compression of nonequilibrium Green's functions is the\nquantics tensor train representation. Here, we explore this approach by\nimplementing equilibrium and nonequilibrium simulations of the two-dimensional\nHubbard model with a second-order weak-coupling approximation to the\nself-energy. We show that calculations with compressed two-time functions are\npossible without any loss of accuracy, and that the quantics tensor train\nimplementation shows a much improved scaling of the computational effort and\nmemory demand with the length of the time contour.",
            "author": [
                "Matthias Murray",
                "Hiroshi Shinaoka",
                "Philipp Werner"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03809v1",
                "http://arxiv.org/pdf/2312.03809v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03641v1",
            "title": "MotionCtrl: A Unified and Flexible Motion Controller for Video\n  Generation",
            "updated": "2023-12-06T17:49:57Z",
            "published": "2023-12-06T17:49:57Z",
            "summary": "Motions in a video primarily consist of camera motion, induced by camera\nmovement, and object motion, resulting from object movement. Accurate control\nof both camera and object motion is essential for video generation. However,\nexisting works either mainly focus on one type of motion or do not clearly\ndistinguish between the two, limiting their control capabilities and diversity.\nTherefore, this paper presents MotionCtrl, a unified and flexible motion\ncontroller for video generation designed to effectively and independently\ncontrol camera and object motion. The architecture and training strategy of\nMotionCtrl are carefully devised, taking into account the inherent properties\nof camera motion, object motion, and imperfect training data. Compared to\nprevious methods, MotionCtrl offers three main advantages: 1) It effectively\nand independently controls camera motion and object motion, enabling more\nfine-grained motion control and facilitating flexible and diverse combinations\nof both types of motion. 2) Its motion conditions are determined by camera\nposes and trajectories, which are appearance-free and minimally impact the\nappearance or shape of objects in generated videos. 3) It is a relatively\ngeneralizable model that can adapt to a wide array of camera poses and\ntrajectories once trained. Extensive qualitative and quantitative experiments\nhave been conducted to demonstrate the superiority of MotionCtrl over existing\nmethods.",
            "author": [
                "Zhouxia Wang",
                "Ziyang Yuan",
                "Xintao Wang",
                "Tianshui Chen",
                "Menghan Xia",
                "Ping Luo",
                "Ying Shan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03641v1",
                "http://arxiv.org/pdf/2312.03641v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03640v1",
            "title": "Training Neural Networks on RAW and HDR Images for Restoration Tasks",
            "updated": "2023-12-06T17:47:16Z",
            "published": "2023-12-06T17:47:16Z",
            "summary": "The vast majority of standard image and video content available online is\nrepresented in display-encoded color spaces, in which pixel values are\nconveniently scaled to a limited range (0-1) and the color distribution is\napproximately perceptually uniform. In contrast, both camera RAW and high\ndynamic range (HDR) images are often represented in linear color spaces, in\nwhich color values are linearly related to colorimetric quantities of light.\nWhile training on commonly available display-encoded images is a\nwell-established practice, there is no consensus on how neural networks should\nbe trained for tasks on RAW and HDR images in linear color spaces. In this\nwork, we test several approaches on three popular image restoration\napplications: denoising, deblurring, and single-image super-resolution. We\nexamine whether HDR/RAW images need to be display-encoded using popular\ntransfer functions (PQ, PU21, mu-law), or whether it is better to train in\nlinear color spaces, but use loss functions that correct for perceptual\nnon-uniformity. Our results indicate that neural networks train significantly\nbetter on HDR and RAW images represented in display-encoded color spaces, which\noffer better perceptual uniformity than linear spaces. This small change to the\ntraining strategy can bring a very substantial gain in performance, up to 10-15\ndB.",
            "author": [
                "Lei Luo",
                "Alexandre Chapiro",
                "Xiaoyu Xiang",
                "Yuchen Fan",
                "Rakesh Ranjan",
                "Rafal Mantiuk"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03640v1",
                "http://arxiv.org/pdf/2312.03640v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03636v1",
            "title": "Fed-urlBERT: Client-side Lightweight Federated Transformers for URL\n  Threat Analysis",
            "updated": "2023-12-06T17:31:16Z",
            "published": "2023-12-06T17:31:16Z",
            "summary": "In evolving cyber landscapes, the detection of malicious URLs calls for\ncooperation and knowledge sharing across domains. However, collaboration is\noften hindered by concerns over privacy and business sensitivities. Federated\nlearning addresses these issues by enabling multi-clients collaboration without\ndirect data exchange. Unfortunately, if highly expressive Transformer models\nare used, clients may face intolerable computational burdens, and the exchange\nof weights could quickly deplete network bandwidth. In this paper, we propose\nFed-urlBERT, a federated URL pre-trained model designed to address both privacy\nconcerns and the need for cross-domain collaboration in cybersecurity.\nFed-urlBERT leverages split learning to divide the pre-training model into\nclient and server part, so that the client part takes up less extensive\ncomputation resources and bandwidth. Our appraoch achieves performance\ncomparable to centralized model under both independently and identically\ndistributed (IID) and two non-IID data scenarios. Significantly, our federated\nmodel shows about an 7% decrease in the FPR compared to the centralized model.\nAdditionally, we implement an adaptive local aggregation strategy that\nmitigates heterogeneity among clients, demonstrating promising performance\nimprovements. Overall, our study validates the applicability of the proposed\nTransformer federated learning for URL threat analysis, establishing a\nfoundation for real-world collaborative cybersecurity efforts. The source code\nis accessible at https://github.com/Davidup1/FedURLBERT.",
            "author": [
                "Yujie Li",
                "Yanbin Wang",
                "Haitao Xu",
                "Zhenhao Guo",
                "Fan Zhang",
                "Ruitong Liu",
                "Wenrui Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03636v1",
                "http://arxiv.org/pdf/2312.03636v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03635v1",
            "title": "Towards Time Sensitive Networking on Smart Cities: Techniques,\n  Challenges, and Solutions",
            "updated": "2023-12-06T17:31:03Z",
            "published": "2023-12-06T17:31:03Z",
            "summary": "The rapid proliferation of smart cities has transformed urban landscapes into\ndynamic ecosystems teeming with interconnected computational nodes and sensors.\nDuring this evolution, the search for seamless communication in time-critical\nscenarios has become evident. With the escalating complexity of urban\nenvironments, envisioning a future with a blend of autonomous and conventional\nsystems, each demanding distinct quality-of-service considerations, services in\nsmart cities vary criticality levels and necessitate differentiated traffic\nhandling, prioritizing critical flows without compromising the network's\nreliability or failing on hard real-time requirements.\n  To tackle these challenges, in this article we propose a Time-Sensitive\nNetworking (TSN) approach which, at the scale of a smart city network, presents\nmultifaceted challenges, notably interoperability among diverse technologies\nand standards. Nonetheless, TSN emerges as a promising toolkit, encompassing\nsynchronization, latency management, redundancy, and configuration\nfunctionalities crucial for addressing smart city challenges. Moreover, the\narticle scrutinizes how TSN, predominantly utilized in domains like automotive\nand industry, can be tailored to suit the intricate needs of smart cities,\nemphasizing the necessity for adaptability and scalability in network design.\n  This survey consolidates current research on TSN, outlining its potential in\nfortifying critical machine-to-machine communications within smart cities while\nhighlighting future challenges, potential solutions, and a roadmap for\nintegrating TSN effectively into the fabric of urban connectivity.",
            "author": [
                "Rui Lopes",
                "Duarte Raposo",
                "Susana Sargento"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03635v1",
                "http://arxiv.org/pdf/2312.03635v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "C.2.1"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03633v1",
            "title": "Not All Large Language Models (LLMs) Succumb to the \"Reversal Curse\": A\n  Comparative Study of Deductive Logical Reasoning in BERT and GPT Models",
            "updated": "2023-12-06T17:29:45Z",
            "published": "2023-12-06T17:29:45Z",
            "summary": "The \"Reversal Curse\" refers to the scenario where auto-regressive decoder\nlarge language models (LLMs), such as ChatGPT, trained on \"A is B\" fail to\nlearn \"B is A\", demonstrating a basic failure of logical deduction. This raises\na red flag in the use of GPT models for certain general tasks such as\nconstructing knowledge graphs, considering their adherence to this symmetric\nprinciple. In our study, we examined a bidirectional LLM, BERT, and found that\nit is immune to the reversal curse. Driven by ongoing efforts to construct\nbiomedical knowledge graphs with LLMs, we also embarked on evaluating more\ncomplex but essential deductive reasoning capabilities. This process included\nfirst training encoder and decoder language models to master the intersection\n($\\cap$) and union ($\\cup$) operations on two sets and then moving on to assess\ntheir capability to infer different combinations of union ($\\cup$) and\nintersection ($\\cap$) operations on three newly created sets. The findings\nshowed that while both encoder and decoder language models, trained for tasks\ninvolving two sets (union/intersection), were proficient in such scenarios,\nthey encountered difficulties when dealing with operations that included three\nsets (various combinations of union and intersection). Our research highlights\nthe distinct characteristics of encoder and decoder models in simple and\ncomplex logical reasoning. In practice, the choice between BERT and GPT should\nbe guided by the specific requirements and nature of the task at hand,\nleveraging their respective strengths in bidirectional context comprehension\nand sequence prediction.",
            "author": [
                "Jingye Yang",
                "Da Wu",
                "Kai Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03633v1",
                "http://arxiv.org/pdf/2312.03633v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03631v1",
            "title": "MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations",
            "updated": "2023-12-06T17:28:03Z",
            "published": "2023-12-06T17:28:03Z",
            "summary": "While recent years have seen rapid progress in image-conditioned text\ngeneration, image captioning still suffers from the fundamental issue of\nhallucinations, the generation of spurious details that cannot be inferred from\nthe given image. Dedicated methods for reducing hallucinations in image\ncaptioning largely focus on closed-vocabulary object tokens, ignoring most\ntypes of hallucinations that occur in practice. In this work, we propose MOCHa,\nan approach that harnesses advancements in reinforcement learning (RL) to\naddress the sequence-level nature of hallucinations in an open-world setup. To\noptimize for caption fidelity to the input image, we leverage ground-truth\nreference captions as proxies to measure the logical consistency of generated\ncaptions. However, optimizing for caption fidelity alone fails to preserve the\nsemantic adequacy of generations; therefore, we propose a multi-objective\nreward function that jointly targets these qualities, without requiring any\nstrong supervision. We demonstrate that these goals can be simultaneously\noptimized with our framework, enhancing performance for various captioning\nmodels of different scales. Our qualitative and quantitative results\ndemonstrate MOCHa's superior performance across various established metrics. We\nalso demonstrate the benefit of our method in the open-vocabulary setting. To\nthis end, we contribute OpenCHAIR, a new benchmark for quantifying\nopen-vocabulary hallucinations in image captioning models, constructed using\ngenerative foundation models. We will release our code, benchmark, and trained\nmodels.",
            "author": [
                "Assaf Ben-Kish",
                "Moran Yanuka",
                "Morris Alper",
                "Raja Giryes",
                "Hadar Averbuch-Elor"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03631v1",
                "http://arxiv.org/pdf/2312.03631v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03629v1",
            "title": "Freeform Direct-write and Rewritable Photonic Integrated Circuits in\n  Phase-Change Thin Films",
            "updated": "2023-12-06T17:21:05Z",
            "published": "2023-12-06T17:21:05Z",
            "summary": "Photonic integrated circuits (PICs) with rapid prototyping and reprogramming\ncapabilities promise revolutionary impacts on a plethora of photonic\ntechnologies. Here, we report direct-write and rewritable photonic circuits on\na low-loss phase change material (PCM) thin film. Complete end-to-end PICs are\ndirectly laser written in one step without additional fabrication processes,\nand any part of the circuit can be erased and rewritten, facilitating rapid\ndesign modification. We demonstrate the versatility of this technique for\ndiverse applications, including an optical interconnect fabric for\nreconfigurable networking, a photonic crossbar array for optical computing, and\na tunable optical filter for optical signal processing. By combining the\nprogrammability of the direct laser writing technique with PCM, our technique\nunlocks opportunities for programmable photonic networking, computing, and\nsignal processing. Moreover, the rewritable photonic circuits enable rapid\nprototyping and testing in a convenient and cost-efficient manner, eliminate\nthe need for nanofabrication facilities, and thus promote the proliferation of\nphotonics research and education to a broader community.",
            "author": [
                "Changming Wu",
                "Haoqin Deng",
                "Yi-Siou Huang",
                "Heshan Yu",
                "Ichiro Takeuchi",
                "Carlos A. R\u00edos Ocampo",
                "Mo Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03629v1",
                "http://arxiv.org/pdf/2312.03629v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "cond-mat.mtrl-sci",
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03628v1",
            "title": "Boosting Segment Anything Model Towards Open-Vocabulary Learning",
            "updated": "2023-12-06T17:19:00Z",
            "published": "2023-12-06T17:19:00Z",
            "summary": "The recent Segment Anything Model (SAM) has emerged as a new paradigmatic\nvision foundation model, showcasing potent zero-shot generalization and\nflexible prompting. Despite SAM finding applications and adaptations in various\ndomains, its primary limitation lies in the inability to grasp object\nsemantics. In this paper, we present Sambor to seamlessly integrate SAM with\nthe open-vocabulary object detector in an end-to-end framework. While retaining\nall the remarkable capabilities inherent to SAM, we enhance it with the\ncapacity to detect arbitrary objects based on human inputs like category names\nor reference expressions. To accomplish this, we introduce a novel SideFormer\nmodule that extracts SAM features to facilitate zero-shot object localization\nand inject comprehensive semantic information for open-vocabulary recognition.\nIn addition, we devise an open-set region proposal network (Open-set RPN),\nenabling the detector to acquire the open-set proposals generated by SAM.\nSambor demonstrates superior zero-shot performance across benchmarks, including\nCOCO and LVIS, proving highly competitive against previous SoTA methods. We\naspire for this work to serve as a meaningful endeavor in endowing SAM to\nrecognize diverse object categories and advancing open-vocabulary learning with\nthe support of vision foundation models.",
            "author": [
                "Xumeng Han",
                "Longhui Wei",
                "Xuehui Yu",
                "Zhiyang Dou",
                "Xin He",
                "Kuiran Wang",
                "Zhenjun Han",
                "Qi Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03628v1",
                "http://arxiv.org/pdf/2312.03628v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03626v1",
            "title": "TokenCompose: Grounding Diffusion with Token-level Supervision",
            "updated": "2023-12-06T17:13:15Z",
            "published": "2023-12-06T17:13:15Z",
            "summary": "We present TokenCompose, a Latent Diffusion Model for text-to-image\ngeneration that achieves enhanced consistency between user-specified text\nprompts and model-generated images. Despite its tremendous success, the\nstandard denoising process in the Latent Diffusion Model takes text prompts as\nconditions only, absent explicit constraint for the consistency between the\ntext prompts and the image contents, leading to unsatisfactory results for\ncomposing multiple object categories. TokenCompose aims to improve\nmulti-category instance composition by introducing the token-wise consistency\nterms between the image content and object segmentation maps in the finetuning\nstage. TokenCompose can be applied directly to the existing training pipeline\nof text-conditioned diffusion models without extra human labeling information.\nBy finetuning Stable Diffusion, the model exhibits significant improvements in\nmulti-category instance composition and enhanced photorealism for its generated\nimages.",
            "author": [
                "Zirui Wang",
                "Zhizhou Sha",
                "Zheng Ding",
                "Yilin Wang",
                "Zhuowen Tu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03626v1",
                "http://arxiv.org/pdf/2312.03626v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03618v1",
            "title": "Beyond discounted returns: Robust Markov decision processes with average\n  and Blackwell optimality",
            "updated": "2023-12-06T17:04:08Z",
            "published": "2023-12-06T17:04:08Z",
            "summary": "Robust Markov Decision Processes (RMDPs) are a widely used framework for\nsequential decision-making under parameter uncertainty. RMDPs have been\nextensively studied when the objective is to maximize the discounted return,\nbut little is known for average optimality (optimizing the long-run average of\nthe rewards obtained over time) and Blackwell optimality (remaining discount\noptimal for all discount factors sufficiently close to 1). In this paper, we\nprove several foundational results for RMDPs beyond the discounted return. We\nshow that average optimal policies can be chosen stationary and deterministic\nfor sa-rectangular RMDPs but, perhaps surprisingly, that history-dependent\n(Markovian) policies strictly outperform stationary policies for average\noptimality in s-rectangular RMDPs. We also study Blackwell optimality for\nsa-rectangular RMDPs, where we show that {\\em approximate} Blackwell optimal\npolicies always exist, although Blackwell optimal policies may not exist. We\nalso provide a sufficient condition for their existence, which encompasses\nvirtually any examples from the literature. We then discuss the connection\nbetween average and Blackwell optimality, and we describe several algorithms to\ncompute the optimal average return. Interestingly, our approach leverages the\nconnections between RMDPs and stochastic games.",
            "author": [
                "Julien Grand-Clement",
                "Marek Petrik",
                "Nicolas Vieille"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03618v1",
                "http://arxiv.org/pdf/2312.03618v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03613v1",
            "title": "Augmenting optimization-based molecular design with graph neural\n  networks",
            "updated": "2023-12-06T16:56:38Z",
            "published": "2023-12-06T16:56:38Z",
            "summary": "Computer-aided molecular design (CAMD) studies quantitative\nstructure-property relationships and discovers desired molecules using\noptimization algorithms. With the emergence of machine learning models, CAMD\nscore functions may be replaced by various surrogates to automatically learn\nthe structure-property relationships. Due to their outstanding performance on\ngraph domains, graph neural networks (GNNs) have recently appeared frequently\nin CAMD. But using GNNs introduces new optimization challenges. This paper\nformulates GNNs using mixed-integer programming and then integrates this GNN\nformulation into the optimization and machine learning toolkit OMLT. To\ncharacterize and formulate molecules, we inherit the well-established\nmixed-integer optimization formulation for CAMD and propose symmetry-breaking\nconstraints to remove symmetric solutions caused by graph isomorphism. In two\ncase studies, we investigate fragment-based odorant molecular design with more\npractical requirements to test the compatibility and performance of our\napproaches.",
            "author": [
                "Shiqiang Zhang",
                "Juan S. Campos",
                "Christian Feldmann",
                "Frederik Sandfort",
                "Miriam Mathea",
                "Ruth Misener"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03613v1",
                "http://arxiv.org/pdf/2312.03613v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03612v1",
            "title": "Physical Symbolic Optimization",
            "updated": "2023-12-06T16:56:28Z",
            "published": "2023-12-06T16:56:28Z",
            "summary": "We present a framework for constraining the automatic sequential generation\nof equations to obey the rules of dimensional analysis by construction.\nCombining this approach with reinforcement learning, we built $\\Phi$-SO, a\nPhysical Symbolic Optimization method for recovering analytical functions from\nphysical data leveraging units constraints. Our symbolic regression algorithm\nachieves state-of-the-art results in contexts in which variables and constants\nhave known physical units, outperforming all other methods on SRBench's Feynman\nbenchmark in the presence of noise (exceeding 0.1%) and showing resilience even\nin the presence of significant (10%) levels of noise.",
            "author": [
                "Wassim Tenachi",
                "Rodrigo Ibata",
                "Foivos I. Diakogiannis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03612v1",
                "http://arxiv.org/pdf/2312.03612v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "astro-ph.IM",
                "cs.SC",
                "physics.comp-ph",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03611v1",
            "title": "DreamComposer: Controllable 3D Object Generation via Multi-View\n  Conditions",
            "updated": "2023-12-06T16:55:53Z",
            "published": "2023-12-06T16:55:53Z",
            "summary": "Utilizing pre-trained 2D large-scale generative models, recent works are\ncapable of generating high-quality novel views from a single in-the-wild image.\nHowever, due to the lack of information from multiple views, these works\nencounter difficulties in generating controllable novel views. In this paper,\nwe present DreamComposer, a flexible and scalable framework that can enhance\nexisting view-aware diffusion models by injecting multi-view conditions.\nSpecifically, DreamComposer first uses a view-aware 3D lifting module to obtain\n3D representations of an object from multiple views. Then, it renders the\nlatent features of the target view from 3D representations with the multi-view\nfeature fusion module. Finally the target view features extracted from\nmulti-view inputs are injected into a pre-trained diffusion model. Experiments\nshow that DreamComposer is compatible with state-of-the-art diffusion models\nfor zero-shot novel view synthesis, further enhancing them to generate\nhigh-fidelity novel view images with multi-view conditions, ready for\ncontrollable 3D object reconstruction and various other applications.",
            "author": [
                "Yunhan Yang",
                "Yukun Huang",
                "Xiaoyang Wu",
                "Yuan-Chen Guo",
                "Song-Hai Zhang",
                "Hengshuang Zhao",
                "Tong He",
                "Xihui Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03611v1",
                "http://arxiv.org/pdf/2312.03611v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03609v1",
            "title": "Time-Domain Operational Metrics for Real-time Resilience Assessment in\n  DC Microgrids",
            "updated": "2023-12-06T16:55:16Z",
            "published": "2023-12-06T16:55:16Z",
            "summary": "Resilience is emerging as an evolving notion, reflecting a system's ability\nto endure and adapt to sudden and catastrophic changes and disruptions. This\npaper spotlights the significance of the quantitative resilience indices of\nmedium-voltage DC (MVDC) distribution technology in marine vessels, notably\nnaval ships. Given the intricate electrical requirements of modern naval ships,\nthe need for a robust power supply underlines the imperative of resilient DC\nmicrogrids. Addressing this, our study introduces a novel quantitative metric\nfor operational resilience of DC microgrids based on the measured voltage of\nmain DC bus. This metric not only fuses real-time tracking, compatibility, and\ncomputational efficiency, but also adeptly monitors multiple event phases based\non time-domain analysis of dc bus voltage dynamics. The intricacies of the dc\nbus voltage, including overshoots and undershoots, are meticulously accounted\nfor in the algorithm design. With respect to existing research that typically\nfocuses on offline resilience assessments, the proposed index provides valuable\nreal-time information for microgrid operators and identifies whether microgrid\nresilience is deteriorating over time.",
            "author": [
                "Maral Shadaei",
                "Ali Hosseinipour",
                "Javad Khazaei"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03609v1",
                "http://arxiv.org/pdf/2312.03609v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03608v1",
            "title": "Automated Multimodal Data Annotation via Calibration With Indoor\n  Positioning System",
            "updated": "2023-12-06T16:54:24Z",
            "published": "2023-12-06T16:54:24Z",
            "summary": "Learned object detection methods based on fusion of LiDAR and camera data\nrequire labeled training samples, but niche applications, such as warehouse\nrobotics or automated infrastructure, require semantic classes not available in\nlarge existing datasets. Therefore, to facilitate the rapid creation of\nmultimodal object detection datasets and alleviate the burden of human\nlabeling, we propose a novel automated annotation pipeline. Our method uses an\nindoor positioning system (IPS) to produce accurate detection labels for both\npoint clouds and images and eliminates manual annotation entirely. In an\nexperiment, the system annotates objects of interest 261.8 times faster than a\nhuman baseline and speeds up end-to-end dataset creation by 61.5%.",
            "author": [
                "Ryan Rubel",
                "Andrew Dudash",
                "Mohammad Goli",
                "James O'Hara",
                "Karl Wunderlich"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03608v1",
                "http://arxiv.org/pdf/2312.03608v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "I.4.m"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03607v1",
            "title": "From concrete mixture to structural design -- a holistic optimization\n  procedure in the presence of uncertainties",
            "updated": "2023-12-06T16:54:14Z",
            "published": "2023-12-06T16:54:14Z",
            "summary": "Designing civil structures such as bridges, dams or buildings is a complex\ntask requiring many synergies from several experts. Each is responsible for\ndifferent parts of the process. This is often done in a sequential manner, e.g.\nthe structural engineer makes a design under the assumption of certain material\nproperties (e.g. the strength class of the concrete), and then the material\nengineer optimizes the material with these restrictions. This paper proposes a\nholistic optimization procedure, which combines the concrete mixture design and\nstructural simulations in a joint, forward workflow that we ultimately seek to\ninvert. In this manner, new mixtures beyond standard ranges can be considered.\nAny design effort should account for the presence of uncertainties which can be\naleatoric or epistemic as when data is used to calibrate physical models or\nidentify models that fill missing links in the workflow. Inverting the causal\nrelations established poses several challenges especially when these involve\nphysics-based models which most often than not do not provide\nderivatives/sensitivities or when design constraints are present. To this end,\nwe advocate Variational Optimization, with proposed extensions and\nappropriately chosen heuristics to overcome the aforementioned challenges. The\nproposed methodology is illustrated using the design of a precast concrete beam\nwith the objective to minimize the global warming potential while satisfying a\nnumber of constraints associated with its load-bearing capacity after 28days\naccording to the Eurocode, the demoulding time as computed by a complex\nnonlinear Finite Element model, and the maximum temperature during the\nhydration.",
            "author": [
                "Atul Agrawal",
                "Erik Tamsen",
                "Phaedon-Stelios Koutsourelakis",
                "Joerg F. Unger"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03607v1",
                "http://arxiv.org/pdf/2312.03607v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "physics.comp-ph",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03606v1",
            "title": "DiffusionSat: A Generative Foundation Model for Satellite Imagery",
            "updated": "2023-12-06T16:53:17Z",
            "published": "2023-12-06T16:53:17Z",
            "summary": "Diffusion models have achieved state-of-the-art results on many modalities\nincluding images, speech, and video. However, existing models are not tailored\nto support remote sensing data, which is widely used in important applications\nincluding environmental monitoring and crop-yield prediction. Satellite images\nare significantly different from natural images -- they can be multi-spectral,\nirregularly sampled across time -- and existing diffusion models trained on\nimages from the Web do not support them. Furthermore, remote sensing data is\ninherently spatio-temporal, requiring conditional generation tasks not\nsupported by traditional methods based on captions or images. In this paper, we\npresent DiffusionSat, to date the largest generative foundation model trained\non a collection of publicly available large, high-resolution remote sensing\ndatasets. As text-based captions are sparsely available for satellite images,\nwe incorporate the associated metadata such as geolocation as conditioning\ninformation. Our method produces realistic samples and can be used to solve\nmultiple generative tasks including temporal generation, superresolution given\nmulti-spectral inputs and in-painting. Our method outperforms previous\nstate-of-the-art methods for satellite image generation and is the first\nlarge-scale $\\textit{generative}$ foundation model for satellite imagery.",
            "author": [
                "Samar Khanna",
                "Patrick Liu",
                "Linqi Zhou",
                "Chenlin Meng",
                "Robin Rombach",
                "Marshall Burke",
                "David Lobell",
                "Stefano Ermon"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03606v1",
                "http://arxiv.org/pdf/2312.03606v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03808v1",
            "title": "SurfaceAug: Closing the Gap in Multimodal Ground Truth Sampling",
            "updated": "2023-12-06T16:51:30Z",
            "published": "2023-12-06T16:51:30Z",
            "summary": "Despite recent advances in both model architectures and data augmentation,\nmultimodal object detectors still barely outperform their LiDAR-only\ncounterparts. This shortcoming has been attributed to a lack of sufficiently\npowerful multimodal data augmentation. To address this, we present SurfaceAug,\na novel ground truth sampling algorithm. SurfaceAug pastes objects by\nresampling both images and point clouds, enabling object-level transformations\nin both modalities. We evaluate our algorithm by training a multimodal detector\non KITTI and compare its performance to previous works. We show experimentally\nthat SurfaceAug outperforms existing methods on car detection tasks and\nestablishes a new state of the art for multimodal ground truth sampling.",
            "author": [
                "Ryan Rubel",
                "Nathan Clark",
                "Andrew Dudash"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03808v1",
                "http://arxiv.org/pdf/2312.03808v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03600v1",
            "title": "A Mixed Integer Quadratic Program for Valuing the Impact of Price and\n  Forecast Uncertainty for Wind Generators",
            "updated": "2023-12-06T16:42:53Z",
            "published": "2023-12-06T16:42:53Z",
            "summary": "Owners of wind power plants are exposed to financial risk in wholesale\nelectricity markets due to the uncertain nature of wind forecasts and price\nvolatility. In the event of a wind shortfall, the plant may have to repurchase\npower at a higher price in the real-time market. However, reducing the power\noffered in the day-ahead market may also be interpreted by regulators as\nphysical withholding. We formulate and solve a mixed-integer quadratic program\n(MIQP) that prices the uncertain portion of a wind generator's forecast to\nhedge against uncertainties and which addresses concerns around withholding. We\nexploit the structure of the MIQP inputs to introduce additional constraints to\nimprove computation time. Additionally, we provide a qualitative approach for\ngenerators and regulators to interpret the results of the MIQP. Finally, we\nsimulate a real-world application for a wind farm in New York using past wind\nforecasts and NYISO prices.",
            "author": [
                "Daniel Shen",
                "Marija Ilic"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03600v1",
                "http://arxiv.org/pdf/2312.03600v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03598v1",
            "title": "A Machine-Learning-Accelerated Quantum Transport Study on the Effects of\n  Superlattice Disorder and Strain in a Mid-wave Infrared Curved Sensor",
            "updated": "2023-12-06T16:42:34Z",
            "published": "2023-12-06T16:42:34Z",
            "summary": "An emerging device architecture for infrared imaging is the curved\nfocal-plane array which benefits from several optical advantages over the\ntraditional flat design. However, the curving process introduces additional\nstrain in the active region which must be taken into account. Type-II\nsuperlattices, a promising alternative to traditional bulk materials for use in\ninfrared photodetectors, is a candidate material for use in these devices, but\nthe transport properties of these highly heterogeneous materials are not\nstraightforward and can be affected by different material conditions, such as\nsuperlattice disorder and external strain. We present a comprehensive study of\nthe internal QE calculated for a curved device that incorporates finite element\nanalysis (FEA) modeling, nonequilibirium Green's functions (NEGF) calculations,\nand Gaussian Process (GP) regression. FEA is used for predicting the strain\nconfiguration throughout the active region induced by the curving procedure of\nthe device. NEGF is used to calculate the vertical hole mobility for a select\nset of strain configurations, from which the internal quantum efficiency of the\ndevice is approximated to predict performance under strained conditions. Then\nthis data set is used to train a GP model that maps the quantum efficiency QE\npredictions onto the spatial coordinates of the curved device, based on the\nstrain configuration predicted using FEA. This analysis is performed for ideal\nand disordered SLs to understand both the fundamental and practical limitations\nof the performance of these materials in curved devices.",
            "author": [
                "John Glennon",
                "Alexandros Kyrtsos",
                "Mark O'Masta",
                "Binh-Minh Nyguyen",
                "Enrico Bellotti"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03598v1",
                "http://arxiv.org/pdf/2312.03598v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03596v1",
            "title": "MMM: Generative Masked Motion Model",
            "updated": "2023-12-06T16:35:59Z",
            "published": "2023-12-06T16:35:59Z",
            "summary": "Recent advances in text-to-motion generation using diffusion and\nautoregressive models have shown promising results. However, these models often\nsuffer from a trade-off between real-time performance, high fidelity, and\nmotion editability. To address this gap, we introduce MMM, a novel yet simple\nmotion generation paradigm based on Masked Motion Model. MMM consists of two\nkey components: (1) a motion tokenizer that transforms 3D human motion into a\nsequence of discrete tokens in latent space, and (2) a conditional masked\nmotion transformer that learns to predict randomly masked motion tokens,\nconditioned on the pre-computed text tokens. By attending to motion and text\ntokens in all directions, MMM explicitly captures inherent dependency among\nmotion tokens and semantic mapping between motion and text tokens. During\ninference, this allows parallel and iterative decoding of multiple motion\ntokens that are highly consistent with fine-grained text descriptions,\ntherefore simultaneously achieving high-fidelity and high-speed motion\ngeneration. In addition, MMM has innate motion editability. By simply placing\nmask tokens in the place that needs editing, MMM automatically fills the gaps\nwhile guaranteeing smooth transitions between editing and non-editing parts.\nExtensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMM\nsurpasses current leading methods in generating high-quality motion (evidenced\nby superior FID scores of 0.08 and 0.429), while offering advanced editing\nfeatures such as body-part modification, motion in-betweening, and the\nsynthesis of long motion sequences. In addition, MMM is two orders of magnitude\nfaster on a single mid-range GPU than editable motion diffusion models. Our\nproject page is available at \\url{https://exitudio.github.io/MMM-page}.",
            "author": [
                "Ekkasit Pinyoanuntapong",
                "Pu Wang",
                "Minwoo Lee",
                "Chen Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03596v1",
                "http://arxiv.org/pdf/2312.03596v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03807v1",
            "title": "Achieving ${O}(\u03b5^{-1.5})$ Complexity in Hessian/Jacobian-free\n  Stochastic Bilevel Optimization",
            "updated": "2023-12-06T16:34:58Z",
            "published": "2023-12-06T16:34:58Z",
            "summary": "In this paper, we revisit the bilevel optimization problem, in which the\nupper-level objective function is generally nonconvex and the lower-level\nobjective function is strongly convex. Although this type of problem has been\nstudied extensively, it still remains an open question how to achieve an\n${O}(\\epsilon^{-1.5})$ sample complexity of ${O}(\\epsilon^{-1.5})$ in\nHessian/Jacobian-free stochastic bilevel optimization without any second-order\nderivative computation. To fill this gap, we propose a novel\nHessian/Jacobian-free bilevel optimizer named FdeHBO, which features a simple\nfully single-loop structure, a projection-aided finite-difference\nHessian/Jacobian-vector approximation, and momentum-based updates.\nTheoretically, we show that FdeHBO requires ${O}(\\epsilon^{-1.5})$ iterations\n(each using ${O}(1)$ samples and only first-order gradient information) to find\nan $\\epsilon$-accurate stationary point. As far as we know, this is the first\nHessian/Jacobian-free method with an ${O}(\\epsilon^{-1.5})$ sample complexity\nfor nonconvex-strongly-convex stochastic bilevel optimization.",
            "author": [
                "Yifan Yang",
                "Peiyao Xiao",
                "Kaiyi Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03807v1",
                "http://arxiv.org/pdf/2312.03807v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03594v2",
            "title": "A Task is Worth One Word: Learning with Task Prompts for High-Quality\n  Versatile Image Inpainting",
            "updated": "2023-12-07T03:13:33Z",
            "published": "2023-12-06T16:34:46Z",
            "summary": "Achieving high-quality versatile image inpainting, where user-specified\nregions are filled with plausible content according to user intent, presents a\nsignificant challenge. Existing methods face difficulties in simultaneously\naddressing context-aware image inpainting and text-guided object inpainting due\nto the distinct optimal training strategies required. To overcome this\nchallenge, we introduce PowerPaint, the first high-quality and versatile\ninpainting model that excels in both tasks. First, we introduce learnable task\nprompts along with tailored fine-tuning strategies to guide the model's focus\non different inpainting targets explicitly. This enables PowerPaint to\naccomplish various inpainting tasks by utilizing different task prompts,\nresulting in state-of-the-art performance. Second, we demonstrate the\nversatility of the task prompt in PowerPaint by showcasing its effectiveness as\na negative prompt for object removal. Additionally, we leverage prompt\ninterpolation techniques to enable controllable shape-guided object inpainting.\nFinally, we extensively evaluate PowerPaint on various inpainting benchmarks to\ndemonstrate its superior performance for versatile image inpainting. We release\nour codes and models on our project page: https://powerpaint.github.io/.",
            "author": [
                "Junhao Zhuang",
                "Yanhong Zeng",
                "Wenran Liu",
                "Chun Yuan",
                "Kai Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03594v2",
                "http://arxiv.org/pdf/2312.03594v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03590v1",
            "title": "Revisiting Micro and Macro Expressions in Computer Graphics Characters",
            "updated": "2023-12-06T16:30:40Z",
            "published": "2023-12-06T16:30:40Z",
            "summary": "This paper presents the reproduction of two studies focused on the perception\nof micro and macro expressions of Virtual Humans (VHs) generated by Computer\nGraphics (CG), first described in 2014 and replicated in 2021. The 2014 study\nreferred to a VH realistic, whereas, in 2021, it referred to a VH cartoon. In\nour work, we replicate the study by using a realistic CG character. Our main\ngoals are to compare the perceptions of micro and macro expressions between\nlevels of realism (2021 cartoon versus 2023 realistic) and between realistic\ncharacters in different periods (i.e., 2014 versus 2023). In one of our\nresults, people more easily recognized micro expressions in realistic VHs than\nin a cartoon VH. In another result, we show that the participants' perception\nwas similar for both micro and macro expressions in 2014 and 2023.",
            "author": [
                "Rubens Montanha",
                "Giovana Raupp",
                "Vitoria Gonzalez",
                "Yanny Partichelli",
                "Andr\u00e9 Bins",
                "Marcos Ferreira",
                "Victor Araujo",
                "Soraia Musse"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03590v1",
                "http://arxiv.org/pdf/2312.03590v1"
            ],
            "primary_category": "cs.GR",
            "category": [
                "cs.GR",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03587v1",
            "title": "Language-Informed Visual Concept Learning",
            "updated": "2023-12-06T16:24:47Z",
            "published": "2023-12-06T16:24:47Z",
            "summary": "Our understanding of the visual world is centered around various concept\naxes, characterizing different aspects of visual entities. While different\nconcept axes can be easily specified by language, e.g. color, the exact visual\nnuances along each axis often exceed the limitations of linguistic\narticulations, e.g. a particular style of painting. In this work, our goal is\nto learn a language-informed visual concept representation, by simply\ndistilling large pre-trained vision-language models. Specifically, we train a\nset of concept encoders to encode the information pertinent to a set of\nlanguage-informed concept axes, with an objective of reproducing the input\nimage through a pre-trained Text-to-Image (T2I) model. To encourage better\ndisentanglement of different concept encoders, we anchor the concept embeddings\nto a set of text embeddings obtained from a pre-trained Visual Question\nAnswering (VQA) model. At inference time, the model extracts concept embeddings\nalong various axes from new test images, which can be remixed to generate\nimages with novel compositions of visual concepts. With a lightweight test-time\nfinetuning procedure, it can also generalize to novel concepts unseen at\ntraining.",
            "author": [
                "Sharon Lee",
                "Yunzhi Zhang",
                "Shangzhe Wu",
                "Jiajun Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03587v1",
                "http://arxiv.org/pdf/2312.03587v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03806v1",
            "title": "XCube ($\\mathcal{X}^3$): Large-Scale 3D Generative Modeling using Sparse\n  Voxel Hierarchies",
            "updated": "2023-12-06T16:23:26Z",
            "published": "2023-12-06T16:23:26Z",
            "summary": "We present $\\mathcal{X}^3$ (pronounced XCube), a novel generative model for\nhigh-resolution sparse 3D voxel grids with arbitrary attributes. Our model can\ngenerate millions of voxels with a finest effective resolution of up to\n$1024^3$ in a feed-forward fashion without time-consuming test-time\noptimization. To achieve this, we employ a hierarchical voxel latent diffusion\nmodel which generates progressively higher resolution grids in a coarse-to-fine\nmanner using a custom framework built on the highly efficient VDB data\nstructure. Apart from generating high-resolution objects, we demonstrate the\neffectiveness of XCube on large outdoor scenes at scales of 100m$\\times$100m\nwith a voxel size as small as 10cm. We observe clear qualitative and\nquantitative improvements over past approaches. In addition to unconditional\ngeneration, we show that our model can be used to solve a variety of tasks such\nas user-guided editing, scene completion from a single scan, and text-to-3D.\nMore results and details can be found at\nhttps://research.nvidia.com/labs/toronto-ai/xcube/.",
            "author": [
                "Xuanchi Ren",
                "Jiahui Huang",
                "Xiaohui Zeng",
                "Ken Museth",
                "Sanja Fidler",
                "Francis Williams"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03806v1",
                "http://arxiv.org/pdf/2312.03806v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03585v1",
            "title": "Foundation Model Assisted Weakly Supervised Semantic Segmentation",
            "updated": "2023-12-06T16:21:06Z",
            "published": "2023-12-06T16:21:06Z",
            "summary": "This work aims to leverage pre-trained foundation models, such as contrastive\nlanguage-image pre-training (CLIP) and segment anything model (SAM), to address\nweakly supervised semantic segmentation (WSSS) using image-level labels. To\nthis end, we propose a coarse-to-fine framework based on CLIP and SAM for\ngenerating high-quality segmentation seeds. Specifically, we construct an image\nclassification task and a seed segmentation task, which are jointly performed\nby CLIP with frozen weights and two sets of learnable task-specific prompts. A\nSAM-based seeding (SAMS) module is designed and applied to each task to produce\neither coarse or fine seed maps. Moreover, we design a multi-label contrastive\nloss supervised by image-level labels and a CAM activation loss supervised by\nthe generated coarse seed map. These losses are used to learn the prompts,\nwhich are the only parts need to be learned in our framework. Once the prompts\nare learned, we input each image along with the learned segmentation-specific\nprompts into CLIP and the SAMS module to produce high-quality segmentation\nseeds. These seeds serve as pseudo labels to train an off-the-shelf\nsegmentation network like other two-stage WSSS methods. Experiments show that\nour method achieves the state-of-the-art performance on PASCAL VOC 2012 and\ncompetitive results on MS COCO 2014.",
            "author": [
                "Xiaobo Yang",
                "Xiaojin Gong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03585v1",
                "http://arxiv.org/pdf/2312.03585v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03584v1",
            "title": "Context Diffusion: In-Context Aware Image Generation",
            "updated": "2023-12-06T16:19:51Z",
            "published": "2023-12-06T16:19:51Z",
            "summary": "We propose Context Diffusion, a diffusion-based framework that enables image\ngeneration models to learn from visual examples presented in context. Recent\nwork tackles such in-context learning for image generation, where a query image\nis provided alongside context examples and text prompts. However, the quality\nand fidelity of the generated images deteriorate when the prompt is not\npresent, demonstrating that these models are unable to truly learn from the\nvisual context. To address this, we propose a novel framework that separates\nthe encoding of the visual context and preserving the structure of the query\nimages. This results in the ability to learn from the visual context and text\nprompts, but also from either one of them. Furthermore, we enable our model to\nhandle few-shot settings, to effectively address diverse in-context learning\nscenarios. Our experiments and user study demonstrate that Context Diffusion\nexcels in both in-domain and out-of-domain tasks, resulting in an overall\nenhancement in image quality and fidelity compared to counterpart models.",
            "author": [
                "Ivona Najdenkoska",
                "Animesh Sinha",
                "Abhimanyu Dubey",
                "Dhruv Mahajan",
                "Vignesh Ramanathan",
                "Filip Radenovic"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03584v1",
                "http://arxiv.org/pdf/2312.03584v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03579v1",
            "title": "The Implication Problem for Functional Dependencies and Variants of\n  Marginal Distribution Equivalences",
            "updated": "2023-12-06T16:15:16Z",
            "published": "2023-12-06T16:15:16Z",
            "summary": "We study functional dependencies together with two different probabilistic\ndependency notions: unary marginal identity and unary marginal distribution\nequivalence. A unary marginal identity states that two variables x and y are\nidentically distributed. A unary marginal distribution equivalence states that\nthe multiset consisting of the marginal probabilities of all the values for\nvariable x is the same as the corresponding multiset for y. We present a sound\nand complete axiomatization for the class of these dependencies and show that\nit has Armstrong relations. The axiomatization is infinite, but we show that\nthere can be no finite axiomatization. The implication problem for the subclass\nthat contains only functional dependencies and unary marginal identities can be\nsimulated with functional dependencies and unary inclusion atoms, and therefore\nthe problem is in polynomial-time. This complexity bound also holds in the case\nof the full class, which we show by constructing a polynomial-time algorithm.",
            "author": [
                "Minna Hirvonen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03579v1",
                "http://arxiv.org/pdf/2312.03579v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03577v1",
            "title": "Improving Bias Mitigation through Bias Experts in Natural Language\n  Understanding",
            "updated": "2023-12-06T16:15:00Z",
            "published": "2023-12-06T16:15:00Z",
            "summary": "Biases in the dataset often enable the model to achieve high performance on\nin-distribution data, while poorly performing on out-of-distribution data. To\nmitigate the detrimental effect of the bias on the networks, previous works\nhave proposed debiasing methods that down-weight the biased examples identified\nby an auxiliary model, which is trained with explicit bias labels. However,\nfinding a type of bias in datasets is a costly process. Therefore, recent\nstudies have attempted to make the auxiliary model biased without the guidance\n(or annotation) of bias labels, by constraining the model's training\nenvironment or the capability of the model itself. Despite the promising\ndebiasing results of recent works, the multi-class learning objective, which\nhas been naively used to train the auxiliary model, may harm the bias\nmitigation effect due to its regularization effect and competitive nature\nacross classes. As an alternative, we propose a new debiasing framework that\nintroduces binary classifiers between the auxiliary model and the main model,\ncoined bias experts. Specifically, each bias expert is trained on a binary\nclassification task derived from the multi-class classification task via the\nOne-vs-Rest approach. Experimental results demonstrate that our proposed\nstrategy improves the bias identification ability of the auxiliary model.\nConsequently, our debiased model consistently outperforms the state-of-the-art\non various challenge datasets.",
            "author": [
                "Eojin Jeon",
                "Mingyu Lee",
                "Juhyeong Park",
                "Yeachan Kim",
                "Wing-Lam Mok",
                "SangKeun Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03577v1",
                "http://arxiv.org/pdf/2312.03577v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03576v1",
            "title": "Operational Resilience Assessment: A Frequency-Domain Approach for DC\n  Microgrids",
            "updated": "2023-12-06T16:14:12Z",
            "published": "2023-12-06T16:14:12Z",
            "summary": "DC shipboard microgrids (SMGs) are highly dynamic systems susceptible to\nfailure due to various cyber-physical disturbances, such as extreme weather and\nmission operations during wartime. In this paper, the real-time operational\nresilience (OR) evaluation of DC SMGs against dynamic disturbances is proposed\nusing frequency-domain metrics. To this end, first the drawbacks of time-domain\nOR evaluation using an energy imbalance index is discussed. As the time-domain\nenergy imbalance index is shown to be incapable of real-time OR assessment,\nparticularly in the context of droop-controlled DC SMGs without secondary\nvoltage restoration control, the $\\mathcal{H}_2$ and $\\mathcal{H_\\infty}$ norms\nof candidate transfer functions (TFs) of the system are proposed as measures of\nresilience. It is shown that the proposed norms calculated for the bus\nimpedance TF of the system provides intuitive results in terms of energy\nimbalance and can be computed in real time. The case studies conducted for the\nstudy DC SMG under pulsed power load (PPL) disturbances demonstrate the\nshortcoming of the time-domain OR evaluation and the capability of the proposed\nfrequency-domain metrics in intuitive OR evaluation of DC SMGs.",
            "author": [
                "Ali Hosseinipour",
                "Maral Shadaei",
                "Javad Khazaei"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03576v1",
                "http://arxiv.org/pdf/2312.03576v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03574v1",
            "title": "Enhanced star formation and metallicity deficit in the USS 1558-003\n  forming protocluster at z=2.53",
            "updated": "2023-12-06T16:13:15Z",
            "published": "2023-12-06T16:13:15Z",
            "summary": "We use K-band multi-object near-infrared spectroscopy with Keck/MOSFIRE to\nsearch for environmental imprints on the gas properties of 27 narrow-band\nselected H$\\alpha$ emitters (HAEs) across the three major clumps of the\nassembling USS1558--003 protocluster at $z=2.53$. We target the H$\\alpha$ and\n[NII]$\\lambda$6584 emission lines to obtain star-formation rates (SFR) and\ngas-phase oxygen abundances for our sources, confirming the membership of 23\nobjects. HAEs belonging to this protocluster display enhanced SFRs with respect\nto the main sequence of star formation at the same cosmic epoch. This effect is\nmore prominent for low-mass galaxies ($\\mathrm{\\log M_*/M_\\odot<10.0}$), which\nmay be experiencing a vigorous phase of mass assembly shortly after they were\nformed. We compute the individual and stacked gas-phase metallicities for our\nsources finding a metallicity deficit for low-mass objects when compared\nagainst the field mass-metallicity relation and the massive Spiderweb\nprotocluster at $z=2.16$. These results suggest that HAEs within USS1558--003\nmay be less evolved than those in the Spiderweb protocluster. Finally, we\nexplore the gas metallicity - gas fraction relation for a small sample of five\ngalaxies with CO(3-2) molecular gas information. Assuming our objects are in\nequilibrium, we obtain a relatively wide range of mass loading factors\n($\\mathrm{\\lambda=0.5-2}$) matching field samples at the cosmic noon but in\ncontrast with our previous results in the Spiderweb protocluster. We speculate\nthat these discrepancies between protoclusters may be (partly) driven by\ndifferences in their current dynamical and mass assembly stages, hinting at the\nco-evolution of protoclusters and their galaxy populations at $2<z<3$.",
            "author": [
                "Jose Manuel P\u00e9rez-Mart\u00ednez",
                "Tadayuki Kodama",
                "Yusei Koyama",
                "Rhythm Shimakawa",
                "Tomoko L. Suzuki",
                "Kazuki Daikuhara",
                "Kota Adachi",
                "Masato Onodera",
                "Ichi Tanaka"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03574v1",
                "http://arxiv.org/pdf/2312.03574v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03568v1",
            "title": "DocBinFormer: A Two-Level Transformer Network for Effective Document\n  Image Binarization",
            "updated": "2023-12-06T16:01:29Z",
            "published": "2023-12-06T16:01:29Z",
            "summary": "In real life, various degradation scenarios exist that might damage document\nimages, making it harder to recognize and analyze them, thus binarization is a\nfundamental and crucial step for achieving the most optimal performance in any\ndocument analysis task. We propose DocBinFormer (Document Binarization\nTransformer), a novel two-level vision transformer (TL-ViT) architecture based\non vision transformers for effective document image binarization. The presented\narchitecture employs a two-level transformer encoder to effectively capture\nboth global and local feature representation from the input images. These\ncomplimentary bi-level features are exploited for efficient document image\nbinarization, resulting in improved results for system-generated as well as\nhandwritten document images in a comprehensive approach. With the absence of\nconvolutional layers, the transformer encoder uses the pixel patches and\nsub-patches along with their positional information to operate directly on\nthem, while the decoder generates a clean (binarized) output image from the\nlatent representation of the patches. Instead of using a simple vision\ntransformer block to extract information from the image patches, the proposed\narchitecture uses two transformer blocks for greater coverage of the extracted\nfeature space on a global and local scale. The encoded feature representation\nis used by the decoder block to generate the corresponding binarized output.\nExtensive experiments on a variety of DIBCO and H-DIBCO benchmarks show that\nthe proposed model outperforms state-of-the-art techniques on four metrics. The\nsource code will be made available at\nhttps://github.com/RisabBiswas/DocBinFormer.",
            "author": [
                "Risab Biswas",
                "Swalpa Kumar Roy",
                "Ning Wang",
                "Umapada Pal",
                "Guang-Bin Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03568v1",
                "http://arxiv.org/pdf/2312.03568v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03567v1",
            "title": "XAIQA: Explainer-Based Data Augmentation for Extractive Question\n  Answering",
            "updated": "2023-12-06T15:59:06Z",
            "published": "2023-12-06T15:59:06Z",
            "summary": "Extractive question answering (QA) systems can enable physicians and\nresearchers to query medical records, a foundational capability for designing\nclinical studies and understanding patient medical history. However, building\nthese systems typically requires expert-annotated QA pairs. Large language\nmodels (LLMs), which can perform extractive QA, depend on high quality data in\ntheir prompts, specialized for the application domain. We introduce a novel\napproach, XAIQA, for generating synthetic QA pairs at scale from data naturally\navailable in electronic health records. Our method uses the idea of a\nclassification model explainer to generate questions and answers about medical\nconcepts corresponding to medical codes. In an expert evaluation with two\nphysicians, our method identifies $2.2\\times$ more semantic matches and\n$3.8\\times$ more clinical abbreviations than two popular approaches that use\nsentence transformers to create QA pairs. In an ML evaluation, adding our QA\npairs improves performance of GPT-4 as an extractive QA model, including on\ndifficult questions. In both the expert and ML evaluations, we examine\ntrade-offs between our method and sentence transformers for QA pair generation\ndepending on question difficulty.",
            "author": [
                "Joel Stremmel",
                "Ardavan Saeedi",
                "Hamid Hassanzadeh",
                "Sanjit Batra",
                "Jeffrey Hertzberg",
                "Jaime Murillo",
                "Eran Halperin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03567v1",
                "http://arxiv.org/pdf/2312.03567v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03565v1",
            "title": "The first five years of the AAA algorithm",
            "updated": "2023-12-06T15:57:12Z",
            "published": "2023-12-06T15:57:12Z",
            "summary": "The AAA algorithm, introduced in 2018, computes best or near-best rational\napproximations to functions or data on subsets of the real line or the complex\nplane. It is much faster and more robust than previous algorithms for such\nproblems and has been used in many applications since its appearance, including\nthe numerical solution of Laplace, Poisson, and biharmonic PDE problems in\nirregular domains. AAA has also been extended in new directions and seems\nlikely to be a tool of lasting importance in the future.",
            "author": [
                "Yuji Nakatsukasa",
                "Olivier Sete",
                "Lloyd N. Trefethen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03565v1",
                "http://arxiv.org/pdf/2312.03565v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "41A20, 65D15"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03805v1",
            "title": "SYNC-CLIP: Synthetic Data Make CLIP Generalize Better in Data-Limited\n  Scenarios",
            "updated": "2023-12-06T15:54:05Z",
            "published": "2023-12-06T15:54:05Z",
            "summary": "Prompt learning is a powerful technique for transferring Vision-Language\nModels (VLMs) such as CLIP to downstream tasks. However, the prompt-based\nmethods that are fine-tuned solely with base classes may struggle to generalize\nto novel classes in open-vocabulary scenarios, especially when data are\nlimited. To address this issue, we propose an innovative approach called\nSYNC-CLIP that leverages SYNthetiC data for enhancing the generalization\ncapability of CLIP. Based on the observation of the distribution shift between\nthe real and synthetic samples, we treat real and synthetic samples as distinct\ndomains and propose to optimize separate domain prompts to capture\ndomain-specific information, along with the shared visual prompts to preserve\nthe semantic consistency between two domains. By aligning the cross-domain\nfeatures, the synthetic data from novel classes can provide implicit guidance\nto rebalance the decision boundaries. Experimental results on three model\ngeneralization tasks demonstrate that our method performs very competitively\nacross various benchmarks. Notably, SYNC-CLIP outperforms the state-of-the-art\ncompetitor PromptSRC by an average improvement of 3.0% on novel classes across\n11 datasets in open-vocabulary scenarios.",
            "author": [
                "Mushui Liu",
                "Weijie He",
                "Ziqian Lu",
                "Yunlong Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03805v1",
                "http://arxiv.org/pdf/2312.03805v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03562v1",
            "title": "Enhancing Kinship Verification through Multiscale Retinex and Combined\n  Deep-Shallow features",
            "updated": "2023-12-06T15:52:31Z",
            "published": "2023-12-06T15:52:31Z",
            "summary": "The challenge of kinship verification from facial images represents a\ncutting-edge and formidable frontier in the realms of pattern recognition and\ncomputer vision. This area of study holds a myriad of potential applications,\nspanning from image annotation and forensic analysis to social media research.\nOur research stands out by integrating a preprocessing method named Multiscale\nRetinex (MSR), which elevates image quality and amplifies contrast, ultimately\nbolstering the end results. Strategically, our methodology capitalizes on the\nharmonious blend of deep and shallow texture descriptors, merging them\nproficiently at the score level through the Logistic Regression (LR) method. To\nelucidate, we employ the Local Phase Quantization (LPQ) descriptor to extract\nshallow texture characteristics. For deep feature extraction, we turn to the\nprowess of the VGG16 model, which is pre-trained on a convolutional neural\nnetwork (CNN). The robustness and efficacy of our method have been put to the\ntest through meticulous experiments on three rigorous kinship datasets, namely:\nCornell Kin Face, UB Kin Face, and TS Kin Face.",
            "author": [
                "El Ouanas Belabbaci",
                "Mohammed Khammari",
                "Ammar Chouchane",
                "Mohcene Bessaoudi",
                "Abdelmalik Ouamane",
                "Yassine Himeur",
                "Shadi Atalla",
                "Wathiq Mansoor"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03562v1",
                "http://arxiv.org/pdf/2312.03562v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03561v1",
            "title": "Blueprinting the Future: Automatic Item Categorization using\n  Hierarchical Zero-Shot and Few-Shot Classifiers",
            "updated": "2023-12-06T15:51:49Z",
            "published": "2023-12-06T15:51:49Z",
            "summary": "In testing industry, precise item categorization is pivotal to align exam\nquestions with the designated content domains outlined in the assessment\nblueprint. Traditional methods either entail manual classification, which is\nlaborious and error-prone, or utilize machine learning requiring extensive\ntraining data, often leading to model underfit or overfit issues. This study\nunveils a novel approach employing the zero-shot and few-shot Generative\nPretrained Transformer (GPT) classifier for hierarchical item categorization,\nminimizing the necessity for training data, and instead, leveraging human-like\nlanguage descriptions to define categories. Through a structured python\ndictionary, the hierarchical nature of examination blueprints is navigated\nseamlessly, allowing for a tiered classification of items across multiple\nlevels. An initial simulation with artificial data demonstrates the efficacy of\nthis method, achieving an average accuracy of 92.91% measured by the F1 score.\nThis method was further applied to real exam items from the 2022 In-Training\nExamination (ITE) conducted by the American Board of Family Medicine (ABFM),\nreclassifying 200 items according to a newly formulated blueprint swiftly in 15\nminutes, a task that traditionally could span several days among editors and\nphysicians. This innovative approach not only drastically cuts down\nclassification time but also ensures a consistent, principle-driven\ncategorization, minimizing human biases and discrepancies. The ability to\nrefine classifications by adjusting definitions adds to its robustness and\nsustainability.",
            "author": [
                "Ting Wang",
                "Keith Stelter",
                "Jenn Floyd",
                "Thomas O'Neill",
                "Nathaniel Hendrix",
                "Andrew Bazemore",
                "Kevin Rode",
                "Warren Newton"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03561v1",
                "http://arxiv.org/pdf/2312.03561v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03558v1",
            "title": "When an Image is Worth 1,024 x 1,024 Words: A Case Study in\n  Computational Pathology",
            "updated": "2023-12-06T15:40:28Z",
            "published": "2023-12-06T15:40:28Z",
            "summary": "This technical report presents LongViT, a vision Transformer that can process\ngigapixel images in an end-to-end manner. Specifically, we split the gigapixel\nimage into a sequence of millions of patches and project them linearly into\nembeddings. LongNet is then employed to model the extremely long sequence,\ngenerating representations that capture both short-range and long-range\ndependencies. The linear computation complexity of LongNet, along with its\ndistributed algorithm, enables us to overcome the constraints of both\ncomputation and memory. We apply LongViT in the field of computational\npathology, aiming for cancer diagnosis and prognosis within gigapixel\nwhole-slide images. Experimental results demonstrate that LongViT effectively\nencodes gigapixel images and outperforms previous state-of-the-art methods on\ncancer subtyping and survival prediction. Code and models will be available at\nhttps://aka.ms/LongViT.",
            "author": [
                "Wenhui Wang",
                "Shuming Ma",
                "Hanwen Xu",
                "Naoto Usuyama",
                "Jiayu Ding",
                "Hoifung Poon",
                "Furu Wei"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03558v1",
                "http://arxiv.org/pdf/2312.03558v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03556v1",
            "title": "Personalized Face Inpainting with Diffusion Models by Parallel Visual\n  Attention",
            "updated": "2023-12-06T15:39:03Z",
            "published": "2023-12-06T15:39:03Z",
            "summary": "Face inpainting is important in various applications, such as photo\nrestoration, image editing, and virtual reality. Despite the significant\nadvances in face generative models, ensuring that a person's unique facial\nidentity is maintained during the inpainting process is still an elusive goal.\nCurrent state-of-the-art techniques, exemplified by MyStyle, necessitate\nresource-intensive fine-tuning and a substantial number of images for each new\nidentity. Furthermore, existing methods often fall short in accommodating\nuser-specified semantic attributes, such as beard or expression. To improve\ninpainting results, and reduce the computational complexity during inference,\nthis paper proposes the use of Parallel Visual Attention (PVA) in conjunction\nwith diffusion models. Specifically, we insert parallel attention matrices to\neach cross-attention module in the denoising network, which attends to features\nextracted from reference images by an identity encoder. We train the added\nattention modules and identity encoder on CelebAHQ-IDI, a dataset proposed for\nidentity-preserving face inpainting. Experiments demonstrate that PVA attains\nunparalleled identity resemblance in both face inpainting and face inpainting\nwith language guidance tasks, in comparison to various benchmarks, including\nMyStyle, Paint by Example, and Custom Diffusion. Our findings reveal that PVA\nensures good identity preservation while offering effective\nlanguage-controllability. Additionally, in contrast to Custom Diffusion, PVA\nrequires just 40 fine-tuning steps for each new identity, which translates to a\nsignificant speed increase of over 20 times.",
            "author": [
                "Jianjin Xu",
                "Saman Motamed",
                "Praneetha Vaddamanu",
                "Chen Henry Wu",
                "Christian Haene",
                "Jean-Charles Bazin",
                "Fernando de la Torre"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03556v1",
                "http://arxiv.org/pdf/2312.03556v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03555v1",
            "title": "Enabling Edge Artificial Intelligence via Goal-oriented Deep Neural\n  Network Splitting",
            "updated": "2023-12-06T15:38:53Z",
            "published": "2023-12-06T15:38:53Z",
            "summary": "Deep Neural Network (DNN) splitting is one of the key enablers of edge\nArtificial Intelligence (AI), as it allows end users to pre-process data and\noffload part of the computational burden to nearby Edge Cloud Servers (ECSs).\nThis opens new opportunities and degrees of freedom in balancing energy\nconsumption, delay, accuracy, privacy, and other trustworthiness metrics. In\nthis work, we explore the opportunity of DNN splitting at the edge of 6G\nwireless networks to enable low energy cooperative inference with target delay\nand accuracy with a goal-oriented perspective. Going beyond the current\nliterature, we explore new trade-offs that take into account the accuracy\ndegradation as a function of the Splitting Point (SP) selection and wireless\nchannel conditions. Then, we propose an algorithm that dynamically controls SP\nselection, local computing resources, uplink transmit power and bandwidth\nallocation, in a goal-oriented fashion, to meet a target goal-effectiveness. To\nthe best of our knowledge, this is the first work proposing adaptive SP\nselection on the basis of all learning performance (i.e., energy, delay,\naccuracy), with the aim of guaranteeing the accomplishment of a goal (e.g.,\nminimize the energy consumption under latency and accuracy constraints).\nNumerical results show the advantages of the proposed SP selection and resource\nallocation, to enable energy frugal and effective edge AI.",
            "author": [
                "Francesco Binucci",
                "Mattia Merluzzi",
                "Paolo Banelli",
                "Emilio Calvanese Strinati",
                "Paolo Di Lorenzo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03555v1",
                "http://arxiv.org/pdf/2312.03555v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03804v1",
            "title": "How Low Can You Go? Surfacing Prototypical In-Distribution Samples for\n  Unsupervised Anomaly Detection",
            "updated": "2023-12-06T15:30:47Z",
            "published": "2023-12-06T15:30:47Z",
            "summary": "Unsupervised anomaly detection (UAD) alleviates large labeling efforts by\ntraining exclusively on unlabeled in-distribution data and detecting outliers\nas anomalies. Generally, the assumption prevails that large training datasets\nallow the training of higher-performing UAD models. However, in this work, we\nshow that using only very few training samples can already match - and in some\ncases even improve - anomaly detection compared to training with the whole\ntraining dataset. We propose three methods to identify prototypical samples\nfrom a large dataset of in-distribution samples. We demonstrate that by\ntraining with a subset of just ten such samples, we achieve an area under the\nreceiver operating characteristics curve (AUROC) of $96.37 \\%$ on CIFAR10,\n$92.59 \\%$ on CIFAR100, $95.37 \\%$ on MNIST, $95.38 \\%$ on Fashion-MNIST,\n$96.37 \\%$ on MVTec-AD, $98.81 \\%$ on BraTS, and $81.95 \\%$ on RSNA pneumonia\ndetection, even exceeding the performance of full training in $25/67$ classes\nwe tested. Additionally, we show that the prototypical in-distribution samples\nidentified by our proposed methods translate well to different models and other\ndatasets and that using their characteristics as guidance allows for successful\nmanual selection of small subsets of high-performing samples. Our code is\navailable at https://anonymous.4open.science/r/uad_prototypical_samples/",
            "author": [
                "Felix Meissen",
                "Johannes Getzner",
                "Alexander Ziller",
                "Georgios Kaissis",
                "Daniel Rueckert"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03804v1",
                "http://arxiv.org/pdf/2312.03804v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03549v2",
            "title": "Holmes: Towards Distributed Training Across Clusters with Heterogeneous\n  NIC Environment",
            "updated": "2023-12-07T09:26:07Z",
            "published": "2023-12-06T15:27:26Z",
            "summary": "Large language models (LLMs) such as GPT-3, OPT, and LLaMA have demonstrated\nremarkable accuracy in a wide range of tasks. However, training these models\ncan incur significant expenses, often requiring tens of thousands of GPUs for\nmonths of continuous operation. Typically, this training is carried out in\nspecialized GPU clusters equipped with homogeneous high-speed Remote Direct\nMemory Access (RDMA) network interface cards (NICs). The acquisition and\nmaintenance of such dedicated clusters is challenging. Current LLM training\nframeworks, like Megatron-LM and Megatron-DeepSpeed, focus primarily on\noptimizing training within homogeneous cluster settings. In this paper, we\nintroduce Holmes, a training framework for LLMs that employs thoughtfully\ncrafted data and model parallelism strategies over the heterogeneous NIC\nenvironment. Our primary technical contribution lies in a novel scheduling\nmethod that intelligently allocates distinct computational tasklets in LLM\ntraining to specific groups of GPU devices based on the characteristics of\ntheir connected NICs. Furthermore, our proposed framework, utilizing pipeline\nparallel techniques, demonstrates scalability to multiple GPU clusters, even in\nscenarios without high-speed interconnects between nodes in distinct clusters.\nWe conducted comprehensive experiments that involved various scenarios in the\nheterogeneous NIC environment. In most cases, our framework achieves\nperformance levels close to those achievable with homogeneous RDMA-capable\nnetworks (InfiniBand or RoCE), significantly exceeding training efficiency\nwithin the pure Ethernet environment. Additionally, we verified that our\nframework outperforms other mainstream LLM frameworks under heterogeneous NIC\nenvironment in terms of training efficiency and can be seamlessly integrated\nwith them.",
            "author": [
                "Fei Yang",
                "Shuang Peng",
                "Ning Sun",
                "Fangyu Wang",
                "Ke Tan",
                "Fu Wu",
                "Jiezhong Qiu",
                "Aimin Pan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03549v2",
                "http://arxiv.org/pdf/2312.03549v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03548v1",
            "title": "Texture-Semantic Collaboration Network for ORSI Salient Object Detection",
            "updated": "2023-12-06T15:26:38Z",
            "published": "2023-12-06T15:26:38Z",
            "summary": "Salient object detection (SOD) in optical remote sensing images (ORSIs) has\nbecome increasingly popular recently. Due to the characteristics of ORSIs,\nORSI-SOD is full of challenges, such as multiple objects, small objects, low\nilluminations, and irregular shapes. To address these challenges, we propose a\nconcise yet effective Texture-Semantic Collaboration Network (TSCNet) to\nexplore the collaboration of texture cues and semantic cues for ORSI-SOD.\nSpecifically, TSCNet is based on the generic encoder-decoder structure. In\naddition to the encoder and decoder, TSCNet includes a vital Texture-Semantic\nCollaboration Module (TSCM), which performs valuable feature modulation and\ninteraction on basic features extracted from the encoder. The main idea of our\nTSCM is to make full use of the texture features at the lowest level and the\nsemantic features at the highest level to achieve the expression enhancement of\nsalient regions on features. In the TSCM, we first enhance the position of\npotential salient regions using semantic features. Then, we render and restore\nthe object details using the texture features. Meanwhile, we also perceive\nregions of various scales, and construct interactions between different\nregions. Thanks to the perfect combination of TSCM and generic structure, our\nTSCNet can take care of both the position and details of salient objects,\neffectively handling various scenes. Extensive experiments on three datasets\ndemonstrate that our TSCNet achieves competitive performance compared to 14\nstate-of-the-art methods. The code and results of our method are available at\nhttps://github.com/MathLee/TSCNet.",
            "author": [
                "Gongyang Li",
                "Zhen Bai",
                "Zhi Liu"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TCSII.2023.3333436",
                "http://arxiv.org/abs/2312.03548v1",
                "http://arxiv.org/pdf/2312.03548v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03801v1",
            "title": "Generalization to New Sequential Decision Making Tasks with In-Context\n  Learning",
            "updated": "2023-12-06T15:19:28Z",
            "published": "2023-12-06T15:19:28Z",
            "summary": "Training autonomous agents that can learn new tasks from only a handful of\ndemonstrations is a long-standing problem in machine learning. Recently,\ntransformers have been shown to learn new language or vision tasks without any\nweight updates from only a few examples, also referred to as in-context\nlearning. However, the sequential decision making setting poses additional\nchallenges having a lower tolerance for errors since the environment's\nstochasticity or the agent's actions can lead to unseen, and sometimes\nunrecoverable, states. In this paper, we use an illustrative example to show\nthat naively applying transformers to sequential decision making problems does\nnot enable in-context learning of new tasks. We then demonstrate how training\non sequences of trajectories with certain distributional properties leads to\nin-context learning of new sequential decision making tasks. We investigate\ndifferent design choices and find that larger model and dataset sizes, as well\nas more task diversity, environment stochasticity, and trajectory burstiness,\nall result in better in-context learning of new out-of-distribution tasks. By\ntraining on large diverse offline datasets, our model is able to learn new\nMiniHack and Procgen tasks without any weight updates from just a handful of\ndemonstrations.",
            "author": [
                "Sharath Chandra Raparthy",
                "Eric Hambro",
                "Robert Kirk",
                "Mikael Henaff",
                "Roberta Raileanu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03801v1",
                "http://arxiv.org/pdf/2312.03801v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03543v1",
            "title": "GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging\n  Cross-Modal Attention with Large Language Models",
            "updated": "2023-12-06T15:14:30Z",
            "published": "2023-12-06T15:14:30Z",
            "summary": "In the field of autonomous vehicles (AVs), accurately discerning commander\nintent and executing linguistic commands within a visual context presents a\nsignificant challenge. This paper introduces a sophisticated encoder-decoder\nframework, developed to address visual grounding in AVs.Our Context-Aware\nVisual Grounding (CAVG) model is an advanced system that integrates five core\nencoders-Text, Image, Context, and Cross-Modal-with a Multimodal decoder. This\nintegration enables the CAVG model to adeptly capture contextual semantics and\nto learn human emotional features, augmented by state-of-the-art Large Language\nModels (LLMs) including GPT-4. The architecture of CAVG is reinforced by the\nimplementation of multi-head cross-modal attention mechanisms and a\nRegion-Specific Dynamic (RSD) layer for attention modulation. This\narchitectural design enables the model to efficiently process and interpret a\nrange of cross-modal inputs, yielding a comprehensive understanding of the\ncorrelation between verbal commands and corresponding visual scenes. Empirical\nevaluations on the Talk2Car dataset, a real-world benchmark, demonstrate that\nCAVG establishes new standards in prediction accuracy and operational\nefficiency. Notably, the model exhibits exceptional performance even with\nlimited training data, ranging from 50% to 75% of the full dataset. This\nfeature highlights its effectiveness and potential for deployment in practical\nAV applications. Moreover, CAVG has shown remarkable robustness and\nadaptability in challenging scenarios, including long-text command\ninterpretation, low-light conditions, ambiguous command contexts, inclement\nweather conditions, and densely populated urban environments. The code for the\nproposed model is available at our Github.",
            "author": [
                "Haicheng Liao",
                "Huanming Shen",
                "Zhenning Li",
                "Chengyue Wang",
                "Guofa Li",
                "Yiming Bie",
                "Chengzhong Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03543v1",
                "http://arxiv.org/pdf/2312.03543v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03542v1",
            "title": "Incorporating the algorithm for the boundary condition from FVM into the\n  framework of Eulerian SPH",
            "updated": "2023-12-06T15:12:21Z",
            "published": "2023-12-06T15:12:21Z",
            "summary": "Finite volume method (FVM) is a widely used mesh-based technique, renowned\nfor its computational efficiency and accuracy but it bears significant\ndrawbacks, particularly in mesh generation and handling complex boundary\ninterfaces or conditions. On the other hand, smoothed particle hydrodynamics\n(SPH) method, a popular meshless alternative, inherently circumvents the mesh\ngeneration and yields smoother numerical outcomes but at the expense of\ncomputational efficiency. Therefore, numerous researchers have strategically\namalgamated the strengths of both methods to investigate complex flow phenomena\nand this synergy has yielded precise and computationally efficient outcomes.\nHowever, algorithms involving the weak coupling of these two methods tend to be\nintricate, which has issues pertaining to versatility, implementation, and\nmutual adaptation to hardware and coding structures. Thus, achieving a robust\nand strong coupling of FVM and SPH in a unified framework is imperative. Due to\ndiffering boundary algorithms between these methods in Wang's work, the crucial\nstep for establishing a strong coupling of both methods within a unified SPH\nframework lies in incorporating the FVM boundary algorithm into the Eulerian\nSPH method. In this paper, we propose a straightforward algorithm in the\nEulerian SPH method, algorithmically equivalent to that in FVM, grounded in the\nprinciple of zero-order consistency. Moreover, several numerical examples,\nincluding fully and weakly compressible flows with various boundary conditions\nin the Eulerian SPH method, validate the stability and accuracy of the proposed\nalgorithm.",
            "author": [
                "Zhentong Wang",
                "Oskar J. Haidn",
                "Xiangyu Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03542v1",
                "http://arxiv.org/pdf/2312.03542v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03540v1",
            "title": "FoodFusion: A Latent Diffusion Model for Realistic Food Image Generation",
            "updated": "2023-12-06T15:07:12Z",
            "published": "2023-12-06T15:07:12Z",
            "summary": "Current state-of-the-art image generation models such as Latent Diffusion\nModels (LDMs) have demonstrated the capacity to produce visually striking\nfood-related images. However, these generated images often exhibit an artistic\nor surreal quality that diverges from the authenticity of real-world food\nrepresentations. This inadequacy renders them impractical for applications\nrequiring realistic food imagery, such as training models for image-based\ndietary assessment. To address these limitations, we introduce FoodFusion, a\nLatent Diffusion model engineered specifically for the faithful synthesis of\nrealistic food images from textual descriptions. The development of the\nFoodFusion model involves harnessing an extensive array of open-source food\ndatasets, resulting in over 300,000 curated image-caption pairs. Additionally,\nwe propose and employ two distinct data cleaning methodologies to ensure that\nthe resulting image-text pairs maintain both realism and accuracy. The\nFoodFusion model, thus trained, demonstrates a remarkable ability to generate\nfood images that exhibit a significant improvement in terms of both realism and\ndiversity over the publicly available image generation models. We openly share\nthe dataset and fine-tuned models to support advancements in this critical\nfield of food image synthesis at https://bit.ly/genai4good.",
            "author": [
                "Olivia Markham",
                "Yuhao Chen",
                "Chi-en Amy Tai",
                "Alexander Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03540v1",
                "http://arxiv.org/pdf/2312.03540v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03538v1",
            "title": "Bayesian variable selection in sample selection models using\n  spike-and-slab priors",
            "updated": "2023-12-06T15:01:47Z",
            "published": "2023-12-06T15:01:47Z",
            "summary": "Sample selection models represent a common methodology for correcting bias\ninduced by data missing not at random. It is well known that these models are\nnot empirically identifiable without exclusion restrictions. In other words,\nsome variables predictive of missingness do not affect the outcome model of\ninterest. The drive to establish this requirement often leads to the inclusion\nof irrelevant variables in the model. A recent proposal uses adaptive LASSO to\ncircumvent this problem, but its performance depends on the so-called\ncovariance assumption, which can be violated in small to moderate samples.\nAdditionally, there are no tools yet for post-selection inference for this\nmodel. To address these challenges, we propose two families of spike-and-slab\npriors to conduct Bayesian variable selection in sample selection models. These\nprior structures allow for constructing a Gibbs sampler with tractable\nconditionals, which is scalable to the dimensions of practical interest. We\nillustrate the performance of the proposed methodology through a simulation\nstudy and present a comparison against adaptive LASSO and stepwise selection.\nWe also provide two applications using publicly available real data. An\nimplementation and code to reproduce the results in this paper can be found at\nhttps://github.com/adam-iqbal/selection-spike-slab",
            "author": [
                "Adam Iqbal",
                "Emmanuel Ogundimu",
                "F. Javier Rubio"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03538v1",
                "http://arxiv.org/pdf/2312.03538v1"
            ],
            "primary_category": "stat.CO",
            "category": [
                "stat.CO",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03799v1",
            "title": "Low-power, Continuous Remote Behavioral Localization with Event Cameras",
            "updated": "2023-12-06T14:58:03Z",
            "published": "2023-12-06T14:58:03Z",
            "summary": "Researchers in natural science need reliable methods for quantifying animal\nbehavior. Recently, numerous computer vision methods emerged to automate the\nprocess. However, observing wild species at remote locations remains a\nchallenging task due to difficult lighting conditions and constraints on power\nsupply and data storage. Event cameras offer unique advantages for\nbattery-dependent remote monitoring due to their low power consumption and high\ndynamic range capabilities. We use this novel sensor to quantify a behavior in\nChinstrap penguins called ecstatic display. We formulate the problem as a\ntemporal action detection task, determining the start and end times of the\nbehavior. For this purpose, we recorded a colony of breeding penguins in\nAntarctica during several weeks and labeled event data on 16 nests. The\ndeveloped method consists of a generator of candidate time intervals\n(proposals) and a classifier of the actions within them. The experiments show\nthat the event cameras' natural response to motion is effective for continuous\nbehavior monitoring and detection, reaching a mean average precision (mAP) of\n58% (which increases to 63% in good weather conditions). The results also\ndemonstrate the robustness against various lighting conditions contained in the\nchallenging dataset. The low-power capabilities of the event camera allows to\nrecord three times longer than with a conventional camera. This work pioneers\nthe use of event cameras for remote wildlife observation, opening new\ninterdisciplinary opportunities. https://tub-rip.github.io/eventpenguins/",
            "author": [
                "Friedhelm Hamann",
                "Suman Ghosh",
                "Ignacio Juarez Martinez",
                "Tom Hart",
                "Alex Kacelnik",
                "Guillermo Gallego"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03799v1",
                "http://arxiv.org/pdf/2312.03799v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03534v1",
            "title": "Validation and benchmarking of quantum annealing technology",
            "updated": "2023-12-06T14:56:45Z",
            "published": "2023-12-06T14:56:45Z",
            "summary": "In this thesis, we focus on the problem of validating and benchmarking\nquantum annealers. To this end, we propose two algorithms for solving\nreal-world problems and test how they perform on the current generation of\nquantum annealers. The first algorithm allows for solving the dynamics of\nquantum systems (or, in fact, any dynamical systems). The second of the\nproposed algorithms is suitable for solving a particular family of railway\ndispatching problems. We assess the performance of those algorithms on the\ncurrent generation of D-Wave quantum annealers with the assistance of two\nnovel, classical strategies for solving an Ising model also presented in the\nthesis. The first, tensor network-based approach is a heuristic algorithm\ntailored for solving instances defined on Chimera-like graphs, thus making it\nideal for providing a baseline with which the results from physical annealers\ncan be compared. The other presented approach is a massively parallel\nimplementation of the exhaustive (brute-force) search through the whole\nsolution space. Although the brute-force approach is limited to moderate\ninstance sizes, it has the advantage of being able to compute the low energy\nspectrum and certify the solutions. Our results suggest that present-day\nquantum annealers are able to solve a subset of the aforementioned problems. In\nparticular, we show that the D-Wave annealers are capable of capturing the\ndynamics of a simple quantum system in a specific regime of parameters, and can\nbe used to obtain good-quality solutions for instances of railway conflict\nmanagement problems. Finally, our findings indicate that the current generation\nof D-Wave annealers is far from perfect. We discuss problem instances for which\nthe annealers failed to find a good or even feasible solution. We also provide,\nwhere possible, a plausible explanation of why some of the presented problems\nmight be hard for the annealers.",
            "author": [
                "Konrad Ja\u0142owiecki"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03534v1",
                "http://arxiv.org/pdf/2312.03534v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03533v1",
            "title": "Low-shot Object Learning with Mutual Exclusivity Bias",
            "updated": "2023-12-06T14:54:10Z",
            "published": "2023-12-06T14:54:10Z",
            "summary": "This paper introduces Low-shot Object Learning with Mutual Exclusivity Bias\n(LSME), the first computational framing of mutual exclusivity bias, a\nphenomenon commonly observed in infants during word learning. We provide a\nnovel dataset, comprehensive baselines, and a state-of-the-art method to enable\nthe ML community to tackle this challenging learning task. The goal of LSME is\nto analyze an RGB image of a scene containing multiple objects and correctly\nassociate a previously-unknown object instance with a provided category label.\nThis association is then used to perform low-shot learning to test category\ngeneralization. We provide a data generation pipeline for the LSME problem and\nconduct a thorough analysis of the factors that contribute to its difficulty.\nAdditionally, we evaluate the performance of multiple baselines, including\nstate-of-the-art foundation models. Finally, we present a baseline approach\nthat outperforms state-of-the-art models in terms of low-shot accuracy.",
            "author": [
                "Anh Thai",
                "Ahmad Humayun",
                "Stefan Stojanov",
                "Zixuan Huang",
                "Bikram Boote",
                "James M. Rehg"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03533v1",
                "http://arxiv.org/pdf/2312.03533v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03798v1",
            "title": "Single Image Reflection Removal with Reflection Intensity Prior\n  Knowledge",
            "updated": "2023-12-06T14:52:11Z",
            "published": "2023-12-06T14:52:11Z",
            "summary": "Single Image Reflection Removal (SIRR) in real-world images is a challenging\ntask due to diverse image degradations occurring on the glass surface during\nlight transmission and reflection. Many existing methods rely on specific prior\nassumptions to resolve the problem. In this paper, we propose a general\nreflection intensity prior that captures the intensity of the reflection\nphenomenon and demonstrate its effectiveness. To learn the reflection intensity\nprior, we introduce the Reflection Prior Extraction Network (RPEN). By\nsegmenting images into regional patches, RPEN learns non-uniform reflection\nprior in an image. We propose Prior-based Reflection Removal Network (PRRN)\nusing a simple transformer U-Net architecture that adapts reflection prior fed\nfrom RPEN. Experimental results on real-world benchmarks demonstrate the\neffectiveness of our approach achieving state-of-the-art accuracy in SIRR.",
            "author": [
                "Dongshen Han",
                "Seungkyu Lee",
                "Chaoning Zhang",
                "Heechan Yoon",
                "Hyukmin Kwon",
                "HyunCheol Kim",
                "HyonGon Choo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03798v1",
                "http://arxiv.org/pdf/2312.03798v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03528v1",
            "title": "Personalized Pose Forecasting",
            "updated": "2023-12-06T14:43:38Z",
            "published": "2023-12-06T14:43:38Z",
            "summary": "Human pose forecasting is the task of predicting articulated human motion\ngiven past human motion. There exists a number of popular benchmarks that\nevaluate an array of different models performing human pose forecasting. These\nbenchmarks do not reflect that a human interacting system, such as a delivery\nrobot, observes and plans for the motion of the same individual over an\nextended period of time. Every individual has unique and distinct movement\npatterns. This is however not reflected in existing benchmarks that evaluate a\nmodel's ability to predict an average human's motion rather than a particular\nindividual's. We reformulate the human motion forecasting problem and present a\nmodel-agnostic personalization method. Motion forecasting personalization can\nbe performed efficiently online by utilizing a low-parametric time-series\nanalysis model that personalizes neural network pose predictions.",
            "author": [
                "Maria Priisalu",
                "Ted Kronvall",
                "Cristian Sminchisescu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03528v1",
                "http://arxiv.org/pdf/2312.03528v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03526v1",
            "title": "On the Diversity and Realism of Distilled Dataset: An Efficient Dataset\n  Distillation Paradigm",
            "updated": "2023-12-06T14:40:05Z",
            "published": "2023-12-06T14:40:05Z",
            "summary": "Contemporary machine learning requires training large neural networks on\nmassive datasets and thus faces the challenges of high computational demands.\nDataset distillation, as a recent emerging strategy, aims to compress\nreal-world datasets for efficient training. However, this line of research\ncurrently struggle with large-scale and high-resolution datasets, hindering its\npracticality and feasibility. To this end, we re-examine the existing dataset\ndistillation methods and identify three properties required for large-scale\nreal-world applications, namely, realism, diversity, and efficiency. As a\nremedy, we propose RDED, a novel computationally-efficient yet effective data\ndistillation paradigm, to enable both diversity and realism of the distilled\ndata. Extensive empirical results over various neural architectures and\ndatasets demonstrate the advancement of RDED: we can distill the full\nImageNet-1K to a small dataset comprising 10 images per class within 7 minutes,\nachieving a notable 42% top-1 accuracy with ResNet-18 on a single RTX-4090 GPU\n(while the SOTA only achieves 21% but requires 6 hours).",
            "author": [
                "Peng Sun",
                "Bei Shi",
                "Daiwei Yu",
                "Tao Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03526v1",
                "http://arxiv.org/pdf/2312.03526v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03523v1",
            "title": "Sig-Networks Toolkit: Signature Networks for Longitudinal Language\n  Modelling",
            "updated": "2023-12-06T14:34:30Z",
            "published": "2023-12-06T14:34:30Z",
            "summary": "We present an open-source, pip installable toolkit, Sig-Networks, the first\nof its kind for longitudinal language modelling. A central focus is the\nincorporation of Signature-based Neural Network models, which have recently\nshown success in temporal tasks. We apply and extend published research\nproviding a full suite of signature-based models. Their components can be used\nas PyTorch building blocks in future architectures. Sig-Networks enables\ntask-agnostic dataset plug-in, seamless pre-processing for sequential data,\nparameter flexibility, automated tuning across a range of models. We examine\nsignature networks under three different NLP tasks of varying temporal\ngranularity: counselling conversations, rumour stance switch and mood changes\nin social media threads, showing SOTA performance in all three, and provide\nguidance for future tasks. We release the Toolkit as a PyTorch package with an\nintroductory video, Git repositories for preprocessing and modelling including\nsample notebooks on the modeled NLP tasks.",
            "author": [
                "Talia Tseriotou",
                "Ryan Sze-Yin Chan",
                "Adam Tsakalidis",
                "Iman Munire Bilal",
                "Elena Kochkina",
                "Terry Lyons",
                "Maria Liakata"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03523v1",
                "http://arxiv.org/pdf/2312.03523v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03521v1",
            "title": "Optimal Wildfire Escape Route Planning for Drones under Dynamic Fire and\n  Smoke",
            "updated": "2023-12-06T14:30:15Z",
            "published": "2023-12-06T14:30:15Z",
            "summary": "In recent years, the increasing prevalence and intensity of wildfires have\nposed significant challenges to emergency response teams. The utilization of\nunmanned aerial vehicles (UAVs), commonly known as drones, has shown promise in\naiding wildfire management efforts. This work focuses on the development of an\noptimal wildfire escape route planning system specifically designed for drones,\nconsidering dynamic fire and smoke models. First, the location of the source of\nthe wildfire can be well located by information fusion between UAV and\nsatellite, and the road conditions in the vicinity of the fire can be assessed\nand analyzed using multi-channel remote sensing data. Second, the road network\ncan be extracted and segmented in real time using UAV vision technology, and\neach road in the road network map can be given priority based on the results of\nroad condition classification. Third, the spread model of dynamic fires\ncalculates the new location of the fire source based on the fire intensity,\nwind speed and direction, and the radius increases as the wildfire spreads.\nSmoke is generated around the fire source to create a visual representation of\na burning fire. Finally, based on the improved A* algorithm, which considers\nall the above factors, the UAV can quickly plan an escape route based on the\nstarting and destination locations that avoid the location of the fire source\nand the area where it is spreading. By considering dynamic fire and smoke\nmodels, the proposed system enhances the safety and efficiency of drone\noperations in wildfire environments.",
            "author": [
                "Chang Liu",
                "Tamas Sziranyi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03521v1",
                "http://arxiv.org/pdf/2312.03521v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03520v1",
            "title": "Defense Against Adversarial Attacks using Convolutional Auto-Encoders",
            "updated": "2023-12-06T14:29:16Z",
            "published": "2023-12-06T14:29:16Z",
            "summary": "Deep learning models, while achieving state-of-the-art performance on many\ntasks, are susceptible to adversarial attacks that exploit inherent\nvulnerabilities in their architectures. Adversarial attacks manipulate the\ninput data with imperceptible perturbations, causing the model to misclassify\nthe data or produce erroneous outputs. This work is based on enhancing the\nrobustness of targeted classifier models against adversarial attacks. To\nachieve this, an convolutional autoencoder-based approach is employed that\neffectively counters adversarial perturbations introduced to the input images.\nBy generating images closely resembling the input images, the proposed\nmethodology aims to restore the model's accuracy.",
            "author": [
                "Shreyasi Mandal"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03520v1",
                "http://arxiv.org/pdf/2312.03520v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "I.4.5; I.5.1; I.5.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03519v1",
            "title": "Active Wildfires Detection and Dynamic Escape Routes Planning for Humans\n  through Information Fusion between Drones and Satellites",
            "updated": "2023-12-06T14:25:47Z",
            "published": "2023-12-06T14:25:47Z",
            "summary": "UAVs are playing an increasingly important role in the field of wilderness\nrescue by virtue of their flexibility. This paper proposes a fusion of UAV\nvision technology and satellite image analysis technology for active wildfires\ndetection and road networks extraction of wildfire areas and real-time dynamic\nescape route planning for people in distress. Firstly, the fire source location\nand the segmentation of smoke and flames are targeted based on Sentinel 2\nsatellite imagery. Secondly, the road segmentation and the road condition\nassessment are performed by D-linkNet and NDVI values in the central area of\nthe fire source by UAV. Finally, the dynamic optimal route planning for humans\nin real time is performed by the weighted A* algorithm in the road network with\nthe dynamic fire spread model. Taking the Chongqing wildfire on August 24,\n2022, as a case study, the results demonstrate that the dynamic escape route\nplanning algorithm can provide an optimal real-time navigation path for humans\nin the presence of fire through the information fusion of UAVs and satellites.",
            "author": [
                "Chang Liu",
                "Tamas Sziranyi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03519v1",
                "http://arxiv.org/pdf/2312.03519v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03517v1",
            "title": "FRDiff: Feature Reuse for Exquisite Zero-shot Acceleration of Diffusion\n  Models",
            "updated": "2023-12-06T14:24:26Z",
            "published": "2023-12-06T14:24:26Z",
            "summary": "The substantial computational costs of diffusion models, particularly due to\nthe repeated denoising steps crucial for high-quality image generation, present\na major obstacle to their widespread adoption. While several studies have\nattempted to address this issue by reducing the number of score function\nevaluations using advanced ODE solvers without fine-tuning, the decreased\nnumber of denoising iterations misses the opportunity to update fine details,\nresulting in noticeable quality degradation. In our work, we introduce an\nadvanced acceleration technique that leverages the temporal redundancy inherent\nin diffusion models. Reusing feature maps with high temporal similarity opens\nup a new opportunity to save computation without sacrificing output quality. To\nrealize the practical benefits of this intuition, we conduct an extensive\nanalysis and propose a novel method, FRDiff. FRDiff is designed to harness the\nadvantages of both reduced NFE and feature reuse, achieving a Pareto frontier\nthat balances fidelity and latency trade-offs in various generative tasks.",
            "author": [
                "Junhyuk So",
                "Jungwon Lee",
                "Eunhyeok Park"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03517v1",
                "http://arxiv.org/pdf/2312.03517v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03516v1",
            "title": "Clustering by Contour coreset and variational quantum eigensolver",
            "updated": "2023-12-06T14:21:17Z",
            "published": "2023-12-06T14:21:17Z",
            "summary": "Recent work has proposed solving the k-means clustering problem on quantum\ncomputers via the Quantum Approximate Optimization Algorithm (QAOA) and coreset\ntechniques. Although the current method demonstrates the possibility of quantum\nk-means clustering, it does not ensure high accuracy and consistency across a\nwide range of datasets. The existing coreset techniques are designed for\nclassical algorithms and there has been no quantum-tailored coreset technique\nwhich is designed to boost the accuracy of quantum algorithms. In this work, we\npropose solving the k-means clustering problem with the variational quantum\neigensolver (VQE) and a customised coreset method, the Contour coreset, which\nhas been formulated with specific focus on quantum algorithms. Extensive\nsimulations with synthetic and real-life data demonstrated that our VQE+Contour\nCoreset approach outperforms existing QAOA+Coreset k-means clustering\napproaches with higher accuracy and lower standard deviation. Our work has\nshown that quantum tailored coreset techniques has the potential to\nsignificantly boost the performance of quantum algorithms when compared to\nusing generic off-the-shelf coreset techniques.",
            "author": [
                "Canaan Yung",
                "Muhammad Usman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03516v1",
                "http://arxiv.org/pdf/2312.03516v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03515v1",
            "title": "The Hadamard gate cannot be replaced by a resource state in universal\n  quantum computation",
            "updated": "2023-12-06T14:19:02Z",
            "published": "2023-12-06T14:19:02Z",
            "summary": "We consider models of quantum computation that involve operations performed\non some fixed resourceful quantum state. Examples that fit this paradigm\ninclude magic state injection and measurement-based approaches. We introduce a\nframework that incorporates both of these cases and focus on the role of\ncoherence (or superposition) in this context, as exemplified through the\nHadamard gate. We prove that given access to incoherent unitaries (those that\nare unable to generate superposition from computational basis states, e.g.\nCNOT, diagonal gates), classical control, computational basis measurements, and\nany resourceful ancillary state (of arbitrary dimension), it is not possible to\nimplement any coherent unitary (e.g. Hadamard) exactly with non-zero\nprobability. We also consider the approximate case by providing lower bounds\nfor the induced trace distance between the above operations and $n$ Hadamard\ngates. To demonstrate the stability of this result, this is then extended to a\nsimilar no-go result for the case of using $k$ Hadamard gates to exactly\nimplement $n>k$ Hadamard gates.",
            "author": [
                "Benjamin D. M. Jones",
                "Paul Skrzypczyk",
                "Noah Linden"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03515v1",
                "http://arxiv.org/pdf/2312.03515v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03795v1",
            "title": "AnimatableDreamer: Text-Guided Non-rigid 3D Model Generation and\n  Reconstruction with Canonical Score Distillation",
            "updated": "2023-12-06T14:13:54Z",
            "published": "2023-12-06T14:13:54Z",
            "summary": "Text-to-3D model adaptations have advanced static 3D model quality, but\nsequential 3D model generation, particularly for animatable objects with large\nmotions, is still scarce. Our work proposes AnimatableDreamer, a text-to-4D\ngeneration framework capable of generating diverse categories of non-rigid\nobjects while adhering to the object motions extracted from a monocular video.\nAt its core, AnimatableDreamer is equipped with our novel optimization design\ndubbed Canonical Score Distillation (CSD), which simplifies the generation\ndimension from 4D to 3D by denoising over different frames in the time-varying\ncamera spaces while conducting the distillation process in a unique canonical\nspace shared per video. Concretely, CSD ensures that score gradients\nback-propagate to the canonical space through differentiable warping, hence\nguaranteeing the time-consistent generation and maintaining morphological\nplausibility across different poses. By lifting the 3D generator to 4D with\nwarping functions, AnimatableDreamer offers a novel perspective on non-rigid 3D\nmodel generation and reconstruction. Besides, with inductive knowledge from a\nmulti-view consistent diffusion model, CSD regularizes reconstruction from\nnovel views, thus cyclically enhancing the generation process. Extensive\nexperiments demonstrate the capability of our method in generating\nhigh-flexibility text-guided 3D models from the monocular video, while also\nshowing improved reconstruction performance over typical non-rigid\nreconstruction methods. Project page https://AnimatableDreamer.github.io.",
            "author": [
                "Xinzhou Wang",
                "Yikai Wang",
                "Junliang Ye",
                "Zhengyi Wang",
                "Fuchun Sun",
                "Pengkun Liu",
                "Ling Wang",
                "Kai Sun",
                "Xintong Wang",
                "Bin He"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03795v1",
                "http://arxiv.org/pdf/2312.03795v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "I.4.5"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03511v1",
            "title": "Kandinsky 3.0 Technical Report",
            "updated": "2023-12-06T14:13:38Z",
            "published": "2023-12-06T14:13:38Z",
            "summary": "We present Kandinsky 3.0, a large-scale text-to-image generation model based\non latent diffusion, continuing the series of text-to-image Kandinsky models\nand reflecting our progress to achieve higher quality and realism of image\ngeneration. Compared to previous versions of Kandinsky 2.x, Kandinsky 3.0\nleverages a two times larger U-Net backbone, a ten times larger text encoder\nand removes diffusion mapping. We describe the architecture of the model, the\ndata collection procedure, the training technique, and the production system of\nuser interaction. We focus on the key components that, as we have identified as\na result of a large number of experiments, had the most significant impact on\nimproving the quality of our model compared to the others. By our side-by-side\ncomparisons, Kandinsky becomes better in text understanding and works better on\nspecific domains. Project page: https://ai-forever.github.io/Kandinsky-3",
            "author": [
                "Vladimir Arkhipkin",
                "Andrei Filatov",
                "Viacheslav Vasilev",
                "Anastasia Maltseva",
                "Said Azizov",
                "Igor Pavlov",
                "Julia Agafonova",
                "Andrey Kuznetsov",
                "Denis Dimitrov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03511v1",
                "http://arxiv.org/pdf/2312.03511v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03510v2",
            "title": "Towards Sobolev Pruning",
            "updated": "2023-12-07T10:38:56Z",
            "published": "2023-12-06T14:13:30Z",
            "summary": "The increasing use of stochastic models for describing complex phenomena\nwarrants surrogate models that capture the reference model characteristics at a\nfraction of the computational cost, foregoing potentially expensive Monte Carlo\nsimulation. The predominant approach of fitting a large neural network and then\npruning it to a reduced size has commonly neglected shortcomings. The produced\nsurrogate models often will not capture the sensitivities and uncertainties\ninherent in the original model. In particular, (higher-order) derivative\ninformation of such surrogates could differ drastically. Given a large enough\nnetwork, we expect this derivative information to match. However, the pruned\nmodel will almost certainly not share this behavior.\n  In this paper, we propose to find surrogate models by using sensitivity\ninformation throughout the learning and pruning process. We build on work using\nInterval Adjoint Significance Analysis for pruning and combine it with the\nrecent advancements in Sobolev Training to accurately model the original\nsensitivity information in the pruned neural network based surrogate model. We\nexperimentally underpin the method on an example of pricing a multidimensional\nBasket option modelled through a stochastic differential equation with Brownian\nmotion. The proposed method is, however, not limited to the domain of\nquantitative finance, which was chosen as a case study for intuitive\ninterpretations of the sensitivities. It serves as a foundation for building\nfurther surrogate modelling techniques considering sensitivity information.",
            "author": [
                "Neil Kichler",
                "Sher Afghan",
                "Uwe Naumann"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03510v2",
                "http://arxiv.org/pdf/2312.03510v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-fin.CP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03509v1",
            "title": "Gravitational cell detection and tracking in fluorescence microscopy\n  data",
            "updated": "2023-12-06T14:08:05Z",
            "published": "2023-12-06T14:08:05Z",
            "summary": "Automatic detection and tracking of cells in microscopy images are major\napplications of computer vision technologies in both biomedical research and\nclinical practice. Though machine learning methods are increasingly common in\nthese fields, classical algorithms still offer significant advantages for both\ntasks, including better explainability, faster computation, lower hardware\nrequirements and more consistent performance. In this paper, we present a novel\napproach based on gravitational force fields that can compete with, and\npotentially outperform modern machine learning models when applied to\nfluorescence microscopy images. This method includes detection, segmentation,\nand tracking elements, with the results demonstrated on a Cell Tracking\nChallenge dataset.",
            "author": [
                "Nikomidisz Eftimiu",
                "Michal Kozubek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03509v1",
                "http://arxiv.org/pdf/2312.03509v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "q-bio.CB",
                "I.4.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03508v1",
            "title": "Convolutional neural network based decoders for surface codes",
            "updated": "2023-12-06T14:07:31Z",
            "published": "2023-12-06T14:07:31Z",
            "summary": "The decoding of error syndromes of surface codes with classical algorithms\nmay slow down quantum computation. To overcome this problem it is possible to\nimplement decoding algorithms based on artificial neural networks. This work\nreports a study of decoders based on convolutional neural networks, tested on\ndifferent code distances and noise models. The results show that decoders based\non convolutional neural networks have good performance and can adapt to\ndifferent noise models. Moreover, explainable machine learning techniques have\nbeen applied to the neural network of the decoder to better understand the\nbehaviour and errors of the algorithm, in order to produce a more robust and\nperforming algorithm.",
            "author": [
                "Simone Bordoni",
                "Stefano Giagu"
            ],
            "link": [
                "http://dx.doi.org/10.1007/s11128-023-03898-2",
                "http://arxiv.org/abs/2312.03508v1",
                "http://arxiv.org/pdf/2312.03508v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03502v1",
            "title": "Improving the Generalization of Segmentation Foundation Model under\n  Distribution Shift via Weakly Supervised Adaptation",
            "updated": "2023-12-06T13:59:22Z",
            "published": "2023-12-06T13:59:22Z",
            "summary": "The success of large language models has inspired the computer vision\ncommunity to explore image segmentation foundation model that is able to\nzero/few-shot generalize through prompt engineering. Segment-Anything(SAM),\namong others, is the state-of-the-art image segmentation foundation model\ndemonstrating strong zero/few-shot generalization. Despite the success, recent\nstudies reveal the weakness of SAM under strong distribution shift. In\nparticular, SAM performs awkwardly on corrupted natural images, camouflaged\nimages, medical images, etc. Motivated by the observations, we aim to develop a\nself-training based strategy to adapt SAM to target distribution. Given the\nunique challenges of large source dataset, high computation cost and incorrect\npseudo label, we propose a weakly supervised self-training architecture with\nanchor regularization and low-rank finetuning to improve the robustness and\ncomputation efficiency of adaptation. We validate the effectiveness on 5 types\nof downstream segmentation tasks including natural clean/corrupted images,\nmedical images, camouflaged images and robotic images. Our proposed method is\ntask-agnostic in nature and outperforms pre-trained SAM and state-of-the-art\ndomain adaptation methods on almost all downstream tasks with the same testing\nprompt inputs.",
            "author": [
                "Haojie Zhang",
                "Yongyi Su",
                "Xun Xu",
                "Kui Jia"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03502v1",
                "http://arxiv.org/pdf/2312.03502v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03498v1",
            "title": "Non-Gaussianities and the large $|\u03b7|$ approach to inflation",
            "updated": "2023-12-06T13:47:08Z",
            "published": "2023-12-06T13:47:08Z",
            "summary": "The physics of primordial black holes can be affected by the non-Gaussian\nstatistics of the density fluctuations that generate them. Therefore, it is\nimportant to have good theoretical control of the higher-order correlation\nfunctions for primordial curvature perturbations. By working at leading order\nin a $1/|\\eta|$ expansion, we analytically determine the bispectrum of\ncurvature fluctuations for single field inflationary scenarios producing\nprimordial black holes. The bispectrum has a rich scale and shape dependence,\nand its features depend on the dynamics of the would-be decaying mode. We apply\nour analytical results to study gravitational waves induced at second order by\nenhanced curvature fluctuations. Their statistical properties are derived in\nterms of convolution integrals over wide momentum ranges, and they are\nsensitive on the scale and shape dependence of the curvature bispectrum we\nanalytically computed.",
            "author": [
                "Gianmassimo Tasinato"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03498v1",
                "http://arxiv.org/pdf/2312.03498v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03793v1",
            "title": "AnimateZero: Video Diffusion Models are Zero-Shot Image Animators",
            "updated": "2023-12-06T13:39:35Z",
            "published": "2023-12-06T13:39:35Z",
            "summary": "Large-scale text-to-video (T2V) diffusion models have great progress in\nrecent years in terms of visual quality, motion and temporal consistency.\nHowever, the generation process is still a black box, where all attributes\n(e.g., appearance, motion) are learned and generated jointly without precise\ncontrol ability other than rough text descriptions. Inspired by image animation\nwhich decouples the video as one specific appearance with the corresponding\nmotion, we propose AnimateZero to unveil the pre-trained text-to-video\ndiffusion model, i.e., AnimateDiff, and provide more precise appearance and\nmotion control abilities for it. For appearance control, we borrow intermediate\nlatents and their features from the text-to-image (T2I) generation for ensuring\nthe generated first frame is equal to the given generated image. For temporal\ncontrol, we replace the global temporal attention of the original T2V model\nwith our proposed positional-corrected window attention to ensure other frames\nalign with the first frame well. Empowered by the proposed methods, AnimateZero\ncan successfully control the generating progress without further training. As a\nzero-shot image animator for given images, AnimateZero also enables multiple\nnew applications, including interactive video generation and real image\nanimation. The detailed experiments demonstrate the effectiveness of the\nproposed method in both T2V and related applications.",
            "author": [
                "Jiwen Yu",
                "Xiaodong Cun",
                "Chenyang Qi",
                "Yong Zhang",
                "Xintao Wang",
                "Ying Shan",
                "Jian Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03793v1",
                "http://arxiv.org/pdf/2312.03793v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03792v1",
            "title": "PCDP-SGD: Improving the Convergence of Differentially Private SGD via\n  Projection in Advance",
            "updated": "2023-12-06T13:34:15Z",
            "published": "2023-12-06T13:34:15Z",
            "summary": "The paradigm of Differentially Private SGD~(DP-SGD) can provide a theoretical\nguarantee for training data in both centralized and federated settings.\nHowever, the utility degradation caused by DP-SGD limits its wide application\nin high-stakes tasks, such as medical image diagnosis. In addition to the\nnecessary perturbation, the convergence issue is attributed to the information\nloss on the gradient clipping. In this work, we propose a general framework\nPCDP-SGD, which aims to compress redundant gradient norms and preserve more\ncrucial top gradient components via projection operation before gradient\nclipping. Additionally, we extend PCDP-SGD as a fundamental component in\ndifferential privacy federated learning~(DPFL) for mitigating the data\nheterogeneous challenge and achieving efficient communication. We prove that\npre-projection enhances the convergence of DP-SGD by reducing the dependence of\nclipping error and bias to a fraction of the top gradient eigenspace, and in\ntheory, limits cross-client variance to improve the convergence under\nheterogeneous federation. Experimental results demonstrate that PCDP-SGD\nachieves higher accuracy compared with state-of-the-art DP-SGD variants in\ncomputer vision tasks. Moreover, PCDP-SGD outperforms current federated\nlearning frameworks when DP is guaranteed on local training sets.",
            "author": [
                "Haichao Sha",
                "Ruixuan Liu",
                "Yixuan Liu",
                "Hong Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03792v1",
                "http://arxiv.org/pdf/2312.03792v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03490v1",
            "title": "PneumoLLM: Harnessing the Power of Large Language Model for\n  Pneumoconiosis Diagnosis",
            "updated": "2023-12-06T13:31:52Z",
            "published": "2023-12-06T13:31:52Z",
            "summary": "The conventional pretraining-and-finetuning paradigm, while effective for\ncommon diseases with ample data, faces challenges in diagnosing data-scarce\noccupational diseases like pneumoconiosis. Recently, large language models\n(LLMs) have exhibits unprecedented ability when conducting multiple tasks in\ndialogue, bringing opportunities to diagnosis. A common strategy might involve\nusing adapter layers for vision-language alignment and diagnosis in a dialogic\nmanner. Yet, this approach often requires optimization of extensive learnable\nparameters in the text branch and the dialogue head, potentially diminishing\nthe LLMs' efficacy, especially with limited training data. In our work, we\ninnovate by eliminating the text branch and substituting the dialogue head with\na classification head. This approach presents a more effective method for\nharnessing LLMs in diagnosis with fewer learnable parameters. Furthermore, to\nbalance the retention of detailed image information with progression towards\naccurate diagnosis, we introduce the contextual multi-token engine. This engine\nis specialized in adaptively generating diagnostic tokens. Additionally, we\npropose the information emitter module, which unidirectionally emits\ninformation from image tokens to diagnosis tokens. Comprehensive experiments\nvalidate the superiority of our methods and the effectiveness of proposed\nmodules. Our codes can be found at\nhttps://github.com/CodeMonsterPHD/PneumoLLM/tree/main.",
            "author": [
                "Meiyue Song",
                "Zhihua Yu",
                "Jiaxin Wang",
                "Jiarui Wang",
                "Yuting Lu",
                "Baicun Li",
                "Xiaoxu Wang",
                "Qinghua Huang",
                "Zhijun Li",
                "Nikolaos I. Kanellakis",
                "Jiangfeng Liu",
                "Jing Wang",
                "Binglu Wang",
                "Juntao Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03490v1",
                "http://arxiv.org/pdf/2312.03490v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03485v1",
            "title": "Precision of Individual Shapley Value Explanations",
            "updated": "2023-12-06T13:29:23Z",
            "published": "2023-12-06T13:29:23Z",
            "summary": "Shapley values are extensively used in explainable artificial intelligence\n(XAI) as a framework to explain predictions made by complex machine learning\n(ML) models. In this work, we focus on conditional Shapley values for\npredictive models fitted to tabular data and explain the prediction\n$f(\\boldsymbol{x}^{*})$ for a single observation $\\boldsymbol{x}^{*}$ at the\ntime. Numerous Shapley value estimation methods have been proposed and\nempirically compared on an average basis in the XAI literature. However, less\nfocus has been devoted to analyzing the precision of the Shapley value\nexplanations on an individual basis. We extend our work in Olsen et al. (2023)\nby demonstrating and discussing that the explanations are systematically less\nprecise for observations on the outer region of the training data distribution\nfor all used estimation methods. This is expected from a statistical point of\nview, but to the best of our knowledge, it has not been systematically\naddressed in the Shapley value literature. This is crucial knowledge for\nShapley values practitioners, who should be more careful in applying these\nobservations' corresponding Shapley value explanations.",
            "author": [
                "Lars Henry Berge Olsen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03485v1",
                "http://arxiv.org/pdf/2312.03485v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "stat.AP",
                "stat.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03483v1",
            "title": "Exploring Answer Information Methods for Question Generation with\n  Transformers",
            "updated": "2023-12-06T13:26:16Z",
            "published": "2023-12-06T13:26:16Z",
            "summary": "There has been a lot of work in question generation where different methods\nto provide target answers as input, have been employed. This experimentation\nhas been mostly carried out for RNN based models. We use three different\nmethods and their combinations for incorporating answer information and explore\ntheir effect on several automatic evaluation metrics. The methods that are used\nare answer prompting, using a custom product method using answer embeddings and\nencoder outputs, choosing sentences from the input paragraph that have answer\nrelated information, and using a separate cross-attention attention block in\nthe decoder which attends to the answer. We observe that answer prompting\nwithout any additional modes obtains the best scores across rouge, meteor\nscores. Additionally, we use a custom metric to calculate how many of the\ngenerated questions have the same answer, as the answer which is used to\ngenerate them.",
            "author": [
                "Talha Chafekar",
                "Aafiya Hussain",
                "Grishma Sharma",
                "Deepak Sharma"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03483v1",
                "http://arxiv.org/pdf/2312.03483v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03480v1",
            "title": "AMR Parsing is Far from Solved: GrAPES, the Granular AMR Parsing\n  Evaluation Suite",
            "updated": "2023-12-06T13:19:56Z",
            "published": "2023-12-06T13:19:56Z",
            "summary": "We present the Granular AMR Parsing Evaluation Suite (GrAPES), a challenge\nset for Abstract Meaning Representation (AMR) parsing with accompanying\nevaluation metrics. AMR parsers now obtain high scores on the standard AMR\nevaluation metric Smatch, close to or even above reported inter-annotator\nagreement. But that does not mean that AMR parsing is solved; in fact, human\nevaluation in previous work indicates that current parsers still quite\nfrequently make errors on node labels or graph structure that substantially\ndistort sentence meaning. Here, we provide an evaluation suite that tests AMR\nparsers on a range of phenomena of practical, technical, and linguistic\ninterest. Our 36 categories range from seen and unseen labels, to structural\ngeneralization, to coreference. GrAPES reveals in depth the abilities and\nshortcomings of current AMR parsers.",
            "author": [
                "Jonas Groschwitz",
                "Shay B. Cohen",
                "Lucia Donatelli",
                "Meaghan Fowlie"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03480v1",
                "http://arxiv.org/pdf/2312.03480v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "J.5"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03479v1",
            "title": "JAMMIN-GPT: Text-based Improvisation using LLMs in Ableton Live",
            "updated": "2023-12-06T13:19:34Z",
            "published": "2023-12-06T13:19:34Z",
            "summary": "We introduce a system that allows users of Ableton Live to create MIDI-clips\nby naming them with musical descriptions. Users can compose by typing the\ndesired musical content directly in Ableton's clip view, which is then inserted\nby our integrated system. This allows users to stay in the flow of their\ncreative process while quickly generating musical ideas. The system works by\nprompting ChatGPT to reply using one of several text-based musical formats,\nsuch as ABC notation, chord symbols, or drum tablature. This is an important\nstep in integrating generative AI tools into pre-existing musical workflows,\nand could be valuable for content makers who prefer to express their creative\nvision through descriptive language. Code is available at\nhttps://github.com/supersational/JAMMIN-GPT.",
            "author": [
                "Sven Hollowell",
                "Tashi Namgyal",
                "Paul Marshall"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03479v1",
                "http://arxiv.org/pdf/2312.03479v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.AI",
                "cs.HC",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03477v1",
            "title": "From Detection to Action Recognition: An Edge-Based Pipeline for Robot\n  Human Perception",
            "updated": "2023-12-06T13:10:02Z",
            "published": "2023-12-06T13:10:02Z",
            "summary": "Mobile service robots are proving to be increasingly effective in a range of\napplications, such as healthcare, monitoring Activities of Daily Living (ADL),\nand facilitating Ambient Assisted Living (AAL). These robots heavily rely on\nHuman Action Recognition (HAR) to interpret human actions and intentions.\nHowever, for HAR to function effectively on service robots, it requires prior\nknowledge of human presence (human detection) and identification of individuals\nto monitor (human tracking). In this work, we propose an end-to-end pipeline\nthat encompasses the entire process, starting from human detection and\ntracking, leading to action recognition. The pipeline is designed to operate in\nnear real-time while ensuring all stages of processing are performed on the\nedge, reducing the need for centralised computation. To identify the most\nsuitable models for our mobile robot, we conducted a series of experiments\ncomparing state-of-the-art solutions based on both their detection performance\nand efficiency. To evaluate the effectiveness of our proposed pipeline, we\nproposed a dataset comprising daily household activities. By presenting our\nfindings and analysing the results, we demonstrate the efficacy of our approach\nin enabling mobile robots to understand and respond to human behaviour in\nreal-world scenarios relying mainly on the data from their RGB cameras.",
            "author": [
                "Petros Toupas",
                "Georgios Tsamis",
                "Dimitrios Giakoumis",
                "Konstantinos Votis",
                "Dimitrios Tzovaras"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03477v1",
                "http://arxiv.org/pdf/2312.03477v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03791v1",
            "title": "Towards Quantum Computational Mechanics",
            "updated": "2023-12-06T12:53:02Z",
            "published": "2023-12-06T12:53:02Z",
            "summary": "The rapid advancements in quantum computing as ushered in a new era for\ncomputer simulations, presenting groundbreaking opportunities across diverse\ndisciplines. Central to this revolution is the quantum processor's capacity to\nentangle qubits, unlocking unprecedented possibilities for addressing\ncomputational challenges on an extreme scale, far beyond the reach of classical\ncomputing. In this study, we explore how quantum computing can be employed to\nenhance computational mechanics. Our focus is on the analysis of Representative\nVolume Element (RVE) within the framework of multiscale solid mechanics. We\nintroduce an innovative quantum algorithm designed to solve the RVE problem.\nThis algorithm is capable of compute RVEs of discretization size $N$ in\n$\\mathcal{O}(\\textrm{Poly log}(N))$ time, thus achieving an exponential\nspeed-up over traditional classical computing approaches that typically scales\nlinearly with $N$. We validate our approach with case studies including the\nsolution of one and two dimensional Poisson's equation, as well as an RVE of a\ncomposite bar with piece-wise constant phases. We provide quantum circuit\ndesigns that requires only $\\mathcal{O}(\\textrm{Poly log}(N))$ universal\nquantum gates,underscoring the efficiency of our approach. Our work suggests a\nmajor way in which quantum computing can be combined with and brought to bear\non computational mechanics.",
            "author": [
                "Burigede Liu",
                "Michael Ortiz",
                "Fehmi Cirak"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03791v1",
                "http://arxiv.org/pdf/2312.03791v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03470v1",
            "title": "Regular polygons, line operators, and elliptic modular surfaces as\n  realization spaces of matroids",
            "updated": "2023-12-06T12:50:03Z",
            "published": "2023-12-06T12:50:03Z",
            "summary": "We investigate the matroid realization space of a specific deformation of the\nregular $n$-gon with its lines of symmetry. It turns out that these particular\nrealization spaces are birational to the elliptic modular surfaces $\\Xi_{1}(n)$\nover the modular curve $X_1(n)$.\n  We obtain in that way a model of $\\Xi_{1}(n)$ defined over the rational\nnumbers. Furthermore, a natural geometric operator acts on these matroid\nrealizations. On the elliptic modular surface this operator corresponds to the\nmultiplication by $-2$ on the elliptic curves. That gives a new geometric way\nto compute the multiplication by $-2$ on elliptic curves.",
            "author": [
                "Lukas K\u00fchne",
                "Xavier Roulleau"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03470v1",
                "http://arxiv.org/pdf/2312.03470v1"
            ],
            "primary_category": "math.AG",
            "category": [
                "math.AG",
                "math.CO",
                "14N20, 14J27, 14J25, 14G35"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03790v1",
            "title": "Memory-Efficient Optical Flow via Radius-Distribution Orthogonal Cost\n  Volume",
            "updated": "2023-12-06T12:43:11Z",
            "published": "2023-12-06T12:43:11Z",
            "summary": "The full 4D cost volume in Recurrent All-Pairs Field Transforms (RAFT) or\nglobal matching by Transformer achieves impressive performance for optical flow\nestimation. However, their memory consumption increases quadratically with\ninput resolution, rendering them impractical for high-resolution images. In\nthis paper, we present MeFlow, a novel memory-efficient method for\nhigh-resolution optical flow estimation. The key of MeFlow is a recurrent local\northogonal cost volume representation, which decomposes the 2D search space\ndynamically into two 1D orthogonal spaces, enabling our method to scale\neffectively to very high-resolution inputs. To preserve essential information\nin the orthogonal space, we utilize self attention to propagate feature\ninformation from the 2D space to the orthogonal space. We further propose a\nradius-distribution multi-scale lookup strategy to model the correspondences of\nlarge displacements at a negligible cost. We verify the efficiency and\neffectiveness of our method on the challenging Sintel and KITTI benchmarks, and\nreal-world 4K ($2160\\!\\times\\!3840$) images. Our method achieves competitive\nperformance on both Sintel and KITTI benchmarks, while maintaining the highest\nmemory efficiency on high-resolution inputs.",
            "author": [
                "Gangwei Xu",
                "Shujun Chen",
                "Hao Jia",
                "Miaojie Feng",
                "Xin Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03790v1",
                "http://arxiv.org/pdf/2312.03790v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03463v1",
            "title": "DBCopilot: Scaling Natural Language Querying to Massive Databases",
            "updated": "2023-12-06T12:37:28Z",
            "published": "2023-12-06T12:37:28Z",
            "summary": "Text-to-SQL simplifies database interactions by enabling non-experts to\nconvert their natural language (NL) questions into Structured Query Language\n(SQL) queries. While recent advances in large language models (LLMs) have\nimproved the zero-shot text-to-SQL paradigm, existing methods face scalability\nchallenges when dealing with massive, dynamically changing databases. This\npaper introduces DBCopilot, a framework that addresses these challenges by\nemploying a compact and flexible copilot model for routing across massive\ndatabases. Specifically, DBCopilot decouples the text-to-SQL process into\nschema routing and SQL generation, leveraging a lightweight\nsequence-to-sequence neural network-based router to formulate database\nconnections and navigate natural language questions through databases and\ntables. The routed schemas and questions are then fed into LLMs for efficient\nSQL generation. Furthermore, DBCopilot also introduced a reverse\nschema-to-question generation paradigm, which can learn and adapt the router\nover massive databases automatically without requiring manual intervention.\nExperimental results demonstrate that DBCopilot is a scalable and effective\nsolution for real-world text-to-SQL tasks, providing a significant advancement\nin handling large-scale schemas.",
            "author": [
                "Tianshu Wang",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun",
                "Xiaoyang Chen",
                "Hao Wang",
                "Zhenyu Zeng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03463v1",
                "http://arxiv.org/pdf/2312.03463v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.DB",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03461v2",
            "title": "HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian\n  Splatting",
            "updated": "2023-12-07T12:46:07Z",
            "published": "2023-12-06T12:36:53Z",
            "summary": "We have recently seen tremendous progress in photo-real human modeling and\nrendering. Yet, efficiently rendering realistic human performance and\nintegrating it into the rasterization pipeline remains challenging. In this\npaper, we present HiFi4G, an explicit and compact Gaussian-based approach for\nhigh-fidelity human performance rendering from dense footage. Our core\nintuition is to marry the 3D Gaussian representation with non-rigid tracking,\nachieving a compact and compression-friendly representation. We first propose a\ndual-graph mechanism to obtain motion priors, with a coarse deformation graph\nfor effective initialization and a fine-grained Gaussian graph to enforce\nsubsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with\nadaptive spatial-temporal regularizers to effectively balance the non-rigid\nprior and Gaussian updating. We also present a companion compression scheme\nwith residual compensation for immersive experiences on various platforms. It\nachieves a substantial compression rate of approximately 25 times, with less\nthan 2MB of storage per frame. Extensive experiments demonstrate the\neffectiveness of our approach, which significantly outperforms existing\napproaches in terms of optimization speed, rendering quality, and storage\noverhead.",
            "author": [
                "Yuheng Jiang",
                "Zhehao Shen",
                "Penghao Wang",
                "Zhuo Su",
                "Yu Hong",
                "Yingliang Zhang",
                "Jingyi Yu",
                "Lan Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03461v2",
                "http://arxiv.org/pdf/2312.03461v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03460v1",
            "title": "Modeling the dynamics of quantum systems coupled to large dimensional\n  baths using effective energy states",
            "updated": "2023-12-06T12:35:24Z",
            "published": "2023-12-06T12:35:24Z",
            "summary": "The quantum dynamics of a low-dimensional system in contact with a large but\nfinite harmonic bath is theoretically investigated by coarse-graining the bath\ninto a reduced set of effective energy states. In this model, the couplings\nbetween the system and the bath are obtained from the statistical average over\nthe discrete, degenerate effective states. Our model is aimed at intermediate\nbath sizes in which non-Markovian processes and energy transfer between the\nbath and the main system are important. The method is applied to a model system\nof a Morse oscillator coupled to 40 harmonic modes. The results are found to be\nin excellent agreement with the direct quantum dynamics simulations of\nBouakline et al. [J. Phys. Chem. A 116, 11118-11127 (2012)], but at a much\nlower computational cost. Extension to larger baths is discussed in comparison\nto the time-convolutionless method. We also extend this study to the case of a\nmicrocanonical bath with finite initial internal energies. The computational\nefficiency and convergence properties of the effective bath states model with\nrespect to relevant parameters are also discussed.",
            "author": [
                "Lo\u00efse Attal",
                "Cyril Falvo",
                "Florent Calvo",
                "Pascal Parneix"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03460v1",
                "http://arxiv.org/pdf/2312.03460v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph",
                "physics.comp-ph",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03459v1",
            "title": "F3-Pruning: A Training-Free and Generalized Pruning Strategy towards\n  Faster and Finer Text-to-Video Synthesis",
            "updated": "2023-12-06T12:34:47Z",
            "published": "2023-12-06T12:34:47Z",
            "summary": "Recently Text-to-Video (T2V) synthesis has undergone a breakthrough by\ntraining transformers or diffusion models on large-scale datasets.\nNevertheless, inferring such large models incurs huge costs.Previous inference\nacceleration works either require costly retraining or are model-specific.To\naddress this issue, instead of retraining we explore the inference process of\ntwo mainstream T2V models using transformers and diffusion models.The\nexploration reveals the redundancy in temporal attention modules of both\nmodels, which are commonly utilized to establish temporal relations among\nframes.Consequently, we propose a training-free and generalized pruning\nstrategy called F3-Pruning to prune redundant temporal attention\nweights.Specifically, when aggregate temporal attention values are ranked below\na certain ratio, corresponding weights will be pruned.Extensive experiments on\nthree datasets using a classic transformer-based model CogVideo and a typical\ndiffusion-based model Tune-A-Video verify the effectiveness of F3-Pruning in\ninference acceleration, quality assurance and broad applicability.",
            "author": [
                "Sitong Su",
                "Jianzhi Liu",
                "Lianli Gao",
                "Jingkuan Song"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03459v1",
                "http://arxiv.org/pdf/2312.03459v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03458v1",
            "title": "Think from Words(TFW): Initiating Human-Like Cognition in Large Language\n  Models Through Think from Words for Japanese Text-level Classification",
            "updated": "2023-12-06T12:34:46Z",
            "published": "2023-12-06T12:34:46Z",
            "summary": "The proliferation of Large Language Models (LLMs) has spurred extensive\nresearch into LLM-related Prompt investigations, such as Instruction Learning\n(IL), In-context Learning (ICL), and Chain-of-Thought (CoT). These approaches\naim to improve LLMs' responses by enabling them to provide concise statements\nor examples for deeper contemplation when addressing questions. However,\nindependent thinking by LLMs can introduce variability in their thought\nprocesses, leading to potential inaccuracies. In response, our study seeks to\nbridge the gap between LLM and human-like thinking processes, recognizing that\ntext comprehension begins with understanding individual words. To tackle this\nchallenge, we have expanded the CoT method to cater to a specific domain. Our\napproach, known as \"Think from Words\" (TFW), initiates the comprehension\nprocess at the word level and then extends it to encompass the entire text. We\nalso propose \"TFW with Extra word-level information\" (TFW Extra), augmenting\ncomprehension with additional word-level data. To assess our methods, we employ\ntext classification on six Japanese datasets comprising text-level and\nword-level elements. Our findings not only validate the effectiveness of TFW\nbut also shed light on the impact of various word-level information types on\nLLMs' text comprehension, offering insights into their potential to cause\nmisinterpretations and errors in the overall comprehension of the final text.",
            "author": [
                "Chengguang Gan",
                "Qinghao Zhang",
                "Tatsunori Mori"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03458v1",
                "http://arxiv.org/pdf/2312.03458v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03457v1",
            "title": "On class groups of upper cluster algebras",
            "updated": "2023-12-06T12:34:11Z",
            "published": "2023-12-06T12:34:11Z",
            "summary": "We compute the class groups of full rank upper cluster algebras in terms of\nthe exchange polynomials. This characterizes the UFDs among these algebras. Our\nresults simultaneously generalize theorems of Garcia Elsener, Lampe, and\nSmertnig from 2019 and of Cao, Keller, and Qin from 2023. Furthermore we show\nthat every (upper) cluster algebra is a finite factorization domain.",
            "author": [
                "Mara Pompili"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03457v1",
                "http://arxiv.org/pdf/2312.03457v1"
            ],
            "primary_category": "math.AC",
            "category": [
                "math.AC",
                "Primary 13F60, Secondary 13F05, 13F15"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03455v1",
            "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence\n  of Training Data",
            "updated": "2023-12-06T12:27:25Z",
            "published": "2023-12-06T12:27:25Z",
            "summary": "Perceptual metrics are traditionally used to evaluate the quality of natural\nsignals, such as images and audio. They are designed to mimic the perceptual\nbehaviour of human observers and usually reflect structures found in natural\nsignals. This motivates their use as loss functions for training generative\nmodels such that models will learn to capture the structure held in the metric.\nWe take this idea to the extreme in the audio domain by training a compressive\nautoencoder to reconstruct uniform noise, in lieu of natural data. We show that\ntraining with perceptual losses improves the reconstruction of spectrograms and\nre-synthesized audio at test time over models trained with a standard Euclidean\nloss. This demonstrates better generalisation to unseen natural signals when\nusing perceptual metrics.",
            "author": [
                "Tashi Namgyal",
                "Alexander Hepburn",
                "Raul Santos-Rodriguez",
                "Valero Laparra",
                "Jesus Malo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03455v1",
                "http://arxiv.org/pdf/2312.03455v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.AI",
                "cs.CV",
                "cs.LG",
                "eess.AS",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03447v1",
            "title": "Quantum-Inspired Neural Network Model of Optical Illusions",
            "updated": "2023-12-06T12:10:56Z",
            "published": "2023-12-06T12:10:56Z",
            "summary": "Ambiguous optical illusions have been a paradigmatic object of fascination,\nresearch and inspiration in arts, psychology and video games. However, accurate\ncomputational models of perception of ambiguous figures have been elusive. In\nthis paper, we design and train a deep neural network model to simulate the\nhuman's perception of the Necker cube, an ambiguous drawing with several\nalternating possible interpretations. Defining the weights of the neural\nnetwork connection using a quantum generator of truly random numbers, in\nagreement with the emerging concepts of quantum artificial intelligence and\nquantum cognition we reveal that the actual perceptual state of the Necker cube\nis a qubit-like superposition of the two fundamental perceptual states\npredicted by classical theories. Our results will find applications in video\ngames and virtual reality systems employed for training of astronauts and\noperators of unmanned aerial vehicles. They will also be useful for researchers\nworking in the fields of machine learning and vision, psychology of perception\nand quantum-mechanical models of human mind and decision-making.",
            "author": [
                "Ivan S. Maksymov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03447v1",
                "http://arxiv.org/pdf/2312.03447v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "cs.AI",
                "cs.CV",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03789v1",
            "title": "Comparative Analysis of Multilingual Text Classification &\n  Identification through Deep Learning and Embedding Visualization",
            "updated": "2023-12-06T12:03:27Z",
            "published": "2023-12-06T12:03:27Z",
            "summary": "This research conducts a comparative study on multilingual text\nclassification methods, utilizing deep learning and embedding visualization.\nThe study employs LangDetect, LangId, FastText, and Sentence Transformer on a\ndataset encompassing 17 languages. It explores dimensionality's impact on\nclustering, revealing FastText's clearer clustering in 2D visualization due to\nits extensive multilingual corpus training. Notably, the FastText multi-layer\nperceptron model achieved remarkable accuracy, precision, recall, and F1 score,\noutperforming the Sentence Transformer model. The study underscores the\neffectiveness of these techniques in multilingual text classification,\nemphasizing the importance of large multilingual corpora for training\nembeddings. It lays the groundwork for future research and assists\npractitioners in developing language detection and classification systems.\nAdditionally, it includes the comparison of multi-layer perceptron, LSTM, and\nConvolution models for classification.",
            "author": [
                "Arinjay Wyawhare"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03789v1",
                "http://arxiv.org/pdf/2312.03789v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03443v1",
            "title": "Data-driven Crop Growth Simulation on Time-varying Generated Images\n  using Multi-conditional Generative Adversarial Networks",
            "updated": "2023-12-06T11:54:50Z",
            "published": "2023-12-06T11:54:50Z",
            "summary": "Image-based crop growth modeling can substantially contribute to precision\nagriculture by revealing spatial crop development over time, which allows an\nearly and location-specific estimation of relevant future plant traits, such as\nleaf area or biomass. A prerequisite for realistic and sharp crop image\ngeneration is the integration of multiple growth-influencing conditions in a\nmodel, such as an image of an initial growth stage, the associated growth time,\nand further information about the field treatment. We present a two-stage\nframework consisting first of an image prediction model and second of a growth\nestimation model, which both are independently trained. The image prediction\nmodel is a conditional Wasserstein generative adversarial network (CWGAN). In\nthe generator of this model, conditional batch normalization (CBN) is used to\nintegrate different conditions along with the input image. This allows the\nmodel to generate time-varying artificial images dependent on multiple\ninfluencing factors of different kinds. These images are used by the second\npart of the framework for plant phenotyping by deriving plant-specific traits\nand comparing them with those of non-artificial (real) reference images. For\nvarious crop datasets, the framework allows realistic, sharp image predictions\nwith a slight loss of quality from short-term to long-term predictions.\nSimulations of varying growth-influencing conditions performed with the trained\nframework provide valuable insights into how such factors relate to crop\nappearances, which is particularly useful in complex, less explored crop\nmixture systems. Further results show that adding process-based simulated\nbiomass as a condition increases the accuracy of the derived phenotypic traits\nfrom the predicted images. This demonstrates the potential of our framework to\nserve as an interface between an image- and process-based crop growth model.",
            "author": [
                "Lukas Drees",
                "Dereje T. Demie",
                "Madhuri R. Paul",
                "Johannes Leonhardt",
                "Sabine J. Seidel",
                "Thomas F. D\u00f6ring",
                "Ribana Roscher"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03443v1",
                "http://arxiv.org/pdf/2312.03443v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03442v1",
            "title": "High-Quality Facial Geometry and Appearance Capture at Home",
            "updated": "2023-12-06T11:51:06Z",
            "published": "2023-12-06T11:51:06Z",
            "summary": "Facial geometry and appearance capture have demonstrated tremendous success\nin 3D scanning real humans in studios. Recent works propose to democratize this\ntechnique while keeping the results high quality. However, they are still\ninconvenient for daily usage. In addition, they focus on an easier problem of\nonly capturing facial skin. This paper proposes a novel method for high-quality\nface capture, featuring an easy-to-use system and the capability to model the\ncomplete face with skin, mouth interior, hair, and eyes. We reconstruct facial\ngeometry and appearance from a single co-located smartphone flashlight sequence\ncaptured in a dim room where the flashlight is the dominant light source (e.g.\nrooms with curtains or at night). To model the complete face, we propose a\nnovel hybrid representation to effectively model both eyes and other facial\nregions, along with novel techniques to learn it from images. We apply a\ncombined lighting model to compactly represent real illuminations and exploit a\nmorphable face albedo model as a reflectance prior to disentangle diffuse and\nspecular. Experiments show that our method can capture high-quality 3D\nrelightable scans.",
            "author": [
                "Yuxuan Han",
                "Junfeng Lyu",
                "Feng Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03442v1",
                "http://arxiv.org/pdf/2312.03442v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03441v1",
            "title": "UFineBench: Towards Text-based Person Retrieval with Ultra-fine\n  Granularity",
            "updated": "2023-12-06T11:50:14Z",
            "published": "2023-12-06T11:50:14Z",
            "summary": "Existing text-based person retrieval datasets often have relatively\ncoarse-grained text annotations. This hinders the model to comprehend the\nfine-grained semantics of query texts in real scenarios. To address this\nproblem, we contribute a new benchmark named \\textbf{UFineBench} for text-based\nperson retrieval with ultra-fine granularity.\n  Firstly, we construct a new \\textbf{dataset} named UFine6926. We collect a\nlarge number of person images and manually annotate each image with two\ndetailed textual descriptions, averaging 80.8 words each. The average word\ncount is three to four times that of the previous datasets. In addition of\nstandard in-domain evaluation, we also propose a special \\textbf{evaluation\nparadigm} more representative of real scenarios. It contains a new evaluation\nset with cross domains, cross textual granularity and cross textual styles,\nnamed UFine3C, and a new evaluation metric for accurately measuring retrieval\nability, named mean Similarity Distribution (mSD). Moreover, we propose CFAM, a\nmore efficient \\textbf{algorithm} especially designed for text-based person\nretrieval with ultra fine-grained texts. It achieves fine granularity mining by\nadopting a shared cross-modal granularity decoder and hard negative match\nmechanism.\n  With standard in-domain evaluation, CFAM establishes competitive performance\nacross various datasets, especially on our ultra fine-grained UFine6926.\nFurthermore, by evaluating on UFine3C, we demonstrate that training on our\nUFine6926 significantly improves generalization to real scenarios compared with\nother coarse-grained datasets. The dataset and code will be made publicly\navailable at \\url{https://github.com/Zplusdragon/UFineBench}.",
            "author": [
                "Jialong Zuo",
                "Hanyu Zhou",
                "Ying Nie",
                "Feng Zhang",
                "Tianyu Guo",
                "Nong Sang",
                "Yunhe Wang",
                "Changxin Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03441v1",
                "http://arxiv.org/pdf/2312.03441v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03437v1",
            "title": "Data-Centric Digital Agriculture: A Perspective",
            "updated": "2023-12-06T11:38:26Z",
            "published": "2023-12-06T11:38:26Z",
            "summary": "In response to the increasing global demand for food, feed, fiber, and fuel,\ndigital agriculture is rapidly evolving to meet these demands while reducing\nenvironmental impact. This evolution involves incorporating data science,\nmachine learning, sensor technologies, robotics, and new management strategies\nto establish a more sustainable agricultural framework. So far, machine\nlearning research in digital agriculture has predominantly focused on\nmodel-centric approaches, focusing on model design and evaluation. These\nefforts aim to optimize model accuracy and efficiency, often treating data as a\nstatic benchmark. Despite the availability of agricultural data and\nmethodological advancements, a saturation point has been reached, with many\nestablished machine learning methods achieving comparable levels of accuracy\nand facing similar limitations. To fully realize the potential of digital\nagriculture, it is crucial to have a comprehensive understanding of the role of\ndata in the field and to adopt data-centric machine learning. This involves\ndeveloping strategies to acquire and curate valuable data and implementing\neffective learning and evaluation strategies that utilize the intrinsic value\nof data. This approach has the potential to create accurate, generalizable, and\nadaptable machine learning methods that effectively and sustainably address\nagricultural tasks such as yield prediction, weed detection, and early disease\nidentification",
            "author": [
                "Ribana Roscher",
                "Lukas Roth",
                "Cyrill Stachniss",
                "Achim Walter"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03437v1",
                "http://arxiv.org/pdf/2312.03437v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03431v1",
            "title": "Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle",
            "updated": "2023-12-06T11:25:52Z",
            "published": "2023-12-06T11:25:52Z",
            "summary": "We introduce Gaussian-Flow, a novel point-based approach for fast dynamic\nscene reconstruction and real-time rendering from both multi-view and monocular\nvideos. In contrast to the prevalent NeRF-based approaches hampered by slow\ntraining and rendering speeds, our approach harnesses recent advancements in\npoint-based 3D Gaussian Splatting (3DGS). Specifically, a novel Dual-Domain\nDeformation Model (DDDM) is proposed to explicitly model attribute deformations\nof each Gaussian point, where the time-dependent residual of each attribute is\ncaptured by a polynomial fitting in the time domain, and a Fourier series\nfitting in the frequency domain. The proposed DDDM is capable of modeling\ncomplex scene deformations across long video footage, eliminating the need for\ntraining separate 3DGS for each frame or introducing an additional implicit\nneural field to model 3D dynamics. Moreover, the explicit deformation modeling\nfor discretized Gaussian points ensures ultra-fast training and rendering of a\n4D scene, which is comparable to the original 3DGS designed for static 3D\nreconstruction. Our proposed approach showcases a substantial efficiency\nimprovement, achieving a $5\\times$ faster training speed compared to the\nper-frame 3DGS modeling. In addition, quantitative results demonstrate that the\nproposed Gaussian-Flow significantly outperforms previous leading methods in\nnovel view rendering quality. Project page:\nhttps://nju-3dv.github.io/projects/Gaussian-Flow",
            "author": [
                "Youtian Lin",
                "Zuozhuo Dai",
                "Siyu Zhu",
                "Yao Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03431v1",
                "http://arxiv.org/pdf/2312.03431v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03430v1",
            "title": "ShareCMP: Polarization-Aware RGB-P Semantic Segmentation",
            "updated": "2023-12-06T11:25:40Z",
            "published": "2023-12-06T11:25:40Z",
            "summary": "Multimodal semantic segmentation is developing rapidly, but the modality of\nRGB-Polarization remains underexplored. To delve into this problem, we\nconstruct a UPLight RGB-P segmentation benchmark with 12 typical underwater\nsemantic classes which provides data support for Autonomous Underwater Vehicles\n(AUVs) to perform special perception tasks. In this work, we design the\nShareCMP, an RGB-P semantic segmentation framework with a shared dual-branch\narchitecture, which reduces the number of parameters by about 26-33% compared\nto previous dual-branch models. It encompasses a Polarization Generate\nAttention (PGA) module designed to generate polarization modal images with\nricher polarization properties for the encoder. In addition, we introduce the\nClass Polarization-Aware Loss (CPALoss) to improve the learning and\nunderstanding of the encoder for polarization modal information and to optimize\nthe PGA module. With extensive experiments on a total of three RGB-P\nbenchmarks, our ShareCMP achieves state-of-the-art performance in mIoU with\nfewer parameters on the UPLight (92.45%), ZJU (92.7%), and MCubeS (50.99%)\ndatasets. The code is available at https://github.com/LEFTeyex/ShareCMP.",
            "author": [
                "Zhuoyan Liu",
                "Bo Wang",
                "Lizhi Wang",
                "Chenyu Mao",
                "Ye Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03430v1",
                "http://arxiv.org/pdf/2312.03430v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "I.4.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03427v1",
            "title": "Latent State Space Extension for interpretable hybrid mechanistic models",
            "updated": "2023-12-06T11:19:24Z",
            "published": "2023-12-06T11:19:24Z",
            "summary": "Mechanistic growth models play a major role in bioprocess engineering,\ndesign, and control. Their reasonable predictive power and their high level of\ninterpretability make them an essential tool for computer aided engineering\nmethods. Additionally, since they contain knowledge about cell physiology, the\nparameter estimates provide meaningful insights into the metabolism of the\nmicroorganism under study. However, the assumption of time invariance of the\nmodel parameters is often violated in real experiments, limiting their capacity\nto fully explain the observed dynamics. In this work, we propose a framework\nfor identifying such violations and producing insights into misspecified\nmechanisms. The framework achieves this by allowing kinetic and process\nparameters to vary in time. We demonstrate the framework's capabilities by\nfitting a hybrid model based on a simple mechanistic growth model for E. coli\nwith data generated in-silico by a much more complex one and identifying\nmissing kinetics.",
            "author": [
                "Judit Aizpuru",
                "Maxim Borisyak",
                "Peter Neubauer",
                "M. Nicolas Cruz Bournazou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03427v1",
                "http://arxiv.org/pdf/2312.03427v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03426v1",
            "title": "Internal and External Calculi: Ordering the Jungle without Being Lost in\n  Translations",
            "updated": "2023-12-06T11:17:14Z",
            "published": "2023-12-06T11:17:14Z",
            "summary": "This paper gives a broad account of the various sequent-based proof\nformalisms in the proof-theoretic literature. We consider formalisms for\nvarious modal and tense logics, intuitionistic logic, conditional logics, and\nbunched logics. After providing an overview of the logics and proof formalisms\nunder consideration, we show how these sequent-based formalisms can be placed\nin a hierarchy in terms of the underlying data structure of the sequents. We\nthen discuss how this hierarchy can be traversed using translations.\nTranslating proofs up this hierarchy is found to be relatively easy while\ntranslating proofs down the hierarchy is substantially more difficult. Finally,\nwe inspect the prevalent distinction in structural proof theory between\n'internal calculi' and 'external calculi'. It is observed that these classes\nresist a rigorous separation, and we critically assess the properties that\n(calculi from) these classes are purported to possess.",
            "author": [
                "Tim S. Lyon",
                "Agata Ciabattoni",
                "Didier Galmiche",
                "Dominique Larchey-Wendling",
                "Daniel M\u00e9ry",
                "Nicola Olivetti",
                "Revantha Ramanayake"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03426v1",
                "http://arxiv.org/pdf/2312.03426v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "math.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03425v1",
            "title": "Topological phase singularities in light reflection from non-Hermitian\n  uniaxial media",
            "updated": "2023-12-06T11:13:43Z",
            "published": "2023-12-06T11:13:43Z",
            "summary": "Perfect light transmission into a dielectric at the Brewster angle is one of\nthe simplest effects in macroscopic electromagnetism. The common wisdom states\nthat absorption in the dielectric violates Brewster angle and leads to a\nnon-vanishing reflection. Yet, incorporating anisotropy may recover perfect\ntransmission of $p$-polarized light into the absorbing medium. Unlike the\ntraditional \"lossless\" Brewster angle, perfect transmission in this case is\naccompanied by phase singularities of the reflection amplitude. In this paper,\nwe examine theoretically phase singularities and the associated topological\ncharges emerging in the wavelength-incidence angle space upon perfect\ntransmission into absorbing uniaxial dielectrics. We derive the analytical\ncriterion of perfect light transmission into an anisotropic medium, demonstrate\nphase singularities in these scenarios, and study their dynamics as a function\nof material parameters. Finally, by lowering the symmetry of the problem, we\ntranslate this phenomenon into a different parameter space of wave vector\ncomponents, and illustrate the feasibility of this phenomenon with available\noptically anisotropic materials. Our results may could become valuable for the\ndevelopment of novel analog computing schemes and holography approaches.",
            "author": [
                "Valeria Maslova",
                "Petr Lebedev",
                "Denis G. Baranov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03425v1",
                "http://arxiv.org/pdf/2312.03425v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03788v1",
            "title": "SmoothQuant+: Accurate and Efficient 4-bit Post-Training\n  WeightQuantization for LLM",
            "updated": "2023-12-06T11:10:55Z",
            "published": "2023-12-06T11:10:55Z",
            "summary": "Large language models (LLMs) have shown remarkable capabilities in various\ntasks. However their huge model size and the consequent demand for\ncomputational and memory resources also pose challenges to model deployment.\nCurrently, 4-bit post-training quantization (PTQ) has achieved some success in\nLLMs, reducing the memory footprint by approximately 75% compared to FP16\nmodels, albeit with some accuracy loss. In this paper, we propose SmoothQuant+,\nan accurate and efficient 4-bit weight-only PTQ that requires no additional\ntraining, which enables lossless in accuracy for LLMs for the first time. Based\non the fact that the loss of weight quantization is amplified by the activation\noutliers, SmoothQuant+ smoothes the activation outliers by channel before\nquantization, while adjusting the corresponding weights for mathematical\nequivalence, and then performs group-wise 4-bit weight quantization for linear\nlayers. We have integrated SmoothQuant+ into the vLLM framework, an advanced\nhigh-throughput inference engine specially developed for LLMs, and equipped it\nwith an efficient W4A16 CUDA kernels, so that vLLM can seamlessly support\nSmoothQuant+ 4-bit weight quantization. Our results show that, with\nSmoothQuant+, the Code Llama-34B model can be quantized and deployed on a A100\n40GB GPU, achieving lossless accuracy and a throughput increase of 1.9 to 4.0\ntimes compared to the FP16 model deployed on two A100 40GB GPUs. Moreover, the\nlatency per token is only 68% of the FP16 model deployed on two A100 40GB GPUs.\nThis is the state-of-the-art 4-bit weight quantization for LLMs as we know.",
            "author": [
                "Jiayi Pan",
                "Chengcan Wang",
                "Kaifu Zheng",
                "Yangguang Li",
                "Zhenyu Wang",
                "Bin Feng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03788v1",
                "http://arxiv.org/pdf/2312.03788v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03420v1",
            "title": "Artist-Friendly Relightable and Animatable Neural Heads",
            "updated": "2023-12-06T11:06:46Z",
            "published": "2023-12-06T11:06:46Z",
            "summary": "An increasingly common approach for creating photo-realistic digital avatars\nis through the use of volumetric neural fields. The original neural radiance\nfield (NeRF) allowed for impressive novel view synthesis of static heads when\ntrained on a set of multi-view images, and follow up methods showed that these\nneural representations can be extended to dynamic avatars. Recently, new\nvariants also surpassed the usual drawback of baked-in illumination in neural\nrepresentations, showing that static neural avatars can be relit in any\nenvironment. In this work we simultaneously tackle both the motion and\nillumination problem, proposing a new method for relightable and animatable\nneural heads. Our method builds on a proven dynamic avatar approach based on a\nmixture of volumetric primitives, combined with a recently-proposed lightweight\nhardware setup for relightable neural fields, and includes a novel architecture\nthat allows relighting dynamic neural avatars performing unseen expressions in\nany environment, even with nearfield illumination and viewpoints.",
            "author": [
                "Yingyan Xu",
                "Prashanth Chandran",
                "Sebastian Weiss",
                "Markus Gross",
                "Gaspard Zoss",
                "Derek Bradley"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03420v1",
                "http://arxiv.org/pdf/2312.03420v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03415v1",
            "title": "Run LoRA Run: Faster and Lighter LoRA Implementations",
            "updated": "2023-12-06T10:54:34Z",
            "published": "2023-12-06T10:54:34Z",
            "summary": "LoRA is a technique that reduces the number of trainable parameters in a\nneural network by introducing low-rank adapters to linear layers. This\ntechnique is used both for fine-tuning (LoRA, QLoRA) and full train (ReLoRA).\nThis paper presents the RunLoRA framework for efficient implementations of LoRA\nthat significantly improves the speed of neural network training and\nfine-tuning using low-rank adapters. The proposed implementation optimizes the\ncomputation of LoRA operations based on dimensions of corresponding linear\nlayer, layer input dimensions and lora rank by choosing best forward and\nbackward computation graph based on FLOPs and time estimations, resulting in\nfaster training without sacrificing accuracy. The experimental results show up\nto 17% speedup on Llama family of models.",
            "author": [
                "Daria Cherniuk",
                "Aleksandr Mikhalev",
                "Ivan Oseledets"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03415v1",
                "http://arxiv.org/pdf/2312.03415v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03414v1",
            "title": "Compressed Context Memory For Online Language Model Interaction",
            "updated": "2023-12-06T10:50:43Z",
            "published": "2023-12-06T10:50:43Z",
            "summary": "This paper presents a novel context compression method for Transformer\nlanguage models in online scenarios such as ChatGPT, where the context\ncontinually expands. As the context lengthens, the attention process requires\nmore memory and computational resources, which in turn reduces the throughput\nof the language model. To this end, we propose a compressed context memory\nsystem that continually compresses the growing context into a compact memory\nspace. The compression process simply involves integrating a lightweight\nconditional LoRA into the language model's forward pass during inference. Based\non the compressed context memory, the language model can perform inference with\nreduced memory and attention operations. Through evaluations on conversation,\npersonalization, and multi-task learning, we demonstrate that our approach\nachieves the performance level of a full context model with $5\\times$ smaller\ncontext memory space. Codes are available at\nhttps://github.com/snu-mllab/context-memory.",
            "author": [
                "Jang-Hyun Kim",
                "Junyoung Yeom",
                "Sangdoo Yun",
                "Hyun Oh Song"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03414v1",
                "http://arxiv.org/pdf/2312.03414v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03413v1",
            "title": "Approximating Solutions to the Knapsack Problem using the Lagrangian\n  Dual Framework",
            "updated": "2023-12-06T10:50:27Z",
            "published": "2023-12-06T10:50:27Z",
            "summary": "The Knapsack Problem is a classic problem in combinatorial optimisation.\nSolving these problems may be computationally expensive. Recent years have seen\na growing interest in the use of deep learning methods to approximate the\nsolutions to such problems. A core problem is how to enforce or encourage\nconstraint satisfaction in predicted solutions. A promising approach for\npredicting solutions to constrained optimisation problems is the Lagrangian\nDual Framework which builds on the method of Lagrangian Relaxation. In this\npaper we develop neural network models to approximate Knapsack Problem\nsolutions using the Lagrangian Dual Framework while improving constraint\nsatisfaction. We explore the problems of output interpretation and model\nselection within this context. Experimental results show strong constraint\nsatisfaction with a minor reduction of optimality as compared to a baseline\nneural network which does not explicitly model the constraints.",
            "author": [
                "Mitchell Keegan",
                "Mahdi Abolghasemi"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-981-99-8388-9_37",
                "http://arxiv.org/abs/2312.03413v1",
                "http://arxiv.org/pdf/2312.03413v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03409v1",
            "title": "DeepPyramid+: Medical Image Segmentation using Pyramid View Fusion and\n  Deformable Pyramid Reception",
            "updated": "2023-12-06T10:47:11Z",
            "published": "2023-12-06T10:47:11Z",
            "summary": "Semantic Segmentation plays a pivotal role in many applications related to\nmedical image and video analysis. However, designing a neural network\narchitecture for medical image and surgical video segmentation is challenging\ndue to the diverse features of relevant classes, including heterogeneity,\ndeformability, transparency, blunt boundaries, and various distortions. We\npropose a network architecture, DeepPyramid+, which addresses diverse\nchallenges encountered in medical image and surgical video segmentation. The\nproposed DeepPyramid+ incorporates two major modules, namely \"Pyramid View\nFusion\" (PVF) and \"Deformable Pyramid Reception,\" (DPR), to address the\noutlined challenges. PVF replicates a deduction process within the neural\nnetwork, aligning with the human visual system, thereby enhancing the\nrepresentation of relative information at each pixel position. Complementarily,\nDPR introduces shape- and scale-adaptive feature extraction techniques using\ndilated deformable convolutions, enhancing accuracy and robustness in handling\nheterogeneous classes and deformable shapes. Extensive experiments conducted on\ndiverse datasets, including endometriosis videos, MRI images, OCT scans, and\ncataract and laparoscopy videos, demonstrate the effectiveness of DeepPyramid+\nin handling various challenges such as shape and scale variation, reflection,\nand blur degradation. DeepPyramid+ demonstrates significant improvements in\nsegmentation performance, achieving up to a 3.65% increase in Dice coefficient\nfor intra-domain segmentation and up to a 17% increase in Dice coefficient for\ncross-domain segmentation. DeepPyramid+ consistently outperforms\nstate-of-the-art networks across diverse modalities considering different\nbackbone networks, showcasing its versatility.",
            "author": [
                "Negin Ghamsarian",
                "Sebastian Wolf",
                "Martin Zinkernagel",
                "Klaus Schoeffmann",
                "Raphael Sznitman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03409v1",
                "http://arxiv.org/pdf/2312.03409v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03408v1",
            "title": "Open-sourced Data Ecosystem in Autonomous Driving: the Present and\n  Future",
            "updated": "2023-12-06T10:46:53Z",
            "published": "2023-12-06T10:46:53Z",
            "summary": "With the continuous maturation and application of autonomous driving\ntechnology, a systematic examination of open-source autonomous driving datasets\nbecomes instrumental in fostering the robust evolution of the industry\necosystem. Current autonomous driving datasets can broadly be categorized into\ntwo generations. The first-generation autonomous driving datasets are\ncharacterized by relatively simpler sensor modalities, smaller data scale, and\nis limited to perception-level tasks. KITTI, introduced in 2012, serves as a\nprominent representative of this initial wave. In contrast, the\nsecond-generation datasets exhibit heightened complexity in sensor modalities,\ngreater data scale and diversity, and an expansion of tasks from perception to\nencompass prediction and control. Leading examples of the second generation\ninclude nuScenes and Waymo, introduced around 2019. This comprehensive review,\nconducted in collaboration with esteemed colleagues from both academia and\nindustry, systematically assesses over seventy open-source autonomous driving\ndatasets from domestic and international sources. It offers insights into\nvarious aspects, such as the principles underlying the creation of high-quality\ndatasets, the pivotal role of data engine systems, and the utilization of\ngenerative foundation models to facilitate scalable data generation.\nFurthermore, this review undertakes an exhaustive analysis and discourse\nregarding the characteristics and data scales that future third-generation\nautonomous driving datasets should possess. It also delves into the scientific\nand technical challenges that warrant resolution. These endeavors are pivotal\nin advancing autonomous innovation and fostering technological enhancement in\ncritical domains. For further details, please refer to\nhttps://github.com/OpenDriveLab/DriveAGI.",
            "author": [
                "Hongyang Li",
                "Yang Li",
                "Huijie Wang",
                "Jia Zeng",
                "Pinlong Cai",
                "Huilin Xu",
                "Dahua Lin",
                "Junchi Yan",
                "Feng Xu",
                "Lu Xiong",
                "Jingdong Wang",
                "Futang Zhu",
                "Kai Yan",
                "Chunjing Xu",
                "Tiancai Wang",
                "Beipeng Mu",
                "Shaoqing Ren",
                "Zhihui Peng",
                "Yu Qiao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03408v1",
                "http://arxiv.org/pdf/2312.03408v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03784v1",
            "title": "Computation of the optimal error exponent function for fixed-length\n  lossy source coding in discrete memoryless sources",
            "updated": "2023-12-06T10:44:02Z",
            "published": "2023-12-06T10:44:02Z",
            "summary": "The error exponent of fixed-length lossy source coding was established by\nMarton. Ahlswede showed that this exponent can be discontinuous at a rate $R$,\ndepending on the source distribution $P$ and the distortion measure $d(x,y)$.\nThe reason for the discontinuity in the error exponent is that there exists a\ndistortion measure $d(x,y)$ and a distortion level $\\Delta$ such that the\nrate-distortion function $R(\\Delta|P)$ is neither concave nor quasi-concave\nwith respect to $P$. Arimoto's algorithm for computing the error exponent in\nlossy source coding is based on Blahut's parametric representation of the error\nexponent. However, Blahut's parametric representation is a lower convex\nenvelope of Marton's exponent, and the two do not generally agree. A major\ncontribution of this paper is to provide a parametric representation that\nperfectly matches the inverse function of Marton's exponent, thereby preventing\nthe problems arising from the above-mentioned non-concavity of $R(\\Delta|P)$.\nFor fixed parameters, an optimal distribution can be obtained using Arimoto's\nalgorithm. By performing a nonconvex optimization over the parameters, the\ninverse function of Marton's exponent is obtained.",
            "author": [
                "Yutaka Jitsumatsu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03784v1",
                "http://arxiv.org/pdf/2312.03784v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03406v2",
            "title": "SVQ: Sparse Vector Quantization for Spatiotemporal Forecasting",
            "updated": "2023-12-07T01:24:54Z",
            "published": "2023-12-06T10:42:40Z",
            "summary": "Spatiotemporal forecasting tasks, such as weather forecasting and traffic\nprediction, offer significant societal benefits. These tasks can be effectively\napproached as image forecasting problems using computer vision models. Vector\nquantization (VQ) is a well-known method for discrete representation that\nimproves the latent space, leading to enhanced generalization and transfer\nlearning capabilities. One of the main challenges in using VQ for\nspatiotemporal forecasting is how to balance between keeping enough details and\nremoving noises from the original patterns for better generalization. We\naddress this challenge by developing sparse vector quantization, or {\\bf SVQ}\nfor short, that leverages sparse regression to make better trade-off between\nthe two objectives. The main innovation of this work is to approximate sparse\nregression by a two-layer MLP and a randomly fixed or learnable matrix,\ndramatically improving its computational efficiency. Through experiments\nconducted on diverse datasets in multiple fields including weather forecasting,\ntraffic flow prediction, and video forecasting, we unequivocally demonstrate\nthat our proposed method consistently enhances the performance of base models\nand achieves state-of-the-art results across all benchmarks.",
            "author": [
                "Chao Chen",
                "Tian Zhou",
                "Yanjun Zhao",
                "Hui Liu",
                "Liang Sun",
                "Rong Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03406v2",
                "http://arxiv.org/pdf/2312.03406v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03405v1",
            "title": "Nonlinear magnetotransport in MoTe${}_2$",
            "updated": "2023-12-06T10:41:23Z",
            "published": "2023-12-06T10:41:23Z",
            "summary": "The shape of the Fermi surface influences many physical phenomena in\nmaterials and a growing interest in how the spin-dependent properties are\nrelated to the fermiology of crystals has surged. Recently, a novel\ncurrent-dependent nonlinear magnetoresistance effect, known as bilinear\nmagnetoelectric resistance (BMR), has been shown to be not only sensitive to\nthe spin-texture in spin-polarized non-magnetic materials, but also dependent\non the convexity of the Fermi surface in topological semimetals. In this paper,\nwe show that the temperature dependence of the BMR signal strongly depends on\nthe crystal axis of the semimetallic MoTe${}_2$. For the a-axis, the amplitude\nof the signal remains fairly constant, while for the b-axis it reverses sign at\nabout 100 K. We calculate the BMR efficiencies at 10 K to be $\\chi^{J}_{A} =\n(100\\pm3)$ nm${}^2$T${}^{-1}$A${}^{-1}$ and $\\chi^{J}_{B} = (-364\\pm13)$\nnm${}^2$T${}^{-1}$A${}^{-1}$ for the a- and b-axis, respectively, and we find\nthat they are comparable to the efficiencies measured for WTe${}_2$. We use\ndensity functional theory calculations to compute the Fermi surfaces of both\nphases at different energy levels and we observe a change in convexity of the\nouter-most electron pocket as a function of the Fermi energy. Our results\nsuggest that the BMR signal is mostly dominated by the change in the Fermi\nsurface convexity.",
            "author": [
                "Anna C. Marx",
                "Homayoun Jafari",
                "Eelco Tekelenburg",
                "Maria A. Loi",
                "Jagoda Slawinska",
                "Marcos H. D. Guimaraes"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03405v1",
                "http://arxiv.org/pdf/2312.03405v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03401v1",
            "title": "Predicting Postoperative Intraocular Lens Dislocation in Cataract\n  Surgery via Deep Learning",
            "updated": "2023-12-06T10:27:15Z",
            "published": "2023-12-06T10:27:15Z",
            "summary": "A critical yet unpredictable complication following cataract surgery is\nintraocular lens dislocation. Postoperative stability is imperative, as even a\ntiny decentration of multifocal lenses or inadequate alignment of the torus in\ntoric lenses due to postoperative rotation can lead to a significant drop in\nvisual acuity. Investigating possible intraoperative indicators that can\npredict post-surgical instabilities of intraocular lenses can help prevent this\ncomplication. In this paper, we develop and evaluate the first fully-automatic\nframework for the computation of lens unfolding delay, rotation, and\ninstability during surgery. Adopting a combination of three types of CNNs,\nnamely recurrent, region-based, and pixel-based, the proposed framework is\nemployed to assess the possibility of predicting post-operative lens\ndislocation during cataract surgery. This is achieved via performing a\nlarge-scale study on the statistical differences between the behavior of\ndifferent brands of intraocular lenses and aligning the results with expert\nsurgeons' hypotheses and observations about the lenses. We exploit a\nlarge-scale dataset of cataract surgery videos featuring four intraocular lens\nbrands. Experimental results confirm the reliability of the proposed framework\nin evaluating the lens' statistics during the surgery. The Pearson correlation\nand t-test results reveal significant correlations between lens unfolding delay\nand lens rotation and significant differences between the intra-operative\nrotations stability of four groups of lenses. These results suggest that the\nproposed framework can help surgeons select the lenses based on the patient's\neye conditions and predict post-surgical lens dislocation.",
            "author": [
                "Negin Ghamsarian",
                "Doris Putzgruber-Adamitsch",
                "Stephanie Sarny",
                "Raphael Sznitman",
                "Klaus Schoeffmann",
                "Yosuf El-Shabrawi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03401v1",
                "http://arxiv.org/pdf/2312.03401v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03395v1",
            "title": "Diffused Task-Agnostic Milestone Planner",
            "updated": "2023-12-06T10:09:22Z",
            "published": "2023-12-06T10:09:22Z",
            "summary": "Addressing decision-making problems using sequence modeling to predict future\ntrajectories shows promising results in recent years. In this paper, we take a\nstep further to leverage the sequence predictive method in wider areas such as\nlong-term planning, vision-based control, and multi-task decision-making. To\nthis end, we propose a method to utilize a diffusion-based generative sequence\nmodel to plan a series of milestones in a latent space and to have an agent to\nfollow the milestones to accomplish a given task. The proposed method can learn\ncontrol-relevant, low-dimensional latent representations of milestones, which\nmakes it possible to efficiently perform long-term planning and vision-based\ncontrol. Furthermore, our approach exploits generation flexibility of the\ndiffusion model, which makes it possible to plan diverse trajectories for\nmulti-task decision-making. We demonstrate the proposed method across offline\nreinforcement learning (RL) benchmarks and an visual manipulation environment.\nThe results show that our approach outperforms offline RL methods in solving\nlong-horizon, sparse-reward tasks and multi-task problems, while also achieving\nthe state-of-the-art performance on the most challenging vision-based\nmanipulation benchmark.",
            "author": [
                "Mineui Hong",
                "Minjae Kang",
                "Songhwai Oh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03395v1",
                "http://arxiv.org/pdf/2312.03395v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03392v1",
            "title": "O'Neill's Theorem for Games",
            "updated": "2023-12-06T10:04:03Z",
            "published": "2023-12-06T10:04:03Z",
            "summary": "We present an analog of O'Neill's Theorem (Theorem 5.2 in [17]) for finite\ngames, which reveals a picture of the structure of equilibria under payoff\nperturbations in finite games.",
            "author": [
                "Srihari Govindan",
                "Rida Laraki",
                "Lucas Pahl"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03392v1",
                "http://arxiv.org/pdf/2312.03392v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03391v1",
            "title": "Action Scene Graphs for Long-Form Understanding of Egocentric Videos",
            "updated": "2023-12-06T10:01:43Z",
            "published": "2023-12-06T10:01:43Z",
            "summary": "We present Egocentric Action Scene Graphs (EASGs), a new representation for\nlong-form understanding of egocentric videos. EASGs extend standard\nmanually-annotated representations of egocentric videos, such as verb-noun\naction labels, by providing a temporally evolving graph-based description of\nthe actions performed by the camera wearer, including interacted objects, their\nrelationships, and how actions unfold in time. Through a novel annotation\nprocedure, we extend the Ego4D dataset by adding manually labeled Egocentric\nAction Scene Graphs offering a rich set of annotations designed for long-from\negocentric video understanding. We hence define the EASG generation task and\nprovide a baseline approach, establishing preliminary benchmarks. Experiments\non two downstream tasks, egocentric action anticipation and egocentric activity\nsummarization, highlight the effectiveness of EASGs for long-form egocentric\nvideo understanding. We will release the dataset and the code to replicate\nexperiments and annotations.",
            "author": [
                "Ivan Rodin",
                "Antonino Furnari",
                "Kyle Min",
                "Subarna Tripathi",
                "Giovanni Maria Farinella"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03391v1",
                "http://arxiv.org/pdf/2312.03391v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03782v1",
            "title": "Novel class discovery meets foundation models for 3D semantic\n  segmentation",
            "updated": "2023-12-06T09:59:30Z",
            "published": "2023-12-06T09:59:30Z",
            "summary": "The task of Novel Class Discovery (NCD) in semantic segmentation entails\ntraining a model able to accurately segment unlabelled (novel) classes, relying\non the available supervision from annotated (base) classes. Although\nextensively investigated in 2D image data, the extension of the NCD task to the\ndomain of 3D point clouds represents a pioneering effort, characterized by\nassumptions and challenges that are not present in the 2D case. This paper\nrepresents an advancement in the analysis of point cloud data in four\ndirections. Firstly, it introduces the novel task of NCD for point cloud\nsemantic segmentation. Secondly, it demonstrates that directly transposing the\nonly existing NCD method for 2D image semantic segmentation to 3D data yields\nsuboptimal results. Thirdly, a new NCD approach based on online clustering,\nuncertainty estimation, and semantic distillation is presented. Lastly, a novel\nevaluation protocol is proposed to rigorously assess the performance of NCD in\npoint cloud semantic segmentation. Through comprehensive evaluations on the\nSemanticKITTI, SemanticPOSS, and S3DIS datasets, the paper demonstrates\nsubstantial superiority of the proposed method over the considered baselines.",
            "author": [
                "Luigi Riz",
                "Cristiano Saltori",
                "Yiming Wang",
                "Elisa Ricci",
                "Fabio Poiesi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03782v1",
                "http://arxiv.org/pdf/2312.03782v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03388v1",
            "title": "High power narrow linewidth laser for scaling barium ion optical qubit",
            "updated": "2023-12-06T09:56:59Z",
            "published": "2023-12-06T09:56:59Z",
            "summary": "The linewidth of a laser plays a pivotal role in ensuring high fidelity of\nion trap quantum processors and optical clocks. As quantum computing endeavors\nscale up, the demand for higher laser power with ultra-narrow linewidth becomes\nimperative, and leveraging fiber amplifiers emerges as a promising approach to\nmeet these requirements. This study explores the effectiveness of Thulium-doped\nfiber amplifiers as a viable solution for addressing optical qubit transitions\nin trapped barium ion qubits. We demonstrate that by performing high-fidelity\ngates on the qubit while introducing minimal intensity noise, TDFAs do not\nsignificantly broaden the linewidth of the seed lasers. We employed a Voigt\nfitting scheme in conjunction with a delayed self-heterodyne method to\naccurately measure the linewidth independently, corroborating our findings\nthrough quadrupole spectroscopy with trapped barium ions. Our results show\nlinewidth values of $\\sim$ $160$~Hz and $156$~Hz, respectively, using these two\nmethods, underscoring the reliability of our measurement techniques. The slight\nvariation between the two methods may be attributed to factors such as\namplified spontaneous emission in the TDFA or the influence of $1/f$ noise\nwithin the heterodyne setup delay line. These contribute to advancing our\nunderstanding of laser linewidth control in the context of ion trap quantum\ncomputing as well as stretching the availability of narrow linewidth,\nhigh-power tunable lasers beyond the C-band.",
            "author": [
                "Morteza Ahmadi",
                "Tarun Dutta",
                "Manas Mukherjee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03388v1",
                "http://arxiv.org/pdf/2312.03388v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "physics.atom-ph",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03387v1",
            "title": "Three-dimensional harmonic oscillator as a quantum Otto engine",
            "updated": "2023-12-06T09:52:53Z",
            "published": "2023-12-06T09:52:53Z",
            "summary": "A quantum Otto engine based on a three-dimensional harmonic oscillator is\nproposed. One of the modes of this oscillator functions as the working fluid,\nwhile the other two play the role of baths. The coupling between the working\nfluid and the baths is controlled using an external central potential. All four\nstrokes of the engine are simulated numerically, exploring the nonadiabatic\neffects in the compression and expansion phases, as well as the energy transfer\nduring the working fluid's contact with the baths. The efficiency and power of\nseveral realizations of the proposed engine are also computed with the former\nagreeing well with the theoretical predictions for the quantum Otto cycle.",
            "author": [
                "Aleksandr Rodin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03387v1",
                "http://arxiv.org/pdf/2312.03387v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03781v1",
            "title": "Lite-Mind: Towards Efficient and Versatile Brain Representation Network",
            "updated": "2023-12-06T09:39:38Z",
            "published": "2023-12-06T09:39:38Z",
            "summary": "Research in decoding visual information from the brain, particularly through\nthe non-invasive fMRI method, is rapidly progressing. The challenge arises from\nthe limited data availability and the low signal-to-noise ratio of fMRI\nsignals, leading to a low-precision task of fMRI-to-image retrieval.\nState-of-the-art MindEye remarkably improves fMRI-to-image retrieval\nperformance by leveraging a deep MLP with a high parameter count orders of\nmagnitude, i.e., a 996M MLP Backbone per subject, to align fMRI embeddings to\nthe final hidden layer of CLIP's vision transformer. However, significant\nindividual variations exist among subjects, even within identical experimental\nsetups, mandating the training of subject-specific models. The substantial\nparameters pose significant challenges in deploying fMRI decoding on practical\ndevices, especially with the necessitating of specific models for each subject.\nTo this end, we propose Lite-Mind, a lightweight, efficient, and versatile\nbrain representation network based on discrete Fourier transform, that\nefficiently aligns fMRI voxels to fine-grained information of CLIP. Our\nexperiments demonstrate that Lite-Mind achieves an impressive 94.3%\nfMRI-to-image retrieval accuracy on the NSD dataset for Subject 1, with 98.7%\nfewer parameters than MindEye. Lite-Mind is also proven to be able to be\nmigrated to smaller brain datasets and establishes a new state-of-the-art for\nzero-shot classification on the GOD dataset. The code is available at\nhttps://github.com/gongzix/Lite-Mind.",
            "author": [
                "Zixuan Gong",
                "Qi Zhang",
                "Duoqian Miao",
                "Guangyin Bao",
                "Liang Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03781v1",
                "http://arxiv.org/pdf/2312.03781v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03379v1",
            "title": "A Text-to-Text Model for Multilingual Offensive Language Identification",
            "updated": "2023-12-06T09:37:27Z",
            "published": "2023-12-06T09:37:27Z",
            "summary": "The ubiquity of offensive content on social media is a growing cause for\nconcern among companies and government organizations. Recently,\ntransformer-based models such as BERT, XLNET, and XLM-R have achieved\nstate-of-the-art performance in detecting various forms of offensive content\n(e.g. hate speech, cyberbullying, and cyberaggression). However, the majority\nof these models are limited in their capabilities due to their encoder-only\narchitecture, which restricts the number and types of labels in downstream\ntasks. Addressing these limitations, this study presents the first pre-trained\nmodel with encoder-decoder architecture for offensive language identification\nwith text-to-text transformers (T5) trained on two large offensive language\nidentification datasets; SOLID and CCTK. We investigate the effectiveness of\ncombining two datasets and selecting an optimal threshold in semi-supervised\ninstances in SOLID in the T5 retraining step. Our pre-trained T5 model\noutperforms other transformer-based models fine-tuned for offensive language\ndetection, such as fBERT and HateBERT, in multiple English benchmarks.\nFollowing a similar approach, we also train the first multilingual pre-trained\nmodel for offensive language identification using mT5 and evaluate its\nperformance on a set of six different languages (German, Hindi, Korean,\nMarathi, Sinhala, and Spanish). The results demonstrate that this multilingual\nmodel achieves a new state-of-the-art on all the above datasets, showing its\nusefulness in multilingual scenarios. Our proposed T5-based models will be made\nfreely available to the community.",
            "author": [
                "Tharindu Ranasinghe",
                "Marcos Zampieri"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03379v1",
                "http://arxiv.org/pdf/2312.03379v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03378v1",
            "title": "Riemannian Complex Matrix Convolution Network for PolSAR Image\n  Classification",
            "updated": "2023-12-06T09:33:33Z",
            "published": "2023-12-06T09:33:33Z",
            "summary": "Recently, deep learning methods have achieved superior performance for\nPolarimetric Synthetic Aperture Radar(PolSAR) image classification. Existing\ndeep learning methods learn PolSAR data by converting the covariance matrix\ninto a feature vector or complex-valued vector as the input. However, all these\nmethods cannot learn the structure of complex matrix directly and destroy the\nchannel correlation. To learn geometric structure of complex matrix, we propose\na Riemannian complex matrix convolution network for PolSAR image classification\nin Riemannian space for the first time, which directly utilizes the complex\nmatrix as the network input and defines the Riemannian operations to learn\ncomplex matrix's features. The proposed Riemannian complex matrix convolution\nnetwork considers PolSAR complex matrix endowed in Riemannian manifold, and\ndefines a series of new Riemannian convolution, ReLu and LogEig operations in\nRiemannian space, which breaks through the Euclidean constraint of conventional\nnetworks. Then, a CNN module is appended to enhance contextual Riemannian\nfeatures. Besides, a fast kernel learning method is developed for the proposed\nmethod to learn class-specific features and reduce the computation time\neffectively. Experiments are conducted on three sets of real PolSAR data with\ndifferent bands and sensors. Experiments results demonstrates the proposed\nmethod can obtain superior performance than the state-of-the-art methods.",
            "author": [
                "Junfei Shi",
                "Wei Wang",
                "Haiyan Jin",
                "Mengmeng Nie",
                "Shanshan Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03378v1",
                "http://arxiv.org/pdf/2312.03378v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03375v1",
            "title": "A novel coupled-cluster singles and doubles implementation that combines\n  the exploitation of point-group symmetry and Cholesky decomposition of the\n  two-electron integrals",
            "updated": "2023-12-06T09:22:14Z",
            "published": "2023-12-06T09:22:14Z",
            "summary": "A novel implementation of the coupled-cluster singles and doubles (CCSD)\napproach is presented that is specifically tailored for the treatment of large,\nsymmetric systems. It fully exploits Abelian point-group symmetry and the use\nof the Cholesky decomposition of the two-electron repulsion integrals. In\naccordance with modern CCSD algorithms, we propose two alternative strategies\nfor the computation of the so-called particle-particle ladder term. The code is\ndriven towards the optimal choice depending on the available hardware\nresources. As a large-scale application, we computed the frozen-core\ncorrelation energy of buckminsterfullerene (C$_{60}$) with a polarized valence\ntriple-zeta basis set (240 correlated electrons in 1740 orbitals).",
            "author": [
                "Tommaso Nottoli",
                "J\u00fcrgen Gauss",
                "Filippo Lipparini"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03375v1",
                "http://arxiv.org/pdf/2312.03375v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03372v1",
            "title": "Evaluating the point cloud of individual trees generated from images\n  based on Neural Radiance fields (NeRF) method",
            "updated": "2023-12-06T09:13:34Z",
            "published": "2023-12-06T09:13:34Z",
            "summary": "Three-dimensional (3D) reconstruction of trees has always been a key task in\nprecision forestry management and research. Due to the complex branch\nmorphological structure of trees themselves and the occlusions from tree stems,\nbranches and foliage, it is difficult to recreate a complete three-dimensional\ntree model from a two-dimensional image by conventional photogrammetric\nmethods. In this study, based on tree images collected by various cameras in\ndifferent ways, the Neural Radiance Fields (NeRF) method was used for\nindividual tree reconstruction and the exported point cloud models are compared\nwith point cloud derived from photogrammetric reconstruction and laser scanning\nmethods. The results show that the NeRF method performs well in individual tree\n3D reconstruction, as it has higher successful reconstruction rate, better\nreconstruction in the canopy area, it requires less amount of images as input.\nCompared with photogrammetric reconstruction method, NeRF has significant\nadvantages in reconstruction efficiency and is adaptable to complex scenes, but\nthe generated point cloud tends to be noisy and low resolution. The accuracy of\ntree structural parameters (tree height and diameter at breast height)\nextracted from the photogrammetric point cloud is still higher than those of\nderived from the NeRF point cloud. The results of this study illustrate the\ngreat potential of NeRF method for individual tree reconstruction, and it\nprovides new ideas and research directions for 3D reconstruction and\nvisualization of complex forest scenes.",
            "author": [
                "Hongyu Huang",
                "Guoji Tian",
                "Chongcheng Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03372v1",
                "http://arxiv.org/pdf/2312.03372v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03368v1",
            "title": "Bottom-Up Instance Segmentation of Catheters for Chest X-Rays",
            "updated": "2023-12-06T09:09:27Z",
            "published": "2023-12-06T09:09:27Z",
            "summary": "Chest X-ray (CXR) is frequently employed in emergency departments and\nintensive care units to verify the proper placement of central lines and tubes\nand to rule out related complications. The automation of the X-ray reading\nprocess can be a valuable support tool for non-specialist technicians and\nminimize reporting delays due to non-availability of experts. While existing\nsolutions for automated catheter segmentation and malposition detection show\npromising results, the disentanglement of individual catheters remains an open\nchallenge, especially in complex cases where multiple devices appear\nsuperimposed in the X-ray projection. Moreover, conventional top-down instance\nsegmentation methods are ineffective on such thin and long devices, that often\nextend through the entire image. In this paper, we propose a deep learning\napproach based on associative embeddings for catheter instance segmentation,\nable to overcome those limitations and effectively handle device intersections.",
            "author": [
                "Francesca Boccardi",
                "Axel Saalbach",
                "Heinrich Schulz",
                "Samuele Salti",
                "Ilyas Sirazitdinov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03368v1",
                "http://arxiv.org/pdf/2312.03368v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03367v1",
            "title": "Lazy-k: Decoding for Constrained Token Classification",
            "updated": "2023-12-06T09:08:32Z",
            "published": "2023-12-06T09:08:32Z",
            "summary": "We explore the possibility of improving probabilistic models in structured\nprediction. Specifically, we combine the models with constrained decoding\napproaches in the context of token classification for information extraction.\nThe decoding methods search for constraint-satisfying label-assignments while\nmaximizing the total probability. To do this, we evaluate several existing\napproaches, as well as propose a novel decoding method called Lazy-$k$. Our\nfindings demonstrate that constrained decoding approaches can significantly\nimprove the models' performances, especially when using smaller models. The\nLazy-$k$ approach allows for more flexibility between decoding time and\naccuracy. The code for using Lazy-$k$ decoding can be found here:\nhttps://github.com/ArthurDevNL/lazyk.",
            "author": [
                "Arthur Hemmer",
                "Micka\u00ebl Coustaty",
                "Nicola Bartolo",
                "J\u00e9r\u00f4me Brachat",
                "Jean-Marc Ogier"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03367v1",
                "http://arxiv.org/pdf/2312.03367v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03365v1",
            "title": "Demand response for residential building heating: Effective Monte Carlo\n  Tree Search control based on physics-informed neural networks",
            "updated": "2023-12-06T09:06:14Z",
            "published": "2023-12-06T09:06:14Z",
            "summary": "Controlling energy consumption in buildings through demand response (DR) has\nbecome increasingly important to reduce global carbon emissions and limit\nclimate change. In this paper, we specifically focus on controlling the heating\nsystem of a residential building to optimize its energy consumption while\nrespecting user's thermal comfort. Recent works in this area have mainly\nfocused on either model-based control, e.g., model predictive control (MPC), or\nmodel-free reinforcement learning (RL) to implement practical DR algorithms. A\nspecific RL method that recently has achieved impressive success in domains\nsuch as board games (go, chess) is Monte Carlo Tree Search (MCTS). Yet, for\nbuilding control it has remained largely unexplored. Thus, we study MCTS\nspecifically for building demand response. Its natural structure allows a\nflexible optimization that implicitly integrate exogenous constraints (as\nopposed, for example, to conventional RL solutions), making MCTS a promising\ncandidate for DR control problems. We demonstrate how to improve MCTS control\nperformance by incorporating a Physics-informed Neural Network (PiNN) model for\nits underlying thermal state prediction, as opposed to traditional purely\ndata-driven Black-Box approaches. Our MCTS implementation aligned with a PiNN\nmodel is able to obtain a 3% increment of the obtained reward compared to a\nrule-based controller; leading to a 10% cost reduction and 35% reduction on\ntemperature difference with the desired one when applied to an artificial price\nprofile. We further implemented a Deep Learning layer into the Monte Carlo Tree\nSearch technique using a neural network that leads the tree search through more\noptimal nodes. We then compared this addition with its Vanilla version, showing\nthe improvement in computational cost required.",
            "author": [
                "Fabio Pavirani",
                "Gargya Gokhale",
                "Bert Claessens",
                "Chris Develder"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03365v1",
                "http://arxiv.org/pdf/2312.03365v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.AI",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03361v1",
            "title": "KhabarChin: Automatic Detection of Important News in the Persian\n  Language",
            "updated": "2023-12-06T09:01:21Z",
            "published": "2023-12-06T09:01:21Z",
            "summary": "Being aware of important news is crucial for staying informed and making\nwell-informed decisions efficiently. Natural Language Processing (NLP)\napproaches can significantly automate this process. This paper introduces the\ndetection of important news, in a previously unexplored area, and presents a\nnew benchmarking dataset (Khabarchin) for detecting important news in the\nPersian language. We define important news articles as those deemed significant\nfor a considerable portion of society, capable of influencing their mindset or\ndecision-making. The news articles are obtained from seven different prominent\nPersian news agencies, resulting in the annotation of 7,869 samples and the\ncreation of the dataset. Two challenges of high disagreement and imbalance\nbetween classes were faced, and solutions were provided for them. We also\npropose several learning-based models, ranging from conventional machine\nlearning to state-of-the-art transformer models, to tackle this task.\nFurthermore, we introduce the second task of important sentence detection in\nnews articles, as they often come with a significant contextual length that\nmakes it challenging for readers to identify important information. We identify\nthese sentences in a weakly supervised manner.",
            "author": [
                "Hamed Hematian Hemati",
                "Arash Lagzian",
                "Moein Salimi Sartakhti",
                "Hamid Beigy",
                "Ehsaneddin Asgari"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03361v1",
                "http://arxiv.org/pdf/2312.03361v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03360v1",
            "title": "Teaching Specific Scientific Knowledge into Large Language Models\n  through Additional Training",
            "updated": "2023-12-06T08:55:55Z",
            "published": "2023-12-06T08:55:55Z",
            "summary": "Through additional training, we explore embedding specialized scientific\nknowledge into the Llama 2 Large Language Model (LLM). Key findings reveal that\neffective knowledge integration requires reading texts from multiple\nperspectives, especially in instructional formats. We utilize text augmentation\nto tackle the scarcity of specialized texts, including style conversions and\ntranslations. Hyperparameter optimization proves crucial, with different size\nmodels (7b, 13b, and 70b) reasonably undergoing additional training. Validating\nour methods, we construct a dataset of 65,000 scientific papers. Although we\nhave succeeded in partially embedding knowledge, the study highlights the\ncomplexities and limitations of incorporating specialized information into\nLLMs, suggesting areas for further improvement.",
            "author": [
                "Kan Hatakeyama-Sato",
                "Yasuhiko Igarashi",
                "Shun Katakami",
                "Yuta Nabae",
                "Teruaki Hayakawa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03360v1",
                "http://arxiv.org/pdf/2312.03360v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03357v1",
            "title": "RING-NeRF: A Versatile Architecture based on Residual Implicit Neural\n  Grids",
            "updated": "2023-12-06T08:54:04Z",
            "published": "2023-12-06T08:54:04Z",
            "summary": "Since their introduction, Neural Fields have become very popular for 3D\nreconstruction and new view synthesis. Recent researches focused on\naccelerating the process, as well as improving the robustness to variation of\nthe observation distance and limited number of supervised viewpoints. However,\nthose approaches often led to dedicated solutions that cannot be easily\ncombined. To tackle this issue, we introduce a new simple but efficient\narchitecture named RING-NeRF, based on Residual Implicit Neural Grids, that\nprovides a control on the level of detail of the mapping function between the\nscene and the latent spaces. Associated with a distance-aware forward mapping\nmechanism and a continuous coarse-to-fine reconstruction process, our versatile\narchitecture demonstrates both fast training and state-of-the-art performances\nin terms of: (1) anti-aliased rendering, (2) reconstruction quality from few\nsupervised viewpoints, and (3) robustness in the absence of appropriate\nscene-specific initialization for SDF-based NeRFs. We also demonstrate that our\narchitecture can dynamically add grids to increase the details of the\nreconstruction, opening the way to adaptive reconstruction.",
            "author": [
                "Doriand Petit",
                "Steve Bourgeois",
                "Dumitru Pavel",
                "Vincent Gay-Bellile",
                "Florian Chabot",
                "Loic Barthe"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03357v1",
                "http://arxiv.org/pdf/2312.03357v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03355v1",
            "title": "Homological stability for the space of hypersurfaces with marked points",
            "updated": "2023-12-06T08:53:12Z",
            "published": "2023-12-06T08:53:12Z",
            "summary": "We study the space of smooth marked hypersurfaces in a given linear system.\nSpecifically, we prove a homology h-principle to compare it with a space of\nsections of an appropriate jet bundle. Using rational models, we compute its\nrational cohomology in a range of degrees, and deduce a homological stability\nresult for hypersurfaces of increasing degree. We also describe the Hodge\nweights on the stable cohomology, and thereby connect our topological result to\nmotivic results of Howe.",
            "author": [
                "Alexis Aumonier",
                "Ronno Das"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03355v1",
                "http://arxiv.org/pdf/2312.03355v1"
            ],
            "primary_category": "math.AT",
            "category": [
                "math.AT",
                "math.AG",
                "55R80, 14J70"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03352v1",
            "title": "Large Non-Volatile Frequency Tuning of Spin Hall Nano-Oscillators using\n  Circular Memristive Nano-Gates",
            "updated": "2023-12-06T08:50:34Z",
            "published": "2023-12-06T08:50:34Z",
            "summary": "Spin Hall nano oscillators (SHNOs) are promising candidates for neuromorphic\ncomputing due to their miniaturized dimensions, non-linearity, fast dynamics,\nand ability to synchronize in long chains and arrays. However, tuning the\nindividual SHNOs in large chains/arrays, which is key to implementing synaptic\ncontrol, has remained a challenge. Here, we demonstrate circular memristive\nnano-gates, both precisely aligned and shifted with respect to\nnano-constriction SHNOs of W/CoFeB/HfOx, with increased quality of the device\ntunability. Gating at the exact center of the nano-constriction region is found\nto cause irreversible degradation to the oxide layer, resulting in a permanent\nfrequency shift of the auto-oscillating modes. As a remedy, gates shifted\noutside of the immediate nano-constriction region can tune the frequency\ndramatically (>200 MHz) without causing any permanent change to the\nconstriction region. Circular memristive nano-gates can, therefore, be used in\nSHNO chains/arrays to manipulate the synchronization states precisely over\nlarge networks of oscillators.",
            "author": [
                "Maha Khademi",
                "Akash Kumar",
                "Mona Rajabali",
                "Saroj P. Dash",
                "Johan \u00c5kerman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03352v1",
                "http://arxiv.org/pdf/2312.03352v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03350v1",
            "title": "PointMoment:Mixed-Moment-based Self-Supervised Representation Learning\n  for 3D Point Clouds",
            "updated": "2023-12-06T08:49:55Z",
            "published": "2023-12-06T08:49:55Z",
            "summary": "Large and rich data is a prerequisite for effective training of deep neural\nnetworks. However, the irregularity of point cloud data makes manual annotation\ntime-consuming and laborious. Self-supervised representation learning, which\nleverages the intrinsic structure of large-scale unlabelled data to learn\nmeaningful feature representations, has attracted increasing attention in the\nfield of point cloud research. However, self-supervised representation learning\noften suffers from model collapse, resulting in reduced information and\ndiversity of the learned representation, and consequently degrading the\nperformance of downstream tasks. To address this problem, we propose\nPointMoment, a novel framework for point cloud self-supervised representation\nlearning that utilizes a high-order mixed moment loss function rather than the\nconventional contrastive loss function. Moreover, our framework does not\nrequire any special techniques such as asymmetric network architectures,\ngradient stopping, etc. Specifically, we calculate the high-order mixed moment\nof the feature variables and force them to decompose into products of their\nindividual moment, thereby making multiple variables more independent and\nminimizing the feature redundancy. We also incorporate a contrastive learning\napproach to maximize the feature invariance under different data augmentations\nof the same point cloud. Experimental results show that our approach\noutperforms previous unsupervised learning methods on the downstream task of 3D\npoint cloud classification and segmentation.",
            "author": [
                "Xin Cao",
                "Xinxin Han",
                "Yifan Wang",
                "Mengna Yang",
                "Kang Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03350v1",
                "http://arxiv.org/pdf/2312.03350v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03345v1",
            "title": "GraNet: A Multi-Level Graph Network for 6-DoF Grasp Pose Generation in\n  Cluttered Scenes",
            "updated": "2023-12-06T08:36:29Z",
            "published": "2023-12-06T08:36:29Z",
            "summary": "6-DoF object-agnostic grasping in unstructured environments is a critical yet\nchallenging task in robotics. Most current works use non-optimized approaches\nto sample grasp locations and learn spatial features without concerning the\ngrasping task. This paper proposes GraNet, a graph-based grasp pose generation\nframework that translates a point cloud scene into multi-level graphs and\npropagates features through graph neural networks. By building graphs at the\nscene level, object level, and grasp point level, GraNet enhances feature\nembedding at multiple scales while progressively converging to the ideal\ngrasping locations by learning. Our pipeline can thus characterize the spatial\ndistribution of grasps in cluttered scenes, leading to a higher rate of\neffective grasping. Furthermore, we enhance the representation ability of\nscalable graph networks by a structure-aware attention mechanism to exploit\nlocal relations in graphs. Our method achieves state-of-the-art performance on\nthe large-scale GraspNet-1Billion benchmark, especially in grasping unseen\nobjects (+11.62 AP). The real robot experiment shows a high success rate in\ngrasping scattered objects, verifying the effectiveness of the proposed\napproach in unstructured environments.",
            "author": [
                "Haowen Wang",
                "Wanhao Niu",
                "Chungang Zhuang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03345v1",
                "http://arxiv.org/pdf/2312.03345v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03342v1",
            "title": "Topic and genre in dialogue",
            "updated": "2023-12-06T08:33:51Z",
            "published": "2023-12-06T08:33:51Z",
            "summary": "In this paper we argue that topic plays a fundamental role in conversations,\nand that the concept is needed in addition to that of genre to define\ninteractions. In particular, the concepts of genre and topic need to be\nseparated and orthogonally defined. This would enable modular, reliable and\ncontrollable flexible-domain dialogue systems.",
            "author": [
                "Amandine Decker",
                "Ellen Breitholtz",
                "Christine Howes",
                "Staffan Larsson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03342v1",
                "http://arxiv.org/pdf/2312.03342v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03341v1",
            "title": "Online Vectorized HD Map Construction using Geometry",
            "updated": "2023-12-06T08:26:26Z",
            "published": "2023-12-06T08:26:26Z",
            "summary": "The construction of online vectorized High-Definition (HD) maps is critical\nfor downstream prediction and planning. Recent efforts have built strong\nbaselines for this task, however, shapes and relations of instances in urban\nroad systems are still under-explored, such as parallelism, perpendicular, or\nrectangle-shape. In our work, we propose GeMap ($\\textbf{Ge}$ometry\n$\\textbf{Map}$), which end-to-end learns Euclidean shapes and relations of map\ninstances beyond basic perception. Specifically, we design a geometric loss\nbased on angle and distance clues, which is robust to rigid transformations. We\nalso decouple self-attention to independently handle Euclidean shapes and\nrelations. Our method achieves new state-of-the-art performance on the NuScenes\nand Argoverse 2 datasets. Remarkably, it reaches a 71.8% mAP on the large-scale\nArgoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP\nthreshold for the first time. Code is available at\nhttps://github.com/cnzzx/GeMap",
            "author": [
                "Zhixin Zhang",
                "Yiyuan Zhang",
                "Xiaohan Ding",
                "Fusheng Jin",
                "Xiangyu Yue"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03341v1",
                "http://arxiv.org/pdf/2312.03341v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03339v1",
            "title": "PointJEM: Self-supervised Point Cloud Understanding for Reducing Feature\n  Redundancy via Joint Entropy Maximization",
            "updated": "2023-12-06T08:21:42Z",
            "published": "2023-12-06T08:21:42Z",
            "summary": "Most deep learning-based point cloud processing methods are supervised and\nrequire large scale of labeled data. However, manual labeling of point cloud\ndata is laborious and time-consuming. Self-supervised representation learning\ncan address the aforementioned issue by learning robust and generalized\nrepresentations from unlabeled datasets. Nevertheless, the embedded features\nobtained by representation learning usually contain redundant information, and\nmost current methods reduce feature redundancy by linear correlation\nconstraints. In this paper, we propose PointJEM, a self-supervised\nrepresentation learning method applied to the point cloud field. PointJEM\ncomprises an embedding scheme and a loss function based on joint entropy. The\nembedding scheme divides the embedding vector into different parts, each part\ncan learn a distinctive feature. To reduce redundant information in the\nfeatures, PointJEM maximizes the joint entropy between the different parts,\nthereby rendering the learned feature variables pairwise independent. To\nvalidate the effectiveness of our method, we conducted experiments on multiple\ndatasets. The results demonstrate that our method can significantly reduce\nfeature redundancy beyond linear correlation. Furthermore, PointJEM achieves\ncompetitive performance in downstream tasks such as classification and\nsegmentation.",
            "author": [
                "Xin Cao",
                "Huan Xia",
                "Xinxin Han",
                "Yifan Wang",
                "Kang Li",
                "Linzhi Su"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03339v1",
                "http://arxiv.org/pdf/2312.03339v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03330v1",
            "title": "Measuring Misogyny in Natural Language Generation: Preliminary Results\n  from a Case Study on two Reddit Communities",
            "updated": "2023-12-06T07:38:46Z",
            "published": "2023-12-06T07:38:46Z",
            "summary": "Generic `toxicity' classifiers continue to be used for evaluating the\npotential for harm in natural language generation, despite mounting evidence of\ntheir shortcomings. We consider the challenge of measuring misogyny in natural\nlanguage generation, and argue that generic `toxicity' classifiers are\ninadequate for this task. We use data from two well-characterised `Incel'\ncommunities on Reddit that differ primarily in their degrees of misogyny to\nconstruct a pair of training corpora which we use to fine-tune two language\nmodels. We show that an open source `toxicity' classifier is unable to\ndistinguish meaningfully between generations from these models. We contrast\nthis with a misogyny-specific lexicon recently proposed by feminist\nsubject-matter experts, demonstrating that, despite the limitations of simple\nlexicon-based approaches, this shows promise as a benchmark to evaluate\nlanguage models for misogyny, and that it is sensitive enough to reveal the\nknown differences in these Reddit communities. Our preliminary findings\nhighlight the limitations of a generic approach to evaluating harms, and\nfurther emphasise the need for careful benchmark design and selection in\nnatural language evaluation.",
            "author": [
                "Aaron J. Snoswell",
                "Lucinda Nelson",
                "Hao Xue",
                "Flora D. Salim",
                "Nicolas Suzor",
                "Jean Burgess"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03330v1",
                "http://arxiv.org/pdf/2312.03330v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03327v1",
            "title": "Building Category Graphs Representation with Spatial and Temporal\n  Attention for Visual Navigation",
            "updated": "2023-12-06T07:28:43Z",
            "published": "2023-12-06T07:28:43Z",
            "summary": "Given an object of interest, visual navigation aims to reach the object's\nlocation based on a sequence of partial observations. To this end, an agent\nneeds to 1) learn a piece of certain knowledge about the relations of object\ncategories in the world during training and 2) look for the target object based\non the pre-learned object category relations and its moving trajectory in the\ncurrent unseen environment. In this paper, we propose a Category Relation Graph\n(CRG) to learn the knowledge of object category layout relations and a\nTemporal-Spatial-Region (TSR) attention architecture to perceive the long-term\nspatial-temporal dependencies of objects helping the navigation. We learn prior\nknowledge of object layout, establishing a category relationship graph to\ndeduce the positions of specific objects. Subsequently, we introduced TSR to\ncapture the relationships of objects in temporal, spatial, and regions within\nthe observation trajectories. Specifically, we propose a Temporal attention\nmodule (T) to model the temporal structure of the observation sequence, which\nimplicitly encodes the historical moving or trajectory information. Then, a\nSpatial attention module (S) is used to uncover the spatial context of the\ncurrent observation objects based on the category relation graph and past\nobservations. Last, a Region attention module (R) shifts the attention to the\ntarget-relevant region. Based on the visual representation extracted by our\nmethod, the agent can better perceive the environment and easily learn superior\nnavigation policy. Experiments on AI2-THOR demonstrate our CRG-TSR method\nsignificantly outperforms existing methods regarding both effectiveness and\nefficiency. The code has been included in the supplementary material and will\nbe publicly available.",
            "author": [
                "Xiaobo Hu",
                "Youfang Lin",
                "HeHe Fan",
                "Shuo Wang",
                "Zhihao Wu",
                "Kai Lv"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03327v1",
                "http://arxiv.org/pdf/2312.03327v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03325v1",
            "title": "GCFA:Geodesic Curve Feature Augmentation via Shape Space Theory",
            "updated": "2023-12-06T07:26:02Z",
            "published": "2023-12-06T07:26:02Z",
            "summary": "Deep learning has yielded remarkable outcomes in various domains. However,\nthe challenge of requiring large-scale labeled samples still persists in deep\nlearning. Thus, data augmentation has been introduced as a critical strategy to\ntrain deep learning models. However, data augmentation suffers from information\nloss and poor performance in small sample environments. To overcome these\ndrawbacks, we propose a feature augmentation method based on shape space\ntheory, i.e., Geodesic curve feature augmentation, called GCFA in brevity.\nFirst, we extract features from the image with the neural network model. Then,\nthe multiple image features are projected into a pre-shape space as features.\nIn the pre-shape space, a Geodesic curve is built to fit the features. Finally,\nthe many generated features on the Geodesic curve are used to train the various\nmachine learning models. The GCFA module can be seamlessly integrated with most\nmachine learning methods. And the proposed method is simple, effective and\ninsensitive for the small sample datasets. Several examples demonstrate that\nthe GCFA method can greatly improve the performance of the data preprocessing\nmodel in a small sample environment.",
            "author": [
                "Yuexing Han",
                "Guanxin Wan",
                "Bing Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03325v1",
                "http://arxiv.org/pdf/2312.03325v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03324v1",
            "title": "Lightweight Speaker Verification Using Transformation Module with\n  Feature Partition and Fusion",
            "updated": "2023-12-06T07:25:16Z",
            "published": "2023-12-06T07:25:16Z",
            "summary": "Although many efforts have been made on decreasing the model complexity for\nspeaker verification, it is still challenging to deploy speaker verification\nsystems with satisfactory result on low-resource terminals. We design a\ntransformation module that performs feature partition and fusion to implement\nlightweight speaker verification. The transformation module consists of\nmultiple simple but effective operations, such as convolution, pooling, mean,\nconcatenation, normalization, and element-wise summation. It works in a\nplug-and-play way, and can be easily implanted into a wide variety of models to\nreduce the model complexity while maintaining the model error. First, the input\nfeature is split into several low-dimensional feature subsets for decreasing\nthe model complexity. Then, each feature subset is updated by fusing it with\nthe inter-feature-subsets correlational information to enhance its\nrepresentational capability. Finally, the updated feature subsets are\nindependently fed into the block (one or several layers) of the model for\nfurther processing. The features that are output from current block of the\nmodel are processed according to the steps above before they are fed into the\nnext block of the model. Experimental data are selected from two public speech\ncorpora (namely VoxCeleb1 and VoxCeleb2). Results show that implanting the\ntransformation module into three models (namely AMCRN, ResNet34, and\nECAPA-TDNN) for speaker verification slightly increases the model error and\nsignificantly decreases the model complexity. Our proposed method outperforms\nbaseline methods on the whole in memory requirement and computational\ncomplexity with lower equal error rate. It also generalizes well across\ntruncated segments with various lengths.",
            "author": [
                "Yanxiong Li",
                "Zhongjie Jiang",
                "Qisheng Huang",
                "Wenchang Cao",
                "Jialong Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03324v1",
                "http://arxiv.org/pdf/2312.03324v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03322v1",
            "title": "Background Clustering Pre-training for Few-shot Segmentation",
            "updated": "2023-12-06T07:16:32Z",
            "published": "2023-12-06T07:16:32Z",
            "summary": "Recent few-shot segmentation (FSS) methods introduce an extra pre-training\nstage before meta-training to obtain a stronger backbone, which has become a\nstandard step in few-shot learning. Despite the effectiveness, current\npre-training scheme suffers from the merged background problem: only base\nclasses are labelled as foregrounds, making it hard to distinguish between\nnovel classes and actual background. In this paper, we propose a new\npre-training scheme for FSS via decoupling the novel classes from background,\ncalled Background Clustering Pre-Training (BCPT). Specifically, we adopt online\nclustering to the pixel embeddings of merged background to explore the\nunderlying semantic structures, bridging the gap between pre-training and\nadaptation to novel classes. Given the clustering results, we further propose\nthe background mining loss and leverage base classes to guide the clustering\nprocess, improving the quality and stability of clustering results. Experiments\non PASCAL-5i and COCO-20i show that BCPT yields advanced performance. Code will\nbe available.",
            "author": [
                "Zhimiao Yu",
                "Tiancheng Lin",
                "Yi Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03322v1",
                "http://arxiv.org/pdf/2312.03322v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03318v1",
            "title": "Complementary Benefits of Contrastive Learning and Self-Training Under\n  Distribution Shift",
            "updated": "2023-12-06T07:02:22Z",
            "published": "2023-12-06T07:02:22Z",
            "summary": "Self-training and contrastive learning have emerged as leading techniques for\nincorporating unlabeled data, both under distribution shift (unsupervised\ndomain adaptation) and when it is absent (semi-supervised learning). However,\ndespite the popularity and compatibility of these techniques, their efficacy in\ncombination remains unexplored. In this paper, we undertake a systematic\nempirical investigation of this combination, finding that (i) in domain\nadaptation settings, self-training and contrastive learning offer significant\ncomplementary gains; and (ii) in semi-supervised learning settings,\nsurprisingly, the benefits are not synergistic. Across eight distribution shift\ndatasets (e.g., BREEDs, WILDS), we demonstrate that the combined method obtains\n3--8% higher accuracy than either approach independently. We then theoretically\nanalyze these techniques in a simplified model of distribution shift,\ndemonstrating scenarios under which the features produced by contrastive\nlearning can yield a good initialization for self-training to further amplify\ngains and achieve optimal performance, even when either method alone would\nfail.",
            "author": [
                "Saurabh Garg",
                "Amrith Setlur",
                "Zachary Chase Lipton",
                "Sivaraman Balakrishnan",
                "Virginia Smith",
                "Aditi Raghunathan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03318v1",
                "http://arxiv.org/pdf/2312.03318v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03315v1",
            "title": "Impact of parallel code optimization on computer power consumption",
            "updated": "2023-12-06T06:48:16Z",
            "published": "2023-12-06T06:48:16Z",
            "summary": "The increase in performance and power of computing systems requires the wider\nuse of program optimizations. The goal of performing optimizations is not only\nto reduce program runtime, but also to reduce other computer resources\nincluding power consumption. The goal of the study was to evaluate the impact\nof different optimization levels and various optimization strategies on power\nconsumption. In a series of experiments, it was established that the average\npower consumption tends to peak for the programs with optimized source code.\nThe articles also describes the impact of changing computer architecture on\npower consumption graphs. The relationships between the average and median\nvalues of power consumption by example programs are considered. The possibility\nof creating program energy consumption profile for a parallel program is shown.",
            "author": [
                "E. A. Kiselev",
                "P. N. Telegin",
                "A. V. Baranov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03315v1",
                "http://arxiv.org/pdf/2312.03315v1"
            ],
            "primary_category": "cs.MS",
            "category": [
                "cs.MS",
                "cs.DC",
                "68M20"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03312v1",
            "title": "Optimizing Two-Pass Cross-Lingual Transfer Learning: Phoneme Recognition\n  and Phoneme to Grapheme Translation",
            "updated": "2023-12-06T06:37:24Z",
            "published": "2023-12-06T06:37:24Z",
            "summary": "This research optimizes two-pass cross-lingual transfer learning in\nlow-resource languages by enhancing phoneme recognition and phoneme-to-grapheme\ntranslation models. Our approach optimizes these two stages to improve speech\nrecognition across languages. We optimize phoneme vocabulary coverage by\nmerging phonemes based on shared articulatory characteristics, thus improving\nrecognition accuracy. Additionally, we introduce a global phoneme noise\ngenerator for realistic ASR noise during phoneme-to-grapheme training to reduce\nerror propagation. Experiments on the CommonVoice 12.0 dataset show significant\nreductions in Word Error Rate (WER) for low-resource languages, highlighting\nthe effectiveness of our approach. This research contributes to the\nadvancements of two-pass ASR systems in low-resource languages, offering the\npotential for improved cross-lingual transfer learning.",
            "author": [
                "Wonjun Lee",
                "Gary Geunbae Lee",
                "Yunsu Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03312v1",
                "http://arxiv.org/pdf/2312.03312v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03311v1",
            "title": "On the Nystrom Approximation for Preconditioning in Kernel Machines",
            "updated": "2023-12-06T06:33:25Z",
            "published": "2023-12-06T06:33:25Z",
            "summary": "Kernel methods are a popular class of nonlinear predictive models in machine\nlearning. Scalable algorithms for learning kernel models need to be iterative\nin nature, but convergence can be slow due to poor conditioning. Spectral\npreconditioning is an important tool to speed-up the convergence of such\niterative algorithms for training kernel models. However computing and storing\na spectral preconditioner can be expensive which can lead to large\ncomputational and storage overheads, precluding the application of kernel\nmethods to problems with large datasets. A Nystrom approximation of the\nspectral preconditioner is often cheaper to compute and store, and has\ndemonstrated success in practical applications. In this paper we analyze the\ntrade-offs of using such an approximated preconditioner. Specifically, we show\nthat a sample of logarithmic size (as a function of the size of the dataset)\nenables the Nystrom-based approximated preconditioner to accelerate gradient\ndescent nearly as well as the exact preconditioner, while also reducing the\ncomputational and storage overheads.",
            "author": [
                "Amirhesam Abedsoltan",
                "Mikhail Belkin",
                "Parthe Pandit",
                "Luis Rademacher"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03311v1",
                "http://arxiv.org/pdf/2312.03311v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03303v1",
            "title": "Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking\n  Technique",
            "updated": "2023-12-06T06:07:50Z",
            "published": "2023-12-06T06:07:50Z",
            "summary": "This paper presents a novel benchmarking framework Dyport for evaluating\nbiomedical hypothesis generation systems. Utilizing curated datasets, our\napproach tests these systems under realistic conditions, enhancing the\nrelevance of our evaluations. We integrate knowledge from the curated databases\ninto a dynamic graph, accompanied by a method to quantify discovery importance.\nThis not only assesses hypothesis accuracy but also their potential impact in\nbiomedical research which significantly extends traditional link prediction\nbenchmarks. Applicability of our benchmarking process is demonstrated on\nseveral link prediction systems applied on biomedical semantic knowledge\ngraphs. Being flexible, our benchmarking system is designed for broad\napplication in hypothesis generation quality verification, aiming to expand the\nscope of scientific discovery within the biomedical research community.\nAvailability and implementation: Dyport framework is fully open-source. All\ncode and datasets are available at: https://github.com/IlyaTyagin/Dyport",
            "author": [
                "Ilya Tyagin",
                "Ilya Safro"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03303v1",
                "http://arxiv.org/pdf/2312.03303v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03301v1",
            "title": "Masking Behaviors in Epidemiological Networks with Cognitively-plausible\n  Reinforcement Learning",
            "updated": "2023-12-06T05:57:27Z",
            "published": "2023-12-06T05:57:27Z",
            "summary": "The COVID-19 pandemic highlighted the critical role of human behavior in\ninfluencing infectious disease transmission and the need for models capturing\nthis complex dynamic. We present an agent-based model integrating an\nepidemiological simulation of disease spread with a cognitive architecture\ndriving individual mask-wearing decisions. Agents decide whether to mask based\non a utility function weighting factors like peer conformity, personal risk\ntolerance, and mask-wearing discomfort. By conducting experiments\nsystematically varying behavioral model parameters and social network\nstructures, we demonstrate how adaptive decision-making interacts with network\nconnectivity patterns to impact population-level infection outcomes. The model\nprovides a flexible computational framework for gaining insights into how\nbehavioral interventions like mask mandates may differentially influence\ndisease spread across communities with diverse social structures. Findings\nhighlight the importance of integrating realistic human decision processes in\nepidemiological models to inform policy decisions during public health crises.",
            "author": [
                "Konstantinos Mitsopoulos",
                "Lawrence Baker",
                "Christian Lebiere",
                "Peter Pirolli",
                "Mark Orr",
                "Raffaele Vardavas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03301v1",
                "http://arxiv.org/pdf/2312.03301v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03298v1",
            "title": "DiffPMAE: Diffusion Masked Autoencoders for Point Cloud Reconstruction",
            "updated": "2023-12-06T05:39:00Z",
            "published": "2023-12-06T05:39:00Z",
            "summary": "Point cloud streaming is increasingly getting popular, evolving into the norm\nfor interactive service delivery and the future Metaverse. However, the\nsubstantial volume of data associated with point clouds presents numerous\nchallenges, particularly in terms of high bandwidth consumption and large\nstorage capacity. Despite various solutions proposed thus far, with a focus on\npoint cloud compression, upsampling, and completion, these\nreconstruction-related methods continue to fall short in delivering high\nfidelity point cloud output. As a solution, in DiffPMAE, we propose an\neffective point cloud reconstruction architecture. Inspired by self-supervised\nlearning concepts, we combine Masked Auto-Encoding and Diffusion Model\nmechanism to remotely reconstruct point cloud data. By the nature of this\nreconstruction process, DiffPMAE can be extended to many related downstream\ntasks including point cloud compression, upsampling and completion. Leveraging\nShapeNet-55 and ModelNet datasets with over 60000 objects, we validate the\nperformance of DiffPMAE exceeding many state-of-the-art methods in-terms of\nauto-encoding and downstream tasks considered.",
            "author": [
                "Yanlong Li",
                "Chamara Madarasingha",
                "Kanchana Thilakarathna"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03298v1",
                "http://arxiv.org/pdf/2312.03298v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03296v1",
            "title": "Cooperative Probabilistic Trajectory Forecasting under Occlusion",
            "updated": "2023-12-06T05:36:52Z",
            "published": "2023-12-06T05:36:52Z",
            "summary": "Perception and planning under occlusion is essential for safety-critical\ntasks. Occlusion-aware planning often requires communicating the information of\nthe occluded object to the ego agent for safe navigation. However,\ncommunicating rich sensor information under adverse conditions during\ncommunication loss and limited bandwidth may not be always feasible. Further,\nin GPS denied environments and indoor navigation, localizing and sharing of\noccluded objects can be challenging. To overcome this, relative pose estimation\nbetween connected agents sharing a common field of view can be a\ncomputationally effective way of communicating information about surrounding\nobjects. In this paper, we design an end-to-end network that cooperatively\nestimates the current states of occluded pedestrian in the reference frame of\nego agent and then predicts the trajectory with safety guarantees.\nExperimentally, we show that the uncertainty-aware trajectory prediction of\noccluded pedestrian by the ego agent is almost similar to the ground truth\ntrajectory assuming no occlusion. The current research holds promise for\nuncertainty-aware navigation among multiple connected agents under occlusion.",
            "author": [
                "Anshul Nayak",
                "Azim Eskandarian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03296v1",
                "http://arxiv.org/pdf/2312.03296v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03295v1",
            "title": "Semi-analytic physics informed neural network for convection-dominated\n  boundary layer problems in 2D",
            "updated": "2023-12-06T05:23:55Z",
            "published": "2023-12-06T05:23:55Z",
            "summary": "This research investigates the numerical approximation of the two-dimensional\nconvection-dominated singularly perturbed problem on square, circular, and\nelliptic domains. Singularly perturbed boundary value problems present a\nsignificant challenge due to the presence of sharp boundary layers in their\nsolutions. Additionally, the considered domain exhibits characteristic points,\ngiving rise to a degenerate boundary layer problem. The stiffness of the\nproblem is attributed to the sharp singular layers, which can result in\nsubstantial computational errors if not appropriately addressed. Traditional\nnumerical methods typically require extensive mesh refinements near the\nboundary to achieve accurate solutions, which can be computationally expensive.\nTo address the challenges posed by singularly perturbed problems, we employ\nphysics-informed neural networks (PINNs). However, PINNs may struggle with\nrapidly varying singularly perturbed solutions over a small domain region,\nleading to inadequate resolution and potentially inaccurate or unstable\nresults. To overcome this limitation, we introduce a semi-analytic method that\naugments PINNs with singular layers or corrector functions. Through our\nnumerical experiments, we demonstrate significant improvements in both accuracy\nand stability, thus demonstrating the effectiveness of our proposed approach.",
            "author": [
                "Gung-Min Gie",
                "Youngjoon Hong",
                "Chang-Yeol Jung",
                "Dongseok Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03295v1",
                "http://arxiv.org/pdf/2312.03295v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03294v1",
            "title": "A General Framework for Portfolio Construction Based on Generative\n  Models of Asset Returns",
            "updated": "2023-12-06T05:08:53Z",
            "published": "2023-12-06T05:08:53Z",
            "summary": "In this paper, we present an integrated approach to portfolio construction\nand optimization, leveraging high-performance computing capabilities. We first\nexplore diverse pairings of generative model forecasts and objective functions\nused for portfolio optimization, which are evaluated using\nperformance-attribution models based on LASSO. We illustrate our approach using\nextensive simulations of crypto-currency portfolios, and we show that the\nportfolios constructed using the vine-copula generative model and the\nSharpe-ratio objective function consistently outperform. To accommodate a wide\narray of investment strategies, we further investigate portfolio blending and\npropose a general framework for evaluating and combining investment strategies.\nWe employ an extension of the multi-armed bandit framework and use value models\nand policy models to construct eclectic blended portfolios based on past\nperformance. We consider similarity and optimality measures for value models\nand employ probability-matching (\"blending\") and a greedy algorithm\n(\"switching\") for policy models. The eclectic portfolios are also evaluated\nusing LASSO models. We show that the value model utilizing cosine similarity\nand logit optimality consistently delivers robust superior performances. The\nextent of outperformance by eclectic portfolios over their benchmarks\nsignificantly surpasses that achieved by individual generative model-based\nportfolios over their respective benchmarks.",
            "author": [
                "Tuoyuan Cheng",
                "Kan Chen"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.jfds.2023.100113",
                "http://arxiv.org/abs/2312.03294v1",
                "http://arxiv.org/pdf/2312.03294v1"
            ],
            "primary_category": "q-fin.PM",
            "category": [
                "q-fin.PM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03777v1",
            "title": "On the Robustness of Large Multimodal Models Against Image Adversarial\n  Attacks",
            "updated": "2023-12-06T04:59:56Z",
            "published": "2023-12-06T04:59:56Z",
            "summary": "Recent advances in instruction tuning have led to the development of\nState-of-the-Art Large Multimodal Models (LMMs). Given the novelty of these\nmodels, the impact of visual adversarial attacks on LMMs has not been\nthoroughly examined. We conduct a comprehensive study of the robustness of\nvarious LMMs against different adversarial attacks, evaluated across tasks\nincluding image classification, image captioning, and Visual Question Answer\n(VQA). We find that in general LMMs are not robust to visual adversarial\ninputs. However, our findings suggest that context provided to the model via\nprompts, such as questions in a QA pair helps to mitigate the effects of visual\nadversarial inputs. Notably, the LMMs evaluated demonstrated remarkable\nresilience to such attacks on the ScienceQA task with only an 8.10% drop in\nperformance compared to their visual counterparts which dropped 99.73%. We also\npropose a new approach to real-world image classification which we term query\ndecomposition. By incorporating existence queries into our input prompt we\nobserve diminished attack effectiveness and improvements in image\nclassification accuracy. This research highlights a previously under-explored\nfacet of LMM robustness and sets the stage for future work aimed at\nstrengthening the resilience of multimodal systems in adversarial environments.",
            "author": [
                "Xuanimng Cui",
                "Alejandro Aparcedo",
                "Young Kyun Jang",
                "Ser-Nam Lim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03777v1",
                "http://arxiv.org/pdf/2312.03777v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03290v1",
            "title": "Can language agents be alternatives to PPO? A Preliminary Empirical\n  Study On OpenAI Gym",
            "updated": "2023-12-06T04:48:26Z",
            "published": "2023-12-06T04:48:26Z",
            "summary": "The formidable capacity for zero- or few-shot decision-making in language\nagents encourages us to pose a compelling question: Can language agents be\nalternatives to PPO agents in traditional sequential decision-making tasks? To\ninvestigate this, we first take environments collected in OpenAI Gym as our\ntestbeds and ground them to textual environments that construct the TextGym\nsimulator. This allows for straightforward and efficient comparisons between\nPPO agents and language agents, given the widespread adoption of OpenAI Gym. To\nensure a fair and effective benchmarking, we introduce $5$ levels of scenario\nfor accurate domain-knowledge controlling and a unified RL-inspired framework\nfor language agents. Additionally, we propose an innovative\nexplore-exploit-guided language (EXE) agent to solve tasks within TextGym.\nThrough numerical experiments and ablation studies, we extract valuable\ninsights into the decision-making capabilities of language agents and make a\npreliminary evaluation of their potential to be alternatives to PPO in\nclassical sequential decision-making problems. This paper sheds light on the\nperformance of language agents and paves the way for future research in this\nexciting domain. Our code is publicly available\nat~\\url{https://github.com/mail-ecnu/Text-Gym-Agents}.",
            "author": [
                "Junjie Sheng",
                "Zixiao Huang",
                "Chuyun Shen",
                "Wenhao Li",
                "Yun Hua",
                "Bo Jin",
                "Hongyuan Zha",
                "Xiangfeng Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03290v1",
                "http://arxiv.org/pdf/2312.03290v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03289v2",
            "title": "Class Incremental Learning for Adversarial Robustness",
            "updated": "2023-12-07T04:21:33Z",
            "published": "2023-12-06T04:38:02Z",
            "summary": "Adversarial training integrates adversarial examples during model training to\nenhance robustness. However, its application in fixed dataset settings differs\nfrom real-world dynamics, where data accumulates incrementally. In this study,\nwe investigate Adversarially Robust Class Incremental Learning (ARCIL), a\nmethod that combines adversarial robustness with incremental learning. We\nobserve that combining incremental learning with naive adversarial training\neasily leads to a loss of robustness. We discover that this is attributed to\nthe disappearance of the flatness of the loss function, a characteristic of\nadversarial training. To address this issue, we propose the Flatness Preserving\nDistillation (FPD) loss that leverages the output difference between\nadversarial and clean examples. Additionally, we introduce the Logit Adjustment\nDistillation (LAD) loss, which adapts the model's knowledge to perform well on\nnew tasks. Experimental results demonstrate the superiority of our method over\napproaches that apply adversarial training to existing incremental learning\nmethods, which provides a strong baseline for incremental learning on\nadversarial robustness in the future. Our method achieves AutoAttack accuracy\nthat is 5.99\\%p, 5.27\\%p, and 3.90\\%p higher on average than the baseline on\nsplit CIFAR-10, CIFAR-100, and Tiny ImageNet, respectively. The code will be\nmade available.",
            "author": [
                "Seungju Cho",
                "Hongsin Lee",
                "Changick Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03289v2",
                "http://arxiv.org/pdf/2312.03289v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03288v1",
            "title": "STEP CATFormer: Spatial-Temporal Effective Body-Part Cross Attention\n  Transformer for Skeleton-based Action Recognition",
            "updated": "2023-12-06T04:36:58Z",
            "published": "2023-12-06T04:36:58Z",
            "summary": "Graph convolutional networks (GCNs) have been widely used and achieved\nremarkable results in skeleton-based action recognition. We think the key to\nskeleton-based action recognition is a skeleton hanging in frames, so we focus\non how the Graph Convolutional Convolution networks learn different topologies\nand effectively aggregate joint features in the global temporal and local\ntemporal. In this work, we propose three Channel-wise Tolopogy Graph\nConvolution based on Channel-wise Topology Refinement Graph Convolution\n(CTR-GCN). Combining CTR-GCN with two joint cross-attention modules can capture\nthe upper-lower body part and hand-foot relationship skeleton features. After\nthat, to capture features of human skeletons changing in frames we design the\nTemporal Attention Transformers to extract skeletons effectively. The Temporal\nAttention Transformers can learn the temporal features of human skeleton\nsequences. Finally, we fuse the temporal features output scale with MLP and\nclassification. We develop a powerful graph convolutional network named Spatial\nTemporal Effective Body-part Cross Attention Transformer which notably\nhigh-performance on the NTU RGB+D, NTU RGB+D 120 datasets. Our code and models\nare available at https://github.com/maclong01/STEP-CATFormer",
            "author": [
                "Nguyen Huu Bao Long"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03288v1",
                "http://arxiv.org/pdf/2312.03288v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "I.2.10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03286v1",
            "title": "Indirect Gradient Matching for Adversarial Robust Distillation",
            "updated": "2023-12-06T04:32:38Z",
            "published": "2023-12-06T04:32:38Z",
            "summary": "Adversarial training significantly improves adversarial robustness, but\nsuperior performance is primarily attained with large models. This substantial\nperformance gap for smaller models has spurred active research into adversarial\ndistillation (AD) to mitigate the difference. Existing AD methods leverage the\nteacher's logits as a guide. In contrast to these approaches, we aim to\ntransfer another piece of knowledge from the teacher, the input gradient. In\nthis paper, we propose a distillation module termed Indirect Gradient\nDistillation Module (IGDM) that indirectly matches the student's input gradient\nwith that of the teacher. We hypothesize that students can better acquire the\nteacher's knowledge by matching the input gradient. Leveraging the observation\nthat adversarial training renders the model locally linear on the input space,\nwe employ Taylor approximation to effectively align gradients without directly\ncalculating them. Experimental results show that IGDM seamlessly integrates\nwith existing AD methods, significantly enhancing the performance of all AD\nmethods. Particularly, utilizing IGDM on the CIFAR-100 dataset improves the\nAutoAttack accuracy from 28.06% to 30.32% with the ResNet-18 model and from\n26.18% to 29.52% with the MobileNetV2 model when integrated into the SOTA\nmethod without additional data augmentation. The code will be made available.",
            "author": [
                "Hongsin Lee",
                "Seungju Cho",
                "Changick Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03286v1",
                "http://arxiv.org/pdf/2312.03286v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03285v1",
            "title": "Periodic boundary conditions and $G_2$ cosmology",
            "updated": "2023-12-06T04:32:11Z",
            "published": "2023-12-06T04:32:11Z",
            "summary": "In the standard concordance cosmology the spatial curvature is assumed to be\nconstant and zero (or at least very small). In particular, in numerical\ncomputations of the structure of the universe using N-body simulations, exact\nperiodic boundary conditions are assumed which constrains the spatial\ncurvature. In order to confirm this qualitatively, we numerically evolve a\nspecial class of spatially inhomogeneous $G_2$ models with both periodic\ninitial data and non periodic initial data using zooming techniques. We\nconsequently demonstrate that in these models periodic initial conditions do\nindeed suppress the growth of the spatial curvature as the models evolve away\nfrom their initial isotropic and spatially homogeneous state, thereby verifying\nthat the spatial curvature is necessarily very small in standard cosmology.",
            "author": [
                "Alan Coley",
                "Woei Chet Lim"
            ],
            "link": [
                "http://dx.doi.org/10.1088/1361-6382/ad0b9f",
                "http://arxiv.org/abs/2312.03285v1",
                "http://arxiv.org/pdf/2312.03285v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03283v1",
            "title": "On the Cohomology of Two Stranded Braid Varieties",
            "updated": "2023-12-06T04:25:12Z",
            "published": "2023-12-06T04:25:12Z",
            "summary": "We compute the cohomologies of two strand braid varieties using the two-form\npresent in cluster structures. We confirm these results with proof using\nAlexander and Poincar\\'e duality. Further, we consider products of braid\nvarieties and their interactions with the cohomologies.",
            "author": [
                "Tonie Scroggin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03283v1",
                "http://arxiv.org/pdf/2312.03283v1"
            ],
            "primary_category": "math.AG",
            "category": [
                "math.AG",
                "math.RT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03282v1",
            "title": "Monte Carlo Optimization for Solving Multilevel Stackelberg Games",
            "updated": "2023-12-06T04:20:59Z",
            "published": "2023-12-06T04:20:59Z",
            "summary": "Stackelberg games originate where there are market leaders and followers, and\nthe actions of leaders influence the behavior of the followers. Mathematical\nmodelling of such games results in what's called a Bilevel Optimization\nproblem. There is an entire area of research dedicated to analyzing and solving\nBilevel Optimization problems which are often complex, and finding solutions\nfor such problems is known to be NP-Hard. A generalization of Stackelberg games\nis a Multilevel Stackelberg game where we may have nested leaders and\nfollowers, such that a follower is, in turn, a leader for all lower-level\nplayers. These problems are much more difficult to solve, and existing solution\napproaches typically require extensive cooperation between the players (which\ngenerally can't be assumed) or make restrictive assumptions about the structure\nof the problem. In this paper, we present a stochastic algorithm to approximate\nthe local equilibrium solutions for these Multilevel games. We then construct a\nfew examples of such Multilevel problems, including: a) a nested toll-setting\nproblem; and b) an adversarial initial condition determination problem for\nRobust Trajectory Optimization. We test our algorithm on our constructed\nproblems as well as some trilevel problems from the literature, and show that\nit is able to approximate the optimum solutions for these problems within a\nreasonable error margin. We also provide an asymptotic proof for the\nconvergence of the algorithm and empirically analyze its accuracy and\nconvergence speed for different parameters. Lastly, we compare it with existing\nsolution strategies from the literature and demonstrate that it outperforms\nthem.",
            "author": [
                "Pravesh Koirala",
                "Forrest Laine"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03282v1",
                "http://arxiv.org/pdf/2312.03282v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03279v1",
            "title": "Quantum Fusion of Independent Networks Based on Multi-user Entanglement\n  Swapping",
            "updated": "2023-12-06T04:13:05Z",
            "published": "2023-12-06T04:13:05Z",
            "summary": "With the advance development in quantum science, constructing a large-scale\nquantum network has become a hot area of future quantum information technology.\nFuture quantum networks promise to enable many fantastic applications and will\nunlock fundamentally new technologies in information security and large-scale\ncomputation. The future quantum internet is required to connect quantum\ninformation processors to achieve unparalleled capabilities in secret\ncommunication and enable quantum communication between any two points on Earth.\nHowever, the existing quantum networks are basically constructed to realize the\ncommunication between the end users in their own networks. How to bridge\ndifferent independent networks to form a fully-connected quantum internet\nbecomes a pressing challenge for future networks. Here, we demonstrate the\nquantum fusion of two independent networks for the first time based on\nmultiuser entanglement swapping, to merge two 10-user networks into a larger\nnetwork with 18 users in quantum correlation layer. By performing the Bell\nstate measurement between two nonneighboring nodes, the users from different\nnetworks can establish entanglement and ultimately every pair of the 18 users\nare able to communicate with each other using the swapped states. Our approach\nopens attractive opportunities for the establishment of quantum entanglement\nbetween remote nodes in different networks, which facilitates versatile quantum\ninformation interconnects and has great application in constructing large-scale\nintercity quantum communication networks.",
            "author": [
                "Yiwen Huang",
                "Yilin Yang",
                "Hao Li",
                "Jing Qiu",
                "Zhantong Qi",
                "Jiayu Wang",
                "Yuting Zhang",
                "Yuanhua Li",
                "Yuanlin Zheng",
                "Xianfeng Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03279v1",
                "http://arxiv.org/pdf/2312.03279v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03277v1",
            "title": "Anomaly Detection for Scalable Task Grouping in Reinforcement\n  Learning-based RAN Optimization",
            "updated": "2023-12-06T04:05:17Z",
            "published": "2023-12-06T04:05:17Z",
            "summary": "The use of learning-based methods for optimizing cellular radio access\nnetworks (RAN) has received increasing attention in recent years. This\ncoincides with a rapid increase in the number of cell sites worldwide, driven\nlargely by dramatic growth in cellular network traffic. Training and\nmaintaining learned models that work well across a large number of cell sites\nhas thus become a pertinent problem. This paper proposes a scalable framework\nfor constructing a reinforcement learning policy bank that can perform RAN\noptimization across a large number of cell sites with varying traffic patterns.\nCentral to our framework is a novel application of anomaly detection techniques\nto assess the compatibility between sites (tasks) and the policy bank. This\nallows our framework to intelligently identify when a policy can be reused for\na task, and when a new policy needs to be trained and added to the policy bank.\nOur results show that our approach to compatibility assessment leads to an\nefficient use of computational resources, by allowing us to construct a\nperformant policy bank without exhaustively training on all tasks, which makes\nit applicable under real-world constraints.",
            "author": [
                "Jimmy Li",
                "Igor Kozlov",
                "Di Wu",
                "Xue Liu",
                "Gregory Dudek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03277v1",
                "http://arxiv.org/pdf/2312.03277v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03275v1",
            "title": "VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation",
            "updated": "2023-12-06T04:02:28Z",
            "published": "2023-12-06T04:02:28Z",
            "summary": "Understanding how humans leverage semantic knowledge to navigate unfamiliar\nenvironments and decide where to explore next is pivotal for developing robots\ncapable of human-like search behaviors. We introduce a zero-shot navigation\napproach, Vision-Language Frontier Maps (VLFM), which is inspired by human\nreasoning and designed to navigate towards unseen semantic objects in novel\nenvironments. VLFM builds occupancy maps from depth observations to identify\nfrontiers, and leverages RGB observations and a pre-trained vision-language\nmodel to generate a language-grounded value map. VLFM then uses this map to\nidentify the most promising frontier to explore for finding an instance of a\ngiven target object category. We evaluate VLFM in photo-realistic environments\nfrom the Gibson, Habitat-Matterport 3D (HM3D), and Matterport 3D (MP3D)\ndatasets within the Habitat simulator. Remarkably, VLFM achieves\nstate-of-the-art results on all three datasets as measured by success weighted\nby path length (SPL) for the Object Goal Navigation task. Furthermore, we show\nthat VLFM's zero-shot nature enables it to be readily deployed on real-world\nrobots such as the Boston Dynamics Spot mobile manipulation platform. We deploy\nVLFM on Spot and demonstrate its capability to efficiently navigate to target\nobjects within an office building in the real world, without any prior\nknowledge of the environment. The accomplishments of VLFM underscore the\npromising potential of vision-language models in advancing the field of\nsemantic navigation. Videos of real-world deployment can be viewed at\nnaoki.io/vlfm.",
            "author": [
                "Naoki Yokoyama",
                "Sehoon Ha",
                "Dhruv Batra",
                "Jiuguang Wang",
                "Bernadette Bucher"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03275v1",
                "http://arxiv.org/pdf/2312.03275v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03273v1",
            "title": "Perfectly matched layers for the Boltzmann equation: stability and\n  sensitivity analysis",
            "updated": "2023-12-06T03:58:32Z",
            "published": "2023-12-06T03:58:32Z",
            "summary": "We study the stability and sensitivity of an absorbing layer for the\nBoltzmann equation by examining the Bhatnagar-Gross-Krook (BGK) approximation\nand using the perfectly matched layer (PML) technique. To ensure stability, we\ndiscard some parameters in the model and calculate the total sensitivity\nindices of the remaining parameters using the ANOVA expansion of multivariate\nfunctions. We conduct extensive numerical experiments to study stability and\ncompute the total sensitivity indices, which allow us to identify the essential\nparameters of the model.",
            "author": [
                "Marco Sutti",
                "Jan S. Hesthaven"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03273v1",
                "http://arxiv.org/pdf/2312.03273v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "physics.comp-ph",
                "76P05, 65N06, 35L45, 35B35, 35L05, 35Q30"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03270v1",
            "title": "Geometric Deep Learning Towards the Iterative Classification of\n  Graph-Based Aircraft Thermal Management Systems",
            "updated": "2023-12-06T03:49:13Z",
            "published": "2023-12-06T03:49:13Z",
            "summary": "In this paper, we use graph-based techniques to investigate the use of\ngeometric deep learning (GDL) in the classification and down-selection of\naircraft thermal management systems (TMS). Previous work developed an\nenumerative graph generation procedure using a component catalog with network\nstructure constraints to represent novel aircraft TMSs as graphs. However, as\nwith many enumerative approaches, combinatorial explosion limits its efficacy\nin many real-world problems, particularly when simulations and optimization\nmust be performed on the many (automatically-generated) physics models.\nTherefore, we present an approach that takes the directed graphs representing\naircraft TMSs and use GDL to predict the critical characteristics of the\nremaining graphs. This paper's findings demonstrate that incorporating\nadditional graph-based features enhances performance, achieving an accuracy of\n97% for determining a graph's compilability and simulatability while using only\n5% of the data for training. By applying iterative classification methods, we\nalso successfully segmented the total set of graphs into more specific groups\nwith an average inclusion of 84.7 of the top 100 highest-performing graphs,\nachieved by training on 45% of the data.",
            "author": [
                "Anthony Sirico Jr.",
                "Daniel R Herber"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03270v1",
                "http://arxiv.org/pdf/2312.03270v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03266v1",
            "title": "SO-NeRF: Active View Planning for NeRF using Surrogate Objectives",
            "updated": "2023-12-06T03:31:13Z",
            "published": "2023-12-06T03:31:13Z",
            "summary": "Despite the great success of Neural Radiance Fields (NeRF), its\ndata-gathering process remains vague with only a general rule of thumb of\nsampling as densely as possible. The lack of understanding of what actually\nconstitutes good views for NeRF makes it difficult to actively plan a sequence\nof views that yield the maximal reconstruction quality. We propose Surrogate\nObjectives for Active Radiance Fields (SOAR), which is a set of interpretable\nfunctions that evaluates the goodness of views using geometric and photometric\nvisual cues - surface coverage, geometric complexity, textural complexity, and\nray diversity. Moreover, by learning to infer the SOAR scores from a deep\nnetwork, SOARNet, we are able to effectively select views in mere seconds\ninstead of hours, without the need for prior visits to all the candidate views\nor training any radiance field during such planning. Our experiments show\nSOARNet outperforms the baselines with $\\sim$80x speed-up while achieving\nbetter or comparable reconstruction qualities. We finally show that SOAR is\nmodel-agnostic, thus it generalizes across fully neural-implicit to fully\nexplicit approaches.",
            "author": [
                "Keifer Lee",
                "Shubham Gupta",
                "Sunglyoung Kim",
                "Bhargav Makwana",
                "Chao Chen",
                "Chen Feng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03266v1",
                "http://arxiv.org/pdf/2312.03266v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03265v1",
            "title": "Generalized monodromy method in gauge/gravity duality",
            "updated": "2023-12-06T03:30:05Z",
            "published": "2023-12-06T03:30:05Z",
            "summary": "The method of monodromy is an important tool for computing Virasoro conformal\nblocks in a two-dimensional Conformal Field Theory (2d CFT) at large central\ncharge and external dimensions. In deriving the form of the monodromy problem,\nwhich defines the method, one needs to insert a degenerate operator, usually a\nlevel-two operator, into the corresponding correlation function. It can be\nobserved that the choice of which degenerate operator to insert is arbitrary,\nand they shall reveal the same physical principles underlying the method. In\nthis paper, we exploit this freedom and generalize the method of monodromy by\ninserting higher-level degenerate operators. We illustrate the case with a\nlevel-three operator, and derive the corresponding form of the monodromy\nproblem. We solve the monodromy problem perturbatively and numerically; and\ncheck that it agrees with the standard monodromy method, despite the fact that\nthe two versions of the monodromy problem do not seem to be related in any\nobvious way. The forms corresponding to other higher-level degenerate operators\nare also discussed. We explain the physical origin of the coincidence and\ndiscuss its implication from a mathematical perspective.",
            "author": [
                "Yuanpeng Hou"
            ],
            "link": [
                "http://dx.doi.org/10.1038/s41598-022-16054-0",
                "http://arxiv.org/abs/2312.03265v1",
                "http://arxiv.org/pdf/2312.03265v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03262v1",
            "title": "Low-Cost High-Power Membership Inference by Boosting Relativity",
            "updated": "2023-12-06T03:18:49Z",
            "published": "2023-12-06T03:18:49Z",
            "summary": "We present a robust membership inference attack (RMIA) that amplifies the\ndistinction between population data and the training data on any target model,\nby effectively leveraging both reference models and reference data in our\nlikelihood ratio test. Our algorithm exhibits superior test power\n(true-positive rate) when compared to prior methods, even at extremely low\nfalse-positive error rates (as low as 0). Also, under computation constraints,\nwhere only a limited number of reference models (as few as 1) are available,\nour method performs exceptionally well, unlike some prior attacks that approach\nrandom guessing in such scenarios. Our method lays the groundwork for\ncost-effective and practical yet powerful and robust privacy risk analysis of\nmachine learning algorithms.",
            "author": [
                "Sajjad Zarifzadeh",
                "Philippe Liu",
                "Reza Shokri"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03262v1",
                "http://arxiv.org/pdf/2312.03262v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03775v1",
            "title": "FAAC: Facial Animation Generation with Anchor Frame and Conditional\n  Control for Superior Fidelity and Editability",
            "updated": "2023-12-06T02:55:35Z",
            "published": "2023-12-06T02:55:35Z",
            "summary": "Over recent years, diffusion models have facilitated significant advancements\nin video generation. Yet, the creation of face-related videos still confronts\nissues such as low facial fidelity, lack of frame consistency, limited\neditability and uncontrollable human poses. To address these challenges, we\nintroduce a facial animation generation method that enhances both face identity\nfidelity and editing capabilities while ensuring frame consistency. This\napproach incorporates the concept of an anchor frame to counteract the\ndegradation of generative ability in original text-to-image models when\nincorporating a motion module. We propose two strategies towards this\nobjective: training-free and training-based anchor frame methods. Our method's\nefficacy has been validated on multiple representative DreamBooth and LoRA\nmodels, delivering substantial improvements over the original outcomes in terms\nof facial fidelity, text-to-image editability, and video motion. Moreover, we\nintroduce conditional control using a 3D parametric face model to capture\naccurate facial movements and expressions. This solution augments the creative\npossibilities for facial animation generation through the integration of\nmultiple control signals. For additional samples, please visit\nhttps://anonymous.4open.science/r/FAAC.",
            "author": [
                "Linze Li",
                "Sunqi Fan",
                "Hengjun Pu",
                "Zhaodong Bing",
                "Yao Tang",
                "Tianzhu Ye",
                "Tong Yang",
                "Liangyu Chen",
                "Jiajun Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03775v1",
                "http://arxiv.org/pdf/2312.03775v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03774v1",
            "title": "OctreeOcc: Efficient and Multi-Granularity Occupancy Prediction Using\n  Octree Queries",
            "updated": "2023-12-06T02:52:54Z",
            "published": "2023-12-06T02:52:54Z",
            "summary": "Occupancy prediction has increasingly garnered attention in recent years for\nits fine-grained understanding of 3D scenes. Traditional approaches typically\nrely on dense, regular grid representations, which often leads to excessive\ncomputational demands and a loss of spatial details for small objects. This\npaper introduces OctreeOcc, an innovative 3D occupancy prediction framework\nthat leverages the octree representation to adaptively capture valuable\ninformation in 3D, offering variable granularity to accommodate object shapes\nand semantic regions of varying sizes and complexities. In particular, we\nincorporate image semantic information to improve the accuracy of initial\noctree structures and design an effective rectification mechanism to refine the\noctree structure iteratively. Our extensive evaluations show that OctreeOcc not\nonly surpasses state-of-the-art methods in occupancy prediction, but also\nachieves a 15%-24% reduction in computational overhead compared to\ndense-grid-based methods.",
            "author": [
                "Yuhang Lu",
                "Xinge Zhu",
                "Tai Wang",
                "Yuexin Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03774v1",
                "http://arxiv.org/pdf/2312.03774v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03245v1",
            "title": "A Simple Framework to Enhance the Adversarial Robustness of Deep\n  Learning-based Intrusion Detection System",
            "updated": "2023-12-06T02:33:12Z",
            "published": "2023-12-06T02:33:12Z",
            "summary": "Deep learning based intrusion detection systems (DL-based IDS) have emerged\nas one of the best choices for providing security solutions against various\nnetwork intrusion attacks. However, due to the emergence and development of\nadversarial deep learning technologies, it becomes challenging for the adoption\nof DL models into IDS. In this paper, we propose a novel IDS architecture that\ncan enhance the robustness of IDS against adversarial attacks by combining\nconventional machine learning (ML) models and Deep Learning models. The\nproposed DLL-IDS consists of three components: DL-based IDS, adversarial\nexample (AE) detector, and ML-based IDS. We first develop a novel AE detector\nbased on the local intrinsic dimensionality (LID). Then, we exploit the low\nattack transferability between DL models and ML models to find a robust ML\nmodel that can assist us in determining the maliciousness of AEs. If the input\ntraffic is detected as an AE, the ML-based IDS will predict the maliciousness\nof input traffic, otherwise the DL-based IDS will work for the prediction. The\nfusion mechanism can leverage the high prediction accuracy of DL models and low\nattack transferability between DL models and ML models to improve the\nrobustness of the whole system. In our experiments, we observe a significant\nimprovement in the prediction performance of the IDS when subjected to\nadversarial attack, achieving high accuracy with low resource consumption.",
            "author": [
                "Xinwei Yuan",
                "Shu Han",
                "Wei Huang",
                "Hongliang Ye",
                "Xianglong Kong",
                "Fan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03245v1",
                "http://arxiv.org/pdf/2312.03245v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03244v1",
            "title": "Improving Constraint on $\u03a9_{m}$ from SDSS Using Marked Correlation\n  Functions",
            "updated": "2023-12-06T02:32:07Z",
            "published": "2023-12-06T02:32:07Z",
            "summary": "Large-scale structure (LSS) surveys will increasingly provide stringent\nconstraints on our cosmological models. Recently, the density-marked\ncorrelation function (MCF) has been introduced, offering an easily computable\ndensity-correlation statistic. Simulations have demonstrated that MCFs offer\nadditional, independent constraints on cosmological models beyond the standard\ntwo-point correlation (2PCF). In this study, we apply MCFs for the first time\nto SDSS CMASS data, aiming to investigate the statistical information regarding\nclustering and anisotropy properties in the Universe and assess the performance\nof various weighting schemes in MCFs. Upon analyzing the CMASS data, we observe\nthat, by combining different weights ($\\alpha = [-0.2, 0, 0.2, 0.6]$), the MCFs\nprovide a tight and independent constraint on the cosmological parameter\n$\\Omega_m$, yielding $\\Omega_m = 0.293 \\pm0.006$ at the $1\\sigma$ level, which\nrepresents a significant reduction in the statistical error by a factor of 3.4\ncompared to that from 2PCF. Our constraint is consistent with recent findings\nfrom the small-scale clustering of BOSS galaxies \\cite{arXiv:2203.08999v2}\nwithin the 1$\\sigma$ level. However, we also find that our estimate is lower\nthan the Planck measurements by about 2.6$\\sigma$, indicating the potential\npresence of new physics beyond the standard cosmological model if all the\nsystematics are fully corrected. The method outlined in this study can be\nextended to other surveys and datasets, allowing for the constraint of other\ncosmological parameters. Additionally, it serves as a valuable tool for\nforthcoming emulator analysis on the Chinese Space Station Telescope (CSST).",
            "author": [
                "L. M. Lai",
                "J. C. Ding",
                "X. L. Luo",
                "Y. Z. Yang",
                "Z. H. Wang",
                "K. S. Liu",
                "G. F. Liu",
                "X. Wang",
                "Y. Zheng",
                "Z. Y. Li",
                "L. Zhang",
                "X. D. Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03244v1",
                "http://arxiv.org/pdf/2312.03244v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03243v1",
            "title": "Generalizable Neural Physics Solvers by Baldwinian Evolution",
            "updated": "2023-12-06T02:31:12Z",
            "published": "2023-12-06T02:31:12Z",
            "summary": "Physics-informed neural networks (PINNs) are at the forefront of scientific\nmachine learning, making possible the creation of machine intelligence that is\ncognizant of physical laws and able to accurately simulate them. In this paper,\nthe potential of discovering PINNs that generalize over an entire family of\nphysics tasks is studied, for the first time, through a biological lens of the\nBaldwin effect. Drawing inspiration from the neurodevelopment of precocial\nspecies that have evolved to learn, predict and react quickly to their\nenvironment, we envision PINNs that are pre-wired with connection strengths\ninducing strong biases towards efficient learning of physics. To this end,\nevolutionary selection pressure (guided by proficiency over a family of tasks)\nis coupled with lifetime learning (to specialize on a smaller subset of those\ntasks) to produce PINNs that demonstrate fast and physics-compliant prediction\ncapabilities across a range of empirically challenging problem instances. The\nBaldwinian approach achieves an order of magnitude improvement in prediction\naccuracy at a fraction of the computation cost compared to state-of-the-art\nresults with PINNs meta-learned by gradient descent. This paper marks a leap\nforward in the meta-learning of PINNs as generalizable physics solvers.",
            "author": [
                "Jian Cheng Wong",
                "Chin Chun Ooi",
                "Abhishek Gupta",
                "Pao-Hsiung Chiu",
                "Joshua Shao Zheng Low",
                "My Ha Dao",
                "Yew-Soon Ong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03243v1",
                "http://arxiv.org/pdf/2312.03243v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE",
                "cs.CE",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03235v1",
            "title": "HEET: A Heterogeneity Measure to Quantify the Difference across\n  Distributed Computing Systems",
            "updated": "2023-12-06T02:15:19Z",
            "published": "2023-12-06T02:15:19Z",
            "summary": "Although system heterogeneity has been extensively studied in the past, there\nis yet to be a study on measuring the impact of heterogeneity on system\nperformance. For this purpose, we propose a heterogeneity measure that can\ncharacterize the impact of the heterogeneity of a system on its performance\nbehavior in terms of throughput or makespan. We develop a mathematical model to\ncharacterize a heterogeneous system in terms of its task and machine\nheterogeneity dimensions and then reduce it to a single value, called\nHomogeneous Equivalent Execution Time (HEET), which represents the execution\ntime behavior of the entire system. We used AWS EC2 instances to implement a\nreal-world machine learning inference system. Performance evaluation of the\nHEET score across different heterogeneous system configurations demonstrates\nthat HEET can accurately characterize the performance behavior of these\nsystems. In particular, the results show that our proposed method is capable of\npredicting the true makespan of heterogeneous systems without online\nevaluations with an average precision of 84%. This heterogeneity measure is\ninstrumental for solution architects to configure their systems proactively to\nbe sufficiently heterogeneous to meet their desired performance objectives.",
            "author": [
                "Ali Mokhtari",
                "Saeid Ghafouri",
                "Pooyan Jamshidi",
                "Mohsen Amini Salehi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03235v1",
                "http://arxiv.org/pdf/2312.03235v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.PF"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03231v1",
            "title": "Deep Multimodal Fusion for Surgical Feedback Classification",
            "updated": "2023-12-06T01:59:47Z",
            "published": "2023-12-06T01:59:47Z",
            "summary": "Quantification of real-time informal feedback delivered by an experienced\nsurgeon to a trainee during surgery is important for skill improvements in\nsurgical training. Such feedback in the live operating room is inherently\nmultimodal, consisting of verbal conversations (e.g., questions and answers) as\nwell as non-verbal elements (e.g., through visual cues like pointing to\nanatomic elements). In this work, we leverage a clinically-validated\nfive-category classification of surgical feedback: \"Anatomic\", \"Technical\",\n\"Procedural\", \"Praise\" and \"Visual Aid\". We then develop a multi-label machine\nlearning model to classify these five categories of surgical feedback from\ninputs of text, audio, and video modalities. The ultimate goal of our work is\nto help automate the annotation of real-time contextual surgical feedback at\nscale. Our automated classification of surgical feedback achieves AUCs ranging\nfrom 71.5 to 77.6 with the fusion improving performance by 3.1%. We also show\nthat high-quality manual transcriptions of feedback audio from experts improve\nAUCs to between 76.5 and 96.2, which demonstrates a clear path toward future\nimprovements. Empirically, we find that the Staged training strategy, with\nfirst pre-training each modality separately and then training them jointly, is\nmore effective than training different modalities altogether. We also present\nintuitive findings on the importance of modalities for different feedback\ncategories. This work offers an important first look at the feasibility of\nautomated classification of real-world live surgical feedback based on text,\naudio, and video modalities.",
            "author": [
                "Rafal Kocielnik",
                "Elyssa Y. Wong",
                "Timothy N. Chu",
                "Lydia Lin",
                "De-An Huang",
                "Jiayun Wang",
                "Anima Anandkumar",
                "Andrew J. Hung"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03231v1",
                "http://arxiv.org/pdf/2312.03231v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV",
                "cs.HC",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03229v1",
            "title": "Attaining Equilibria Using Control Sets",
            "updated": "2023-12-06T01:56:30Z",
            "published": "2023-12-06T01:56:30Z",
            "summary": "Many interactions result in a socially suboptimal equilibrium, or in a\nnon-equilibrium state, from which arriving at an equilibrium through simple\ndynamics can be impossible of too long. Aiming to achieve a certain\nequilibrium, we persuade, bribe, or coerce a group of participants to make them\nact in a way that will motivate the rest of the players to act accordingly to\nthe desired equilibrium. Formally, we ask which subset of the players can adopt\nthe goal equilibrium strategies that will make acting according to the desired\nequilibrium a best response for the other players. We call such a subset a\ndirect control set, prove some connections to strength of equilibrium, and\nstudy the hardness to find such lightest sets, even approximately. We then\nsolve important subcases and provide approximation algorithms, assuming\nmonotonicity. Next, we concentrate on potential games and prove that, while the\nproblem of finding such a set is \\NP-hard, even for constant-factor\napproximation, we can still solve the problem approximately or even precisely\nin relevant special cases. We approximately solve this problem for singleton\npotential games and treat more closely specific potential games, such as\nsymmetric games and coordination games on graphs.",
            "author": [
                "Gleb Polevoy",
                "Jonas Schweichhart"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03229v1",
                "http://arxiv.org/pdf/2312.03229v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "91A10, 91A68",
                "J.4; F.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03227v1",
            "title": "Human Body Model based ID using Shape and Pose Parameters",
            "updated": "2023-12-06T01:51:54Z",
            "published": "2023-12-06T01:51:54Z",
            "summary": "We present a Human Body model based IDentification system (HMID) system that\nis jointly trained for shape, pose and biometric identification. HMID is based\non the Human Mesh Recovery (HMR) network and we propose additional losses to\nimprove and stabilize shape estimation and biometric identification while\nmaintaining the pose and shape output. We show that when our HMID network is\ntrained using additional shape and pose losses, it shows a significant\nimprovement in biometric identification performance when compared to an\nidentical model that does not use such losses. The HMID model uses raw images\ninstead of silhouettes and is able to perform robust recognition on images\ncollected at range and altitude as many anthropometric properties are\nreasonably invariant to clothing, view and range. We show results on the USF\ndataset as well as the BRIAR dataset which includes probes with both clothing\nand view changes. Our approach (using body model losses) shows a significant\nimprovement in Rank20 accuracy and True Accuracy Rate on the BRIAR evaluation\ndataset.",
            "author": [
                "Aravind Sundaresan",
                "Brian Burns",
                "Indranil Sur",
                "Yi Yao",
                "Xiao Lin",
                "Sujeong Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03227v1",
                "http://arxiv.org/pdf/2312.03227v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03226v1",
            "title": "Rethinking Object Saliency Ranking: A Novel Whole-flow Processing\n  Paradigm",
            "updated": "2023-12-06T01:51:03Z",
            "published": "2023-12-06T01:51:03Z",
            "summary": "Existing salient object detection methods are capable of predicting binary\nmaps that highlight visually salient regions. However, these methods are\nlimited in their ability to differentiate the relative importance of multiple\nobjects and the relationships among them, which can lead to errors and reduced\naccuracy in downstream tasks that depend on the relative importance of multiple\nobjects. To conquer, this paper proposes a new paradigm for saliency ranking,\nwhich aims to completely focus on ranking salient objects by their \"importance\norder\". While previous works have shown promising performance, they still face\nill-posed problems. First, the saliency ranking ground truth (GT) orders\ngeneration methods are unreasonable since determining the correct ranking order\nis not well-defined, resulting in false alarms. Second, training a ranking\nmodel remains challenging because most saliency ranking methods follow the\nmulti-task paradigm, leading to conflicts and trade-offs among different tasks.\nThird, existing regression-based saliency ranking methods are complex for\nsaliency ranking models due to their reliance on instance mask-based saliency\nranking orders. These methods require a significant amount of data to perform\naccurately and can be challenging to implement effectively. To solve these\nproblems, this paper conducts an in-depth analysis of the causes and proposes a\nwhole-flow processing paradigm of saliency ranking task from the perspective of\n\"GT data generation\", \"network structure design\" and \"training protocol\". The\nproposed approach outperforms existing state-of-the-art methods on the\nwidely-used SALICON set, as demonstrated by extensive experiments with fair and\nreasonable comparisons. The saliency ranking task is still in its infancy, and\nour proposed unified framework can serve as a fundamental strategy to guide\nfuture work.",
            "author": [
                "Mengke Song",
                "Linfeng Li",
                "Dunquan Wu",
                "Wenfeng Song",
                "Chenglizhao Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03226v1",
                "http://arxiv.org/pdf/2312.03226v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03225v1",
            "title": "Snake Robot with Tactile Perception Navigates on Large-scale Challenging\n  Terrain",
            "updated": "2023-12-06T01:50:47Z",
            "published": "2023-12-06T01:50:47Z",
            "summary": "Along with the advancement of robot skin technology, there has been notable\nprogress in the development of snake robots featuring body-surface tactile\nperception. In this study, we proposed a locomotion control framework for snake\nrobots that integrates tactile perception to augment their adaptability to\nvarious terrains. Our approach embraces a hierarchical reinforcement learning\n(HRL) architecture, wherein the high-level orchestrates global navigation\nstrategies while the low-level uses curriculum learning for local navigation\nmaneuvers. Due to the significant computational demands of collision detection\nin whole-body tactile sensing, the efficiency of the simulator is severely\ncompromised. Thus a distributed training pattern to mitigate the efficiency\nreduction was adopted. We evaluated the navigation performance of the snake\nrobot in complex large-scale cave exploration with challenging terrains to\nexhibit improvements in motion efficiency, evidencing the efficacy of tactile\nperception in terrain-adaptive locomotion of snake robots.",
            "author": [
                "Shuo Jiang",
                "Adarsh Salagame",
                "Alireza Ramezani",
                "Lawson Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03225v1",
                "http://arxiv.org/pdf/2312.03225v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03224v1",
            "title": "Novel Hydrodynamic Schemes Capturing Shocks and Contact Discontinuities\n  and Comparison Study with Existing Methods",
            "updated": "2023-12-06T01:47:08Z",
            "published": "2023-12-06T01:47:08Z",
            "summary": "We present a new hydrodynamic scheme named Godunov Density-Independent\nSmoothed Particle Hydrodynamics (GDISPH), that can accurately handle shock\nwaves and contact discontinuities without any manually tuned parameters. This\nis in contrast to the standard formulation of smoothed particle hydrodynamics\n(SSPH), which requires the parameters for an artificial viscosity term to\nhandle the shocks and struggles to accurately handle the contact\ndiscontinuities due to unphysical repulsive forces, resulting in surface\ntension that disrupts pressure equilibrium and suppresses fluid instabilities.\nWhile Godunov SPH (GSPH) can handle the shocks without the parameters by using\nsolutions from a Riemann solver, it still cannot fully handle the contact\ndiscontinuities. Density-Independent Smoothed Particle Hydrodynamics (DISPH),\none of several schemes proposed to handle contact discontinuities more\neffectively than SSPH, demonstrates superior performance in our tests involving\nstrong shocks and contact discontinuities. However, DISPH still requires the\nartificial viscosity term. We integrate the Riemann solver into DISPH in\nseveral ways, yielding some patterns of GDISPH. The results of standard tests\nsuch as the one-dimensional Riemann problem, pressure equilibrium,\nSedov-Taylor, and Kelvin-Helmholtz tests are favourable to GDISPH Case 1 and\nGDISPH Case 2, as well as DISPH. We conclude that GDISPH Case 1 has an\nadvantage over GDISPH Case 2, effectively handling shocks and contact\ndiscontinuities without the need for specific parameters or kernels and without\nintroducing any additional numerical diffusion.",
            "author": [
                "Takuhiro Yuasa",
                "Masao Mori"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03224v1",
                "http://arxiv.org/pdf/2312.03224v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "astro-ph.CO",
                "astro-ph.GA",
                "physics.comp-ph",
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03222v1",
            "title": "Predicting Scores of Various Aesthetic Attribute Sets by Learning from\n  Overall Score Labels",
            "updated": "2023-12-06T01:41:49Z",
            "published": "2023-12-06T01:41:49Z",
            "summary": "Now many mobile phones embed deep-learning models for evaluation or guidance\non photography. These models cannot provide detailed results like human pose\nscores or scene color scores because of the rare of corresponding aesthetic\nattribute data. However, the annotation of image aesthetic attribute scores\nrequires experienced artists and professional photographers, which hinders the\ncollection of large-scale fully-annotated datasets. In this paper, we propose\nto replace image attribute labels with feature extractors. First, a novel\naesthetic attribute evaluation framework based on attribute features is\nproposed to predict attribute scores and overall scores. We call it the F2S\n(attribute features to attribute scores) model. We use networks from different\ntasks to provide attribute features to our F2S models. Then, we define an\naesthetic attribute contribution to describe the role of aesthetic attributes\nthroughout an image and use it with the attribute scores and the overall scores\nto train our F2S model. Sufficient experiments on publicly available datasets\ndemonstrate that our F2S model achieves comparable performance with those\ntrained on the datasets with fully-annotated aesthetic attribute score labels.\nOur method makes it feasible to learn meaningful attribute scores for various\naesthetic attribute sets in different types of images with only overall\naesthetic scores.",
            "author": [
                "Heng Huang",
                "Xin Jin",
                "Yaqi Liu",
                "Hao Lou",
                "Chaoen Xiao",
                "Shuai Cui",
                "Xinning Li",
                "Dongqing Zou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03222v1",
                "http://arxiv.org/pdf/2312.03222v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03219v1",
            "title": "Multifractality in critical neural field dynamics",
            "updated": "2023-12-06T01:20:58Z",
            "published": "2023-12-06T01:20:58Z",
            "summary": "The brain criticality framework has largely considered brain dynamics to be\nmonofractal even though experimental evidence suggests that the brain exhibits\nsignificant multifractality. To understand how multifractality may emerge in\ncritical-like systems, we use a computational model for critical neural\noscillations. We find that multifractality emerges near a synchronization phase\ntransition. These findings show multifractality in temporal dynamics peaks at\ncriticality in neural fields, providing a generative model for interpreting\nmultifractality in brain recordings.",
            "author": [
                "Merlin Dumeur",
                "Sheng H. Wang",
                "J. Matias Palva",
                "Philippe Ciuciu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03219v1",
                "http://arxiv.org/pdf/2312.03219v1"
            ],
            "primary_category": "cond-mat.dis-nn",
            "category": [
                "cond-mat.dis-nn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03218v1",
            "title": "Accelerated Gradient Algorithms with Adaptive Subspace Search for\n  Instance-Faster Optimization",
            "updated": "2023-12-06T01:16:10Z",
            "published": "2023-12-06T01:16:10Z",
            "summary": "Gradient-based minimax optimal algorithms have greatly promoted the\ndevelopment of continuous optimization and machine learning. One seminal work\ndue to Yurii Nesterov [Nes83a] established $\\tilde{\\mathcal{O}}(\\sqrt{L/\\mu})$\ngradient complexity for minimizing an $L$-smooth $\\mu$-strongly convex\nobjective. However, an ideal algorithm would adapt to the explicit complexity\nof a particular objective function and incur faster rates for simpler problems,\ntriggering our reconsideration of two defeats of existing optimization modeling\nand analysis. (i) The worst-case optimality is neither the instance optimality\nnor such one in reality. (ii) Traditional $L$-smoothness condition may not be\nthe primary abstraction/characterization for modern practical problems.\n  In this paper, we open up a new way to design and analyze gradient-based\nalgorithms with direct applications in machine learning, including linear\nregression and beyond. We introduce two factors $(\\alpha, \\tau_{\\alpha})$ to\nrefine the description of the degenerated condition of the optimization\nproblems based on the observation that the singular values of Hessian often\ndrop sharply. We design adaptive algorithms that solve simpler problems without\npre-known knowledge with reduced gradient or analogous oracle accesses. The\nalgorithms also improve the state-of-art complexities for several problems in\nmachine learning, thereby solving the open problem of how to design faster\nalgorithms in light of the known complexity lower bounds. Specially, with the\n$\\mathcal{O}(1)$-nuclear norm bounded, we achieve an optimal\n$\\tilde{\\mathcal{O}}(\\mu^{-1/3})$ (v.s. $\\tilde{\\mathcal{O}}(\\mu^{-1/2})$)\ngradient complexity for linear regression. We hope this work could invoke the\nrethinking for understanding the difficulty of modern problems in optimization.",
            "author": [
                "Yuanshi Liu",
                "Hanzhen Zhao",
                "Yang Xu",
                "Pengyun Yue",
                "Cong Fang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03218v1",
                "http://arxiv.org/pdf/2312.03218v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CC",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03217v1",
            "title": "Rethinking E-Commerce Search",
            "updated": "2023-12-06T01:15:40Z",
            "published": "2023-12-06T01:15:40Z",
            "summary": "E-commerce search and recommendation usually operate on structured data such\nas product catalogs and taxonomies. However, creating better search and\nrecommendation systems often requires a large variety of unstructured data\nincluding customer reviews and articles on the web. Traditionally, the solution\nhas always been converting unstructured data into structured data through\ninformation extraction, and conducting search over the structured data.\nHowever, this is a costly approach that often has low quality. In this paper,\nwe envision a solution that does entirely the opposite. Instead of converting\nunstructured data (web pages, customer reviews, etc) to structured data, we\ninstead convert structured data (product inventory, catalogs, taxonomies, etc)\ninto textual data, which can be easily integrated into the text corpus that\ntrains LLMs. Then, search and recommendation can be performed through a Q/A\nmechanism through an LLM instead of using traditional information retrieval\nmethods over structured data.",
            "author": [
                "Haixun Wang",
                "Taesik Na"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03217v1",
                "http://arxiv.org/pdf/2312.03217v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03209v1",
            "title": "Cache Me if You Can: Accelerating Diffusion Models through Block Caching",
            "updated": "2023-12-06T00:51:38Z",
            "published": "2023-12-06T00:51:38Z",
            "summary": "Diffusion models have recently revolutionized the field of image synthesis\ndue to their ability to generate photorealistic images. However, one of the\nmajor drawbacks of diffusion models is that the image generation process is\ncostly. A large image-to-image network has to be applied many times to\niteratively refine an image from random noise. While many recent works propose\ntechniques to reduce the number of required steps, they generally treat the\nunderlying denoising network as a black box. In this work, we investigate the\nbehavior of the layers within the network and find that 1) the layers' output\nchanges smoothly over time, 2) the layers show distinct patterns of change, and\n3) the change from step to step is often very small. We hypothesize that many\nlayer computations in the denoising network are redundant. Leveraging this, we\nintroduce block caching, in which we reuse outputs from layer blocks of\nprevious steps to speed up inference. Furthermore, we propose a technique to\nautomatically determine caching schedules based on each block's changes over\ntimesteps. In our experiments, we show through FID, human evaluation and\nqualitative analysis that Block Caching allows to generate images with higher\nvisual quality at the same computational cost. We demonstrate this for\ndifferent state-of-the-art models (LDM and EMU) and solvers (DDIM and DPM).",
            "author": [
                "Felix Wimbauer",
                "Bichen Wu",
                "Edgar Schoenfeld",
                "Xiaoliang Dai",
                "Ji Hou",
                "Zijian He",
                "Artsiom Sanakoyeu",
                "Peizhao Zhang",
                "Sam Tsai",
                "Jonas Kohler",
                "Christian Rupprecht",
                "Daniel Cremers",
                "Peter Vajda",
                "Jialiang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03209v1",
                "http://arxiv.org/pdf/2312.03209v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03207v1",
            "title": "Satellite Imagery and AI: A New Era in Ocean Conservation, from Research\n  to Deployment and Impact",
            "updated": "2023-12-06T00:48:50Z",
            "published": "2023-12-06T00:48:50Z",
            "summary": "Illegal, unreported, and unregulated (IUU) fishing poses a global threat to\nocean habitats. Publicly available satellite data offered by NASA and the\nEuropean Space Agency (ESA) provide an opportunity to actively monitor this\nactivity. Effectively leveraging satellite data for maritime conservation\nrequires highly reliable machine learning models operating globally with\nminimal latency. This paper introduces three specialized computer vision models\ndesigned for synthetic aperture radar (Sentinel-1), optical imagery\n(Sentinel-2), and nighttime lights (Suomi-NPP/NOAA-20). It also presents best\npractices for developing and delivering real-time computer vision services for\nconservation. These models have been deployed in Skylight, a real time maritime\nmonitoring platform, which is provided at no cost to users worldwide.",
            "author": [
                "Patrick Beukema",
                "Favyen Bastani",
                "Piper Wolters",
                "Henry Herzog",
                "Joe Ferdinando"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03207v1",
                "http://arxiv.org/pdf/2312.03207v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03205v1",
            "title": "Who Leaked the Model? Tracking IP Infringers in Accountable Federated\n  Learning",
            "updated": "2023-12-06T00:47:55Z",
            "published": "2023-12-06T00:47:55Z",
            "summary": "Federated learning (FL) emerges as an effective collaborative learning\nframework to coordinate data and computation resources from massive and\ndistributed clients in training. Such collaboration results in non-trivial\nintellectual property (IP) represented by the model parameters that should be\nprotected and shared by the whole party rather than an individual user.\nMeanwhile, the distributed nature of FL endorses a malicious client the\nconvenience to compromise IP through illegal model leakage to unauthorized\nthird parties. To block such IP leakage, it is essential to make the IP\nidentifiable in the shared model and locate the anonymous infringer who first\nleaks it. The collective challenges call for \\emph{accountable federated\nlearning}, which requires verifiable ownership of the model and is capable of\nrevealing the infringer's identity upon leakage. In this paper, we propose\nDecodable Unique Watermarking (DUW) for complying with the requirements of\naccountable FL. Specifically, before a global model is sent to a client in an\nFL round, DUW encodes a client-unique key into the model by leveraging a\nbackdoor-based watermark injection. To identify the infringer of a leaked\nmodel, DUW examines the model and checks if the triggers can be decoded as the\ncorresponding keys. Extensive empirical results show that DUW is highly\neffective and robust, achieving over $99\\%$ watermark success rate for Digits,\nCIFAR-10, and CIFAR-100 datasets under heterogeneous FL settings, and\nidentifying the IP infringer with $100\\%$ accuracy even after common watermark\nremoval attempts.",
            "author": [
                "Shuyang Yu",
                "Junyuan Hong",
                "Yi Zeng",
                "Fei Wang",
                "Ruoxi Jia",
                "Jiayu Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03205v1",
                "http://arxiv.org/pdf/2312.03205v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03203v1",
            "title": "Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled\n  Feature Fields",
            "updated": "2023-12-06T00:46:30Z",
            "published": "2023-12-06T00:46:30Z",
            "summary": "3D scene representations have gained immense popularity in recent years.\nMethods that use Neural Radiance fields are versatile for traditional tasks\nsuch as novel view synthesis. In recent times, some work has emerged that aims\nto extend the functionality of NeRF beyond view synthesis, for semantically\naware tasks such as editing and segmentation using 3D feature field\ndistillation from 2D foundation models. However, these methods have two major\nlimitations: (a) they are limited by the rendering speed of NeRF pipelines, and\n(b) implicitly represented feature fields suffer from continuity artifacts\nreducing feature quality. Recently, 3D Gaussian Splatting has shown\nstate-of-the-art performance on real-time radiance field rendering. In this\nwork, we go one step further: in addition to radiance field rendering, we\nenable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D\nfoundation model distillation. This translation is not straightforward: naively\nincorporating feature fields in the 3DGS framework leads to warp-level\ndivergence. We propose architectural and training changes to efficiently avert\nthis problem. Our proposed method is general, and our experiments showcase\nnovel view semantic segmentation, language-guided editing and segment anything\nthrough learning feature fields from state-of-the-art 2D foundation models such\nas SAM and CLIP-LSeg. Across experiments, our distillation method is able to\nprovide comparable or better results, while being significantly faster to both\ntrain and render. Additionally, to the best of our knowledge, we are the first\nmethod to enable point and bounding-box prompting for radiance field\nmanipulation, by leveraging the SAM model. Project website at:\nhttps://feature-3dgs.github.io/",
            "author": [
                "Shijie Zhou",
                "Haoran Chang",
                "Sicheng Jiang",
                "Zhiwen Fan",
                "Zehao Zhu",
                "Dejia Xu",
                "Pradyumna Chari",
                "Suya You",
                "Zhangyang Wang",
                "Achuta Kadambi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03203v1",
                "http://arxiv.org/pdf/2312.03203v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03773v1",
            "title": "A multi-layer refined network model for the identification of essential\n  proteins",
            "updated": "2023-12-06T00:11:08Z",
            "published": "2023-12-06T00:11:08Z",
            "summary": "The identification of essential proteins in protein-protein interaction\nnetworks (PINs) can help to discover drug targets and prevent disease. In order\nto improve the accuracy of the identification of essential proteins,\nresearchers attempted to obtain a refined PIN by combining multiple biological\ninformation to filter out some unreliable interactions in the PIN.\nUnfortunately, such approaches drastically reduce the number of nodes in the\nPIN after multiple refinements and result in a sparser PIN. It makes a\nconsiderable portion of essential proteins unidentifiable. In this paper, we\npropose a multi-layer refined network (MR-PIN) that addresses this problem.\nFirstly, four refined networks are constructed by respectively integrating\ndifferent biological information into the static PIN to form a multi-layer\nheterogeneous network. Then scores of proteins in each network layer are\ncalculated by the existing node ranking method, and the importance score of a\nprotein in the MR-PIN is evaluated in terms of the geometric mean of its scores\nin all layers. Finally, all nodes are sorted by their importance scores to\ndetermine their essentiality. To evaluate the effectiveness of the multi-layer\nrefined network model, we apply 16 node ranking methods on the MR-PIN, and\ncompare the results with those on the SPIN, DPIN and RDPIN. Then the predictive\nperformances of these ranking methods are validated in terms of the\nidentification number of essential protein at top100 - top600, sensitivity,\nspecificity, positive predictive value, negative predictive value, F-measure,\naccuracy, Jackknife, ROCAUC and PRAUC. The experimental results show that the\nMR-PIN is superior to the existing refined PINs in the identification accuracy\nof essential proteins.",
            "author": [
                "Haoyue Wang",
                "Li Pan",
                "Bo Yang",
                "Junqiang Jiang",
                "Wenbin Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03773v1",
                "http://arxiv.org/pdf/2312.03773v1"
            ],
            "primary_category": "q-bio.MN",
            "category": [
                "q-bio.MN",
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03195v1",
            "title": "Detecting Rumor Veracity with Only Textual Information by Double-Channel\n  Structure",
            "updated": "2023-12-06T00:08:44Z",
            "published": "2023-12-06T00:08:44Z",
            "summary": "Kyle (1985) proposes two types of rumors: informed rumors which are based on\nsome private information and uninformed rumors which are not based on any\ninformation (i.e. bluffing). Also, prior studies find that when people have\ncredible source of information, they are likely to use a more confident textual\ntone in their spreading of rumors. Motivated by these theoretical findings, we\npropose a double-channel structure to determine the ex-ante veracity of rumors\non social media. Our ultimate goal is to classify each rumor into true, false,\nor unverifiable category. We first assign each text into either certain\n(informed rumor) or uncertain (uninformed rumor) category. Then, we apply lie\ndetection algorithm to informed rumors and thread-reply agreement detection\nalgorithm to uninformed rumors. Using the dataset of SemEval 2019 Task 7, which\nrequires ex-ante threefold classification (true, false, or unverifiable) of\nsocial media rumors, our model yields a macro-F1 score of 0.4027, outperforming\nall the baseline models and the second-place winner (Gorrell et al., 2019).\nFurthermore, we empirically validate that the double-channel structure\noutperforms single-channel structures which use either lie detection or\nagreement detection algorithm to all posts.",
            "author": [
                "Alex Kim",
                "Sangwon Yoon"
            ],
            "link": [
                "http://dx.doi.org/10.18653/v1/2022.socialnlp-1.3",
                "http://arxiv.org/abs/2312.03195v1",
                "http://arxiv.org/pdf/2312.03195v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03194v1",
            "title": "Corporate Bankruptcy Prediction with Domain-Adapted BERT",
            "updated": "2023-12-06T00:05:25Z",
            "published": "2023-12-06T00:05:25Z",
            "summary": "This study performs BERT-based analysis, which is a representative\ncontextualized language model, on corporate disclosure data to predict\nimpending bankruptcies. Prior literature on bankruptcy prediction mainly\nfocuses on developing more sophisticated prediction methodologies with\nfinancial variables. However, in our study, we focus on improving the quality\nof input dataset. Specifically, we employ BERT model to perform sentiment\nanalysis on MD&A disclosures. We show that BERT outperforms dictionary-based\npredictions and Word2Vec-based predictions in terms of adjusted R-square in\nlogistic regression, k-nearest neighbor (kNN-5), and linear kernel support\nvector machine (SVM). Further, instead of pre-training the BERT model from\nscratch, we apply self-learning with confidence-based filtering to corporate\ndisclosure data (10-K). We achieve the accuracy rate of 91.56% and demonstrate\nthat the domain adaptation procedure brings a significant improvement in\nprediction accuracy.",
            "author": [
                "Alex Kim",
                "Sangwon Yoon"
            ],
            "link": [
                "http://dx.doi.org/10.18653/v1/2021.econlp-1.4",
                "http://arxiv.org/abs/2312.03194v1",
                "http://arxiv.org/pdf/2312.03194v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG",
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03193v1",
            "title": "Conceptualizing the Relationship between AI Explanations and User Agency",
            "updated": "2023-12-05T23:56:05Z",
            "published": "2023-12-05T23:56:05Z",
            "summary": "We grapple with the question: How, for whom and why should explainable\nartificial intelligence (XAI) aim to support the user goal of agency? In\nparticular, we analyze the relationship between agency and explanations through\na user-centric lens through case studies and thought experiments. We find that\nexplanation serves as one of several possible first steps for agency by\nallowing the user convert forethought to outcome in a more effective manner in\nfuture interactions. Also, we observe that XAI systems might better cater to\nlaypersons, particularly \"tinkerers\", when combining explanations and user\ncontrol, so they can make meaningful changes.",
            "author": [
                "Iyadunni Adenuga",
                "Jonathan Dodge"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03193v1",
                "http://arxiv.org/pdf/2312.03193v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03191v1",
            "title": "Can we distinguish black holes with electric and magnetic charges from\n  quasinormal modes?",
            "updated": "2023-12-05T23:43:14Z",
            "published": "2023-12-05T23:43:14Z",
            "summary": "We compute the quasinormal modes of static and spherically symmetric black\nholes (BHs) with electric and magnetic charges. For the electrically charged\ncase, the dynamics of perturbations separates into the odd- and even-parity\nsectors with two coupled differential equations in each sector. In the presence\nof both electric and magnetic charges, the differential equations of four\ndynamical degrees of freedom are coupled with each other between odd- and\neven-parity perturbations. Despite this notable modification, we show that, for\na given total charge and mass, a BH with mixed electric and magnetic charges\ngives rise to the same quasinormal frequencies for fundamental modes. This\nincludes the case in which two BHs have equal electric and magnetic charges for\neach of them. Thus, the gravitational-wave observations of quasinormal modes\nduring the ringdown phase alone do not distinguish between electrically and\nmagnetically charged BHs.",
            "author": [
                "Antonio De Felice",
                "Shinji Tsujikawa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03191v1",
                "http://arxiv.org/pdf/2312.03191v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc",
                "astro-ph.CO",
                "hep-ph",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03772v1",
            "title": "DiffusionAtlas: High-Fidelity Consistent Diffusion Video Editing",
            "updated": "2023-12-05T23:40:30Z",
            "published": "2023-12-05T23:40:30Z",
            "summary": "We present a diffusion-based video editing framework, namely DiffusionAtlas,\nwhich can achieve both frame consistency and high fidelity in editing video\nobject appearance. Despite the success in image editing, diffusion models still\nencounter significant hindrances when it comes to video editing due to the\nchallenge of maintaining spatiotemporal consistency in the object's appearance\nacross frames. On the other hand, atlas-based techniques allow propagating\nedits on the layered representations consistently back to frames. However, they\noften struggle to create editing effects that adhere correctly to the\nuser-provided textual or visual conditions due to the limitation of editing the\ntexture atlas on a fixed UV mapping field. Our method leverages a\nvisual-textual diffusion model to edit objects directly on the diffusion\natlases, ensuring coherent object identity across frames. We design a loss term\nwith atlas-based constraints and build a pretrained text-driven diffusion model\nas pixel-wise guidance for refining shape distortions and correcting texture\ndeviations. Qualitative and quantitative experiments show that our method\noutperforms state-of-the-art methods in achieving consistent high-fidelity\nvideo-object editing.",
            "author": [
                "Shao-Yu Chang",
                "Hwann-Tzong Chen",
                "Tyng-Luh Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03772v1",
                "http://arxiv.org/pdf/2312.03772v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03187v1",
            "title": "FERGI: Automatic Annotation of User Preferences for Text-to-Image\n  Generation from Spontaneous Facial Expression Reaction",
            "updated": "2023-12-05T23:33:49Z",
            "published": "2023-12-05T23:33:49Z",
            "summary": "Researchers have proposed to use data of human preference feedback to\nfine-tune text-to-image generative models. However, the scalability of human\nfeedback collection has been limited by its reliance on manual annotation.\nTherefore, we develop and test a method to automatically annotate user\npreferences from their spontaneous facial expression reaction to the generated\nimages. We collect a dataset of Facial Expression Reaction to Generated Images\n(FERGI) and show that the activations of multiple facial action units (AUs) are\nhighly correlated with user evaluations of the generated images. Specifically,\nAU4 (brow lowerer) is most consistently reflective of negative evaluations of\nthe generated image. This can be useful in two ways. Firstly, we can\nautomatically annotate user preferences between image pairs with substantial\ndifference in AU4 responses to them with an accuracy significantly\noutperforming state-of-the-art scoring models. Secondly, directly integrating\nthe AU4 responses with the scoring models improves their consistency with human\npreferences. Additionally, the AU4 response best reflects the user's evaluation\nof the image fidelity, making it complementary to the state-of-the-art scoring\nmodels, which are generally better at reflecting image-text alignment. Finally,\nthis method of automatic annotation with facial expression analysis can be\npotentially generalized to other generation tasks. The code is available at\nhttps://github.com/ShuangquanFeng/FERGI, and the dataset is also available at\nthe same link for research purposes.",
            "author": [
                "Shuangquan Feng",
                "Junhua Ma",
                "Virginia R. de Sa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03187v1",
                "http://arxiv.org/pdf/2312.03187v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03186v1",
            "title": "Data-Driven Traffic Reconstruction and Kernel Methods for Identifying\n  Stop-and-Go Congestion",
            "updated": "2023-12-05T23:32:48Z",
            "published": "2023-12-05T23:32:48Z",
            "summary": "Identifying stop-and-go events (SAGs) in traffic flow presents an important\navenue for advancing data-driven research for climate change mitigation and\nsustainability, owing to their substantial impact on carbon emissions, travel\ntime, fuel consumption, and roadway safety. In fact, SAGs are estimated to\naccount for 33-50% of highway driving externalities. However, insufficient\nattention has been paid to precisely quantifying where, when, and how much\nthese SAGs take place -necessary for downstream decision making, such as\nintervention design and policy analysis. A key challenge is that the data\navailable to researchers and governments are typically sparse and aggregated to\na granularity that obscures SAGs. To overcome such data limitations, this study\nthus explores the use of traffic reconstruction techniques for SAG\nidentification. In particular, we introduce a kernel-based method for\nidentifying spatio-temporal features in traffic and leverage bootstrapping to\nquantify the uncertainty of the reconstruction process. Experimental results on\nCalifornia highway data demonstrate the promise of the method for capturing\nSAGs. This work contributes to a foundation for data-driven decision making to\nadvance sustainability of traffic systems.",
            "author": [
                "Edgar Ramirez Sanchez",
                "Shreyaa Raghavan",
                "Cathy Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03186v1",
                "http://arxiv.org/pdf/2312.03186v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03180v1",
            "title": "Image reconstructions using sparse dictionary representations and\n  implicit, non-negative mappings",
            "updated": "2023-12-05T23:07:21Z",
            "published": "2023-12-05T23:07:21Z",
            "summary": "Many imaging science tasks can be modeled as a discrete linear inverse\nproblem. Solving linear inverse problems is often challenging, with\nill-conditioned operators and potentially non-unique solutions. Embedding prior\nknowledge, such as smoothness, into the solution can overcome these challenges.\nIn this work, we encode prior knowledge using a non-negative patch dictionary,\nwhich effectively learns a basis from a training set of natural images. In this\ndictionary basis, we desire solutions that are non-negative and sparse (i.e.,\ncontain many zero entries). With these constraints, standard methods for\nsolving discrete linear inverse problems are not directly applicable. One such\napproach is the modified residual norm steepest descent (MRNSD), which produces\nnon-negative solutions but does not induce sparsity. In this paper, we provide\ntwo methods based on MRNSD that promote sparsity. In our first method, we add\nan $\\ell_1$-regularization term with a new, optimal step size. In our second\nmethod, we propose a new non-negative, sparsity-promoting mapping of the\nsolution. We compare the performance of our proposed methods on a number of\nnumerical experiments, including deblurring, image completion, computer\ntomography, and superresolution. Our results show that these methods\neffectively solve discrete linear inverse problems with non-negativity and\nsparsity constraints.",
            "author": [
                "Elizabeth Newman",
                "Jack Michael Solomon",
                "Matthias Chung"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03180v1",
                "http://arxiv.org/pdf/2312.03180v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "65F10, 65F22",
                "G.1.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03179v1",
            "title": "CaloQVAE : Simulating high-energy particle-calorimeter interactions\n  using hybrid quantum-classical generative models",
            "updated": "2023-12-05T23:05:36Z",
            "published": "2023-12-05T23:05:36Z",
            "summary": "The Large Hadron Collider's high luminosity era presents major computational\nchallenges in the analysis of collision events. Large amounts of Monte Carlo\n(MC) simulation will be required to constrain the statistical uncertainties of\nthe simulated datasets below these of the experimental data. Modelling of\nhigh-energy particles propagating through the calorimeter section of the\ndetector is the most computationally intensive MC simulation task. We introduce\na technique combining recent advancements in generative models and quantum\nannealing for fast and efficient simulation of high-energy particle-calorimeter\ninteractions.",
            "author": [
                "Sehmimul Hoque",
                "Hao Jia",
                "Abhishek Abhishek",
                "Mojde Fadaie",
                "J. Quetzalcoatl Toledo-Mar\u00edn",
                "Tiago Vale",
                "Roger G. Melko",
                "Maximilian Swiatlowski",
                "Wojciech T. Fedorko"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03179v1",
                "http://arxiv.org/pdf/2312.03179v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex",
                "cs.LG",
                "quant-ph",
                "81P68, 68T07, 81V99"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03178v1",
            "title": "Exploring Lee-Yang and Fisher Zeros in the 2D Ising Model through\n  Multi-Point Pad\u00e9 Approximants",
            "updated": "2023-12-05T22:57:20Z",
            "published": "2023-12-05T22:57:20Z",
            "summary": "We present a numerical calculation of the Lee-Yang and Fisher zeros of the 2D\nIsing model using multi-point Pad\\'{e} approximants. We perform simulations for\nthe 2D Ising model with ferromagnetic couplings both in the absence and in the\npresence of a magnetic field using a cluster spin-flip algorithm. We show that\nit is possible to extract genuine signature of Lee Yang and Fisher zeros of the\ntheory through the poles of magnetization and specific heat, using multi-point\nPad\\'{e} method. We extract the poles of magnetization using Pad\\'{e}\napproximants and compare their scaling with known results. We verify the circle\ntheorem associated to the well known behaviour of Lee Yang zeros. We present\nour finite volume scaling analysis of the zeros done at $T=T_c$ for a few\nlattice sizes, extracting to a very good precision the (combination of)\ncritical exponents $\\beta \\delta$. The computation at the critical temperature\nis performed after the latter has been determined via the study of Fisher\nzeros, thus extracting both $\\beta_c$ and the critical exponent $\\nu$. Results\nalready exist for extracting the critical exponents for the Ising model in 2\nand 3 dimensions making use of Fisher and Lee Yang zeros. In this work,\nmulti-point Pad\\'{e} is shown to be competitive with this respect and thus a\npowerful tool to study phase transitions.",
            "author": [
                "Simran Singh",
                "Massimo Cipressi",
                "Francesco Di Renzo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03178v1",
                "http://arxiv.org/pdf/2312.03178v1"
            ],
            "primary_category": "hep-lat",
            "category": [
                "hep-lat",
                "cond-mat.stat-mech",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03174v1",
            "title": "On the lengths of Okubo algebras",
            "updated": "2023-12-05T22:30:28Z",
            "published": "2023-12-05T22:30:28Z",
            "summary": "We compute the lengths of two particular cases of (possibly non-unital)\ncomposition algebras, namely, standard composition algebras and Okubo algebras\nover an arbitrary field $\\mathbb{F}$. These results finish the complete\ndescription of lengths of symmetric composition algebras and finite-dimensional\nflexible composition algebras.",
            "author": [
                "Alexander Guterman",
                "Svetlana Zhilina"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03174v1",
                "http://arxiv.org/pdf/2312.03174v1"
            ],
            "primary_category": "math.RA",
            "category": [
                "math.RA",
                "15A03, 17A20, 17A75"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03173v1",
            "title": "A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in\n  Programming Education",
            "updated": "2023-12-05T22:29:43Z",
            "published": "2023-12-05T22:29:43Z",
            "summary": "There is a constant need for educators to develop and maintain effective\nup-to-date assessments. While there is a growing body of research in computing\neducation on utilizing large language models (LLMs) in generation and\nengagement with coding exercises, the use of LLMs for generating programming\nMCQs has not been extensively explored. We analyzed the capability of GPT-4 to\nproduce multiple-choice questions (MCQs) aligned with specific learning\nobjectives (LOs) from Python programming classes in higher education.\nSpecifically, we developed an LLM-powered (GPT-4) system for generation of MCQs\nfrom high-level course context and module-level LOs. We evaluated 651\nLLM-generated and 449 human-crafted MCQs aligned to 246 LOs from 6 Python\ncourses. We found that GPT-4 was capable of producing MCQs with clear language,\na single correct choice, and high-quality distractors. We also observed that\nthe generated MCQs appeared to be well-aligned with the LOs. Our findings can\nbe leveraged by educators wishing to take advantage of the state-of-the-art\ngenerative models to support MCQ authoring efforts.",
            "author": [
                "Jacob Doughty",
                "Zipiao Wan",
                "Anishka Bompelli",
                "Jubahed Qayum",
                "Taozhi Wang",
                "Juran Zhang",
                "Yujia Zheng",
                "Aidan Doyle",
                "Pragnya Sridhar",
                "Arav Agarwal",
                "Christopher Bogart",
                "Eric Keylor",
                "Can Kultur",
                "Jaromir Savelka",
                "Majd Sakr"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3636243.3636256",
                "http://arxiv.org/abs/2312.03173v1",
                "http://arxiv.org/pdf/2312.03173v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03171v1",
            "title": "Combining Counting Processes and Classification Improves a Stopping Rule\n  for Technology Assisted Review",
            "updated": "2023-12-05T22:28:42Z",
            "published": "2023-12-05T22:28:42Z",
            "summary": "Technology Assisted Review (TAR) stopping rules aim to reduce the cost of\nmanually assessing documents for relevance by minimising the number of\ndocuments that need to be examined to ensure a desired level of recall. This\npaper extends an effective stopping rule using information derived from a text\nclassifier that can be trained without the need for any additional annotation.\nExperiments on multiple data sets (CLEF e-Health, TREC Total Recall, TREC Legal\nand RCV1) showed that the proposed approach consistently improves performance\nand outperforms several alternative methods.",
            "author": [
                "Reem Bin-Hezam",
                "Mark Stevenson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03171v1",
                "http://arxiv.org/pdf/2312.03171v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03771v1",
            "title": "DreamInpainter: Text-Guided Subject-Driven Image Inpainting with\n  Diffusion Models",
            "updated": "2023-12-05T22:23:19Z",
            "published": "2023-12-05T22:23:19Z",
            "summary": "This study introduces Text-Guided Subject-Driven Image Inpainting, a novel\ntask that combines text and exemplar images for image inpainting. While both\ntext and exemplar images have been used independently in previous efforts,\ntheir combined utilization remains unexplored. Simultaneously accommodating\nboth conditions poses a significant challenge due to the inherent balance\nrequired between editability and subject fidelity. To tackle this challenge, we\npropose a two-step approach DreamInpainter. First, we compute dense subject\nfeatures to ensure accurate subject replication. Then, we employ a\ndiscriminative token selection module to eliminate redundant subject details,\npreserving the subject's identity while allowing changes according to other\nconditions such as mask shape and text prompts. Additionally, we introduce a\ndecoupling regularization technique to enhance text control in the presence of\nexemplar images. Our extensive experiments demonstrate the superior performance\nof our method in terms of visual quality, identity preservation, and text\ncontrol, showcasing its effectiveness in the context of text-guided\nsubject-driven image inpainting.",
            "author": [
                "Shaoan Xie",
                "Yang Zhao",
                "Zhisheng Xiao",
                "Kelvin C. K. Chan",
                "Yandong Li",
                "Yanwu Xu",
                "Kun Zhang",
                "Tingbo Hou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03771v1",
                "http://arxiv.org/pdf/2312.03771v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03166v1",
            "title": "Deep Learning for Fast Inference of Mechanistic Models' Parameters",
            "updated": "2023-12-05T22:16:54Z",
            "published": "2023-12-05T22:16:54Z",
            "summary": "Inferring parameters of macro-kinetic growth models, typically represented by\nOrdinary Differential Equations (ODE), from the experimental data is a crucial\nstep in bioprocess engineering. Conventionally, estimates of the parameters are\nobtained by fitting the mechanistic model to observations. Fitting, however,\nrequires a significant computational power. Specifically, during the\ndevelopment of new bioprocesses that use previously unknown organisms or\nstrains, efficient, robust, and computationally cheap methods for parameter\nestimation are of great value. In this work, we propose using Deep Neural\nNetworks (NN) for directly predicting parameters of mechanistic models given\nobservations. The approach requires spending computational resources for\ntraining a NN, nonetheless, once trained, such a network can provide parameter\nestimates orders of magnitude faster than conventional methods. We consider a\ntraining procedure that combines Neural Networks and mechanistic models. We\ndemonstrate the performance of the proposed algorithms on data sampled from\nseveral mechanistic models used in bioengineering describing a typical\nindustrial batch process and compare the proposed method, a typical\ngradient-based fitting procedure, and the combination of the two. We find that,\nwhile Neural Network estimates are slightly improved by further fitting, these\nestimates are measurably better than the fitting procedure alone.",
            "author": [
                "Maxim Borisyak",
                "Stefan Born",
                "Peter Neubauer",
                "Mariano Nicolas Cruz-Bournazou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03166v1",
                "http://arxiv.org/pdf/2312.03166v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03160v1",
            "title": "HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces",
            "updated": "2023-12-05T22:04:49Z",
            "published": "2023-12-05T22:04:49Z",
            "summary": "Neural radiance fields provide state-of-the-art view synthesis quality but\ntend to be slow to render. One reason is that they make use of volume\nrendering, thus requiring many samples (and model queries) per ray at render\ntime. Although this representation is flexible and easy to optimize, most\nreal-world objects can be modeled more efficiently with surfaces instead of\nvolumes, requiring far fewer samples per ray. This observation has spurred\nconsiderable progress in surface representations such as signed distance\nfunctions, but these may struggle to model semi-opaque and thin structures. We\npropose a method, HybridNeRF, that leverages the strengths of both\nrepresentations by rendering most objects as surfaces while modeling the\n(typically) small fraction of challenging regions volumetrically. We evaluate\nHybridNeRF against the challenging Eyeful Tower dataset along with other\ncommonly used view synthesis datasets. When comparing to state-of-the-art\nbaselines, including recent rasterization-based approaches, we improve error\nrates by 15-30% while achieving real-time framerates (at least 36 FPS) for\nvirtual-reality resolutions (2Kx2K).",
            "author": [
                "Haithem Turki",
                "Vasu Agrawal",
                "Samuel Rota Bul\u00f2",
                "Lorenzo Porzi",
                "Peter Kontschieder",
                "Deva Ramanan",
                "Michael Zollh\u00f6fer",
                "Christian Richardt"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03160v1",
                "http://arxiv.org/pdf/2312.03160v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03159v1",
            "title": "Robust quantisation of circular photogalvanic effect in multiplicative\n  topological semimetals",
            "updated": "2023-12-05T22:03:24Z",
            "published": "2023-12-05T22:03:24Z",
            "summary": "Nonlinear response signatures are increasingly recognized as useful probes of\ncondensed matter systems, in particular for characterisation of topologically\nnon-trivial states. The circular photogalvanic effect (CPGE) is particularly\nuseful in study of topological semimetals, as the CPGE tensor quantises for\nwell-isolated topological degeneracies in strictly linearly-dispersing band\nstructures. Here, we study multiplicative Weyl semimetal band-structures, and\nfind that the multiplicative structure robustly protects the quantization of\nthe CPGE even in the case of non-linear dispersion. Computing phase diagrams as\na function of Weyl node tilting, we find a variety of quantised values for the\nCPGE tensor, revealing that the CPGE is also a useful tool in detecting and\ncharacterising parent topology of multiplicative topological states.",
            "author": [
                "Adipta Pal",
                "D\u00e1niel Varjas",
                "Ashley M. Cook"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03159v1",
                "http://arxiv.org/pdf/2312.03159v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03769v1",
            "title": "GPT vs Human for Scientific Reviews: A Dual Source Review on\n  Applications of ChatGPT in Science",
            "updated": "2023-12-05T21:41:52Z",
            "published": "2023-12-05T21:41:52Z",
            "summary": "The new polymath Large Language Models (LLMs) can speed-up greatly scientific\nreviews, possibly using more unbiased quantitative metrics, facilitating\ncross-disciplinary connections, and identifying emerging trends and research\ngaps by analyzing large volumes of data. However, at the present time, they\nlack the required deep understanding of complex methodologies, they have\ndifficulty in evaluating innovative claims, and they are unable to assess\nethical issues and conflicts of interest. Herein, we consider 13 GPT-related\npapers across different scientific domains, reviewed by a human reviewer and\nSciSpace, a large language model, with the reviews evaluated by three distinct\ntypes of evaluators, namely GPT-3.5, a crowd panel, and GPT-4. We found that\n50% of SciSpace's responses to objective questions align with those of a human\nreviewer, with GPT-4 (informed evaluator) often rating the human reviewer\nhigher in accuracy, and SciSpace higher in structure, clarity, and\ncompleteness. In subjective questions, the uninformed evaluators (GPT-3.5 and\ncrowd panel) showed varying preferences between SciSpace and human responses,\nwith the crowd panel showing a preference for the human responses. However,\nGPT-4 rated them equally in accuracy and structure but favored SciSpace for\ncompleteness.",
            "author": [
                "Chenxi Wu",
                "Alan John Varghese",
                "Vivek Oommen",
                "George Em Karniadakis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03769v1",
                "http://arxiv.org/pdf/2312.03769v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03154v1",
            "title": "ViscoNet: Bridging and Harmonizing Visual and Textual Conditioning for\n  ControlNet",
            "updated": "2023-12-05T21:41:17Z",
            "published": "2023-12-05T21:41:17Z",
            "summary": "This paper introduces ViscoNet, a novel method that enhances text-to-image\nhuman generation models with visual prompting. Unlike existing methods that\nrely on lengthy text descriptions to control the image structure, ViscoNet\nallows users to specify the visual appearance of the target object with a\nreference image. ViscoNet disentangles the object's appearance from the image\nbackground and injects it into a pre-trained latent diffusion model (LDM) model\nvia a ControlNet branch. This way, ViscoNet mitigates the style mode collapse\nproblem and enables precise and flexible visual control. We demonstrate the\neffectiveness of ViscoNet on human image generation, where it can manipulate\nvisual attributes and artistic styles with text and image prompts. We also show\nthat ViscoNet can learn visual conditioning from small and specific object\ndomains while preserving the generative power of the LDM backbone.",
            "author": [
                "Soon Yau Cheong",
                "Armin Mustafa",
                "Andrew Gilbert"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03154v1",
                "http://arxiv.org/pdf/2312.03154v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03152v1",
            "title": "Probing cosmic isotropy in the Local Universe",
            "updated": "2023-12-05T21:38:45Z",
            "published": "2023-12-05T21:38:45Z",
            "summary": "This is a model-independent analysis that investigates the statistical\nisotropy in the Local Universe using the ALFALFA survey data ($0 < z < 0.06$).\nWe investigate the angular distribution of HI extra-galactic sources from the\nALFALFA catalogue and study whether they are compatible with the statistical\nisotropy hypothesis using the two-point angular correlation function (2PACF).\nAware that the Local Universe is plenty of clustered structures and large\nvoids, we compute the 2PACF with the Landy-Szalay estimator performing\ndirectional analyses to inspect 10 sky regions. We investigate these 2PACF\nusing power-law best-fit analyses, and determine the statistical significance\nof the best-fit parameters for the 10 ALFALFA regions by comparison with the\nones obtained through the same procedure applied to a set of mock catalogues\nproduced under the homogeneity and isotropy hypotheses. Our conclusion is that\nthe Local Universe, as mapped by the HI sources of the ALFALFA survey, is in\nagreement with the hypothesis of statistical isotropy within $2\\,\\sigma$\nconfidence level, for small and large angle analyses, with the only exception\nof one region -- located near the Dipole Repeller -- which appears slightly\noutlier ($2.4\\,\\sigma$). Interestingly, regarding the large angular\ndistribution of the HI sources, we found 3 regions where the presence of cosmic\nvoids reported in the literature left their signature in our 2PACF, suggesting\nprojected large underdensities there, with number-density contrast $\\delta\n\\simeq -0.7$. According to the current literature these regions correspond,\npartially, to the sky position of the void structures known as Local Cosmic\nVoid and Dipole Repeller.",
            "author": [
                "Camila Franco",
                "Felipe Avila",
                "Armando Bernui"
            ],
            "link": [
                "http://dx.doi.org/10.1093/mnras/stad3616",
                "http://arxiv.org/abs/2312.03152v1",
                "http://arxiv.org/pdf/2312.03152v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03151v1",
            "title": "Multitask Learning Can Improve Worst-Group Outcomes",
            "updated": "2023-12-05T21:38:24Z",
            "published": "2023-12-05T21:38:24Z",
            "summary": "In order to create machine learning systems that serve a variety of users\nwell, it is vital to not only achieve high average performance but also ensure\nequitable outcomes across diverse groups. However, most machine learning\nmethods are designed to improve a model's average performance on a chosen end\ntask without consideration for their impact on worst group error. Multitask\nlearning (MTL) is one such widely used technique. In this paper, we seek not\nonly to understand the impact of MTL on worst-group accuracy but also to\nexplore its potential as a tool to address the challenge of group-wise\nfairness. We primarily consider the common setting of fine-tuning a pre-trained\nmodel, where, following recent work (Gururangan et al., 2020; Dery et al.,\n2023), we multitask the end task with the pre-training objective constructed\nfrom the end task data itself. In settings with few or no group annotations, we\nfind that multitasking often, but not always, achieves better worst-group\naccuracy than Just-Train-Twice (JTT; Liu et al. (2021)) -- a representative\ndistributionally robust optimization (DRO) method. Leveraging insights from\nsynthetic data experiments, we propose to modify standard MTL by regularizing\nthe joint multitask representation space. We run a large number of fine-tuning\nexperiments across computer vision and natural language and find that our\nregularized MTL approach consistently outperforms JTT on both worst and average\ngroup outcomes. Our official code can be found here:\nhttps://github.com/atharvajk98/MTL-group-robustness.",
            "author": [
                "Atharva Kulkarni",
                "Lucio Dery",
                "Amrith Setlur",
                "Aditi Raghunathan",
                "Ameet Talwalkar",
                "Graham Neubig"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03151v1",
                "http://arxiv.org/pdf/2312.03151v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03147v1",
            "title": "Neural parameter calibration and uncertainty quantification for epidemic\n  forecasting",
            "updated": "2023-12-05T21:34:59Z",
            "published": "2023-12-05T21:34:59Z",
            "summary": "The recent COVID-19 pandemic has thrown the importance of accurately\nforecasting contagion dynamics and learning infection parameters into sharp\nfocus. At the same time, effective policy-making requires knowledge of the\nuncertainty on such predictions, in order, for instance, to be able to ready\nhospitals and intensive care units for a worst-case scenario without needlessly\nwasting resources. In this work, we apply a novel and powerful computational\nmethod to the problem of learning probability densities on contagion parameters\nand providing uncertainty quantification for pandemic projections. Using a\nneural network, we calibrate an ODE model to data of the spread of COVID-19 in\nBerlin in 2020, achieving both a significantly more accurate calibration and\nprediction than Markov-Chain Monte Carlo (MCMC)-based sampling schemes. The\nuncertainties on our predictions provide meaningful confidence intervals e.g.\non infection figures and hospitalisation rates, while training and running the\nneural scheme takes minutes where MCMC takes hours. We show convergence of our\nmethod to the true posterior on a simplified SIR model of epidemics, and also\ndemonstrate our method's learning capabilities on a reduced dataset, where a\ncomplex model is learned from a small number of compartments for which data is\navailable.",
            "author": [
                "Thomas Gaskin",
                "Tim Conrad",
                "Grigorios A. Pavliotis",
                "Christof Sch\u00fctte"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03147v1",
                "http://arxiv.org/pdf/2312.03147v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC",
                "physics.soc-ph",
                "49-02, 92-02, 68-02",
                "J.3; G.1.6; I.2.1; G.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03146v1",
            "title": "LRMP: Layer Replication with Mixed Precision for Spatial In-memory DNN\n  Accelerators",
            "updated": "2023-12-05T21:31:20Z",
            "published": "2023-12-05T21:31:20Z",
            "summary": "In-memory computing (IMC) with non-volatile memories (NVMs) has emerged as a\npromising approach to address the rapidly growing computational demands of Deep\nNeural Networks (DNNs). Mapping DNN layers spatially onto NVM-based IMC\naccelerators achieves high degrees of parallelism. However, two challenges that\narise in this approach are the highly non-uniform distribution of layer\nprocessing times and high area requirements. We propose LRMP, a method to\njointly apply layer replication and mixed precision quantization to improve the\nperformance of DNNs when mapped to area-constrained NVM-based IMC accelerators.\nLRMP uses a combination of reinforcement learning and integer linear\nprogramming to search the replication-quantization design space using a model\nthat is closely informed by the target hardware architecture. Across five DNN\nbenchmarks, LRMP achieves 2.8-9$\\times$ latency and 11.8-19$\\times$ throughput\nimprovement at iso-accuracy.",
            "author": [
                "Abinand Nallathambi",
                "Christin David Bose",
                "Wilfried Haensch",
                "Anand Raghunathan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03146v1",
                "http://arxiv.org/pdf/2312.03146v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03145v1",
            "title": "Maximum likelihood thresholds of Gaussian graphical models and graphical\n  lasso",
            "updated": "2023-12-05T21:29:37Z",
            "published": "2023-12-05T21:29:37Z",
            "summary": "Associated to each graph G is a Gaussian graphical model. Such models are\noften used in high-dimensional settings, i.e. where there are relatively few\ndata points compared to the number of variables. The maximum likelihood\nthreshold of a graph is the minimum number of data points required to fit the\ncorresponding graphical model using maximum likelihood estimation. Graphical\nlasso is a method for selecting and fitting a graphical model. In this project,\nwe ask: when graphical lasso is used to select and fit a graphical model on n\ndata points, how likely is it that n is greater than or equal to the maximum\nlikelihood threshold of the corresponding graph? Our results are a series of\ncomputational experiments.",
            "author": [
                "Daniel Irving Bernstein",
                "Hayden Outlaw"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03145v1",
                "http://arxiv.org/pdf/2312.03145v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03141v1",
            "title": "In-Storage Acceleration of Graph-Traversal-Based Approximate Nearest\n  Neighbor Search",
            "updated": "2023-12-05T21:21:01Z",
            "published": "2023-12-05T21:21:01Z",
            "summary": "Approximate nearest neighbor search (ANNS) is a key retrieval technique for\nvector database and many data center applications, such as person\nre-identification and recommendation systems. Among all the ANNS algorithms,\ngraph-traversal-based ANNS achieves the highest recall rate. However, as the\nsize of dataset increases, the graph may require hundreds of gigabytes of\nmemory, exceeding the main memory capacity of a single workstation node.\nAlthough we can do partitioning and use solid-state drive (SSD) as the backing\nstorage, the limited SSD I/O bandwidth severely degrades the performance of the\nsystem. To address this challenge, we present NDSearch, a near-data processing\n(NDP) solution for ANNS processing. NDSearch consists of a novel in-storage\ncomputing architecture, namely, SEARSSD, that supports the ANNS kernels and\nleverages logic unit (LUN)-level parallelism inside the NAND flash chips.\nNDSearch also includes a processing model that is customized for NDP and\ncooperates with SEARSSD. The processing model enables us to apply a two-level\nscheduling to improve the data locality and exploit the internal bandwidth in\nNDSEARCH, and a speculative searching mechanism to further accelerate the ANNS\nworkload. Our results show that NDSearch improves the throughput by up to\n31.7x, 14.6x, 7.4x, 2.9x over CPU, GPU, a state-of-the-art SmartSSD-only\ndesign, and DeepStore, respectively. NDSEARCH also achieves two\norders-of-magnitude higher energy efficiency than CPU and GPU.",
            "author": [
                "Yitu Wang",
                "Shiyu Li",
                "Qilin Zheng",
                "Linghao Song",
                "Zongwang Li",
                "Andrew Chang",
                "Hai \"Helen\" Li",
                "Yiran Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03141v1",
                "http://arxiv.org/pdf/2312.03141v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03140v1",
            "title": "FlexModel: A Framework for Interpretability of Distributed Large\n  Language Models",
            "updated": "2023-12-05T21:19:33Z",
            "published": "2023-12-05T21:19:33Z",
            "summary": "With the growth of large language models, now incorporating billions of\nparameters, the hardware prerequisites for their training and deployment have\nseen a corresponding increase. Although existing tools facilitate model\nparallelization and distributed training, deeper model interactions, crucial\nfor interpretability and responsible AI techniques, still demand thorough\nknowledge of distributed computing. This often hinders contributions from\nresearchers with machine learning expertise but limited distributed computing\nbackground. Addressing this challenge, we present FlexModel, a software package\nproviding a streamlined interface for engaging with models distributed across\nmulti-GPU and multi-node configurations. The library is compatible with\nexisting model distribution libraries and encapsulates PyTorch models. It\nexposes user-registerable HookFunctions to facilitate straightforward\ninteraction with distributed model internals, bridging the gap between\ndistributed and single-device model paradigms. Primarily, FlexModel enhances\naccessibility by democratizing model interactions and promotes more inclusive\nresearch in the domain of large-scale neural networks. The package is found at\nhttps://github.com/VectorInstitute/flex_model.",
            "author": [
                "Matthew Choi",
                "Muhammad Adil Asif",
                "John Willes",
                "David Emerson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03140v1",
                "http://arxiv.org/pdf/2312.03140v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03139v1",
            "title": "A Bayesian Skew-heavy-tailed modelling for loss reserving",
            "updated": "2023-12-05T21:19:08Z",
            "published": "2023-12-05T21:19:08Z",
            "summary": "This paper focuses on modelling loss reserving to pay outstanding claims. As\nthe amount liable on any given claim is not known until settlement, we propose\na flexible model via heavy-tailed and skewed distributions to deal with\noutstanding liabilities. The inference relies on Markov chain Monte Carlo via\nGibbs sampler with adaptive Metropolis algorithm steps allowing for fast\ncomputations and providing efficient algorithms. An illustrative example\nemulates a typical dataset based on a runoff triangle and investigates the\nproperties of the proposed models. Also, a case study is considered and shows\nthat the proposed model outperforms the usual loss reserving models well\nestablished in the literature in the presence of skewness and heavy tails.",
            "author": [
                "William L. Le\u00e3o",
                "Viviana G. R. Lobo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03139v1",
                "http://arxiv.org/pdf/2312.03139v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "stat.AP",
                "stat.OT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03768v1",
            "title": "Algoritmo de Contagem Qu\u00e2ntico Aplicado ao Grafo Bipartido Completo",
            "updated": "2023-12-05T21:15:09Z",
            "published": "2023-12-05T21:15:09Z",
            "summary": "Studies on Quantum Computing have been developed since the 1980s, motivating\nresearches on quantum algorithms better than any classical algorithm possible.\nAn example of such algorithms is Grover's algorithm, capable of finding $k$\n(marked) elements in an unordered database with $N$ elements using\n$O(\\sqrt{N/k})$ steps. Grover's algorithm can be interpreted as a quantum walk\nin a complete graph (with loops) containing $N$ vertices from which $k$ are\nmarked. This interpretation motivated search algorithms in other graphs --\ncomplete bipartite graph, grid, and hypercube. Using Grover's algorithm's\nlinear operator, the quantum counting algorithm estimates the value of $k$ with\nan error of $O(\\sqrt{k})$ using $O(\\sqrt{N})$ steps. This work tackles the\nproblem of using the quantum counting algorithm for estimating the value $k$ of\nmarked elements in other graphs; more specifically, the complete bipartite\ngraph. It is concluded that for a particular case, running the proposed\nalgorithm at most $t$ times wields an estimation of $k$ with an error of\n$O(\\sqrt{k})$ using $O(t\\sqrt{N})$ steps and success probability of at least\n$(1 - 2^{-t})8/\\pi^2$.",
            "author": [
                "Gustavo Alves Bezerra"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03768v1",
                "http://arxiv.org/pdf/2312.03768v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03134v1",
            "title": "A Hardware Evaluation Framework for Large Language Model Inference",
            "updated": "2023-12-05T21:01:33Z",
            "published": "2023-12-05T21:01:33Z",
            "summary": "The past year has witnessed the increasing popularity of Large Language\nModels (LLMs). Their unprecedented scale and associated high hardware cost have\nimpeded their broader adoption, calling for efficient hardware designs. With\nthe large hardware needed to simply run LLM inference, evaluating different\nhardware designs becomes a new bottleneck.\n  This work introduces LLMCompass, a hardware evaluation framework for LLM\ninference workloads. LLMCompass is fast, accurate, versatile, and able to\ndescribe and evaluate different hardware designs. LLMCompass includes a mapper\nto automatically find performance-optimal mapping and scheduling. It also\nincorporates an area-based cost model to help architects reason about their\ndesign choices. Compared to real-world hardware, LLMCompass' estimated latency\nachieves an average 10.4% error rate across various operators with various\ninput sizes and an average 4.1% error rate for LLM inference. With LLMCompass,\nsimulating a 4-NVIDIA A100 GPU node running GPT-3 175B inference can be done\nwithin 16 minutes on commodity hardware, including 26,400 rounds of the\nmapper's parameter search.\n  With the aid of LLMCompass, this work draws architectural implications and\nexplores new cost-effective hardware designs. By reducing the compute\ncapability or replacing High Bandwidth Memory (HBM) with traditional DRAM,\nthese new designs can achieve as much as 3.41x improvement in performance/cost\ncompared to an NVIDIA A100, making them promising choices for democratizing\nLLMs.\n  LLMCompass is planned to be fully open-source.",
            "author": [
                "Hengrui Zhang",
                "August Ning",
                "Rohan Prabhakar",
                "David Wentzlaff"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03134v1",
                "http://arxiv.org/pdf/2312.03134v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR",
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03133v1",
            "title": "Predicting Bone Degradation Using Vision Transformer and Synthetic\n  Cellular Microstructures Dataset",
            "updated": "2023-12-05T21:00:08Z",
            "published": "2023-12-05T21:00:08Z",
            "summary": "Bone degradation, especially for astronauts in microgravity conditions, is\ncrucial for space exploration missions since the lower applied external forces\naccelerate the diminution in bone stiffness and strength substantially.\nAlthough existing computational models help us understand this phenomenon and\npossibly restrict its effect in the future, they are time-consuming to simulate\nthe changes in the bones, not just the bone microstructures, of each individual\nin detail. In this study, a robust yet fast computational method to predict and\nvisualize bone degradation has been developed. Our deep-learning method,\nTransVNet, can take in different 3D voxelized images and predict their\nevolution throughout months utilizing a hybrid 3D-CNN-VisionTransformer\nautoencoder architecture. Because of limited available experimental data and\nchallenges of obtaining new samples, a digital twin dataset of diverse and\ninitial bone-like microstructures was generated to train our TransVNet on the\nevolution of the 3D images through a previously developed degradation model for\nmicrogravity.",
            "author": [
                "Mohammad Saber Hashemi",
                "Azadeh Sheidaei"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03133v1",
                "http://arxiv.org/pdf/2312.03133v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "physics.med-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03131v1",
            "title": "Heterogeneous radio access with multiple latency targets",
            "updated": "2023-12-05T20:57:50Z",
            "published": "2023-12-05T20:57:50Z",
            "summary": "Since the advent of ultra-reliable and low-latency communications (URLLC),\nthe requirements of low-latency applications tend to be completely\ncharacterized by a single pre-defined latency-reliability target. That is,\noperation is optimal whenever the pre-defined latency threshold is met but the\nsystem is assumed to be in error when the latency threshold is violated. This\nvision is severely limited and does not capture the real requirements of most\napplications, where multiple latency thresholds can be defined, together with\nincentives or rewards associated with meeting each of them. Such formulation is\na generalization of the single-threshold case popularized by URLLC and, in the\nasymptotic case, approximates to defining a cost for each point in the support\nof the latency distribution. In this paper, we explore the implications of\ndefining multiple latency targets on the design of access protocols and on the\noptimization of repetition-based access strategies in orthogonal and\nnon-orthogonal multiple access scenarios with users that present heterogeneous\ntraffic characteristics and requirements. We observe that the access strategies\nof the users can be effectively adapted to the requirements of the application\nby carefully defining the latency targets and the associated rewards.",
            "author": [
                "Israel Leyva-Mayorga",
                "Jose Manuel Gimenez-Guzman",
                "Lorenzo Valentini",
                "Petar Popovski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03131v1",
                "http://arxiv.org/pdf/2312.03131v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.NI",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03125v1",
            "title": "A construction of Einstein solvmanifolds not based on nilsolitons",
            "updated": "2023-12-05T20:46:44Z",
            "published": "2023-12-05T20:46:44Z",
            "summary": "We construct indefinite Einstein solvmanifolds that are standard, but not of\npseudo-Iwasawa type. Thus, the underlying Lie algebras take the form\n$\\mathfrak{g}\\rtimes_D\\mathbb{R}$, where $\\mathfrak{g}$ is a nilpotent Lie\nalgebra and $D$ is a nonsymmetric derivation. Considering nonsymmetric\nderivations has the consequence that $\\mathfrak{g}$ is not a nilsoliton, but\nsatisfies a more general condition.\n  Our construction is based on the notion of nondiagonal triple on a nice\ndiagram. We present an algorithm to classify nondiagonal triples and the\nassociated Einstein metrics. With the use of a computer, we obtain all\nsolutions up to dimension $5$, and all solutions in dimension $\\leq8$ that\nsatisfy an additional technical restriction.\n  By comparing curvatures, we show that the Einstein solvmanifolds of dimension\n$\\leq 5$ that we obtain by our construction are not isometric to a standard\nextension of a nilsoliton.",
            "author": [
                "Diego Conti",
                "Federico A. Rossi",
                "Romeo Segnan Dalmasso"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03125v1",
                "http://arxiv.org/pdf/2312.03125v1"
            ],
            "primary_category": "math.DG",
            "category": [
                "math.DG",
                "53C25 (Primary), 53C30, 53C50, 22E25 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03122v1",
            "title": "Assertion Enhanced Few-Shot Learning: Instructive Technique for Large\n  Language Models to Generate Educational Explanations",
            "updated": "2023-12-05T20:41:34Z",
            "published": "2023-12-05T20:41:34Z",
            "summary": "Human educators possess an intrinsic ability to anticipate and seek\neducational explanations from students, which drives them to pose\nthought-provoking questions when students cannot articulate these explanations\nindependently. We aim to imbue Intelligent Tutoring Systems with this ability\nusing few-shot learning capability of Large Language Models. Our work proposes\na novel prompting technique, Assertion Enhanced Few-Shot Learning, to\nfacilitate the generation of accurate, detailed oriented educational\nexplanations. Our central hypothesis is that, in educational domain, few-shot\ndemonstrations are necessary but not a sufficient condition for quality\nexplanation generation. We conducted a study involving 12 in-service teachers,\ncomparing our approach to Traditional Few-Shot Learning. The results show that\nAssertion Enhanced Few-Shot Learning improves explanation accuracy by 15% and\nyields higher-quality explanations, as evaluated by teachers. We also conduct a\nqualitative ablation study to factor the impact of assertions to provide\neducator-friendly prompting guidelines for generating explanations in their\ndomain of interest.",
            "author": [
                "Tasmia Shahriar",
                "Noboru Matsuda",
                "Kelly Ramos"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03122v1",
                "http://arxiv.org/pdf/2312.03122v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03121v2",
            "title": "Evaluating Agents using Social Choice Theory",
            "updated": "2023-12-07T02:16:24Z",
            "published": "2023-12-05T20:40:37Z",
            "summary": "We argue that many general evaluation problems can be viewed through the lens\nof voting theory. Each task is interpreted as a separate voter, which requires\nonly ordinal rankings or pairwise comparisons of agents to produce an overall\nevaluation. By viewing the aggregator as a social welfare function, we are able\nto leverage centuries of research in social choice theory to derive principled\nevaluation frameworks with axiomatic foundations. These evaluations are\ninterpretable and flexible, while avoiding many of the problems currently\nfacing cross-task evaluation. We apply this Voting-as-Evaluation (VasE)\nframework across multiple settings, including reinforcement learning, large\nlanguage models, and humans. In practice, we observe that VasE can be more\nrobust than popular evaluation frameworks (Elo and Nash averaging), discovers\nproperties in the evaluation data not evident from scores alone, and can\npredict outcomes better than Elo in a complex seven-player game. We identify\none particular approach, maximal lotteries, that satisfies important\nconsistency properties relevant to evaluation, is computationally efficient\n(polynomial in the size of the evaluation data), and identifies game-theoretic\ncycles.",
            "author": [
                "Marc Lanctot",
                "Kate Larson",
                "Yoram Bachrach",
                "Luke Marris",
                "Zun Li",
                "Avishkar Bhoopchand",
                "Thomas Anthony",
                "Brian Tanner",
                "Anna Koop"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03121v2",
                "http://arxiv.org/pdf/2312.03121v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.GT",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03120v1",
            "title": "The Landscape of Modern Machine Learning: A Review of Machine,\n  Distributed and Federated Learning",
            "updated": "2023-12-05T20:40:05Z",
            "published": "2023-12-05T20:40:05Z",
            "summary": "With the advance of the powerful heterogeneous, parallel and distributed\ncomputing systems and ever increasing immense amount of data, machine learning\nhas become an indispensable part of cutting-edge technology, scientific\nresearch and consumer products. In this study, we present a review of modern\nmachine and deep learning. We provide a high-level overview for the latest\nadvanced machine learning algorithms, applications, and frameworks. Our\ndiscussion encompasses parallel distributed learning, deep learning as well as\nfederated learning. As a result, our work serves as an introductory text to the\nvast field of modern machine learning.",
            "author": [
                "Omer Subasi",
                "Oceane Bel",
                "Joseph Manzano",
                "Kevin Barker"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03120v1",
                "http://arxiv.org/pdf/2312.03120v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03119v1",
            "title": "AI-SAM: Automatic and Interactive Segment Anything Model",
            "updated": "2023-12-05T20:37:38Z",
            "published": "2023-12-05T20:37:38Z",
            "summary": "Semantic segmentation is a core task in computer vision. Existing methods are\ngenerally divided into two categories: automatic and interactive. Interactive\napproaches, exemplified by the Segment Anything Model (SAM), have shown promise\nas pre-trained models. However, current adaptation strategies for these models\ntend to lean towards either automatic or interactive approaches. Interactive\nmethods depend on prompts user input to operate, while automatic ones bypass\nthe interactive promptability entirely. Addressing these limitations, we\nintroduce a novel paradigm and its first model: the Automatic and Interactive\nSegment Anything Model (AI-SAM). In this paradigm, we conduct a comprehensive\nanalysis of prompt quality and introduce the pioneering Automatic and\nInteractive Prompter (AI-Prompter) that automatically generates initial point\nprompts while accepting additional user inputs. Our experimental results\ndemonstrate AI-SAM's effectiveness in the automatic setting, achieving\nstate-of-the-art performance. Significantly, it offers the flexibility to\nincorporate additional user prompts, thereby further enhancing its performance.\nThe project page is available at https://github.com/ymp5078/AI-SAM.",
            "author": [
                "Yimu Pan",
                "Sitao Zhang",
                "Alison D. Gernand",
                "Jeffery A. Goldstein",
                "James Z. Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03119v1",
                "http://arxiv.org/pdf/2312.03119v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03118v1",
            "title": "Longitudinal (curvature) couplings of an $N$-level qudit to a\n  superconducting resonator at the adiabatic limit and beyond",
            "updated": "2023-12-05T20:33:59Z",
            "published": "2023-12-05T20:33:59Z",
            "summary": "Understanding how and to what magnitude solid-state qubits couple to metallic\nwires is crucial to the design of quantum systems such as quantum computers.\nHere, we investigate the coupling between a multi-level system, or qudit, and a\nsuperconducting (SC) resonator's electromagnetic field, focusing on the\ninteraction involving both the transition and diagonal dipole moments of the\nqudit. Specifically, we explore the effective dynamical (time-dependent)\nlongitudinal coupling that arises when a solid-state qudit is adiabatically\nmodulated at small gate frequencies and amplitudes, in addition to a static\ndispersive interaction with the SC resonator. For the first time, we derive\nHamiltonians describing the longitudinal multi-level interactions in a general\ndispersive regime, encompassing both dynamical longitudinal and dispersive\ninteractions. These Hamiltonians smoothly transition between their adiabatic\nvalues, where the couplings of the n-th level are proportional to the level's\nenergy curvature concerning a qudit gate voltage, and the substantially larger\ndispersive values, which occur due to a resonant form factor. We provide\nseveral examples illustrating the transition from adiabatic to dispersive\ncoupling in different qubit systems, including the charge (1e DQD) qubit, the\ntransmon, the double quantum dot singlet-triplet qubit, and the triple quantum\ndot exchange-only qubit. In some of these qubits, higher energy levels play a\ncritical role, particularly when their qubit's dipole moment is minimal or\nzero. For an experimentally relevant scenario involving a spin-charge qubit\nwith magnetic field gradient coupled capacitively to a SC resonator, we\nshowcase the potential of these interactions. They enable\nclose-to-quantum-limited quantum non-demolition (QND) measurements and remote\ngeometric phase gates, demonstrating their practical utility in quantum\ninformation processing.",
            "author": [
                "Rusko Ruskov",
                "Charles Tahan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03118v1",
                "http://arxiv.org/pdf/2312.03118v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03113v1",
            "title": "GPU Graph Processing on CXL-Based Microsecond-Latency External Memory",
            "updated": "2023-12-05T20:17:38Z",
            "published": "2023-12-05T20:17:38Z",
            "summary": "In GPU graph analytics, the use of external memory such as the host DRAM and\nsolid-state drives is a cost-effective approach to processing large graphs\nbeyond the capacity of the GPU onboard memory. This paper studies the use of\nCompute Express Link (CXL) memory as alternative external memory for GPU graph\nprocessing in order to see if this emerging memory expansion technology enables\ngraph processing that is as fast as using the host DRAM. Through analysis and\nevaluation using FPGA prototypes, we show that representative GPU graph\ntraversal algorithms involving fine-grained random access can tolerate an\nexternal memory latency of up to a few microseconds introduced by the CXL\ninterface as well as by the underlying memory devices. This insight indicates\nthat microsecond-latency flash memory may be used as CXL memory devices to\nrealize even more cost-effective GPU graph processing while still achieving\nperformance close to using the host DRAM.",
            "author": [
                "Shintaro Sano",
                "Yosuke Bando",
                "Kazuhiro Hiwada",
                "Hirotsugu Kajihara",
                "Tomoya Suzuki",
                "Yu Nakanishi",
                "Daisuke Taki",
                "Akiyuki Kaneko",
                "Tatsuo Shiozawa"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3624062.3624173",
                "http://arxiv.org/abs/2312.03113v1",
                "http://arxiv.org/pdf/2312.03113v1"
            ],
            "primary_category": "cs.PF",
            "category": [
                "cs.PF"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03111v1",
            "title": "Parallel Proof-of-Work with DAG-Style Voting and Targeted Reward\n  Discounting",
            "updated": "2023-12-05T20:14:33Z",
            "published": "2023-12-05T20:14:33Z",
            "summary": "We present parallel proof-of-work with DAG-style voting, a novel\nproof-of-work cryptocurrency protocol that, compared to Bitcoin, provides\nbetter consistency guarantees, higher transaction throughput, lower transaction\nconfirmation latency, and higher resilience against incentive attacks. The\nsuperior consistency guarantees follow from implementing parallel\nproof-of-work, a recent consensus scheme that enforces a configurable number of\nproof-of-work votes per block. Our work is inspired by another recent protocol,\nTailstorm, which structures the individual votes as tree and mitigates\nincentive attacks by discounting the mining rewards proportionally to the depth\nof the tree. We propose to structure the votes as a directed acyclic graph\n(DAG) instead of a tree. This allows for a more targeted punishment of\noffending miners and, as we show through a reinforcement learning based attack\nsearch, makes the protocol even more resilient to incentive attacks. An\ninteresting by-product of our analysis is that parallel proof-of-work without\nreward discounting is less resilient to incentive attacks than Bitcoin in some\nrealistic network scenarios.",
            "author": [
                "Patrik Keller"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03111v1",
                "http://arxiv.org/pdf/2312.03111v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03110v1",
            "title": "The Automated Bias Triangle Feature Extraction Framework",
            "updated": "2023-12-05T20:12:31Z",
            "published": "2023-12-05T20:12:31Z",
            "summary": "Bias triangles represent features in stability diagrams of Quantum Dot (QD)\ndevices, whose occurrence and property analysis are crucial indicators for spin\nphysics. Nevertheless, challenges associated with quality and availability of\ndata as well as the subtlety of physical phenomena of interest have hindered an\nautomatic and bespoke analysis framework, often still relying (in part) on\nhuman labelling and verification. We introduce a feature extraction framework\nfor bias triangles, built from unsupervised, segmentation-based computer vision\nmethods, which facilitates the direct identification and quantification of\nphysical properties of the former. Thereby, the need for human input or large\ntraining datasets to inform supervised learning approaches is circumvented,\nwhile additionally enabling the automation of pixelwise shape and feature\nlabeling. In particular, we demonstrate that Pauli Spin Blockade (PSB)\ndetection can be conducted effectively, efficiently and without any training\ndata as a direct result of this approach.",
            "author": [
                "Madeleine Kotzagiannidis",
                "Jonas Schuff",
                "Nathan Korda"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03110v1",
                "http://arxiv.org/pdf/2312.03110v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "cs.CV",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03108v1",
            "title": "Absence of backscattering in Fermi-arc-mediated conductivity of\n  topological Dirac semimetal Cd$_{3}$As$_{2}$",
            "updated": "2023-12-05T20:10:19Z",
            "published": "2023-12-05T20:10:19Z",
            "summary": "Having previously been the subject of decades of semiconductor research,\ncadmium arsenide has now reemerged as a topological material, realizing ideal\nthree-dimensional Dirac points at the Fermi level. These topological Dirac\npoints lead to a number of extraordinary transport phenomena, including strong\nquantum oscillations, large magnetoresistance, ultrahigh mobilities, and Fermi\nvelocities exceeding graphene. The large mobilities persist even in thin films\nand nanowires of cadmium arsenide, suggesting the involvement of topological\nsurface states. However, computational studies of the surface states in this\nmaterial are lacking, in part due to the large 80-atom unit cell. Here we\npresent the computed Fermi arc surface states of a cadmium arsenide thin film,\nbased on a tight-binding model derived directly from the electronic structure.\nWe show that despite the close proximity of the Dirac points, the Fermi arcs\nare very long and straight, extending through nearly the entire Brillouin zone.\nThe shape and spin properties of the Fermi arcs suppress both back- and side-\nscattering at the surface, which we show by explicit integrals over the phase\nspace. The introduction of a small symmetry-breaking term, expected in a strong\nelectric field, gaps the electronic structure, creating a weak topological\ninsulator phase that exhibits similar transport properties. Crucially, the\nmechanisms suppressing scattering in this material differ from those in other\ntopological materials such as Weyl semimetals and topological insulators,\nsuggesting a new route for engineering high-mobility devices based on Dirac\nsemimetal surface states.",
            "author": [
                "Vsevolod Ivanov",
                "Lotte Borkowski",
                "Xiangang Wan",
                "Sergey Y. Savrasov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03108v1",
                "http://arxiv.org/pdf/2312.03108v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el",
                "cond-mat.mtrl-sci",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03767v1",
            "title": "Unknown Sample Discovery for Source Free Open Set Domain Adaptation",
            "updated": "2023-12-05T20:07:51Z",
            "published": "2023-12-05T20:07:51Z",
            "summary": "Open Set Domain Adaptation (OSDA) aims to adapt a model trained on a source\ndomain to a target domain that undergoes distribution shift and contains\nsamples from novel classes outside the source domain. Source-free OSDA\n(SF-OSDA) techniques eliminate the need to access source domain samples, but\ncurrent SF-OSDA methods utilize only the known classes in the target domain for\nadaptation, and require access to the entire target domain even during\ninference after adaptation, to make the distinction between known and unknown\nsamples. In this paper, we introduce Unknown Sample Discovery (USD) as an\nSF-OSDA method that utilizes a temporally ensembled teacher model to conduct\nknown-unknown target sample separation and adapts the student model to the\ntarget domain over all classes using co-training and temporal consistency\nbetween the teacher and the student. USD promotes Jensen-Shannon distance (JSD)\nas an effective measure for known-unknown sample separation. Our\nteacher-student framework significantly reduces error accumulation resulting\nfrom imperfect known-unknown sample separation, while curriculum guidance helps\nto reliably learn the distinction between target known and target unknown\nsubspaces. USD appends the target model with an unknown class node, thus\nreadily classifying a target sample into any of the known or unknown classes in\nsubsequent post-adaptation inference stages. Empirical results show that USD is\nsuperior to existing SF-OSDA methods and is competitive with current OSDA\nmodels that utilize both source and target domains during adaptation.",
            "author": [
                "Chowdhury Sadman Jahan",
                "Andreas Savakis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03767v1",
                "http://arxiv.org/pdf/2312.03767v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03766v1",
            "title": "Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment",
            "updated": "2023-12-05T20:07:34Z",
            "published": "2023-12-05T20:07:34Z",
            "summary": "While existing image-text alignment models reach high quality binary\nassessments, they fall short of pinpointing the exact source of misalignment.\nIn this paper, we present a method to provide detailed textual and visual\nexplanation of detected misalignments between text-image pairs. We leverage\nlarge language models and visual grounding models to automatically construct a\ntraining set that holds plausible misaligned captions for a given image and\ncorresponding textual explanations and visual indicators. We also publish a new\nhuman curated test set comprising ground-truth textual and visual misalignment\nannotations. Empirical results show that fine-tuning vision language models on\nour training set enables them to articulate misalignments and visually indicate\nthem within images, outperforming strong baselines both on the binary alignment\nclassification and the explanation generation tasks. Our method code and human\ncurated test set are available at: https://mismatch-quest.github.io/",
            "author": [
                "Brian Gordon",
                "Yonatan Bitton",
                "Yonatan Shafir",
                "Roopal Garg",
                "Xi Chen",
                "Dani Lischinski",
                "Daniel Cohen-Or",
                "Idan Szpektor"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03766v1",
                "http://arxiv.org/pdf/2312.03766v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03106v1",
            "title": "U(1) quantum spin liquids in dipolar-octupolar pyrochlore magnets: a\n  fermionic parton approach",
            "updated": "2023-12-05T19:56:31Z",
            "published": "2023-12-05T19:56:31Z",
            "summary": "We study the uniform $U(1)$ quantum spin liquid (QSL) with low-energy\nfermionic quasiparticles for pyrochlore magnets with dipolar-octupolar\nsymmetry, employing a fermionic parton mean field theory approach.\nSelf-consistent calculations stabilize 12 fully symmetric uniform $U(1)$ QSLs;\nof which four mean-field states are \"monopole-flux\" states. Several of these\nmean-field states show a linear temperature dependence of specific heat at low\ntemperatures; the other phases show a power law temperature dependence of\nspecific heat $C \\sim T^\\alpha$, where $\\alpha $ is close to 1. We further\ncompute the dynamic spin structure factors and discuss the possible signature\nof these fermionic spinons in neutron-scattering experiments on DO magnetic\nsystems. Our results provide a possible way to understand the metallic specific\nheat response in $Nd_2 Sc Nb O_7$.",
            "author": [
                "Krushna Chandra Sahu",
                "Sambuddha Sanyal"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03106v1",
                "http://arxiv.org/pdf/2312.03106v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03105v1",
            "title": "Improving Automated Algorithm Selection by Advancing Fitness Landscape\n  Analysis",
            "updated": "2023-12-05T19:53:25Z",
            "published": "2023-12-05T19:53:25Z",
            "summary": "Optimization is ubiquitous in our daily lives. In the past, (sub-)optimal\nsolutions to any problem have been derived by trial and error, sheer luck, or\nthe expertise of knowledgeable individuals. In our contemporary age, there\nthankfully exists a plethora of different algorithms that can find solutions\nmore reliably than ever before. Yet, choosing an appropriate algorithm for any\ngiven problem is challenging in itself. The field of automated algorithm\nselection provides various approaches to tackle this latest problem. This is\ndone by delegating the selection of a suitable algorithm for a given problem to\na complex computer model. This computer model is generated through the use of\nArtificial Intelligence. Many of these computer models rely on some sort of\ninformation about the problem to make a reasonable selection. Various methods\nexist to provide this informative input to the computer model in the form of\nnumerical data.\n  In this cumulative dissertation, I propose several improvements to the\ndifferent variants of informative inputs. This in turn enhances and refines the\ncurrent state-of-the-art of automated algorithm selection. Specifically, I\nidentify and address current issues with the existing body of work to\nstrengthen the foundation that future work builds upon. Furthermore, the rise\nof deep learning offers ample opportunities for automated algorithm selection.\nIn several joint works, my colleagues and I developed and evaluated several\ndifferent methods that replace the existing methods to extract an informative\ninput. Lastly, automated algorithm selection approaches have been restricted to\ncertain types of problems. I propose a method to extend the generation of\ninformative inputs to other problem types and provide an outlook on further\npromising research directions.",
            "author": [
                "Raphael Patrick Prager"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03105v1",
                "http://arxiv.org/pdf/2312.03105v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03102v1",
            "title": "Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI",
            "updated": "2023-12-05T19:45:44Z",
            "published": "2023-12-05T19:45:44Z",
            "summary": "In magnetic resonance imaging (MRI), slice-to-volume reconstruction (SVR)\nrefers to computational reconstruction of an unknown 3D magnetic resonance\nvolume from stacks of 2D slices corrupted by motion. While promising, current\nSVR methods require multiple slice stacks for accurate 3D reconstruction,\nleading to long scans and limiting their use in time-sensitive applications\nsuch as fetal fMRI. Here, we propose a SVR method that overcomes the\nshortcomings of previous work and produces state-of-the-art reconstructions in\nthe presence of extreme inter-slice motion. Inspired by the recent success of\nsingle-view depth estimation methods, we formulate SVR as a single-stack motion\nestimation task and train a fully convolutional network to predict a motion\nstack for a given slice stack, producing a 3D reconstruction as a byproduct of\nthe predicted motion. Extensive experiments on the SVR of adult and fetal\nbrains demonstrate that our fully convolutional method is twice as accurate as\nprevious SVR methods. Our code is available at github.com/seannz/svr.",
            "author": [
                "Sean I. Young",
                "Ya\u00ebl Balbastre",
                "Bruce Fischl",
                "Polina Golland",
                "Juan Eugenio Iglesias"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03102v1",
                "http://arxiv.org/pdf/2312.03102v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03097v1",
            "title": "State of Health Estimation for Battery Modules with Parallel-Connected\n  Cells Under Cell-to-Cell Variations",
            "updated": "2023-12-05T19:33:03Z",
            "published": "2023-12-05T19:33:03Z",
            "summary": "State of health (SOH) estimation for lithium-ion battery modules with cells\nconnected in parallel is a challenging problem, especially with cell-to-cell\nvariations. Incremental capacity analysis (ICA) and differential voltage\nanalysis (DVA) are effective at the cell level, but they cannot be directly\napplied to module-level SOH estimation, when only module-level measurements are\navailable. This paper proposes a new method and demonstrates that, with\nmultiple features systematically selected from the module-level ICA and DVA,\nthe module-level SOH can be estimated with high accuracy and confidence in the\npresence of cell-to-cell variations. First, a new information theory-based\nfeature selection algorithm is proposed to find an optimal set of features for\nmodule-level SOH estimation. Second, a new relevance vector regression\n(RVR)-based module-level SOH estimation model is proposed to provide both point\nestimates and three-sigma credible intervals while maintaining model sparsity.\nExperimental datasets are used to illustrate and evaluate the proposed method.\nWith more selected features incorporated, the proposed method achieves better\nestimation accuracy and higher confidence at the expense of higher model\ncomplexity. This trade-off is explored through a case study. When applied to a\nlarge experimental dataset, the proposed method and the resulting sparse model\nlead to module-level SOH estimates with 0.5% root-mean-square errors and 1.5%\naverage three-sigma values. With all the optimization and training processes\ncompleted offboard, the proposed method has low computational complexity for\nonboard implementations.",
            "author": [
                "Qinan Zhou",
                "Dyche Anderson",
                "Jing Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03097v1",
                "http://arxiv.org/pdf/2312.03097v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03096v1",
            "title": "Incidental Polysemanticity",
            "updated": "2023-12-05T19:29:54Z",
            "published": "2023-12-05T19:29:54Z",
            "summary": "Polysemantic neurons (neurons that activate for a set of unrelated features)\nhave been seen as a significant obstacle towards interpretability of\ntask-optimized deep networks, with implications for AI safety. The classic\norigin story of polysemanticity is that the data contains more \"features\" than\nneurons, such that learning to perform a task forces the network to co-allocate\nmultiple unrelated features to the same neuron, endangering our ability to\nunderstand the network's internal processing. In this work, we present a second\nand non-mutually exclusive origin story of polysemanticity. We show that\npolysemanticity can arise incidentally, even when there are ample neurons to\nrepresent all features in the data, using a combination of theory and\nexperiments. This second type of polysemanticity occurs because random\ninitialization can, by chance alone, initially assign multiple features to the\nsame neuron, and the training dynamics then strengthen such overlap. Due to its\norigin, we term this \\textit{incidental polysemanticity}.",
            "author": [
                "Victor Lecomte",
                "Kushal Thaman",
                "Trevor Chow",
                "Rylan Schaeffer",
                "Sanmi Koyejo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03096v1",
                "http://arxiv.org/pdf/2312.03096v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03095v1",
            "title": "Understanding Environmental Posts: Sentiment and Emotion Analysis of\n  Social Media Data",
            "updated": "2023-12-05T19:26:28Z",
            "published": "2023-12-05T19:26:28Z",
            "summary": "Social media is now the predominant source of information due to the\navailability of immediate public response. As a result, social media data has\nbecome a valuable resource for comprehending public sentiments. Studies have\nshown that it can amplify ideas and influence public sentiments. This study\nanalyzes the public perception of climate change and the environment over a\ndecade from 2014 to 2023. Using the Pointwise Mutual Information (PMI)\nalgorithm, we identify sentiment and explore prevailing emotions expressed\nwithin environmental tweets across various social media platforms, namely\nTwitter, Reddit, and YouTube. Accuracy on a human-annotated dataset was 0.65,\nhigher than Vader score but lower than that of an expert rater (0.90). Our\nfindings suggest that negative environmental tweets are far more common than\npositive or neutral ones. Climate change, air quality, emissions, plastic, and\nrecycling are the most discussed topics on all social media platforms,\nhighlighting its huge global concern. The most common emotions in environmental\ntweets are fear, trust, and anticipation, demonstrating public reactions wide\nand complex nature. By identifying patterns and trends in opinions related to\nthe environment, we hope to provide insights that can help raise awareness\nregarding environmental issues, inform the development of interventions, and\nadapt further actions to meet environmental challenges.",
            "author": [
                "Daniyar Amangeldi",
                "Aida Usmanova",
                "Pakizar Shamoi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03095v1",
                "http://arxiv.org/pdf/2312.03095v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03094v1",
            "title": "Magneto-optical response of the Weyl semimetal NbAs: Experimental\n  results and hyperbolic-band computations",
            "updated": "2023-12-05T19:25:46Z",
            "published": "2023-12-05T19:25:46Z",
            "summary": "The magneto-optical properties of (001)-oriented NbAs single crystals have\nbeen studied in the spectral range from 5 to 150 meV and in magnetic fields of\nup to 13 T. A rich spectrum of inter-Landau-level transitions is revealed by\nthese measurements. The transitions follow a square-root-like dependence with\nmagnetic field, but the simple linear-band approximation is unable to\naccurately reproduce the observed behavior of the transitions in applied\nfields. We argue that the detected magneto-optical spectra should be related to\ncrossing hyperbolic bands, which form the W1 cones. We propose a model\nHamiltonian, which describes coupled hyperbolic bands and reproduces the shape\nof the relevant bands in NbAs. The magneto-optical spectra computed from this\nHamiltonian nicely reproduce our observations. We conclude that the\nhyperbolic-band approach is a minimal model to adequately describe the\nmagneto-optical response of NbAs and that the chiral (conical) bands do not\nexplicitly manifest themselves in the spectra.",
            "author": [
                "S. Polatkan",
                "E. Uykur",
                "J. Wyzula",
                "M. Orlita",
                "C. Shekhar",
                "C. Felser",
                "M. Dressel",
                "A. V. Pronin"
            ],
            "link": [
                "http://dx.doi.org/10.1103/PhysRevB.108.L241201",
                "http://arxiv.org/abs/2312.03094v1",
                "http://arxiv.org/pdf/2312.03094v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03093v1",
            "title": "RESIN-EDITOR: A Schema-guided Hierarchical Event Graph Visualizer and\n  Editor",
            "updated": "2023-12-05T19:25:38Z",
            "published": "2023-12-05T19:25:38Z",
            "summary": "In this paper, we present RESIN-EDITOR, an interactive event graph visualizer\nand editor designed for analyzing complex events. Our RESIN-EDITOR system\nallows users to render and freely edit hierarchical event graphs extracted from\nmultimedia and multi-document news clusters with guidance from human-curated\nevent schemas. RESIN-EDITOR's unique features include hierarchical graph\nvisualization, comprehensive source tracing, and interactive user editing,\nwhich is more powerful and versatile than existing Information Extraction (IE)\nvisualization tools. In our evaluation of RESIN-EDITOR, we demonstrate ways in\nwhich our tool is effective in understanding complex events and enhancing\nsystem performance. The source code, a video demonstration, and a live website\nfor RESIN-EDITOR have been made publicly available.",
            "author": [
                "Khanh Duy Nguyen",
                "Zixuan Zhang",
                "Reece Suchocki",
                "Sha Li",
                "Martha Palmer",
                "Susan Brown",
                "Jiawei Han",
                "Heng Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03093v1",
                "http://arxiv.org/pdf/2312.03093v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03090v1",
            "title": "Critiquing Computing Artifacts through Programming Satirical Python\n  Scripts",
            "updated": "2023-12-05T19:17:25Z",
            "published": "2023-12-05T19:17:25Z",
            "summary": "Computing artifacts tend to exclude marginalized students, so we must create\nnew methods to critique and change them. We studied the potential for\n\"satirical programming\" to critique artifacts as part of culturally responsive\ncomputing (CRC) pedagogy. We conducted a one-hour session for three different\nBPC programs (N=51). We showed an example of a satirical Python script and\ntaught elements of Python to create a script. Our findings suggest this method\nis a promising CRC pedagogical approach: 50% of marginalized students worked\ntogether to create a satirical script, and 80% enjoyed translating their\n\"glitches\" into satirical Python scripts.",
            "author": [
                "Aadarsh Padiyath",
                "Tamara Nelson-Fromm",
                "Barbara Ericson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03090v1",
                "http://arxiv.org/pdf/2312.03090v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03763v1",
            "title": "Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and\n  Editing",
            "updated": "2023-12-05T19:05:58Z",
            "published": "2023-12-05T19:05:58Z",
            "summary": "We present a novel framework for generating photorealistic 3D human head and\nsubsequently manipulating and reposing them with remarkable flexibility. The\nproposed approach leverages an implicit function representation of 3D human\nheads, employing 3D Gaussians anchored on a parametric face model. To enhance\nrepresentational capabilities and encode spatial information, we embed a\nlightweight tri-plane payload within each Gaussian rather than directly storing\ncolor and opacity. Additionally, we parameterize the Gaussians in a 2D UV space\nvia a 3DMM, enabling effective utilization of the diffusion model for 3D head\navatar generation. Our method facilitates the creation of diverse and realistic\n3D human heads with fine-grained editing over facial features and expressions.\nExtensive experiments demonstrate the effectiveness of our method.",
            "author": [
                "Yushi Lan",
                "Feitong Tan",
                "Di Qiu",
                "Qiangeng Xu",
                "Kyle Genova",
                "Zeng Huang",
                "Sean Fanello",
                "Rohit Pandey",
                "Thomas Funkhouser",
                "Chen Change Loy",
                "Yinda Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03763v1",
                "http://arxiv.org/pdf/2312.03763v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03088v1",
            "title": "LLMs for Multi-Modal Knowledge Extraction and Analysis in\n  Intelligence/Safety-Critical Applications",
            "updated": "2023-12-05T19:04:50Z",
            "published": "2023-12-05T19:04:50Z",
            "summary": "Large Language Models have seen rapid progress in capability in recent years;\nthis progress has been accelerating and their capabilities, measured by various\nbenchmarks, are beginning to approach those of humans. There is a strong demand\nto use such models in a wide variety of applications but, due to unresolved\nvulnerabilities and limitations, great care needs to be used before applying\nthem to intelligence and safety-critical applications. This paper reviews\nrecent literature related to LLM assessment and vulnerabilities to synthesize\nthe current research landscape and to help understand what advances are most\ncritical to enable use of of these technologies in intelligence and\nsafety-critical applications. The vulnerabilities are broken down into ten\nhigh-level categories and overlaid onto a high-level life cycle of an LLM. Some\ngeneral categories of mitigations are reviewed.",
            "author": [
                "Brett Israelsen",
                "Soumalya Sarkar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03088v1",
                "http://arxiv.org/pdf/2312.03088v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03085v1",
            "title": "ScAR: Scaling Adversarial Robustness for LiDAR Object Detection",
            "updated": "2023-12-05T19:03:33Z",
            "published": "2023-12-05T19:03:33Z",
            "summary": "The adversarial robustness of a model is its ability to resist adversarial\nattacks in the form of small perturbations to input data. Universal adversarial\nattack methods such as Fast Sign Gradient Method (FSGM) and Projected Gradient\nDescend (PGD) are popular for LiDAR object detection, but they are often\ndeficient compared to task-specific adversarial attacks. Additionally, these\nuniversal methods typically require unrestricted access to the model's\ninformation, which is difficult to obtain in real-world applications. To\naddress these limitations, we present a black-box Scaling Adversarial\nRobustness (ScAR) method for LiDAR object detection. By analyzing the\nstatistical characteristics of 3D object detection datasets such as KITTI,\nWaymo, and nuScenes, we have found that the model's prediction is sensitive to\nscaling of 3D instances. We propose three black-box scaling adversarial attack\nmethods based on the available information: model-aware attack,\ndistribution-aware attack, and blind attack. We also introduce a strategy for\ngenerating scaling adversarial examples to improve the model's robustness\nagainst these three scaling adversarial attacks. Comparison with other methods\non public datasets under different 3D object detection architectures\ndemonstrates the effectiveness of our proposed method.",
            "author": [
                "Xiaohu Lu",
                "Hayder Radha"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03085v1",
                "http://arxiv.org/pdf/2312.03085v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03083v1",
            "title": "Dual-VQE: A quantum algorithm to lower bound the ground-state energy",
            "updated": "2023-12-05T19:02:19Z",
            "published": "2023-12-05T19:02:19Z",
            "summary": "The variational quantum eigensolver (VQE) is a hybrid quantum--classical\nvariational algorithm that produces an upper-bound estimate of the ground-state\nenergy of a Hamiltonian. As quantum computers become more powerful and go\nbeyond the reach of classical brute-force simulation, it is important to assess\nthe quality of solutions produced by them. Here we propose a dual variational\nquantum eigensolver (dual-VQE) that produces a lower-bound estimate of the\nground-state energy. As such, VQE and dual-VQE can serve as quality checks on\ntheir solutions; in the ideal case, the VQE upper bound and the dual-VQE lower\nbound form an interval containing the true optimal value of the ground-state\nenergy. The idea behind dual-VQE is to employ semi-definite programming duality\nto rewrite the ground-state optimization problem as a constrained maximization\nproblem, which itself can be bounded from below by an unconstrained\noptimization problem to be solved by a variational quantum algorithm. When\nusing a convex combination ansatz in conjunction with a classical generative\nmodel, the quantum computational resources needed to evaluate the objective\nfunction of dual-VQE are no greater than those needed for that of VQE. We\nsimulated the performance of dual-VQE on the transverse-field Ising model, and\nfound that, for the example considered, while dual-VQE training is slower and\nnoisier than VQE, it approaches the true value with error of order $10^{-2}$.",
            "author": [
                "Hanna Westerheim",
                "Jingxuan Chen",
                "Zo\u00eb Holmes",
                "Ivy Luo",
                "Theshani Nuradha",
                "Dhrumil Patel",
                "Soorya Rethinasamy",
                "Kathie Wang",
                "Mark M. Wilde"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03083v1",
                "http://arxiv.org/pdf/2312.03083v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03080v1",
            "title": "Scale-invariant magnetic anisotropy in $\u03b1$-RuCl$_3$: A quantum\n  Monte Carlo study",
            "updated": "2023-12-05T19:00:51Z",
            "published": "2023-12-05T19:00:51Z",
            "summary": "We compute the rotational anisotropy of the free energy of $\\alpha$-RuCl$_3$\nin an external magnetic field. This quantity, known as the magnetotropic\nsusceptibility, $k$, relates to the second derivative of the free energy with\nrespect to the angle of rotation. We have used approximation-free,\nauxiliary-field quantum Monte Carlo simulations for a realistic model of\n$\\alpha$-RuCl$_3$ and optimized the path integral to alleviate the negative\nsign problem. This allows us to reach temperatures down to $30~\\rm{K}$ -- an\nenergy scale below the dominant Kitaev coupling. We demonstrate that the\nmagnetotropic susceptibility in this model of $\\alpha$-RuCl$_3$ displays unique\nscaling, $k = Tf(B/T)$, with distinct scaling functions $f$ at high and low\ntemperatures. In comparison, for the XXZ Heisenberg model, the scaling $k =\nTf(B/T)$ breaks down at a temperature scale where the uniform spin\nsusceptibility deviates from the Curie law (i.e. at the energy scale of the\nexchange interactions) and never recovers at low temperatures. Our findings\nsuggest that correlations in $\\alpha$-RuCl$_3$ lead to degrees of freedom that\nrespond isotropically to a magnetic field. One possible interpretation for the\napparent scale-invariance observed in experiments could be fractionalization of\nthe spin degrees of freedom in the extended Kitaev model.",
            "author": [
                "Toshihiro Sato",
                "B. J. Ramshaw",
                "K. A. Modic",
                "Fakher F. Assaad"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03080v1",
                "http://arxiv.org/pdf/2312.03080v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03079v1",
            "title": "LooseControl: Lifting ControlNet for Generalized Depth Conditioning",
            "updated": "2023-12-05T19:00:20Z",
            "published": "2023-12-05T19:00:20Z",
            "summary": "We present LooseControl to allow generalized depth conditioning for\ndiffusion-based image generation. ControlNet, the SOTA for depth-conditioned\nimage generation, produces remarkable results but relies on having access to\ndetailed depth maps for guidance. Creating such exact depth maps, in many\nscenarios, is challenging. This paper introduces a generalized version of depth\nconditioning that enables many new content-creation workflows. Specifically, we\nallow (C1) scene boundary control for loosely specifying scenes with only\nboundary conditions, and (C2) 3D box control for specifying layout locations of\nthe target objects rather than the exact shape and appearance of the objects.\nUsing LooseControl, along with text guidance, users can create complex\nenvironments (e.g., rooms, street views, etc.) by specifying only scene\nboundaries and locations of primary objects. Further, we provide two editing\nmechanisms to refine the results: (E1) 3D box editing enables the user to\nrefine images by changing, adding, or removing boxes while freezing the style\nof the image. This yields minimal changes apart from changes induced by the\nedited boxes. (E2) Attribute editing proposes possible editing directions to\nchange one particular aspect of the scene, such as the overall object density\nor a particular object. Extensive tests and comparisons with baselines\ndemonstrate the generality of our method. We believe that LooseControl can\nbecome an important design tool for easily creating complex environments and be\nextended to other forms of guidance channels. Code and more information are\navailable at https://shariqfarooq123.github.io/loose-control/ .",
            "author": [
                "Shariq Farooq Bhat",
                "Niloy J. Mitra",
                "Peter Wonka"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03079v1",
                "http://arxiv.org/pdf/2312.03079v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03078v2",
            "title": "Islands Far Outside the Horizon",
            "updated": "2023-12-07T03:08:34Z",
            "published": "2023-12-05T19:00:19Z",
            "summary": "Information located in an entanglement island in semiclassical gravity can be\nnonperturbatively reconstructed from distant radiation, implying a radical\nbreakdown of effective field theory. We show that this occurs well outside of\nthe black hole stretched horizon. We compute the island associated to\nlarge-angular momentum Hawking modes of a four-dimensional Schwarzschild black\nhole. These modes typically fall back into the black hole but can be extracted\nto infinity by relativistic strings or, more abstractly, by asymptotic boundary\noperators constructed using the timelike tube theorem. Remarkably, we find that\ntheir island can protrude a distance of order $\\sqrt{\\ell_p r_{\\rm hor}}$\noutside the horizon. This is parametrically larger than the Planck scale\n$\\ell_p$ and is comparable to the Bohr radius for supermassive black holes.\nTherefore, in principle, a distant observer can determine experimentally\nwhether the black hole information paradox is resolved by complementarity, or\nby a firewall.",
            "author": [
                "Raphael Bousso",
                "Geoff Penington"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03078v2",
                "http://arxiv.org/pdf/2312.03078v2"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "gr-qc",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03077v1",
            "title": "Clinical Notes Reveal Physician Fatigue",
            "updated": "2023-12-05T19:00:18Z",
            "published": "2023-12-05T19:00:18Z",
            "summary": "Physicians write notes about patients. In doing so, they reveal much about\nthemselves. Using data from 129,228 emergency room visits, we train a model to\nidentify notes written by fatigued physicians -- those who worked 5 or more of\nthe prior 7 days. In a hold-out set, the model accurately identifies notes\nwritten by these high-workload physicians, and also flags notes written in\nother high-fatigue settings: on overnight shifts, and after high patient\nvolumes. Model predictions also correlate with worse decision-making on at\nleast one important metric: yield of testing for heart attack is 18% lower with\neach standard deviation increase in model-predicted fatigue. Finally, the model\nindicates that notes written about Black and Hispanic patients have 12% and 21%\nhigher predicted fatigue than Whites -- larger than overnight vs. daytime\ndifferences. These results have an important implication for large language\nmodels (LLMs). Our model indicates that fatigued doctors write more predictable\nnotes. Perhaps unsurprisingly, because word prediction is the core of how LLMs\nwork, we find that LLM-written notes have 17% higher predicted fatigue than\nreal physicians' notes. This indicates that LLMs may introduce distortions in\ngenerated text that are not yet fully understood.",
            "author": [
                "Chao-Chun Hsu",
                "Ziad Obermeyer",
                "Chenhao Tan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03077v1",
                "http://arxiv.org/pdf/2312.03077v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03076v1",
            "title": "XOR Lemmas for Communication via Marginal Information",
            "updated": "2023-12-05T19:00:16Z",
            "published": "2023-12-05T19:00:16Z",
            "summary": "We define the $\\textit{marginal information}$ of a communication protocol,\nand use it to prove XOR lemmas for communication complexity. We show that if\nevery $C$-bit protocol has bounded advantage for computing a Boolean function\n$f$, then every $\\tilde \\Omega(C \\sqrt{n})$-bit protocol has advantage\n$\\exp(-\\Omega(n))$ for computing the $n$-fold xor $f^{\\oplus n}$. We prove\nexponentially small bounds in the average case setting, and near optimal bounds\nfor product distributions and for bounded-round protocols.",
            "author": [
                "Siddharth Iyer",
                "Anup Rao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03076v1",
                "http://arxiv.org/pdf/2312.03076v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03063v1",
            "title": "Real-time correlators in 3+1D thermal lattice gauge theory",
            "updated": "2023-12-05T19:00:02Z",
            "published": "2023-12-05T19:00:02Z",
            "summary": "Real-time quantities like spectral functions and transport coefficients are\ncrucial for a proper understanding of the quark-gluon plasma created in\nrelativistic heavy-ion collisions. Their numerical computation is plagued by a\nsevere sign problem inherent in the real-time formulation of lattice field\ntheories. In this letter, we present the first direct ab-initio computation of\nunequal-time correlation functions in non-Abelian lattice gauge theory, which\nare necessary to extract real-time quantities. We demonstrate non-trivial\nconsistency relations among correlators, time-translation invariance, and\nagreement with Monte-Carlo results for thermal equilibrium in 3+1 dimensions by\nemploying our stabilized complex Langevin method. Our work sets the stage to\nextract real-time observables in lattice gauge theory in a first-principles\nreal-time framework.",
            "author": [
                "Kirill Boguslavski",
                "Paul Hotzy",
                "David I. M\u00fcller"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03063v1",
                "http://arxiv.org/pdf/2312.03063v1"
            ],
            "primary_category": "hep-lat",
            "category": [
                "hep-lat",
                "cond-mat.other",
                "hep-ph",
                "hep-th",
                "nucl-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03059v1",
            "title": "A Foray on SCFT$_3$ via Super Spinor-Helicity and Grassmann Twistor\n  Variables",
            "updated": "2023-12-05T19:00:01Z",
            "published": "2023-12-05T19:00:01Z",
            "summary": "In this paper, we develop a momentum super space formalism for\n$\\mathcal{N}=1,2$ superconformal field theories in three dimensions. First, we\nsolve for super-correlators in the usual momentum superspace variables.\nHowever, we found that expressing quantities in super space spinor helicity\nvariables greatly simplifies the analysis. Further, by performing a \"half\"\nFourier transform of the Grassmann coordinates which is analogous to the\nTwistor transform, an even more remarkable simplification occurs. Using these\nformalism, we first compute all three point correlation functions involving\nconserved super-currents with arbitrary spins in $\\mathcal{N}=1,2$ theories. We\ndiscover interesting double copy relations in $\\mathcal{N}=1$\nsuper-correlators. Also, we discovered super double copy relations that take us\nfrom $\\mathcal{N}=1$ to $\\mathcal{N}=2$ super-correlators. We also comment on\nthe connection of our results with the flat space super amplitudes in one\nhigher dimension.",
            "author": [
                "Sachin Jain",
                "Dhruva K. S",
                "Deep Mazumdar",
                "Shivang Yadav"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03059v1",
                "http://arxiv.org/pdf/2312.03059v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03061v1",
            "title": "Resonant Spin-Flavor Precession of Sterile Neutrinos",
            "updated": "2023-12-05T19:00:01Z",
            "published": "2023-12-05T19:00:01Z",
            "summary": "We analyze the impact of resonant conversions mediated by non-vanishing\nmagnetic moments between active neutrinos and a heavy sterile neutrino on the\nsupernova neutrino flux. We present the level-crossing scheme for such a\nscenario and derive the neutrino fluxes after conversion, paying special\nattention to the order in which the resonances occur. We then compute the\nexpected event rates from the neutronization burst of a future supernova at\nDUNE and Hyper-Kamiokande to derive new constraints on the neutrino magnetic\nmoment. With this, we find a sensitivity down to a few $10^{-15} \\mu_B$ for a\nsterile neutrino in the $O(\\rm{eV})$ mass range.",
            "author": [
                "Edward Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03061v1",
                "http://arxiv.org/pdf/2312.03061v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "astro-ph.HE",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03057v1",
            "title": "Advantage of Quantum Machine Learning from General Computational\n  Advantages",
            "updated": "2023-12-05T19:00:00Z",
            "published": "2023-12-05T19:00:00Z",
            "summary": "An overarching milestone of quantum machine learning (QML) is to demonstrate\nthe advantage of QML over all possible classical learning methods in\naccelerating a common type of learning task as represented by supervised\nlearning with classical data. However, the provable advantages of QML in\nsupervised learning have been known so far only for the learning tasks designed\nfor using the advantage of specific quantum algorithms, i.e., Shor's\nalgorithms. Here we explicitly construct an unprecedentedly broader family of\nsupervised learning tasks with classical data to offer the provable advantage\nof QML based on general quantum computational advantages, progressing beyond\nShor's algorithms. Our learning task is feasibly achievable by executing a\ngeneral class of functions that can be computed efficiently in polynomial time\nfor a large fraction of inputs by arbitrary quantum algorithms but not by any\nclassical algorithm. We prove the hardness of achieving this learning task for\nany possible polynomial-time classical learning method. We also clarify\nprotocols for preparing the classical data to demonstrate this learning task in\nexperiments. These results open routes to exploit a variety of quantum\nadvantages in computing functions for the experimental demonstration of the\nadvantage of QML.",
            "author": [
                "Hayata Yamasaki",
                "Natsuto Isogai",
                "Mio Murao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03057v1",
                "http://arxiv.org/pdf/2312.03057v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02981v1",
            "title": "ReconFusion: 3D Reconstruction with Diffusion Priors",
            "updated": "2023-12-05T18:59:58Z",
            "published": "2023-12-05T18:59:58Z",
            "summary": "3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at\nrendering photorealistic novel views of complex scenes. However, recovering a\nhigh-quality NeRF typically requires tens to hundreds of input images,\nresulting in a time-consuming capture process. We present ReconFusion to\nreconstruct real-world scenes using only a few photos. Our approach leverages a\ndiffusion prior for novel view synthesis, trained on synthetic and multiview\ndatasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel\ncamera poses beyond those captured by the set of input images. Our method\nsynthesizes realistic geometry and texture in underconstrained regions while\npreserving the appearance of observed regions. We perform an extensive\nevaluation across various real-world datasets, including forward-facing and\n360-degree scenes, demonstrating significant performance improvements over\nprevious few-view NeRF reconstruction approaches.",
            "author": [
                "Rundi Wu",
                "Ben Mildenhall",
                "Philipp Henzler",
                "Keunhong Park",
                "Ruiqi Gao",
                "Daniel Watson",
                "Pratul P. Srinivasan",
                "Dor Verbin",
                "Jonathan T. Barron",
                "Ben Poole",
                "Aleksander Holynski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02981v1",
                "http://arxiv.org/pdf/2312.02981v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02980v1",
            "title": "GPT4Point: A Unified Framework for Point-Language Understanding and\n  Generation",
            "updated": "2023-12-05T18:59:55Z",
            "published": "2023-12-05T18:59:55Z",
            "summary": "Multimodal Large Language Models (MLLMs) have excelled in 2D image-text\ncomprehension and image generation, but their understanding of the 3D world is\nnotably deficient, limiting progress in 3D language understanding and\ngeneration. To solve this problem, we introduce GPT4Point, an innovative\ngroundbreaking point-language multimodal model designed specifically for\nunified 3D object understanding and generation within the MLLM framework.\nGPT4Point as a powerful 3D MLLM seamlessly can execute a variety of point-text\nreference tasks such as point-cloud captioning and Q&A. Additionally, GPT4Point\nis equipped with advanced capabilities for controllable 3D generation, it can\nget high-quality results through a low-quality point-text feature maintaining\nthe geometric shapes and colors. To support the expansive needs of 3D\nobject-text pairs, we develop Pyramid-XL, a point-language dataset annotation\nengine. It constructs a large-scale database over 1M objects of varied text\ngranularity levels from the Objaverse-XL dataset, essential for training\nGPT4Point. A comprehensive benchmark has been proposed to evaluate 3D\npoint-language understanding capabilities. In extensive evaluations, GPT4Point\nhas demonstrated superior performance in understanding and generation.",
            "author": [
                "Zhangyang Qi",
                "Ye Fang",
                "Zeyi Sun",
                "Xiaoyang Wu",
                "Tong Wu",
                "Jiaqi Wang",
                "Dahua Lin",
                "Hengshuang Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02980v1",
                "http://arxiv.org/pdf/2312.02980v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02979v1",
            "title": "Floquet Chiral Quantum Walk in Quantum Computer",
            "updated": "2023-12-05T18:59:54Z",
            "published": "2023-12-05T18:59:54Z",
            "summary": "Chiral edge states in quantum Hall effect are the paradigmatic example of the\nquasi-particle with chirality. In even space-time dimensions, the\nNielsen-Ninomiya theorem strictly forbids the chiral states in physical\nisolation. The exceptions to this theorem only occur in the presence of\nnon-locality, non-Hermiticity, or by embedding the system at the boundary of\nthe higher-dimensional bulk. In this work, using the IBM quantum computer\nplatform, we realize the floquet chiral quantum walk enabled by non-locality.\nThe unitary time evolution operator is described by the effective floquet\nHamiltonian with infinitely long-ranged coupling. We find that the chiral wave\npackets lack the common features of the conventional wave phenomena such as\nAnderson localization. The absence of localization is witnessed by the\nrobustness against the external perturbations. However, the intrinsic quantum\nerrors of the current quantum device give rise to the finite lifetime where the\nchiral wave packet eventually disperses in the long-time limit. Nevertheless,\nwe observe the stability of the chiral wave by comparing it with the\nconventional non-chiral model.",
            "author": [
                "Chan Bin Bark",
                "Youngseok Kim",
                "Moon Jip Park"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02979v1",
                "http://arxiv.org/pdf/2312.02979v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.mes-hall",
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02976v1",
            "title": "Imitating Shortest Paths in Simulation Enables Effective Navigation and\n  Manipulation in the Real World",
            "updated": "2023-12-05T18:59:45Z",
            "published": "2023-12-05T18:59:45Z",
            "summary": "Reinforcement learning (RL) with dense rewards and imitation learning (IL)\nwith human-generated trajectories are the most widely used approaches for\ntraining modern embodied agents. RL requires extensive reward shaping and\nauxiliary losses and is often too slow and ineffective for long-horizon tasks.\nWhile IL with human supervision is effective, collecting human trajectories at\nscale is extremely expensive. In this work, we show that imitating\nshortest-path planners in simulation produces agents that, given a language\ninstruction, can proficiently navigate, explore, and manipulate objects in both\nsimulation and in the real world using only RGB sensors (no depth map or GPS\ncoordinates). This surprising result is enabled by our end-to-end,\ntransformer-based, SPOC architecture, powerful visual encoders paired with\nextensive image augmentation, and the dramatic scale and diversity of our\ntraining data: millions of frames of shortest-path-expert trajectories\ncollected inside approximately 200,000 procedurally generated houses containing\n40,000 unique 3D assets. Our models, data, training code, and newly proposed\n10-task benchmarking suite CHORES will be open-sourced.",
            "author": [
                "Kiana Ehsani",
                "Tanmay Gupta",
                "Rose Hendrix",
                "Jordi Salvador",
                "Luca Weihs",
                "Kuo-Hao Zeng",
                "Kunal Pratap Singh",
                "Yejin Kim",
                "Winson Han",
                "Alvaro Herrasti",
                "Ranjay Krishna",
                "Dustin Schwenk",
                "Eli VanderBilt",
                "Aniruddha Kembhavi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02976v1",
                "http://arxiv.org/pdf/2312.02976v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03053v1",
            "title": "DiffusionPCR: Diffusion Models for Robust Multi-Step Point Cloud\n  Registration",
            "updated": "2023-12-05T18:59:41Z",
            "published": "2023-12-05T18:59:41Z",
            "summary": "Point Cloud Registration (PCR) estimates the relative rigid transformation\nbetween two point clouds. We propose formulating PCR as a denoising diffusion\nprobabilistic process, mapping noisy transformations to the ground truth.\nHowever, using diffusion models for PCR has nontrivial challenges, such as\nadapting a generative model to a discriminative task and leveraging the\nestimated nonlinear transformation from the previous step. Instead of training\na diffusion model to directly map pure noise to ground truth, we map the\npredictions of an off-the-shelf PCR model to ground truth. The predictions of\noff-the-shelf models are often imperfect, especially in challenging cases where\nthe two points clouds have low overlap, and thus could be seen as noisy\nversions of the real rigid transformation. In addition, we transform the\nrotation matrix into a spherical linear space for interpolation between samples\nin the forward process, and convert rigid transformations into auxiliary\ninformation to implicitly exploit last-step estimations in the reverse process.\nAs a result, conditioned on time step, the denoising model adapts to the\nincreasing accuracy across steps and refines registrations. Our extensive\nexperiments showcase the effectiveness of our DiffusionPCR, yielding\nstate-of-the-art registration recall rates (95.3%/81.6%) on 3DMatch and\n3DLoMatch. The code will be made public upon publication.",
            "author": [
                "Zhi Chen",
                "Yufan Ren",
                "Tong Zhang",
                "Zheng Dang",
                "Wenbing Tao",
                "Sabine S\u00fcsstrunk",
                "Mathieu Salzmann"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03053v1",
                "http://arxiv.org/pdf/2312.03053v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02975v1",
            "title": "Dexterous Functional Grasping",
            "updated": "2023-12-05T18:59:23Z",
            "published": "2023-12-05T18:59:23Z",
            "summary": "While there have been significant strides in dexterous manipulation, most of\nit is limited to benchmark tasks like in-hand reorientation which are of\nlimited utility in the real world. The main benefit of dexterous hands over\ntwo-fingered ones is their ability to pickup tools and other objects (including\nthin ones) and grasp them firmly to apply force. However, this task requires\nboth a complex understanding of functional affordances as well as precise\nlow-level control. While prior work obtains affordances from human data this\napproach doesn't scale to low-level control. Similarly, simulation training\ncannot give the robot an understanding of real-world semantics. In this paper,\nwe aim to combine the best of both worlds to accomplish functional grasping for\nin-the-wild objects. We use a modular approach. First, affordances are obtained\nby matching corresponding regions of different objects and then a low-level\npolicy trained in sim is run to grasp it. We propose a novel application of\neigengrasps to reduce the search space of RL using a small amount of human data\nand find that it leads to more stable and physically realistic motion. We find\nthat eigengrasp action space beats baselines in simulation and outperforms\nhardcoded grasping in real and matches or outperforms a trained human\nteleoperator. Results visualizations and videos at https://dexfunc.github.io/",
            "author": [
                "Ananye Agarwal",
                "Shagun Uppal",
                "Kenneth Shaw",
                "Deepak Pathak"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02975v1",
                "http://arxiv.org/pdf/2312.02975v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02974v1",
            "title": "Describing Differences in Image Sets with Natural Language",
            "updated": "2023-12-05T18:59:16Z",
            "published": "2023-12-05T18:59:16Z",
            "summary": "How do two sets of images differ? Discerning set-level differences is crucial\nfor understanding model behaviors and analyzing datasets, yet manually sifting\nthrough thousands of images is impractical. To aid in this discovery process,\nwe explore the task of automatically describing the differences between two\n$\\textbf{sets}$ of images, which we term Set Difference Captioning. This task\ntakes in image sets $D_A$ and $D_B$, and outputs a description that is more\noften true on $D_A$ than $D_B$. We outline a two-stage approach that first\nproposes candidate difference descriptions from image sets and then re-ranks\nthe candidates by checking how well they can differentiate the two sets. We\nintroduce VisDiff, which first captions the images and prompts a language model\nto propose candidate descriptions, then re-ranks these descriptions using CLIP.\nTo evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired image\nsets with ground truth difference descriptions. We apply VisDiff to various\ndomains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparing\nclassification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizing\nmodel failure modes (supervised ResNet), characterizing differences between\ngenerative models (e.g., StableDiffusionV1 and V2), and discovering what makes\nimages memorable. Using VisDiff, we are able to find interesting and previously\nunknown differences in datasets and models, demonstrating its utility in\nrevealing nuanced insights.",
            "author": [
                "Lisa Dunlap",
                "Yuhui Zhang",
                "Xiaohan Wang",
                "Ruiqi Zhong",
                "Trevor Darrell",
                "Jacob Steinhardt",
                "Joseph E. Gonzalez",
                "Serena Yeung-Levy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02974v1",
                "http://arxiv.org/pdf/2312.02974v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02973v1",
            "title": "GauHuman: Articulated Gaussian Splatting from Monocular Human Videos",
            "updated": "2023-12-05T18:59:14Z",
            "published": "2023-12-05T18:59:14Z",
            "summary": "We present, GauHuman, a 3D human model with Gaussian Splatting for both fast\ntraining (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with\nexisting NeRF-based implicit representation modelling frameworks demanding\nhours of training and seconds of rendering per frame. Specifically, GauHuman\nencodes Gaussian Splatting in the canonical space and transforms 3D Gaussians\nfrom canonical space to posed space with linear blend skinning (LBS), in which\neffective pose and LBS refinement modules are designed to learn fine details of\n3D humans under negligible computational cost. Moreover, to enable fast\noptimization of GauHuman, we initialize and prune 3D Gaussians with 3D human\nprior, while splitting/cloning via KL divergence guidance, along with a novel\nmerge operation for further speeding up. Extensive experiments on ZJU_Mocap and\nMonoCap datasets demonstrate that GauHuman achieves state-of-the-art\nperformance quantitatively and qualitatively with fast training and real-time\nrendering speed. Notably, without sacrificing rendering quality, GauHuman can\nfast model the 3D human performer with ~13k 3D Gaussians.",
            "author": [
                "Shoukang Hu",
                "Ziwei Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02973v1",
                "http://arxiv.org/pdf/2312.02973v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02972v1",
            "title": "PROSPECT: A profile likelihood code for frequentist cosmological\n  parameter inference",
            "updated": "2023-12-05T18:58:52Z",
            "published": "2023-12-05T18:58:52Z",
            "summary": "Cosmological parameter inference has been dominated by the Bayesian approach\nfor the past two decades, primarily due to its computational efficiency.\nHowever, the Bayesian approach involves integration of the posterior\nprobability and therefore depends on both the choice of model parametrisation\nand the choice of prior on the model parameter space. In some cases, this can\nlead to conclusions which are driven by choice of parametrisation and priors\nrather than by data. The profile likelihood method provides a complementary\nfrequentist tool which can be used to investigate this effect.\n  In this paper, we present the code PROSPECT for computing profile likelihoods\nin cosmology. We showcase the code using a phenomenological model for\nconverting dark matter into dark radiation that suffers from large volume\neffects and prior dependence. PROSPECT is compatible with both cobaya and\nMontePython, and is publicly available at\nhttps://github.com/AarhusCosmology/prospect_public.",
            "author": [
                "Emil Brinch Holm",
                "Andreas Nygaard",
                "Jeppe Dakin",
                "Steen Hannestad",
                "Thomas Tram"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02972v1",
                "http://arxiv.org/pdf/2312.02972v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "astro-ph.IM",
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03052v1",
            "title": "Visual Program Distillation: Distilling Tools and Programmatic Reasoning\n  into Vision-Language Models",
            "updated": "2023-12-05T18:58:37Z",
            "published": "2023-12-05T18:58:37Z",
            "summary": "Solving complex visual tasks such as \"Who invented the musical instrument on\nthe right?\" involves a composition of skills: understanding space, recognizing\ninstruments, and also retrieving prior knowledge. Recent work shows promise by\ndecomposing such tasks using a large language model (LLM) into an executable\nprogram that invokes specialized vision models. However, generated programs are\nerror-prone: they omit necessary steps, include spurious ones, and are unable\nto recover when the specialized models give incorrect outputs. Moreover, they\nrequire loading multiple models, incurring high latency and computation costs.\nWe propose Visual Program Distillation (VPD), an instruction tuning framework\nthat produces a vision-language model (VLM) capable of solving complex visual\ntasks with a single forward pass. VPD distills the reasoning ability of LLMs by\nusing them to sample multiple candidate programs, which are then executed and\nverified to identify a correct one. It translates each correct program into a\nlanguage description of the reasoning steps, which are then distilled into a\nVLM. Extensive experiments show that VPD improves the VLM's ability to count,\nunderstand spatial relations, and reason compositionally. Our VPD-trained\nPaLI-X outperforms all prior VLMs, achieving state-of-the-art performance\nacross complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE,\nand Hateful Memes. An evaluation with human annotators also confirms that VPD\nimproves model response factuality and consistency. Finally, experiments on\ncontent moderation demonstrate that VPD is also helpful for adaptation to\nreal-world applications with limited data.",
            "author": [
                "Yushi Hu",
                "Otilia Stretcu",
                "Chun-Ta Lu",
                "Krishnamurthy Viswanathan",
                "Kenji Hata",
                "Enming Luo",
                "Ranjay Krishna",
                "Ariel Fuxman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03052v1",
                "http://arxiv.org/pdf/2312.03052v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02970v1",
            "title": "Alchemist: Parametric Control of Material Properties with Diffusion\n  Models",
            "updated": "2023-12-05T18:58:26Z",
            "published": "2023-12-05T18:58:26Z",
            "summary": "We propose a method to control material attributes of objects like roughness,\nmetallic, albedo, and transparency in real images. Our method capitalizes on\nthe generative prior of text-to-image models known for photorealism, employing\na scalar value and instructions to alter low-level material properties.\nAddressing the lack of datasets with controlled material attributes, we\ngenerated an object-centric synthetic dataset with physically-based materials.\nFine-tuning a modified pre-trained text-to-image model on this synthetic\ndataset enables us to edit material properties in real-world images while\npreserving all other attributes. We show the potential application of our model\nto material edited NeRFs.",
            "author": [
                "Prafull Sharma",
                "Varun Jampani",
                "Yuanzhen Li",
                "Xuhui Jia",
                "Dmitry Lagun",
                "Fredo Durand",
                "William T. Freeman",
                "Mark Matthews"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02970v1",
                "http://arxiv.org/pdf/2312.02970v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02969v1",
            "title": "Rank-without-GPT: Building GPT-Independent Listwise Rerankers on\n  Open-Source Large Language Models",
            "updated": "2023-12-05T18:57:40Z",
            "published": "2023-12-05T18:57:40Z",
            "summary": "Listwise rerankers based on large language models (LLM) are the zero-shot\nstate-of-the-art. However, current works in this direction all depend on the\nGPT models, making it a single point of failure in scientific reproducibility.\nMoreover, it raises the concern that the current research findings only hold\nfor GPT models but not LLM in general. In this work, we lift this pre-condition\nand build for the first time effective listwise rerankers without any form of\ndependency on GPT. Our passage retrieval experiments show that our best list se\nreranker surpasses the listwise rerankers based on GPT-3.5 by 13% and achieves\n97% effectiveness of the ones built on GPT-4. Our results also show that the\nexisting training datasets, which were expressly constructed for pointwise\nranking, are insufficient for building such listwise rerankers. Instead,\nhigh-quality listwise ranking data is required and crucial, calling for further\nwork on building human-annotated listwise data resources.",
            "author": [
                "Xinyu Zhang",
                "Sebastian Hofst\u00e4tter",
                "Patrick Lewis",
                "Raphael Tang",
                "Jimmy Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02969v1",
                "http://arxiv.org/pdf/2312.02969v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02967v1",
            "title": "AmbiGen: Generating Ambigrams from Pre-trained Diffusion Model",
            "updated": "2023-12-05T18:56:06Z",
            "published": "2023-12-05T18:56:06Z",
            "summary": "Ambigrams are calligraphic designs that have different meanings depending on\nthe viewing orientation. Creating ambigrams is a challenging task even for\nskilled artists, as it requires maintaining the meaning under two different\nviewpoints at the same time. In this work, we propose to generate ambigrams by\ndistilling a large-scale vision and language diffusion model, namely DeepFloyd\nIF, to optimize the letters' outline for legibility in the two viewing\norientations. Empirically, we demonstrate that our approach outperforms\nexisting ambigram generation methods. On the 500 most common words in English,\nour method achieves more than an 11.6% increase in word accuracy and at least a\n41.9% reduction in edit distance.",
            "author": [
                "Boheng Zhao",
                "Rana Hanocka",
                "Raymond A. Yeh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02967v1",
                "http://arxiv.org/pdf/2312.02967v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03051v1",
            "title": "Generating Interpretable Networks using Hypernetworks",
            "updated": "2023-12-05T18:55:32Z",
            "published": "2023-12-05T18:55:32Z",
            "summary": "An essential goal in mechanistic interpretability to decode a network, i.e.,\nto convert a neural network's raw weights to an interpretable algorithm. Given\nthe difficulty of the decoding problem, progress has been made to understand\nthe easier encoding problem, i.e., to convert an interpretable algorithm into\nnetwork weights. Previous works focus on encoding existing algorithms into\nnetworks, which are interpretable by definition. However, focusing on encoding\nlimits the possibility of discovering new algorithms that humans have never\nstumbled upon, but that are nevertheless interpretable. In this work, we\nexplore the possibility of using hypernetworks to generate interpretable\nnetworks whose underlying algorithms are not yet known. The hypernetwork is\ncarefully designed such that it can control network complexity, leading to a\ndiverse family of interpretable algorithms ranked by their complexity. All of\nthem are interpretable in hindsight, although some of them are less intuitive\nto humans, hence providing new insights regarding how to \"think\" like a neural\nnetwork. For the task of computing L1 norms, hypernetworks find three\nalgorithms: (a) the double-sided algorithm, (b) the convexity algorithm, (c)\nthe pudding algorithm, although only the first algorithm was expected by the\nauthors before experiments. We automatically classify these algorithms and\nanalyze how these algorithmic phases develop during training, as well as how\nthey are affected by complexity control. Furthermore, we show that a trained\nhypernetwork can correctly construct models for input dimensions not seen in\ntraining, demonstrating systematic generalization.",
            "author": [
                "Isaac Liao",
                "Ziming Liu",
                "Max Tegmark"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03051v1",
                "http://arxiv.org/pdf/2312.03051v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NE",
                "68T07",
                "I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02966v1",
            "title": "Diffusion-SS3D: Diffusion Model for Semi-supervised 3D Object Detection",
            "updated": "2023-12-05T18:54:03Z",
            "published": "2023-12-05T18:54:03Z",
            "summary": "Semi-supervised object detection is crucial for 3D scene understanding,\nefficiently addressing the limitation of acquiring large-scale 3D bounding box\nannotations. Existing methods typically employ a teacher-student framework with\npseudo-labeling to leverage unlabeled point clouds. However, producing reliable\npseudo-labels in a diverse 3D space still remains challenging. In this work, we\npropose Diffusion-SS3D, a new perspective of enhancing the quality of\npseudo-labels via the diffusion model for semi-supervised 3D object detection.\nSpecifically, we include noises to produce corrupted 3D object size and class\nlabel distributions, and then utilize the diffusion model as a denoising\nprocess to obtain bounding box outputs. Moreover, we integrate the diffusion\nmodel into the teacher-student framework, so that the denoised bounding boxes\ncan be used to improve pseudo-label generation, as well as the entire\nsemi-supervised learning process. We conduct experiments on the ScanNet and SUN\nRGB-D benchmark datasets to demonstrate that our approach achieves\nstate-of-the-art performance against existing methods. We also present\nextensive analysis to understand how our diffusion model design affects\nperformance in semi-supervised learning.",
            "author": [
                "Cheng-Ju Ho",
                "Chen-Hsuan Tai",
                "Yen-Yu Lin",
                "Ming-Hsuan Yang",
                "Yi-Hsuan Tsai"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02966v1",
                "http://arxiv.org/pdf/2312.02966v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02963v1",
            "title": "MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human\n  Captures",
            "updated": "2023-12-05T18:50:12Z",
            "published": "2023-12-05T18:50:12Z",
            "summary": "In this era, the success of large language models and text-to-image models\ncan be attributed to the driving force of large-scale datasets. However, in the\nrealm of 3D vision, while remarkable progress has been made with models trained\non large-scale synthetic and real-captured object data like Objaverse and\nMVImgNet, a similar level of progress has not been observed in the domain of\nhuman-centric tasks partially due to the lack of a large-scale human dataset.\nExisting datasets of high-fidelity 3D human capture continue to be mid-sized\ndue to the significant challenges in acquiring large-scale high-quality 3D\nhuman data. To bridge this gap, we present MVHumanNet, a dataset that comprises\nmulti-view human action sequences of 4,500 human identities. The primary focus\nof our work is on collecting human data that features a large number of diverse\nidentities and everyday clothing using a multi-view human capture system, which\nfacilitates easily scalable data collection. Our dataset contains 9,000 daily\noutfits, 60,000 motion sequences and 645 million frames with extensive\nannotations, including human masks, camera parameters, 2D and 3D keypoints,\nSMPL/SMPLX parameters, and corresponding textual descriptions. To explore the\npotential of MVHumanNet in various 2D and 3D visual tasks, we conducted pilot\nstudies on view-consistent action recognition, human NeRF reconstruction,\ntext-driven view-unconstrained human image generation, as well as 2D\nview-unconstrained human image and 3D avatar generation. Extensive experiments\ndemonstrate the performance improvements and effective applications enabled by\nthe scale provided by MVHumanNet. As the current largest-scale 3D human\ndataset, we hope that the release of MVHumanNet data with annotations will\nfoster further innovations in the domain of 3D human-centric tasks at scale.",
            "author": [
                "Zhangyang Xiong",
                "Chenghong Li",
                "Kenkun Liu",
                "Hongjie Liao",
                "Jianqiao Hu",
                "Junyi Zhu",
                "Shuliang Ning",
                "Lingteng Qiu",
                "Chongjie Wang",
                "Shijie Wang",
                "Shuguang Cui",
                "Xiaoguang Han"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02963v1",
                "http://arxiv.org/pdf/2312.02963v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02959v2",
            "title": "Detecting algorithmic bias in medical AI-models",
            "updated": "2023-12-06T20:57:39Z",
            "published": "2023-12-05T18:47:34Z",
            "summary": "With the growing prevalence of machine learning and artificial\nintelligence-based medical decision support systems, it is equally important to\nensure that these systems provide patient outcomes in a fair and equitable\nfashion. This paper presents an innovative framework for detecting areas of\nalgorithmic bias in medical-AI decision support systems. Our approach\nefficiently identifies potential biases in medical-AI models, specifically in\nthe context of sepsis prediction, by employing the Classification and\nRegression Trees (CART) algorithm. We verify our methodology by conducting a\nseries of synthetic data experiments, showcasing its ability to estimate areas\nof bias in controlled settings precisely. The effectiveness of the concept is\nfurther validated by experiments using electronic medical records from Grady\nMemorial Hospital in Atlanta, Georgia. These tests demonstrate the practical\nimplementation of our strategy in a clinical environment, where it can function\nas a vital instrument for guaranteeing fairness and equity in AI-based medical\ndecisions.",
            "author": [
                "Jeffrey Smith",
                "Andre Holder",
                "Rishikesan Kamaleswaran",
                "Yao Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02959v2",
                "http://arxiv.org/pdf/2312.02959v2"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.CY",
                "cs.LG",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03050v1",
            "title": "HIG: Hierarchical Interlacement Graph Approach to Scene Graph Generation\n  in Video Understanding",
            "updated": "2023-12-05T18:47:19Z",
            "published": "2023-12-05T18:47:19Z",
            "summary": "Visual interactivity understanding within visual scenes presents a\nsignificant challenge in computer vision. Existing methods focus on complex\ninteractivities while leveraging a simple relationship model. These methods,\nhowever, struggle with a diversity of appearance, situation, position,\ninteraction, and relation in videos. This limitation hinders the ability to\nfully comprehend the interplay within the complex visual dynamics of subjects.\nIn this paper, we delve into interactivities understanding within visual\ncontent by deriving scene graph representations from dense interactivities\namong humans and objects. To achieve this goal, we first present a new dataset\ncontaining Appearance-Situation-Position-Interaction-Relation predicates, named\nASPIRe, offering an extensive collection of videos marked by a wide range of\ninteractivities. Then, we propose a new approach named Hierarchical\nInterlacement Graph (HIG), which leverages a unified layer and graph within a\nhierarchical structure to provide deep insights into scene changes across five\ndistinct tasks. Our approach demonstrates superior performance to other methods\nthrough extensive experiments conducted in various scenarios.",
            "author": [
                "Trong-Thuan Nguyen",
                "Pha Nguyen",
                "Khoa Luu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03050v1",
                "http://arxiv.org/pdf/2312.03050v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03049v1",
            "title": "Architectural Approaches to Overcome Challenges in the Development of\n  Data-Intensive Systems",
            "updated": "2023-12-05T18:42:25Z",
            "published": "2023-12-05T18:42:25Z",
            "summary": "Orientation of modern software systems towards data-intensive processing\nraises new difficulties in software engineering on how to build and maintain\nsuch systems. Some of the important challenges concern the design of software\narchitecture. In this article, we survey the fundamental challenges when\ndesigning data-intensive computing systems and present some of the most popular\nsoftware architectural styles together with their potential to tackle these\nchallenges.",
            "author": [
                "Aleksandar Dimov",
                "Simeon Emanuilov",
                "Boyan Bontchev",
                "Yavor Dankov",
                "Tasos Papapostolu"
            ],
            "link": [
                "http://dx.doi.org/10.54941/ahfe1002521",
                "http://arxiv.org/abs/2312.03049v1",
                "http://arxiv.org/pdf/2312.03049v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02957v1",
            "title": "Classification for everyone : Building geography agnostic models for\n  fairer recognition",
            "updated": "2023-12-05T18:41:03Z",
            "published": "2023-12-05T18:41:03Z",
            "summary": "In this paper, we analyze different methods to mitigate inherent geographical\nbiases present in state of the art image classification models. We first\nquantitatively present this bias in two datasets - The Dollar Street Dataset\nand ImageNet, using images with location information. We then present different\nmethods which can be employed to reduce this bias. Finally, we analyze the\neffectiveness of the different techniques on making these models more robust to\ngeographical locations of the images.",
            "author": [
                "Akshat Jindal",
                "Shreya Singh",
                "Soham Gadgil"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02957v1",
                "http://arxiv.org/pdf/2312.02957v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02956v1",
            "title": "Choroidalyzer: An open-source, end-to-end pipeline for choroidal\n  analysis in optical coherence tomography",
            "updated": "2023-12-05T18:40:40Z",
            "published": "2023-12-05T18:40:40Z",
            "summary": "Purpose: To develop Choroidalyzer, an open-source, end-to-end pipeline for\nsegmenting the choroid region, vessels, and fovea, and deriving choroidal\nthickness, area, and vascular index.\n  Methods: We used 5,600 OCT B-scans (233 subjects, 6 systemic disease cohorts,\n3 device types, 2 manufacturers). To generate region and vessel ground-truths,\nwe used state-of-the-art automatic methods following manual correction of\ninaccurate segmentations, with foveal positions manually annotated. We trained\na U-Net deep-learning model to detect the region, vessels, and fovea to\ncalculate choroid thickness, area, and vascular index in a fovea-centred region\nof interest. We analysed segmentation agreement (AUC, Dice) and choroid metrics\nagreement (Pearson, Spearman, mean absolute error (MAE)) in internal and\nexternal test sets. We compared Choroidalyzer to two manual graders on a small\nsubset of external test images and examined cases of high error.\n  Results: Choroidalyzer took 0.299 seconds per image on a standard laptop and\nachieved excellent region (Dice: internal 0.9789, external 0.9749), very good\nvessel segmentation performance (Dice: internal 0.8817, external 0.8703) and\nexcellent fovea location prediction (MAE: internal 3.9 pixels, external 3.4\npixels). For thickness, area, and vascular index, Pearson correlations were\n0.9754, 0.9815, and 0.8285 (internal) / 0.9831, 0.9779, 0.7948 (external),\nrespectively (all p<0.0001). Choroidalyzer's agreement with graders was\ncomparable to the inter-grader agreement across all metrics.\n  Conclusions: Choroidalyzer is an open-source, end-to-end pipeline that\naccurately segments the choroid and reliably extracts thickness, area, and\nvascular index. Especially choroidal vessel segmentation is a difficult and\nsubjective task, and fully-automatic methods like Choroidalyzer could provide\nobjectivity and standardisation.",
            "author": [
                "Justin Engelmann",
                "Jamie Burke",
                "Charlene Hamid",
                "Megan Reid-Schachter",
                "Dan Pugh",
                "Neeraj Dhaun",
                "Diana Moukaddem",
                "Lyle Gray",
                "Niall Strang",
                "Paul McGraw",
                "Amos Storkey",
                "Paul J. Steptoe",
                "Stuart King",
                "Tom MacGillivray",
                "Miguel O. Bernabeu",
                "Ian J. C. MacCormick"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02956v1",
                "http://arxiv.org/pdf/2312.02956v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02955v1",
            "title": "Switch Points of Bi-Persistence Matching Distance",
            "updated": "2023-12-05T18:40:39Z",
            "published": "2023-12-05T18:40:39Z",
            "summary": "In multi-parameter persistence, the matching distance is defined as the\nsupremum of weighted bottleneck distances on the barcodes given by the\nrestriction of persistence modules to lines with a positive slope. In the case\nof finitely presented bi-persistence modules, all the available methods to\ncompute the matching distance are based on restricting the computation to lines\nthrough pairs from a finite set of points in the plane. Some of these points\nare determined by the filtration data as they are entrance values of critical\nsimplices. However, these critical values alone are not sufficient for the\nmatching distance computation and it is necessary to add so-called switch\npoints, i.e. points such that on a line through any of them, the bottleneck\nmatching switches the matched pair.\n  This paper is devoted to the algorithmic computation of the set of switch\npoints given a set of critical values. We find conditions under which a\ncandidate switch point is erroneous or superfluous. The obtained conditions are\nturned into algorithms that have been implemented. With this, we analyze how\nthe size of the set of switch points increases as the number of critical values\nincreases, and how it varies depending on the distribution of critical values.\nExperiments are carried out on various types of bi-persistence modules.",
            "author": [
                "Robyn Brooks",
                "Celia Hacker",
                "Claudia Landi",
                "Barbara I. Mahler",
                "Elizabeth R. Stephenson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02955v1",
                "http://arxiv.org/pdf/2312.02955v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG",
                "math.AT",
                "55N31, 62R40"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03048v1",
            "title": "DGInStyle: Domain-Generalizable Semantic Segmentation with Image\n  Diffusion Models and Stylized Semantic Control",
            "updated": "2023-12-05T18:34:12Z",
            "published": "2023-12-05T18:34:12Z",
            "summary": "Large, pretrained latent diffusion models (LDMs) have demonstrated an\nextraordinary ability to generate creative content, specialize to user data\nthrough few-shot fine-tuning, and condition their output on other modalities,\nsuch as semantic maps. However, are they usable as large-scale data generators,\ne.g., to improve tasks in the perception stack, like semantic segmentation? We\ninvestigate this question in the context of autonomous driving, and answer it\nwith a resounding \"yes\". We propose an efficient data generation pipeline\ntermed DGInStyle. First, we examine the problem of specializing a pretrained\nLDM to semantically-controlled generation within a narrow domain. Second, we\ndesign a Multi-resolution Latent Fusion technique to overcome the bias of LDMs\ntowards dominant objects. Third, we propose a Style Swap technique to endow the\nrich generative prior with the learned semantic control. Using DGInStyle, we\ngenerate a diverse dataset of street scenes, train a domain-agnostic semantic\nsegmentation model on it, and evaluate the model on multiple popular autonomous\ndriving datasets. Our approach consistently increases the performance of\nseveral domain generalization methods, in some cases by +2.5 mIoU compared to\nthe previous state-of-the-art method without our generative augmentation\nscheme. Source code and dataset are available at https://dginstyle.github.io .",
            "author": [
                "Yuru Jia",
                "Lukas Hoyer",
                "Shengyu Huang",
                "Tianfu Wang",
                "Luc Van Gool",
                "Konrad Schindler",
                "Anton Obukhov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03048v1",
                "http://arxiv.org/pdf/2312.03048v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02949v1",
            "title": "LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models",
            "updated": "2023-12-05T18:29:31Z",
            "published": "2023-12-05T18:29:31Z",
            "summary": "With the recent significant advancements in large multi-modal models (LMMs),\nthe importance of their grounding capability in visual chat is increasingly\nrecognized. Despite recent efforts to enable LMMs to support grounding, their\ncapabilities for grounding and chat are usually separate, and their chat\nperformance drops dramatically when asked to ground. The problem is the lack of\na dataset for grounded visual chat (GVC). Existing grounding datasets only\ncontain short captions. To address this issue, we have created GVC data that\nallows for the combination of grounding and chat capabilities. To better\nevaluate the GVC capabilities, we have introduced a benchmark called\nGrounding-Bench. Additionally, we have proposed a model design that can support\nGVC and various types of visual prompts by connecting segmentation models with\nlanguage models. Experimental results demonstrate that our model outperforms\nother LMMs on Grounding-Bench. Furthermore, our model achieves competitive\nperformance on classic grounding benchmarks like RefCOCO/+/g and Flickr30K\nEntities. Our code will be released at\nhttps://github.com/UX-Decoder/LLaVA-Grounding .",
            "author": [
                "Hao Zhang",
                "Hongyang Li",
                "Feng Li",
                "Tianhe Ren",
                "Xueyan Zou",
                "Shilong Liu",
                "Shijia Huang",
                "Jianfeng Gao",
                "Lei Zhang",
                "Chunyuan Li",
                "Jianwei Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02949v1",
                "http://arxiv.org/pdf/2312.02949v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02947v1",
            "title": "Remarks on the spectra of minimal hypersurfaces in the hyperbolic space",
            "updated": "2023-12-05T18:18:10Z",
            "published": "2023-12-05T18:18:10Z",
            "summary": "We compute the Laplacian spectra of singular area-minimising hypersurfaces in\nthe hyperbolic space with prescribed asymptotic data. We also obtain similar\nresults in higher codimension, and explore related extremal properties of the\nbottom of the spectrum.",
            "author": [
                "Gerasim Kokarev"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02947v1",
                "http://arxiv.org/pdf/2312.02947v1"
            ],
            "primary_category": "math.DG",
            "category": [
                "math.DG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02946v1",
            "title": "Calibrating dimension reduction hyperparameters in the presence of noise",
            "updated": "2023-12-05T18:16:17Z",
            "published": "2023-12-05T18:16:17Z",
            "summary": "The goal of dimension reduction tools is to construct a low-dimensional\nrepresentation of high-dimensional data. These tools are employed for a variety\nof reasons such as noise reduction, visualization, and to lower computational\ncosts. However, there is a fundamental issue that is highly discussed in other\nmodeling problems, but almost entirely ignored in the dimension reduction\nliterature: overfitting. If we interpret data as a combination of signal and\nnoise, prior works judge dimension reduction techniques on their ability to\ncapture the entirety of the data, i.e. both the signal and the noise. In the\ncontext of other modeling problems, techniques such as feature-selection,\ncross-validation, and regularization are employed to combat overfitting, but no\nsuch precautions are taken when performing dimension reduction. In this paper,\nwe present a framework that models dimension reduction problems in the presence\nof noise and use this framework to explore the role perplexity and number of\nneighbors play in overfitting data when applying t-SNE and UMAP. We also\npresent a workflow others may use to calibrate perplexity or number of\nneighbors in the presence of noise.",
            "author": [
                "Justin Lin",
                "Julia Fukuyama"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02946v1",
                "http://arxiv.org/pdf/2312.02946v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02941v1",
            "title": "Fast CT anatomic localization algorithm",
            "updated": "2023-12-05T18:09:47Z",
            "published": "2023-12-05T18:09:47Z",
            "summary": "Automatically determining the position of every slice in a CT scan is a basic\nyet powerful capability allowing fast retrieval of region of interest for\nvisual inspection and automated analysis. Unlike conventional localization\napproaches which work at the slice level, we directly localize only a fraction\nof the slices and and then fit a linear model which maps slice index to its\nestimated axial anatomical position based on those slices. The model is then\nused to assign axial position to every slices of the scan. This approach proves\nto be both computationally efficient, with a typical processing time of less\nthan a second per scan (regardless of its size), accurate, with a typical\nmedian localization error of 1 cm, and robust to different noise sources,\nimaging protocols, metal induced artifacts, anatomical deformations etc.\nAnother key element of our approach is the introduction of a mapping confidence\nscore. This score acts as a fail safe mechanism which allows a rejection of\nunreliable localization results in rare cases of anomalous scans. Our algorithm\nsets new State Of The Art results in terms of localization accuracy. It also\noffers a decrease of two orders of magnitude in processing time with respect to\nall published processing times. It was designed to be invariant to various scan\nresolutions, scan protocols, patient orientations, strong artifacts and various\ndeformations and abnormalities. Additionally, our algorithm is the first one to\nthe best of our knowledge which supports the entire body from head to feet and\nis not confined to specific anatomical region. This algorithm was tested on\nthousands of scans and proves to be very reliable and useful as a preprocessing\nstage for many applications.",
            "author": [
                "Amit Oved"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02941v1",
                "http://arxiv.org/pdf/2312.02941v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02936v1",
            "title": "Drag-A-Video: Non-rigid Video Editing with Point-based Interaction",
            "updated": "2023-12-05T18:05:59Z",
            "published": "2023-12-05T18:05:59Z",
            "summary": "Video editing is a challenging task that requires manipulating videos on both\nthe spatial and temporal dimensions. Existing methods for video editing mainly\nfocus on changing the appearance or style of the objects in the video, while\nkeeping their structures unchanged. However, there is no existing method that\nallows users to interactively ``drag'' any points of instances on the first\nframe to precisely reach the target points with other frames consistently\ndeformed. In this paper, we propose a new diffusion-based method for\ninteractive point-based video manipulation, called Drag-A-Video. Our method\nallows users to click pairs of handle points and target points as well as masks\non the first frame of an input video. Then, our method transforms the inputs\ninto point sets and propagates these sets across frames. To precisely modify\nthe contents of the video, we employ a new video-level motion supervision to\nupdate the features of the video and introduce the latent offsets to achieve\nthis update at multiple denoising timesteps. We propose a temporal-consistent\npoint tracking module to coordinate the movement of the points in the handle\npoint sets. We demonstrate the effectiveness and flexibility of our method on\nvarious videos. The website of our work is available here:\nhttps://drag-a-video.github.io/.",
            "author": [
                "Yao Teng",
                "Enze Xie",
                "Yue Wu",
                "Haoyu Han",
                "Zhenguo Li",
                "Xihui Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02936v1",
                "http://arxiv.org/pdf/2312.02936v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02934v2",
            "title": "WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera\n  Driving Scene Generation",
            "updated": "2023-12-06T18:45:29Z",
            "published": "2023-12-05T18:05:14Z",
            "summary": "Generating multi-camera street-view videos is critical for augmenting\nautonomous driving datasets, addressing the urgent demand for extensive and\nvaried data. Due to the limitations in diversity and challenges in handling\nlighting conditions, traditional rendering-based methods are increasingly being\nsupplanted by diffusion-based methods. However, a significant challenge in\ndiffusion-based methods is ensuring that the generated sensor data preserve\nboth intra-world consistency and inter-sensor coherence. To address these\nchallenges, we combine an additional explicit world volume and propose the\nWorld Volume-aware Multi-camera Driving Scene Generator (WoVoGen). This system\nis specifically designed to leverage 4D world volume as a foundational element\nfor video generation. Our model operates in two distinct phases: (i)\nenvisioning the future 4D temporal world volume based on vehicle control\nsequences, and (ii) generating multi-camera videos, informed by this envisioned\n4D temporal world volume and sensor interconnectivity. The incorporation of the\n4D world volume empowers WoVoGen not only to generate high-quality street-view\nvideos in response to vehicle control inputs but also to facilitate scene\nediting tasks.",
            "author": [
                "Jiachen Lu",
                "Ze Huang",
                "Jiahui Zhang",
                "Zeyu Yang",
                "Li Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02934v2",
                "http://arxiv.org/pdf/2312.02934v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02931v2",
            "title": "WhisBERT: Multimodal Text-Audio Language Modeling on 100M Words",
            "updated": "2023-12-07T00:37:29Z",
            "published": "2023-12-05T18:03:13Z",
            "summary": "Training on multiple modalities of input can augment the capabilities of a\nlanguage model. Here, we ask whether such a training regime can improve the\nquality and efficiency of these systems as well. We focus on text--audio and\nintroduce Whisbert, which is inspired by the text--image approach of FLAVA\n(Singh et al., 2022). In accordance with Babylm guidelines (Warstadt et al.,\n2023), we pretrain Whisbert on a dataset comprising only 100 million words plus\ntheir corresponding speech from the word-aligned version of the People's Speech\ndataset (Galvez et al., 2021). To assess the impact of multimodality, we\ncompare versions of the model that are trained on text only and on both audio\nand text simultaneously. We find that while Whisbert is able to perform well on\nmultimodal masked modeling and surpasses the Babylm baselines in most benchmark\ntasks, it struggles to optimize its complex objective and outperform its\ntext-only Whisbert baseline.",
            "author": [
                "Lukas Wolf",
                "Greta Tuckute",
                "Klemen Kotar",
                "Eghbal Hosseini",
                "Tamar Regev",
                "Ethan Wilcox",
                "Alex Warstadt"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02931v2",
                "http://arxiv.org/pdf/2312.02931v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02928v1",
            "title": "LivePhoto: Real Image Animation with Text-guided Motion Control",
            "updated": "2023-12-05T17:59:52Z",
            "published": "2023-12-05T17:59:52Z",
            "summary": "Despite the recent progress in text-to-video generation, existing studies\nusually overlook the issue that only spatial contents but not temporal motions\nin synthesized videos are under the control of text. Towards such a challenge,\nthis work presents a practical system, named LivePhoto, which allows users to\nanimate an image of their interest with text descriptions. We first establish a\nstrong baseline that helps a well-learned text-to-image generator (i.e., Stable\nDiffusion) take an image as a further input. We then equip the improved\ngenerator with a motion module for temporal modeling and propose a carefully\ndesigned training pipeline to better link texts and motions. In particular,\nconsidering the facts that (1) text can only describe motions roughly (e.g.,\nregardless of the moving speed) and (2) text may include both content and\nmotion descriptions, we introduce a motion intensity estimation module as well\nas a text re-weighting module to reduce the ambiguity of text-to-motion\nmapping. Empirical evidence suggests that our approach is capable of well\ndecoding motion-related textual instructions into videos, such as actions,\ncamera movements, or even conjuring new contents from thin air (e.g., pouring\nwater into an empty glass). Interestingly, thanks to the proposed intensity\nlearning mechanism, our system offers users an additional control signal (i.e.,\nthe motion intensity) besides text for video customization.",
            "author": [
                "Xi Chen",
                "Zhiheng Liu",
                "Mengting Chen",
                "Yutong Feng",
                "Yu Liu",
                "Yujun Shen",
                "Hengshuang Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02928v1",
                "http://arxiv.org/pdf/2312.02928v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03047v1",
            "title": "MagicStick: Controllable Video Editing via Control Handle\n  Transformations",
            "updated": "2023-12-05T17:58:06Z",
            "published": "2023-12-05T17:58:06Z",
            "summary": "Text-based video editing has recently attracted considerable interest in\nchanging the style or replacing the objects with a similar structure. Beyond\nthis, we demonstrate that properties such as shape, size, location, motion,\netc., can also be edited in videos. Our key insight is that the keyframe\ntransformations of the specific internal feature (e.g., edge maps of objects or\nhuman pose), can easily propagate to other frames to provide generation\nguidance. We thus propose MagicStick, a controllable video editing method that\nedits the video properties by utilizing the transformation on the extracted\ninternal control signals. In detail, to keep the appearance, we inflate both\nthe pretrained image diffusion model and ControlNet to the temporal dimension\nand train low-rank adaptions (LORA) layers to fit the specific scenes. Then, in\nediting, we perform an inversion and editing framework. Differently, finetuned\nControlNet is introduced in both inversion and generation for attention\nguidance with the proposed attention remix between the spatial attention maps\nof inversion and editing. Yet succinct, our method is the first method to show\nthe ability of video property editing from the pre-trained text-to-image model.\nWe present experiments on numerous examples within our unified framework. We\nalso compare with shape-aware text-based editing and handcrafted motion video\ngeneration, demonstrating our superior temporal consistency and editing\ncapability than previous works. The code and models will be made publicly\navailable.",
            "author": [
                "Yue Ma",
                "Xiaodong Cun",
                "Yingqing He",
                "Chenyang Qi",
                "Xintao Wang",
                "Ying Shan",
                "Xiu Li",
                "Qifeng Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03047v1",
                "http://arxiv.org/pdf/2312.03047v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02926v1",
            "title": "Symmetry resolution of the computable cross-norm negativity of two\n  disjoint intervals in the massless Dirac field theory",
            "updated": "2023-12-05T17:56:48Z",
            "published": "2023-12-05T17:56:48Z",
            "summary": "We investigate how entanglement in the mixed state of a quantum field theory\ncan be described using the cross-computable norm or realignment (CCNR)\ncriterion, employing a recently introduced negativity. We study its symmetry\nresolution for two disjoint intervals in the ground state of the massless Dirac\nfermion field theory, extending previous results for the case of adjacent\nintervals. By applying the replica trick, this problem boils down to compute\nthe charged moments of the realignment matrix. We show that, for two disjoint\nintervals, they correspond to the partition function of the theory on a torus\nwith a non-contractible charged loop. This confers a great advantage compared\nto the negativity based on the partial transposition, for which the Riemann\nsurfaces generated by the replica trick have higher genus. This result empowers\nus to carry out the replica limit, yielding analytic expressions for the\nsymmetry-resolved CCNR negativity. Furthermore, these expressions provide also\nthe symmetry decomposition of other related quantities such as the operator\nentanglement of the reduced density matrix or the reflected entropy.",
            "author": [
                "Andrea Bruno",
                "Filiberto Ares",
                "Sara Murciano",
                "Pasquale Calabrese"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02926v1",
                "http://arxiv.org/pdf/2312.02926v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "cond-mat.stat-mech",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02923v1",
            "title": "Split & Merge: Unlocking the Potential of Visual Adapters via Sparse\n  Training",
            "updated": "2023-12-05T17:50:55Z",
            "published": "2023-12-05T17:50:55Z",
            "summary": "With the rapid growth in the scale of pre-trained foundation models,\nparameter-efficient fine-tuning techniques have gained significant attention,\namong which Adapter Tuning is the most widely used. Despite achieving\nefficiency, Adapter Tuning still underperforms full fine-tuning, and the\nperformance improves at the cost of an increase in parameters. Recent efforts\naddress this issue by pruning the original adapters, but it also introduces\ntraining instability and suboptimal performance on certain datasets. Motivated\nby this, we propose Mixture of Sparse Adapters, or MoSA, as a novel Adapter\nTuning method to fully unleash the potential of each parameter in the adapter.\nWe first split the standard adapter into multiple non-overlapping modules, then\nstochastically activate modules for sparse training, and finally merge them to\nform a complete adapter after tuning. In this way, MoSA can achieve\nsignificantly better performance than standard adapters without any additional\ncomputational or storage overhead. Furthermore, we propose a hierarchical\nsparse strategy to better leverage limited training data. Extensive experiments\non a series of 27 visual tasks demonstrate that MoSA consistently outperforms\nother Adapter Tuning methods as well as other baselines by a significant\nmargin. Furthermore, in two challenging scenarios with low-resource and\nmulti-task settings, MoSA achieves satisfactory results, further demonstrating\nthe effectiveness of our design. Our code will be released.",
            "author": [
                "Qizhe Zhang",
                "Bocheng Zou",
                "Ruichuan An",
                "Jiaming Liu",
                "Shanghang Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02923v1",
                "http://arxiv.org/pdf/2312.02923v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02919v1",
            "title": "Fine-grained Controllable Video Generation via Object Appearance and\n  Context",
            "updated": "2023-12-05T17:47:33Z",
            "published": "2023-12-05T17:47:33Z",
            "summary": "Text-to-video generation has shown promising results. However, by taking only\nnatural languages as input, users often face difficulties in providing detailed\ninformation to precisely control the model's output. In this work, we propose\nfine-grained controllable video generation (FACTOR) to achieve detailed\ncontrol. Specifically, FACTOR aims to control objects' appearances and context,\nincluding their location and category, in conjunction with the text prompt. To\nachieve detailed control, we propose a unified framework to jointly inject\ncontrol signals into the existing text-to-video model. Our model consists of a\njoint encoder and adaptive cross-attention layers. By optimizing the encoder\nand the inserted layer, we adapt the model to generate videos that are aligned\nwith both text prompts and fine-grained control. Compared to existing methods\nrelying on dense control signals such as edge maps, we provide a more intuitive\nand user-friendly interface to allow object-level fine-grained control. Our\nmethod achieves controllability of object appearances without finetuning, which\nreduces the per-subject optimization efforts for the users. Extensive\nexperiments on standard benchmark datasets and user-provided inputs validate\nthat our model obtains a 70% improvement in controllability metrics over\ncompetitive baselines.",
            "author": [
                "Hsin-Ping Huang",
                "Yu-Chuan Su",
                "Deqing Sun",
                "Lu Jiang",
                "Xuhui Jia",
                "Yukun Zhu",
                "Ming-Hsuan Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02919v1",
                "http://arxiv.org/pdf/2312.02919v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02918v1",
            "title": "Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and\n  Fidelity for All-in-One Image Restoration",
            "updated": "2023-12-05T17:47:11Z",
            "published": "2023-12-05T17:47:11Z",
            "summary": "Despite substantial progress, all-in-one image restoration (IR) grapples with\npersistent challenges in handling intricate real-world degradations. This paper\nintroduces MPerceiver: a novel multimodal prompt learning approach that\nharnesses Stable Diffusion (SD) priors to enhance adaptiveness,\ngeneralizability and fidelity for all-in-one image restoration. Specifically,\nwe develop a dual-branch module to master two types of SD prompts: textual for\nholistic representation and visual for multiscale detail representation. Both\nprompts are dynamically adjusted by degradation predictions from the CLIP image\nencoder, enabling adaptive responses to diverse unknown degradations. Moreover,\na plug-in detail refinement module improves restoration fidelity via direct\nencoder-to-decoder information transformation. To assess our method, MPerceiver\nis trained on 9 tasks for all-in-one IR and outperforms state-of-the-art\ntask-specific methods across most tasks. Post multitask pre-training,\nMPerceiver attains a generalized representation in low-level vision, exhibiting\nremarkable zero-shot and few-shot capabilities in unseen tasks. Extensive\nexperiments on 16 IR tasks and 26 benchmarks underscore the superiority of\nMPerceiver in terms of adaptiveness, generalizability and fidelity.",
            "author": [
                "Yuang Ai",
                "Huaibo Huang",
                "Xiaoqiang Zhou",
                "Jiexiang Wang",
                "Ran He"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02918v1",
                "http://arxiv.org/pdf/2312.02918v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02916v1",
            "title": "MIND: Multi-Task Incremental Network Distillation",
            "updated": "2023-12-05T17:46:52Z",
            "published": "2023-12-05T17:46:52Z",
            "summary": "The recent surge in pervasive devices generating dynamic data streams has\nunderscored the necessity for learning systems to adapt to data distributional\nshifts continually. To tackle this challenge, the research community has put\nforth a spectrum of methodologies, including the demanding pursuit of\nclass-incremental learning without replay data. In this study, we present MIND,\na parameter isolation method that aims to significantly enhance the performance\nof replay-free solutions and achieve state-of-the-art results on several widely\nstudied datasets. Our approach introduces two main contributions: two\nalternative distillation procedures that significantly improve the efficiency\nof MIND increasing the accumulated knowledge of each sub-network, and the\noptimization of the BachNorm layers across tasks inside the sub-networks.\nOverall, MIND outperforms all the state-of-the-art methods for rehearsal-free\nClass-Incremental learning (with an increment in classification accuracy of\napprox. +6% on CIFAR-100/10 and +10% on TinyImageNet/10) reaching up to approx.\n+40% accuracy in Domain-Incremental scenarios. Moreover, we ablated each\ncontribution to demonstrate its impact on performance improvement. Our results\nshowcase the superior performance of MIND indicating its potential for\naddressing the challenges posed by Class-incremental and Domain-Incremental\nlearning in resource-constrained environments.",
            "author": [
                "Jacopo Bonato",
                "Francesco Pelosin",
                "Luigi Sabetta",
                "Alessandro Nicolosi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02916v1",
                "http://arxiv.org/pdf/2312.02916v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02914v2",
            "title": "Unsupervised Video Domain Adaptation with Masked Pre-Training and\n  Collaborative Self-Training",
            "updated": "2023-12-06T19:12:32Z",
            "published": "2023-12-05T17:39:19Z",
            "summary": "In this work, we tackle the problem of unsupervised domain adaptation (UDA)\nfor video action recognition. Our approach, which we call UNITE, uses an image\nteacher model to adapt a video student model to the target domain. UNITE first\nemploys self-supervised pre-training to promote discriminative feature learning\non target domain videos using a teacher-guided masked distillation objective.\nWe then perform self-training on masked target data, using the video student\nmodel and image teacher model together to generate improved pseudolabels for\nunlabeled target videos. Our self-training process successfully leverages the\nstrengths of both models to achieve strong transfer performance across domains.\nWe evaluate our approach on multiple video domain adaptation benchmarks and\nobserve significant improvements upon previously reported results.",
            "author": [
                "Arun Reddy",
                "William Paul",
                "Corban Rivera",
                "Ketul Shah",
                "Celso M. de Melo",
                "Rama Chellappa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02914v2",
                "http://arxiv.org/pdf/2312.02914v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02913v1",
            "title": "Let the LLMs Talk: Simulating Human-to-Human Conversational QA via\n  Zero-Shot LLM-to-LLM Interactions",
            "updated": "2023-12-05T17:38:02Z",
            "published": "2023-12-05T17:38:02Z",
            "summary": "Conversational question-answering (CQA) systems aim to create interactive\nsearch systems that effectively retrieve information by interacting with users.\nTo replicate human-to-human conversations, existing work uses human annotators\nto play the roles of the questioner (student) and the answerer (teacher).\nDespite its effectiveness, challenges exist as human annotation is\ntime-consuming, inconsistent, and not scalable. To address this issue and\ninvestigate the applicability of large language models (LLMs) in CQA\nsimulation, we propose a simulation framework that employs zero-shot learner\nLLMs for simulating teacher-student interactions. Our framework involves two\nLLMs interacting on a specific topic, with the first LLM acting as a student,\ngenerating questions to explore a given search topic. The second LLM plays the\nrole of a teacher by answering questions and is equipped with additional\ninformation, including a text on the given topic. We implement both the student\nand teacher by zero-shot prompting the GPT-4 model. To assess the effectiveness\nof LLMs in simulating CQA interactions and understand the disparities between\nLLM- and human-generated conversations, we evaluate the simulated data from\nvarious perspectives. We begin by evaluating the teacher's performance through\nboth automatic and human assessment. Next, we evaluate the performance of the\nstudent, analyzing and comparing the disparities between questions generated by\nthe LLM and those generated by humans. Furthermore, we conduct extensive\nanalyses to thoroughly examine the LLM performance by benchmarking\nstate-of-the-art reading comprehension models on both datasets. Our results\nreveal that the teacher LLM generates lengthier answers that tend to be more\naccurate and complete. The student LLM generates more diverse questions,\ncovering more aspects of a given topic.",
            "author": [
                "Zahra Abbasiantaeb",
                "Yifei Yuan",
                "Evangelos Kanoulas",
                "Mohammad Aliannejadi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02913v1",
                "http://arxiv.org/pdf/2312.02913v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02912v1",
            "title": "Realistic Scatterer Based Adversarial Attacks on SAR Image Classifiers",
            "updated": "2023-12-05T17:36:34Z",
            "published": "2023-12-05T17:36:34Z",
            "summary": "Adversarial attacks have highlighted the vulnerability of classifiers based\non machine learning for Synthetic Aperture Radar (SAR) Automatic Target\nRecognition (ATR) tasks. An adversarial attack perturbs SAR images of on-ground\ntargets such that the classifiers are misled into making incorrect predictions.\nHowever, many existing attacking techniques rely on arbitrary manipulation of\nSAR images while overlooking the feasibility of executing the attacks on\nreal-world SAR imagery. Instead, adversarial attacks should be able to be\nimplemented by physical actions, for example, placing additional false objects\nas scatterers around the on-ground target to perturb the SAR image and fool the\nSAR ATR.\n  In this paper, we propose the On-Target Scatterer Attack (OTSA), a\nscatterer-based physical adversarial attack. To ensure the feasibility of its\nphysical execution, we enforce a constraint on the positioning of the\nscatterers. Specifically, we restrict the scatterers to be placed only on the\ntarget instead of in the shadow regions or the background. To achieve this, we\nintroduce a positioning score based on Gaussian kernels and formulate an\noptimization problem for our OTSA attack. Using a gradient ascent method to\nsolve the optimization problem, the OTSA can generate a vector of parameters\ndescribing the positions, shapes, sizes and amplitudes of the scatterers to\nguide the physical execution of the attack that will mislead SAR image\nclassifiers. The experimental results show that our attack obtains\nsignificantly higher success rates under the positioning constraint compared\nwith the existing method.",
            "author": [
                "Tian Ye",
                "Rajgopal Kannan",
                "Viktor Prasanna",
                "Carl Busart",
                "Lance Kaplan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02912v1",
                "http://arxiv.org/pdf/2312.02912v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02910v1",
            "title": "Rare Galaxy Classes Identified In Foundation Model Representations",
            "updated": "2023-12-05T17:36:04Z",
            "published": "2023-12-05T17:36:04Z",
            "summary": "We identify rare and visually distinctive galaxy populations by searching for\nstructure within the learned representations of pretrained models. We show that\nthese representations arrange galaxies by appearance in patterns beyond those\nneeded to predict the pretraining labels. We design a clustering approach to\nisolate specific local patterns, revealing groups of galaxies with rare and\nscientifically-interesting morphologies.",
            "author": [
                "Mike Walmsley",
                "Anna M. M. Scaife"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02910v1",
                "http://arxiv.org/pdf/2312.02910v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02908v1",
            "title": "Deep Learning Segmentation of Spiral Arms and Bars",
            "updated": "2023-12-05T17:30:16Z",
            "published": "2023-12-05T17:30:16Z",
            "summary": "We present the first deep learning model for segmenting galactic spiral arms\nand bars. In a blinded assessment by expert astronomers, our predicted spiral\narm masks are preferred over both current automated methods (99% of\nevaluations) and our original volunteer labels (79% of evaluations). Experts\nrated our spiral arm masks as `mostly good' to `perfect' in 89% of evaluations.\nBar lengths trivially derived from our predicted bar masks are in excellent\nagreement with a dedicated crowdsourcing project. The pixelwise precision of\nour masks, previously impossible at scale, will underpin new research into how\nspiral arms and bars evolve.",
            "author": [
                "Mike Walmsley",
                "Ashley Spindler"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02908v1",
                "http://arxiv.org/pdf/2312.02908v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02902v1",
            "title": "HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting",
            "updated": "2023-12-05T17:19:22Z",
            "published": "2023-12-05T17:19:22Z",
            "summary": "3D head animation has seen major quality and runtime improvements over the\nlast few years, particularly empowered by the advances in differentiable\nrendering and neural radiance fields. Real-time rendering is a highly desirable\ngoal for real-world applications. We propose HeadGaS, the first model to use 3D\nGaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper\nwe introduce a hybrid model that extends the explicit representation from 3DGS\nwith a base of learnable latent features, which can be linearly blended with\nlow-dimensional parameters from parametric head models to obtain\nexpression-dependent final color and opacity values. We demonstrate that\nHeadGaS delivers state-of-the-art results in real-time inference frame rates,\nwhich surpasses baselines by up to ~2dB, while accelerating rendering speed by\nover x10.",
            "author": [
                "Helisa Dhamo",
                "Yinyu Nie",
                "Arthur Moreau",
                "Jifei Song",
                "Richard Shaw",
                "Yiren Zhou",
                "Eduardo P\u00e9rez-Pellitero"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02902v1",
                "http://arxiv.org/pdf/2312.02902v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03046v1",
            "title": "Diversified in-domain synthesis with efficient fine-tuning for few-shot\n  classification",
            "updated": "2023-12-05T17:18:09Z",
            "published": "2023-12-05T17:18:09Z",
            "summary": "Few-shot image classification aims to learn an image classifier using only a\nsmall set of labeled examples per class. A recent research direction for\nimproving few-shot classifiers involves augmenting the labelled samples with\nsynthetic images created by state-of-the-art text-to-image generation models.\nFollowing this trend, we propose Diversified in-domain synthesis with efficient\nfine-tuning (DISEF), a novel approach which addresses the generalization\nchallenge in few-shot learning using synthetic data. DISEF consists of two main\ncomponents. First, we propose a novel text-to-image augmentation pipeline that,\nby leveraging the real samples and their rich semantics coming from an advanced\ncaptioning model, promotes in-domain sample diversity for better\ngeneralization. Second, we emphasize the importance of effective model\nfine-tuning in few-shot recognition, proposing to use Low-Rank Adaptation\n(LoRA) for joint adaptation of the text and image encoders in a Vision Language\nModel. We validate our method in ten different benchmarks, consistently\noutperforming baselines and establishing a new state-of-the-art for few-shot\nclassification. Code is available at \\url{https://github.com/vturrisi/disef}",
            "author": [
                "Victor G. Turrisi da Costa",
                "Nicola Dall'Asen",
                "Yiming Wang",
                "Nicu Sebe",
                "Elisa Ricci"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03046v1",
                "http://arxiv.org/pdf/2312.03046v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02901v1",
            "title": "Concept Drift Adaptation in Text Stream Mining Settings: A Comprehensive\n  Review",
            "updated": "2023-12-05T17:15:16Z",
            "published": "2023-12-05T17:15:16Z",
            "summary": "Due to the advent and increase in the popularity of the Internet, people have\nbeen producing and disseminating textual data in several ways, such as reviews,\nsocial media posts, and news articles. As a result, numerous researchers have\nbeen working on discovering patterns in textual data, especially because social\nmedia posts function as social sensors, indicating peoples' opinions,\ninterests, etc. However, most tasks regarding natural language processing are\naddressed using traditional machine learning methods and static datasets. This\nsetting can lead to several problems, such as an outdated dataset, which may\nnot correspond to reality, and an outdated model, which has its performance\ndegrading over time. Concept drift is another aspect that emphasizes these\nissues, which corresponds to data distribution and pattern changes. In a text\nstream scenario, it is even more challenging due to its characteristics, such\nas the high speed and data arriving sequentially. In addition, models for this\ntype of scenario must adhere to the constraints mentioned above while learning\nfrom the stream by storing texts for a limited time and consuming low memory.\nIn this study, we performed a systematic literature review regarding concept\ndrift adaptation in text stream scenarios. Considering well-defined criteria,\nwe selected 40 papers to unravel aspects such as text drift categories, types\nof text drift detection, model update mechanism, the addressed stream mining\ntasks, types of text representations, and text representation update mechanism.\nIn addition, we discussed drift visualization and simulation and listed\nreal-world datasets used in the selected papers. Therefore, this paper\ncomprehensively reviews the concept drift adaptation in text stream mining\nscenarios.",
            "author": [
                "Cristiano Mesquita Garcia",
                "Ramon Simoes Abilio",
                "Alessandro Lameiras Koerich",
                "Alceu de Souza Britto Jr.",
                "Jean Paul Barddal"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02901v1",
                "http://arxiv.org/pdf/2312.02901v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02898v1",
            "title": "Madgraph5_aMC@NLO on GPUs and vector CPUs Experience with the first\n  alpha release",
            "updated": "2023-12-05T17:12:03Z",
            "published": "2023-12-05T17:12:03Z",
            "summary": "Madgraph5_aMC@NLO is one of the most-frequently used Monte-Carlo event\ngenerators at the LHC, and an important consumer of compute resources. The\nsoftware has been reengineered to maintain the overall look and feel of the\nuser interface while speeding up event generation on CPUs and GPUs. The most\ncomputationally intensive part, the calculation of \"matrix elements\", is\noffloaded to new implementations optimised for GPUs and for CPU vector\ninstructions, using event-level data parallelism. We present the work to\nsupport accelerated leading-order QCD processes, and discuss how this work is\ngoing to be released to Madgraph5_aMC@NLO's users.",
            "author": [
                "Stephan Hageboeck",
                "Taylor Childers",
                "Walter Hopkins",
                "Olivier Mattelaer",
                "Nathan Nichols",
                "Stefan Roiser",
                "J\u00f8rgen Teig",
                "Andrea Valassi",
                "Carl Vuosalo",
                "Zenny Wettersten"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02898v1",
                "http://arxiv.org/pdf/2312.02898v1"
            ],
            "primary_category": "physics.comp-ph",
            "category": [
                "physics.comp-ph",
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02897v1",
            "title": "Perspectives from Naive Participants and Experienced Social Science\n  Researchers on Addressing Embodiment in a Virtual Cyberball Task",
            "updated": "2023-12-05T17:09:59Z",
            "published": "2023-12-05T17:09:59Z",
            "summary": "We describe the design of an immersive virtual Cyberball task that included\navatar customization, and user feedback on this design. We first created a\nprototype of an avatar customization template and added it to a Cyberball\nprototype built in the Unity3D game engine. Then, we conducted in-depth user\ntesting and feedback sessions with 15 Cyberball stakeholders: five naive\nparticipants with no prior knowledge of Cyberball and ten experienced\nresearchers with extensive experience using the Cyberball paradigm. We report\nthe divergent perspectives of the two groups on the following design insights;\ndesigning for intuitive use, inclusivity, and realistic experiences versus\nminimalism. Participant responses shed light on how system design problems may\ncontribute to or perpetuate negative experiences when customizing avatars. They\nalso demonstrate the value of considering multiple stakeholders' feedback in\nthe design process for virtual reality, presenting a more comprehensive view in\ndesigning future Cyberball prototypes and interactive systems for social\nscience research.",
            "author": [
                "Tao Long",
                "Swati Pandita",
                "Andrea Stevenson Won"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02897v1",
                "http://arxiv.org/pdf/2312.02897v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02896v2",
            "title": "BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal\n  Models",
            "updated": "2023-12-06T03:46:47Z",
            "published": "2023-12-05T17:06:59Z",
            "summary": "Large Multimodal Models (LMMs) such as GPT-4V and LLaVA have shown remarkable\ncapabilities in visual reasoning with common image styles. However, their\nrobustness against diverse style shifts, crucial for practical applications,\nremains largely unexplored. In this paper, we propose a new benchmark,\nBenchLMM, to assess the robustness of LMMs against three different styles:\nartistic image style, imaging sensor style, and application style, where each\nstyle has five sub-styles. Utilizing BenchLMM, we comprehensively evaluate\nstate-of-the-art LMMs and reveal: 1) LMMs generally suffer performance\ndegradation when working with other styles; 2) An LMM performs better than\nanother model in common style does not guarantee its superior performance in\nother styles; 3) LMMs' reasoning capability can be enhanced by prompting LMMs\nto predict the style first, based on which we propose a versatile and\ntraining-free method for improving LMMs; 4) An intelligent LMM is expected to\ninterpret the causes of its errors when facing stylistic variations. We hope\nthat our benchmark and analysis can shed new light on developing more\nintelligent and versatile LMMs.",
            "author": [
                "Rizhao Cai",
                "Zirui Song",
                "Dayan Guan",
                "Zhenhao Chen",
                "Xing Luo",
                "Chenyu Yi",
                "Alex Kot"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02896v2",
                "http://arxiv.org/pdf/2312.02896v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02891v1",
            "title": "Inexact linear solves in the low-rank ADI iteration for large Sylvester\n  equations",
            "updated": "2023-12-05T17:02:38Z",
            "published": "2023-12-05T17:02:38Z",
            "summary": "We consider the low-rank alternating directions implicit (ADI) iteration for\napproximately solving large-scale algebraic Sylvester equations. Inside every\niteration step of this iterative process a pair of linear systems of equations\nhas to be solved. We investigate the situation when those inner linear systems\nare solved inexactly by an iterative methods such as, for example,\npreconditioned Krylov subspace methods. The main contribution of this work are\nthresholds for the required accuracies regarding the inner linear systems which\ndictate when the employed inner Krylov subspace methods can be safely\nterminated. The goal is to save computational effort by solving the inner\nlinear system as inaccurate as possible without endangering the functionality\nof the low-rank Sylvester-ADI method. Ideally, the inexact ADI method mimics\nthe convergence behaviour of the more expensive exact ADI method, where the\nlinear systems are solved directly. Alongside the theoretical results, also\nstrategies for an actual practical implementation of the stopping criteria are\ndeveloped. Numerical experiments confirm the effectiveness of the proposed\nstrategies.",
            "author": [
                "Patrick K\u00fcrschner"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02891v1",
                "http://arxiv.org/pdf/2312.02891v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "15A06, 15A24, 65F45, 65F55"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03045v1",
            "title": "Customization Assistant for Text-to-image Generation",
            "updated": "2023-12-05T16:54:42Z",
            "published": "2023-12-05T16:54:42Z",
            "summary": "Customizing pre-trained text-to-image generation model has attracted massive\nresearch interest recently, due to its huge potential in real-world\napplications. Although existing methods are able to generate creative content\nfor a novel concept contained in single user-input image, their capability are\nstill far from perfection. Specifically, most existing methods require\nfine-tuning the generative model on testing images. Some existing methods do\nnot require fine-tuning, while their performance are unsatisfactory.\nFurthermore, the interaction between users and models are still limited to\ndirective and descriptive prompts such as instructions and captions. In this\nwork, we build a customization assistant based on pre-trained large language\nmodel and diffusion model, which can not only perform customized generation in\na tuning-free manner, but also enable more user-friendly interactions: users\ncan chat with the assistant and input either ambiguous text or clear\ninstruction. Specifically, we propose a new framework consists of a new model\ndesign and a novel training strategy. The resulting assistant can perform\ncustomized generation in 2-5 seconds without any test time fine-tuning.\nExtensive experiments are conducted, competitive results have been obtained\nacross different domains, illustrating the effectiveness of the proposed\nmethod.",
            "author": [
                "Yufan Zhou",
                "Ruiyi Zhang",
                "Jiuxiang Gu",
                "Tong Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03045v1",
                "http://arxiv.org/pdf/2312.03045v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02882v1",
            "title": "Zero Trust for Cyber Resilience",
            "updated": "2023-12-05T16:53:20Z",
            "published": "2023-12-05T16:53:20Z",
            "summary": "The increased connectivity and potential insider threats make traditional\nnetwork defense vulnerable. Instead of assuming that everything behind the\nsecurity perimeter is safe, the zero-trust security model verifies every\nincoming request before granting access. This chapter draws attention to the\ncyber resilience within the zero-trust model. We introduce the evolution from\ntraditional perimeter-based security to zero trust and discuss their\ndifference. Two key elements of the zero-trust engine are trust evaluation (TE)\nand policy engine (PE). We introduce the design of the two components and\ndiscuss how their interplay would contribute to cyber resilience. Dynamic game\ntheory and learning are applied as quantitative approaches to achieve automated\nzero-trust cyber resilience. Several case studies and implementations are\nintroduced to illustrate the benefits of such a security model.",
            "author": [
                "Yunfei Ge",
                "Quanyan Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02882v1",
                "http://arxiv.org/pdf/2312.02882v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02880v1",
            "title": "PULSAR: Simultaneous Many-Row Activation for Reliable and\n  High-Performance Computing in Off-the-Shelf DRAM Chips",
            "updated": "2023-12-05T16:52:20Z",
            "published": "2023-12-05T16:52:20Z",
            "summary": "Data movement between the processor and the main memory is a first-order\nobstacle against improving performance and energy efficiency in modern systems.\nTo address this obstacle, Processing-using-Memory (PuM) is a promising approach\nwhere bulk-bitwise operations are performed leveraging intrinsic analog\nproperties within the DRAM array and massive parallelism across DRAM columns.\nUnfortunately, 1) modern off-the-shelf DRAM chips do not officially support PuM\noperations, and 2) existing techniques of performing PuM operations on\noff-the-shelf DRAM chips suffer from two key limitations. First, these\ntechniques have low success rates, i.e., only a small fraction of DRAM columns\ncan correctly execute PuM operations because they operate beyond\nmanufacturer-recommended timing constraints, causing these operations to be\nhighly susceptible to noise and process variation. Second, these techniques\nhave limited compute primitives, preventing them from fully leveraging\nparallelism across DRAM columns and thus hindering their performance benefits.\n  We propose PULSAR, a new technique to enable high-success-rate and\nhigh-performance PuM operations in off-the-shelf DRAM chips. PULSAR leverages\nour new observation that a carefully crafted sequence of DRAM commands\nsimultaneously activates up to 32 DRAM rows. PULSAR overcomes the limitations\nof existing techniques by 1) replicating the input data to improve the success\nrate and 2) enabling new bulk bitwise operations (e.g., many-input majority,\nMulti-RowInit, and Bulk-Write) to improve the performance.\n  Our analysis on 120 off-the-shelf DDR4 chips from two major manufacturers\nshows that PULSAR achieves a 24.18% higher success rate and 121% higher\nperformance over seven arithmetic-logic operations compared to FracDRAM, a\nstate-of-the-art off-the-shelf DRAM-based PuM technique.",
            "author": [
                "Ismail Emir Yuksel",
                "Yahya Can Tugrul",
                "F. Nisa Bostanci",
                "Abdullah Giray Yaglikci",
                "Ataberk Olgun",
                "Geraldo F. Oliveira",
                "Melina Soysal",
                "Haocong Luo",
                "Juan Gomez Luna",
                "Mohammad Sadrosadati",
                "Onur Mutlu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02880v1",
                "http://arxiv.org/pdf/2312.02880v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02878v1",
            "title": "Towards More Practical Group Activity Detection: A New Benchmark and\n  Model",
            "updated": "2023-12-05T16:48:17Z",
            "published": "2023-12-05T16:48:17Z",
            "summary": "Group activity detection (GAD) is the task of identifying members of each\ngroup and classifying the activity of the group at the same time in a video.\nWhile GAD has been studied recently, there is still much room for improvement\nin both dataset and methodology due to their limited capability to address\npractical GAD scenarios. To resolve these issues, we first present a new\ndataset, dubbed Caf\\'e. Unlike existing datasets, Caf\\'e is constructed\nprimarily for GAD and presents more practical evaluation scenarios and metrics,\nas well as being large-scale and providing rich annotations. Along with the\ndataset, we propose a new GAD model that deals with an unknown number of groups\nand latent group members efficiently and effectively. We evaluated our model on\nthree datasets including Caf\\'e, where it outperformed previous work in terms\nof both accuracy and inference speed. Both our dataset and code base will be\nopen to the public to promote future research on GAD.",
            "author": [
                "Dongkeun Kim",
                "Youngkil Song",
                "Minsu Cho",
                "Suha Kwak"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02878v1",
                "http://arxiv.org/pdf/2312.02878v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02877v1",
            "title": "A Dynamic Network for Efficient Point Cloud Registration",
            "updated": "2023-12-05T16:47:46Z",
            "published": "2023-12-05T16:47:46Z",
            "summary": "For the point cloud registration task, a significant challenge arises from\nnon-overlapping points that consume extensive computational resources while\nnegatively affecting registration accuracy. In this paper, we introduce a\ndynamic approach, widely utilized to improve network efficiency in computer\nvision tasks, to the point cloud registration task. We employ an iterative\nregistration process on point cloud data multiple times to identify regions\nwhere matching points cluster, ultimately enabling us to remove noisy points.\nSpecifically, we begin with deep global sampling to perform coarse global\nregistration. Subsequently, we employ the proposed refined node proposal module\nto further narrow down the registration region and perform local registration.\nFurthermore, we utilize a spatial consistency-based classifier to evaluate the\nresults of each registration stage. The model terminates once it reaches\nsufficient confidence, avoiding unnecessary computations. Extended experiments\ndemonstrate that our model significantly reduces time consumption compared to\nother methods with similar results, achieving a speed improvement of over 41%\non indoor dataset (3DMatch) and 33% on outdoor datasets (KITTI) while\nmaintaining competitive registration recall requirements.",
            "author": [
                "Yang Ai",
                "Xi Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02877v1",
                "http://arxiv.org/pdf/2312.02877v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02872v1",
            "title": "Experimental Insights Towards Explainable and Interpretable Pedestrian\n  Crossing Prediction",
            "updated": "2023-12-05T16:39:32Z",
            "published": "2023-12-05T16:39:32Z",
            "summary": "In the context of autonomous driving, pedestrian crossing prediction is a key\ncomponent for improving road safety. Presently, the focus of these predictions\nextends beyond achieving trustworthy results; it is shifting towards the\nexplainability and interpretability of these predictions. This research\nintroduces a novel neuro-symbolic approach that combines deep learning and\nfuzzy logic for an explainable and interpretable pedestrian crossing\nprediction. We have developed an explainable predictor (ExPedCross), which\nutilizes a set of explainable features and employs a fuzzy inference system to\npredict whether the pedestrian will cross or not. Our approach was evaluated on\nboth the PIE and JAAD datasets. The results offer experimental insights into\nachieving explainability and interpretability in the pedestrian crossing\nprediction task. Furthermore, the testing results yield a set of guidelines and\nrecommendations regarding the process of dataset selection, feature selection,\nand explainability.",
            "author": [
                "Angie Nataly Melo",
                "Carlota Salinas",
                "Miguel Angel Sotelo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02872v1",
                "http://arxiv.org/pdf/2312.02872v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NE",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02871v1",
            "title": "Attention-enhanced neural differential equations for physics-informed\n  deep learning of ion transport",
            "updated": "2023-12-05T16:39:24Z",
            "published": "2023-12-05T16:39:24Z",
            "summary": "Species transport models typically combine partial differential equations\n(PDEs) with relations from hindered transport theory to quantify\nelectromigrative, convective, and diffusive transport through complex\nnanoporous systems; however, these formulations are frequently substantial\nsimplifications of the governing dynamics, leading to the poor generalization\nperformance of PDE-based models. Given the growing interest in deep learning\nmethods for the physical sciences, we develop a machine learning-based approach\nto characterize ion transport across nanoporous membranes. Our proposed\nframework centers around attention-enhanced neural differential equations that\nincorporate electroneutrality-based inductive biases to improve generalization\nperformance relative to conventional PDE-based methods. In addition, we study\nthe role of the attention mechanism in illuminating physically-meaningful\nion-pairing relationships across diverse mixture compositions. Further, we\ninvestigate the importance of pre-training on simulated data from PDE-based\nmodels, as well as the performance benefits from hard vs. soft inductive\nbiases. Our results indicate that physics-informed deep learning solutions can\noutperform their classical PDE-based counterparts and provide promising avenues\nfor modelling complex transport phenomena across diverse applications.",
            "author": [
                "Danyal Rehman",
                "John H. Lienhard"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02871v1",
                "http://arxiv.org/pdf/2312.02871v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math-ph",
                "math.MP",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02869v1",
            "title": "Can a Tabula Recta provide security in the XXI century?",
            "updated": "2023-12-05T16:36:27Z",
            "published": "2023-12-05T16:36:27Z",
            "summary": "In the not so unlikely scenario of total compromise of computers accessible\nto a group of users, they might be tempted to resort to human-computable\npaper-and-pencil cryptographic methods aided by a classic Tabula Recta, which\nhelps to perform addition and subtraction directly with letters. But do these\nclassic algorithms, or some new ones using the same simple tools, have any\nchance against computer-aided cryptanalysis? In this paper I discuss how some\nhuman-computable algorithms can indeed afford sufficient security in this\nsituation, drawing conclusions from computer-based statistical analysis. Three\nkinds of algorithms are discussed: those that concentrate entropy from shared\ntext sources, stream ciphers based on arithmetic of non-binary spaces, and\nhash-like algorithms that may be used to generate a password from a challenge\ntext.",
            "author": [
                "Francisco Ruiz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02869v1",
                "http://arxiv.org/pdf/2312.02869v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.CL",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02868v1",
            "title": "Multipole expansion at the level of the action in $d$-dimensions",
            "updated": "2023-12-05T16:31:26Z",
            "published": "2023-12-05T16:31:26Z",
            "summary": "In this paper we study the multipole expansion of the long-wavelength\neffective action for radiative sources in ($d$+1) spacetime dimensions. We\npresent detailed expressions for the multipole moments for the case of scalar-,\nelectromagnetic-, and (linearized) gravitational-wave emission. For\nelectromagnetism and gravity, we derive expressions for the odd-parity,\nmagnetic-type moments as SO($d$) duals of the ones traditionally used in the\nliterature. The $d$-dimensional case features a novel set of `Weyl-type'\nmoments, coupling to the spatial part of the Weyl tensor, which are absent in\nthree dimensions. Agreement is found in the overlap with previous known\nresults, notably in the $d \\to 3$ limit. Due to its reliance on dimensional\nregularization, the results presented here play a crucial role for the further\ndevelopment of the Effective Field Theory approach to gravitational dynamics,\nand in particular for the computation of the gravitational-wave flux, starting\nat the third post-Newtonian order.",
            "author": [
                "Loris Amalberti",
                "Fran\u00e7ois Larrouturou",
                "Zixin Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02868v1",
                "http://arxiv.org/pdf/2312.02868v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03044v1",
            "title": "REST: Enhancing Group Robustness in DNNs through Reweighted Sparse\n  Training",
            "updated": "2023-12-05T16:27:54Z",
            "published": "2023-12-05T16:27:54Z",
            "summary": "The deep neural network (DNN) has been proven effective in various domains.\nHowever, they often struggle to perform well on certain minority groups during\ninference, despite showing strong performance on the majority of data groups.\nThis is because over-parameterized models learned \\textit{bias attributes} from\na large number of \\textit{bias-aligned} training samples. These bias attributes\nare strongly spuriously correlated with the target variable, causing the models\nto be biased towards spurious correlations (i.e., \\textit{bias-conflicting}).\nTo tackle this issue, we propose a novel \\textbf{re}weighted \\textbf{s}parse\n\\textbf{t}raining framework, dubbed as \\textit{\\textbf{REST}}, which aims to\nenhance the performance of biased data while improving computation and memory\nefficiency. Our proposed REST framework has been experimentally validated on\nthree datasets, demonstrating its effectiveness in exploring unbiased\nsubnetworks. We found that REST reduces the reliance on spuriously correlated\nfeatures, leading to better performance across a wider range of data groups\nwith fewer training and inference resources. We highlight that the\n\\textit{REST} framework represents a promising approach for improving the\nperformance of DNNs on biased data, while simultaneously improving computation\nand memory efficiency. By reducing the reliance on spurious correlations, REST\nhas the potential to enhance the robustness of DNNs and improve their\ngeneralization capabilities. Code is released at\n\\url{https://github.com/zhao1402072392/REST}",
            "author": [
                "Jiaxu Zhao",
                "Lu Yin",
                "Shiwei Liu",
                "Meng Fang",
                "Mykola Pechenizkiy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03044v1",
                "http://arxiv.org/pdf/2312.03044v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03043v1",
            "title": "Navigating the Synthetic Realm: Harnessing Diffusion-based Models for\n  Laparoscopic Text-to-Image Generation",
            "updated": "2023-12-05T16:20:22Z",
            "published": "2023-12-05T16:20:22Z",
            "summary": "Recent advances in synthetic imaging open up opportunities for obtaining\nadditional data in the field of surgical imaging. This data can provide\nreliable supplements supporting surgical applications and decision-making\nthrough computer vision. Particularly the field of image-guided surgery, such\nas laparoscopic and robotic-assisted surgery, benefits strongly from synthetic\nimage datasets and virtual surgical training methods. Our study presents an\nintuitive approach for generating synthetic laparoscopic images from short text\nprompts using diffusion-based generative models. We demonstrate the usage of\nstate-of-the-art text-to-image architectures in the context of laparoscopic\nimaging with regard to the surgical removal of the gallbladder as an example.\nResults on fidelity and diversity demonstrate that diffusion-based models can\nacquire knowledge about the style and semantics in the field of image-guided\nsurgery. A validation study with a human assessment survey underlines the\nrealistic nature of our synthetic data, as medical personnel detects actual\nimages in a pool with generated images causing a false-positive rate of 66%. In\naddition, the investigation of a state-of-the-art machine learning model to\nrecognize surgical actions indicates enhanced results when trained with\nadditional generated images of up to 5.20%. Overall, the achieved image quality\ncontributes to the usage of computer-generated images in surgical applications\nand enhances its path to maturity.",
            "author": [
                "Simeon Allmendinger",
                "Patrick Hemmer",
                "Moritz Queisner",
                "Igor Sauer",
                "Leopold M\u00fcller",
                "Johannes Jakubik",
                "Michael V\u00f6ssing",
                "Niklas K\u00fchl"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03043v1",
                "http://arxiv.org/pdf/2312.03043v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.AI",
                "cs.CV",
                "q-bio.TO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02855v1",
            "title": "Exploring Error Bits for Memory Failure Prediction: An In-Depth\n  Correlative Study",
            "updated": "2023-12-05T16:11:52Z",
            "published": "2023-12-05T16:11:52Z",
            "summary": "In large-scale datacenters, memory failure is a common cause of server\ncrashes, with uncorrectable errors (UEs) being a major indicator of Dual Inline\nMemory Module (DIMM) defects. Existing approaches primarily focus on predicting\nUEs using correctable errors (CEs), without fully considering the information\nprovided by error bits. However, error bit patterns have a strong correlation\nwith the occurrence of uncorrectable errors (UEs). In this paper, we present a\ncomprehensive study on the correlation between CEs and UEs, specifically\nemphasizing the importance of spatio-temporal error bit information. Our\nanalysis reveals a strong correlation between spatio-temporal error bits and UE\noccurrence. Through evaluations using real-world datasets, we demonstrate that\nour approach significantly improves prediction performance by 15% in F1-score\ncompared to the state-of-the-art algorithms. Overall, our approach effectively\nreduces the number of virtual machine interruptions caused by UEs by\napproximately 59%.",
            "author": [
                "Qiao Yu",
                "Wengui Zhang",
                "Jorge Cardoso",
                "Odej Kao"
            ],
            "link": [
                "http://dx.doi.org/10.1109/ICCAD57390.2023.10323692",
                "http://arxiv.org/abs/2312.02855v1",
                "http://arxiv.org/pdf/2312.02855v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR",
                "cs.AI",
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02853v1",
            "title": "The Dual Pair $\\mathrm{Aut}(C)\\times F_{4}$ ($p$-adic case)",
            "updated": "2023-12-05T16:10:23Z",
            "published": "2023-12-05T16:10:23Z",
            "summary": "We study the local theta correspondence for dual pairs of the form\n$\\mathrm{Aut}(C)\\times F_{4}$ over a $p$-adic field, where $C$ is a composition\nalgebra of dimension 2 or 4, by restricting the minimal representation of a\ngroup of type $E$. We investigate this restriction through the computation of\nmaximal parabolic Jacquet modules and the Fourier-Jacobi functor.\n  As a consequence of our results we prove a multiplicity one result for the\n$\\mathrm{Spin}(9)$-invariant linear functionals of irreducible representations\nof $F_{4}$ and classify the $\\mathrm{Spin}(9)$-distinguished representations.",
            "author": [
                "Edmund Karasiewicz",
                "Gordan Savin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02853v1",
                "http://arxiv.org/pdf/2312.02853v1"
            ],
            "primary_category": "math.RT",
            "category": [
                "math.RT",
                "math.NT",
                "11F27, 22E50"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03042v1",
            "title": "Inherent limitations of LLMs regarding spatial information",
            "updated": "2023-12-05T16:02:20Z",
            "published": "2023-12-05T16:02:20Z",
            "summary": "Despite the significant advancements in natural language processing\ncapabilities demonstrated by large language models such as ChatGPT, their\nproficiency in comprehending and processing spatial information, especially\nwithin the domains of 2D and 3D route planning, remains notably underdeveloped.\nThis paper investigates the inherent limitations of ChatGPT and similar models\nin spatial reasoning and navigation-related tasks, an area critical for\napplications ranging from autonomous vehicle guidance to assistive technologies\nfor the visually impaired. In this paper, we introduce a novel evaluation\nframework complemented by a baseline dataset, meticulously crafted for this\nstudy. This dataset is structured around three key tasks: plotting spatial\npoints, planning routes in two-dimensional (2D) spaces, and devising pathways\nin three-dimensional (3D) environments. We specifically developed this dataset\nto assess the spatial reasoning abilities of ChatGPT. Our evaluation reveals\nkey insights into the model's capabilities and limitations in spatial\nunderstanding.",
            "author": [
                "He Yan",
                "Xinyao Hu",
                "Xiangpeng Wan",
                "Chengyu Huang",
                "Kai Zou",
                "Shiqi Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03042v1",
                "http://arxiv.org/pdf/2312.03042v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02847v2",
            "title": "A complex-projected Rayleigh quotient iteration for targeting interior\n  eigenvalues",
            "updated": "2023-12-06T08:10:27Z",
            "published": "2023-12-05T16:01:00Z",
            "summary": "We introduce a new Projected Rayleigh Quotient Iteration aimed at improving\nthe convergence behaviour of classic Rayleigh Quotient iteration (RQI) by\nincorporating approximate information about the target eigenvector at each\nstep. While classic RQI exhibits local cubic convergence for Hermitian\nmatrices, its global behaviour can be unpredictable, whereby it may converge to\nan eigenvalue far away from the target, even when started with accurate initial\nconditions. This problem is exacerbated when the eigenvalues are closely\nspaced. The key idea of the new algorithm is at each step to add a\ncomplex-valued projection to the original matrix (that depends on the current\neigenvector approximation), such that the unwanted eigenvalues are lifted into\nthe complex plane while the target stays close to the real line, thereby\nincreasing the spacing between the target eigenvalue and the rest of the\nspectrum. Making better use of the eigenvector approximation leads to more\nrobust convergence behaviour and the new method converges reliably to the\ncorrect target eigenpair for a significantly wider range of initial vectors\nthan does classic RQI. We prove that the method converges locally cubically and\nwe present several numerical examples demonstrating the improved global\nconvergence behaviour. In particular, we apply it to compute eigenvalues in a\nband-gap spectrum of a Sturm-Liouville operator used to model photonic crystal\nfibres, where the target and unwanted eigenvalues are closely spaced. The\nexamples show that the new method converges to the desired eigenpair even when\nthe eigenvalue spacing is very small, often succeeding when classic RQI fails.",
            "author": [
                "Nils Friess",
                "Alexander D. Gilbert",
                "Robert Scheichl"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02847v2",
                "http://arxiv.org/pdf/2312.02847v2"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02846v1",
            "title": "MATLAB-based general approach for square-root extended-unscented and\n  fifth-degree cubature Kalman filtering methods",
            "updated": "2023-12-05T15:59:01Z",
            "published": "2023-12-05T15:59:01Z",
            "summary": "A stable square-root approach has been recently proposed for the unscented\nKalman filter (UKF) and fifth-degree cubature Kalman filter (5D-CKF) as well as\nfor the mixed-type methods consisting of the extended Kalman filter (EKF) time\nupdate and the UKF/5D-CKF measurement update steps. The mixed-type estimators\nprovide a good balance in trading between estimation accuracy and computational\ndemand because of the EKF moment differential equations involved. The key\nbenefit is a consolidation of reliable state mean and error covariance\npropagation by using delicate discretization error control while solving the\nEKF moment differential equations and an accurate measurement update according\nto the advanced UKF and/or 5D-CKF filtering strategies. Meanwhile the drawback\nof the previously proposed estimators is an utilization of sophisticated\nnumerical integration scheme with the built-in discretization error control\nthat is, in fact, a complicated and computationally costly tool. In contrast,\nwe design here the mixed-type methods that keep the same estimation quality but\nreduce a computational time significantly. The novel estimators elegantly\nutilize any MATLAB-based numerical integration scheme developed for solving\nordinary differential equations (ODEs) with the required accuracy tolerance\npre-defined by users. In summary, a simplicity of the suggested estimators,\ntheir numerical robustness with respect to roundoff due to the square-root form\nutilized as well as their estimation accuracy due to the MATLAB ODEs solvers\nwith discretization error control involved are the attractive features of the\nnovel estimators. The numerical experiments are provided for illustrating a\nperformance of the suggested methods in comparison with the existing ones.",
            "author": [
                "Maria V. Kulikova",
                "Gennady Yu. Kulikov"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.ejcon.2021.01.003",
                "http://arxiv.org/abs/2312.02846v1",
                "http://arxiv.org/pdf/2312.02846v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03041v1",
            "title": "Transformer-Based Deep Learning Model for Bored Pile Load-Deformation\n  Prediction in Bangkok Subsoil",
            "updated": "2023-12-05T15:54:13Z",
            "published": "2023-12-05T15:54:13Z",
            "summary": "This paper presents a novel deep learning model based on the transformer\narchitecture to predict the load-deformation behavior of large bored piles in\nBangkok subsoil. The model encodes the soil profile and pile features as\ntokenization input, and generates the load-deformation curve as output. The\nmodel also incorporates the previous sequential data of load-deformation curve\ninto the decoder to improve the prediction accuracy. The model also\nincorporates the previous sequential data of load-deformation curve into the\ndecoder. The model shows a satisfactory accuracy and generalization ability for\nthe load-deformation curve prediction, with a mean absolute error of 5.72% for\nthe test data. The model could also be used for parametric analysis and design\noptimization of piles under different soil and pile conditions, pile cross\nsection, pile length and type of pile.",
            "author": [
                "Sompote Youwai",
                "Chissanupong Thongnoo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03041v1",
                "http://arxiv.org/pdf/2312.03041v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02843v1",
            "title": "Are Vision Transformers More Data Hungry Than Newborn Visual Systems?",
            "updated": "2023-12-05T15:53:24Z",
            "published": "2023-12-05T15:53:24Z",
            "summary": "Vision transformers (ViTs) are top performing models on many computer vision\nbenchmarks and can accurately predict human behavior on object recognition\ntasks. However, researchers question the value of using ViTs as models of\nbiological learning because ViTs are thought to be more data hungry than\nbrains, with ViTs requiring more training data to reach similar levels of\nperformance. To test this assumption, we directly compared the learning\nabilities of ViTs and animals, by performing parallel controlled rearing\nexperiments on ViTs and newborn chicks. We first raised chicks in impoverished\nvisual environments containing a single object, then simulated the training\ndata available in those environments by building virtual animal chambers in a\nvideo game engine. We recorded the first-person images acquired by agents\nmoving through the virtual chambers and used those images to train self\nsupervised ViTs that leverage time as a teaching signal, akin to biological\nvisual systems. When ViTs were trained through the eyes of newborn chicks, the\nViTs solved the same view invariant object recognition tasks as the chicks.\nThus, ViTs were not more data hungry than newborn visual systems: both learned\nview invariant object representations in impoverished visual environments. The\nflexible and generic attention based learning mechanism in ViTs combined with\nthe embodied data streams available to newborn animals appears sufficient to\ndrive the development of animal-like object recognition.",
            "author": [
                "Lalit Pandey",
                "Samantha M. W. Wood",
                "Justin N. Wood"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02843v1",
                "http://arxiv.org/pdf/2312.02843v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02839v1",
            "title": "Low-complexity Linear Multicast Beamforming for Cache-aided MIMO\n  Communications",
            "updated": "2023-12-05T15:45:19Z",
            "published": "2023-12-05T15:45:19Z",
            "summary": "A practical and scalable multicast beamformer design in multi-input\nmulti-output~(MIMO) coded caching~(CC) systems is introduced in this paper. The\nproposed approach allows multicast transmission to multiple groups with\npartially overlapping user sets using receiver dimensions to distinguish\nbetween different group-specific streams. Additionally, it provides flexibility\nin accommodating various parameter configurations of the MIMO-CC setup and\novercomes practical limitations, such as the requirement to use successive\ninterference cancellation~(SIC) at the receiver, while achieving the same\ndegrees-of-freedom~(DoF). To evaluate the proposed scheme, we define the\nsymmetric rate as the sum rate of the partially overlapping streams received\nper user, comprising a linear multistream multicast transmission vector and the\nlinear minimum mean square error~(LMMSE) receiver. The resulting non-convex\nsymmetric rate maximization problem is solved using alternative optimization\nand successive convex approximation~(SCA). Moreover, a fast iterative\nLagrangian-based algorithm is developed, significantly reducing the\ncomputational overhead compared to previous designs. The effectiveness of our\nproposed method is demonstrated by extensive simulations.",
            "author": [
                "Mohammad NaseriTehrani",
                "MohammadJavad Salehi",
                "Antti T\u00f6lli"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02839v1",
                "http://arxiv.org/pdf/2312.02839v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02838v1",
            "title": "The $2\\times 2$-upper triangular matrix algebra and its generalized\n  polynomial identities",
            "updated": "2023-12-05T15:44:54Z",
            "published": "2023-12-05T15:44:54Z",
            "summary": "Let $UT_2$ be the algebra of $2\\times 2$ upper triangular matrices over a\nfield $F$ of characteristic zero. Here we study the generalized polynomial\nidentities of $UT_2$, i.e., identical relations holding for $UT_2$ regarded as\n$UT_2$-algebra. We determine a set of two generators of the $T_{UT_2}$-ideal of\ngeneralized polynomial identities of $UT_2$ and compute the exact values of the\ncorresponding sequence of generalized codimensions. Moreover, we give a\ncomplete description of the space of multilinear generalized identities in $n$\nvariables in the language of Young diagrams through the representation theory\nof the symmetric group $S_n$. Finally, we prove that, unlike in the ordinary\ncase, the generalized variety of $UT_2$-algebras generated by $UT_2$ has no\nalmost polynomial growth; nevertheless, we exhibit two distinct generalized\nvarieties of almost polynomial growth.",
            "author": [
                "F. Martino",
                "C. Rizzo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02838v1",
                "http://arxiv.org/pdf/2312.02838v1"
            ],
            "primary_category": "math.RA",
            "category": [
                "math.RA",
                "16R10, 16R50 (Primary) 16P90, 20C30 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02835v1",
            "title": "Design and performance of a Collimated Beam Projector for telescope\n  transmission measurement using a broadband light source",
            "updated": "2023-12-05T15:37:16Z",
            "published": "2023-12-05T15:37:16Z",
            "summary": "Type Ia supernovae are one such cosmological probe to study dark energy, for\nwhich the dominant source of systematic uncertainties is the accuracy of the\nphotometric calibration. To address this, recent advancements introduce\nCollimated Beam Projectors (CBP), aiming to enhance calibration by precisely\nmeasuring a telescope's throughput as a function of wavelength. This work\ndescribes the performance of a prototype portable CBP. The experimental setup\nconsists of a broadband Xenon light source replacing a more customary but much\nmore demanding high-power laser source, coupled with a monochromator emitting\nlight inside an integrating sphere monitored with a photodiode and a\nspectrograph. Light is injected at the focus of the CBP telescope projecting a\ncollimated beam onto a solar cell whose quantum efficiency has been obtained by\ncomparison with a NIST-calibrated photodiode. The throughput and\nsignal-to-noise ratio achieved by comparing the photocurrent signal in the CBP\nphotodiode to the one in the solar cell are computed. We prove that the\nprototype, in its current state of development, is capable of achieving 1.2 per\ncent and 2.3 per cent precision on the integrated g and r bands of the ZTF\nphotometric filter system respectively, in a reasonable amount of integration\ntime. Central wavelength determination accuracy is kept below ~0.91 nm and\n~0.58 nm for g and r bands, respectively. The expected photometric uncertainty\ncaused by filter throughput measurement is approximately 5 mmag on the\nzero-point magnitude. Several straightforward improvement paths are discussed\nto upgrade the current setup.",
            "author": [
                "K. Sommer",
                "J. Cohen-Tanugi",
                "B. Plez",
                "M. Betoule",
                "S. Bongard",
                "L. Le Guillou",
                "J. Neveu",
                "E. Nuss",
                "E. Sepulveda",
                "T. Souverin",
                "M. Moniez",
                "C. W. Stubbs"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02835v1",
                "http://arxiv.org/pdf/2312.02835v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02831v1",
            "title": "Detection of Seismic Infrasonic Elephant Rumbles Using Spectrogram-Based\n  Machine Learning",
            "updated": "2023-12-05T15:26:14Z",
            "published": "2023-12-05T15:26:14Z",
            "summary": "This paper presents an effective method of identifying elephant rumbles in\ninfrasonic seismic signals. The design and implementation of electronic\ncircuitry to amplify, filter, and digitize the seismic signals captured through\ngeophones are presented. A collection of seismic infrasonic elephant rumbles\nwas collected at a free-ranging area of an elephant orphanage in Sri Lanka. The\nseismic rumbles were converted to spectrograms, and several methods were used\nfor spectral feature extraction. Using LasyPredict, the features extracted\nusing different methods were fed into their corresponding machine-learning\nalgorithms to train them for automatic seismic rumble identification. It was\nfound that the Mel frequency cepstral coefficient (MFCC) together with the\nRidge classifier machine learning algorithm produced the best performance in\nidentifying seismic elephant rumbles. A novel method for denoising the spectrum\nthat leads to enhanced accuracy in identifying seismic rumbles is also\npresented.",
            "author": [
                "A. M. J. V. Costa",
                "C. S. Pallikkonda",
                "H. H. R. Hiroshan",
                "G. R. U. Y. Gamlath",
                "S. R. Munasinghe",
                "C. U. S. Edussooriya"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02831v1",
                "http://arxiv.org/pdf/2312.02831v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02829v1",
            "title": "MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting\n  Computation in Superposition",
            "updated": "2023-12-05T15:25:45Z",
            "published": "2023-12-05T15:25:45Z",
            "summary": "With the advent of deep learning, progressively larger neural networks have\nbeen designed to solve complex tasks. We take advantage of these capacity-rich\nmodels to lower the cost of inference by exploiting computation in\nsuperposition. To reduce the computational burden per input, we propose\nMultiple-Input-Multiple-Output Neural Networks (MIMONets) capable of handling\nmany inputs at once. MIMONets augment various deep neural network architectures\nwith variable binding mechanisms to represent an arbitrary number of inputs in\na compositional data structure via fixed-width distributed representations.\nAccordingly, MIMONets adapt nonlinear neural transformations to process the\ndata structure holistically, leading to a speedup nearly proportional to the\nnumber of superposed input items in the data structure. After processing in\nsuperposition, an unbinding mechanism recovers each transformed input of\ninterest. MIMONets also provide a dynamic trade-off between accuracy and\nthroughput by an instantaneous on-demand switching between a set of\naccuracy-throughput operating points, yet within a single set of fixed\nparameters. We apply the concept of MIMONets to both CNN and Transformer\narchitectures resulting in MIMOConv and MIMOFormer, respectively. Empirical\nevaluations show that MIMOConv achieves about 2-4 x speedup at an accuracy\ndelta within [+0.68, -3.18]% compared to WideResNet CNNs on CIFAR10 and\nCIFAR100. Similarly, MIMOFormer can handle 2-4 inputs at once while maintaining\na high average accuracy within a [-1.07, -3.43]% delta on the long range arena\nbenchmark. Finally, we provide mathematical bounds on the interference between\nsuperposition channels in MIMOFormer. Our code is available at\nhttps://github.com/IBM/multiple-input-multiple-output-nets.",
            "author": [
                "Nicolas Menet",
                "Michael Hersche",
                "Geethan Karunaratne",
                "Luca Benini",
                "Abu Sebastian",
                "Abbas Rahimi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02829v1",
                "http://arxiv.org/pdf/2312.02829v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02828v1",
            "title": "Convergence Rates for Stochastic Approximation: Biased Noise with\n  Unbounded Variance, and Applications",
            "updated": "2023-12-05T15:22:39Z",
            "published": "2023-12-05T15:22:39Z",
            "summary": "The Stochastic Approximation (SA) algorithm introduced by Robbins and Monro\nin 1951 has been a standard method for solving equations of the form\n$\\mathbf{f}({\\boldsymbol {\\theta}}) = \\mathbf{0}$, when only noisy measurements\nof $\\mathbf{f}(\\cdot)$ are available. If $\\mathbf{f}({\\boldsymbol {\\theta}}) =\n\\nabla J({\\boldsymbol {\\theta}})$ for some function $J(\\cdot)$, then SA can\nalso be used to find a stationary point of $J(\\cdot)$. In much of the\nliterature, it is assumed that the error term ${\\boldsymbol {xi}}_{t+1}$ has\nzero conditional mean, and that its conditional variance is bounded as a\nfunction of $t$ (though not necessarily with respect to ${\\boldsymbol\n{\\theta}}_t$). Also, for the most part, the emphasis has been on\n``synchronous'' SA, whereby, at each time $t$, \\textit{every} component of\n${\\boldsymbol {\\theta}}_t$ is updated. Over the years, SA has been applied to a\nvariety of areas, out of which two are the focus in this paper: Convex and\nnonconvex optimization, and Reinforcement Learning (RL). As it turns out, in\nthese applications, the above-mentioned assumptions do not always hold. In\nzero-order methods, the error neither has zero mean nor bounded conditional\nvariance. In the present paper, we extend SA theory to encompass errors with\nnonzero conditional mean and/or unbounded conditional variance, and also\nasynchronous SA. In addition, we derive estimates for the rate of convergence\nof the algorithm. Then we apply the new results to problems in nonconvex\noptimization, and to Markovian SA, a recently emerging area in RL. We prove\nthat SA converges in these situations, and compute the ``optimal step size\nsequences'' to maximize the estimated rate of convergence.",
            "author": [
                "Rajeeva L. Karandikar",
                "M. Vidyasagar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02828v1",
                "http://arxiv.org/pdf/2312.02828v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "math.OC",
                "math.PR",
                "62L20, 60G17, 93D05"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02827v1",
            "title": "Computing $k$-Crossing Visibility through $k$-levels",
            "updated": "2023-12-05T15:22:26Z",
            "published": "2023-12-05T15:22:26Z",
            "summary": "Let $\\mathcal{A}$ be an arrangement of straight lines in the plane (or planes\nin $\\mathbb{R}^3$). The $k$-crossing visibility of a point $p$ on $\\mathcal{A}$\nis the set of point $q$ in elements of $\\mathcal{A}$ such that the segment $pq$\nintersects at most $k$ elements of $\\mathcal{A}$. In this paper, we obtain\nalgorithms for computing the $k$-crossing visibility. In particular we obtain\n$O(n\\log n + kn)$ and $O(n\\log n + k^2n)$ time algorithms, for arrangements of\nlines in the plane and planes in $\\mathbb{R}^3$; which are optimal for\n$k=\\Omega(\\log n)$ and $k=\\Omega(\\sqrt{\\log n})$, respectively. We also\nintroduce another algorithm for computing $k$-crossing visibilities on\npolygons, which reaches the same asymptotical time as the one presented by\nBahoo et al. The ideas introduced in this paper can be easily adapted for\nobtaining $k$-crossing visibilities on other arrangements whose $(\\leq\nk)$-level is known.",
            "author": [
                "Frank Duque"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02827v1",
                "http://arxiv.org/pdf/2312.02827v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03039v1",
            "title": "Insights into the first and second hydrostatic core stages from\n  numerical simulations",
            "updated": "2023-12-05T15:12:59Z",
            "published": "2023-12-05T15:12:59Z",
            "summary": "The theory of how low mass stars form from the collapse of a dense molecular\ncloud core has been well-established for decades. Thanks to significant\nprogress in computing and numerical modelling, more physical models have been\ndeveloped and a wider parameter space explored to understand the early stages\nof star formation more fully. In this review, I describe the expected physical\nproperties of the first and second core stages and how the inclusion of\ndifferent physics affects those predicted characteristics. I provide an\noverview of chemical models and synthetic observations, looking towards the\npositive identification of the first core in nature, which remains elusive.\nHowever, there are a few likely candidate first cores, which are listed, and I\nbriefly discuss the recent progress in characterising the youngest protostellar\nsources. Chemistry will be instrumental in the firm identification of the first\ncore so we require robust theoretical predictions of the chemical evolution of\nprotostellar cores, especially of the first and second core outflows. Looking\nahead, simulations can shed light on how the protostellar collapse phase shapes\nthe evolution of the protostellar disc. Simulations of dust evolution during\nprotostellar core collapse show there is significant enhancement in grain size\nand abundance towards the centre of the core. Chemical models show that the\nwarm, dense conditions of the first core drive chemical evolution. There is a\nwide scope for further study of the role that the first and second core stages\nplay in determining the structure and composition of the protostellar disc and\nenvelope and, of course, the eventual influence on the formation of planets.",
            "author": [
                "Alison K. Young"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03039v1",
                "http://arxiv.org/pdf/2312.03039v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR",
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02825v1",
            "title": "Energy-consistent integration of mechanical systems based on Livens\n  principle",
            "updated": "2023-12-05T15:11:06Z",
            "published": "2023-12-05T15:11:06Z",
            "summary": "In this work we make us of Livens principle (sometimes also referred to as\nHamilton-Pontryagin principle) in order to obtain a novel structure-preserving\nintegrator for mechanical systems. In contrast to the canonical Hamiltonian\nequations of motion, the Euler-Lagrange equations pertaining to Livens\nprinciple circumvent the need to invert the mass matrix. This is an essential\nadvantage with respect to singular mass matrices, which can yield severe\ndifficulties for the modelling and simulation of multibody systems. Moreover,\nLivens principle unifies both Lagrangian and Hamiltonian viewpoints on\nmechanics. Additionally, the present framework avoids the need to set up the\nsystem's Hamiltonian. The novel scheme algorithmically conserves a general\nenergy function and aims at the preservation of momentum maps corresponding to\nsymmetries of the system. We present an extension to mechanical systems subject\nto holonomic constraints. The performance of the newly devised method is\nstudied in representative examples.",
            "author": [
                "Philipp L. Kinon",
                "Peter Betsch"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02825v1",
                "http://arxiv.org/pdf/2312.02825v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE",
                "cs.NA",
                "math.DS",
                "math.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02821v1",
            "title": "RotaTR: Detection Transformer for Dense and Rotated Object",
            "updated": "2023-12-05T15:06:04Z",
            "published": "2023-12-05T15:06:04Z",
            "summary": "Detecting the objects in dense and rotated scenes is a challenging task.\nRecent works on this topic are mostly based on Faster RCNN or Retinanet. As\nthey are highly dependent on the pre-set dense anchors and the NMS operation,\nthe approach is indirect and suboptimal.The end-to-end DETR-based detectors\nhave achieved great success in horizontal object detection and many other areas\nlike segmentation, tracking, action recognition and etc.However, the DETR-based\ndetectors perform poorly on dense rotated target tasks and perform worse than\nmost modern CNN-based detectors. In this paper, we find the most significant\nreason for the poor performance is that the original attention can not\naccurately focus on the oriented targets. Accordingly, we propose Rotated\nobject detection TRansformer (RotaTR) as an extension of DETR to oriented\ndetection. Specifically, we design Rotation Sensitive deformable (RSDeform)\nattention to enhance the DETR's ability to detect oriented targets. It is used\nto build the feature alignment module and rotation-sensitive decoder for our\nmodel. We test RotaTR on four challenging-oriented benchmarks. It shows a great\nadvantage in detecting dense and oriented objects compared to the original\nDETR. It also achieves competitive results when compared to the\nstate-of-the-art.",
            "author": [
                "Zhu Yuke",
                "Ruan Yumeng",
                "Yang Lei",
                "Guo Sheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02821v1",
                "http://arxiv.org/pdf/2312.02821v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03038v1",
            "title": "Sample-based Dynamic Hierarchical Transformer with Layer and Head\n  Flexibility via Contextual Bandit",
            "updated": "2023-12-05T15:04:11Z",
            "published": "2023-12-05T15:04:11Z",
            "summary": "Transformer requires a fixed number of layers and heads which makes them\ninflexible to the complexity of individual samples and expensive in training\nand inference. To address this, we propose a sample-based Dynamic Hierarchical\nTransformer (DHT) model whose layers and heads can be dynamically configured\nwith single data samples via solving contextual bandit problems. To determine\nthe number of layers and heads, we use the Uniform Confidence Bound while we\ndeploy combinatorial Thompson Sampling in order to select specific head\ncombinations given their number. Different from previous work that focuses on\ncompressing trained networks for inference only, DHT is not only advantageous\nfor adaptively optimizing the underlying network architecture during training\nbut also has a flexible network for efficient inference. To the best of our\nknowledge, this is the first comprehensive data-driven dynamic transformer\nwithout any additional auxiliary neural networks that implement the dynamic\nsystem. According to the experiment results, we achieve up to 74% computational\nsavings for both training and inference with a minimal loss of accuracy.",
            "author": [
                "Fanfei Meng",
                "Lele Zhang",
                "Yu Chen",
                "Yuxin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03038v1",
                "http://arxiv.org/pdf/2312.03038v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02820v1",
            "title": "Clustering Pseudo Language Family in Multilingual Translation Models\n  with Fisher Information Matrix",
            "updated": "2023-12-05T15:03:27Z",
            "published": "2023-12-05T15:03:27Z",
            "summary": "In multilingual translation research, the comprehension and utilization of\nlanguage families are of paramount importance. Nevertheless, clustering\nlanguages based solely on their ancestral families can yield suboptimal results\ndue to variations in the datasets employed during the model's training phase.\nTo mitigate this challenge, we introduce an innovative method that leverages\nthe fisher information matrix (FIM) to cluster language families, anchored on\nthe multilingual translation model's characteristics. We hypothesize that\nlanguage pairs with similar effects on model parameters exhibit a considerable\ndegree of linguistic congruence and should thus be grouped cohesively. This\nconcept has led us to define pseudo language families. We provide an in-depth\ndiscussion regarding the inception and application of these pseudo language\nfamilies. Empirical evaluations reveal that employing these pseudo language\nfamilies enhances performance over conventional language families in adapting a\nmultilingual translation model to unfamiliar language pairs. The proposed\nmethodology may also be extended to scenarios requiring language similarity\nmeasurements. The source code and associated scripts can be accessed at\nhttps://github.com/ecoli-hit/PseudoFamily.",
            "author": [
                "Xinyu Ma",
                "Xuebo Liu",
                "Min Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02820v1",
                "http://arxiv.org/pdf/2312.02820v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02819v1",
            "title": "Deterministic Guidance Diffusion Model for Probabilistic Weather\n  Forecasting",
            "updated": "2023-12-05T15:03:15Z",
            "published": "2023-12-05T15:03:15Z",
            "summary": "Weather forecasting requires not only accuracy but also the ability to\nperform probabilistic prediction. However, deterministic weather forecasting\nmethods do not support probabilistic predictions, and conversely, probabilistic\nmodels tend to be less accurate. To address these challenges, in this paper, we\nintroduce the \\textbf{\\textit{D}}eterministic \\textbf{\\textit{G}}uidance\n\\textbf{\\textit{D}}iffusion \\textbf{\\textit{M}}odel (DGDM) for probabilistic\nweather forecasting, integrating benefits of both deterministic and\nprobabilistic approaches. During the forward process, both the deterministic\nand probabilistic models are trained end-to-end. In the reverse process,\nweather forecasting leverages the predicted result from the deterministic\nmodel, using as an intermediate starting point for the probabilistic model. By\nfusing deterministic models with probabilistic models in this manner, DGDM is\ncapable of providing accurate forecasts while also offering probabilistic\npredictions. To evaluate DGDM, we assess it on the global weather forecasting\ndataset (WeatherBench) and the common video frame prediction benchmark (Moving\nMNIST). We also introduce and evaluate the Pacific Northwest Windstorm\n(PNW)-Typhoon weather satellite dataset to verify the effectiveness of DGDM in\nhigh-resolution regional forecasting. As a result of our experiments, DGDM\nachieves state-of-the-art results not only in global forecasting but also in\nregional forecasting. The code is available at:\n\\url{https://github.com/DongGeun-Yoon/DGDM}.",
            "author": [
                "Donggeun Yoon",
                "Minseok Seo",
                "Doyi Kim",
                "Yeji Choi",
                "Donghyeon Cho"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02819v1",
                "http://arxiv.org/pdf/2312.02819v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02818v1",
            "title": "Optimally combined incentive for cooperation among interacting agents in\n  population games",
            "updated": "2023-12-05T15:01:57Z",
            "published": "2023-12-05T15:01:57Z",
            "summary": "Combined prosocial incentives, integrating reward for cooperators and\npunishment for defectors, are effective tools to promote cooperation among\ncompeting agents in population games. Existing research concentrated on how to\nadjust reward or punishment, as two mutually exclusive tools, during the\nevolutionary process to achieve the desired proportion of cooperators in the\npopulation, and less attention has been given to exploring a combined\nincentive-based control policy that can steer the system to the full\ncooperation state at the lowest cost. In this work we propose a combined\nincentive scheme in a population of agents whose conflicting interactions are\ndescribed by the prisoner's dilemma game on complete graphs and regular\nnetworks, respectively. By devising an index function for quantifying the\nimplementation cost of the combined incentives, we analytically construct the\noptimally combined incentive protocol by using optimal control theory. By means\nof theoretical analysis, we identify the mathematical conditions, under which\nthe optimally combined incentive scheme requires the minimal amount of cost. In\naddition to numerical calculations, we further perform computer simulations to\nverify our theoretical results and explore their robustness on different types\nof network structures.",
            "author": [
                "Shengxian Wang",
                "Ming Cao",
                "Xiaojie Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02818v1",
                "http://arxiv.org/pdf/2312.02818v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02813v1",
            "title": "BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis\n  via Bridging Image and Video Diffusion Models",
            "updated": "2023-12-05T14:56:55Z",
            "published": "2023-12-05T14:56:55Z",
            "summary": "Diffusion models have made tremendous progress in text-driven image and video\ngeneration. Now text-to-image foundation models are widely applied to various\ndownstream image synthesis tasks, such as controllable image generation and\nimage editing, while downstream video synthesis tasks are less explored for\nseveral reasons. First, it requires huge memory and compute overhead to train a\nvideo generation foundation model. Even with video foundation models,\nadditional costly training is still required for downstream video synthesis\ntasks. Second, although some works extend image diffusion models into videos in\na training-free manner, temporal consistency cannot be well kept. Finally,\nthese adaption methods are specifically designed for one task and fail to\ngeneralize to different downstream video synthesis tasks. To mitigate these\nissues, we propose a training-free general-purpose video synthesis framework,\ncoined as BIVDiff, via bridging specific image diffusion models and general\ntext-to-video foundation diffusion models. Specifically, we first use an image\ndiffusion model (like ControlNet, Instruct Pix2Pix) for frame-wise video\ngeneration, then perform Mixed Inversion on the generated video, and finally\ninput the inverted latents into the video diffusion model for temporal\nsmoothing. Decoupling image and video models enables flexible image model\nselection for different purposes, which endows the framework with strong task\ngeneralization and high efficiency. To validate the effectiveness and general\nuse of BIVDiff, we perform a wide range of video generation tasks, including\ncontrollable video generation video editing, video inpainting and outpainting.\nOur project page is available at https://bivdiff.github.io.",
            "author": [
                "Fengyuan Shi",
                "Jiaxi Gu",
                "Hang Xu",
                "Songcen Xu",
                "Wei Zhang",
                "Limin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02813v1",
                "http://arxiv.org/pdf/2312.02813v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02812v1",
            "title": "Simulating Vision Impairment in Virtual Reality -- A Comparison of\n  Visual Task Performance with Real and Simulated Tunnel Vision",
            "updated": "2023-12-05T14:56:12Z",
            "published": "2023-12-05T14:56:12Z",
            "summary": "Purpose: In this work, we explore the potential and limitations of simulating\ngaze-contingent tunnel vision conditions using Virtual Reality (VR) with\nbuilt-in eye tracking technology. This approach promises an easy and accessible\nway of expanding study populations and test groups for visual training, visual\naids, or accessibility evaluations. However, it is crucial to assess the\nvalidity and reliability of simulating these types of visual impairments and\nevaluate the extend to which participants with simulated tunnel vision can\nrepresent real patients. Methods: Two age-matched participant groups were\nacquired: The first group (n=8 aged 20-60, average 49.1, sd 13.2) consisted of\npatients diagnosed with Retinitis pigmentosa (RP). The second group (n=8, aged\n27-59, average 46.5, sd 10.8) consisted of visually healthy participants with\nsimulated tunnel vision. Both groups carried out different visual tasks in a\nvirtual environment for 30 minutes per day over the course of four weeks. Task\nperformances as well as gaze characteristics were evaluated in both groups over\nthe course of the study. Results: Using the \"two one-sided tests for\nequivalence\" method, the two groups were found to perform similar in all three\nvisual tasks. Significant differences between groups were found in different\naspects of their gaze behavior, though most of these aspects seem to converge\nover time. Conclusion: Our study evaluates the potential and limitations of\nusing Virtual Reality technology to simulate the effects of tunnel vision\nwithin controlled virtual environments. We find that the simulation accurately\nrepresents performance of RP patients in the context of group averages, but\nfails to fully replicate effects on gaze behavior.",
            "author": [
                "Alexander Neugebauer",
                "Nora Castner",
                "Bj\u00f6rn Severitt",
                "Katarina Stingl",
                "Iliya Ivanov",
                "Siegfried Wahl"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02812v1",
                "http://arxiv.org/pdf/2312.02812v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02809v1",
            "title": "Semi-implicit Continuous Newton Method for Power Flow Analysis",
            "updated": "2023-12-05T14:52:06Z",
            "published": "2023-12-05T14:52:06Z",
            "summary": "This paper proposes a semi-implicit version of continuous Newton method (CNM)\nfor power flow analysis. The proposed method succeeds the numerical robustness\nfrom the implicit CNM (ICNM) framework while prevents the iterative solution of\nnonlinear systems, hence revealing higher convergence speed and computation\nefficiency. The intractability of ICNM consists in its nonlinear implicit\nordinary-differential-equation (ODE) nature. We circumvent this by introducing\nintermediate variables, hence converting the implicit ODEs into differential\nalgebraic equations (DAEs), and solve the DAEs with a linear scheme, the\nstiffly accurate Rosenbrock type method (SARM). A new 4-stage 3rd-order\nhyper-stable SARM, together with a 2nd-order embedded formula to control the\nstep size, is constructed. Case studies on system 9241pegase verified the\nalleged performance.",
            "author": [
                "Ruizhi Yu",
                "Wei Gu",
                "Shuai Lu",
                "Yijun Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02809v1",
                "http://arxiv.org/pdf/2312.02809v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02807v1",
            "title": "Online Change Detection in SAR Time-Series with Kronecker Product\n  Structured Scaled Gaussian Models",
            "updated": "2023-12-05T14:49:53Z",
            "published": "2023-12-05T14:49:53Z",
            "summary": "We develop the information geometry of scaled Gaussian distributions for\nwhich the covariance matrix exhibits a Kronecker product structure. This model\nand its geometry are then used to propose an online change detection (CD)\nalgorithm for multivariate image times series (MITS). The proposed approach\nrelies mainly on the online estimation of the structured covariance matrix\nunder the null hypothesis, which is performed through a recursive (natural)\nRiemannian gradient descent. This approach exhibits a practical interest\ncompared to the corresponding offline version, as its computational cost\nremains constant for each new image added in the time series. Simulations show\nthat the proposed recursive estimators reach the Intrinsic Cram\\'er-Rao bound.\nThe interest of the proposed online CD approach is demonstrated on both\nsimulated and real data.",
            "author": [
                "Ammar Mian",
                "Guillaume Ginolhac",
                "Florent Bouchard",
                "Arnaud Breloy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02807v1",
                "http://arxiv.org/pdf/2312.02807v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02803v1",
            "title": "Leveraging Domain Adaptation and Data Augmentation to Improve Qur'anic\n  IR in English and Arabic",
            "updated": "2023-12-05T14:44:08Z",
            "published": "2023-12-05T14:44:08Z",
            "summary": "In this work, we approach the problem of Qur'anic information retrieval (IR)\nin Arabic and English. Using the latest state-of-the-art methods in neural IR,\nwe research what helps to tackle this task more efficiently. Training retrieval\nmodels requires a lot of data, which is difficult to obtain for training\nin-domain. Therefore, we commence with training on a large amount of general\ndomain data and then continue training on in-domain data. To handle the lack of\nin-domain data, we employed a data augmentation technique, which considerably\nimproved results in MRR@10 and NDCG@5 metrics, setting the state-of-the-art in\nQur'anic IR for both English and Arabic. The absence of an Islamic corpus and\ndomain-specific model for IR task in English motivated us to address this lack\nof resources and take preliminary steps of the Islamic corpus compilation and\ndomain-specific language model (LM) pre-training, which helped to improve the\nperformance of the retrieval models that use the domain-specific LM as the\nshared backbone. We examined several language models (LMs) in Arabic to select\none that efficiently deals with the Qur'anic IR task. Besides transferring\nsuccessful experiments from English to Arabic, we conducted additional\nexperiments with retrieval task in Arabic to amortize the scarcity of general\ndomain datasets used to train the retrieval models. Handling Qur'anic IR task\ncombining English and Arabic allowed us to enhance the comparison and share\nvaluable insights across models and languages.",
            "author": [
                "Vera Pavlova"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02803v1",
                "http://arxiv.org/pdf/2312.02803v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03036v1",
            "title": "Loop correction and resummation of vertex functions for a self\n  interacting scalar field in the de Sitter spacetime",
            "updated": "2023-12-05T14:43:21Z",
            "published": "2023-12-05T14:43:21Z",
            "summary": "We consider a massless and minimally coupled self interacting quantum scalar\nfield theory in the inflationary de Sitter background of dimension four. The\nself interaction potential is taken to be either quartic, $\\lambda \\phi^4/4!$,\nor quartic plus cubic, $\\lambda \\phi^4/4!+\\beta \\phi^3/3!$ ($\\lambda\n\\,{\\ensuremath >}\\,0$). We compute the four and three point vertex functions up\nto two loop. The purely local or partly local part of these renormalised loop\ncorrected vertex functions grow unboundedly after sufficient number of de\nSitter $e$-foldings, due to the appearances of secular logarithms. We focus on\nthe purely local part of the vertex functions and attempt a resummation of them\nin terms of the dynamically generated mass of the scalar field at late times.\nIt turns out that the resummed, non-perturbative effective vertex functions\nhave values less than that of the tree level. The variation of these vertex\nfunctions are investigated with respect to the tree level couplings\nnumerically. Since neither the secular effect, nor the dynamical generation of\nfield mass is possible in the Minkowski spacetime, the above phenomenon has no\nflat spacetime analogue.",
            "author": [
                "Sourav Bhattacharya",
                "Sudesh Kumar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03036v1",
                "http://arxiv.org/pdf/2312.03036v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02798v1",
            "title": "Weakly Supervised Detection of Hallucinations in LLM Activations",
            "updated": "2023-12-05T14:35:11Z",
            "published": "2023-12-05T14:35:11Z",
            "summary": "We propose an auditing method to identify whether a large language model\n(LLM) encodes patterns such as hallucinations in its internal states, which may\npropagate to downstream tasks. We introduce a weakly supervised auditing\ntechnique using a subset scanning approach to detect anomalous patterns in LLM\nactivations from pre-trained models. Importantly, our method does not need\nknowledge of the type of patterns a-priori. Instead, it relies on a reference\ndataset devoid of anomalies during testing. Further, our approach enables the\nidentification of pivotal nodes responsible for encoding these patterns, which\nmay offer crucial insights for fine-tuning specific sub-networks for bias\nmitigation. We introduce two new scanning methods to handle LLM activations for\nanomalous sentences that may deviate from the expected distribution in either\ndirection. Our results confirm prior findings of BERT's limited internal\ncapacity for encoding hallucinations, while OPT appears capable of encoding\nhallucination information internally. Importantly, our scanning approach,\nwithout prior exposure to false statements, performs comparably to a fully\nsupervised out-of-distribution classifier.",
            "author": [
                "Miriam Rateike",
                "Celia Cintas",
                "John Wamburu",
                "Tanya Akumu",
                "Skyler Speakman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02798v1",
                "http://arxiv.org/pdf/2312.02798v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02791v1",
            "title": "Unsupervised learning on spontaneous retinal activity leads to efficient\n  neural representation geometry",
            "updated": "2023-12-05T14:22:46Z",
            "published": "2023-12-05T14:22:46Z",
            "summary": "Prior to the onset of vision, neurons in the developing mammalian retina\nspontaneously fire in correlated activity patterns known as retinal waves.\nExperimental evidence suggests that retinal waves strongly influence the\nemergence of sensory representations before visual experience. We aim to model\nthis early stage of functional development by using movies of neurally active\ndeveloping retinas as pre-training data for neural networks. Specifically, we\npre-train a ResNet-18 with an unsupervised contrastive learning objective\n(SimCLR) on both simulated and experimentally-obtained movies of retinal waves,\nthen evaluate its performance on image classification tasks. We find that\npre-training on retinal waves significantly improves performance on tasks that\ntest object invariance to spatial translation, while slightly improving\nperformance on more complex tasks like image classification. Notably, these\nperformance boosts are realized on held-out natural images even though the\npre-training procedure does not include any natural image data. We then propose\na geometrical explanation for the increase in network performance, namely that\nthe spatiotemporal characteristics of retinal waves facilitate the formation of\nseparable feature representations. In particular, we demonstrate that networks\npre-trained on retinal waves are more effective at separating image manifolds\nthan randomly initialized networks, especially for manifolds defined by sets of\nspatial translations. These findings indicate that the broad spatiotemporal\nproperties of retinal waves prepare networks for higher order feature\nextraction.",
            "author": [
                "Andrew Ligeralde",
                "Yilun Kuang",
                "Thomas Edward Yerxa",
                "Miah N. Pitcher",
                "Marla Feller",
                "SueYeon Chung"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02791v1",
                "http://arxiv.org/pdf/2312.02791v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02788v1",
            "title": "Low-rank Monte Carlo for Smoluchowski-class equations",
            "updated": "2023-12-05T14:18:39Z",
            "published": "2023-12-05T14:18:39Z",
            "summary": "The work discusses a new low-rank Monte Carlo technique to solve\nSmoluchowski-like kinetic equations. It drastically decreases the computational\ncomplexity of modeling of size-polydisperse systems. For the studied systems it\ncan outperform the existing methods by more than ten times; its superiority\nfurther grows with increasing system size. Application to the recently\ndeveloped temperature-dependent Smoluchowski equations is also demonstrated.",
            "author": [
                "Alexander Osinsky"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02788v1",
                "http://arxiv.org/pdf/2312.02788v1"
            ],
            "primary_category": "cond-mat.stat-mech",
            "category": [
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02786v1",
            "title": "Machine Learning Driven Sensitivity Analysis of E3SM Land Model\n  Parameters for Wetland Methane Emissions",
            "updated": "2023-12-05T14:16:13Z",
            "published": "2023-12-05T14:16:13Z",
            "summary": "Methane (CH4) is the second most critical greenhouse gas after carbon\ndioxide, contributing to 16-25% of the observed atmospheric warming. Wetlands\nare the primary natural source of methane emissions globally. However, wetland\nmethane emission estimates from biogeochemistry models contain considerable\nuncertainty. One of the main sources of this uncertainty arises from the\nnumerous uncertain model parameters within various physical, biological, and\nchemical processes that influence methane production, oxidation, and transport.\nSensitivity Analysis (SA) can help identify critical parameters for methane\nemission and achieve reduced biases and uncertainties in future projections.\nThis study performs SA for 19 selected parameters responsible for critical\nbiogeochemical processes in the methane module of the Energy Exascale Earth\nSystem Model (E3SM) land model (ELM). The impact of these parameters on various\nCH4 fluxes is examined at 14 FLUXNET- CH4 sites with diverse vegetation types.\nGiven the extensive number of model simulations needed for global\nvariance-based SA, we employ a machine learning (ML) algorithm to emulate the\ncomplex behavior of ELM methane biogeochemistry. ML enables the computational\ntime to be shortened significantly from 6 CPU hours to 0.72 milliseconds,\nachieving reduced computational costs. We found that parameters linked to CH4\nproduction and diffusion generally present the highest sensitivities despite\napparent seasonal variation. Comparing simulated emissions from perturbed\nparameter sets against FLUXNET-CH4 observations revealed that better\nperformances can be achieved at each site compared to the default parameter\nvalues. This presents a scope for further improving simulated emissions using\nparameter calibration with advanced optimization techniques like Bayesian\noptimization.",
            "author": [
                "Sandeep Chinta",
                "Xiang Gao",
                "Qing Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02786v1",
                "http://arxiv.org/pdf/2312.02786v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph",
                "cs.LG",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02783v1",
            "title": "Large Language Models on Graphs: A Comprehensive Survey",
            "updated": "2023-12-05T14:14:27Z",
            "published": "2023-12-05T14:14:27Z",
            "summary": "Large language models (LLMs), such as ChatGPT and LLaMA, are creating\nsignificant advancements in natural language processing, due to their strong\ntext encoding/decoding ability and newly found emergent capability (e.g.,\nreasoning). While LLMs are mainly designed to process pure texts, there are\nmany real-world scenarios where text data are associated with rich structure\ninformation in the form of graphs (e.g., academic networks, and e-commerce\nnetworks) or scenarios where graph data are paired with rich textual\ninformation (e.g., molecules with descriptions). Besides, although LLMs have\nshown their pure text-based reasoning ability, it is underexplored whether such\nability can be generalized to graph scenarios (i.e., graph-based reasoning). In\nthis paper, we provide a systematic review of scenarios and techniques related\nto large language models on graphs. We first summarize potential scenarios of\nadopting LLMs on graphs into three categories, namely pure graphs, text-rich\ngraphs, and text-paired graphs. We then discuss detailed techniques for\nutilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM\nas Aligner, and compare the advantages and disadvantages of different schools\nof models. Furthermore, we mention the real-world applications of such methods\nand summarize open-source codes and benchmark datasets. Finally, we conclude\nwith potential future research directions in this fast-growing field. The\nrelated source can be found at\nhttps://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.",
            "author": [
                "Bowen Jin",
                "Gang Liu",
                "Chi Han",
                "Meng Jiang",
                "Heng Ji",
                "Jiawei Han"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02783v1",
                "http://arxiv.org/pdf/2312.02783v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02781v1",
            "title": "PMMTalk: Speech-Driven 3D Facial Animation from Complementary Pseudo\n  Multi-modal Features",
            "updated": "2023-12-05T14:12:38Z",
            "published": "2023-12-05T14:12:38Z",
            "summary": "Speech-driven 3D facial animation has improved a lot recently while most\nrelated works only utilize acoustic modality and neglect the influence of\nvisual and textual cues, leading to unsatisfactory results in terms of\nprecision and coherence. We argue that visual and textual cues are not trivial\ninformation. Therefore, we present a novel framework, namely PMMTalk, using\ncomplementary Pseudo Multi-Modal features for improving the accuracy of facial\nanimation. The framework entails three modules: PMMTalk encoder, cross-modal\nalignment module, and PMMTalk decoder. Specifically, the PMMTalk encoder\nemploys the off-the-shelf talking head generation architecture and speech\nrecognition technology to extract visual and textual information from speech,\nrespectively. Subsequently, the cross-modal alignment module aligns the\naudio-image-text features at temporal and semantic levels. Then PMMTalk decoder\nis employed to predict lip-syncing facial blendshape coefficients. Contrary to\nprior methods, PMMTalk only requires an additional random reference face image\nbut yields more accurate results. Additionally, it is artist-friendly as it\nseamlessly integrates into standard animation production workflows by\nintroducing facial blendshape coefficients. Finally, given the scarcity of 3D\ntalking face datasets, we introduce a large-scale 3D Chinese Audio-Visual\nFacial Animation (3D-CAVFA) dataset. Extensive experiments and user studies\nshow that our approach outperforms the state of the art. We recommend watching\nthe supplementary video.",
            "author": [
                "Tianshun Han",
                "Shengnan Gui",
                "Yiqing Huang",
                "Baihui Li",
                "Lijian Liu",
                "Benjia Zhou",
                "Ning Jiang",
                "Quan Lu",
                "Ruicong Zhi",
                "Yanyan Liang",
                "Du Zhang",
                "Jun Wan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02781v1",
                "http://arxiv.org/pdf/2312.02781v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02780v1",
            "title": "Scaling Laws for Adversarial Attacks on Language Model Activations",
            "updated": "2023-12-05T14:12:15Z",
            "published": "2023-12-05T14:12:15Z",
            "summary": "We explore a class of adversarial attacks targeting the activations of\nlanguage models. By manipulating a relatively small subset of model\nactivations, $a$, we demonstrate the ability to control the exact prediction of\na significant number (in some cases up to 1000) of subsequent tokens $t$. We\nempirically verify a scaling law where the maximum number of target tokens\n$t_\\mathrm{max}$ predicted depends linearly on the number of tokens $a$ whose\nactivations the attacker controls as $t_\\mathrm{max} = \\kappa a$. We find that\nthe number of bits of control in the input space needed to control a single bit\nin the output space (what we call attack resistance $\\chi$) is remarkably\nconstant between $\\approx 16$ and $\\approx 25$ over 2 orders of magnitude of\nmodel sizes for different language models. Compared to attacks on tokens,\nattacks on activations are predictably much stronger, however, we identify a\nsurprising regularity where one bit of input steered either via activations or\nvia tokens is able to exert control over a similar amount of output bits. This\ngives support for the hypothesis that adversarial attacks are a consequence of\ndimensionality mismatch between the input and output spaces. A practical\nimplication of the ease of attacking language model activations instead of\ntokens is for multi-modal and selected retrieval models, where additional data\nsources are added as activations directly, sidestepping the tokenized input.\nThis opens up a new, broad attack surface. By using language models as a\ncontrollable test-bed to study adversarial attacks, we were able to experiment\nwith input-output dimensions that are inaccessible in computer vision,\nespecially where the output dimension dominates.",
            "author": [
                "Stanislav Fort"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02780v1",
                "http://arxiv.org/pdf/2312.02780v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02779v1",
            "title": "Positron annihilation and binding in aromatic and other ring molecules",
            "updated": "2023-12-05T14:11:30Z",
            "published": "2023-12-05T14:11:30Z",
            "summary": "Annihilation spectra are presented for aromatic and heterocyclic ring\nmolecules resolved as a function of incident positron energy using a trap-based\npositron beam. Comparisons with the vibrational mode spectra yield\npositron-molecule binding energies. Good to excellent agreement is found\nbetween the measured binding energies and the predictions of an \\textit{ab\ninitio} many-body theory that takes proper account of electron-positron\ncorrelations including virtual-positronium formation. The calculations\nelucidate the competition between permanent dipole moments and $\\pi$ bonds in\ndetermining the spatial distribution of the bound-state positron density. The\nimplications of these results and the role of multimode features in\nannihilation in these molecules, including Fermi resonances, are discussed.",
            "author": [
                "E. Arthur-Baidoo",
                "J. R. Danielson",
                "C. M. Surko",
                "J. P. Cassidy",
                "S. K. Gregg",
                "J. Hofierka",
                "B. Cunningham",
                "C. H. Patterson",
                "D. G. Green"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02779v1",
                "http://arxiv.org/pdf/2312.02779v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph",
                "physics.atm-clus",
                "physics.atom-ph",
                "physics.comp-ph",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02778v2",
            "title": "Spin-dependent multiple reentrant localization in an antiferromagnetic\n  helix with transverse electric field: Hopping dimerization-free scenario",
            "updated": "2023-12-07T03:51:21Z",
            "published": "2023-12-05T14:11:03Z",
            "summary": "Reentrant localization (RL), a recently prominent phenomenon, traditionally\nlinks to the interplay of staggered correlated disorder and hopping\ndimerization, as indicated by prior research. Contrary to this paradigm, our\npresent study demonstrates that hopping dimerization is not a pivotal factor in\nrealizing RL. Considering a helical magnetic system with antiferromagnetic\nordering, we uncover spin-dependent RL at multiple energy regions, in the {\\em\nabsence} of hopping dimerization. This phenomenon persists even in the\nthermodynamic limit. The correlated disorder in the form of\nAubry-Andr\\'{e}-Harper model is introduced by applying a transverse electric\nfield to the helical system, circumventing the use of traditional\nsubstitutional disorder. Described within a tight-binding framework, present\nwork provides a novel outlook on RL, highlighting the crucial role of electric\nfield, antiferromagnetic ordering, and the helicity of the geometry.",
            "author": [
                "Sudin Ganguly",
                "Kallol Mondal",
                "Santanu K. Maiti"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02778v2",
                "http://arxiv.org/pdf/2312.02778v2"
            ],
            "primary_category": "cond-mat.dis-nn",
            "category": [
                "cond-mat.dis-nn",
                "cond-mat.str-el",
                "physics.comp-ph",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02772v1",
            "title": "Generating Fine-Grained Human Motions Using ChatGPT-Refined Descriptions",
            "updated": "2023-12-05T14:01:43Z",
            "published": "2023-12-05T14:01:43Z",
            "summary": "Recently, significant progress has been made in text-based motion generation,\nenabling the generation of diverse and high-quality human motions that conform\nto textual descriptions. However, it remains challenging to generate\nfine-grained or stylized motions due to the lack of datasets annotated with\ndetailed textual descriptions. By adopting a divide-and-conquer strategy, we\npropose a new framework named Fine-Grained Human Motion Diffusion Model\n(FG-MDM) for human motion generation. Specifically, we first parse previous\nvague textual annotation into fine-grained description of different body parts\nby leveraging a large language model (GPT-3.5). We then use these fine-grained\ndescriptions to guide a transformer-based diffusion model. FG-MDM can generate\nfine-grained and stylized motions even outside of the distribution of the\ntraining data. Our experimental results demonstrate the superiority of FG-MDM\nover previous methods, especially the strong generalization capability. We will\nrelease our fine-grained textual annotations for HumanML3D and KIT.",
            "author": [
                "Xu Shi",
                "Chuanchen Luo",
                "Junran Peng",
                "Hongwen Zhang",
                "Yunlian Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02772v1",
                "http://arxiv.org/pdf/2312.02772v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02771v1",
            "title": "Scaling-up Memristor Monte Carlo with magnetic domain-wall physics",
            "updated": "2023-12-05T14:01:28Z",
            "published": "2023-12-05T14:01:28Z",
            "summary": "By exploiting the intrinsic random nature of nanoscale devices, Memristor\nMonte Carlo (MMC) is a promising enabler of edge learning systems. However, due\nto multiple algorithmic and device-level limitations, existing demonstrations\nhave been restricted to very small neural network models and datasets. We\ndiscuss these limitations, and describe how they can be overcome, by mapping\nthe stochastic gradient Langevin dynamics (SGLD) algorithm onto the physics of\nmagnetic domain-wall Memristors to scale-up MMC models by five orders of\nmagnitude. We propose the push-pull pulse programming method that realises SGLD\nin-physics, and use it to train a domain-wall based ResNet18 on the CIFAR-10\ndataset. On this task, we observe no performance degradation relative to a\nfloating point model down to an update precision of between 6 and 7-bits,\nindicating we have made a step towards a large-scale edge learning system\nleveraging noisy analogue devices.",
            "author": [
                "Thomas Dalgaty",
                "Shogo Yamada",
                "Anca Molnos",
                "Eiji Kawasaki",
                "Thomas Mesquida",
                "Fran\u00e7ois Rummens",
                "Tatsuo Shibata",
                "Yukihiro Urakawa",
                "Yukio Terasaki",
                "Tomoyuki Sasaki",
                "Marc Duranton"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02771v1",
                "http://arxiv.org/pdf/2312.02771v1"
            ],
            "primary_category": "cs.ET",
            "category": [
                "cs.ET",
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02769v1",
            "title": "Blockchain Participation Games",
            "updated": "2023-12-05T13:59:07Z",
            "published": "2023-12-05T13:59:07Z",
            "summary": "We study game-theoretic models for capturing participation in blockchain\nsystems. Permissionless blockchains can be naturally viewed as games, where a\nset of potentially interested users is faced with the dilemma of whether to\nengage with the protocol or not. Engagement here implies that the user will be\nasked to complete certain tasks, whenever they are selected to contribute\n(typically according to some stochastic process) and be rewarded if they choose\nto do so. Apart from the basic dilemma of engaging or not, even more strategic\nconsiderations arise in settings where users may be able to declare\nparticipation and then retract before completing their tasks (but are still\nable to receive rewards) or are rewarded independently of whether they\ncontribute. Such variations occur naturally in the blockchain setting due to\nthe complexity of tracking ``on-chain'' the behavior of the participants.\n  We capture these participation considerations offering a series of models\nthat enable us to reason about the basic dilemma, the case where retraction\neffects influence the outcome and the case when payments are given universally\nirrespective of the stochastic process. In all cases we provide\ncharacterization results or necessary conditions on the structure of Nash\nequilibria. Our findings reveal that appropriate reward mechanisms can be used\nto stimulate participation and avoid negative effects of free riding, results\nthat are in line but also can inform real world blockchain system deployments.",
            "author": [
                "Pyrros Chaidos",
                "Aggelos Kiayias",
                "Evangelos Markakis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02769v1",
                "http://arxiv.org/pdf/2312.02769v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03035v1",
            "title": "SEVA: Leveraging sketches to evaluate alignment between human and\n  machine visual abstraction",
            "updated": "2023-12-05T13:54:55Z",
            "published": "2023-12-05T13:54:55Z",
            "summary": "Sketching is a powerful tool for creating abstract images that are sparse but\nmeaningful. Sketch understanding poses fundamental challenges for\ngeneral-purpose vision algorithms because it requires robustness to the\nsparsity of sketches relative to natural visual inputs and because it demands\ntolerance for semantic ambiguity, as sketches can reliably evoke multiple\nmeanings. While current vision algorithms have achieved high performance on a\nvariety of visual tasks, it remains unclear to what extent they understand\nsketches in a human-like way. Here we introduce SEVA, a new benchmark dataset\ncontaining approximately 90K human-generated sketches of 128 object concepts\nproduced under different time constraints, and thus systematically varying in\nsparsity. We evaluated a suite of state-of-the-art vision algorithms on their\nability to correctly identify the target concept depicted in these sketches and\nto generate responses that are strongly aligned with human response patterns on\nthe same sketch recognition task. We found that vision algorithms that better\npredicted human sketch recognition performance also better approximated human\nuncertainty about sketch meaning, but there remains a sizable gap between model\nand human response patterns. To explore the potential of models that emulate\nhuman visual abstraction in generative tasks, we conducted further evaluations\nof a recently developed sketch generation algorithm (Vinker et al., 2022)\ncapable of generating sketches that vary in sparsity. We hope that public\nrelease of this dataset and evaluation protocol will catalyze progress towards\nalgorithms with enhanced capacities for human-like visual abstraction.",
            "author": [
                "Kushin Mukherjee",
                "Holly Huey",
                "Xuanchen Lu",
                "Yael Vinker",
                "Rio Aguina-Kang",
                "Ariel Shamir",
                "Judith E. Fan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03035v1",
                "http://arxiv.org/pdf/2312.03035v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02762v1",
            "title": "Learning Cortical Anomaly through Masked Encoding for Unsupervised\n  Heterogeneity Mapping",
            "updated": "2023-12-05T13:44:25Z",
            "published": "2023-12-05T13:44:25Z",
            "summary": "The detection of heterogeneous mental disorders based on brain readouts\nremains challenging due to the complexity of symptoms and the absence of\nreliable biomarkers. This paper introduces CAM (Cortical Anomaly Detection\nthrough Masked Image Modeling), a novel self-supervised framework designed for\nthe unsupervised detection of complex brain disorders using cortical surface\nfeatures. We employ this framework for the detection of individuals on the\npsychotic spectrum and demonstrate its capabilities compared to state-ofthe-art\nmethods, achieving an AUC of 0.696 for Schizoaffective and 0.769 for\nSchizophreniform, without the need for any labels. Furthermore, the analysis of\natypical cortical regions includes Pars Triangularis and several frontal areas,\noften implicated in schizophrenia, provide further confidence in our approach.\nAltogether, we demonstrate a scalable approach for anomaly detection of complex\nbrain disorders based on cortical abnormalities.",
            "author": [
                "Hao-Chun Yang",
                "Ole Andreassen",
                "Lars Tjelta Westlye",
                "Andre F. Marquand",
                "Christian F. Beckmann",
                "Thomas Wolfers"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02762v1",
                "http://arxiv.org/pdf/2312.02762v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02756v1",
            "title": "GenVectorX: A performance-portable SYCL library for Lorentz Vectors\n  operations",
            "updated": "2023-12-05T13:35:21Z",
            "published": "2023-12-05T13:35:21Z",
            "summary": "The Large Hadron Collider (LHC) at CERN will see an upgraded hardware\nconfiguration which will bring a new era of physics data taking and related\ncomputational challenges. To this end, it is necessary to exploit the ever\nincreasing variety of computational architectures, featuring GPUs from multiple\nvendors and new accelerators. Performance portable frameworks, like SYCL, allow\nto offload the computational work on non-CPU resources, while retaining their\nperformance, without the need to maintain different implementations of the same\ncode. The High Energy Physics (HEP) community employs a wide variety of\nalgorithms and tools for accelerators, but it still lacks a streamlined\ncoherent approach that can target many use cases without compromising the\nusability aspect. In this paper, we present our efforts in creating GenVectorX,\na C++ package that provides classes and functionalities to represent and\nmanipulate particle events using the SYCL programming model. The SYCL-based\nimplementation exhibits comparable performance and scalability as the CUDA\nimplementation when targeting NVIDIA GPUs.",
            "author": [
                "Monica Dessole",
                "Jolly Chen",
                "Axel Naumann"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02756v1",
                "http://arxiv.org/pdf/2312.02756v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02753v1",
            "title": "C3: High-performance and low-complexity neural compression from a single\n  image or video",
            "updated": "2023-12-05T13:28:59Z",
            "published": "2023-12-05T13:28:59Z",
            "summary": "Most neural compression models are trained on large datasets of images or\nvideos in order to generalize to unseen data. Such generalization typically\nrequires large and expressive architectures with a high decoding complexity.\nHere we introduce C3, a neural compression method with strong rate-distortion\n(RD) performance that instead overfits a small model to each image or video\nseparately. The resulting decoding complexity of C3 can be an order of\nmagnitude lower than neural baselines with similar RD performance. C3 builds on\nCOOL-CHIC (Ladune et al.) and makes several simple and effective improvements\nfor images. We further develop new methodology to apply C3 to videos. On the\nCLIC2020 image benchmark, we match the RD performance of VTM, the reference\nimplementation of the H.266 codec, with less than 3k MACs/pixel for decoding.\nOn the UVG video benchmark, we match the RD performance of the Video\nCompression Transformer (Mentzer et al.), a well-established neural video\ncodec, with less than 5k MACs/pixel for decoding.",
            "author": [
                "Hyunjik Kim",
                "Matthias Bauer",
                "Lucas Theis",
                "Jonathan Richard Schwarz",
                "Emilien Dupont"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02753v1",
                "http://arxiv.org/pdf/2312.02753v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02751v1",
            "title": "C-NERF: Representing Scene Changes as Directional Consistency\n  Difference-based NeRF",
            "updated": "2023-12-05T13:27:12Z",
            "published": "2023-12-05T13:27:12Z",
            "summary": "In this work, we aim to detect the changes caused by object variations in a\nscene represented by the neural radiance fields (NeRFs). Given an arbitrary\nview and two sets of scene images captured at different timestamps, we can\npredict the scene changes in that view, which has significant potential\napplications in scene monitoring and measuring. We conducted preliminary\nstudies and found that such an exciting task cannot be easily achieved by\nutilizing existing NeRFs and 2D change detection methods with many false or\nmissing detections. The main reason is that the 2D change detection is based on\nthe pixel appearance difference between spatial-aligned image pairs and\nneglects the stereo information in the NeRF. To address the limitations, we\npropose the C-NERF to represent scene changes as directional consistency\ndifference-based NeRF, which mainly contains three modules. We first perform\nthe spatial alignment of two NeRFs captured before and after changes. Then, we\nidentify the change points based on the direction-consistent constraint; that\nis, real change points have similar change representations across view\ndirections, but fake change points do not. Finally, we design the change map\nrendering process based on the built NeRFs and can generate the change map of\nan arbitrarily specified view direction. To validate the effectiveness, we\nbuild a new dataset containing ten scenes covering diverse scenarios with\ndifferent changing objects. Our approach surpasses state-of-the-art 2D change\ndetection and NeRF-based methods by a significant margin.",
            "author": [
                "Rui Huang",
                "Binbin Jiang",
                "Qingyi Zhao",
                "William Wang",
                "Yuxiang Zhang",
                "Qing Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02751v1",
                "http://arxiv.org/pdf/2312.02751v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02748v1",
            "title": "Compositional Generalization for Data-to-Text Generation",
            "updated": "2023-12-05T13:23:15Z",
            "published": "2023-12-05T13:23:15Z",
            "summary": "Data-to-text generation involves transforming structured data, often\nrepresented as predicate-argument tuples, into coherent textual descriptions.\nDespite recent advances, systems still struggle when confronted with unseen\ncombinations of predicates, producing unfaithful descriptions (e.g.\nhallucinations or omissions). We refer to this issue as compositional\ngeneralisation, and it encouraged us to create a benchmark for assessing the\nperformance of different approaches on this specific problem. Furthermore, we\npropose a novel model that addresses compositional generalization by clustering\npredicates into groups. Our model generates text in a sentence-by-sentence\nmanner, relying on one cluster of predicates at a time. This approach\nsignificantly outperforms T5~baselines across all evaluation metrics.Notably,\nit achieved a 31% improvement over T5 in terms of a metric focused on\nmaintaining faithfulness to the input.",
            "author": [
                "Xinnuo Xu",
                "Ivan Titov",
                "Mirella Lapata"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02748v1",
                "http://arxiv.org/pdf/2312.02748v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03034v1",
            "title": "Distributed Speech Dereverberation Using Weighted Prediction Error",
            "updated": "2023-12-05T13:22:58Z",
            "published": "2023-12-05T13:22:58Z",
            "summary": "Speech dereverberation aims to alleviate the negative impact of late\nreverberant reflections. The weighted prediction error (WPE) method is a\nwell-established technique known for its superior performance in\ndereverberation. However, in scenarios where microphone nodes are dispersed,\nthe centralized approach of the WPE method requires aggregating all\nobservations for inverse filtering, resulting in a significant computational\nburden. This paper introduces a distributed speech dereverberation method that\nemphasizes low computational complexity at each node. Specifically, we leverage\nthe distributed adaptive node-specific signal estimation (DANSE) algorithm\nwithin the multichannel linear prediction (MCLP) process. This approach\nempowers each node to perform local operations with reduced complexity while\nachieving the global performance through inter-node cooperation. Experimental\nresults validate the effectiveness of our proposed method, showcasing its\nability to achieve efficient speech dereverberation in dispersed microphone\nnode scenarios.",
            "author": [
                "Ziye Yang",
                "Mengfei Zhang",
                "Jie Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03034v1",
                "http://arxiv.org/pdf/2312.03034v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02742v1",
            "title": "Search for the edge-on galaxies using an artificial neural network",
            "updated": "2023-12-05T13:08:40Z",
            "published": "2023-12-05T13:08:40Z",
            "summary": "We present an application of an artificial neural network methodology to a\nmodern wide-field sky survey Pan-STARRS1 in order to build a high-quality\nsample of disk galaxies visible in edge-on orientation. Such galaxies play an\nimportant role in the study of the vertical distribution of stars, gas and\ndust, which is usually not available to study in other galaxies outside the\nMilky Way. We give a detailed description of the network architecture and the\nlearning process. The method demonstrates good effectiveness with detection\nrate about 97\\% and it works equally well for galaxies over a wide range of\nbrightnesses and sizes, which resulted in a creation of a catalogue of edge-on\ngalaxies with $10^5$ of objects. The catalogue is published on-line with an\nopen access.",
            "author": [
                "S. S. Savchenko",
                "D. I. Makarov",
                "A. V. Antipova",
                "I. S. Tikhonenko"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.ascom.2023.100771",
                "http://arxiv.org/abs/2312.02742v1",
                "http://arxiv.org/pdf/2312.02742v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02741v1",
            "title": "Part-time Power Measurements: nvidia-smi's Lack of Attention",
            "updated": "2023-12-05T13:08:14Z",
            "published": "2023-12-05T13:08:14Z",
            "summary": "The GPU has emerged as the go-to accelerator for high throughput and parallel\nworkloads, spanning scientific simulations to AI, thanks to its performance and\npower efficiency. Given that 6 out of the top 10 fastest supercomputers in the\nworld use NVIDIA GPUs and many AI companies each employ 10,000's of NVIDIA\nGPUs, an accurate understanding of GPU power consumption is essential for\nmaking progress to further improve its efficiency. Despite the limited\ndocumentation and the lack of understanding of its mechanisms, NVIDIA GPUs'\nbuilt-in power sensor, providing easily accessible power readings via the\nnvidia-smi interface, is widely used in energy efficient computing research on\nGPUs. Our study seeks to elucidate the internal mechanisms of the power\nreadings provided by nvidia-smi and assess the accuracy of the power and energy\nconsumption data. We have developed a suite of micro-benchmarks to profile the\nbehaviour of nvidia-smi power readings and have evaluated them on over 70\ndifferent GPUs from all architectural generations since power measurement was\nfirst introduced in the 'Fermi' generation. We have identified several\nunforeseen problems in terms of power/energy measurement using nvidia-smi, for\nexample on the A100 and H100 GPUs only 25% of the runtime is sampled for power\nconsumption, during the other 75% of the time, the GPU can be using drastically\ndifferent power and nvidia-smi and results presented by it are unaware of this.\nThis along with other findings can lead to a drastic under/overestimation of\nenergy consumed, especially when considering data centres housing tens of\nthousands of GPUs. We proposed several good practices that help to mitigate\nthese problems. By comparing our results to those measured from an external\npower-meter, we have reduced the error in the energy measurement by an average\nof 35% and in some cases by as much as 65% in the test cases we present.",
            "author": [
                "Zeyu Yang",
                "Karel Adamek",
                "Wesley Armour"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02741v1",
                "http://arxiv.org/pdf/2312.02741v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02740v1",
            "title": "Observable imprints of primordial gravitational waves on the temperature\n  anisotropies of the Cosmic Microwave Background",
            "updated": "2023-12-05T13:06:44Z",
            "published": "2023-12-05T13:06:44Z",
            "summary": "We examine the contribution of tensor modes, in addition to the dominant\nscalar ones, on the temperature anisotropies of the cosmic microwave background\n(CMB). To this end, we analyze in detail the temperature two-point angular\ncorrelation function $C(\\theta)$ from the Planck 2018 dataset, focusing on\nlarge angles ($\\theta \\gtrsim 120^{\\circ}$) corresponding to small $\\ell$\nmultipoles. A hierarchical set of infrared cutoffs are naturally introduced to\nthe scalar and tensor power spectra of the CMB by invoking an extra\nKaluza-Klein dimension compactifying at about the GUT scale between the Planck\nepoch and the start of inflation. We associate this set of lower scalar and\ntensor cutoffs with the parity of the multipole expansion of the $C(\\theta)$\nfunction. By fitting the Planck 2018 data we compute the multipole coefficients\nthereby reproducing the well-known odd-parity preference in angular\ncorrelations seen by all three satellite missions COBE, WMAP and Planck. Our\nfits improve significantly once tensor modes are included in the analysis,\nhence providing a hint of the imprints of primordial gravitational waves on the\ntemperature correlations observed in the CMB today. To conclude we suggest a\nrelationship between, on the one hand, the lack of (positive) large-angle\ncorrelations and the odd-parity dominance in the CMB and, on the other hand,\nthe effect of primordial gravitational waves on the CMB temperature\nanisotropies.",
            "author": [
                "Miguel-Angel Sanchis-Lozano",
                "Veronica Sanz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02740v1",
                "http://arxiv.org/pdf/2312.02740v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02737v1",
            "title": "Track finding with deep neural networks",
            "updated": "2023-12-05T13:01:41Z",
            "published": "2023-12-05T13:01:41Z",
            "summary": "High-energy physics experiments require fast and efficient methods for\nreconstructing the tracks of charged particles. The commonly used algorithms\nare sequential, and the required CPU power increases rapidly with the number of\ntracks. Neural networks can speed up the process due to their capability of\nmodeling complex non-linear data dependencies and finding all tracks in\nparallel. In this paper, we describe the application of a deep neural network\nfor reconstructing straight tracks in a toy two-dimensional model. It is\nplanned to apply this method to the experimental data obtained by the MUonE\nexperiment at CERN.",
            "author": [
                "Marcin Kucharczyk",
                "Marcin Wolter"
            ],
            "link": [
                "http://dx.doi.org/10.7494/csci.2019.20.4.3376",
                "http://arxiv.org/abs/2312.02737v1",
                "http://arxiv.org/pdf/2312.02737v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02734v1",
            "title": "Geometric Data-Driven Dimensionality Reduction in MPC with Guarantees",
            "updated": "2023-12-05T12:58:19Z",
            "published": "2023-12-05T12:58:19Z",
            "summary": "We consider the problem of reducing the dimension of the discrete-time\noptimal control problem that is solved repeatedly online in model predictive\ncontrol. We show that a reduced-order scheme, which solves the optimization\nproblem in a low-dimensional subspace, inherits the stability and recursive\nfeasibility properties from the original formulation. We introduce a necessary\nand sufficient condition for initial feasibility and incorporate that in the\nsubspace design. Finally, we use concepts of optimization over Riemannian\nmanifolds to compute a subspace that provides optimal representations for a set\nof pre-defined high-dimensional optimizers under the initial admissibility\nconstraint.",
            "author": [
                "Roland Schurig",
                "Andreas Himmel",
                "Rolf Findeisen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02734v1",
                "http://arxiv.org/pdf/2312.02734v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02732v1",
            "title": "Optical cycling in charged complexes with Ra-N bonds",
            "updated": "2023-12-05T12:51:30Z",
            "published": "2023-12-05T12:51:30Z",
            "summary": "The extension of laser cooling and trapping methods to polyatomic molecular\nions, including those with Ra--N bonds, would have advanced scientific\napplications such as quantum sensors for fundamental physics, high resolution\nspectroscopy, and testing predictions of the Standard Model. The essential\nprerequisite for laser coolability is that molecule is able to scatter hundreds\nof photons without changing its initial rovibronic state. Thus laser-cooled\nmolecules exhibit a probability of population leak out of the multitude of\nworking vibronic levels (``optical leak'') typically less than 10$^{-2}$. This\nprobability is highly sensitive to small variations of electronic density in\nthe vicinity of its optical cycling center. In the present work we employed the\nFock space relativistic coupled cluster approach to obtain information on\nelectronic states and potential energy surfaces of RaNCH$^+$, RaNH$_3^+$ and\nRaNCCH$_3^+$ molecular ions. Laser coolability of these species was estimated\nthrough evaluating Frank-Condon factors, and the peculiarities of\nunpaired-electron distributions were analyzed to assess coolability from the\npoint of view of the molecular electronic structure. RaNH$_3^+$ and\nRaNCCH$_3^+$ are the first symmetric top molecular ions found to be promising\ncandidates for direct laser cooling.",
            "author": [
                "Timur Isaev",
                "Alexander V. Oleynichenko",
                "Dmitrii A. Makinskii",
                "Andr\u00e9i Zaitsevskii"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02732v1",
                "http://arxiv.org/pdf/2312.02732v1"
            ],
            "primary_category": "physics.atom-ph",
            "category": [
                "physics.atom-ph",
                "physics.atm-clus",
                "physics.chem-ph",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02731v1",
            "title": "D-LGP: Dynamic Logic-Geometric Program for Combined Task and Motion\n  Planning",
            "updated": "2023-12-05T12:49:11Z",
            "published": "2023-12-05T12:49:11Z",
            "summary": "Many real-world sequential manipulation tasks involve a combination of\ndiscrete symbolic search and continuous motion planning, collectively known as\ncombined task and motion planning (TAMP). However, prevailing methods often\nstruggle with the computational burden and intricate combinatorial challenges\nstemming from the multitude of action skeletons. To address this, we propose\nDynamic Logic-Geometric Program (D-LGP), a novel approach integrating Dynamic\nTree Search and global optimization for efficient hybrid planning. Through\nempirical evaluation on three benchmarks, we demonstrate the efficacy of our\napproach, showcasing superior performance in comparison to state-of-the-art\ntechniques. We validate our approach through simulation and demonstrate its\ncapability for online replanning under uncertainty and external disturbances in\nthe real world.",
            "author": [
                "Teng Xue",
                "Amirreza Razmjoo",
                "Sylvain Calinon"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02731v1",
                "http://arxiv.org/pdf/2312.02731v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02730v1",
            "title": "Towards Measuring Representational Similarity of Large Language Models",
            "updated": "2023-12-05T12:48:04Z",
            "published": "2023-12-05T12:48:04Z",
            "summary": "Understanding the similarity of the numerous released large language models\n(LLMs) has many uses, e.g., simplifying model selection, detecting illegal\nmodel reuse, and advancing our understanding of what makes LLMs perform well.\nIn this work, we measure the similarity of representations of a set of LLMs\nwith 7B parameters. Our results suggest that some LLMs are substantially\ndifferent from others. We identify challenges of using representational\nsimilarity measures that suggest the need of careful study of similarity scores\nto avoid false conclusions.",
            "author": [
                "Max Klabunde",
                "Mehdi Ben Amor",
                "Michael Granitzer",
                "Florian Lemmerich"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02730v1",
                "http://arxiv.org/pdf/2312.02730v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03033v1",
            "title": "LiDAR-based Person Re-identification",
            "updated": "2023-12-05T12:44:17Z",
            "published": "2023-12-05T12:44:17Z",
            "summary": "Camera-based person re-identification (ReID) systems have been widely applied\nin the field of public security. However, cameras often lack the perception of\n3D morphological information of human and are susceptible to various\nlimitations, such as inadequate illumination, complex background, and personal\nprivacy. In this paper, we propose a LiDAR-based ReID framework, ReID3D, that\nutilizes pre-training strategy to retrieve features of 3D body shape and\nintroduces Graph-based Complementary Enhancement Encoder for extracting\ncomprehensive features. Due to the lack of LiDAR datasets, we build LReID, the\nfirst LiDAR-based person ReID dataset, which is collected in several outdoor\nscenes with variations in natural conditions. Additionally, we introduce\nLReID-sync, a simulated pedestrian dataset designed for pre-training encoders\nwith tasks of point cloud completion and shape parameter learning. Extensive\nexperiments on LReID show that ReID3D achieves exceptional performance with a\nrank-1 accuracy of 94.0, highlighting the significant potential of LiDAR in\naddressing person ReID tasks. To the best of our knowledge, we are the first to\npropose a solution for LiDAR-based ReID. The code and datasets will be released\nsoon.",
            "author": [
                "Wenxuan Guo",
                "Zhiyu Pan",
                "Yingping Liang",
                "Ziheng Xi",
                "Zhi Chen Zhong",
                "Jianjiang Feng",
                "Jie Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03033v1",
                "http://arxiv.org/pdf/2312.03033v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02725v1",
            "title": "R3D-SWIN:Use Shifted Window Attention for Single-View 3D Reconstruction",
            "updated": "2023-12-05T12:42:37Z",
            "published": "2023-12-05T12:42:37Z",
            "summary": "Recently, vision transformers have performed well in various computer vision\ntasks, including voxel 3D reconstruction. However, the windows of the vision\ntransformer are not multi-scale, and there is no connection between the\nwindows, which limits the accuracy of voxel 3D reconstruction . Therefore, we\npropose a shifted windows attention voxel 3D reconstruction network. To the\nbest of our knowledge, this is the first work to apply shifted window attention\nto voxel 3D reconstruction. Experimental results on ShapeNet verify our method\nachieves SOTA accuracy in single-view reconstruction.",
            "author": [
                "Chenhuan Li",
                "Meihua Xiao",
                "zehuan li",
                "Mengxi Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02725v1",
                "http://arxiv.org/pdf/2312.02725v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02723v1",
            "title": "Accurate and efficient approximation of large-scale appointment\n  schedules",
            "updated": "2023-12-05T12:37:54Z",
            "published": "2023-12-05T12:37:54Z",
            "summary": "Setting up optimal appointment schedules requires the computation of an\ninherently involved objective function, typically requiring distributional\nknowledge of the clients' waiting times and the server's idle times (as a\nfunction of the appointment times of the individual clients). A frequently used\nidea is to approximate the clients' service times by their phase-type\ncounterpart, thus leading to explicit expressions for the waiting-time and\nidle-time distributions. This method, however, requires the evaluation of the\nmatrix exponential of potentially large matrices, which already becomes\nprohibitively slow from, say, 20 clients on. In this paper we remedy this issue\nby recursively approximating the distributions involved relying on a\ntwo-moments fit. More specifically, we approximate the sojourn time of each of\nthe clients by a low-dimensional phase-type, Weibull or Lognormal random\nvariable with the desired mean and variance. Our computational experiments show\nthat this elementary, yet highly accurate, technique facilitates the evaluation\nof optimal appointment schedules even if the number of clients is large. The\nthree ways to approximate the sojourn-time distribution turn out to be roughly\nequally accurate, except in certain specific regimes, where the low-dimensional\nphase-type fit performs well across all instances considered. As this\nlow-dimensional phase-type fit is by far the fastest of the three alternatives,\nit is the approximation that we recommend.",
            "author": [
                "Ren\u00e9 Bekker",
                "Bharti Bharti",
                "Michel Mandjes"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02723v1",
                "http://arxiv.org/pdf/2312.02723v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02722v1",
            "title": "Improved Algorithms for Minimum-Membership Geometric Set Cover",
            "updated": "2023-12-05T12:35:53Z",
            "published": "2023-12-05T12:35:53Z",
            "summary": "Bandyapadhyay et al. introduced the generalized minimum-membership geometric\nset cover (GMMGSC) problem [SoCG, 2023], which is defined as follows. We are\ngiven two sets $P$ and $P'$ of points in $\\mathbb{R}^{2}$, $n=\\max(|P|, |P'|)$,\nand a set $\\mathcal{S}$ of $m$ axis-parallel unit squares. The goal is to find\na subset $\\mathcal{S}^{*}\\subseteq \\mathcal{S}$ that covers all the points in\n$P$ while minimizing $\\mathsf{memb}(P', \\mathcal{S}^{*})$, where\n$\\mathsf{memb}(P', \\mathcal{S}^{*})=\\max_{p\\in P'}|\\{s\\in \\mathcal{S}^{*}: p\\in\ns\\}|$. We study GMMGSC problem and give a $16$-approximation algorithm that\nruns in $O(m^2\\log m + m^2n)$ time. Our result is a significant improvement to\nthe $144$-approximation given by Bandyapadhyay et al. that runs in\n$\\tilde{O}(nm)$ time.\n  GMMGSC problem is a generalization of another well-studied problem called\nMinimum Ply Geometric Set Cover (MPGSC), in which the goal is to minimize the\nply of $\\mathcal{S}^{*}$, where the ply is the maximum cardinality of a subset\nof the unit squares that have a non-empty intersection. The best-known result\nfor the MPGSC problem is an $8$-approximation algorithm by Durocher et al. that\nruns in $O(n + m^{8}k^{4}\\log k + m^{8}\\log m\\log k)$ time, where $k$ is the\noptimal ply value [WALCOM, 2023].",
            "author": [
                "Sathish Govindarajan",
                "Siddhartha Sarkar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02722v1",
                "http://arxiv.org/pdf/2312.02722v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02712v2",
            "title": "Computing the Frequency-Dependent NMR Relaxation of $^1$H Nuclei in\n  Liquid Water",
            "updated": "2023-12-06T15:12:50Z",
            "published": "2023-12-05T12:16:51Z",
            "summary": "It is the purpose of this paper to present a computational framework for\nreliably determining the frequency-dependent intermolecular and intramolecular\nNMR dipole-dipole relaxation rate of spin $1/2$ nuclei from MD simulations. The\napproach avoids alterations caused by well-known finite-size effects of the\ntranslational diffusion. Moreover, a procedure is derived to control and\ncorrect for effects caused by fixed distance-sampling cutoffs and periodic\nboundary conditions. By construction, this approach is capable of accurately\npredicting the correct low-frequency scaling behavior of the intermolecular NMR\ndipole-dipole relaxation rate and thus allows the reliable calculation of the\nfrequency-dependent relaxation rate over many orders of magnitude. Our approach\nis based on the utilisation of the theory of Hwang and Freed for the\nintermolecular dipole-dipole correlation function and its corresponding\nspectral density [J. Chem. Phys. 63, 4017 (1975)] and its combination with data\nfrom molecular dynamics (MD) simulations. The deviations from the Hwang and\nFreed theory caused by periodic boundary conditions and sampling distance\ncutoffs are quantified by means of random walker Monte Carlo simulations. An\nexpression based on the Hwang and Freed theoryis also suggested for correcting\nthose effects. As a proof of principle, our approach is demonstrated by\ncomputing the frequency-dependent inter- and intramolecular dipolar NMR\nrelaxation rate of the $^1$H nuclei in liquid water at $273\\,\\mbox{K}$ and\n$298\\,\\mbox{K}$ based on simulations of the TIP4P/2005 model. Our calculations\nare suggesting that the intermolecular contribution to the $^1$H NMR relaxation\nrate of the TIP4P/2005 model in the extreme narrowing limit has previously been\nsubstantially underestimated.",
            "author": [
                "Dietmar Paschek",
                "Johanna Busch",
                "Eduard Mock",
                "Ralf Ludwig",
                "Anne Strate"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02712v2",
                "http://arxiv.org/pdf/2312.02712v2"
            ],
            "primary_category": "cond-mat.soft",
            "category": [
                "cond-mat.soft",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02706v1",
            "title": "Large Knowledge Model: Perspectives and Challenges",
            "updated": "2023-12-05T12:07:30Z",
            "published": "2023-12-05T12:07:30Z",
            "summary": "Humankind's understanding of the world is fundamentally linked to our\nperception and cognition, with \\emph{human languages} serving as one of the\nmajor carriers of \\emph{world knowledge}. In this vein, \\emph{Large Language\nModels} (LLMs) like ChatGPT epitomize the pre-training of extensive,\nsequence-based world knowledge into neural networks, facilitating the\nprocessing and manipulation of this knowledge in a parametric space. This\narticle explores large models through the lens of ``knowledge''. We initially\ninvestigate the role of symbolic knowledge such as Knowledge Graphs (KGs) in\nenhancing LLMs, covering aspects like knowledge-augmented language model,\nstructure-inducing pre-training, knowledgeable prompts, structured CoT,\nknowledge editing, semantic tools for LLM and knowledgeable AI agents.\nSubsequently, we examine how LLMs can amplify traditional symbolic knowledge\nbases, encompassing aspects like using LLM as KG builder and controller,\nstructured knowledge pretraining, LLM-enhanced symbolic reasoning, and the\namalgamation of perception with cognition. Considering the intricate nature of\nhuman knowledge, we advocate for the creation of \\emph{Large Knowledge Models}\n(LKM), specifically engineered to manage diversified spectrum of knowledge\nstructures. This ambitious undertaking could entail several key challenges,\nsuch as disentangling knowledge representation from language models,\nrestructuring pre-training with structured knowledge, and building large\ncommonsense models, among others. We finally propose a five-``A'' principle to\ndistinguish the concept of LKM.",
            "author": [
                "Huajun Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02706v1",
                "http://arxiv.org/pdf/2312.02706v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02705v1",
            "title": "Unified learning-based lossy and lossless JPEG recompression",
            "updated": "2023-12-05T12:07:27Z",
            "published": "2023-12-05T12:07:27Z",
            "summary": "JPEG is still the most widely used image compression algorithm. Most image\ncompression algorithms only consider uncompressed original image, while\nignoring a large number of already existing JPEG images. Recently, JPEG\nrecompression approaches have been proposed to further reduce the size of JPEG\nfiles. However, those methods only consider JPEG lossless recompression, which\nis just a special case of the rate-distortion theorem. In this paper, we\npropose a unified lossly and lossless JPEG recompression framework, which\nconsists of learned quantization table and Markovian hierarchical variational\nautoencoders. Experiments show that our method can achieve arbitrarily low\ndistortion when the bitrate is close to the upper bound, namely the bitrate of\nthe lossless compression model. To the best of our knowledge, this is the first\nlearned method that bridges the gap between lossy and lossless recompression of\nJPEG images.",
            "author": [
                "Jianghui Zhang",
                "Yuanyuan Wang",
                "Lina Guo",
                "Jixiang Luo",
                "Tongda Xu",
                "Yan Wang",
                "Zhi Wang",
                "Hongwei Qin"
            ],
            "link": [
                "http://dx.doi.org/10.1109/ICIP49359.2023.10222354",
                "http://arxiv.org/abs/2312.02705v1",
                "http://arxiv.org/pdf/2312.02705v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02703v1",
            "title": "MyPortrait: Morphable Prior-Guided Personalized Portrait Generation",
            "updated": "2023-12-05T12:05:01Z",
            "published": "2023-12-05T12:05:01Z",
            "summary": "Generating realistic talking faces is an interesting and long-standing topic\nin the field of computer vision. Although significant progress has been made,\nit is still challenging to generate high-quality dynamic faces with\npersonalized details. This is mainly due to the inability of the general model\nto represent personalized details and the generalization problem to unseen\ncontrollable parameters. In this work, we propose Myportrait, a simple,\ngeneral, and flexible framework for neural portrait generation. We incorporate\npersonalized prior in a monocular video and morphable prior in 3D face\nmorphable space for generating personalized details under novel controllable\nparameters. Our proposed framework supports both video-driven and audio-driven\nface animation given a monocular video of a single person. Distinguished by\nwhether the test data is sent to training or not, our method provides a\nreal-time online version and a high-quality offline version. Comprehensive\nexperiments in various metrics demonstrate the superior performance of our\nmethod over the state-of-the-art methods. The code will be publicly available.",
            "author": [
                "Bo Ding",
                "Zhenfeng Fan",
                "Shuang Yang",
                "Shihong Xia"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02703v1",
                "http://arxiv.org/pdf/2312.02703v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02702v1",
            "title": "Neural Sign Actors: A diffusion model for 3D sign language production\n  from text",
            "updated": "2023-12-05T12:04:34Z",
            "published": "2023-12-05T12:04:34Z",
            "summary": "Sign Languages (SL) serve as the predominant mode of communication for the\nDeaf and Hard of Hearing communities. The advent of deep learning has aided\nnumerous methods in SL recognition and translation, achieving remarkable\nresults. However, Sign Language Production (SLP) poses a challenge for the\ncomputer vision community as the motions generated must be realistic and have\nprecise semantic meanings. Most SLP methods rely on 2D data, thus impeding\ntheir ability to attain a necessary level of realism. In this work, we propose\na diffusion-based SLP model trained on a curated large-scale dataset of 4D\nsigning avatars and their corresponding text transcripts. The proposed method\ncan generate dynamic sequences of 3D avatars from an unconstrained domain of\ndiscourse using a diffusion process formed on a novel and anatomically informed\ngraph neural network defined on the SMPL-X body skeleton. Through a series of\nquantitative and qualitative experiments, we show that the proposed method\nconsiderably outperforms previous methods of SLP. We believe that this work\npresents an important and necessary step towards realistic neural sign avatars,\nbridging the communication gap between Deaf and hearing communities. The code,\nmethod and generated data will be made publicly available.",
            "author": [
                "Vasileios Baltatzis",
                "Rolandos Alexandros Potamias",
                "Evangelos Ververas",
                "Guanxiong Sun",
                "Jiankang Deng",
                "Stefanos Zafeiriou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02702v1",
                "http://arxiv.org/pdf/2312.02702v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02700v1",
            "title": "Revisit Human-Scene Interaction via Space Occupancy",
            "updated": "2023-12-05T12:03:00Z",
            "published": "2023-12-05T12:03:00Z",
            "summary": "Human-scene Interaction (HSI) generation is a challenging task and crucial\nfor various downstream tasks. However, one of the major obstacles is the\nlimited data scale. High-quality data with simultaneously captured human and 3D\nenvironments is rare, resulting in limited data diversity and complexity. In\nthis work, we argue that interaction with a scene is essentially interacting\nwith the space occupancy of the scene from an abstract physical perspective,\nleading us to a unified novel view of Human-Occupancy Interaction. By treating\npure motion sequences as records of humans interacting with invisible scene\noccupancy, we can aggregate motion-only data into a large-scale paired\nhuman-occupancy interaction database: Motion Occupancy Base (MOB). Thus, the\nneed for costly paired motion-scene datasets with high-quality scene scans can\nbe substantially alleviated. With this new unified view of Human-Occupancy\ninteraction, a single motion controller is proposed to reach the target state\ngiven the surrounding occupancy. Once trained on MOB with complex occupancy\nlayout, the controller could handle cramped scenes and generalize well to\ngeneral scenes with limited complexity. With no GT 3D scenes for training, our\nmethod can generate realistic and stable HSI motions in diverse scenarios,\nincluding both static and dynamic scenes. Our code and data would be made\npublicly available at https://foruck.github.io/occu-page/.",
            "author": [
                "Xinpeng Liu",
                "Haowen Hou",
                "Yanchao Yang",
                "Yong-Lu Li",
                "Cewu Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02700v1",
                "http://arxiv.org/pdf/2312.02700v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02699v1",
            "title": "Enhancing Vehicle Entrance and Parking Management: Deep Learning\n  Solutions for Efficiency and Security",
            "updated": "2023-12-05T12:02:53Z",
            "published": "2023-12-05T12:02:53Z",
            "summary": "The auto-management of vehicle entrance and parking in any organization is a\ncomplex challenge encompassing record-keeping, efficiency, and security\nconcerns. Manual methods for tracking vehicles and finding parking spaces are\nslow and a waste of time. To solve the problem of auto management of vehicle\nentrance and parking, we have utilized state-of-the-art deep learning models\nand automated the process of vehicle entrance and parking into any\norganization. To ensure security, our system integrated vehicle detection,\nlicense number plate verification, and face detection and recognition models to\nensure that the person and vehicle are registered with the organization. We\nhave trained multiple deep-learning models for vehicle detection, license\nnumber plate detection, face detection, and recognition, however, the YOLOv8n\nmodel outperformed all the other models. Furthermore, License plate recognition\nis facilitated by Google's Tesseract-OCR Engine. By integrating these\ntechnologies, the system offers efficient vehicle detection, precise\nidentification, streamlined record keeping, and optimized parking slot\nallocation in buildings, thereby enhancing convenience, accuracy, and security.\nFuture research opportunities lie in fine-tuning system performance for a wide\nrange of real-world applications.",
            "author": [
                "Muhammad Umer Ramzan",
                "Usman Ali",
                "Syed Haider Abbas Naqvi",
                "Zeeshan Aslam",
                "Tehseen",
                "Husnain Ali",
                "Muhammad Faheem"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02699v1",
                "http://arxiv.org/pdf/2312.02699v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02698v1",
            "title": "Power means of random variables and characterizations of distributions\n  via fractional calculus",
            "updated": "2023-12-05T12:02:46Z",
            "published": "2023-12-05T12:02:46Z",
            "summary": "We investigate fractional moments and expectations of power means of\ncomplex-valued random variables by using fractional calculus. We deal with both\nnegative and positive orders of the fractional derivatives. The one-dimensional\ndistributions are characterized in terms of the fractional moments without any\nmoment assumptions. We explicitly compute the expectations of the power means\nfor both the univariate Cauchy distribution and the Poincar\\'e distribution on\nthe upper-half plane. We show that for these distributions the expectations are\ninvariant with respect to the sample size and the value of the power.",
            "author": [
                "Kazuki Okamura",
                "Yoshiki Otobe"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02698v1",
                "http://arxiv.org/pdf/2312.02698v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02697v1",
            "title": "Hierarchical Visual Policy Learning for Long-Horizon Robot Manipulation\n  in Densely Cluttered Scenes",
            "updated": "2023-12-05T11:57:39Z",
            "published": "2023-12-05T11:57:39Z",
            "summary": "In this work, we focus on addressing the long-horizon manipulation tasks in\ndensely cluttered scenes. Such tasks require policies to effectively manage\nsevere occlusions among objects and continually produce actions based on visual\nobservations. We propose a vision-based Hierarchical policy for Cluttered-scene\nLong-horizon Manipulation (HCLM). It employs a high-level policy and three\noptions to select and instantiate three parameterized action primitives: push,\npick, and place. We first train the pick and place options by behavior cloning\n(BC). Subsequently, we use hierarchical reinforcement learning (HRL) to train\nthe high-level policy and push option. During HRL, we propose a Spatially\nExtended Q-update (SEQ) to augment the updates for the push option and a\nTwo-Stage Update Scheme (TSUS) to alleviate the non-stationary transition\nproblem in updating the high-level policy. We demonstrate that HCLM\nsignificantly outperforms baseline methods in terms of success rate and\nefficiency in diverse tasks. We also highlight our method's ability to\ngeneralize to more cluttered environments with more additional blocks.",
            "author": [
                "Hecheng Wang",
                "Lizhe Qi",
                "Bin Fang",
                "Yunquan Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02697v1",
                "http://arxiv.org/pdf/2312.02697v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02696v1",
            "title": "Analyzing and Improving the Training Dynamics of Diffusion Models",
            "updated": "2023-12-05T11:55:47Z",
            "published": "2023-12-05T11:55:47Z",
            "summary": "Diffusion models currently dominate the field of data-driven image synthesis\nwith their unparalleled scaling to large datasets. In this paper, we identify\nand rectify several causes for uneven and ineffective training in the popular\nADM diffusion model architecture, without altering its high-level structure.\nObserving uncontrolled magnitude changes and imbalances in both the network\nactivations and weights over the course of training, we redesign the network\nlayers to preserve activation, weight, and update magnitudes on expectation. We\nfind that systematic application of this philosophy eliminates the observed\ndrifts and imbalances, resulting in considerably better networks at equal\ncomputational complexity. Our modifications improve the previous record FID of\n2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic\nsampling.\n  As an independent contribution, we present a method for setting the\nexponential moving average (EMA) parameters post-hoc, i.e., after completing\nthe training run. This allows precise tuning of EMA length without the cost of\nperforming several training runs, and reveals its surprising interactions with\nnetwork architecture, training time, and guidance.",
            "author": [
                "Tero Karras",
                "Miika Aittala",
                "Jaakko Lehtinen",
                "Janne Hellsten",
                "Timo Aila",
                "Samuli Laine"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02696v1",
                "http://arxiv.org/pdf/2312.02696v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.NE",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02694v1",
            "title": "UPOCR: Towards Unified Pixel-Level OCR Interface",
            "updated": "2023-12-05T11:53:17Z",
            "published": "2023-12-05T11:53:17Z",
            "summary": "In recent years, the optical character recognition (OCR) field has been\nproliferating with plentiful cutting-edge approaches for a wide spectrum of\ntasks. However, these approaches are task-specifically designed with divergent\nparadigms, architectures, and training strategies, which significantly\nincreases the complexity of research and maintenance and hinders the fast\ndeployment in applications. To this end, we propose UPOCR, a\nsimple-yet-effective generalist model for Unified Pixel-level OCR interface.\nSpecifically, the UPOCR unifies the paradigm of diverse OCR tasks as\nimage-to-image transformation and the architecture as a vision Transformer\n(ViT)-based encoder-decoder. Learnable task prompts are introduced to push the\ngeneral feature representations extracted by the encoder toward task-specific\nspaces, endowing the decoder with task awareness. Moreover, the model training\nis uniformly aimed at minimizing the discrepancy between the generated and\nground-truth images regardless of the inhomogeneity among tasks. Experiments\nare conducted on three pixel-level OCR tasks including text removal, text\nsegmentation, and tampered text detection. Without bells and whistles, the\nexperimental results showcase that the proposed method can simultaneously\nachieve state-of-the-art performance on three tasks with a unified single\nmodel, which provides valuable strategies and insights for future research on\ngeneralist OCR models. Code will be publicly available.",
            "author": [
                "Dezhi Peng",
                "Zhenhua Yang",
                "Jiaxin Zhang",
                "Chongyu Liu",
                "Yongxin Shi",
                "Kai Ding",
                "Fengjun Guo",
                "Lianwen Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02694v1",
                "http://arxiv.org/pdf/2312.02694v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02692v1",
            "title": "Gate-tunable graphene Josephson diode effect due to magnetochiral\n  anisotropy",
            "updated": "2023-12-05T11:52:21Z",
            "published": "2023-12-05T11:52:21Z",
            "summary": "Usually the magnetochiral anisotropy related Josephson diode effect is\nassumed to be based on conventional two-dimensional electron gas, such as the\nInAs quantum well. Here we propose a graphene-based Josephson junction as a\nbroadly gate-tunable platform for achieving nonreciprocal supercurrent within\nthe context of magnetochiral anisotropy. We show that the resulting\nnonreciprocal supercurrents will exhibit a sign reversal when the graphene\nswitches from $n$-type doping to $p$-type doping. Particularly, the magnitude\nof the nonreciprocity is highly sensitive to the electrostatic doping level of\ngraphene, enabling gate control of the diode efficiency from zero up to\napproximately $40\\%$. This giant gate-tunability stems from the chiral nature\nof the pseudo-relativistic carriers in grapehe, allowing the graphene Josephson\ndiode emerges as a promising element for advanced superconducting circuits and\ncomputation devices. Moreover, we have also obtained the so-called $0-\\pi$-like\nphase transitions in the current-phase relation, in coincidence with recent\nexperimental finding.",
            "author": [
                "Chuan-Shuai Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02692v1",
                "http://arxiv.org/pdf/2312.02692v1"
            ],
            "primary_category": "cond-mat.supr-con",
            "category": [
                "cond-mat.supr-con"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02685v1",
            "title": "On the differential equations of frozen Calogero-Moser-Sutherland\n  particle models",
            "updated": "2023-12-05T11:42:42Z",
            "published": "2023-12-05T11:42:42Z",
            "summary": "Multivariate Bessel and Jacobi processes describe Calogero-Moser-Sutherland\nparticle models. They depend on a parameter $k$ and are related to\ntime-dependent classical random matrix models like Dysom Brownian motions,\nwhere $k$ has the interpretation of an inverse temperature. There are several\nstochastic limit theorems for $k\\to\\infty$ were the limits depend on the\nsolutions of associated ODEs where these ODEs admit particular simple solutions\nwhich are connected with the zeros of the classical orthogonal polynomials. In\nthis paper we show that these solutions attract all solutions. Moreover we\npresent a connection between the solutions of these ODEs with associated\ninverse heat equations. These inverse heat equations are used to compute the\nexpectations of some determinantal formulas for the Bessel and Jacobi\nprocesses.",
            "author": [
                "Michael Voit"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02685v1",
                "http://arxiv.org/pdf/2312.02685v1"
            ],
            "primary_category": "math.CA",
            "category": [
                "math.CA",
                "math.PR",
                "70F10, 34F05, 60J60, 60B20, 82C22, 33C67"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02684v1",
            "title": "DeepPointMap: Advancing LiDAR SLAM with Unified Neural Descriptors",
            "updated": "2023-12-05T11:40:41Z",
            "published": "2023-12-05T11:40:41Z",
            "summary": "Point clouds have shown significant potential in various domains, including\nSimultaneous Localization and Mapping (SLAM). However, existing approaches\neither rely on dense point clouds to achieve high localization accuracy or use\ngeneralized descriptors to reduce map size. Unfortunately, these two aspects\nseem to conflict with each other. To address this limitation, we propose a\nunified architecture, DeepPointMap, achieving excellent preference on both\naspects. We utilize neural network to extract highly representative and sparse\nneural descriptors from point clouds, enabling memory-efficient map\nrepresentation and accurate multi-scale localization tasks (e.g., odometry and\nloop-closure). Moreover, we showcase the versatility of our framework by\nextending it to more challenging multi-agent collaborative SLAM. The promising\nresults obtained in these scenarios further emphasize the effectiveness and\npotential of our approach.",
            "author": [
                "Xiaze Zhang",
                "Ziheng Ding",
                "Qi Jing",
                "Yuejie Zhang",
                "Wenchao Ding",
                "Rui Feng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02684v1",
                "http://arxiv.org/pdf/2312.02684v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02683v1",
            "title": "Diffusion-Based Speech Enhancement in Matched and Mismatched Conditions\n  Using a Heun-Based Sampler",
            "updated": "2023-12-05T11:40:38Z",
            "published": "2023-12-05T11:40:38Z",
            "summary": "Diffusion models are a new class of generative models that have recently been\napplied to speech enhancement successfully. Previous works have demonstrated\ntheir superior performance in mismatched conditions compared to state-of-the\nart discriminative models. However, this was investigated with a single\ndatabase for training and another one for testing, which makes the results\nhighly dependent on the particular databases. Moreover, recent developments\nfrom the image generation literature remain largely unexplored for speech\nenhancement. These include several design aspects of diffusion models, such as\nthe noise schedule or the reverse sampler. In this work, we systematically\nassess the generalization performance of a diffusion-based speech enhancement\nmodel by using multiple speech, noise and binaural room impulse response (BRIR)\ndatabases to simulate mismatched acoustic conditions. We also experiment with a\nnoise schedule and a sampler that have not been applied to speech enhancement\nbefore. We show that the proposed system substantially benefits from using\nmultiple databases for training, and achieves superior performance compared to\nstate-of-the-art discriminative models in both matched and mismatched\nconditions. We also show that a Heun-based sampler achieves superior\nperformance at a smaller computational cost compared to a sampler commonly used\nfor speech enhancement.",
            "author": [
                "Philippe Gonzalez",
                "Zheng-Hua Tan",
                "Jan \u00d8stergaard",
                "Jesper Jensen",
                "Tommy Sonne Alstr\u00f8m",
                "Tobias May"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02683v1",
                "http://arxiv.org/pdf/2312.02683v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.LG",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02682v1",
            "title": "H-GAP: Humanoid Control with a Generalist Planner",
            "updated": "2023-12-05T11:40:24Z",
            "published": "2023-12-05T11:40:24Z",
            "summary": "Humanoid control is an important research challenge offering avenues for\nintegration into human-centric infrastructures and enabling physics-driven\nhumanoid animations. The daunting challenges in this field stem from the\ndifficulty of optimizing in high-dimensional action spaces and the instability\nintroduced by the bipedal morphology of humanoids. However, the extensive\ncollection of human motion-captured data and the derived datasets of humanoid\ntrajectories, such as MoCapAct, paves the way to tackle these challenges. In\nthis context, we present Humanoid Generalist Autoencoding Planner (H-GAP), a\nstate-action trajectory generative model trained on humanoid trajectories\nderived from human motion-captured data, capable of adeptly handling downstream\ncontrol tasks with Model Predictive Control (MPC). For 56 degrees of freedom\nhumanoid, we empirically demonstrate that H-GAP learns to represent and\ngenerate a wide range of motor behaviours. Further, without any learning from\nonline interactions, it can also flexibly transfer these behaviors to solve\nnovel downstream control tasks via planning. Notably, H-GAP excels established\nMPC baselines that have access to the ground truth dynamics model, and is\nsuperior or comparable to offline RL methods trained for individual tasks.\nFinally, we do a series of empirical studies on the scaling properties of\nH-GAP, showing the potential for performance gains via additional data but not\ncomputing. Code and videos are available at\nhttps://ycxuyingchen.github.io/hgap/.",
            "author": [
                "Zhengyao Jiang",
                "Yingchen Xu",
                "Nolan Wagener",
                "Yicheng Luo",
                "Michael Janner",
                "Edward Grefenstette",
                "Tim Rockt\u00e4schel",
                "Yuandong Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02682v1",
                "http://arxiv.org/pdf/2312.02682v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02679v1",
            "title": "Entanglement and Pseudo Entanglement Dynamics versus Fusion in CFT",
            "updated": "2023-12-05T11:33:33Z",
            "published": "2023-12-05T11:33:33Z",
            "summary": "The fusion rules and operator product expansion (OPE) serve as crucial tools\nin the study of operator algebras within conformal field theory (CFT). Building\nupon the vision of using entanglement to explore the connections between fusion\ncoefficients and OPE coefficients, we employ the replica method and Schmidt\ndecomposition method to investigate the time evolution of entanglement entropy\n(EE) and pseudo entropy (PE) for linear combinations of operators in rational\nconformal field theory (RCFT). We obtain a formula that links fusion\ncoefficients, quantum dimensions, and OPE coefficients. We also identify two\ndefinition schemes for linear combination operators. Under one scheme, the EE\ncaptures information solely for the heaviest operators, while the PE retains\ninformation for all operators, reflecting the phenomenon of pseudo entropy\namplification. Irrespective of the scheme employed, the EE demonstrates a\nstep-like evolution, illustrating the effectiveness of the quasiparticle\npropagation picture for the general superposition of locally excited states in\nRCFT. From the perspective of quasiparticle propagation, we observe spontaneous\nblock-diagonalization of the reduced density matrix of a subsystem when\nquasiparticles enter the subsystem.",
            "author": [
                "Song He",
                "Yu-Xuan Zhang",
                "Long Zhao",
                "Zi-Xuan Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02679v1",
                "http://arxiv.org/pdf/2312.02679v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03032v1",
            "title": "Zero-Shot Point Cloud Registration",
            "updated": "2023-12-05T11:33:16Z",
            "published": "2023-12-05T11:33:16Z",
            "summary": "Learning-based point cloud registration approaches have significantly\noutperformed their traditional counterparts. However, they typically require\nextensive training on specific datasets. In this paper, we propose , the first\nzero-shot point cloud registration approach that eliminates the need for\ntraining on point cloud datasets. The cornerstone of ZeroReg is the novel\ntransfer of image features from keypoints to the point cloud, enriched by\naggregating information from 3D geometric neighborhoods. Specifically, we\nextract keypoints and features from 2D image pairs using a frozen pretrained 2D\nbackbone. These features are then projected in 3D, and patches are constructed\nby searching for neighboring points. We integrate the geometric and visual\nfeatures of each point using our novel parameter-free geometric decoder.\nSubsequently, the task of determining correspondences between point clouds is\nformulated as an optimal transport problem. Extensive evaluations of ZeroReg\ndemonstrate its competitive performance against both traditional and\nlearning-based methods. On benchmarks such as 3DMatch, 3DLoMatch, and ScanNet,\nZeroReg achieves impressive Recall Ratios (RR) of over 84%, 46%, and 75%,\nrespectively.",
            "author": [
                "Weijie Wang",
                "Guofeng Mei",
                "Bin Ren",
                "Xiaoshui Huang",
                "Fabio Poiesi",
                "Luc Van Gool",
                "Nicu Sebe",
                "Bruno Lepri"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03032v1",
                "http://arxiv.org/pdf/2312.03032v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03031v1",
            "title": "Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving?",
            "updated": "2023-12-05T11:32:31Z",
            "published": "2023-12-05T11:32:31Z",
            "summary": "End-to-end autonomous driving recently emerged as a promising research\ndirection to target autonomy from a full-stack perspective. Along this line,\nmany of the latest works follow an open-loop evaluation setting on nuScenes to\nstudy the planning behavior. In this paper, we delve deeper into the problem by\nconducting thorough analyses and demystifying more devils in the details. We\ninitially observed that the nuScenes dataset, characterized by relatively\nsimple driving scenarios, leads to an under-utilization of perception\ninformation in end-to-end models incorporating ego status, such as the ego\nvehicle's velocity. These models tend to rely predominantly on the ego\nvehicle's status for future path planning. Beyond the limitations of the\ndataset, we also note that current metrics do not comprehensively assess the\nplanning quality, leading to potentially biased conclusions drawn from existing\nbenchmarks. To address this issue, we introduce a new metric to evaluate\nwhether the predicted trajectories adhere to the road. We further propose a\nsimple baseline able to achieve competitive results without relying on\nperception annotations. Given the current limitations on the benchmark and\nmetrics, we suggest the community reassess relevant prevailing research and be\ncautious whether the continued pursuit of state-of-the-art would yield\nconvincing and universal conclusions. Code and models are available at\n\\url{https://github.com/NVlabs/BEV-Planner}",
            "author": [
                "Zhiqi Li",
                "Zhiding Yu",
                "Shiyi Lan",
                "Jiahan Li",
                "Jan Kautz",
                "Tong Lu",
                "Jose M. Alvarez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03031v1",
                "http://arxiv.org/pdf/2312.03031v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02676v1",
            "title": "The homology digraph of a preordered space",
            "updated": "2023-12-05T11:30:19Z",
            "published": "2023-12-05T11:30:19Z",
            "summary": "This paper studies a notion of directed homology for preordered spaces,\ncalled the homology digraph. We show that the homology digraph is a directed\nhomotopy invariant and establish variants of the main results of ordinary\nsingular homology theory for the homology digraph. In particular, we prove a\nK\\\"unneth formula, which enables one to compute the homology digraph of a\nproduct of preordered spaces from the homology digraphs of the components.",
            "author": [
                "Catarina Faustino",
                "Thomas Kahl"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02676v1",
                "http://arxiv.org/pdf/2312.02676v1"
            ],
            "primary_category": "math.AT",
            "category": [
                "math.AT",
                "55N35, 55U25"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02674v1",
            "title": "Amortized Bayesian Decision Making for simulation-based models",
            "updated": "2023-12-05T11:29:54Z",
            "published": "2023-12-05T11:29:54Z",
            "summary": "Simulation-based inference (SBI) provides a powerful framework for inferring\nposterior distributions of stochastic simulators in a wide range of domains. In\nmany settings, however, the posterior distribution is not the end goal itself\n-- rather, the derived parameter values and their uncertainties are used as a\nbasis for deciding what actions to take. Unfortunately, because posterior\ndistributions provided by SBI are (potentially crude) approximations of the\ntrue posterior, the resulting decisions can be suboptimal. Here, we address the\nquestion of how to perform Bayesian decision making on stochastic simulators,\nand how one can circumvent the need to compute an explicit approximation to the\nposterior. Our method trains a neural network on simulated data and can predict\nthe expected cost given any data and action, and can, thus, be directly used to\ninfer the action with lowest cost. We apply our method to several benchmark\nproblems and demonstrate that it induces similar cost as the true posterior\ndistribution. We then apply the method to infer optimal actions in a real-world\nsimulator in the medical neurosciences, the Bayesian Virtual Epileptic Patient,\nand demonstrate that it allows to infer actions associated with low cost after\nfew simulations.",
            "author": [
                "Mila Gorecki",
                "Jakob H. Macke",
                "Michael Deistler"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02674v1",
                "http://arxiv.org/pdf/2312.02674v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02673v1",
            "title": "Robust Backdoor Detection for Deep Learning via Topological Evolution\n  Dynamics",
            "updated": "2023-12-05T11:29:12Z",
            "published": "2023-12-05T11:29:12Z",
            "summary": "A backdoor attack in deep learning inserts a hidden backdoor in the model to\ntrigger malicious behavior upon specific input patterns. Existing detection\napproaches assume a metric space (for either the original inputs or their\nlatent representations) in which normal samples and malicious samples are\nseparable. We show that this assumption has a severe limitation by introducing\na novel SSDT (Source-Specific and Dynamic-Triggers) backdoor, which obscures\nthe difference between normal samples and malicious samples.\n  To overcome this limitation, we move beyond looking for a perfect metric\nspace that would work for different deep-learning models, and instead resort to\nmore robust topological constructs. We propose TED (Topological Evolution\nDynamics) as a model-agnostic basis for robust backdoor detection. The main\nidea of TED is to view a deep-learning model as a dynamical system that evolves\ninputs to outputs. In such a dynamical system, a benign input follows a natural\nevolution trajectory similar to other benign inputs. In contrast, a malicious\nsample displays a distinct trajectory, since it starts close to benign samples\nbut eventually shifts towards the neighborhood of attacker-specified target\nsamples to activate the backdoor.\n  Extensive evaluations are conducted on vision and natural language datasets\nacross different network architectures. The results demonstrate that TED not\nonly achieves a high detection rate, but also significantly outperforms\nexisting state-of-the-art detection approaches, particularly in addressing the\nsophisticated SSDT attack. The code to reproduce the results is made public on\nGitHub.",
            "author": [
                "Xiaoxing Mo",
                "Yechao Zhang",
                "Leo Yu Zhang",
                "Wei Luo",
                "Nan Sun",
                "Shengshan Hu",
                "Shang Gao",
                "Yang Xiang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02673v1",
                "http://arxiv.org/pdf/2312.02673v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02672v1",
            "title": "Are Synthetic Data Useful for Egocentric Hand-Object Interaction\n  Detection? An Investigation and the HOI-Synth Domain Adaptation Benchmark",
            "updated": "2023-12-05T11:29:00Z",
            "published": "2023-12-05T11:29:00Z",
            "summary": "In this study, we investigate the effectiveness of synthetic data in\nenhancing hand-object interaction detection within the egocentric vision\ndomain. We introduce a simulator able to generate synthetic images of\nhand-object interactions automatically labeled with hand-object contact states,\nbounding boxes, and pixel-wise segmentation masks. Through comprehensive\nexperiments and comparative analyses on three egocentric datasets, VISOR,\nEgoHOS, and ENIGMA-51, we demonstrate that the use of synthetic data and domain\nadaptation techniques allows for comparable performance to conventional\nsupervised methods while requiring annotations on only a fraction of the real\ndata. When tested with in-domain synthetic data generated from 3D models of\nreal target environments and objects, our best models show consistent\nperformance improvements with respect to standard fully supervised approaches\nbased on labeled real data only. Our study also sets a new benchmark of domain\nadaptation for egocentric hand-object interaction detection (HOI-Synth) and\nprovides baseline results to encourage the community to engage in this\nchallenging task. We release the generated data, code, and the simulator at the\nfollowing link: https://iplab.dmi.unict.it/HOI-Synth/.",
            "author": [
                "Rosario Leonardi",
                "Antonino Furnari",
                "Francesco Ragusa",
                "Giovanni Maria Farinella"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02672v1",
                "http://arxiv.org/pdf/2312.02672v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02670v1",
            "title": "Qubit-environment entanglement in time-dependent pure dephasing",
            "updated": "2023-12-05T11:23:25Z",
            "published": "2023-12-05T11:23:25Z",
            "summary": "We show that the methods for quantification of system-environment\nentanglement that were recently developed for interactions that lead to pure\ndecoherence of the system can be straightforwardly generalized to\ntime-dependent Hamiltonians of the same type. This includes the if-and-only-if\ncriteria of separability, as well as the entanglement measure applicable to\nqubit systems, and methods of detection of entanglement by operations and\nmeasurements performed solely on the system without accessing the environment.\nWe use these methods to study the nature of the decoherence of a\nqubit-oscillator system. Qubit-oscillator entanglement is essential for\ndeveloping bosonic quantum technology with quantum non-Gaussian states and its\napplications in quantum sensing and computing. The dominating bosonic\nplatforms, trapped ions, electromechanics, and superconducting circuits, are\nbased on the time-dependent gates that use such entanglement to achieve new\nquantum sensors and quantum error correction. The step-like time-dependence of\nthe Hamiltonian that is taken into account allows us to capture complex\ninterplay between the build-up of classical and quantum correlations, which\ncould not be replicated in time-independent scenarios.",
            "author": [
                "Ma\u0142gorzata Strza\u0142ka",
                "Radim Filip",
                "Katarzyna Roszak"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02670v1",
                "http://arxiv.org/pdf/2312.02670v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02668v1",
            "title": "Approximate Nash Equilibria Algorithms for Network Design Games",
            "updated": "2023-12-05T11:16:49Z",
            "published": "2023-12-05T11:16:49Z",
            "summary": "We consider a weighted network design game, where selfish players chooses\npaths in a network to minimize their cost. The cost function of each edge in\nthe network is affine linear, namely c_e(W_e) = a_eW_e + b_e, where a_e, b_e >\n0 are only related to the edge e and We is the total weight of the players that\nchoose a path containing edge e. We first show the existence of\n\\alpha-approximate Nash equilibrium and prove the upper bound of \\alpha is\nO(log2(W)), where W is the sum of the weight of all players. Furthermore,\nconsidering that compute the \\alpha-approximate Nash equilibrium is\nPLS-complete, we assume that {ae, be}_{e\\in E} are \\phi-smooth random variables\non [0, 1]. In this case, we show that \\epsilon-better response dynamics can\ncompute the {\\alpha}-approximate Nash Equilibrium in polynomial time by proving\nthe expected number of iterations is polynomial in 1/\\epsilon, \\phi, the number\nof players and the number of edges in the network.",
            "author": [
                "Hangxin Gan",
                "Xianhao Meng",
                "Chunying Ren",
                "Yongtang Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02668v1",
                "http://arxiv.org/pdf/2312.02668v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "math.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02664v1",
            "title": "Domain-Specific Tensor Languages",
            "updated": "2023-12-05T11:09:54Z",
            "published": "2023-12-05T11:09:54Z",
            "summary": "The tensor notation used in several areas of mathematics is a useful one, but\nit is not widely available to the functional programming community. In a\npractical sense, the (embedded) domain-specific languages (DSLs) that are\ncurrently in use for tensor algebra are either 1. array-oriented languages that\ndo not enforce or take advantage of tensor properties and algebraic structure\nor 2. follow the categorical structure of tensors but require the programmer to\nmanipulate tensors in an unwieldy point-free notation. A deeper issue is that\nfor tensor calculus, the dominant pedagogical paradigm assumes an audience\nwhich is either comfortable with notational liberties which programmers cannot\nafford, or focus on the applied mathematics of tensors, largely leaving their\nlinguistic aspects (behaviour of variable binding, syntax and semantics, etc.)\nfor the reader to figure out by themselves. This state of affairs is hardly\nsurprising, because, as we highlight, several properties of standard tensor\nnotation are somewhat exotic from the perspective of lambda calculi. We bridge\nthe gap by defining a DSL, embedded in Haskell, whose syntax closely captures\nthe index notation for tensors in wide use in the literature. The semantics of\nthis EDSL is defined in terms of the algebraic structures which define tensors\nin their full generality. This way, we believe that our EDSL can be used both\nas a tool for scientific computing, but also as a vehicle to express and\npresent the theory and applications of tensors.",
            "author": [
                "Jean-Philippe Bernardy",
                "Patrik Jansson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02664v1",
                "http://arxiv.org/pdf/2312.02664v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03030v1",
            "title": "Generating Visually Realistic Adversarial Patch",
            "updated": "2023-12-05T11:07:39Z",
            "published": "2023-12-05T11:07:39Z",
            "summary": "Deep neural networks (DNNs) are vulnerable to various types of adversarial\nexamples, bringing huge threats to security-critical applications. Among these,\nadversarial patches have drawn increasing attention due to their good\napplicability to fool DNNs in the physical world. However, existing works often\ngenerate patches with meaningless noise or patterns, making it conspicuous to\nhumans. To address this issue, we explore how to generate visually realistic\nadversarial patches to fool DNNs. Firstly, we analyze that a high-quality\nadversarial patch should be realistic, position irrelevant, and printable to be\ndeployed in the physical world. Based on this analysis, we propose an effective\nattack called VRAP, to generate visually realistic adversarial patches.\nSpecifically, VRAP constrains the patch in the neighborhood of a real image to\nensure the visual reality, optimizes the patch at the poorest position for\nposition irrelevance, and adopts Total Variance loss as well as gamma\ntransformation to make the generated patch printable without losing\ninformation. Empirical evaluations on the ImageNet dataset demonstrate that the\nproposed VRAP exhibits outstanding attack performance in the digital world.\nMoreover, the generated adversarial patches can be disguised as the scrawl or\nlogo in the physical world to fool the deep models without being detected,\nbringing significant threats to DNNs-enabled applications.",
            "author": [
                "Xiaosen Wang",
                "Kunyu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03030v1",
                "http://arxiv.org/pdf/2312.03030v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02663v2",
            "title": "FaceStudio: Put Your Face Everywhere in Seconds",
            "updated": "2023-12-06T12:23:36Z",
            "published": "2023-12-05T11:02:45Z",
            "summary": "This study investigates identity-preserving image synthesis, an intriguing\ntask in image generation that seeks to maintain a subject's identity while\nadding a personalized, stylistic touch. Traditional methods, such as Textual\nInversion and DreamBooth, have made strides in custom image creation, but they\ncome with significant drawbacks. These include the need for extensive resources\nand time for fine-tuning, as well as the requirement for multiple reference\nimages. To overcome these challenges, our research introduces a novel approach\nto identity-preserving synthesis, with a particular focus on human images. Our\nmodel leverages a direct feed-forward mechanism, circumventing the need for\nintensive fine-tuning, thereby facilitating quick and efficient image\ngeneration. Central to our innovation is a hybrid guidance framework, which\ncombines stylized images, facial images, and textual prompts to guide the image\ngeneration process. This unique combination enables our model to produce a\nvariety of applications, such as artistic portraits and identity-blended\nimages. Our experimental results, including both qualitative and quantitative\nevaluations, demonstrate the superiority of our method over existing baseline\nmodels and previous works, particularly in its remarkable efficiency and\nability to preserve the subject's identity with high fidelity.",
            "author": [
                "Yuxuan Yan",
                "Chi Zhang",
                "Rui Wang",
                "Yichao Zhou",
                "Gege Zhang",
                "Pei Cheng",
                "Gang Yu",
                "Bin Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02663v2",
                "http://arxiv.org/pdf/2312.02663v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03029v1",
            "title": "Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic\n  Gaussians",
            "updated": "2023-12-05T11:01:44Z",
            "published": "2023-12-05T11:01:44Z",
            "summary": "Creating high-fidelity 3D head avatars has always been a research hotspot,\nbut there remains a great challenge under lightweight sparse view setups. In\nthis paper, we propose Gaussian Head Avatar represented by controllable 3D\nGaussians for high-fidelity head avatar modeling. We optimize the neutral 3D\nGaussians and a fully learned MLP-based deformation field to capture complex\nexpressions. The two parts benefit each other, thereby our method can model\nfine-grained dynamic details while ensuring expression accuracy. Furthermore,\nwe devise a well-designed geometry-guided initialization strategy based on\nimplicit SDF and Deep Marching Tetrahedra for the stability and convergence of\nthe training procedure. Experiments show our approach outperforms other\nstate-of-the-art sparse-view methods, achieving ultra high-fidelity rendering\nquality at 2K resolution even under exaggerated expressions.",
            "author": [
                "Yuelang Xu",
                "Benwang Chen",
                "Zhe Li",
                "Hongwen Zhang",
                "Lizhen Wang",
                "Zerong Zheng",
                "Yebin Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03029v1",
                "http://arxiv.org/pdf/2312.03029v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02661v1",
            "title": "A Self-Commissioning Edge Computing Method for Data-Driven Anomaly\n  Detection in Power Electronic Systems",
            "updated": "2023-12-05T10:56:25Z",
            "published": "2023-12-05T10:56:25Z",
            "summary": "Ensuring the reliability of power electronic converters is a matter of great\nimportance, and data-driven condition monitoring techniques are cementing\nthemselves as an important tool for this purpose. However, translating methods\nthat work well in controlled lab environments to field applications presents\nsignificant challenges, notably because of the limited diversity and accuracy\nof the lab training data. By enabling the use of field data, online machine\nlearning can be a powerful tool to overcome this problem, but it introduces\nadditional challenges in ensuring the stability and predictability of the\ntraining processes. This work presents an edge computing method that mitigates\nthese shortcomings with minimal additional memory usage, by employing an\nautonomous algorithm that prioritizes the storage of training samples with\nlarger prediction errors. The method is demonstrated on the use case of a\nself-commissioning condition monitoring system, in the form of a thermal\nanomaly detection scheme for a variable frequency motor drive, where the\nalgorithm self-learned to distinguish normal and anomalous operation with\nminimal prior knowledge. The obtained results, based on experimental data, show\na significant improvement in prediction accuracy and training speed, when\ncompared to equivalent models trained online without the proposed data\nselection process.",
            "author": [
                "Pere Izquierdo Gomez",
                "Miguel E. Lopez Gajardo",
                "Nenad Mijatovic",
                "Tomislav Dragicevic"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02661v1",
                "http://arxiv.org/pdf/2312.02661v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02660v1",
            "title": "Uniswap Daily Transaction Indices by Network",
            "updated": "2023-12-05T10:53:46Z",
            "published": "2023-12-05T10:53:46Z",
            "summary": "DeFi is transforming financial services by removing intermediaries and\nproducing a wealth of open-source data. This transformation is propelled by\nLayer 2 (L2) solutions, aimed at boosting network efficiency and scalability\nbeyond current Layer 1 (L1) capabilities. This study addresses the lack of\ndetailed L2 impact analysis by examining over 50 million transactions from\nUniswap. Our dataset, featuring transactions from L1 and L2 across networks\nlike Ethereum and Polygon, provides daily indices revealing adoption,\nscalability, and decentralization within the DeFi space. These indices help to\nelucidate the complex relationship between DeFi and L2 technologies, advancing\nour understanding of the ecosystem. The dataset is enhanced by an open-source\nPython framework for computing decentralization indices, adaptable for various\nresearch needs. This positions the dataset as a vital resource for machine\nlearning endeavors, particularly deep learning, contributing significantly to\nthe development of Blockchain as Web3's infrastructure.",
            "author": [
                "Nir Chemaya",
                "Lin William Cong",
                "Emma Jorgensen",
                "Dingyue Liu",
                "Luyao Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02660v1",
                "http://arxiv.org/pdf/2312.02660v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "cs.CE",
                "cs.CR",
                "cs.CY",
                "q-fin.EC",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03028v1",
            "title": "Double Integral Enhanced Zeroing Neural Network Optimized with ALSOA\n  fostered Lung Cancer Classification using CT Images",
            "updated": "2023-12-05T10:53:35Z",
            "published": "2023-12-05T10:53:35Z",
            "summary": "Lung cancer is one of the deadliest diseases and the leading cause of illness\nand death. Since lung cancer cannot predicted at premature stage, it able to\nonly be discovered more broadly once it has spread to other lung parts. The\nrisk grows when radiologists and other specialists determine whether lung\ncancer is current. Owing to significance of determining type of treatment and\nits depth based on severity of the illness, critical to develop smart and\nautomatic cancer prediction scheme is precise, at which stage of cancer. In\nthis paper, Double Integral Enhanced Zeroing Neural Network Optimized with\nALSOA fostered Lung Cancer Classification using CT Images (LCC-DIEZNN-ALSO-CTI)\nis proposed. Initially, input CT image is amassed from lung cancer dataset. The\ninput CT image is pre-processing via Unscented Trainable Kalman Filtering\n(UTKF) technique. In pre-processing stage unwanted noise are removed from CT\nimages. Afterwards, grayscale statistic features and Haralick texture features\nextracted by Adaptive and Concise Empirical Wavelet Transform (ACEWT). The\nproposed model is implemented on MATLAB. The performance of the proposed method\nis analyzed through existing techniques. The proposed method attains 18.32%,\n27.20%, and 34.32% higher accuracy analyzed with existing method likes Deep\nLearning Assisted Predict of Lung Cancer on Computed Tomography Images\nUtilizing AHHMM (LCC-AHHMM-CT), Convolutional neural networks based pulmonary\nnodule malignancy assessment in pipeline for classifying lung cancer\n(LCC-ICNN-CT), Automated Decision Support Scheme for Lung Cancer Identification\nwith Categorization (LCC-RFCN-MLRPN-CT) methods respectively.",
            "author": [
                "V S Priya Sumitha",
                "V. Keerthika",
                "A. Geetha"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03028v1",
                "http://arxiv.org/pdf/2312.03028v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02659v1",
            "title": "Supervised learning of spatial features with STDP and homeostasis using\n  Spiking Neural Networks on SpiNNaker",
            "updated": "2023-12-05T10:53:31Z",
            "published": "2023-12-05T10:53:31Z",
            "summary": "Artificial Neural Networks (ANN) have gained large popularity thanks to their\nability to learn using the well-known backpropagation algorithm. On the other\nhand, Spiking Neural Networks (SNNs), despite having wider abilities than ANNs,\nhave always presented a challenge in the training phase. This paper shows a new\nmethod to perform supervised learning on SNNs, using Spike Timing Dependent\nPlasticity (STDP) and homeostasis, aiming at training the network to identify\nspatial patterns. The method is tested using the SpiNNaker digital\narchitecture. A SNN is trained to recognise one or multiple patterns and\nperformance metrics are extracted to measure the performance of the network.\nSome considerations are drawn from the results showing that, in the case of a\nsingle trained pattern, the network behaves as the ideal detector, with 100%\naccuracy in detecting the trained pattern. However, as the number of trained\npatterns on a single network increases, the accuracy of the identification is\nlinked to the similarities between these patterns. This method of training an\nSNN to detect spatial patterns may be applied on pattern recognition in static\nimages or traffic analysis in computer networks, where each network packet\nrepresents a spatial pattern. It will be stipulated that the homeostatic factor\nmay enable the network to detect patterns with some degree of similarities,\nrather than only perfectly matching patterns.",
            "author": [
                "Sergio Davies",
                "Andrew Gait",
                "Andrew Rowley",
                "Alessandro Di Nuovo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02659v1",
                "http://arxiv.org/pdf/2312.02659v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02652v1",
            "title": "What Machine Learning Can Do for Focusing Aerogel Detectors",
            "updated": "2023-12-05T10:46:16Z",
            "published": "2023-12-05T10:46:16Z",
            "summary": "Particle identification at the Super Charm-Tau factory experiment will be\nprovided by a Focusing Aerogel Ring Imaging CHerenkov detector (FARICH). The\nspecifics of detector location make proper cooling difficult, therefore a\nsignificant number of ambient background hits are captured. They must be\nmitigated to reduce the data flow and improve particle velocity resolution. In\nthis work we present several approaches to filtering signal hits, inspired by\nmachine learning techniques from computer vision.",
            "author": [
                "Foma Shipilov",
                "Alexander Barnyakov",
                "Vladimir Bobrovnikov",
                "Sergey Kononov",
                "Fedor Ratnikov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02652v1",
                "http://arxiv.org/pdf/2312.02652v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03759v1",
            "title": "How should the advent of large language models affect the practice of\n  science?",
            "updated": "2023-12-05T10:45:12Z",
            "published": "2023-12-05T10:45:12Z",
            "summary": "Large language models (LLMs) are being increasingly incorporated into\nscientific workflows. However, we have yet to fully grasp the implications of\nthis integration. How should the advent of large language models affect the\npractice of science? For this opinion piece, we have invited four diverse\ngroups of scientists to reflect on this query, sharing their perspectives and\nengaging in debate. Schulz et al. make the argument that working with LLMs is\nnot fundamentally different from working with human collaborators, while Bender\net al. argue that LLMs are often misused and over-hyped, and that their\nlimitations warrant a focus on more specialized, easily interpretable tools.\nMarelli et al. emphasize the importance of transparent attribution and\nresponsible use of LLMs. Finally, Botvinick and Gershman advocate that humans\nshould retain responsibility for determining the scientific roadmap. To\nfacilitate the discussion, the four perspectives are complemented with a\nresponse from each group. By putting these different perspectives in\nconversation, we aim to bring attention to important considerations within the\nacademic community regarding the adoption of LLMs and their impact on both\ncurrent and future scientific practices.",
            "author": [
                "Marcel Binz",
                "Stephan Alaniz",
                "Adina Roskies",
                "Balazs Aczel",
                "Carl T. Bergstrom",
                "Colin Allen",
                "Daniel Schad",
                "Dirk Wulff",
                "Jevin D. West",
                "Qiong Zhang",
                "Richard M. Shiffrin",
                "Samuel J. Gershman",
                "Ven Popov",
                "Emily M. Bender",
                "Marco Marelli",
                "Matthew M. Botvinick",
                "Zeynep Akata",
                "Eric Schulz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03759v1",
                "http://arxiv.org/pdf/2312.03759v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CY",
                "cs.DL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02650v1",
            "title": "Learning convex objectives to reduce the complexity of model predictive\n  control",
            "updated": "2023-12-05T10:42:10Z",
            "published": "2023-12-05T10:42:10Z",
            "summary": "For large systems that consider uncertainty the online solution of model\npredictive control problems can be computationally taxing, and even infeasible.\nThis can be offset by using a shorter horizon, however this can in turn result\nin poor controller performance. In this work we consider the task of learning a\nconvex cost-to-go to allow the use of a short, potentially single step, control\nhorizon to reduce the online computational cost. We consider two surrogates:\n(1) a convex interpolating function and (2) an input-convex neural network. We\nhighlight that irrespective of the choice of surrogate the behaviour of the\nsurrogate near the origin and ability of the surrogate to describe the feasible\nregion are key concerns for the closed loop performance of the new MPC problem.\nWe address these concerns by tailoring the design of the surrogate to ensure\ngood performance in both aspects. The paper concludes with a numerical example,\nshowing the clear and significant reduction in computational complexity through\nthe use of these convex surrogates.",
            "author": [
                "E. M. Turan",
                "Z. Mdoe",
                "J. J\u00e4schke"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02650v1",
                "http://arxiv.org/pdf/2312.02650v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02647v1",
            "title": "TPA3D: Triplane Attention for Fast Text-to-3D Generation",
            "updated": "2023-12-05T10:39:37Z",
            "published": "2023-12-05T10:39:37Z",
            "summary": "Due to the lack of large-scale text-3D correspondence data, recent text-to-3D\ngeneration works mainly rely on utilizing 2D diffusion models for synthesizing\n3D data. Since diffusion-based methods typically require significant\noptimization time for both training and inference, the use of GAN-based models\nwould still be desirable for fast 3D generation. In this work, we propose\nTriplane Attention for text-guided 3D generation (TPA3D), an end-to-end\ntrainable GAN-based deep learning model for fast text-to-3D generation. With\nonly 3D shape data and their rendered 2D images observed during training, our\nTPA3D is designed to retrieve detailed visual descriptions for synthesizing the\ncorresponding 3D mesh data. This is achieved by the proposed attention\nmechanisms on the extracted sentence and word-level text features. In our\nexperiments, we show that TPA3D generates high-quality 3D textured shapes\naligned with fine-grained descriptions, while impressive computation efficiency\ncan be observed.",
            "author": [
                "Hong-En Chen",
                "Bin-Shih Wu",
                "Sheng-Yu Huang",
                "Yu-Chiang Frank Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02647v1",
                "http://arxiv.org/pdf/2312.02647v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02639v1",
            "title": "Dominance complexes, neighborhood complexes and combinatorial Alexander\n  duals",
            "updated": "2023-12-05T10:25:09Z",
            "published": "2023-12-05T10:25:09Z",
            "summary": "We show that the dominance complex $\\mathcal{D}(G)$ of a graph $G$ coincides\nwith the combinatorial Alexander dual of the neighborhood complex\n$\\mathcal{N}(\\overline{G})$ of the complement of $G$. Using this, we obtain a\nrelation between the chromatic number $\\chi(G)$ of $G$ and the homology group\nof $\\mathcal{D}(G)$. We also obtain several known results related to dominance\ncomplexes from well-known facts of neighborhood complexes. After that, we\nsuggest a new method for computing the homology groups of the dominance\ncomplexes, using independence complexes of simple graphs. We show that several\nknown computations of homology groups of dominance complexes can be reduced to\nknown computations of independence complexes. Finally, we determine the\nhomology group of $\\mathcal{D}(P_n \\times P_3)$ by determining the homotopy\ntypes of the independence complex of $P_n \\times P_3 \\times P_2$.",
            "author": [
                "Takahiro Matsushita",
                "Shun Wakatsuki"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02639v1",
                "http://arxiv.org/pdf/2312.02639v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.AT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02638v1",
            "title": "Synchronization is All You Need: Exocentric-to-Egocentric Transfer for\n  Temporal Action Segmentation with Unlabeled Synchronized Video Pairs",
            "updated": "2023-12-05T10:24:43Z",
            "published": "2023-12-05T10:24:43Z",
            "summary": "We consider the problem of transferring a temporal action segmentation system\ninitially designed for exocentric (fixed) cameras to an egocentric scenario,\nwhere wearable cameras capture video data. The conventional supervised approach\nrequires the collection and labeling of a new set of egocentric videos to adapt\nthe model, which is costly and time-consuming. Instead, we propose a novel\nmethodology which performs the adaptation leveraging existing labeled\nexocentric videos and a new set of unlabeled, synchronized\nexocentric-egocentric video pairs, for which temporal action segmentation\nannotations do not need to be collected. We implement the proposed methodology\nwith an approach based on knowledge distillation, which we investigate both at\nthe feature and model level. To evaluate our approach, we introduce a new\nbenchmark based on the Assembly101 dataset. Results demonstrate the feasibility\nand effectiveness of the proposed method against classic unsupervised domain\nadaptation and temporal sequence alignment approaches. Remarkably, without\nbells and whistles, our best model performs on par with supervised approaches\ntrained on labeled egocentric data, without ever seeing a single egocentric\nlabel, achieving a +15.99% (28.59% vs 12.60%) improvement in the edit score on\nthe Assembly101 dataset compared to a baseline model trained solely on\nexocentric data.",
            "author": [
                "Camillo Quattrocchi",
                "Antonino Furnari",
                "Daniele Di Mauro",
                "Mario Valerio Giuffrida",
                "Giovanni Maria Farinella"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02638v1",
                "http://arxiv.org/pdf/2312.02638v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02634v1",
            "title": "A 3D kinetic Monte Carlo study of streamer discharges in CO$_2$",
            "updated": "2023-12-05T10:17:11Z",
            "published": "2023-12-05T10:17:11Z",
            "summary": "We theoretically study the inception and propagation of positive and negative\nstreamers in CO$_2$. Our study is done in 3D, using a newly formulated kinetic\nMonte Carlo discharge model where the electrons are described as drifting and\ndiffusing particles that adhere to the local field approximation. Our emphasis\nlies on electron attachment and photoionization. For negative streamers we find\nthat dissociative attachment in the streamer channels leads to appearance of\nlocalized segments of increased electric fields, while an analogous feature is\nnot observed for positive-polarity discharges. Positive streamers, unlike\nnegative streamers, require free electrons ahead of them in order to propagate.\nIn CO$_2$, just as in air, these electrons are supplied through\nphotoionization. However, ionizing radiation in CO$_2$ is absorbed quite\nrapidly and is also weaker than in air, which has important ramifications for\nthe emerging positive streamer morphology (radius, velocity, and fields). We\nperform a computational analysis which shows that positive streamers can\npropagate due to photoionization in CO$_2$. Conversely, photoionization has no\naffect on negative streamer fronts, but plays a major role in the coupling\nbetween negative streamers and the cathode. Photoionization in CO$_2$ is\ntherefore important for the propagation of both positive and negative\nstreamers. Our results are relevant in several applications, e.g., CO$_2$\nconversion and high-voltage technology (where CO$_2$ is used in pure form or\nadmixed with other gases).",
            "author": [
                "Robert Marskar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02634v1",
                "http://arxiv.org/pdf/2312.02634v1"
            ],
            "primary_category": "physics.plasm-ph",
            "category": [
                "physics.plasm-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02632v1",
            "title": "How Good Is Open Bicycle Infrastructure Data? A Countrywide Case Study\n  of Denmark",
            "updated": "2023-12-05T10:14:48Z",
            "published": "2023-12-05T10:14:48Z",
            "summary": "Cycling is a key ingredient for a sustainability shift of Denmark's\ntransportation system. To increase cycling rates, a better nationwide network\nof bicycle infrastructure is required. Planning such a network requires\nhigh-quality infrastructure data, however, the quality of bicycle\ninfrastructure data is severely understudied. Here, we compare Denmark's two\nlargest open data sets on dedicated bicycle infrastructure, OpenStreetMap (OSM)\nand GeoDanmark, in a countrywide data quality assessment, asking whether data\nis good enough for network-based analysis of cycling conditions. We find that\nneither of the data sets is of sufficient quality, and that data set conflation\nis necessary to obtain a complete dataset. Our analysis of the spatial\nvariation of data quality suggests that rural areas are more likely to suffer\nfrom problems with data completeness. We demonstrate that the prevalent method\nof using infrastructure density as a proxy for data completeness is not\nsuitable for bicycle infrastructure data, and that matching of corresponding\nfeatures thus is necessary to assess data completeness. Based on our data\nquality assessment we recommend strategic mapping efforts towards data\ncompleteness, consistent standards to support comparability between different\ndata sources, and increased focus on data topology to ensure high-quality\nbicycle network data.",
            "author": [
                "Ane Rahbek Vier\u00f8",
                "Anastassia Vybornova",
                "Michael Szell"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02632v1",
                "http://arxiv.org/pdf/2312.02632v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03027v1",
            "title": "Stable Diffusion Exposed: Gender Bias from Prompt to Image",
            "updated": "2023-12-05T10:12:59Z",
            "published": "2023-12-05T10:12:59Z",
            "summary": "Recent studies have highlighted biases in generative models, shedding light\non their predisposition towards gender-based stereotypes and imbalances. This\npaper contributes to this growing body of research by introducing an evaluation\nprotocol designed to automatically analyze the impact of gender indicators on\nStable Diffusion images. Leveraging insights from prior work, we explore how\ngender indicators not only affect gender presentation but also the\nrepresentation of objects and layouts within the generated images. Our findings\ninclude the existence of differences in the depiction of objects, such as\ninstruments tailored for specific genders, and shifts in overall layouts. We\nalso reveal that neutral prompts tend to produce images more aligned with\nmasculine prompts than their feminine counterparts, providing valuable insights\ninto the nuanced gender biases inherent in Stable Diffusion.",
            "author": [
                "Yankun Wu",
                "Yuta Nakashima",
                "Noa Garcia"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03027v1",
                "http://arxiv.org/pdf/2312.03027v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02626v1",
            "title": "Predicting Network Congestion by Extending Betweenness Centrality to\n  Interacting Agents",
            "updated": "2023-12-05T10:01:28Z",
            "published": "2023-12-05T10:01:28Z",
            "summary": "We present a simple model to predict network activity at the edge level, by\nextending a known approximation method to compute Betweenness Centrality (BC)\nwith a repulsive mechanism to prevent unphysical densities. By taking into\naccount the strong interaction effects often observed in real phenomena, we aim\nto obtain an improved measure of edge usage during rush hours as traffic\ncongestion patterns emerge in urban networks. In this approach, the network is\niteratively populated by agents following dynamically evolving fastest paths,\nthat are progressively attracted towards uncongested parts of the network, as\nthe global traffic volume increases. Following the transition of the network\nstate from empty to saturated, we study the emergence of congestion and the\nprogressive disruption of global connectivity due to a relatively small\nfraction of crowded edges.\n  We assess the predictive power of our model by comparing the speed\ndistribution against a large experimental dataset for the London area with\nremarkable results, which also translate into a qualitative similarity of the\ncongestion maps. Also, percolation analysis confirms a quantitative agreement\nof the model with the real data for London. For seven other topologically\ndifferent cities we performed simulations to obtain the Fisher critical\nexponent $\\tau$ that showed no common functional dependence on the traffic\nlevel. The critical exponent $\\gamma$, studied to assess the power-law decay of\nspatial correlation, was found inversely proportional to the number of vehicles\nboth for real and simulated traffic.\n  This simulation approach seems particularly fit to describe qualitative and\nquantitative properties of the network loading process, culminating in\npeak-hour congestion, by using only topological and geographical network\nfeatures.",
            "author": [
                "Marco Cogoni",
                "Giovanni Busonera"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02626v1",
                "http://arxiv.org/pdf/2312.02626v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02625v1",
            "title": "Diffusion Noise Feature: Accurate and Fast Generated Image Detection",
            "updated": "2023-12-05T10:01:11Z",
            "published": "2023-12-05T10:01:11Z",
            "summary": "Generative models have reached an advanced stage where they can produce\nremarkably realistic images. However, this remarkable generative capability\nalso introduces the risk of disseminating false or misleading information.\nNotably, existing image detectors for generated images encounter challenges\nsuch as low accuracy and limited generalization. This paper seeks to address\nthis issue by seeking a representation with strong generalization capabilities\nto enhance the detection of generated images. Our investigation has revealed\nthat real and generated images display distinct latent Gaussian representations\nwhen subjected to an inverse diffusion process within a pre-trained diffusion\nmodel. Exploiting this disparity, we can amplify subtle artifacts in generated\nimages. Building upon this insight, we introduce a novel image representation\nknown as Diffusion Noise Feature (DNF). DNF is an ensemble representation that\nestimates the noise generated during the inverse diffusion process. A simple\nclassifier, e.g., ResNet, trained on DNF achieves high accuracy, robustness,\nand generalization capabilities for detecting generated images, even from\npreviously unseen classes or models. We conducted experiments using a widely\nrecognized and standard dataset, achieving state-of-the-art effects of\nDetection.",
            "author": [
                "Yichi Zhang",
                "Xiaogang Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02625v1",
                "http://arxiv.org/pdf/2312.02625v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02617v1",
            "title": "DreaMo: Articulated 3D Reconstruction From A Single Casual Video",
            "updated": "2023-12-05T09:47:37Z",
            "published": "2023-12-05T09:47:37Z",
            "summary": "Articulated 3D reconstruction has valuable applications in various domains,\nyet it remains costly and demands intensive work from domain experts. Recent\nadvancements in template-free learning methods show promising results with\nmonocular videos. Nevertheless, these approaches necessitate a comprehensive\ncoverage of all viewpoints of the subject in the input video, thus limiting\ntheir applicability to casually captured videos from online sources. In this\nwork, we study articulated 3D shape reconstruction from a single and casually\ncaptured internet video, where the subject's view coverage is incomplete. We\npropose DreaMo that jointly performs shape reconstruction while solving the\nchallenging low-coverage regions with view-conditioned diffusion prior and\nseveral tailored regularizations. In addition, we introduce a skeleton\ngeneration strategy to create human-interpretable skeletons from the learned\nneural bones and skinning weights. We conduct our study on a self-collected\ninternet video collection characterized by incomplete view coverage. DreaMo\nshows promising quality in novel-view rendering, detailed articulated shape\nreconstruction, and skeleton generation. Extensive qualitative and quantitative\nstudies validate the efficacy of each proposed component, and show existing\nmethods are unable to solve correct geometry due to the incomplete view\ncoverage.",
            "author": [
                "Tao Tu",
                "Ming-Feng Li",
                "Chieh Hubert Lin",
                "Yen-Chi Cheng",
                "Min Sun",
                "Ming-Hsuan Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02617v1",
                "http://arxiv.org/pdf/2312.02617v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02616v1",
            "title": "Facilitating the Production of Well-tailored Video Summaries for Sharing\n  on Social Media",
            "updated": "2023-12-05T09:47:28Z",
            "published": "2023-12-05T09:47:28Z",
            "summary": "This paper presents a web-based tool that facilitates the production of\ntailored summaries for online sharing on social media. Through an interactive\nuser interface, it supports a ``one-click'' video summarization process. Based\non the integrated AI models for video summarization and aspect ratio\ntransformation, it facilitates the generation of multiple summaries of a\nfull-length video according to the needs of target platforms with regard to the\nvideo's length and aspect ratio.",
            "author": [
                "Evlampios Apostolidis",
                "Konstantinos Apostolidis",
                "Vasileios Mezaris"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02616v1",
                "http://arxiv.org/pdf/2312.02616v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02615v1",
            "title": "Projection Regret: Reducing Background Bias for Novelty Detection via\n  Diffusion Models",
            "updated": "2023-12-05T09:44:47Z",
            "published": "2023-12-05T09:44:47Z",
            "summary": "Novelty detection is a fundamental task of machine learning which aims to\ndetect abnormal ($\\textit{i.e.}$ out-of-distribution (OOD)) samples. Since\ndiffusion models have recently emerged as the de facto standard generative\nframework with surprising generation results, novelty detection via diffusion\nmodels has also gained much attention. Recent methods have mainly utilized the\nreconstruction property of in-distribution samples. However, they often suffer\nfrom detecting OOD samples that share similar background information to the\nin-distribution data. Based on our observation that diffusion models can\n\\emph{project} any sample to an in-distribution sample with similar background\ninformation, we propose \\emph{Projection Regret (PR)}, an efficient novelty\ndetection method that mitigates the bias of non-semantic information. To be\nspecific, PR computes the perceptual distance between the test image and its\ndiffusion-based projection to detect abnormality. Since the perceptual distance\noften fails to capture semantic changes when the background information is\ndominant, we cancel out the background bias by comparing it against recursive\nprojections. Extensive experiments demonstrate that PR outperforms the prior\nart of generative-model-based novelty detection methods by a significant\nmargin.",
            "author": [
                "Sungik Choi",
                "Hankook Lee",
                "Honglak Lee",
                "Moontae Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02615v1",
                "http://arxiv.org/pdf/2312.02615v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02614v1",
            "title": "Prompt Optimization via Adversarial In-Context Learning",
            "updated": "2023-12-05T09:44:45Z",
            "published": "2023-12-05T09:44:45Z",
            "summary": "We propose a new method, Adversarial In-Context Learning (adv-ICL), to\noptimize prompt for in-context learning (ICL) by employing one LLM as a\ngenerator, another as a discriminator, and a third as a prompt modifier. As in\ntraditional adversarial learning, adv-ICL is implemented as a two-player game\nbetween the generator and discriminator, where the generator tries to generate\nrealistic enough output to fool the discriminator. In each round, given an\ninput prefixed by task instructions and several exemplars, the generator\nproduces an output. The discriminator is then tasked with classifying the\ngenerator input-output pair as model-generated or real data. Based on the\ndiscriminator loss, the prompt modifier proposes possible edits to the\ngenerator and discriminator prompts, and the edits that most improve the\nadversarial loss are selected. We show that adv-ICL results in significant\nimprovements over state-of-the-art prompt optimization techniques for both open\nand closed-source models on 11 generation and classification tasks including\nsummarization, arithmetic reasoning, machine translation, data-to-text\ngeneration, and the MMLU and big-bench hard benchmarks. In addition, because\nour method uses pre-trained models and updates only prompts rather than model\nparameters, it is computationally efficient, easy to extend to any LLM and\ntask, and effective in low-resource settings.",
            "author": [
                "Xuan Long Do",
                "Yiran Zhao",
                "Hannah Brown",
                "Yuxi Xie",
                "James Xu Zhao",
                "Nancy F. Chen",
                "Kenji Kawaguchi",
                "Michael Qizhe Xie",
                "Junxian He"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02614v1",
                "http://arxiv.org/pdf/2312.02614v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02613v1",
            "title": "A Unified Simulation Framework for Visual and Behavioral Fidelity in\n  Crowd Analysis",
            "updated": "2023-12-05T09:43:27Z",
            "published": "2023-12-05T09:43:27Z",
            "summary": "Simulation is a powerful tool to easily generate annotated data, and a highly\ndesirable feature, especially in those domains where learning models need large\ntraining datasets. Machine learning and deep learning solutions, have proven to\nbe extremely data-hungry and sometimes, the available real-world data are not\nsufficient to effectively model the given task. Despite the initial skepticism\nof a portion of the scientific community, the potential of simulation has been\nlargely confirmed in many application areas, and the recent developments in\nterms of rendering and virtualization engines, have shown a good ability also\nin representing complex scenes. This includes environmental factors, such as\nweather conditions and surface reflectance, as well as human-related events,\nlike human actions and behaviors. We present a human crowd simulator, called\nUniCrowd, and its associated validation pipeline. We show how the simulator can\ngenerate annotated data, suitable for computer vision tasks, in particular for\ndetection and segmentation, as well as the related applications, as crowd\ncounting, human pose estimation, trajectory analysis and prediction, and\nanomaly detection.",
            "author": [
                "Niccol\u00f2 Bisagno",
                "Nicola Garau",
                "Antonio Luigi Stefani",
                "Nicola Conci"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02613v1",
                "http://arxiv.org/pdf/2312.02613v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02611v1",
            "title": "Privacy-Aware Data Acquisition under Data Similarity in Regression\n  Markets",
            "updated": "2023-12-05T09:39:04Z",
            "published": "2023-12-05T09:39:04Z",
            "summary": "Data markets facilitate decentralized data exchange for applications such as\nprediction, learning, or inference. The design of these markets is challenged\nby varying privacy preferences as well as data similarity among data owners.\nRelated works have often overlooked how data similarity impacts pricing and\ndata value through statistical information leakage. We demonstrate that data\nsimilarity and privacy preferences are integral to market design and propose a\nquery-response protocol using local differential privacy for a two-party data\nacquisition mechanism. In our regression data market model, we analyze\nstrategic interactions between privacy-aware owners and the learner as a\nStackelberg game over the asked price and privacy factor. Finally, we\nnumerically evaluate how data similarity affects market participation and\ntraded data value.",
            "author": [
                "Shashi Raj Pandey",
                "Pierre Pinson",
                "Petar Popovski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02611v1",
                "http://arxiv.org/pdf/2312.02611v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02608v1",
            "title": "Panoptica -- instance-wise evaluation of 3D semantic and instance\n  segmentation maps",
            "updated": "2023-12-05T09:34:56Z",
            "published": "2023-12-05T09:34:56Z",
            "summary": "This paper introduces panoptica, a versatile and performance-optimized\npackage designed for computing instance-wise segmentation quality metrics from\n2D and 3D segmentation maps. panoptica addresses the limitations of existing\nmetrics and provides a modular framework that complements the original\nintersection over union-based panoptic quality with other metrics, such as the\ndistance metric Average Symmetric Surface Distance. The package is open-source,\nimplemented in Python, and accompanied by comprehensive documentation and\ntutorials. panoptica employs a three-step metrics computation process to cover\ndiverse use cases. The efficacy of panoptica is demonstrated on various\nreal-world biomedical datasets, where an instance-wise evaluation is\ninstrumental for an accurate representation of the underlying clinical task.\nOverall, we envision panoptica as a valuable tool facilitating in-depth\nevaluation of segmentation methods.",
            "author": [
                "Florian Kofler",
                "Hendrik M\u00f6ller",
                "Josef A. Buchner",
                "Ezequiel de la Rosa",
                "Ivan Ezhov",
                "Marcel Rosier",
                "Isra Mekki",
                "Suprosanna Shit",
                "Moritz Negwer",
                "Rami Al-Maskari",
                "Ali Ert\u00fcrk",
                "Shankeeth Vinayahalingam",
                "Fabian Isensee",
                "Sarthak Pati",
                "Daniel Rueckert",
                "Jan S. Kirschke",
                "Stefan K. Ehrlich",
                "Annika Reinke",
                "Bjoern Menze",
                "Benedikt Wiestler",
                "Marie Piraud"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02608v1",
                "http://arxiv.org/pdf/2312.02608v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02605v1",
            "title": "Accelerating Learnt Video Codecs with Gradient Decay and Layer-wise\n  Distillation",
            "updated": "2023-12-05T09:26:09Z",
            "published": "2023-12-05T09:26:09Z",
            "summary": "In recent years, end-to-end learnt video codecs have demonstrated their\npotential to compete with conventional coding algorithms in term of compression\nefficiency. However, most learning-based video compression models are\nassociated with high computational complexity and latency, in particular at the\ndecoder side, which limits their deployment in practical applications. In this\npaper, we present a novel model-agnostic pruning scheme based on gradient decay\nand adaptive layer-wise distillation. Gradient decay enhances parameter\nexploration during sparsification whilst preventing runaway sparsity and is\nsuperior to the standard Straight-Through Estimation. The adaptive layer-wise\ndistillation regulates the sparse training in various stages based on the\ndistortion of intermediate features. This stage-wise design efficiently updates\nparameters with minimal computational overhead. The proposed approach has been\napplied to three popular end-to-end learnt video codecs, FVC, DCVC, and\nDCVC-HEM. Results confirm that our method yields up to 65% reduction in MACs\nand 2x speed-up with less than 0.3dB drop in BD-PSNR. Supporting code and\nsupplementary material can be downloaded from:\nhttps://jasminepp.github.io/lightweightdvc/",
            "author": [
                "Tianhao Peng",
                "Ge Gao",
                "Heming Sun",
                "Fan Zhang",
                "David Bull"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02605v1",
                "http://arxiv.org/pdf/2312.02605v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02604v1",
            "title": "Shedding light on charmonium",
            "updated": "2023-12-05T09:25:35Z",
            "published": "2023-12-05T09:25:35Z",
            "summary": "We investigate E1 radiative transitions within charmonium in a relativistic\napproach based on light-front QCD. In quantum field theory, two sets of\nprocesses are pure E1: $\\chi_{c0} \\to J/\\psi \\gamma$ ($\\psi\\to\n\\chi_{c0}\\gamma$) and $h_c \\to \\eta_c\\gamma$ ($\\eta_c' \\to h_c\\gamma$), both\ninvolving the $P$-wave charmonia. We compute the E1 radiative decay widths as\nwell as the corresponding transition form factors of various processes\nincluding those involving $2P$ states. These observables provide an access to\nthe microscopic structures of the $P$-wave charmonium. We show that our\nparameter-free predictions are in excellent agreement with the experimental\nmeasurements as well as lattice simulations whenever available.",
            "author": [
                "Zhiguo Wang",
                "Meijian Li",
                "Yang Li",
                "James P. Vary"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02604v1",
                "http://arxiv.org/pdf/2312.02604v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "nucl-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02602v1",
            "title": "Recovery of damaged information via scrambling in indefinite casual\n  order",
            "updated": "2023-12-05T09:21:20Z",
            "published": "2023-12-05T09:21:20Z",
            "summary": "Scrambling prevents the access to local information with local operators and\ntherefore can be used to protect quantum information from damage caused by\nlocal perturbations. Even though partial quantum information can be recovered\nif the type of the damage is known, the initial target state cannot be\ncompletely recovered, because the obtained state is a mixture of the initial\nstate and a maximally mixed state. Here, we demonstrate an improved scheme to\nrecover damaged quantum information via scrambling in indefinite casual order.\nWe can record the type of damage and improve the fidelity of the recovered\nquantum state with respect to the original one. Moreover, by iterating the\nschemes, the initial quantum state can be completely retrieved. In addition, we\nexperimentally demonstrate our schemes on the cloud-based quantum computer,\nnamed as Quafu. Our work proposes a feasible scheme to protect whole quantum\ninformation from damage, which is also compatible with other techniques such as\nquantum error corrections and entanglement purification protocols.",
            "author": [
                "Tian-Ren Jin",
                "Tian-Ming Li",
                "Zheng-An Wang",
                "Kai Xu",
                "Yu-Ran Zhang",
                "Heng Fan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02602v1",
                "http://arxiv.org/pdf/2312.02602v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02601v1",
            "title": "A Neural Receiver for 5G NR Multi-user MIMO",
            "updated": "2023-12-05T09:19:26Z",
            "published": "2023-12-05T09:19:26Z",
            "summary": "We introduce a neural network (NN)-based multiuser multiple-input\nmultiple-output (MU-MIMO) receiver with 5G New Radio (5G NR) physical uplink\nshared channel (PUSCH) compatibility. The NN architecture is based on\nconvolution layers to exploit the time and frequency correlation of the channel\nand a graph neural network (GNN) to handle multiple users. The proposed\narchitecture adapts to an arbitrary number of sub-carriers and supports a\nvarying number of multiple-input multiple-output (MIMO) layers and users\nwithout the need for any retraining. The receiver operates on an entire 5G NR\nslot, i.e., processes the entire received orthogonal frequency division\nmultiplexing (OFDM) time-frequency resource grid by jointly performing channel\nestimation, equalization, and demapping. The proposed architecture operates\nless than 1 dB away from a baseline using linear minimum mean square error\n(LMMSE) channel estimation with K-best detection but benefits from a\nsignificantly lower computational complexity. We show the importance of a\ncarefully designed training process such that the trained receiver is universal\nfor a wide range of different unseen channel conditions. Finally, we\ndemonstrate the results of a hardware-in-the-loop verification based on 3GPP\ncompliant conformance test scenarios.",
            "author": [
                "Sebastian Cammerer",
                "Fay\u00e7al A\u00eft Aoudia",
                "Jakob Hoydis",
                "Andreas Oeldemann",
                "Andreas Roessler",
                "Timo Mayer",
                "Alexander Keller"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02601v1",
                "http://arxiv.org/pdf/2312.02601v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02598v1",
            "title": "Impact of Tokenization on LLaMa Russian Adaptation",
            "updated": "2023-12-05T09:16:03Z",
            "published": "2023-12-05T09:16:03Z",
            "summary": "Latest instruction-tuned large language models (LLM) show great results on\nvarious tasks, however, they often face performance degradation for non-English\ninput. There is evidence that the reason lies in inefficient tokenization\ncaused by low language representation in pre-training data which hinders the\ncomprehension of non-English instructions, limiting the potential of target\nlanguage instruction-tuning. In this work we investigate the possibility of\naddressing the issue with vocabulary substitution in the context of LLaMa\nRussian language adaptation. We explore three variants of vocabulary adaptation\nand test their performance on Saiga instruction-tuning and fine-tuning on\nRussian Super Glue benchmark. The results of automatic evaluation show that\nvocabulary substitution not only improves the model's quality in Russian but\nalso accelerates fine-tuning (35%) and inference (up to 60%) while reducing\nmemory consumption. Additional human evaluation of the instruction-tuned models\ndemonstrates that models with Russian-adapted vocabulary generate answers with\nhigher user preference than the original Saiga-LLaMa model.",
            "author": [
                "Mikhail Tikhomirov",
                "Daniil Chernyshev"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02598v1",
                "http://arxiv.org/pdf/2312.02598v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02592v1",
            "title": "FRAPP\u00c9: A Post-Processing Framework for Group Fairness Regularization",
            "updated": "2023-12-05T09:09:21Z",
            "published": "2023-12-05T09:09:21Z",
            "summary": "Post-processing mitigation techniques for group fairness generally adjust the\ndecision threshold of a base model in order to improve fairness. Methods in\nthis family exhibit several advantages that make them appealing in practice:\npost-processing requires no access to the model training pipeline, is agnostic\nto the base model architecture, and offers a reduced computation cost compared\nto in-processing. Despite these benefits, existing methods face other\nchallenges that limit their applicability: they require knowledge of the\nsensitive attributes at inference time and are oftentimes outperformed by\nin-processing. In this paper, we propose a general framework to transform any\nin-processing method with a penalized objective into a post-processing\nprocedure. The resulting method is specifically designed to overcome the\naforementioned shortcomings of prior post-processing approaches. Furthermore,\nwe show theoretically and through extensive experiments on real-world data that\nthe resulting post-processing method matches or even surpasses the\nfairness-error trade-off offered by the in-processing counterpart.",
            "author": [
                "Alexandru \u0162ifrea",
                "Preethi Lahoti",
                "Ben Packer",
                "Yoni Halpern",
                "Ahmad Beirami",
                "Flavien Prost"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02592v1",
                "http://arxiv.org/pdf/2312.02592v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02590v1",
            "title": "Text Intimacy Analysis using Ensembles of Multilingual Transformers",
            "updated": "2023-12-05T09:04:22Z",
            "published": "2023-12-05T09:04:22Z",
            "summary": "Intimacy estimation of a given text has recently gained importance due to the\nincrease in direct interaction of NLP systems with humans. Intimacy is an\nimportant aspect of natural language and has a substantial impact on our\neveryday communication. Thus the level of intimacy can provide us with deeper\ninsights and richer semantics of conversations. In this paper, we present our\nwork on the SemEval shared task 9 on predicting the level of intimacy for the\ngiven text. The dataset consists of tweets in ten languages, out of which only\nsix are available in the training dataset. We conduct several experiments and\nshow that an ensemble of multilingual models along with a language-specific\nmonolingual model has the best performance. We also evaluate other data\naugmentation methods such as translation and present the results. Lastly, we\nstudy the results thoroughly and present some noteworthy insights into this\nproblem.",
            "author": [
                "Tanmay Chavan",
                "Ved Patwardhan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02590v1",
                "http://arxiv.org/pdf/2312.02590v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02589v1",
            "title": "ESP2CS: Securing Internet of Vehicles through Blockchain-enabled\n  Communications and Payments",
            "updated": "2023-12-05T09:02:56Z",
            "published": "2023-12-05T09:02:56Z",
            "summary": "The burgeoning domain of the Internet of Vehicles (IoV), a subset of the\nInternet of Things (IoT), promises to revolutionize transportation through\nenhanced safety, efficiency, and environmental sustainability. By amalgamating\ntechnologies like sensors and cloud computing, the IoV paves the way for\noptimized traffic management, heightened vehicle safety, and the birth of novel\nbusiness paradigms. However, this growth is shadowed by significant security\nconcerns, especially in the communication and payment sectors. Addressing the\npressing need for secure Vehicle to Everything (V2X) communications and\npayments amidst rising cyber threats, this research introduces the Ethereum\nbased Secure Payment and Communication Solution (ESP2CS). Utilizing Ethereum as\na middleware, ESP2CS ensures robust and secure V2X interactions. The solution\nis complemented by an Android Auto application for vehicles, streamlining inter\nvehicle communication, parking space detection, and transaction management.\nFurthermore, dedicated Android applications are developed for parking space\nrenters and the parking IoT system. Preliminary evaluations underscore ESP2CS's\nsuperior cost effectiveness, integrity and consistency over contemporary\nsolutions, with Ethereum bolstering both security and efficiency.",
            "author": [
                "Rateb Jabbar",
                "Mohamed Kharbeche"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02589v1",
                "http://arxiv.org/pdf/2312.02589v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02582v1",
            "title": "Revolutionary valorization of carbon dioxide into dimethyl carbonate is\n  catalyzed by sodium chloride: cheap, clean, one-pot, and water-free synthesis",
            "updated": "2023-12-05T08:55:23Z",
            "published": "2023-12-05T08:55:23Z",
            "summary": "The robust valorization of carbon dioxide (CO2) stays at the center of\nsustainable development. Since CO2 represents a low-energy compound, its\ntransformation into commercially coveted products is cumbersome. In the present\nwork, we report a revolutionary method to obtain dimethyl carbonate (DMC) out\nof methanol (CH3OH) and CO2 catalyzed by sodium chloride (NaCl) and similar\ninorganic salts. The computational exploration revealed a mechanism of\nfavorable catalysis, which was subsequently confirmed experimentally. Unlike\nall competitive syntheses of DMC, the new one does not produce water and,\ntherefore, the hydrolysis of a carbonate does not occur. No dehydrating agents\nare necessary. The employed catalyst is cheap and permanently exists in the\nsame phase with the reactants and products. The action of NaCl was compared to\nthose of other alkali metal salts, LiI, LiCl, and KI, and competitive\nperformances were recorded. The experimentally obtained result outperforms most\ncompeting technologies according to the DMC yield, 19% with molecular sieves\nand 17% without molecular sieves. All existing competitors are excelled by the\nsimplicity and cleanness of the synthesis. The reported advance substantially\nsimplifies the synthesis of linear organic carbonates and robustly valorizes\nCO2. Keywords: Dimethyl carbonate; carbon dioxide utilization; sodium chloride;\nmethanol.",
            "author": [
                "Vitaly V. Chaban",
                "Nadezhda A. Andreeva",
                "Leonardo Moreira dos Santos",
                "Sandra Einloft"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02582v1",
                "http://arxiv.org/pdf/2312.02582v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02578v1",
            "title": "Empathy and Distress Detection using Ensembles of Transformer Models",
            "updated": "2023-12-05T08:50:34Z",
            "published": "2023-12-05T08:50:34Z",
            "summary": "This paper presents our approach for the WASSA 2023 Empathy, Emotion and\nPersonality Shared Task. Empathy and distress are human feelings that are\nimplicitly expressed in natural discourses. Empathy and distress detection are\ncrucial challenges in Natural Language Processing that can aid our\nunderstanding of conversations. The provided dataset consists of several\nlong-text examples in the English language, with each example associated with a\nnumeric score for empathy and distress. We experiment with several BERT-based\nmodels as a part of our approach. We also try various ensemble methods. Our\nfinal submission has a Pearson's r score of 0.346, placing us third in the\nempathy and distress detection subtask.",
            "author": [
                "Tanmay Chavan",
                "Kshitij Deshpande",
                "Sheetal Sonawane"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02578v1",
                "http://arxiv.org/pdf/2312.02578v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02576v1",
            "title": "An Integrated System for Spatio-Temporal Summarization of 360-degrees\n  Videos",
            "updated": "2023-12-05T08:48:31Z",
            "published": "2023-12-05T08:48:31Z",
            "summary": "In this work, we present an integrated system for spatiotemporal\nsummarization of 360-degrees videos. The video summary production mainly\ninvolves the detection of salient events and their synopsis into a concise\nsummary. The analysis relies on state-of-the-art methods for saliency detection\nin 360-degrees video (ATSal and SST-Sal) and video summarization (CA-SUM). It\nalso contains a mechanism that classifies a 360-degrees video based on the use\nof static or moving camera during recording and decides which saliency\ndetection method will be used, as well as a 2D video production component that\nis responsible to create a conventional 2D video containing the salient events\nin the 360-degrees video. Quantitative evaluations using two datasets for\n360-degrees video saliency detection (VR-EyeTracking, Sports-360) show the\naccuracy and positive impact of the developed decision mechanism, and justify\nour choice to use two different methods for detecting the salient events. A\nqualitative analysis using content from these datasets, gives further insights\nabout the functionality of the decision mechanism, shows the pros and cons of\neach used saliency detection method and demonstrates the advanced performance\nof the trained summarization method against a more conventional approach.",
            "author": [
                "Ioannis Kontostathis",
                "Evlampios Apostolidis",
                "Vasileios Mezaris"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02576v1",
                "http://arxiv.org/pdf/2312.02576v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02575v1",
            "title": "A Bayesian neural network approach to Multi-fidelity surrogate modelling",
            "updated": "2023-12-05T08:47:24Z",
            "published": "2023-12-05T08:47:24Z",
            "summary": "This paper deals with surrogate modelling of a computer code output in a\nhierarchical multi-fidelity context, i.e., when the output can be evaluated at\ndifferent levels of accuracy and computational cost. Using observations of the\noutput at low- and high-fidelity levels, we propose a method that combines\nGaussian process (GP) regression and Bayesian neural network (BNN), in a method\ncalled GPBNN. The low-fidelity output is treated as a single-fidelity code\nusing classical GP regression. The high-fidelity output is approximated by a\nBNN that incorporates, in addition to the high-fidelity observations,\nwell-chosen realisations of the low-fidelity output emulator. The predictive\nuncertainty of the final surrogate model is then quantified by a complete\ncharacterisation of the uncertainties of the different models and their\ninteraction. GPBNN is compared with most of the multi-fidelity regression\nmethods allowing to quantify the prediction uncertainty.",
            "author": [
                "Baptiste Kerleguer",
                "Claire Cannamela",
                "Josselin Garnier"
            ],
            "link": [
                "http://dx.doi.org/10.1615/Int.J.UncertaintyQuantification.2023044584",
                "http://arxiv.org/abs/2312.02575v1",
                "http://arxiv.org/pdf/2312.02575v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02570v1",
            "title": "Dynamically emergent correlations between particles in a switching\n  harmonic trap",
            "updated": "2023-12-05T08:37:08Z",
            "published": "2023-12-05T08:37:08Z",
            "summary": "We study a one dimensional gas of $N$ noninteracting diffusing particles in a\nharmonic trap, whose stiffness switches between two values $\\mu_1$ and $\\mu_2$\nwith constant rates $r_1$ and $r_2$ respectively. Despite the absence of direct\ninteraction between the particles, we show that strong correlations between\nthem emerge in the stationary state at long times, induced purely by the\ndynamics itself. We compute exactly the joint distribution of the positions of\nthe particles in the stationary state, which allows us to compute several\nphysical observables analytically. In particular, we show that the extreme\nvalue statistics (EVS), i.e., the distribution of the position of the rightmost\nparticle has a nontrivial shape in the large $N$ limit. The scaling function\ncharacterizing this EVS has a finite support with a tunable shape (by varying\nthe parameters). Remarkably, this scaling function turns out to be universal.\nFirst, it also describes the distribution of the position of the $k$-th\nrightmost particle in a $1d$ trap. Moreover, the distribution of the position\nof the particle farthest from the center of the harmonic trap in $d$ dimensions\nis also described by the same scaling function for all $d \\geq 1$. Numerical\nsimulations are in excellent agreement with our analytical predictions.",
            "author": [
                "Marco Biroli",
                "Manas Kulkarni",
                "Satya N. Majumdar",
                "Gregory Schehr"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02570v1",
                "http://arxiv.org/pdf/2312.02570v1"
            ],
            "primary_category": "cond-mat.stat-mech",
            "category": [
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02568v1",
            "title": "Prompt2NeRF-PIL: Fast NeRF Generation via Pretrained Implicit Latent",
            "updated": "2023-12-05T08:32:46Z",
            "published": "2023-12-05T08:32:46Z",
            "summary": "This paper explores promptable NeRF generation (e.g., text prompt or single\nimage prompt) for direct conditioning and fast generation of NeRF parameters\nfor the underlying 3D scenes, thus undoing complex intermediate steps while\nproviding full 3D generation with conditional control. Unlike previous\ndiffusion-CLIP-based pipelines that involve tedious per-prompt optimizations,\nPrompt2NeRF-PIL is capable of generating a variety of 3D objects with a single\nforward pass, leveraging a pre-trained implicit latent space of NeRF\nparameters. Furthermore, in zero-shot tasks, our experiments demonstrate that\nthe NeRFs produced by our method serve as semantically informative\ninitializations, significantly accelerating the inference process of existing\nprompt-to-NeRF methods. Specifically, we will show that our approach speeds up\nthe text-to-NeRF model DreamFusion and the 3D reconstruction speed of the\nimage-to-NeRF method Zero-1-to-3 by 3 to 5 times.",
            "author": [
                "Jianmeng Liu",
                "Yuyao Zhang",
                "Zeyuan Meng",
                "Yu-Wing Tai",
                "Chi-Keung Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02568v1",
                "http://arxiv.org/pdf/2312.02568v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02567v1",
            "title": "Think Twice Before Selection: Federated Evidential Active Learning for\n  Medical Image Analysis with Domain Shifts",
            "updated": "2023-12-05T08:32:27Z",
            "published": "2023-12-05T08:32:27Z",
            "summary": "Federated learning facilitates the collaborative learning of a global model\nacross multiple distributed medical institutions without centralizing data.\nNevertheless, the expensive cost of annotation on local clients remains an\nobstacle to effectively utilizing local data. To mitigate this issue, federated\nactive learning methods suggest leveraging local and global model predictions\nto select a relatively small amount of informative local data for annotation.\nHowever, existing methods mainly focus on all local data sampled from the same\ndomain, making them unreliable in realistic medical scenarios with domain\nshifts among different clients. In this paper, we make the first attempt to\nassess the informativeness of local data derived from diverse domains and\npropose a novel methodology termed Federated Evidential Active Learning (FEAL)\nto calibrate the data evaluation under domain shift. Specifically, we introduce\na Dirichlet prior distribution in both local and global models to treat the\nprediction as a distribution over the probability simplex and capture both\naleatoric and epistemic uncertainties by using the Dirichlet-based evidential\nmodel. Then we employ the epistemic uncertainty to calibrate the aleatoric\nuncertainty. Afterward, we design a diversity relaxation strategy to reduce\ndata redundancy and maintain data diversity. Extensive experiments and analyses\nare conducted to show the superiority of FEAL over the state-of-the-art active\nlearning methods and the efficiency of FEAL under the federated active learning\nframework.",
            "author": [
                "Jiayi Chen",
                "Benteng Ma",
                "Hengfei Cui",
                "Yong Xia",
                "Kwang-Ting Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02567v1",
                "http://arxiv.org/pdf/2312.02567v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03026v1",
            "title": "Uni3DL: Unified Model for 3D and Language Understanding",
            "updated": "2023-12-05T08:30:27Z",
            "published": "2023-12-05T08:30:27Z",
            "summary": "In this work, we present Uni3DL, a unified model for 3D and Language\nunderstanding. Distinct from existing unified vision-language models in 3D\nwhich are limited in task variety and predominantly dependent on projected\nmulti-view images, Uni3DL operates directly on point clouds. This approach\nsignificantly expands the range of supported tasks in 3D, encompassing both\nvision and vision-language tasks in 3D. At the core of Uni3DL, a query\ntransformer is designed to learn task-agnostic semantic and mask outputs by\nattending to 3D visual features, and a task router is employed to selectively\ngenerate task-specific outputs required for diverse tasks. With a unified\narchitecture, our Uni3DL model enjoys seamless task decomposition and\nsubstantial parameter sharing across tasks. Uni3DL has been rigorously\nevaluated across diverse 3D vision-language understanding tasks, including\nsemantic segmentation, object detection, instance segmentation, visual\ngrounding, 3D captioning, and text-3D cross-modal retrieval. It demonstrates\nperformance on par with or surpassing state-of-the-art (SOTA) task-specific\nmodels. We hope our benchmark and Uni3DL model will serve as a solid step to\nease future research in unified models in the realm of 3D and language\nunderstanding. Project page: https://uni3dl.github.io.",
            "author": [
                "Xiang Li",
                "Jian Ding",
                "Zhaoyang Chen",
                "Mohamed Elhoseiny"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03026v1",
                "http://arxiv.org/pdf/2312.03026v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03025v1",
            "title": "Training on Synthetic Data Beats Real Data in Multimodal Relation\n  Extraction",
            "updated": "2023-12-05T08:11:34Z",
            "published": "2023-12-05T08:11:34Z",
            "summary": "The task of multimodal relation extraction has attracted significant research\nattention, but progress is constrained by the scarcity of available training\ndata. One natural thought is to extend existing datasets with cross-modal\ngenerative models. In this paper, we consider a novel problem setting, where\nonly unimodal data, either text or image, are available during training. We aim\nto train a multimodal classifier from synthetic data that perform well on real\nmultimodal test data. However, training with synthetic data suffers from two\nobstacles: lack of data diversity and label information loss. To alleviate the\nissues, we propose Mutual Information-aware Multimodal Iterated Relational dAta\nGEneration (MI2RAGE), which applies Chained Cross-modal Generation (CCG) to\npromote diversity in the generated data and exploits a teacher network to\nselect valuable training samples with high mutual information with the\nground-truth labels. Comparing our method to direct training on synthetic data,\nwe observed a significant improvement of 24.06% F1 with synthetic text and\n26.42% F1 with synthetic images. Notably, our best model trained on completely\nsynthetic images outperforms prior state-of-the-art models trained on real\nmultimodal data by a margin of 3.76% in F1. Our codebase will be made available\nupon acceptance.",
            "author": [
                "Zilin Du",
                "Haoxin Li",
                "Xu Guo",
                "Boyang Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03025v1",
                "http://arxiv.org/pdf/2312.03025v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02554v1",
            "title": "ULMA: Unified Language Model Alignment with Demonstration and Point-wise\n  Human Preference",
            "updated": "2023-12-05T07:52:12Z",
            "published": "2023-12-05T07:52:12Z",
            "summary": "Language model alignment is a cutting-edge technique in large language model\ntraining to align the model output to user's intent, e.g., being helpful and\nharmless. Recent alignment framework consists of two steps: supervised\nfine-tuning with demonstration data and preference learning with human\npreference data. Previous preference learning methods, such as RLHF and DPO,\nmainly focus on pair-wise preference data. However, in many real-world\nscenarios where human feedbacks are intrinsically point-wise, these methods\nwill suffer from information loss or even fail. To fill this gap, in this\npaper, we first develop a preference learning method called point-wise DPO to\ntackle point-wise preference data. Further revelation on the connection between\nsupervised fine-tuning and point-wise preference learning enables us to develop\na unified framework for both human demonstration and point-wise preference\ndata, which sheds new light on the construction of preference dataset.\nExtensive experiments on point-wise datasets with binary or continuous labels\ndemonstrate the superior performance and efficiency of our proposed methods. A\nnew dataset with high-quality demonstration samples on harmlessness is\nconstructed and made publicly available.",
            "author": [
                "Tianchi Cai",
                "Xierui Song",
                "Jiyan Jiang",
                "Fei Teng",
                "Jinjie Gu",
                "Guannan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02554v1",
                "http://arxiv.org/pdf/2312.02554v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02549v1",
            "title": "DemaFormer: Damped Exponential Moving Average Transformer with\n  Energy-Based Modeling for Temporal Language Grounding",
            "updated": "2023-12-05T07:37:21Z",
            "published": "2023-12-05T07:37:21Z",
            "summary": "Temporal Language Grounding seeks to localize video moments that semantically\ncorrespond to a natural language query. Recent advances employ the attention\nmechanism to learn the relations between video moments and the text query.\nHowever, naive attention might not be able to appropriately capture such\nrelations, resulting in ineffective distributions where target video moments\nare difficult to separate from the remaining ones. To resolve the issue, we\npropose an energy-based model framework to explicitly learn moment-query\ndistributions. Moreover, we propose DemaFormer, a novel Transformer-based\narchitecture that utilizes exponential moving average with a learnable damping\nfactor to effectively encode moment-query inputs. Comprehensive experiments on\nfour public temporal language grounding datasets showcase the superiority of\nour methods over the state-of-the-art baselines.",
            "author": [
                "Thong Nguyen",
                "Xiaobao Wu",
                "Xinshuai Dong",
                "Cong-Duy Nguyen",
                "See-Kiong Ng",
                "Luu Anh Tuan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02549v1",
                "http://arxiv.org/pdf/2312.02549v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02548v1",
            "title": "GeNIe: Generative Hard Negative Images Through Diffusion",
            "updated": "2023-12-05T07:34:30Z",
            "published": "2023-12-05T07:34:30Z",
            "summary": "Data augmentation is crucial in training deep models, preventing them from\noverfitting to limited data. Common data augmentation methods are effective,\nbut recent advancements in generative AI, such as diffusion models for image\ngeneration, enable more sophisticated augmentation techniques that produce data\nresembling natural images. We recognize that augmented samples closer to the\nideal decision boundary of a classifier are particularly effective and\nefficient in guiding the learning process. We introduce GeNIe which leverages a\ndiffusion model conditioned on a text prompt to merge contrasting data points\n(an image from the source category and a text prompt from the target category)\nto generate challenging samples for the target category. Inspired by recent\nimage editing methods, we limit the number of diffusion iterations and the\namount of noise. This ensures that the generated image retains low-level and\ncontextual features from the source image, potentially conflicting with the\ntarget category. Our extensive experiments, in few-shot and also long-tail\ndistribution settings, demonstrate the effectiveness of our novel augmentation\nmethod, especially benefiting categories with a limited number of examples.",
            "author": [
                "Soroush Abbasi Koohpayegani",
                "Anuj Singh",
                "K L Navaneet",
                "Hadi Jamali-Rad",
                "Hamed Pirsiavash"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02548v1",
                "http://arxiv.org/pdf/2312.02548v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02547v1",
            "title": "On Optimal Consistency-Robustness Trade-Off for Learning-Augmented\n  Multi-Option Ski Rental",
            "updated": "2023-12-05T07:33:51Z",
            "published": "2023-12-05T07:33:51Z",
            "summary": "The learning-augmented multi-option ski rental problem generalizes the\nclassical ski rental problem in two ways: the algorithm is provided with a\nprediction on the number of days we can ski, and the ski rental options now\ncome with a variety of rental periods and prices to choose from, unlike the\nclassical two-option setting. Subsequent to the initial study of the\nmulti-option ski rental problem (without learning augmentation) due to Zhang,\nPoon, and Xu, significant progress has been made for this problem recently in\nparticular. The problem is very well understood when we relinquish one of the\ntwo generalizations -- for the learning-augmented classical ski rental problem,\nalgorithms giving best-possible trade-off between consistency and robustness\nexist; for the multi-option ski rental problem without learning augmentation,\ndeterministic/randomized algorithms giving the best-possible competitiveness\nhave been found. However, in presence of both generalizations, there remained a\nhuge gap between the algorithmic and impossibility results. In fact, for\nrandomized algorithms, we did not have any nontrivial lower bounds on the\nconsistency-robustness trade-off before.\n  This paper bridges this gap for both deterministic and randomized algorithms.\nFor deterministic algorithms, we present a best-possible algorithm that\ncompletely matches the known lower bound. For randomized algorithms, we show\nthe first nontrivial lower bound on the consistency-robustness trade-off, and\nalso present an improved randomized algorithm. Our algorithm matches our lower\nbound on robustness within a factor of e/2 when the consistency is at most\n1.086.",
            "author": [
                "Yongho Shin",
                "Changyeol Lee",
                "Hyung-Chan An"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02547v1",
                "http://arxiv.org/pdf/2312.02547v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.GT",
                "cs.LG",
                "68W27, 68T05",
                "F.2.2; I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02546v1",
            "title": "Machine Vision Therapy: Multimodal Large Language Models Can Enhance\n  Visual Robustness via Denoising In-Context Learning",
            "updated": "2023-12-05T07:29:14Z",
            "published": "2023-12-05T07:29:14Z",
            "summary": "Although vision models such as Contrastive Language-Image Pre-Training (CLIP)\nshow impressive generalization performance, their zero-shot robustness is still\nlimited under Out-of-Distribution (OOD) scenarios without fine-tuning. Instead\nof undesirably providing human supervision as commonly done, it is possible to\ntake advantage of Multi-modal Large Language Models (MLLMs) that hold powerful\nvisual understanding abilities. However, MLLMs are shown to struggle with\nvision problems due to the incompatibility of tasks, thus hindering their\nutilization. In this paper, we propose to effectively leverage MLLMs to conduct\nMachine Vision Therapy which aims to rectify the noisy predictions from vision\nmodels. By fine-tuning with the denoised labels, the learning model performance\ncan be boosted in an unsupervised manner. To solve the incompatibility issue,\nwe propose a novel Denoising In-Context Learning (DICL) strategy to align\nvision tasks with MLLMs. Concretely, by estimating a transition matrix that\ncaptures the probability of one class being confused with another, an\ninstruction containing a correct exemplar and an erroneous one from the most\nprobable noisy class can be constructed. Such an instruction can help any MLLMs\nwith ICL ability to detect and rectify incorrect predictions of vision models.\nThrough extensive experiments on ImageNet, WILDS, DomainBed, and other OOD\ndatasets, we carefully validate the quantitative and qualitative effectiveness\nof our method. Our code is available at\nhttps://github.com/tmllab/Machine_Vision_Therapy.",
            "author": [
                "Zhuo Huang",
                "Chang Liu",
                "Yinpeng Dong",
                "Hang Su",
                "Shibao Zheng",
                "Tongliang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02546v1",
                "http://arxiv.org/pdf/2312.02546v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03022v1",
            "title": "Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph\n  Construction",
            "updated": "2023-12-05T07:27:08Z",
            "published": "2023-12-05T07:27:08Z",
            "summary": "Knowledge graph construction (KGC) is a multifaceted undertaking involving\nthe extraction of entities, relations, and events. Traditionally, large\nlanguage models (LLMs) have been viewed as solitary task-solving agents in this\ncomplex landscape. However, this paper challenges this paradigm by introducing\na novel framework, CooperKGC. Departing from the conventional approach,\nCooperKGC establishes a collaborative processing network, assembling a KGC\ncollaboration team capable of concurrently addressing entity, relation, and\nevent extraction tasks. Our experiments unequivocally demonstrate that\nfostering collaboration and information interaction among diverse agents within\nCooperKGC yields superior results compared to individual cognitive processes\noperating in isolation. Importantly, our findings reveal that the collaboration\nfacilitated by CooperKGC enhances knowledge selection, correction, and\naggregation capabilities across multiple rounds of interactions.",
            "author": [
                "Hongbin Ye",
                "Honghao Gui",
                "Aijia Zhang",
                "Tong Liu",
                "Wei Hua",
                "Weiqiang Jia"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03022v1",
                "http://arxiv.org/pdf/2312.03022v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02545v1",
            "title": "Graph Information Bottleneck for Remote Sensing Segmentation",
            "updated": "2023-12-05T07:23:22Z",
            "published": "2023-12-05T07:23:22Z",
            "summary": "Remote sensing segmentation has a wide range of applications in environmental\nprotection, and urban change detection, etc. Despite the success of deep\nlearning-based remote sensing segmentation methods (e.g., CNN and Transformer),\nthey are not flexible enough to model irregular objects. In addition, existing\ngraph contrastive learning methods usually adopt the way of maximizing mutual\ninformation to keep the node representations consistent between different graph\nviews, which may cause the model to learn task-independent redundant\ninformation. To tackle the above problems, this paper treats images as graph\nstructures and introduces a simple contrastive vision GNN (SC-ViG) architecture\nfor remote sensing segmentation. Specifically, we construct a node-masked and\nedge-masked graph view to obtain an optimal graph structure representation,\nwhich can adaptively learn whether to mask nodes and edges. Furthermore, this\npaper innovatively introduces information bottleneck theory into graph\ncontrastive learning to maximize task-related information while minimizing\ntask-independent redundant information. Finally, we replace the convolutional\nmodule in UNet with the SC-ViG module to complete the segmentation and\nclassification tasks of remote sensing images. Extensive experiments on\npublicly available real datasets demonstrate that our method outperforms\nstate-of-the-art remote sensing image segmentation methods.",
            "author": [
                "Yuntao Shou",
                "Wei Ai",
                "Tao Meng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02545v1",
                "http://arxiv.org/pdf/2312.02545v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02544v1",
            "title": "Characterization of Locality in Spin States and Forced Moves for\n  Optimizations",
            "updated": "2023-12-05T07:21:00Z",
            "published": "2023-12-05T07:21:00Z",
            "summary": "Ising formulations are widely utilized to solve combinatorial optimization\nproblems, and a variety of quantum or semiconductor-based hardware has recently\nbeen made available. In combinatorial optimization problems, the existence of\nlocal minima in energy landscapes is problematic to use to seek the global\nminimum. We note that the aim of the optimization is not to obtain exact\nsamplings from the Boltzmann distribution, and there is thus no need to satisfy\ndetailed balance conditions. In light of this fact, we develop an algorithm to\nget out of the local minima efficiently while it does not yield the exact\nsamplings. For this purpose, we utilize a feature that characterizes locality\nin the current state, which is easy to obtain with a type of specialized\nhardware. Furthermore, as the proposed algorithm is based on a rejection-free\nalgorithm, the computational cost is low. In this work, after presenting the\ndetails of the proposed algorithm, we report the results of numerical\nexperiments that demonstrate the effectiveness of the proposed feature and\nalgorithm.",
            "author": [
                "Yoshiki Sato",
                "Makiko Konoshima",
                "Hirotaka Tamura",
                "Jun Ohkubo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02544v1",
                "http://arxiv.org/pdf/2312.02544v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02543v1",
            "title": "Stress inside the pion in holographic light-front QCD",
            "updated": "2023-12-05T07:15:47Z",
            "published": "2023-12-05T07:15:47Z",
            "summary": "In this work, we propose a method to compute the gravitational form factor\n$D(Q^2)$ in holographic QCD by exploiting the remarkable correspondence between\nsemi-classical light-front QCD and semi-classical field theories in wrapped\nspacetime in 5D. The use of light-front holography bridges physics at large\n$Q^2$ as attained in light-front QCD and physics at small $Q^2$ where the\ncoupling to scalar and tensor fields, i.e. glueballs, are dominant. As an\napplication, we compute the $D$-term for the pion, and compare the results with\nrecent lattice simulations.",
            "author": [
                "Yang Li",
                "James P. Vary"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02543v1",
                "http://arxiv.org/pdf/2312.02543v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "hep-ph",
                "nucl-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02542v1",
            "title": "Fortress: Securing IoT Peripherals with Trusted Execution Environments",
            "updated": "2023-12-05T07:12:58Z",
            "published": "2023-12-05T07:12:58Z",
            "summary": "With the increasing popularity of Internet of Things (IoT) devices, securing\nsensitive user data has emerged as a major challenge. These devices often\ncollect confidential information, such as audio and visual data, through\nperipheral inputs like microphones and cameras. Such sensitive information is\nthen exposed to potential threats, either from malicious software with\nhigh-level access rights or transmitted (sometimes inadvertently) to untrusted\ncloud services. In this paper, we propose a generic design to enhance the\nprivacy in IoT-based systems by isolating peripheral I/O memory regions in a\nsecure kernel space of a trusted execution environment (TEE). Only a minimal\nset of peripheral driver code, resident within the secure kernel, can access\nthis protected memory area.\n  This design effectively restricts any unauthorised access by system software,\nincluding the operating system and hypervisor. The sensitive peripheral data is\nthen securely transferred to a user-space TEE, where obfuscation mechanisms can\nbe applied before it is relayed to third parties, e.g., the cloud. To validate\nour architectural approach, we provide a proof-of-concept implementation of our\ndesign by securing an audio peripheral based on inter-IC sound (I2S), a serial\nbus to interconnect audio devices. The experimental results show that our\ndesign offers a robust security solution with an acceptable computational\noverhead.",
            "author": [
                "Peterson Yuhala",
                "J\u00e4mes M\u00e9n\u00e9trey",
                "Pascal Felber",
                "Marcelo Pasin",
                "Valerio Schiavoni"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3605098.3635994",
                "http://arxiv.org/abs/2312.02542v1",
                "http://arxiv.org/pdf/2312.02542v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02541v1",
            "title": "Explainable Severity ranking via pairwise n-hidden comparison: a case\n  study of glaucoma",
            "updated": "2023-12-05T07:12:05Z",
            "published": "2023-12-05T07:12:05Z",
            "summary": "Primary open-angle glaucoma (POAG) is a chronic and progressive optic nerve\ncondition that results in an acquired loss of optic nerve fibers and potential\nblindness. The gradual onset of glaucoma results in patients progressively\nlosing their vision without being consciously aware of the changes. To diagnose\nPOAG and determine its severity, patients must undergo a comprehensive dilated\neye examination. In this work, we build a framework to rank, compare, and\ninterpret the severity of glaucoma using fundus images. We introduce a\nsiamese-based severity ranking using pairwise n-hidden comparisons. We\nadditionally have a novel approach to explaining why a specific image is deemed\nmore severe than others. Our findings indicate that the proposed severity\nranking model surpasses traditional ones in terms of diagnostic accuracy and\ndelivers improved saliency explanations.",
            "author": [
                "Hong Nguyen",
                "Cuong V. Nguyen",
                "Shrikanth Narayanan",
                "Benjamin Y. Xu",
                "Michael Pazzani"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02541v1",
                "http://arxiv.org/pdf/2312.02541v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03020v1",
            "title": "Enhanced Breast Cancer Tumor Classification using MobileNetV2: A\n  Detailed Exploration on Image Intensity, Error Mitigation, and\n  Streamlit-driven Real-time Deployment",
            "updated": "2023-12-05T06:58:14Z",
            "published": "2023-12-05T06:58:14Z",
            "summary": "This research introduces a sophisticated transfer learning model based on\nGoogle's MobileNetV2 for breast cancer tumor classification into normal,\nbenign, and malignant categories, utilizing a dataset of 1576 ultrasound images\n(265 normal, 891 benign, 420 malignant). The model achieves an accuracy of\n0.82, precision of 0.83, recall of 0.81, ROC-AUC of 0.94, PR-AUC of 0.88, and\nMCC of 0.74. It examines image intensity distributions and misclassification\nerrors, offering improvements for future applications. Addressing dataset\nimbalances, the study ensures a generalizable model. This work, using a dataset\nfrom Baheya Hospital, Cairo, Egypt, compiled by Walid Al-Dhabyani et al.,\nemphasizes MobileNetV2's potential in medical imaging, aiming to improve\ndiagnostic precision in oncology. Additionally, the paper explores\nStreamlit-based deployment for real-time tumor classification, demonstrating\nMobileNetV2's applicability in medical imaging and setting a benchmark for\nfuture research in oncology diagnostics.",
            "author": [
                "Aaditya Surya",
                "Aditya Shah",
                "Jarnell Kabore",
                "Subash Sasikumar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03020v1",
                "http://arxiv.org/pdf/2312.03020v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02535v1",
            "title": "Towards Open-set Gesture Recognition via Feature Activation Enhancement\n  and Orthogonal Prototype Learning",
            "updated": "2023-12-05T06:49:15Z",
            "published": "2023-12-05T06:49:15Z",
            "summary": "Gesture recognition is a foundational task in human-machine interaction\n(HMI). While there has been significant progress in gesture recognition based\non surface electromyography (sEMG), accurate recognition of predefined gestures\nonly within a closed set is still inadequate in practice. It is essential to\neffectively discern and reject unknown gestures of disinterest in a robust\nsystem. Numerous methods based on prototype learning (PL) have been proposed to\ntackle this open set recognition (OSR) problem. However, they do not fully\nexplore the inherent distinctions between known and unknown classes. In this\npaper, we propose a more effective PL method leveraging two novel and inherent\ndistinctions, feature activation level and projection inconsistency.\nSpecifically, the Feature Activation Enhancement Mechanism (FAEM) widens the\ngap in feature activation values between known and unknown classes.\nFurthermore, we introduce Orthogonal Prototype Learning (OPL) to construct\nmultiple perspectives. OPL acts to project a sample from orthogonal directions\nto maximize the distinction between its two projections, where unknown samples\nwill be projected near the clusters of different known classes while known\nsamples still maintain intra-class similarity. Our proposed method\nsimultaneously achieves accurate closed-set classification for predefined\ngestures and effective rejection for unknown gestures. Extensive experiments\ndemonstrate its efficacy and superiority in open-set gesture recognition based\non sEMG.",
            "author": [
                "Chen Liu",
                "Can Han",
                "Chengfeng Zhou",
                "Crystal Cai",
                "Suncheng Xiang",
                "Hualiang Ni",
                "Dahong Qian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02535v1",
                "http://arxiv.org/pdf/2312.02535v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02532v1",
            "title": "DRAFT: Dense Retrieval Augmented Few-shot Topic classifier Framework",
            "updated": "2023-12-05T06:28:45Z",
            "published": "2023-12-05T06:28:45Z",
            "summary": "With the growing volume of diverse information, the demand for classifying\narbitrary topics has become increasingly critical. To address this challenge,\nwe introduce DRAFT, a simple framework designed to train a classifier for\nfew-shot topic classification. DRAFT uses a few examples of a specific topic as\nqueries to construct Customized dataset with a dense retriever model.\nMulti-query retrieval (MQR) algorithm, which effectively handles multiple\nqueries related to a specific topic, is applied to construct the Customized\ndataset. Subsequently, we fine-tune a classifier using the Customized dataset\nto identify the topic. To demonstrate the efficacy of our proposed approach, we\nconduct evaluations on both widely used classification benchmark datasets and\nmanually constructed datasets with 291 diverse topics, which simulate diverse\ncontents encountered in real-world applications. DRAFT shows competitive or\nsuperior performance compared to baselines that use in-context learning, such\nas GPT-3 175B and InstructGPT 175B, on few-shot topic classification tasks\ndespite having 177 times fewer parameters, demonstrating its effectiveness.",
            "author": [
                "Keonwoo Kim",
                "Younggun Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02532v1",
                "http://arxiv.org/pdf/2312.02532v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02528v1",
            "title": "Towards Automatic Power Battery Detection: New Challenge, Benchmark\n  Dataset and Baseline",
            "updated": "2023-12-05T06:18:38Z",
            "published": "2023-12-05T06:18:38Z",
            "summary": "We conduct a comprehensive study on a new task named power battery detection\n(PBD), which aims to localize the dense cathode and anode plates endpoints from\nX-ray images to evaluate the quality of power batteries. Existing manufacturers\nusually rely on human eye observation to complete PBD, which makes it difficult\nto balance the accuracy and efficiency of detection. To address this issue and\ndrive more attention into this meaningful task, we first elaborately collect a\ndataset, called X-ray PBD, which has $1,500$ diverse X-ray images selected from\nthousands of power batteries of $5$ manufacturers, with $7$ different visual\ninterference. Then, we propose a novel segmentation-based solution for PBD,\ntermed multi-dimensional collaborative network (MDCNet). With the help of line\nand counting predictors, the representation of the point segmentation branch\ncan be improved at both semantic and detail aspects. Besides, we design an\neffective distance-adaptive mask generation strategy, which can alleviate the\nvisual challenge caused by the inconsistent distribution density of plates to\nprovide MDCNet with stable supervision. Without any bells and whistles, our\nsegmentation-based MDCNet consistently outperforms various other corner\ndetection, crowd counting and general/tiny object detection-based solutions,\nmaking it a strong baseline that can help facilitate future research in PBD.\nFinally, we share some potential difficulties and works for future researches.\nThe source code and datasets will be publicly available at\n\\href{http://www.gy3000.company/x3000%e5%bc%80%e6%94%be%e5%b9%b3%e5%8f%b0}{X-ray\nPBD}.",
            "author": [
                "Xiaoqi Zhao",
                "Youwei Pang",
                "Zhenyu Chen",
                "Qian Yu",
                "Lihe Zhang",
                "Hanqi Liu",
                "Jiaming Zuo",
                "Huchuan Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02528v1",
                "http://arxiv.org/pdf/2312.02528v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03019v1",
            "title": "Towards Optimizations of Quantum Circuit Simulation for Solving Max-Cut\n  Problems with QAOA",
            "updated": "2023-12-05T06:08:57Z",
            "published": "2023-12-05T06:08:57Z",
            "summary": "Quantum approximate optimization algorithm (QAOA) is one of the popular\nquantum algorithms that are used to solve combinatorial optimization problems\nvia approximations. QAOA is able to be evaluated on both physical and virtual\nquantum computers simulated by classical computers, with virtual ones being\nfavored for their noise-free feature and availability. Nevertheless, performing\nQAOA on virtual quantum computers suffers from a slow simulation speed for\nsolving combinatorial optimization problems which require large-scale quantum\ncircuit simulation (QCS). In this paper, we propose techniques to accelerate\nQCS for QAOA using mathematical optimizations to compress quantum operations,\nincorporating efficient bitwise operations to further lower the computational\ncomplexity, and leveraging different levels of parallelisms from modern\nmulti-core processors, with a study case to show the effectiveness on solving\nmax-cut problems.",
            "author": [
                "Yu-Cheng Lin",
                "Chuan-Chi Wang",
                "Chia-Heng Tu",
                "Shih-Hao Hung"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3605098.3635897",
                "http://arxiv.org/abs/2312.03019v1",
                "http://arxiv.org/pdf/2312.03019v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02522v1",
            "title": "MASP: Scalable GNN-based Planning for Multi-Agent Navigation",
            "updated": "2023-12-05T06:05:04Z",
            "published": "2023-12-05T06:05:04Z",
            "summary": "We investigate the problem of decentralized multi-agent navigation tasks,\nwhere multiple agents need to reach initially unassigned targets in a limited\ntime. Classical planning-based methods suffer from expensive computation\noverhead at each step and offer limited expressiveness for complex cooperation\nstrategies. In contrast, reinforcement learning (RL) has recently become a\npopular paradigm for addressing this issue. However, RL struggles with low data\nefficiency and cooperation when directly exploring (nearly) optimal policies in\nthe large search space, especially with an increased agent number (e.g., 10+\nagents) or in complex environments (e.g., 3D simulators). In this paper, we\npropose Multi-Agent Scalable GNN-based P lanner (MASP), a goal-conditioned\nhierarchical planner for navigation tasks with a substantial number of agents.\nMASP adopts a hierarchical framework to divide a large search space into\nmultiple smaller spaces, thereby reducing the space complexity and accelerating\ntraining convergence. We also leverage graph neural networks (GNN) to model the\ninteraction between agents and goals, improving goal achievement. Besides, to\nenhance generalization capabilities in scenarios with unseen team sizes, we\ndivide agents into multiple groups, each with a previously trained number of\nagents. The results demonstrate that MASP outperforms classical planning-based\ncompetitors and RL baselines, achieving a nearly 100% success rate with minimal\ntraining data in both multi-agent particle environments (MPE) with 50 agents\nand a quadrotor 3-dimensional environment (OmniDrones) with 20 agents.\nFurthermore, the learned policy showcases zero-shot generalization across\nunseen team sizes.",
            "author": [
                "Xinyi Yang",
                "Xinting Yang",
                "Chao Yu",
                "Jiayu Chen",
                "Huazhong Yang",
                "Yu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02522v1",
                "http://arxiv.org/pdf/2312.02522v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02521v1",
            "title": "Retrieving Conditions from Reference Images for Diffusion Models",
            "updated": "2023-12-05T06:04:16Z",
            "published": "2023-12-05T06:04:16Z",
            "summary": "Recent diffusion-based subject driven generative methods have enabled image\ngenerations with good fidelity for specific objects or human portraits.\nHowever, to achieve better versatility for applications, we argue that not only\nimproved datasets and evaluations are desired, but also more careful methods to\nretrieve only relevant information from conditional images are anticipated. To\nthis end, we propose an anime figures dataset RetriBooru-V1, with enhanced\nidentity and clothing labels. We state new tasks enabled by this dataset, and\nintroduce a new diversity metric to measure success in completing these tasks,\nquantifying the flexibility of image generations. We establish an RAG-inspired\nbaseline method, designed to retrieve precise conditional information from\nreference images. Then, we compare with current methods on existing task to\ndemonstrate the capability of the proposed method. Finally, we provide baseline\nexperiment results on new tasks, and conduct ablation studies on the possible\nstructural choices.",
            "author": [
                "Haoran Tang",
                "Xin Zhou",
                "Jieren Deng",
                "Zhihong Pan",
                "Hao Tian",
                "Pratik Chaudhari"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02521v1",
                "http://arxiv.org/pdf/2312.02521v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02520v1",
            "title": "Towards More Unified In-context Visual Understanding",
            "updated": "2023-12-05T06:02:21Z",
            "published": "2023-12-05T06:02:21Z",
            "summary": "The rapid advancement of large language models (LLMs) has accelerated the\nemergence of in-context learning (ICL) as a cutting-edge approach in the\nnatural language processing domain. Recently, ICL has been employed in visual\nunderstanding tasks, such as semantic segmentation and image captioning,\nyielding promising results. However, existing visual ICL framework can not\nenable producing content across multiple modalities, which limits their\npotential usage scenarios. To address this issue, we present a new ICL\nframework for visual understanding with multi-modal output enabled. First, we\nquantize and embed both text and visual prompt into a unified representational\nspace, structured as interleaved in-context sequences. Then a decoder-only\nsparse transformer architecture is employed to perform generative modeling on\nthem, facilitating in-context learning. Thanks to this design, the model is\ncapable of handling in-context vision understanding tasks with multimodal\noutput in a unified pipeline. Experimental results demonstrate that our model\nachieves competitive performance compared with specialized models and previous\nICL baselines. Overall, our research takes a further step toward unified\nmultimodal in-context learning.",
            "author": [
                "Dianmo Sheng",
                "Dongdong Chen",
                "Zhentao Tan",
                "Qiankun Liu",
                "Qi Chu",
                "Jianmin Bao",
                "Tao Gong",
                "Bin Liu",
                "Shengwei Xu",
                "Nenghai Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02520v1",
                "http://arxiv.org/pdf/2312.02520v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02516v1",
            "title": "Generating a Map of Well-being Regions using Multiscale Moving Direction\n  Entropy on Mobile Sensors",
            "updated": "2023-12-05T05:40:17Z",
            "published": "2023-12-05T05:40:17Z",
            "summary": "The well-being of individuals in a crowd is interpreted as a product of the\ncrossover of individuals from heterogeneous communities, which may occur via\ninteractions with other crowds. The index moving-direction entropy\ncorresponding to the diversity of the moving directions of individuals is\nintroduced to represent such an inter-community crossover. Multi-scale moving\ndirection entropies, composed of various geographical mesh sizes to compute the\nindex values, are used to capture the information flow owing to human movements\nfrom/to various crowds. The generated map of high values of multiscale moving\ndirection entropy is shown to coincide significantly with the preference of\npeople to live in each region.",
            "author": [
                "Yukio Ohsawa",
                "Sae Kondo",
                "Yi Sun",
                "Kaira Sekiguchi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02516v1",
                "http://arxiv.org/pdf/2312.02516v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02515v1",
            "title": "ASPEN: High-Throughput LoRA Fine-Tuning of Large Language Models with a\n  Single GPU",
            "updated": "2023-12-05T05:38:38Z",
            "published": "2023-12-05T05:38:38Z",
            "summary": "Transformer-based large language models (LLMs) have demonstrated outstanding\nperformance across diverse domains, particularly when fine-turned for specific\ndomains. Recent studies suggest that the resources required for fine-tuning\nLLMs can be economized through parameter-efficient methods such as Low-Rank\nAdaptation (LoRA). While LoRA effectively reduces computational burdens and\nresource demands, it currently supports only a single-job fine-tuning setup.\n  In this paper, we present ASPEN, a high-throughput framework for fine-tuning\nLLMs. ASPEN efficiently trains multiple jobs on a single GPU using the LoRA\nmethod, leveraging shared pre-trained model and adaptive scheduling. ASPEN is\ncompatible with transformer-based language models like LLaMA and ChatGLM, etc.\nExperiments show that ASPEN saves 53% of GPU memory when training multiple\nLLaMA-7B models on NVIDIA A100 80GB GPU and boosts training throughput by about\n17% compared to existing methods when training with various pre-trained models\non different GPUs. The adaptive scheduling algorithm reduces turnaround time by\n24%, end-to-end training latency by 12%, prioritizing jobs and preventing\nout-of-memory issues.",
            "author": [
                "Zhengmao Ye",
                "Dengchun Li",
                "Jingqi Tian",
                "Tingfeng Lan",
                "Jie Zuo",
                "Lei Duan",
                "Hui Lu",
                "Yexi Jiang",
                "Jian Sha",
                "Ke Zhang",
                "Mingjie Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02515v1",
                "http://arxiv.org/pdf/2312.02515v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02512v1",
            "title": "AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation\n  with Unified Audio-Visual Speech Representation",
            "updated": "2023-12-05T05:36:44Z",
            "published": "2023-12-05T05:36:44Z",
            "summary": "This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech\nTranslation (AV2AV) framework, where the input and output of the system are\nmultimodal (i.e., audio and visual speech). With the proposed AV2AV, two key\nadvantages can be brought: 1) We can perform real-like conversations with\nindividuals worldwide in a virtual meeting by utilizing our own primary\nlanguages. In contrast to Speech-to-Speech Translation (A2A), which solely\ntranslates between audio modalities, the proposed AV2AV directly translates\nbetween audio-visual speech. This capability enhances the dialogue experience\nby presenting synchronized lip movements along with the translated speech. 2)\nWe can improve the robustness of the spoken language translation system. By\nemploying the complementary information of audio-visual speech, the system can\neffectively translate spoken language even in the presence of acoustic noise,\nshowcasing robust performance. To mitigate the problem of the absence of a\nparallel AV2AV translation dataset, we propose to train our spoken language\ntranslation system with the audio-only dataset of A2A. This is done by learning\nunified audio-visual speech representations through self-supervised learning in\nadvance to train the translation system. Moreover, we propose an AV-Renderer\nthat can generate raw audio and video in parallel. It is designed with\nzero-shot speaker modeling, thus the speaker in source audio-visual speech can\nbe maintained at the target translated audio-visual speech. The effectiveness\nof AV2AV is evaluated with extensive experiments in a many-to-many language\ntranslation setting. The demo page is available on\nhttps://choijeongsoo.github.io/av2av.",
            "author": [
                "Jeongsoo Choi",
                "Se Jin Park",
                "Minsu Kim",
                "Yong Man Ro"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02512v1",
                "http://arxiv.org/pdf/2312.02512v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.MM",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03446v1",
            "title": "Visual Hindsight Self-Imitation Learning for Interactive Navigation",
            "updated": "2023-12-05T05:34:12Z",
            "published": "2023-12-05T05:34:12Z",
            "summary": "Interactive visual navigation tasks, which involve following instructions to\nreach and interact with specific targets, are challenging not only because\nsuccessful experiences are very rare but also because the complex visual inputs\nrequire a substantial number of samples. Previous methods for these tasks often\nrely on intricately designed dense rewards or the use of expensive expert data\nfor imitation learning. To tackle these challenges, we propose a novel\napproach, Visual Hindsight Self-Imitation Learning (VHS) for enhancing sample\nefficiency through hindsight goal re-labeling and self-imitation. We also\nintroduce a prototypical goal embedding method derived from experienced goal\nobservations, that is particularly effective in vision-based and partially\nobservable environments. This embedding technique allows the agent to visually\nreinterpret its unsuccessful attempts, enabling vision-based goal re-labeling\nand self-imitation from enhanced successful experiences. Experimental results\nshow that VHS outperforms existing techniques in interactive visual navigation\ntasks, confirming its superior performance and sample efficiency.",
            "author": [
                "Kibeom Kim",
                "Kisung Shin",
                "Min Whoo Lee",
                "Moonhoen Lee",
                "Minsu Lee",
                "Byoung-Tak Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03446v1",
                "http://arxiv.org/pdf/2312.03446v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02509v1",
            "title": "When PETs misbehave: A Contextual Integrity analysis",
            "updated": "2023-12-05T05:27:43Z",
            "published": "2023-12-05T05:27:43Z",
            "summary": "Privacy enhancing technologies, or PETs, have been hailed as a promising\nmeans to protect privacy without compromising on the functionality of digital\nservices. At the same time, and partly because they may encode a narrow\nconceptualization of privacy as confidentiality that is popular among\npolicymakers, engineers and the public, PETs risk being co-opted to promote\nprivacy-invasive practices. In this paper, we resort to the theory of\nContextual Integrity to explain how privacy technologies may be misused to\nerode privacy. To illustrate, we consider three PETs and scenarios: anonymous\ncredentials for age verification, client-side scanning for illegal content\ndetection, and homomorphic encryption for machine learning model training.\nUsing the theory of Contextual Integrity, we reason about the notion of privacy\nthat these PETs encode, and show that CI enables us to identify and reason\nabout the limitations of PETs and their misuse, and which may ultimately lead\nto privacy violations.",
            "author": [
                "Ero Balsa",
                "Yan Shvartzshnaider"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02509v1",
                "http://arxiv.org/pdf/2312.02509v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.CY",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02503v1",
            "title": "SAVE: Protagonist Diversification with Structure Agnostic Video Editing",
            "updated": "2023-12-05T05:13:20Z",
            "published": "2023-12-05T05:13:20Z",
            "summary": "Driven by the upsurge progress in text-to-image (T2I) generation models,\ntext-to-video (T2V) generation has experienced a significant advance as well.\nAccordingly, tasks such as modifying the object or changing the style in a\nvideo have been possible. However, previous works usually work well on trivial\nand consistent shapes, and easily collapse on a difficult target that has a\nlargely different body shape from the original one. In this paper, we spot the\nbias problem in the existing video editing method that restricts the range of\nchoices for the new protagonist and attempt to address this issue using the\nconventional image-level personalization method. We adopt motion\npersonalization that isolates the motion from a single source video and then\nmodifies the protagonist accordingly. To deal with the natural discrepancy\nbetween image and video, we propose a motion word with an inflated textual\nembedding to properly represent the motion in a source video. We also regulate\nthe motion word to attend to proper motion-related areas by introducing a novel\npseudo optical flow, efficiently computed from the pre-calculated attention\nmaps. Finally, we decouple the motion from the appearance of the source video\nwith an additional pseudo word. Extensive experiments demonstrate the editing\ncapability of our method, taking a step toward more diverse and extensive video\nediting.",
            "author": [
                "Yeji Song",
                "Wonsik Shin",
                "Junsoo Lee",
                "Jeesoo Kim",
                "Nojun Kwak"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02503v1",
                "http://arxiv.org/pdf/2312.02503v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02501v1",
            "title": "Inspecting Model Fairness in Ultrasound Segmentation Tasks",
            "updated": "2023-12-05T05:08:08Z",
            "published": "2023-12-05T05:08:08Z",
            "summary": "With the rapid expansion of machine learning and deep learning (DL),\nresearchers are increasingly employing learning-based algorithms to alleviate\ndiagnostic challenges across diverse medical tasks and applications. While\nadvancements in diagnostic precision are notable, some researchers have\nidentified a concerning trend: their models exhibit biased performance across\nsubgroups characterized by different sensitive attributes. This bias not only\ninfringes upon the rights of patients but also has the potential to lead to\nlife-altering consequences. In this paper, we inspect a series of DL\nsegmentation models using two ultrasound datasets, aiming to assess the\npresence of model unfairness in these specific tasks. Our findings reveal that\neven state-of-the-art DL algorithms demonstrate unfair behavior in ultrasound\nsegmentation tasks. These results serve as a crucial warning, underscoring the\nnecessity for careful model evaluation before their deployment in real-world\nscenarios. Such assessments are imperative to ensure ethical considerations and\nmitigate the risk of adverse impacts on patient outcomes.",
            "author": [
                "Zikang Xu",
                "Fenghe Tang",
                "Quan Quan",
                "Jianrui Ding",
                "Chunping Ning",
                "S. Kevin Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02501v1",
                "http://arxiv.org/pdf/2312.02501v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02500v1",
            "title": "Calculation of Relativistic Single-Particle States",
            "updated": "2023-12-05T05:07:09Z",
            "published": "2023-12-05T05:07:09Z",
            "summary": "A computational method is proposed to calculate bound and resonant states by\nsolving the Klein-Gordon and Dirac equations for real and complex energies,\nrespectively. The method is an extension of a non-relativistic one, where the\npotential is represented in a Coulomb-Sturmian basis. This basis facilitates\nthe exact analytic evaluation of the Coulomb Green's operator in terms of a\ncontinued fraction. In the extension to relativistic problems, we cast the\nKlein-Gordon and Dirac equations into an effective Schr\\\"odinger form. Then the\nsolution method is basically an analytic continuation of non-relativistic\nquantities like the angular momentum, charge, energy and potential into the\neffective relativistic counterparts.",
            "author": [
                "D. Wingard",
                "B. K\u00f3nya",
                "Z. Papp"
            ],
            "link": [
                "http://dx.doi.org/10.1007/s00601-023-01869-y",
                "http://arxiv.org/abs/2312.02500v1",
                "http://arxiv.org/pdf/2312.02500v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "math-ph",
                "math.MP",
                "nucl-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02496v1",
            "title": "MKA: A Scalable Medical Knowledge Assisted Mechanism for Generative\n  Models on Medical Conversation Tasks",
            "updated": "2023-12-05T04:55:54Z",
            "published": "2023-12-05T04:55:54Z",
            "summary": "Using natural language processing (NLP) technologies to develop medical\nchatbots makes the diagnosis of the patient more convenient and efficient,\nwhich is a typical application in healthcare AI. Because of its importance,\nlots of research have been come out. Recently, the neural generative models\nhave shown their impressive ability as the core of chatbot, while it cannot\nscale well when directly applied to medical conversation due to the lack of\nmedical-specific knowledge. To address the limitation, a scalable Medical\nKnowledge Assisted mechanism, MKA, is proposed in this paper. The mechanism\naims to assist general neural generative models to achieve better performance\non the medical conversation task. The medical-specific knowledge graph is\ndesigned within the mechanism, which contains 6 types of medical-related\ninformation, including department, drug, check, symptom, disease, food.\nBesides, the specific token concatenation policy is defined to effectively\ninject medical information into the input data. Evaluation of our method is\ncarried out on two typical medical datasets, MedDG and MedDialog-CN. The\nevaluation results demonstrate that models combined with our mechanism\noutperform original methods in multiple automatic evaluation metrics. Besides,\nMKA-Bert-GPT achieves state-of-the-art performance. The open-sourced codes are\npublic:\nhttps://github.com/LIANGKE23/Knowledge_Assisted_Medical_Dialogue_Generation_Mechanism",
            "author": [
                "Ke Liang",
                "Sifan Wu",
                "Jiayi Gu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02496v1",
                "http://arxiv.org/pdf/2312.02496v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02494v1",
            "title": "ReconU-Net: a direct PET image reconstruction using U-Net architecture\n  with back projection-induced skip connection",
            "updated": "2023-12-05T04:51:42Z",
            "published": "2023-12-05T04:51:42Z",
            "summary": "[Objective] This study aims to introduce a novel back projection-induced\nU-Net-shaped architecture, called ReconU-Net, for deep learning-based direct\npositron emission tomography (PET) image reconstruction. Additionally, our\nobjective is to analyze the behavior of direct PET image reconstruction and\ngain deeper insights by comparing the proposed ReconU-Net architecture with\nother encoder-decoder architectures without skip connections. [Approach] The\nproposed ReconU-Net architecture uniquely integrates the physical model of the\nback projection operation into the skip connection. This distinctive feature\nfacilitates the effective transfer of intrinsic spatial information from the\ninput sinogram to the reconstructed image via an embedded physical model. The\nproposed ReconU-Net was trained using Monte Carlo simulation data from the\nBrainweb phantom and tested on both simulated and real Hoffman brain phantom\ndata. [Main results] The proposed ReconU-Net method generated a reconstructed\nimage with a more accurate structure compared to other deep learning-based\ndirect reconstruction methods. Further analysis showed that the proposed\nReconU-Net architecture has the ability to transfer features of multiple\nresolutions, especially non-abstract high-resolution information, through skip\nconnections. Despite limited training on simulated data, the proposed\nReconU-Net successfully reconstructed the real Hoffman brain phantom, unlike\nother deep learning-based direct reconstruction methods, which failed to\nproduce a reconstructed image. [Significance] The proposed ReconU-Net can\nimprove the fidelity of direct PET image reconstruction, even when dealing with\nsmall training datasets, by leveraging the synergistic relationship between\ndata-driven modeling and the physics model of the imaging process.",
            "author": [
                "Fumio Hashimoto",
                "Kibo Ote"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02494v1",
                "http://arxiv.org/pdf/2312.02494v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02493v1",
            "title": "Flexible Communication for Optimal Distributed Learning over\n  Unpredictable Networks",
            "updated": "2023-12-05T04:51:19Z",
            "published": "2023-12-05T04:51:19Z",
            "summary": "Gradient compression alleviates expensive communication in distributed deep\nlearning by sending fewer values and its corresponding indices, typically via\nAllgather (AG). Training with high compression ratio (CR) achieves high\naccuracy like DenseSGD, but has lower parallel scaling due to high\ncommunication cost (i.e., parallel efficiency). Using lower CRs improves\nparallel efficiency by lowering synchronization cost, but degrades model\naccuracy as well (statistical efficiency). Further, speedup attained with\ndifferent models and CRs also varies with network latency, effective bandwidth\nand collective op used for aggregation. In many cases, collectives like\nAllreduce (AR) have lower cost than AG to exchange the same amount of data. In\nthis paper, we propose an AR-compatible Topk compressor that is\nbandwidth-optimal and thus performs better than AG in certain network\nconfigurations. We develop a flexible communication strategy that switches\nbetween AG and AR based on which collective is optimal in the current settings,\nand model the pareto-relationship between parallel and statistical efficiency\nas a multi-objective optimization (MOO) problem to dynamically adjust CR and\naccelerate training while still converging to high accuracy.",
            "author": [
                "Sahil Tyagi",
                "Martin Swany"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02493v1",
                "http://arxiv.org/pdf/2312.02493v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02490v1",
            "title": "Constrained Twin Variational Auto-Encoder for Intrusion Detection in IoT\n  Systems",
            "updated": "2023-12-05T04:42:04Z",
            "published": "2023-12-05T04:42:04Z",
            "summary": "Intrusion detection systems (IDSs) play a critical role in protecting\nbillions of IoT devices from malicious attacks. However, the IDSs for IoT\ndevices face inherent challenges of IoT systems, including the heterogeneity of\nIoT data/devices, the high dimensionality of training data, and the imbalanced\ndata. Moreover, the deployment of IDSs on IoT systems is challenging, and\nsometimes impossible, due to the limited resources such as memory/storage and\ncomputing capability of typical IoT devices. To tackle these challenges, this\narticle proposes a novel deep neural network/architecture called Constrained\nTwin Variational Auto-Encoder (CTVAE) that can feed classifiers of IDSs with\nmore separable/distinguishable and lower-dimensional representation data.\nAdditionally, in comparison to the state-of-the-art neural networks used in\nIDSs, CTVAE requires less memory/storage and computing power, hence making it\nmore suitable for IoT IDS systems. Extensive experiments with the 11 most\npopular IoT botnet datasets show that CTVAE can boost around 1% in terms of\naccuracy and Fscore in detection attack compared to the state-of-the-art\nmachine learning and representation learning methods, whilst the running time\nfor attack detection is lower than 2E-6 seconds and the model size is lower\nthan 1 MB. We also further investigate various characteristics of CTVAE in the\nlatent space and in the reconstruction representation to demonstrate its\nefficacy compared with current well-known methods.",
            "author": [
                "Phai Vu Dinh",
                "Quang Uy Nguyen",
                "Dinh Thai Hoang",
                "Diep N. Nguyen",
                "Son Pham Bao",
                "Eryk Dutkiewicz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02490v1",
                "http://arxiv.org/pdf/2312.02490v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02483v1",
            "title": "EtC: Temporal Boundary Expand then Clarify for Weakly Supervised Video\n  Grounding with Multimodal Large Language Model",
            "updated": "2023-12-05T04:15:56Z",
            "published": "2023-12-05T04:15:56Z",
            "summary": "Early weakly supervised video grounding (WSVG) methods often struggle with\nincomplete boundary detection due to the absence of temporal boundary\nannotations. To bridge the gap between video-level and boundary-level\nannotation, explicit-supervision methods, i.e., generating pseudo-temporal\nboundaries for training, have achieved great success. However, data\naugmentations in these methods might disrupt critical temporal information,\nyielding poor pseudo boundaries. In this paper, we propose a new perspective\nthat maintains the integrity of the original temporal content while introducing\nmore valuable information for expanding the incomplete boundaries. To this end,\nwe propose EtC (Expand then Clarify), first use the additional information to\nexpand the initial incomplete pseudo boundaries, and subsequently refine these\nexpanded ones to achieve precise boundaries. Motivated by video continuity,\ni.e., visual similarity across adjacent frames, we use powerful multimodal\nlarge language models (MLLMs) to annotate each frame within initial pseudo\nboundaries, yielding more comprehensive descriptions for expanded boundaries.\nTo further clarify the noise of expanded boundaries, we combine mutual learning\nwith a tailored proposal-level contrastive objective to use a learnable\napproach to harmonize a balance between incomplete yet clean (initial) and\ncomprehensive yet noisy (expanded) boundaries for more precise ones.\nExperiments demonstrate the superiority of our method on two challenging WSVG\ndatasets.",
            "author": [
                "Guozhang Li",
                "Xinpeng Ding",
                "De Cheng",
                "Jie Li",
                "Nannan Wang",
                "Xinbo Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02483v1",
                "http://arxiv.org/pdf/2312.02483v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02482v1",
            "title": "Treatment heterogeneity with right-censored outcomes using grf",
            "updated": "2023-12-05T04:15:23Z",
            "published": "2023-12-05T04:15:23Z",
            "summary": "This article walks through how to estimate conditional average treatment\neffects (CATEs) with right-censored time-to-event outcomes using the function\ncausal_survival_forest (Cui et al., 2023) in the R package grf (Athey et al.,\n2019, Tibshirani et al., 2023).",
            "author": [
                "Erik Sverdrup",
                "Stefan Wager"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02482v1",
                "http://arxiv.org/pdf/2312.02482v1"
            ],
            "primary_category": "stat.CO",
            "category": [
                "stat.CO",
                "stat.AP",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02481v1",
            "title": "Learning to Holistically Detect Bridges from Large-Size VHR Remote\n  Sensing Imagery",
            "updated": "2023-12-05T04:15:22Z",
            "published": "2023-12-05T04:15:22Z",
            "summary": "Bridge detection in remote sensing images (RSIs) plays a crucial role in\nvarious applications, but it poses unique challenges compared to the detection\nof other objects. In RSIs, bridges exhibit considerable variations in terms of\ntheir spatial scales and aspect ratios. Therefore, to ensure the visibility and\nintegrity of bridges, it is essential to perform holistic bridge detection in\nlarge-size very-high-resolution (VHR) RSIs. However, the lack of datasets with\nlarge-size VHR RSIs limits the deep learning algorithms' performance on bridge\ndetection. Due to the limitation of GPU memory in tackling large-size images,\ndeep learning-based object detection methods commonly adopt the cropping\nstrategy, which inevitably results in label fragmentation and discontinuous\nprediction. To ameliorate the scarcity of datasets, this paper proposes a\nlarge-scale dataset named GLH-Bridge comprising 6,000 VHR RSIs sampled from\ndiverse geographic locations across the globe. These images encompass a wide\nrange of sizes, varying from 2,048*2,048 to 16,38*16,384 pixels, and\ncollectively feature 59,737 bridges. Furthermore, we present an efficient\nnetwork for holistic bridge detection (HBD-Net) in large-size RSIs. The HBD-Net\npresents a separate detector-based feature fusion (SDFF) architecture and is\noptimized via a shape-sensitive sample re-weighting (SSRW) strategy. Based on\nthe proposed GLH-Bridge dataset, we establish a bridge detection benchmark\nincluding the OBB and HBB tasks, and validate the effectiveness of the proposed\nHBD-Net. Additionally, cross-dataset generalization experiments on two publicly\navailable datasets illustrate the strong generalization capability of the\nGLH-Bridge dataset.",
            "author": [
                "Yansheng Li",
                "Junwei Luo",
                "Yongjun Zhang",
                "Yihua Tan",
                "Jin-Gang Yu",
                "Song Bai"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02481v1",
                "http://arxiv.org/pdf/2312.02481v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02480v1",
            "title": "Differentiable Point-based Inverse Rendering",
            "updated": "2023-12-05T04:13:31Z",
            "published": "2023-12-05T04:13:31Z",
            "summary": "We present differentiable point-based inverse rendering, DPIR, an\nanalysis-by-synthesis method that processes images captured under diverse\nilluminations to estimate shape and spatially-varying BRDF. To this end, we\nadopt point-based rendering, eliminating the need for multiple samplings per\nray, typical of volumetric rendering, thus significantly enhancing the speed of\ninverse rendering. To realize this idea, we devise a hybrid point-volumetric\nrepresentation for geometry and a regularized basis-BRDF representation for\nreflectance. The hybrid geometric representation enables fast rendering through\npoint-based splatting while retaining the geometric details and stability\ninherent to SDF-based representations. The regularized basis-BRDF mitigates the\nill-posedness of inverse rendering stemming from limited light-view angular\nsamples. We also propose an efficient shadow detection method using point-based\nshadow map rendering. Our extensive evaluations demonstrate that DPIR\noutperforms prior works in terms of reconstruction accuracy, computational\nefficiency, and memory footprint. Furthermore, our explicit point-based\nrepresentation and rendering enables intuitive geometry and reflectance\nediting. The code will be publicly available.",
            "author": [
                "Hoon-Gyu Chung",
                "Seokjun Choi",
                "Seung-Hwan Baek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02480v1",
                "http://arxiv.org/pdf/2312.02480v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02479v1",
            "title": "Applications of Domain Adversarial Neural Network in phase transition of\n  3D Potts model",
            "updated": "2023-12-05T04:12:13Z",
            "published": "2023-12-05T04:12:13Z",
            "summary": "Machine learning techniques exhibit significant performance in discriminating\ndifferent phases of matter and provide a new avenue for studying phase\ntransitions. We investigate the phase transitions of three dimensional\n$q$-state Potts model on cubic lattice by using a transfer learning approach,\nDomain Adversarial Neural Network (DANN). With the unique neural network\narchitecture, it could evaluate the high-temperature (disordered) and\nlow-temperature (ordered) phases, and identify the first and second order phase\ntransitions. Meanwhile, by training the DANN with a few labeled configurations,\nthe critical points for $q=2,3,4$ and $5$ can be predicted with high accuracy,\nwhich are consistent with those of the Monte Carlo simulations. These findings\nwould promote us to learn and explore the properties of phase transitions in\nhigh-dimensional systems.",
            "author": [
                "Xiangna Chen",
                "Feiyi Liu",
                "Weibing Deng",
                "Shiyang Chen",
                "Jianmin Shen",
                "Gabor Papp",
                "Wei Li",
                "Chunbin Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02479v1",
                "http://arxiv.org/pdf/2312.02479v1"
            ],
            "primary_category": "physics.comp-ph",
            "category": [
                "physics.comp-ph",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02475v2",
            "title": "Accurate Machine Learning Predictions of Coercivity in High-Performance\n  Permanent Magnets",
            "updated": "2023-12-07T03:45:40Z",
            "published": "2023-12-05T03:58:34Z",
            "summary": "Increased demand for high-performance permanent magnets in the electric\nvehicle and wind turbine industries has prompted the search for cost-effective\nalternatives. Nevertheless, the discovery of new magnetic materials with the\ndesired intrinsic and extrinsic permanent magnet properties presents a\nsignificant challenge. Traditional density functional theory (DFT) accurately\npredicts intrinsic permanent magnet properties such as magnetic moments,\nmagneto-crystalline anisotropy constants, and exchange interactions. However,\nit cannot compute extrinsic macroscopic properties, such as coercivity ($H_c$),\nwhich are influenced by factors like microscopic defects and internal grain\nstructures. Although micromagnetic simulation helps compute $H_c$, it\noverestimates the values almost by an order of magnitude due to Brown's\nparadox. To circumvent these limitations, we employ machine learning (ML)\nmethods in an extensive database obtained from experiments, DFT calculations,\nand micromagnetic modeling. Our novel ML approach is computationally much\nfaster than the micromagnetic simulation program, the mumax$^3$. We\nsuccessfully utilize it to predict $H_c$ values for materials like cerium-doped\n$\\mathrm{Nd}_2\\mathrm{Fe}_{14}\\mathrm{B}$, and subsequently compare the\npredicted values with experimental results. Remarkably, our ML model accurately\nidentifies uniaxial magnetic anisotropy as the primary contributor to $H_c$.\nWith DFT calculations, we predict the Nd-site dependent magnetic anisotropy\nbehavior in $\\mathrm{Nd}_2\\mathrm{Fe}_{14}\\mathrm{B}$, confirming $4f$-site\nplanar and $4g$-site uniaxial to crystalline $c$-direction in good agreement\nwith experiment. The Green's function atomic sphere approximation calculated a\nCurie temperature ($T_{\\rm C}$) for $\\mathrm{Nd}_2\\mathrm{Fe}_{14}\\mathrm{B}$\nthat also agrees well with experiment.",
            "author": [
                "Churna Bhandari",
                "Gavin N. Nop",
                "Jonathan D. H. Smith",
                "Durga Paudyal"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02475v2",
                "http://arxiv.org/pdf/2312.02475v2"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02473v1",
            "title": "NeutronStream: A Dynamic GNN Training Framework with Sliding Window for\n  Graph Streams",
            "updated": "2023-12-05T03:58:05Z",
            "published": "2023-12-05T03:58:05Z",
            "summary": "Existing Graph Neural Network (GNN) training frameworks have been designed to\nhelp developers easily create performant GNN implementations. However, most\nexisting GNN frameworks assume that the input graphs are static, but ignore\nthat most real-world graphs are constantly evolving. Though many dynamic GNN\nmodels have emerged to learn from evolving graphs, the training process of\nthese dynamic GNNs is dramatically different from traditional GNNs in that it\ncaptures both the spatial and temporal dependencies of graph updates. This\nposes new challenges for designing dynamic GNN training frameworks. First, the\ntraditional batched training method fails to capture real-time structural\nevolution information. Second, the time-dependent nature makes parallel\ntraining hard to design. Third, it lacks system supports for users to\nefficiently implement dynamic GNNs. In this paper, we present NeutronStream, a\nframework for training dynamic GNN models. NeutronStream abstracts the input\ndynamic graph into a chronologically updated stream of events and processes the\nstream with an optimized sliding window to incrementally capture the\nspatial-temporal dependencies of events. Furthermore, NeutronStream provides a\nparallel execution engine to tackle the sequential event processing challenge\nto achieve high performance. NeutronStream also integrates a built-in graph\nstorage structure that supports dynamic updates and provides a set of\neasy-to-use APIs that allow users to express their dynamic GNNs. Our\nexperimental results demonstrate that, compared to state-of-the-art dynamic GNN\nimplementations, NeutronStream achieves speedups ranging from 1.48X to 5.87X\nand an average accuracy improvement of 3.97%.",
            "author": [
                "Chaoyi Chen",
                "Dechao Gao",
                "Yanfeng Zhang",
                "Qiange Wang",
                "Zhenbo Fu",
                "Xuecang Zhang",
                "Junhua Zhu",
                "Yu Gu",
                "Ge Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02473v1",
                "http://arxiv.org/pdf/2312.02473v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02471v1",
            "title": "Congestion-aware Distributed Task Offloading in Wireless Multi-hop\n  Networks Using Graph Neural Networks",
            "updated": "2023-12-05T03:46:30Z",
            "published": "2023-12-05T03:46:30Z",
            "summary": "Computational offloading has become an enabling component for edge\nintelligence in mobile and smart devices. Existing offloading schemes mainly\nfocus on mobile devices and servers, while ignoring the potential network\ncongestion caused by tasks from multiple mobile devices, especially in wireless\nmulti-hop networks. To fill this gap, we propose a low-overhead,\ncongestion-aware distributed task offloading scheme by augmenting a distributed\ngreedy framework with graph-based machine learning. In simulated wireless\nmulti-hop networks with 20-110 nodes and a resource allocation scheme based on\nshortest path routing and contention-based link scheduling, our approach is\ndemonstrated to be effective in reducing congestion or unstable queues under\nthe context-agnostic baseline, while improving the execution latency over local\ncomputing.",
            "author": [
                "Zhongyuan Zhao",
                "Jake Perazzone",
                "Gunjan Verma",
                "Santiago Segarra"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02471v1",
                "http://arxiv.org/pdf/2312.02471v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.LG",
                "eess.SP",
                "05C90",
                "C.2.1; C.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02470v1",
            "title": "Generator Born from Classifier",
            "updated": "2023-12-05T03:41:17Z",
            "published": "2023-12-05T03:41:17Z",
            "summary": "In this paper, we make a bold attempt toward an ambitious task: given a\npre-trained classifier, we aim to reconstruct an image generator, without\nrelying on any data samples. From a black-box perspective, this challenge seems\nintractable, since it inevitably involves identifying the inverse function for\na classifier, which is, by nature, an information extraction process. As such,\nwe resort to leveraging the knowledge encapsulated within the parameters of the\nneural network. Grounded on the theory of Maximum-Margin Bias of gradient\ndescent, we propose a novel learning paradigm, in which the generator is\ntrained to ensure that the convergence conditions of the network parameters are\nsatisfied over the generated distribution of the samples. Empirical validation\nfrom various image generation tasks substantiates the efficacy of our strategy.",
            "author": [
                "Runpeng Yu",
                "Xinchao Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02470v1",
                "http://arxiv.org/pdf/2312.02470v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02469v1",
            "title": "Learning Energy-based Model via Dual-MCMC Teaching",
            "updated": "2023-12-05T03:39:54Z",
            "published": "2023-12-05T03:39:54Z",
            "summary": "This paper studies the fundamental learning problem of the energy-based model\n(EBM). Learning the EBM can be achieved using the maximum likelihood estimation\n(MLE), which typically involves the Markov Chain Monte Carlo (MCMC) sampling,\nsuch as the Langevin dynamics. However, the noise-initialized Langevin dynamics\ncan be challenging in practice and hard to mix. This motivates the exploration\nof joint training with the generator model where the generator model serves as\na complementary model to bypass MCMC sampling. However, such a method can be\nless accurate than the MCMC and result in biased EBM learning. While the\ngenerator can also serve as an initializer model for better MCMC sampling, its\nlearning can be biased since it only matches the EBM and has no access to\nempirical training examples. Such biased generator learning may limit the\npotential of learning the EBM. To address this issue, we present a joint\nlearning framework that interweaves the maximum likelihood learning algorithm\nfor both the EBM and the complementary generator model. In particular, the\ngenerator model is learned by MLE to match both the EBM and the empirical data\ndistribution, making it a more informative initializer for MCMC sampling of\nEBM. Learning generator with observed examples typically requires inference of\nthe generator posterior. To ensure accurate and efficient inference, we adopt\nthe MCMC posterior sampling and introduce a complementary inference model to\ninitialize such latent MCMC sampling. We show that three separate models can be\nseamlessly integrated into our joint framework through two (dual-) MCMC\nteaching, enabling effective and efficient EBM learning.",
            "author": [
                "Jiali Cui",
                "Tian Han"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02469v1",
                "http://arxiv.org/pdf/2312.02469v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "stat.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02464v1",
            "title": "SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object\n  and Boundary Constraints",
            "updated": "2023-12-05T03:33:47Z",
            "published": "2023-12-05T03:33:47Z",
            "summary": "Semantic segmentation of remote sensing imagery plays a pivotal role in\nextracting precise information for diverse down-stream applications. Recent\ndevelopment of the Segment Anything Model (SAM), an advanced general-purpose\nsegmentation model, has revolutionized this field, presenting new avenues for\naccurate and efficient segmentation. However, SAM is limited to generating\nsegmentation results without class information. Consequently, the utilization\nof such a powerful general vision model for semantic segmentation in remote\nsensing images has become a focal point of research. In this paper, we present\na streamlined framework aimed at leveraging the raw output of SAM by exploiting\ntwo novel concepts called SAM-Generated Object (SGO) and SAM-Generated Boundary\n(SGB). More specifically, we propose a novel object loss and further introduce\na boundary loss as augmentative components to aid in model optimization in a\ngeneral semantic segmentation framework. Taking into account the content\ncharacteristics of SGO, we introduce the concept of object consistency to\nleverage segmented regions lacking semantic information. By imposing\nconstraints on the consistency of predicted values within objects, the object\nloss aims to enhance semantic segmentation performance. Furthermore, the\nboundary loss capitalizes on the distinctive features of SGB by directing the\nmodel's attention to the boundary information of the object. Experimental\nresults on two well-known datasets, namely ISPRS Vaihingen and LoveDA Urban,\ndemonstrate the effectiveness of our proposed method. The source code for this\nwork will be accessible at https://github.com/sstary/SSRS.",
            "author": [
                "Xianping Ma",
                "Qianqian Wu",
                "Xingyu Zhao",
                "Xiaokang Zhang",
                "Man-On Pun",
                "Bo Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02464v1",
                "http://arxiv.org/pdf/2312.02464v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02462v1",
            "title": "Dimensionality Reduction and Dynamical Mode Recognition of Circular\n  Arrays of Flame Oscillators Using Deep Neural Network",
            "updated": "2023-12-05T03:25:45Z",
            "published": "2023-12-05T03:25:45Z",
            "summary": "Oscillatory combustion in aero engines and modern gas turbines often has\nsignificant adverse effects on their operation, and accurately recognizing\nvarious oscillation modes is the prerequisite for understanding and controlling\ncombustion instability. However, the high-dimensional spatial-temporal data of\na complex combustion system typically poses considerable challenges to the\ndynamical mode recognition. Based on a two-layer bidirectional long short-term\nmemory variational autoencoder (Bi-LSTM-VAE) dimensionality reduction model and\na two-dimensional Wasserstein distance-based classifier (WDC), this study\nproposes a promising method (Bi-LSTM-VAE-WDC) for recognizing dynamical modes\nin oscillatory combustion systems. Specifically, the Bi-LSTM-VAE dimension\nreduction model was introduced to reduce the high-dimensional spatial-temporal\ndata of the combustion system to a low-dimensional phase space; Gaussian kernel\ndensity estimates (GKDE) were computed based on the distribution of phase\npoints in a grid; two-dimensional WD values were calculated from the GKDE maps\nto recognize the oscillation modes. The time-series data used in this study\nwere obtained from numerical simulations of circular arrays of laminar flame\noscillators. The results show that the novel Bi-LSTM-VAE method can produce a\nnon-overlapping distribution of phase points, indicating an effective\nunsupervised mode recognition and classification. Furthermore, the present\nmethod exhibits a more prominent performance than VAE and PCA (principal\ncomponent analysis) for distinguishing dynamical modes in complex flame\nsystems, implying its potential in studying turbulent combustion.",
            "author": [
                "Weiming Xu",
                "Tao Yang",
                "Peng Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02462v1",
                "http://arxiv.org/pdf/2312.02462v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02461v1",
            "title": "Conjugate gradient methods without line search for multiobjective\n  optimization",
            "updated": "2023-12-05T03:23:36Z",
            "published": "2023-12-05T03:23:36Z",
            "summary": "This paper aims to further develop the conjugate gradient methods in the work\nof Lucambio P\\'erez and Prudente (SIAM J Optim, 28(3): 2690--2720, 2018) for\nunconstrained multiobjective optimization problems. In such methods, the line\nsearch procedure is replaced by a fixed formula of stepsize, which can aviod\nthe computational cost associated with function evaluations in a specific\napplication. We use the no-line-search scheme to derive the condition of\nZoutendijk's type. Global convergence includes the vector extensions of\nFletcher--Reeves, conjugate descent, Dai--Yuan, Polak--Ribi\\`{e}re--Polyak, and\nHestenes--Stiefel parameters, subject to certain mild assumptions.",
            "author": [
                "Wang Chen",
                "Yong Zhao",
                "Xinmin Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02461v1",
                "http://arxiv.org/pdf/2312.02461v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02459v1",
            "title": "An adaptive preconditioning scheme for the self-consistent field\n  iteration and generalized stacking-fault energy calculations",
            "updated": "2023-12-05T03:18:27Z",
            "published": "2023-12-05T03:18:27Z",
            "summary": "The generalized stacking-fault energy (GSFE) is the fundamental but key\nparameter for the plastic deformation of materials. We perform first-principles\ncalculations by full-potential linearized augmented planewave (FLAPW) method to\nevaluate the GSFE based on the single-shift and triple-shift supercell models.\nDifferent degrees of defects are introduced in the two models, thereby\naffecting the convergence of the self-consistent field (SCF) iterations. We\npresent an adaptive preconditioning scheme which can identify the\nlong-wavelength divergence behavior of the Jacobian during the SCF iteration\nand automatically switch on the Kerker preconditioning to accelerate the\nconvergence. We implement this algorithm in Elk-7.2.42 package and calculate\nthe GSFE curves for Al, Cu, and Si (111) plane <-1-12> direction. We found that\nthe single-shift and triple-shift supercell models have equivalent calculation\naccuracy and are within the experimental data uncertainty. For computational\nefficiency, the triple-shift supercell model is preferable due to its better\nconvergence, exhibiting lower degree of defect compared to the single-shift\nsupercell model.",
            "author": [
                "Sitong Zhang",
                "Xingyu Gao",
                "Haifeng Song",
                "Bin Wen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02459v1",
                "http://arxiv.org/pdf/2312.02459v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03018v1",
            "title": "DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention\n  and Text Guidance",
            "updated": "2023-12-05T03:16:31Z",
            "published": "2023-12-05T03:16:31Z",
            "summary": "Image-to-video generation, which aims to generate a video starting from a\ngiven reference image, has drawn great attention. Existing methods try to\nextend pre-trained text-guided image diffusion models to image-guided video\ngeneration models. Nevertheless, these methods often result in either low\nfidelity or flickering over time due to their limitation to shallow image\nguidance and poor temporal consistency. To tackle these problems, we propose a\nhigh-fidelity image-to-video generation method by devising a frame retention\nbranch on the basis of a pre-trained video diffusion model, named DreamVideo.\nInstead of integrating the reference image into the diffusion process in a\nsemantic level, our DreamVideo perceives the reference image via convolution\nlayers and concatenate the features with the noisy latents as model input. By\nthis means, the details of the reference image can be preserved to the greatest\nextent. In addition, by incorporating double-condition classifier-free\nguidance, a single image can be directed to videos of different actions by\nproviding varying prompt texts. This has significant implications for\ncontrollable video generation and holds broad application prospects. We conduct\ncomprehensive experiments on the public dataset, both quantitative and\nqualitative results indicate that our method outperforms the state-of-the-art\nmethod. Especially for fidelity, our model has powerful image retention ability\nand result in high FVD in UCF101 compared to other image-to-video models. Also,\nprecise control can be achieved by giving different text prompts. Further\ndetails and comprehensive results of our model will be presented in\nhttps://anonymous0769.github.io/DreamVideo/.",
            "author": [
                "Cong Wang",
                "Jiaxi Gu",
                "Panwen Hu",
                "Songcen Xu",
                "Hang Xu",
                "Xiaodan Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03018v1",
                "http://arxiv.org/pdf/2312.03018v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02451v1",
            "title": "Evaluating the Convergence Limit of Quantum Neural Tangent Kernel",
            "updated": "2023-12-05T03:04:26Z",
            "published": "2023-12-05T03:04:26Z",
            "summary": "Quantum variational algorithms have been one of major applications of quantum\ncomputing with current quantum devices. There are recent attempts to establish\nthe foundation for these algorithms. A possible approach is to characterize the\ntraining dynamics with quantum neural tangent kernel. In this work, we\nconstruct the kernel for two models, Quantun Ensemble and Quantum Neural\nNetwork, and show the convergence of these models in the limit of infinitely\nmany qubits. We also show applications of the kernel limit in regression tasks.",
            "author": [
                "Trong Duong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02451v1",
                "http://arxiv.org/pdf/2312.02451v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02450v1",
            "title": "GIT-Net: Generalized Integral Transform for Operator Learning",
            "updated": "2023-12-05T03:03:54Z",
            "published": "2023-12-05T03:03:54Z",
            "summary": "This article introduces GIT-Net, a deep neural network architecture for\napproximating Partial Differential Equation (PDE) operators, inspired by\nintegral transform operators. GIT-NET harnesses the fact that differential\noperators commonly used for defining PDEs can often be represented\nparsimoniously when expressed in specialized functional bases (e.g., Fourier\nbasis). Unlike rigid integral transforms, GIT-Net parametrizes adaptive\ngeneralized integral transforms with deep neural networks. When compared to\nseveral recently proposed alternatives, GIT-Net's computational and memory\nrequirements scale gracefully with mesh discretizations, facilitating its\napplication to PDE problems on complex geometries. Numerical experiments\ndemonstrate that GIT-Net is a competitive neural network operator, exhibiting\nsmall test errors and low evaluations across a range of PDE problems. This\nstands in contrast to existing neural network operators, which typically excel\nin just one of these areas.",
            "author": [
                "Chao Wang",
                "Alexandre Hoang Thiery"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02450v1",
                "http://arxiv.org/pdf/2312.02450v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02449v1",
            "title": "Adaptive unified gas-kinetic scheme for diatomic gases with rotational\n  and vibrational nonequilibrium",
            "updated": "2023-12-05T03:02:57Z",
            "published": "2023-12-05T03:02:57Z",
            "summary": "Multiscale non-equilibrium physics at large variations of local Knudsen\nnumber are encountered in applications of aerospace engineering and\nmicro-electro-mechanical systems, such as high-speed flying vehicles and low\npressure of the encapsulation. An accurate description of flow physics in all\nflow regimes within a single computation requires a genuinely multiscale\nmethod. The adaptive unified gas-kinetic scheme (AUGKS) is developed for such\nmultiscale flow simulation. The AUGKS applies discretized velocity space to\naccurately capture the non-equilibrium physics in the multiscale UGKS, and\nadaptively employs continuous distribution functions following Chapman-Enskog\nexpansion to efficiently recover near-equilibrium flow region in GKS. The UGKS\nand GKS are dynamically connected at the cell interface through the fluxes from\nthe discretized and continuous gas distribution functions, which avoids any\nbuffer zone between them. In this study, the AUGKS method with rotation and\nvibration non-equilibrium is developed. The real gas effect in different flow\nregimes has been captured. To capture aerodynamic heating accurately, the heat\nflux modifications are also included. Unstructured discrete particle velocity\nspace is adopted to further improve the computational performance. Numerical\ntests, including Sod tube, normal shock structure, high-speed flow around the\ntwo-dimensional cylinder and three-dimensional sphere and space vehicles, and\nan unsteady nozzle plume flow from the continuum flow to the background vacuum,\nhave been conducted to validate the current scheme. In comparison with the\noriginal UGKS, the current scheme speeds up the computation, reduces the memory\nrequirement, and maintains the equivalent accuracy for multiscale flow\nsimulation, which provides an effective tool for non-equilibrium flow\nsimulations, especially for the flows at low and medium speed.",
            "author": [
                "Yufeng Wei",
                "Wenpei Long",
                "Kun Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02449v1",
                "http://arxiv.org/pdf/2312.02449v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02448v1",
            "title": "Time-Relative RTK-GNSS: GNSS Loop Closure in Pose Graph Optimization",
            "updated": "2023-12-05T03:00:50Z",
            "published": "2023-12-05T03:00:50Z",
            "summary": "A pose-graph-based optimization technique is widely used to estimate robot\nposes using various sensor measurements from devices such as laser scanners and\ncameras. The global navigation satellite system (GNSS) has recently been used\nto estimate the absolute 3D position of outdoor mobile robots. However, since\nthe accuracy of GNSS single-point positioning is only a few meters, the GNSS is\nnot used for the loop closure of a pose graph. The main purpose of this study\nis to generate a loop closure of a pose graph using a time-relative real-time\nkinematic GNSS (TR-RTK-GNSS) technique. The proposed TR-RTK-GNSS technique uses\ntime-differential carrier phase positioning, which is based on\ncarrier-phase-based differential GNSS with a single GNSS receiver. Unlike a\nconventional RTK-GNSS, we can directly compute the robot's relative position\nusing only a stand-alone GNSS receiver. The initial pose graph is generated\nfrom the accumulated velocity computed from GNSS Doppler measurements. To\nreduce the accumulated error of velocity, we use the TR-RTK-GNSS technique for\nthe loop closure in the graph-based optimization framework. The kinematic\npositioning tests were performed using an unmanned aerial vehicle to confirm\nthe effectiveness of the proposed technique. From the tests, we can estimate\nthe vehicle's trajectory with approximately 3 cm accuracy using only a\nstand-alone GNSS receiver.",
            "author": [
                "Taro Suzuki"
            ],
            "link": [
                "http://dx.doi.org/10.1109/LRA.2020.3003861",
                "http://arxiv.org/abs/2312.02448v1",
                "http://arxiv.org/pdf/2312.02448v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02441v1",
            "title": "MedDM:LLM-executable clinical guidance tree for clinical decision-making",
            "updated": "2023-12-05T02:44:07Z",
            "published": "2023-12-05T02:44:07Z",
            "summary": "It is becoming increasingly emphasis on the importance of LLM participating\nin clinical diagnosis decision-making. However, the low specialization refers\nto that current medical LLMs can not provide specific medical advice, which are\nmore like a medical Q\\&A. And there is no suitable clinical guidance tree data\nset that can be used directly with LLM. To address this issue, we first propose\nLLM-executavle clinical guidance tree(CGT), which can be directly used by large\nlanguage models, and construct medical diagnostic decision-making dataset\n(MedDM), from flowcharts in clinical practice guidelines. We propose an\napproach to screen flowcharts from medical literature, followed by their\nidentification and conversion into standardized diagnostic decision trees.\nConstructed a knowledge base with 1202 decision trees, which came from 5000\nmedical literature and covered 12 hospital departments, including internal\nmedicine, surgery, psychiatry, and over 500 diseases.Moreover, we propose a\nmethod for reasoning on LLM-executable CGT and a Patient-LLM multi-turn\ndialogue framework.",
            "author": [
                "Binbin Li",
                "Tianxin Meng",
                "Xiaoming Shi",
                "Jie Zhai",
                "Tong Ruan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02441v1",
                "http://arxiv.org/pdf/2312.02441v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02439v2",
            "title": "Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language\n  Models with Creative Humor Generation",
            "updated": "2023-12-06T03:20:29Z",
            "published": "2023-12-05T02:41:57Z",
            "summary": "Chain-of-Thought (CoT) guides large language models (LLMs) to reason\nstep-by-step, and can motivate their logical reasoning ability. While effective\nfor logical tasks, CoT is not conducive to creative problem-solving which often\nrequires out-of-box thoughts and is crucial for innovation advancements. In\nthis paper, we explore the Leap-of-Thought (LoT) abilities within LLMs -- a\nnon-sequential, creative paradigm involving strong associations and knowledge\nleaps. To this end, we study LLMs on the popular Oogiri game which needs\nparticipants to have good creativity and strong associative thinking for\nresponding unexpectedly and humorously to the given image, text, or both, and\nthus is suitable for LoT study. Then to investigate LLMs' LoT ability in the\nOogiri game, we first build a multimodal and multilingual Oogiri-GO dataset\nwhich contains over 130,000 samples from the Oogiri game, and observe the\ninsufficient LoT ability or failures of most existing LLMs on the Oogiri game.\nAccordingly, we introduce a creative Leap-of-Thought (CLoT) paradigm to improve\nLLM's LoT ability. CLoT first formulates the Oogiri-GO dataset into\nLoT-oriented instruction tuning data to train pretrained LLM for achieving\ncertain LoT humor generation and discrimination abilities. Then CLoT designs an\nexplorative self-refinement that encourages the LLM to generate more creative\nLoT data via exploring parallels between seemingly unrelated concepts and\nselects high-quality data to train itself for self-refinement. CLoT not only\nexcels in humor generation in the Oogiri game but also boosts creative\nabilities in various tasks like cloud guessing game and divergent association\ntask. These findings advance our understanding and offer a pathway to improve\nLLMs' creative capacities for innovative applications across domains. The\ndataset, code, and models will be released online.\nhttps://zhongshsh.github.io/CLoT/.",
            "author": [
                "Shanshan Zhong",
                "Zhongzhan Huang",
                "Shanghua Gao",
                "Wushao Wen",
                "Liang Lin",
                "Marinka Zitnik",
                "Pan Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02439v2",
                "http://arxiv.org/pdf/2312.02439v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02438v1",
            "title": "Adaptive Instrument Design for Indirect Experiments",
            "updated": "2023-12-05T02:38:04Z",
            "published": "2023-12-05T02:38:04Z",
            "summary": "Indirect experiments provide a valuable framework for estimating treatment\neffects in situations where conducting randomized control trials (RCTs) is\nimpractical or unethical. Unlike RCTs, indirect experiments estimate treatment\neffects by leveraging (conditional) instrumental variables, enabling estimation\nthrough encouragement and recommendation rather than strict treatment\nassignment. However, the sample efficiency of such estimators depends not only\non the inherent variability in outcomes but also on the varying compliance\nlevels of users with the instrumental variables and the choice of estimator\nbeing used, especially when dealing with numerous instrumental variables. While\nadaptive experiment design has a rich literature for direct experiments, in\nthis paper we take the initial steps towards enhancing sample efficiency for\nindirect experiments by adaptively designing a data collection policy over\ninstrumental variables. Our main contribution is a practical computational\nprocedure that utilizes influence functions to search for an optimal data\ncollection policy, minimizing the mean-squared error of the desired\n(non-linear) estimator. Through experiments conducted in various domains\ninspired by real-world applications, we showcase how our method can\nsignificantly improve the sample efficiency of indirect experiments.",
            "author": [
                "Yash Chandak",
                "Shiv Shankar",
                "Vasilis Syrgkanis",
                "Emma Brunskill"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02438v1",
                "http://arxiv.org/pdf/2312.02438v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02437v1",
            "title": "GDN: A Stacking Network Used for Skin Cancer Diagnosis",
            "updated": "2023-12-05T02:33:55Z",
            "published": "2023-12-05T02:33:55Z",
            "summary": "Skin cancer, the primary type of cancer that can be identified by visual\nrecognition, requires an automatic identification system that can accurately\nclassify different types of lesions. This paper presents GoogLe-Dense Network\n(GDN), which is an image-classification model to identify two types of skin\ncancer, Basal Cell Carcinoma, and Melanoma. GDN uses stacking of different\nnetworks to enhance the model performance. Specifically, GDN consists of two\nsequential levels in its structure. The first level performs basic\nclassification tasks accomplished by GoogLeNet and DenseNet, which are trained\nin parallel to enhance efficiency. To avoid low accuracy and long training\ntime, the second level takes the output of the GoogLeNet and DenseNet as the\ninput for a logistic regression model. We compare our method with four baseline\nnetworks including ResNet, VGGNet, DenseNet, and GoogLeNet on the dataset, in\nwhich GoogLeNet and DenseNet significantly outperform ResNet and VGGNet. In the\nsecond level, different stacking methods such as perceptron, logistic\nregression, SVM, decision trees and K-neighbor are studied in which Logistic\nRegression shows the best prediction result among all. The results prove that\nGDN, compared to a single network structure, has higher accuracy in optimizing\nskin cancer detection.",
            "author": [
                "Jingmin Wei",
                "Haoyang Shen",
                "Ziyi Wang",
                "Ziqian Zhang"
            ],
            "link": [
                "http://dx.doi.org/10.1117/12.2631455",
                "http://arxiv.org/abs/2312.02437v1",
                "http://arxiv.org/pdf/2312.02437v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02436v1",
            "title": "MUFFIN: Curating Multi-Faceted Instructions for Improving\n  Instruction-Following",
            "updated": "2023-12-05T02:32:08Z",
            "published": "2023-12-05T02:32:08Z",
            "summary": "In the realm of large language models (LLMs), enhancing instruction-following\ncapability often involves curating expansive training data. This is achieved\nthrough two primary schemes: i) Scaling-Inputs: Amplifying (input, output)\npairs per task instruction, aiming for better instruction adherence. ii)\nScaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction,\noutput) pair (without requiring a separate input anymore). However, LLMs under\nScaling-Inputs tend to be overly sensitive to inputs, leading to\nmisinterpretation or non-compliance with instructions. Conversely, Scaling\nInput-Free Tasks demands a substantial number of tasks but is less effective in\ninstruction following when dealing with instances in Scaling-Inputs. This work\nintroduces MUFFIN, a new scheme of instruction-following dataset curation.\nSpecifically, we automatically Scale Tasks per Input by diversifying these\ntasks with various input facets. Experimental results across four zero-shot\nbenchmarks, spanning both Scaling-Inputs and Scaling Input-Free Tasks schemes,\nreveal that LLMs, at various scales, trained on MUFFIN generally demonstrate\nsuperior instruction-following capabilities compared to those trained on the\ntwo aforementioned schemes.",
            "author": [
                "Renze Lou",
                "Kai Zhang",
                "Jian Xie",
                "Yuxuan Sun",
                "Janice Ahn",
                "Hanzi Xu",
                "Yu Su",
                "Wenpeng Yin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02436v1",
                "http://arxiv.org/pdf/2312.02436v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02435v1",
            "title": "Average-Case Dimensionality Reduction in $\\ell_1$: Tree Ising Models",
            "updated": "2023-12-05T02:30:40Z",
            "published": "2023-12-05T02:30:40Z",
            "summary": "Given an arbitrary set of high dimensional points in $\\ell_1$, there are\nknown negative results that preclude the possibility of mapping them to a low\ndimensional $\\ell_1$ space while preserving distances with small multiplicative\ndistortion. This is in stark contrast with dimension reduction in Euclidean\nspace ($\\ell_2$) where such mappings are always possible. While the first\nnon-trivial lower bounds for $\\ell_1$ dimension reduction were established\nalmost 20 years ago, there has been minimal progress in understanding what sets\nof points in $\\ell_1$ are conducive to a low-dimensional mapping.\n  In this work, we shift the focus from the worst-case setting and initiate the\nstudy of a characterization of $\\ell_1$ metrics that are conducive to dimension\nreduction in $\\ell_1$. Our characterization focuses on metrics that are defined\nby the disagreement of binary variables over a probability distribution -- any\n$\\ell_1$ metric can be represented in this form. We show that, for\nconfigurations of $n$ points in $\\ell_1$ obtained from tree Ising models, we\ncan reduce dimension to $\\mathrm{polylog}(n)$ with constant distortion. In\ndoing so, we develop technical tools for embedding capped metrics (also known\nas truncated metrics) which have been studied because of their applications in\ncomputer vision, and are objects of independent interest in metric geometry.",
            "author": [
                "Moses Charikar",
                "Spencer Compton",
                "Chirag Pabbaraju"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02435v1",
                "http://arxiv.org/pdf/2312.02435v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02434v1",
            "title": "FINER: Flexible spectral-bias tuning in Implicit NEural Representation\n  by Variable-periodic Activation Functions",
            "updated": "2023-12-05T02:23:41Z",
            "published": "2023-12-05T02:23:41Z",
            "summary": "Implicit Neural Representation (INR), which utilizes a neural network to map\ncoordinate inputs to corresponding attributes, is causing a revolution in the\nfield of signal processing. However, current INR techniques suffer from a\nrestricted capability to tune their supported frequency set, resulting in\nimperfect performance when representing complex signals with multiple\nfrequencies. We have identified that this frequency-related problem can be\ngreatly alleviated by introducing variable-periodic activation functions, for\nwhich we propose FINER. By initializing the bias of the neural network within\ndifferent ranges, sub-functions with various frequencies in the\nvariable-periodic function are selected for activation. Consequently, the\nsupported frequency set of FINER can be flexibly tuned, leading to improved\nperformance in signal representation. We demonstrate the capabilities of FINER\nin the contexts of 2D image fitting, 3D signed distance field representation,\nand 5D neural radiance fields optimization, and we show that it outperforms\nexisting INRs.",
            "author": [
                "Zhen Liu",
                "Hao Zhu",
                "Qi Zhang",
                "Jingde Fu",
                "Weibing Deng",
                "Zhan Ma",
                "Yanwen Guo",
                "Xun Cao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02434v1",
                "http://arxiv.org/pdf/2312.02434v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02433v1",
            "title": "Lenna: Language Enhanced Reasoning Detection Assistant",
            "updated": "2023-12-05T02:19:35Z",
            "published": "2023-12-05T02:19:35Z",
            "summary": "With the fast-paced development of multimodal large language models (MLLMs),\nwe can now converse with AI systems in natural languages to understand images.\nHowever, the reasoning power and world knowledge embedded in the large language\nmodels have been much less investigated and exploited for image perception\ntasks. In this paper, we propose Lenna, a language-enhanced reasoning detection\nassistant, which utilizes the robust multimodal feature representation of\nMLLMs, while preserving location information for detection. This is achieved by\nincorporating an additional <DET> token in the MLLM vocabulary that is free of\nexplicit semantic context but serves as a prompt for the detector to identify\nthe corresponding position. To evaluate the reasoning capability of Lenna, we\nconstruct a ReasonDet dataset to measure its performance on reasoning-based\ndetection. Remarkably, Lenna demonstrates outstanding performance on ReasonDet\nand comes with significantly low training costs. It also incurs minimal\ntransferring overhead when extended to other tasks. Our code and model will be\navailable at https://git.io/Lenna.",
            "author": [
                "Fei Wei",
                "Xinyu Zhang",
                "Ailing Zhang",
                "Bo Zhang",
                "Xiangxiang Chu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02433v1",
                "http://arxiv.org/pdf/2312.02433v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02432v1",
            "title": "Orthogonal Adaptation for Modular Customization of Diffusion Models",
            "updated": "2023-12-05T02:17:48Z",
            "published": "2023-12-05T02:17:48Z",
            "summary": "Customization techniques for text-to-image models have paved the way for a\nwide range of previously unattainable applications, enabling the generation of\nspecific concepts across diverse contexts and styles. While existing methods\nfacilitate high-fidelity customization for individual concepts or a limited,\npre-defined set of them, they fall short of achieving scalability, where a\nsingle model can seamlessly render countless concepts. In this paper, we\naddress a new problem called Modular Customization, with the goal of\nefficiently merging customized models that were fine-tuned independently for\nindividual concepts. This allows the merged model to jointly synthesize\nconcepts in one image without compromising fidelity or incurring any additional\ncomputational costs.\n  To address this problem, we introduce Orthogonal Adaptation, a method\ndesigned to encourage the customized models, which do not have access to each\nother during fine-tuning, to have orthogonal residual weights. This ensures\nthat during inference time, the customized models can be summed with minimal\ninterference.\n  Our proposed method is both simple and versatile, applicable to nearly all\noptimizable weights in the model architecture. Through an extensive set of\nquantitative and qualitative evaluations, our method consistently outperforms\nrelevant baselines in terms of efficiency and identity preservation,\ndemonstrating a significant leap toward scalable customization of diffusion\nmodels.",
            "author": [
                "Ryan Po",
                "Guandao Yang",
                "Kfir Aberman",
                "Gordon Wetzstein"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02432v1",
                "http://arxiv.org/pdf/2312.02432v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02431v1",
            "title": "Visually Grounded Language Learning: a review of language games,\n  datasets, tasks, and models",
            "updated": "2023-12-05T02:17:29Z",
            "published": "2023-12-05T02:17:29Z",
            "summary": "In recent years, several machine learning models have been proposed. They are\ntrained with a language modelling objective on large-scale text-only data. With\nsuch pretraining, they can achieve impressive results on many Natural Language\nUnderstanding and Generation tasks. However, many facets of meaning cannot be\nlearned by ``listening to the radio\" only. In the literature, many\nVision+Language (V+L) tasks have been defined with the aim of creating models\nthat can ground symbols in the visual modality. In this work, we provide a\nsystematic literature review of several tasks and models proposed in the V+L\nfield. We rely on Wittgenstein's idea of `language games' to categorise such\ntasks into 3 different families: 1) discriminative games, 2) generative games,\nand 3) interactive games. Our analysis of the literature provides evidence that\nfuture work should be focusing on interactive games where communication in\nNatural Language is important to resolve ambiguities about object referents and\naction plans and that physical embodiment is essential to understand the\nsemantics of situations and events. Overall, these represent key requirements\nfor developing grounded meanings in neural models.",
            "author": [
                "Alessandro Suglia",
                "Ioannis Konstas",
                "Oliver Lemon"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02431v1",
                "http://arxiv.org/pdf/2312.02431v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02428v1",
            "title": "FreestyleRet: Retrieving Images from Style-Diversified Queries",
            "updated": "2023-12-05T02:07:31Z",
            "published": "2023-12-05T02:07:31Z",
            "summary": "Image Retrieval aims to retrieve corresponding images based on a given query.\nIn application scenarios, users intend to express their retrieval intent\nthrough various query styles. However, current retrieval tasks predominantly\nfocus on text-query retrieval exploration, leading to limited retrieval query\noptions and potential ambiguity or bias in user intention. In this paper, we\npropose the Style-Diversified Query-Based Image Retrieval task, which enables\nretrieval based on various query styles. To facilitate the novel setting, we\npropose the first Diverse-Style Retrieval dataset, encompassing diverse query\nstyles including text, sketch, low-resolution, and art. We also propose a\nlight-weighted style-diversified retrieval framework. For various query style\ninputs, we apply the Gram Matrix to extract the query's textural features and\ncluster them into a style space with style-specific bases. Then we employ the\nstyle-init prompt tuning module to enable the visual encoder to comprehend the\ntexture and style information of the query. Experiments demonstrate that our\nmodel, employing the style-init prompt tuning strategy, outperforms existing\nretrieval models on the style-diversified retrieval task. Moreover,\nstyle-diversified queries~(sketch+text, art+text, etc) can be simultaneously\nretrieved in our model. The auxiliary information from other queries enhances\nthe retrieval performance within the respective query.",
            "author": [
                "Hao Li",
                "Curise Jia",
                "Peng Jin",
                "Zesen Cheng",
                "Kehan Li",
                "Jialu Sui",
                "Chang Liu",
                "Li Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02428v1",
                "http://arxiv.org/pdf/2312.02428v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03016v1",
            "title": "Protein Language Model-Powered 3D Ligand Binding Site Prediction from\n  Protein Sequence",
            "updated": "2023-12-05T01:47:38Z",
            "published": "2023-12-05T01:47:38Z",
            "summary": "Prediction of ligand binding sites of proteins is a fundamental and important\ntask for understanding the function of proteins and screening potential drugs.\nMost existing methods require experimentally determined protein holo-structures\nas input. However, such structures can be unavailable on novel or less-studied\nproteins. To tackle this limitation, we propose LaMPSite, which only takes\nprotein sequences and ligand molecular graphs as input for ligand binding site\npredictions. The protein sequences are used to retrieve residue-level\nembeddings and contact maps from the pre-trained ESM-2 protein language model.\nThe ligand molecular graphs are fed into a graph neural network to compute\natom-level embeddings. Then we compute and update the protein-ligand\ninteraction embedding based on the protein residue-level embeddings and ligand\natom-level embeddings, and the geometric constraints in the inferred protein\ncontact map and ligand distance map. A final pooling on protein-ligand\ninteraction embedding would indicate which residues belong to the binding\nsites. Without any 3D coordinate information of proteins, our proposed model\nachieves competitive performance compared to baseline methods that require 3D\nprotein structures when predicting binding sites. Given that less than 50% of\nproteins have reliable structure information in the current stage, LaMPSite\nwill provide new opportunities for drug discovery.",
            "author": [
                "Shuo Zhang",
                "Lei Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03016v1",
                "http://arxiv.org/pdf/2312.03016v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02420v1",
            "title": "Towards Granularity-adjusted Pixel-level Semantic Annotation",
            "updated": "2023-12-05T01:37:18Z",
            "published": "2023-12-05T01:37:18Z",
            "summary": "Recent advancements in computer vision predominantly rely on learning-based\nsystems, leveraging annotations as the driving force to develop specialized\nmodels. However, annotating pixel-level information, particularly in semantic\nsegmentation, presents a challenging and labor-intensive task, prompting the\nneed for autonomous processes. In this work, we propose GranSAM which\ndistinguishes itself by providing semantic segmentation at the user-defined\ngranularity level on unlabeled data without the need for any manual\nsupervision, offering a unique contribution in the realm of semantic mask\nannotation method. Specifically, we propose an approach to enable the Segment\nAnything Model (SAM) with semantic recognition capability to generate\npixel-level annotations for images without any manual supervision. For this, we\naccumulate semantic information from synthetic images generated by the Stable\nDiffusion model or web crawled images and employ this data to learn a mapping\nfunction between SAM mask embeddings and object class labels. As a result, SAM,\nenabled with granularity-adjusted mask recognition, can be used for pixel-level\nsemantic annotation purposes. We conducted experiments on the PASCAL VOC 2012\nand COCO-80 datasets and observed a +17.95% and +5.17% increase in mIoU,\nrespectively, compared to existing state-of-the-art methods when evaluated\nunder our problem setting.",
            "author": [
                "Rohit Kundu",
                "Sudipta Paul",
                "Rohit Lal",
                "Amit K. Roy-Chowdhury"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02420v1",
                "http://arxiv.org/pdf/2312.02420v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03015v1",
            "title": "PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi-View\n  Instance Segmentation and Maximum Likelihood Estimation",
            "updated": "2023-12-05T01:33:04Z",
            "published": "2023-12-05T01:33:04Z",
            "summary": "Open-world 3D part segmentation is pivotal in diverse applications such as\nrobotics and AR/VR. Traditional supervised methods often grapple with limited\n3D data availability and struggle to generalize to unseen object categories.\nPartSLIP, a recent advancement, has made significant strides in zero- and\nfew-shot 3D part segmentation. This is achieved by harnessing the capabilities\nof the 2D open-vocabulary detection module, GLIP, and introducing a heuristic\nmethod for converting and lifting multi-view 2D bounding box predictions into\n3D segmentation masks. In this paper, we introduce PartSLIP++, an enhanced\nversion designed to overcome the limitations of its predecessor. Our approach\nincorporates two major improvements. First, we utilize a pre-trained 2D\nsegmentation model, SAM, to produce pixel-wise 2D segmentations, yielding more\nprecise and accurate annotations than the 2D bounding boxes used in PartSLIP.\nSecond, PartSLIP++ replaces the heuristic 3D conversion process with an\ninnovative modified Expectation-Maximization algorithm. This algorithm\nconceptualizes 3D instance segmentation as unobserved latent variables, and\nthen iteratively refines them through an alternating process of 2D-3D matching\nand optimization with gradient descent. Through extensive evaluations, we show\nthat PartSLIP++ demonstrates better performance over PartSLIP in both low-shot\n3D semantic and instance-based object part segmentation tasks. Code released at\nhttps://github.com/zyc00/PartSLIP2.",
            "author": [
                "Yuchen Zhou",
                "Jiayuan Gu",
                "Xuanlin Li",
                "Minghua Liu",
                "Yunhao Fang",
                "Hao Su"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03015v1",
                "http://arxiv.org/pdf/2312.03015v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02418v1",
            "title": "Decoding Data Quality via Synthetic Corruptions: Embedding-guided\n  Pruning of Code Data",
            "updated": "2023-12-05T01:19:30Z",
            "published": "2023-12-05T01:19:30Z",
            "summary": "Code datasets, often collected from diverse and uncontrolled sources such as\nGitHub, potentially suffer from quality issues, thereby affecting the\nperformance and training efficiency of Large Language Models (LLMs) optimized\nfor code generation. Previous studies demonstrated the benefit of using\nembedding spaces for data pruning, but they mainly focused on duplicate removal\nor increasing variety, and in other modalities, such as images. Our work\nfocuses on using embeddings to identify and remove \"low-quality\" code data.\nFirst, we explore features of \"low-quality\" code in embedding space, through\nthe use of synthetic corruptions. Armed with this knowledge, we devise novel\npruning metrics that operate in embedding space to identify and remove\nlow-quality entries in the Stack dataset. We demonstrate the benefits of this\nsynthetic corruption informed pruning (SCIP) approach on the well-established\nHumanEval and MBPP benchmarks, outperforming existing embedding-based methods.\nImportantly, we achieve up to a 3% performance improvement over no pruning,\nthereby showing the promise of insights from synthetic corruptions for data\npruning.",
            "author": [
                "Yu Yang",
                "Aaditya K. Singh",
                "Mostafa Elhoushi",
                "Anas Mahmoud",
                "Kushal Tirumala",
                "Fabian Gloeckle",
                "Baptiste Rozi\u00e8re",
                "Carole-Jean Wu",
                "Ari S. Morcos",
                "Newsha Ardalani"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02418v1",
                "http://arxiv.org/pdf/2312.02418v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03014v1",
            "title": "Foundation Models for Weather and Climate Data Understanding: A\n  Comprehensive Survey",
            "updated": "2023-12-05T01:10:54Z",
            "published": "2023-12-05T01:10:54Z",
            "summary": "As artificial intelligence (AI) continues to rapidly evolve, the realm of\nEarth and atmospheric sciences is increasingly adopting data-driven models,\npowered by progressive developments in deep learning (DL). Specifically, DL\ntechniques are extensively utilized to decode the chaotic and nonlinear aspects\nof Earth systems, and to address climate challenges via understanding weather\nand climate data. Cutting-edge performance on specific tasks within narrower\nspatio-temporal scales has been achieved recently through DL. The rise of large\nmodels, specifically large language models (LLMs), has enabled fine-tuning\nprocesses that yield remarkable outcomes across various downstream tasks,\nthereby propelling the advancement of general AI. However, we are still\nnavigating the initial stages of crafting general AI for weather and climate.\nIn this survey, we offer an exhaustive, timely overview of state-of-the-art AI\nmethodologies specifically engineered for weather and climate data, with a\nspecial focus on time series and text data. Our primary coverage encompasses\nfour critical aspects: types of weather and climate data, principal model\narchitectures, model scopes and applications, and datasets for weather and\nclimate. Furthermore, in relation to the creation and application of foundation\nmodels for weather and climate data understanding, we delve into the field's\nprevailing challenges, offer crucial insights, and propose detailed avenues for\nfuture research. This comprehensive approach equips practitioners with the\nrequisite knowledge to make substantial progress in this domain. Our survey\nencapsulates the most recent breakthroughs in research on large, data-driven\nmodels for weather and climate data understanding, emphasizing robust\nfoundations, current advancements, practical applications, crucial resources,\nand prospective research opportunities.",
            "author": [
                "Shengchao Chen",
                "Guodong Long",
                "Jing Jiang",
                "Dikai Liu",
                "Chengqi Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03014v1",
                "http://arxiv.org/pdf/2312.03014v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV",
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02412v1",
            "title": "A Turing Incomputable Coloring Function",
            "updated": "2023-12-05T01:03:53Z",
            "published": "2023-12-05T01:03:53Z",
            "summary": "This paper describes a sequence of natural numbers that grows faster than any\nTuring computable function. This sequence is generated from a version of the\ntiling problem, called a coloring system. In our proof that generates the\nsequence, we use the notions of a chain and an unbounded sequence property,\nwhich resemble the methods of point set topology. From this sequence, we define\na Turing incomputable coloring function.",
            "author": [
                "Michael Stephen Fiske"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02412v1",
                "http://arxiv.org/pdf/2312.02412v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC",
                "cs.LO",
                "math.CO",
                "math.LO",
                "03D15, 52C20"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02409v1",
            "title": "MGTR: Multi-Granular Transformer for Motion Prediction with LiDAR",
            "updated": "2023-12-05T00:48:31Z",
            "published": "2023-12-05T00:48:31Z",
            "summary": "Motion prediction has been an essential component of autonomous driving\nsystems since it handles highly uncertain and complex scenarios involving\nmoving agents of different types. In this paper, we propose a Multi-Granular\nTRansformer (MGTR) framework, an encoder-decoder network that exploits context\nfeatures in different granularities for different kinds of traffic agents. To\nfurther enhance MGTR's capabilities, we leverage LiDAR point cloud data by\nincorporating LiDAR semantic features from an off-the-shelf LiDAR feature\nextractor. We evaluate MGTR on Waymo Open Dataset motion prediction benchmark\nand show that the proposed method achieved state-of-the-art performance,\nranking 1st on its leaderboard\n(https://waymo.com/open/challenges/2023/motion-prediction/).",
            "author": [
                "Yiqian Gan",
                "Hao Xiao",
                "Yizhe Zhao",
                "Ethan Zhang",
                "Zhe Huang",
                "Xin Ye",
                "Lingting Ge"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02409v1",
                "http://arxiv.org/pdf/2312.02409v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02407v1",
            "title": "Robust Clustering using Hyperdimensional Computing",
            "updated": "2023-12-05T00:46:29Z",
            "published": "2023-12-05T00:46:29Z",
            "summary": "This paper addresses the clustering of data in the hyperdimensional computing\n(HDC) domain. In prior work, an HDC-based clustering framework, referred to as\nHDCluster, has been proposed. However, the performance of the existing\nHDCluster is not robust. The performance of HDCluster is degraded as the\nhypervectors for the clusters are chosen at random during the initialization\nstep. To overcome this bottleneck, we assign the initial cluster hypervectors\nby exploring the similarity of the encoded data, referred to as \\textit{query}\nhypervectors. Intra-cluster hypervectors have a higher similarity than\ninter-cluster hypervectors. Harnessing the similarity results among query\nhypervectors, this paper proposes four HDC-based clustering algorithms:\nsimilarity-based k-means, equal bin-width histogram, equal bin-height\nhistogram, and similarity-based affinity propagation. Experimental results\nillustrate that: (i) Compared to the existing HDCluster, our proposed HDC-based\nclustering algorithms can achieve better accuracy, more robust performance,\nfewer iterations, and less execution time. Similarity-based affinity\npropagation outperforms the other three HDC-based clustering algorithms on\neight datasets by 2~38% in clustering accuracy. (ii) Even for one-pass\nclustering, i.e., without any iterative update of the cluster hypervectors, our\nproposed algorithms can provide more robust clustering accuracy than HDCluster.\n(iii) Over eight datasets, five out of eight can achieve higher or comparable\naccuracy when projected onto the hyperdimensional space. Traditional clustering\nis more desirable than HDC when the number of clusters, $k$, is large.",
            "author": [
                "Lulu Ge",
                "Keshab K. Parhi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02407v1",
                "http://arxiv.org/pdf/2312.02407v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DB",
                "cs.SC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02406v1",
            "title": "Efficient Online Data Mixing For Language Model Pre-Training",
            "updated": "2023-12-05T00:42:35Z",
            "published": "2023-12-05T00:42:35Z",
            "summary": "The data used to pretrain large language models has a decisive impact on a\nmodel's downstream performance, which has led to a large body of work on data\nselection methods that aim to automatically determine the most suitable data to\nuse for pretraining. Existing data selection methods suffer from slow and\ncomputationally expensive processes, a problem amplified by the increasing size\nof models and of pretraining datasets. Data mixing, on the other hand, reduces\nthe complexity of data selection by grouping data points together and\ndetermining sampling probabilities across entire groups. However, data mixing\nproportions are typically fixed before training and therefore cannot adapt to\nchanging training dynamics. To address these limitations, we develop an\nefficient algorithm for Online Data Mixing (ODM) that combines elements from\nboth data selection and data mixing. Based on multi-armed bandit algorithms,\nour online approach optimizes the data mixing proportions during training.\nRemarkably, our method trains a model that reaches the final perplexity of the\nnext best method with 19\\% fewer training iterations, and improves performance\non the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible\nwall-clock time during pretraining.",
            "author": [
                "Alon Albalak",
                "Liangming Pan",
                "Colin Raffel",
                "William Yang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02406v1",
                "http://arxiv.org/pdf/2312.02406v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03013v1",
            "title": "Breast Ultrasound Report Generation using LangChain",
            "updated": "2023-12-05T00:28:26Z",
            "published": "2023-12-05T00:28:26Z",
            "summary": "Breast ultrasound (BUS) is a critical diagnostic tool in the field of breast\nimaging, aiding in the early detection and characterization of breast\nabnormalities. Interpreting breast ultrasound images commonly involves creating\ncomprehensive medical reports, containing vital information to promptly assess\nthe patient's condition. However, the ultrasound imaging system necessitates\ncapturing multiple images of various parts to compile a single report,\npresenting a time-consuming challenge. To address this problem, we propose the\nintegration of multiple image analysis tools through a LangChain using Large\nLanguage Models (LLM), into the breast reporting process. Through a combination\nof designated tools and text generation through LangChain, our method can\naccurately extract relevant features from ultrasound images, interpret them in\na clinical context, and produce comprehensive and standardized reports. This\napproach not only reduces the burden on radiologists and healthcare\nprofessionals but also enhances the consistency and quality of reports. The\nextensive experiments shows that each tools involved in the proposed method can\noffer qualitatively and quantitatively significant results. Furthermore,\nclinical evaluation on the generated reports demonstrates that the proposed\nmethod can make report in clinically meaningful way.",
            "author": [
                "Jaeyoung Huh",
                "Hyun Jeong Park",
                "Jong Chul Ye"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03013v1",
                "http://arxiv.org/pdf/2312.03013v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02403v1",
            "title": "Deep Neural Operator Enabled Concurrent Multitask Design for\n  Multifunctional Metamaterials under Heterogeneous Fields",
            "updated": "2023-12-05T00:12:41Z",
            "published": "2023-12-05T00:12:41Z",
            "summary": "Multifunctional metamaterials (MMM) bear promise as next-generation material\nplatforms supporting miniaturization and customization. Despite many\nproof-of-concept demonstrations and the proliferation of deep learning assisted\ndesign, grand challenges of inverse design for MMM, especially those involving\nheterogeneous fields possibly subject to either mutual meta-atom coupling or\nlong-range interactions, remain largely under-explored. To this end, we present\na data-driven design framework, which streamlines the inverse design of MMMs\ninvolving heterogeneous fields. A core enabler is implicit Fourier neural\noperator (IFNO), which predicts heterogeneous fields distributed across a\nmetamaterial array, thus in general at odds with homogenization assumptions, in\na parameter-/sample-efficient fashion. Additionally, we propose a standard\nformulation of inverse problem covering a broad class of MMMs, and\ngradient-based multitask concurrent optimization identifying a set of\nPareto-optimal architecture-stimulus (A-S) pairs. Fourier multiclass blending\nis proposed to synthesize inter-class meta-atoms anchored on a set of geometric\nmotifs, while enjoying training-free dimension reduction and built-it\nreconstruction. Interlocking the three pillars, the framework is validated for\nlight-bylight programmable plasmonic nanoantenna, whose design involves vast\nspace jointly spanned by quasi-freeform supercells, maneuverable incident phase\ndistributions, and conflicting figure-of-merits involving on-demand\nlocalization patterns. Accommodating all the challenges without a-priori\nsimplifications, our framework could propel future advancements of MMM.",
            "author": [
                "Doksoo Lee",
                "Lu Zhang",
                "Yue Yu",
                "Wei Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02403v1",
                "http://arxiv.org/pdf/2312.02403v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02399v1",
            "title": "Primordial fluctuations from quantum gravity: 16-cell topological model",
            "updated": "2023-12-04T23:53:59Z",
            "published": "2023-12-04T23:53:59Z",
            "summary": "We present a numerical analysis of an Hartle-Hawking state for the early\nuniverse, in the deep quantum regime, computed using the covariant Loop Quantum\nGravity formalism, in a truncation defined by 16-cell and in a simplified case\nwhere the dynamics is defined by SU(2) BF theory. We compute mean geometry,\nfluctuations and correlations. The results are consistent with the hypothesis\nthat refining the triangulation does not affect the global physical picture\nsubstantially.",
            "author": [
                "Pietropaolo Frisoni",
                "Francesco Gozzini",
                "Francesca Vidotto"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02399v1",
                "http://arxiv.org/pdf/2312.02399v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02396v1",
            "title": "Unsupervised Change Detection for Space Habitats Using 3D Point Clouds",
            "updated": "2023-12-04T23:26:12Z",
            "published": "2023-12-04T23:26:12Z",
            "summary": "This work presents an algorithm for scene change detection from point clouds\nto enable autonomous robotic caretaking in future space habitats. Autonomous\nrobotic systems will help maintain future deep-space habitats, such as the\nGateway space station, which will be uncrewed for extended periods. Existing\nscene analysis software used on the International Space Station (ISS) relies on\nmanually-labeled images for detecting changes. In contrast, the algorithm\npresented in this work uses raw, unlabeled point clouds as inputs. The\nalgorithm first applies modified Expectation-Maximization Gaussian Mixture\nModel (GMM) clustering to two input point clouds. It then performs change\ndetection by comparing the GMMs using the Earth Mover's Distance. The algorithm\nis validated quantitatively and qualitatively using a test dataset collected by\nan Astrobee robot in the NASA Ames Granite Lab comprising single frame depth\nimages taken directly by Astrobee and full-scene reconstructed maps built with\nRGB-D and pose data from Astrobee. The runtimes of the approach are also\nanalyzed in depth. The source code is publicly released to promote further\ndevelopment.",
            "author": [
                "Jamie Santos",
                "Holly Dinkel",
                "Julia Di",
                "Paulo V. K. Borges",
                "Marina Moreira",
                "Oleg Alexandrov",
                "Brian Coltin",
                "Trey Smith"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02396v1",
                "http://arxiv.org/pdf/2312.02396v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02393v1",
            "title": "Lecture Notes on Computerized Tomography",
            "updated": "2023-12-04T23:18:13Z",
            "published": "2023-12-04T23:18:13Z",
            "summary": "These lecture notes give an introduction to the mathematics of computer(ized)\ntomography (CT). Treated are the imaging principle of X-ray tomography, the\nRadon transform as mathematical model for the measurement process and its\nproperties, the ill-posedness of the underlying mathematical reconstruction\nproblem and classical reconstruction techniques. The required background from\nFourier analysis is also briefly summarized.",
            "author": [
                "Matthias Beckmann"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02393v1",
                "http://arxiv.org/pdf/2312.02393v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02391v1",
            "title": "Approximation of Intractable Likelihood Functions in Systems Biology via\n  Normalizing Flows",
            "updated": "2023-12-04T23:10:56Z",
            "published": "2023-12-04T23:10:56Z",
            "summary": "Systems biology relies on mathematical models that often involve complex and\nintractable likelihood functions, posing challenges for efficient inference and\nmodel selection. Generative models, such as normalizing flows, have shown\nremarkable ability in approximating complex distributions in various domains.\nHowever, their application in systems biology for approximating intractable\nlikelihood functions remains unexplored. Here, we elucidate a framework for\nleveraging normalizing flows to approximate complex likelihood functions\ninherent to systems biology models. By using normalizing flows in the\nSimulation-based inference setting, we demonstrate a method that not only\napproximates a likelihood function but also allows for model inference in the\nmodel selection setting. We showcase the effectiveness of this approach on\nreal-world systems biology problems, providing practical guidance for\nimplementation and highlighting its advantages over traditional computational\nmethods.",
            "author": [
                "Vincent D. Zaballa",
                "Elliot E. Hui"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02391v1",
                "http://arxiv.org/pdf/2312.02391v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02387v1",
            "title": "Dissecting Medical Referral Mechanisms in Health Services: Role of\n  Physician Professional Networks",
            "updated": "2023-12-04T23:03:09Z",
            "published": "2023-12-04T23:03:09Z",
            "summary": "Medical referrals between primary care physicians (PC) and specialist care\n(SC) physicians profoundly impact patient care regarding quality, satisfaction,\nand cost. This paper investigates the influence of professional networks among\nmedical doctors on referring patients from PC to SC. Using five-year\nconsultation data from a Portuguese private health provider, we conducted\nexploratory data analysis and constructed both professional and referral\nnetworks among physicians. We then apply Graph Neural Network (GNN) models to\nlearn latent representations of the referral network. Our analysis supports the\nhypothesis that doctors' professional social connections can predict medical\nreferrals, potentially enhancing collaboration within organizations and\nimproving healthcare services. This research contributes to dissecting the\nunderlying mechanisms in primary-specialty referrals, thereby providing\nvaluable insights for enhancing patient care and effective healthcare\nmanagement.",
            "author": [
                "Regina de Brito Duarte",
                "Qiwei Han",
                "Claudia Soares"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02387v1",
                "http://arxiv.org/pdf/2312.02387v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02385v1",
            "title": "Adaptive spectral proper orthogonal decomposition of tonal flows",
            "updated": "2023-12-04T23:01:35Z",
            "published": "2023-12-04T23:01:35Z",
            "summary": "An adaptive algorithm for spectral proper orthogonal decomposition (SPOD) of\nmixed broadband-tonal turbulent flows is developed. Sharp peak resolution at\ntonal frequencies is achieved by locally minimizing bias. Smooth spectrum\nestimates of broadband regions are achieved by locally reducing variance. The\nmethod utilizes multitaper estimation with sine tapers. An iterative criterion\nbased on modal convergence is introduced to enable the SPOD to adapt to\nspectral features. For tonal flows, the adaptivity is controlled by a single\nuser input; for broadband flows, a constant number of sine tapers is\nrecommended without adaptivity. The discrete version of Parseval's theorem for\nSPOD is stated. Proper normalization of the tapers ensures that Parseval's\ntheorem is satisfied in expectation. Drastic savings in computational\ncomplexity and memory usage are facilitated by two aspects: (i) sine tapers,\nwhich permit post hoc windowing of a single Fourier transform; and (ii)\ntime-domain lossless compression using a QR or eigenvalue decomposition.\nSine-taper SPOD is demonstrated on time-resolved particle image velocimetry\n(TR-PIV) data from an open cavity flow and high-fidelity large-eddy simulation\n(LES) data from a round jet, with and without adaptivity. For the tonal cavity\nflow, the adaptive algorithm outperforms Slepian-based multitaper SPOD in terms\nof variance reduction, local bias, mode convergence, and memory usage. For both\nthe tonal cavity and the broadband jet flows, results comparable to or better\nthan those from standard SPOD based on Welch's overlapped segment averaging are\nobtained with up to 75% fewer snapshots. Based on these examples, best practice\nis established.",
            "author": [
                "Brandon C. Y. Yeung",
                "Oliver T. Schmidt"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02385v1",
                "http://arxiv.org/pdf/2312.02385v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn",
                "nlin.CD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02384v1",
            "title": "The Akhiezer iteration",
            "updated": "2023-12-04T22:59:51Z",
            "published": "2023-12-04T22:59:51Z",
            "summary": "We develop the Akhiezer iteration, a generalization of the classical\nChebyshev iteration, for the inner product-free, iterative solution of\nindefinite linear systems using orthogonal polynomials for measures supported\non multiple, disjoint intervals. The iteration applies to shifted linear solves\nand can then be used for efficient matrix function approximation. Using the\nasymptotics of orthogonal polynomials, error bounds are provided. A key\ncomponent in the efficiency of the method is the ability to compute the first\n$k$ orthogonal polynomial recurrence coefficients and the first $k$ weighted\nStieltjes transforms of these orthogonal polynomials in $\\mathrm{O}(k)$\ncomplexity using a numerical Riemann--Hilbert approach. For a special class of\northogonal polynomials, the Akhiezer polynomials, the method can be sped up\nsignificantly, with the greatest speedup occurring in the two interval case\nwhere important formulae of Akhiezer are employed and the Riemann--Hilbert\napproach is bypassed.",
            "author": [
                "Cade Ballew",
                "Thomas Trogdon"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02384v1",
                "http://arxiv.org/pdf/2312.02384v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "math.CV",
                "42C05, 65E05, 33C47, 65F10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02383v1",
            "title": "Homomesy on permutations with toggling actions",
            "updated": "2023-12-04T22:57:40Z",
            "published": "2023-12-04T22:57:40Z",
            "summary": "Homomesy is an invariance phenomenon in dynamical algebraic combinatorics\nwhich occurs when the average value of some statistic on a set of combinatorial\nobjects is the same over each orbit generated by a map on these objects. In\nthis paper we perform a systematic search for statistics homomesic for the set\nof permutations under the rotation map, identifying and proving 34 instances of\nhomomesy. We show that these homomesies actually hold not only for rotation but\nin fact for a whole class of maps related to rotation by the notion of\ntoggling, which is identified initially with composition of simple\ntranspositions. In this way these maps are related to the rowmotion action\ndefined on various combinatorial structures, which has a useful definition in\nterms of toggling. We prove some initial results on maps given by restricted or\nmodified toggles. We discuss also the computational method used to identify\ncandidate statistics from FindStat, a combinatorial statistics database.",
            "author": [
                "William Dowling",
                "Nadia Lafreniere"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02383v1",
                "http://arxiv.org/pdf/2312.02383v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02382v1",
            "title": "New Evaluation Metrics Capture Quality Degradation due to LLM\n  Watermarking",
            "updated": "2023-12-04T22:56:31Z",
            "published": "2023-12-04T22:56:31Z",
            "summary": "With the increasing use of large-language models (LLMs) like ChatGPT,\nwatermarking has emerged as a promising approach for tracing machine-generated\ncontent. However, research on LLM watermarking often relies on simple\nperplexity or diversity-based measures to assess the quality of watermarked\ntext, which can mask important limitations in watermarking. Here we introduce\ntwo new easy-to-use methods for evaluating watermarking algorithms for LLMs: 1)\nevaluation by LLM-judger with specific guidelines; and 2) binary classification\non text embeddings to distinguish between watermarked and unwatermarked text.\nWe apply these methods to characterize the effectiveness of current\nwatermarking techniques. Our experiments, conducted across various datasets,\nreveal that current watermarking methods are detectable by even simple\nclassifiers, challenging the notion of watermarking subtlety. We also found,\nthrough the LLM judger, that watermarking impacts text quality, especially in\ndegrading the coherence and depth of the response. Our findings underscore the\ntrade-off between watermark robustness and text quality and highlight the\nimportance of having more informative metrics to assess watermarking quality.",
            "author": [
                "Karanpartap Singh",
                "James Zou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02382v1",
                "http://arxiv.org/pdf/2312.02382v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02377v1",
            "title": "Clifford Manipulations of Stabilizer States: A graphical rule book for\n  Clifford unitaries and measurements on cluster states, and application to\n  photonic quantum computing",
            "updated": "2023-12-04T22:40:24Z",
            "published": "2023-12-04T22:40:24Z",
            "summary": "Stabilizer states along with Clifford manipulations (unitary transformations\nand measurements) thereof -- despite being efficiently simulable on a classical\ncomputer -- are an important tool in quantum information processing, with\napplications to quantum computing, error correction and networking. Cluster\nstates, defined on a graph, are a special class of stabilizer states that are\ncentral to measurement based quantum computing, all-photonic quantum repeaters,\ndistributed quantum computing, and entanglement distribution in a network. All\ncluster states are local-Clifford equivalent to a stabilizer state. In this\npaper, we review the stabilizer framework, and extend it, by: incorporating\ngeneral stabilizer measurements such as multi-qubit fusions, and providing an\nexplicit procedure -- using Karnaugh maps from Boolean algebra -- for\nconverting arbitrary stabilizer gates into tableau operations of the CHP\nformalism for efficient stabilizer manipulations. Using these tools, we develop\na graphical rule-book and a MATLAB simulator with a graphical user interface\nfor arbitrary stabilizer manipulations of cluster states, a user of which,\ne.g., for research in quantum networks, will not require any background in\nquantum information or the stabilizer framework. We extend our graphical\nrule-book to include dual-rail photonic-qubit cluster state manipulations with\nprobabilistically-heralded linear-optical circuits for various rotated Bell\nmeasurements, i.e., fusions (including new `Type-I' fusions we propose, where\nonly one of the two fused qubits is destructively measured), by incorporating\ngraphical rules for their success and failure modes. Finally, we show how\nstabilizer descriptions of multi-qubit fusions can be mapped to linear optical\ncircuits.",
            "author": [
                "Ashlesha Patil",
                "Saikat Guha"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02377v1",
                "http://arxiv.org/pdf/2312.02377v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02376v1",
            "title": "Fast Fourier Transform periodic interpolation method for superposition\n  sums in a periodic unit cell",
            "updated": "2023-12-04T22:32:42Z",
            "published": "2023-12-04T22:32:42Z",
            "summary": "We propose a Fast Fourier Transform based Periodic Interpolation Method\n(FFT-PIM), a flexible and computationally efficient approach for computing the\nscalar potential given by a superposition sum in a unit cell of an infinitely\nperiodic array. Under the same umbrella, FFT-PIM allows computing the potential\nfor 1D, 2D, and 3D periodicities for dynamic and static problems, including\nproblems with and without a periodic phase shift. The computational complexity\nof the FFT-PIM is of $O(N \\log N)$ for $N$ spatially coinciding sources and\nobserver points. The FFT-PIM uses rapidly converging series representations of\nthe Green's function serving as a kernel in the superposition sum. Based on\nthese representations, the FFT-PIM splits the potential into its near-zone\ncomponent, which includes a small number of images surrounding the unit cell of\ninterest, and far-zone component, which includes the rest of an infinite number\nof images. The far-zone component is evaluated by projecting the non-uniform\nsources onto a sparse uniform grid, performing superposition sums on this\nsparse grid, and interpolating the potential from the uniform grid to the\nnon-uniform observation points. The near-zone component is evaluated using an\nFFT-based method, which is adapted to efficiently handle non-uniform\nsource-observer distributions within the periodic unit cell. The FFT-PIM can be\nused for a broad range of applications, such as periodic problems involving\nintegral equations in computational electromagnetic and acoustic, micromagnetic\nsolvers, and density functional theory solvers.",
            "author": [
                "Fangzhou Ai",
                "Vitaliy Lomakin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02376v1",
                "http://arxiv.org/pdf/2312.02376v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "math-ph",
                "math.MP",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03758v1",
            "title": "Stock Movement and Volatility Prediction from Tweets, Macroeconomic\n  Factors and Historical Prices",
            "updated": "2023-12-04T22:27:43Z",
            "published": "2023-12-04T22:27:43Z",
            "summary": "Predicting stock market is vital for investors and policymakers, acting as a\nbarometer of the economic health. We leverage social media data, a potent\nsource of public sentiment, in tandem with macroeconomic indicators as\ngovernment-compiled statistics, to refine stock market predictions. However,\nprior research using tweet data for stock market prediction faces three\nchallenges. First, the quality of tweets varies widely. While many are filled\nwith noise and irrelevant details, only a few genuinely mirror the actual\nmarket scenario. Second, solely focusing on the historical data of a particular\nstock without considering its sector can lead to oversight. Stocks within the\nsame industry often exhibit correlated price behaviors. Lastly, simply\nforecasting the direction of price movement without assessing its magnitude is\nof limited value, as the extent of the rise or fall truly determines\nprofitability. In this paper, diverging from the conventional methods, we\npioneer an ECON. The framework has following advantages: First, ECON has an\nadept tweets filter that efficiently extracts and decodes the vast array of\ntweet data. Second, ECON discerns multi-level relationships among stocks,\nsectors, and macroeconomic factors through a self-aware mechanism in semantic\nspace. Third, ECON offers enhanced accuracy in predicting substantial stock\nprice fluctuations by capitalizing on stock price movement. We showcase the\nstate-of-the-art performance of our proposed model using a dataset,\nspecifically curated by us, for predicting stock market movements and\nvolatility.",
            "author": [
                "Shengkun Wang",
                "YangXiao Bai",
                "Taoran Ji",
                "Kaiqun Fu",
                "Linhan Wang",
                "Chang-Tien Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03758v1",
                "http://arxiv.org/pdf/2312.03758v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02374v1",
            "title": "Prospects for constraining quasar ages with fiber spectrographs:\n  Quasar-induced Ly$\u03b1$ emission from the intergalactic medium",
            "updated": "2023-12-04T22:16:33Z",
            "published": "2023-12-04T22:16:33Z",
            "summary": "We present a theoretical framework for linking quasar properties, such as\nquasar age, to the surrounding Ly$\\alpha$ emission intensity. In particular, we\nfocus on a method for mapping the large-scale structure of Ly$\\alpha$ emission\nintensity with galaxy spectra from wide-field spectroscopic surveys, e.g., the\nSubaru Prime Focus Spectrograph (PFS) or the Dark Energy Spectroscopic\nInstrument (DESI), and consider the quasar-induced Ly$\\alpha$ emission from the\nintergalactic medium (IGM). To do this, we construct a theoretical model based\non two physical processes: resonant scattering of quasar Ly$\\alpha$ photons and\nfluorescence due to quasar ionizing photons, finding that the fluorescence\ncontribution due to optically thick gas clouds is dominant. Taking into account\nthe light cone effect and assuming a typical quasar spectrum, we calculate the\nfluorescence contribution to the spectrum stacked within each bin of the\nseparation angle from the quasar as a function of quasar age. Furthermore, we\ncompute the quasar-Ly$\\alpha$ emission cross-correlation and its SNR for the\nplanned PFS survey. The predicted signal can account for $\\sim10\\%$ of the\nmeasurements indicated from the BOSS and eBOSS surveys in the outer region of\n$>10\\ \\rm{cMpc}\\ \\rm{h}^{-1}$. The predicted SNR is not enough to detect the\nquasar-induced contribution, while it is enhanced by including contributions\nfrom other Ly$\\alpha$ emission sources, e.g., star-forming galaxies. We discuss\nother possible contributions to the Ly$\\alpha$ emission excess around quasars,\nthe efficiency of using spectroscopic fibers, and the redshift dependence of\nour model.",
            "author": [
                "Ryuichiro Hada",
                "Masahiro Takada",
                "Akio K. Inoue"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02374v1",
                "http://arxiv.org/pdf/2312.02374v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02368v1",
            "title": "RINAS: Training with Dataset Shuffling Can Be General and Fast",
            "updated": "2023-12-04T21:50:08Z",
            "published": "2023-12-04T21:50:08Z",
            "summary": "Deep learning datasets are expanding at an unprecedented pace, creating new\nchallenges for data processing in model training pipelines. A crucial aspect of\nthese pipelines is dataset shuffling, which significantly improves unbiased\nlearning and convergence accuracy by adhering to the principles of random\nsampling. However, loading shuffled data for large datasets incurs significant\noverhead in the deep learning pipeline and severely impacts the end-to-end\ntraining throughput. To mitigate this, current deep learning systems often\nresort to partial dataset shuffling, sacrificing global randomness to maintain\nacceptable training throughput on large datasets, still leaving global\nshuffling efficiency issues not fully explored.\n  In this work, we present RINAS, a data loading framework that systematically\naddresses the performance bottleneck of loading global shuffled datasets. Our\nkey contribution is to offer an intra-batch unordered data fetching approach,\nwhich unleashes unexplored parallelism of data loading. We implement RINAS\nunder the PyTorch framework for common dataset libraries HuggingFace and\nTorchVision. Our experimental results show that RINAS improves the throughput\nof general language model training and vision model training by up to 59% and\n89%, respectively.",
            "author": [
                "Tianle Zhong",
                "Jiechen Zhao",
                "Xindi Guo",
                "Qiang Su",
                "Geoffrey Fox"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02368v1",
                "http://arxiv.org/pdf/2312.02368v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB",
                "cs.DC",
                "cs.LG",
                "cs.PF"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02367v1",
            "title": "States as goal-directed concepts: an epistemic approach to\n  state-representation learning",
            "updated": "2023-12-04T21:48:38Z",
            "published": "2023-12-04T21:48:38Z",
            "summary": "Our goals fundamentally shape how we experience the world. For example, when\nwe are hungry, we tend to view objects in our environment according to whether\nor not they are edible (or tasty). Alternatively, when we are cold, we may view\nthe very same objects according to their ability to produce heat. Computational\ntheories of learning in cognitive systems, such as reinforcement learning, use\nthe notion of \"state-representation\" to describe how agents decide which\nfeatures of their environment are behaviorally-relevant and which can be\nignored. However, these approaches typically assume \"ground-truth\" state\nrepresentations that are known by the agent, and reward functions that need to\nbe learned. Here we suggest an alternative approach in which\nstate-representations are not assumed veridical, or even pre-defined, but\nrather emerge from the agent's goals through interaction with its environment.\nWe illustrate this novel perspective by inferring the goals driving rat\nbehavior in an odor-guided choice task and discuss its implications for\ndeveloping, from first principles, an information-theoretic account of\ngoal-directed state representation learning and behavior.",
            "author": [
                "Nadav Amir",
                "Yael Niv",
                "Angela Langdon"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02367v1",
                "http://arxiv.org/pdf/2312.02367v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02366v1",
            "title": "Towards General Purpose Vision Foundation Models for Medical Image\n  Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks",
            "updated": "2023-12-04T21:47:10Z",
            "published": "2023-12-04T21:47:10Z",
            "summary": "The integration of deep learning systems into the medical domain has been\nhindered by the resource-intensive process of data annotation and the inability\nof these systems to generalize to different data distributions. Foundation\nmodels, which are models pre-trained on large datasets, have emerged as a\nsolution to reduce reliance on annotated data and enhance model\ngeneralizability and robustness. DINOv2, an open-source foundation model\npre-trained with self-supervised learning on 142 million curated natural\nimages, excels in extracting general-purpose visual representations, exhibiting\npromising capabilities across various vision tasks. Nevertheless, a critical\nquestion remains unanswered regarding DINOv2's adaptability to radiological\nimaging, and the clarity on whether its features are sufficiently general to\nbenefit radiology image analysis is yet to be established. Therefore, this\nstudy comprehensively evaluates DINOv2 for radiology, conducting over 100\nexperiments across diverse modalities (X-ray, CT, and MRI). Tasks include\ndisease classification and organ segmentation on both 2D and 3D images,\nevaluated under different settings like kNN, few-shot learning, linear-probing,\nend-to-end fine-tuning, and parameter-efficient fine-tuning, to measure the\neffectiveness and generalizability of the DINOv2 feature embeddings.\nComparative analyses with established medical image analysis models, U-Net and\nTransUnet for segmentation, and CNN and ViT models pre-trained via supervised,\nweakly supervised, and self-supervised learning for classification, reveal\nDINOv2's superior performance in segmentation tasks and competitive results in\ndisease classification. The findings contribute insights to potential avenues\nfor optimizing pre-training strategies for medical imaging and enhancing the\nbroader understanding of DINOv2's role in bridging the gap between natural and\nradiological image analysis.",
            "author": [
                "Mohammed Baharoon",
                "Waseem Qureshi",
                "Jiahong Ouyang",
                "Yanwu Xu",
                "Kilian Phol",
                "Abdulrhman Aljouie",
                "Wei Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02366v1",
                "http://arxiv.org/pdf/2312.02366v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02365v1",
            "title": "MEDPSeg: End-to-end segmentation of pulmonary structures and lesions in\n  computed tomography",
            "updated": "2023-12-04T21:46:39Z",
            "published": "2023-12-04T21:46:39Z",
            "summary": "The COVID-19 pandemic response highlighted the potential of deep learning\nmethods in facilitating the diagnosis and prognosis of lung diseases through\nautomated segmentation of normal and abnormal tissue in computed tomography\n(CT). Such methods not only have the potential to aid in clinical\ndecision-making but also contribute to the comprehension of novel diseases. In\nlight of the labor-intensive nature of manual segmentation for large chest CT\ncohorts, there is a pressing need for reliable automated approaches that enable\nefficient analysis of chest CT anatomy in vast research databases, especially\nin more scarcely annotated targets such as pneumonia consolidations. A limiting\nfactor for the development of such methods is that most current models optimize\na fixed annotation format per network output. To tackle this problem,\npolymorphic training is used to optimize a network with a fixed number of\noutput channels to represent multiple hierarchical anatomic structures,\nindirectly optimizing more complex labels with simpler annotations. We combined\nover 6000 volumetric CT scans containing varying formats of manual and\nautomated labels from different sources, and used polymorphic training along\nwith multitask learning to develop MEDPSeg, an end-to-end method for the\nsegmentation of lungs, airways, pulmonary artery, and lung lesions with\nseparation of ground glass opacities, and parenchymal consolidations, all in a\nsingle forward prediction. We achieve state-of-the-art performance in multiple\ntargets, particularly in the segmentation of ground glass opacities and\nconsolidations, a challenging problem with limited manual annotation\navailability. In addition, we provide an open-source implementation with a\ngraphical user interface at https://github.com/MICLab-Unicamp/medpseg.",
            "author": [
                "Diedre S. Carmo",
                "Jean Ribeiro",
                "Alejandro P. Comellas",
                "Joseph M. Reinhardt",
                "Sarah E. Gerard",
                "Let\u00edcia Rittner",
                "Roberto A. Lotufo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02365v1",
                "http://arxiv.org/pdf/2312.02365v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02364v1",
            "title": "Class-Discriminative Attention Maps for Vision Transformers",
            "updated": "2023-12-04T21:46:21Z",
            "published": "2023-12-04T21:46:21Z",
            "summary": "Interpretability methods are critical components for examining and exploring\ndeep neural networks (DNN), as well as increasing our understanding of and\ntrust in them. Vision transformers (ViT), which can be trained to\nstate-of-the-art performance with a self-supervised learning (SSL) training\nmethod, provide built-in attention maps (AM). While AMs can provide\nhigh-quality semantic segmentation of input images, they do not account for any\nsignal coming from a downstream classifier. We introduce class-discriminative\nattention maps (CDAM), a novel post-hoc explanation method that is highly\nsensitive to the target class. Our method essentially scales attention scores\nby how relevant the corresponding tokens are for the predictions of a\nclassifier head. Alternative to classifier outputs, CDAM can also explain a\nuser-defined concept by targeting similarity measures in the latent space of\nthe ViT. This allows for explanations of arbitrary concepts, defined by the\nuser through a few sample images. We investigate the operating characteristics\nof CDAM in comparison with relevance propagation (RP) and token ablation maps\n(TAM), an alternative to pixel occlusion methods. CDAM is highly\nclass-discriminative and semantically relevant, while providing implicit\nregularization of relevance scores.\n  PyTorch implementation: \\url{https://github.com/lenbrocki/CDAM}\n  Web live demo: \\url{https://cdam.informatism.com/}",
            "author": [
                "Lennart Brocki",
                "Neo Christopher Chung"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02364v1",
                "http://arxiv.org/pdf/2312.02364v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02362v1",
            "title": "PointNeRF++: A multi-scale, point-based Neural Radiance Field",
            "updated": "2023-12-04T21:43:00Z",
            "published": "2023-12-04T21:43:00Z",
            "summary": "Point clouds offer an attractive source of information to complement images\nin neural scene representations, especially when few images are available.\nNeural rendering methods based on point clouds do exist, but they do not\nperform well when the point cloud quality is low -- e.g., sparse or incomplete,\nwhich is often the case with real-world data. We overcome these problems with a\nsimple representation that aggregates point clouds at multiple scale levels\nwith sparse voxel grids at different resolutions. To deal with point cloud\nsparsity, we average across multiple scale levels -- but only among those that\nare valid, i.e., that have enough neighboring points in proximity to the ray of\na pixel. To help model areas without points, we add a global voxel at the\ncoarsest scale, thus unifying \"classical\" and point-based NeRF formulations. We\nvalidate our method on the NeRF Synthetic, ScanNet, and KITTI-360 datasets,\noutperforming the state of the art by a significant margin.",
            "author": [
                "Weiwei Sun",
                "Eduard Trulls",
                "Yang-Che Tseng",
                "Sneha Sambandam",
                "Gopal Sharma",
                "Andrea Tagliasacchi",
                "Kwang Moo Yi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02362v1",
                "http://arxiv.org/pdf/2312.02362v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02357v1",
            "title": "Classification of Minimal Separating Sets of Low Genus Surfaces",
            "updated": "2023-12-04T21:36:28Z",
            "published": "2023-12-04T21:36:28Z",
            "summary": "A minimal separating set in a connected topological space $X$ is a subset $L\n\\subset X$ with the property that $X \\setminus L$ is disconnected, but if\n$L^{\\prime}$ is a proper subset of $L$, then $X \\setminus L^{\\prime}$ is\nconnected. Such sets show up in a variety of contexts. For example, in a wide\nclass of metric spaces, if we choose distinct points p and q, then the set of\npoints x satisfying d(x, p) = d(x, q) is a minimal separating set. In this\npaper we classify which topological graphs can be realized as minimal\nseparating sets in surfaces of low genus. In general the question of whether a\ngraph can be embedded at all in a surface is a difficult one, so our work is\npartly computational. We classify graphs embeddings which are minimal\nseparating in a given genus and write a computer program to find all such\nembeddings and their underlying graphs.",
            "author": [
                "Christopher N. Aagaard",
                "J. J. P. Veerman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02357v1",
                "http://arxiv.org/pdf/2312.02357v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02354v1",
            "title": "Bubble wall velocities with an extended fluid Ansatz",
            "updated": "2023-12-04T21:34:26Z",
            "published": "2023-12-04T21:34:26Z",
            "summary": "We compute the terminal bubble wall velocity during a cosmological phase\ntransition by modelling non-equilibrium effects in the plasma with the\nso-called \"extended fluid Ansatz\". A $\\phi^6$ operator is included in the\nStandard Model effective potential to mimic effects of new physics.\nHydrodynamical heating of the plasma ahead of the bubble is taken into account.\nWe find that the inclusion of higher order terms in the fluid Ansatz is\ntypically relevant, and may even turn detonation solutions into deflagrations.\nOur results also corroborate recent findings in the literature that, for a\nStandard Model particle content in the plasma, only deflagration solutions are\nviable. However, we also show that this outcome may be altered in a theory with\na different particle content.",
            "author": [
                "Glauber C. Dorsch",
                "Daniel A. Pinto"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02354v1",
                "http://arxiv.org/pdf/2312.02354v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02350v1",
            "title": "Calibrated Uncertainties for Neural Radiance Fields",
            "updated": "2023-12-04T21:29:31Z",
            "published": "2023-12-04T21:29:31Z",
            "summary": "Neural Radiance Fields have achieved remarkable results for novel view\nsynthesis but still lack a crucial component: precise measurement of\nuncertainty in their predictions. Probabilistic NeRF methods have tried to\naddress this, but their output probabilities are not typically accurately\ncalibrated, and therefore do not capture the true confidence levels of the\nmodel. Calibration is a particularly challenging problem in the sparse-view\nsetting, where additional held-out data is unavailable for fitting a calibrator\nthat generalizes to the test distribution. In this paper, we introduce the\nfirst method for obtaining calibrated uncertainties from NeRF models. Our\nmethod is based on a robust and efficient metric to calculate per-pixel\nuncertainties from the predictive posterior distribution. We propose two\ntechniques that eliminate the need for held-out data. The first, based on patch\nsampling, involves training two NeRF models for each scene. The second is a\nnovel meta-calibrator that only requires the training of one NeRF model. Our\nproposed approach for obtaining calibrated uncertainties achieves\nstate-of-the-art uncertainty in the sparse-view setting while maintaining image\nquality. We further demonstrate our method's effectiveness in applications such\nas view enhancement and next-best view selection.",
            "author": [
                "Niki Amini-Naieni",
                "Tomas Jakab",
                "Andrea Vedaldi",
                "Ronald Clark"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02350v1",
                "http://arxiv.org/pdf/2312.02350v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03012v1",
            "title": "A Waddington landscape for prototype learning in generalized Hopfield\n  networks",
            "updated": "2023-12-04T21:28:14Z",
            "published": "2023-12-04T21:28:14Z",
            "summary": "Networks in machine learning offer examples of complex high-dimensional\ndynamical systems reminiscent of biological systems. Here, we study the\nlearning dynamics of Generalized Hopfield networks, which permit a\nvisualization of internal memories. These networks have been shown to proceed\nthrough a 'feature-to-prototype' transition, as the strength of network\nnonlinearity is increased, wherein the learned, or terminal, states of internal\nmemories transition from mixed to pure states. Focusing on the prototype\nlearning dynamics of the internal memories we observe a strong resemblance to\nthe canalized, or low-dimensional, dynamics of cells as they differentiate\nwithin a Waddingtonian landscape. Dynamically, we demonstrate that learning in\na Generalized Hopfield Network proceeds through sequential 'splits' in memory\nspace. Furthermore, order of splitting is interpretable and reproducible. The\ndynamics between the splits are canalized in the Waddington sense -- robust to\nvariations in detailed aspects of the system. In attempting to make the analogy\na rigorous equivalence, we study smaller subsystems that exhibit similar\nproperties to the full system. We combine analytical calculations with\nnumerical simulations to study the dynamical emergence of the\nfeature-to-prototype transition, and the behaviour of splits in the landscape,\nsaddles points, visited during learning. We exhibit regimes where saddles\nappear and disappear through saddle-node bifurcations, qualitatively changing\nthe distribution of learned memories as the strength of the nonlinearity is\nvaried -- allowing us to systematically investigate the mechanisms that\nunderlie the emergence of Waddingtonian dynamics. Memories can thus\ndifferentiate in a predictive and controlled way, revealing new bridges between\nexperimental biology, dynamical systems theory, and machine learning.",
            "author": [
                "Nacer Eddine Boukacem",
                "Allen Leary",
                "Robin Th\u00e9riault",
                "Felix Gottlieb",
                "Madhav Mani",
                "Paul Fran\u00e7ois"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03012v1",
                "http://arxiv.org/pdf/2312.03012v1"
            ],
            "primary_category": "cond-mat.dis-nn",
            "category": [
                "cond-mat.dis-nn",
                "cs.LG",
                "cs.NE",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02345v1",
            "title": "CLIPDrawX: Primitive-based Explanations for Text Guided Sketch Synthesis",
            "updated": "2023-12-04T21:11:42Z",
            "published": "2023-12-04T21:11:42Z",
            "summary": "With the goal of understanding the visual concepts that CLIP associates with\ntext prompts, we show that the latent space of CLIP can be visualized solely in\nterms of linear transformations on simple geometric primitives like circles and\nstraight lines. Although existing approaches achieve this by\nsketch-synthesis-through-optimization, they do so on the space of B\\'ezier\ncurves, which exhibit a wastefully large set of structures that they can evolve\ninto, as most of them are non-essential for generating meaningful sketches. We\npresent CLIPDrawX, an algorithm that provides significantly better\nvisualizations for CLIP text embeddings, using only simple primitive shapes\nlike straight lines and circles. This constrains the set of possible outputs to\nlinear transformations on these primitives, thereby exhibiting an inherently\nsimpler mathematical form. The synthesis process of CLIPDrawX can be tracked\nend-to-end, with each visual concept being explained exclusively in terms of\nprimitives. Implementation will be released upon acceptance. Project Page:\n$\\href{https://clipdrawx.github.io/}{\\text{https://clipdrawx.github.io/}}$.",
            "author": [
                "Nityanand Mathur",
                "Shyam Marjit",
                "Abhra Chaudhuri",
                "Anjan Dutta"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02345v1",
                "http://arxiv.org/pdf/2312.02345v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02344v1",
            "title": "STEREOFOG -- Computational DeFogging via Image-to-Image Translation on a\n  real-world Dataset",
            "updated": "2023-12-04T21:07:13Z",
            "published": "2023-12-04T21:07:13Z",
            "summary": "Image-to-Image translation (I2I) is a subtype of Machine Learning (ML) that\nhas tremendous potential in applications where two domains of images and the\nneed for translation between the two exist, such as the removal of fog. For\nexample, this could be useful for autonomous vehicles, which currently struggle\nwith adverse weather conditions like fog. However, datasets for I2I tasks are\nnot abundant and typically hard to acquire. Here, we introduce STEREOFOG, a\ndataset comprised of $10,067$ paired fogged and clear images, captured using a\ncustom-built device, with the purpose of exploring I2I's potential in this\ndomain. It is the only real-world dataset of this kind to the best of our\nknowledge. Furthermore, we apply and optimize the pix2pix I2I ML framework to\nthis dataset. With the final model achieving an average Complex\nWavelet-Structural Similarity (CW-SSIM) score of $0.76$, we prove the\ntechnique's suitability for the problem.",
            "author": [
                "Anton Pollak",
                "Rajesh Menon"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02344v1",
                "http://arxiv.org/pdf/2312.02344v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02342v1",
            "title": "Subcomplexes on filtered manifolds",
            "updated": "2023-12-04T20:59:51Z",
            "published": "2023-12-04T20:59:51Z",
            "summary": "In this paper, we present a general construction of complexes on filtered\nmanifolds. In the particular case of regular subRiemannian manifolds, we show\nthat this yields the so-called Rumin complex when the manifold is also equipped\nwith a compatible Riemannian metric. Another instance is set on a nilpotent Lie\ngroup equipped with a left invariant filtration; our scheme then computes the\nLie algebra cohomology.",
            "author": [
                "Veronique Fischer",
                "Francesca Tripaldi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02342v1",
                "http://arxiv.org/pdf/2312.02342v1"
            ],
            "primary_category": "math.DG",
            "category": [
                "math.DG",
                "58A10, 58J10, 58H99, 53C17, 43A80, 22E25"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02341v1",
            "title": "Incentive Systems for Fleets of New Mobility Services",
            "updated": "2023-12-04T20:58:02Z",
            "published": "2023-12-04T20:58:02Z",
            "summary": "Traffic congestion has become an inevitable challenge in large cities due to\npopulation increases and expansion of urban areas. Various approaches are\nintroduced to mitigate traffic issues, encompassing from expanding the road\ninfrastructure to employing demand management. Congestion pricing and incentive\nschemes are extensively studied for traffic control in traditional networks\nwhere each driver is a network \"player\". In this setup, drivers' \"selfish\"\nbehavior hinders the network from reaching a socially optimal state. In future\nmobility services, on the other hand, a large portion of drivers/vehicles may\nbe controlled by a small number of companies/organizations. In such a system,\noffering incentives to organizations can potentially be much more effective in\nreducing traffic congestion rather than offering incentives directly to\ndrivers. This paper studies the problem of offering incentives to organizations\nto change the behavior of their individual drivers (or individuals relying on\nthe organization's services). We developed a model where incentives are offered\nto each organization based on the aggregated travel time loss across all\ndrivers in that organization. Such an incentive offering mechanism requires\nsolving a large-scale optimization problem to minimize the system-level travel\ntime. We propose an efficient algorithm for solving this optimization problem.\nNumerous experiments on Los Angeles County traffic data reveal the ability of\nour method to reduce system-level travel time by up to 6.9%. Moreover, our\nexperiments demonstrate that incentivizing organizations can be up to 8 times\nmore efficient than incentivizing individual drivers in terms of\nincentivization monetary cost.",
            "author": [
                "Ali Ghafelebashi",
                "Meisam Razaviyayn",
                "Maged Dessouky"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02341v1",
                "http://arxiv.org/pdf/2312.02341v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02338v1",
            "title": "A Contrastive Compositional Benchmark for Text-to-Image Synthesis: A\n  Study with Unified Text-to-Image Fidelity Metrics",
            "updated": "2023-12-04T20:47:48Z",
            "published": "2023-12-04T20:47:48Z",
            "summary": "Text-to-image (T2I) synthesis has recently achieved significant advancements.\nHowever, challenges remain in the model's compositionality, which is the\nability to create new combinations from known components. We introduce\nWinoground-T2I, a benchmark designed to evaluate the compositionality of T2I\nmodels. This benchmark includes 11K complex, high-quality contrastive sentence\npairs spanning 20 categories. These contrastive sentence pairs with subtle\ndifferences enable fine-grained evaluations of T2I synthesis models.\nAdditionally, to address the inconsistency across different metrics, we propose\na strategy that evaluates the reliability of various metrics by using\ncomparative sentence pairs. We use Winoground-T2I with a dual objective: to\nevaluate the performance of T2I models and the metrics used for their\nevaluation. Finally, we provide insights into the strengths and weaknesses of\nthese metrics and the capabilities of current T2I models in tackling challenges\nacross a range of complex compositional categories. Our benchmark is publicly\navailable at https://github.com/zhuxiangru/Winoground-T2I .",
            "author": [
                "Xiangru Zhu",
                "Penglei Sun",
                "Chengyu Wang",
                "Jingping Liu",
                "Zhixu Li",
                "Yanghua Xiao",
                "Jun Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02338v1",
                "http://arxiv.org/pdf/2312.02338v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02337v1",
            "title": "Measuring Distributional Shifts in Text: The Advantage of Language\n  Model-Based Embeddings",
            "updated": "2023-12-04T20:46:48Z",
            "published": "2023-12-04T20:46:48Z",
            "summary": "An essential part of monitoring machine learning models in production is\nmeasuring input and output data drift. In this paper, we present a system for\nmeasuring distributional shifts in natural language data and highlight and\ninvestigate the potential advantage of using large language models (LLMs) for\nthis problem. Recent advancements in LLMs and their successful adoption in\ndifferent domains indicate their effectiveness in capturing semantic\nrelationships for solving various natural language processing problems. The\npower of LLMs comes largely from the encodings (embeddings) generated in the\nhidden layers of the corresponding neural network. First we propose a\nclustering-based algorithm for measuring distributional shifts in text data by\nexploiting such embeddings. Then we study the effectiveness of our approach\nwhen applied to text embeddings generated by both LLMs and classical embedding\nalgorithms. Our experiments show that general-purpose LLM-based embeddings\nprovide a high sensitivity to data drift compared to other embedding methods.\nWe propose drift sensitivity as an important evaluation metric to consider when\ncomparing language models. Finally, we present insights and lessons learned\nfrom deploying our framework as part of the Fiddler ML Monitoring platform over\na period of 18 months.",
            "author": [
                "Gyandev Gupta",
                "Bashir Rastegarpanah",
                "Amalendu Iyer",
                "Joshua Rubin",
                "Krishnaram Kenthapadi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02337v1",
                "http://arxiv.org/pdf/2312.02337v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02334v1",
            "title": "An Evaluation Framework for Mapping News Headlines to Event Classes in a\n  Knowledge Graph",
            "updated": "2023-12-04T20:42:26Z",
            "published": "2023-12-04T20:42:26Z",
            "summary": "Mapping ongoing news headlines to event-related classes in a rich knowledge\nbase can be an important component in a knowledge-based event analysis and\nforecasting solution. In this paper, we present a methodology for creating a\nbenchmark dataset of news headlines mapped to event classes in Wikidata, and\nresources for the evaluation of methods that perform the mapping. We use the\ndataset to study two classes of unsupervised methods for this task: 1)\nadaptations of classic entity linking methods, and 2) methods that treat the\nproblem as a zero-shot text classification problem. For the first approach, we\nevaluate off-the-shelf entity linking systems. For the second approach, we\nexplore a) pre-trained natural language inference (NLI) models, and b)\npre-trained large generative language models. We present the results of our\nevaluation, lessons learned, and directions for future work. The dataset and\nscripts for evaluation are made publicly available.",
            "author": [
                "Steve Fonin Mbouadeu",
                "Martin Lorenzo",
                "Ken Barker",
                "Oktie Hassanzadeh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02334v1",
                "http://arxiv.org/pdf/2312.02334v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03011v1",
            "title": "InstructBooth: Instruction-following Personalized Text-to-Image\n  Generation",
            "updated": "2023-12-04T20:34:46Z",
            "published": "2023-12-04T20:34:46Z",
            "summary": "Personalizing text-to-image models using a limited set of images for a\nspecific object has been explored in subject-specific image generation.\nHowever, existing methods often encounter challenges in aligning with text\nprompts due to overfitting to the limited training images. In this work, we\nintroduce InstructBooth, a novel method designed to enhance image-text\nalignment in personalized text-to-image models. Our approach first personalizes\ntext-to-image models with a small number of subject-specific images using a\nunique identifier. After personalization, we fine-tune personalized\ntext-to-image models using reinforcement learning to maximize a reward that\nquantifies image-text alignment. Additionally, we propose complementary\ntechniques to increase the synergy between these two processes. Our method\ndemonstrates superior image-text alignment compared to baselines while\nmaintaining personalization ability. In human evaluations, InstructBooth\noutperforms DreamBooth when considering all comprehensive factors.",
            "author": [
                "Daewon Chae",
                "Nokyung Park",
                "Jinkyu Kim",
                "Kimin Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03011v1",
                "http://arxiv.org/pdf/2312.03011v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02332v1",
            "title": "Connected Components in Linear Work and Near-Optimal Time",
            "updated": "2023-12-04T20:33:54Z",
            "published": "2023-12-04T20:33:54Z",
            "summary": "Computing the connected components of a graph is a fundamental problem in\nalgorithmic graph theory. A major question in this area is whether we can\ncompute connected components in $o(\\log n)$ parallel time. Recent works showed\nan affirmative answer in the Massively Parallel Computation (MPC) model for a\nwide class of graphs. Specifically, Behnezhad et al. (FOCS'19) showed that\nconnected components can be computed in $O(\\log d + \\log \\log n)$ rounds in the\nMPC model. More recently, Liu et al. (SPAA'20) showed that the same result can\nbe achieved in the standard PRAM model but their result incurs $\\Theta((m+n)\n\\cdot (\\log d + \\log \\log n))$ work which is sub-optimal.\n  In this paper, we show that for graphs that contain \\emph{well-connected}\ncomponents, we can compute connected components on a PRAM in sub-logarithmic\nparallel time with \\emph{optimal}, i.e., $O(m+n)$ total work. Specifically, our\nalgorithm achieves $O(\\log(1/\\lambda) + \\log \\log n)$ parallel time with high\nprobability, where $\\lambda$ is the minimum spectral gap of any connected\ncomponent in the input graph. The algorithm requires no prior knowledge on\n$\\lambda$.\n  Additionally, based on the \\textsc{2-Cycle} Conjecture we provide a time\nlower bound of $\\Omega(\\log(1/\\lambda))$ for solving connected components on a\nPRAM with $O(m+n)$ total memory when $\\lambda \\le (1/\\log n)^c$, giving\nconditional optimality to the running time of our algorithm as a parameter of\n$\\lambda$.",
            "author": [
                "Alireza Farhadi",
                "S. Cliff Liu",
                "Elaine Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02332v1",
                "http://arxiv.org/pdf/2312.02332v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02331v1",
            "title": "Revisiting Topic-Guided Language Models",
            "updated": "2023-12-04T20:33:24Z",
            "published": "2023-12-04T20:33:24Z",
            "summary": "A recent line of work in natural language processing has aimed to combine\nlanguage models and topic models. These topic-guided language models augment\nneural language models with topic models, unsupervised learning methods that\ncan discover document-level patterns of word use. This paper compares the\neffectiveness of these methods in a standardized setting. We study four\ntopic-guided language models and two baselines, evaluating the held-out\npredictive performance of each model on four corpora. Surprisingly, we find\nthat none of these methods outperform a standard LSTM language model baseline,\nand most fail to learn good topics. Further, we train a probe of the neural\nlanguage model that shows that the baseline's hidden states already encode\ntopic information. We make public all code used for this study.",
            "author": [
                "Carolina Zheng",
                "Keyon Vafa",
                "David M. Blei"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02331v1",
                "http://arxiv.org/pdf/2312.02331v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02327v1",
            "title": "FLea: Improving federated learning on scarce and label-skewed data via\n  privacy-preserving feature augmentation",
            "updated": "2023-12-04T20:24:09Z",
            "published": "2023-12-04T20:24:09Z",
            "summary": "Learning a global model by abstracting the knowledge, distributed across\nmultiple clients, without aggregating the raw data is the primary goal of\nFederated Learning (FL). Typically, this works in rounds alternating between\nparallel local training at several clients, followed by model aggregation at a\nserver. We found that existing FL methods under-perform when local datasets are\nsmall and present severe label skew as these lead to over-fitting and local\nmodel bias. This is a realistic setting in many real-world applications. To\naddress the problem, we propose \\textit{FLea}, a unified framework that tackles\nover-fitting and local bias by encouraging clients to exchange\nprivacy-protected features to aid local training. The features refer to\nactivations from an intermediate layer of the model, which are obfuscated\nbefore being shared with other clients to protect sensitive information in the\ndata. \\textit{FLea} leverages a novel way of combining local and shared\nfeatures as augmentations to enhance local model learning. Our extensive\nexperiments demonstrate that \\textit{FLea} outperforms the start-of-the-art FL\nmethods, sharing only model parameters, by up to $17.6\\%$, and FL methods that\nshare data augmentations by up to $6.3\\%$, while reducing the privacy\nvulnerability associated with shared data augmentations.",
            "author": [
                "Tong Xia",
                "Abhirup Ghosh",
                "Cecilia Mascolo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02327v1",
                "http://arxiv.org/pdf/2312.02327v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02322v1",
            "title": "Mixed Quantum/Classical Theory (MQCT) Approach to the Dynamics of\n  Molecule-Molecule Collisions in Complex Systems",
            "updated": "2023-12-04T20:10:07Z",
            "published": "2023-12-04T20:10:07Z",
            "summary": "We developed a general theoretical approach and a user-ready computer code\nthat permit to study the dynamics of collisional energy transfer and\nro-vibrational energy exchange in complex molecule-molecule collisions. The\nmethod is a mixture of classical and quantum mechanics. The internal\nro-vibrational motion of collision partners is treated quantum mechanically\nusing time-dependent Schrodinger equation that captures many quantum phenomena\nincluding state quantization and zero-point energy, propensity and selection\nrules for state-to-state transitions, quantum symmetry and interference\nphenomena. A significant numerical speed up is obtained by describing the\ntranslational motion of collision partners classically, using the Ehrenfest\nmean-field trajectory approach. Within this framework a family of approximate\nmethods for collision dynamics is developed. Several benchmark studies for\ndiatomic and triatomic molecules, such as H$_2$O and ND$_3$ collided with He,\nH$_2$ and D$_2$, show that the results of MQCT are in good agreement with\nfull-quantum calculations in a broad range of energies, especially at high\ncollision energies where they become nearly identical to the full quantum\nresults. Numerical efficiency of the method and massive parallelism of the MQCT\ncode permit us to embrace some of the most complicated collisional systems ever\nstudied, such as C$_6$H$_6$ + He, CH$_3$COOH + He and H$_2$O + H$_2$O.\nApplication of MQCT to the collisions of chiral molecules such as\nCH$_3$CHCH$_2$O + He, and to the molecule-surface collisions is also possible\nand will be pursued in the future.",
            "author": [
                "Carolin Joy",
                "Bikramaditya Mandal",
                "Dulat Bostan",
                "Marie-Lise Dubernet",
                "Dmitri Babikov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02322v1",
                "http://arxiv.org/pdf/2312.02322v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02320v1",
            "title": "Cable Slack Detection for Arresting Gear Application using Machine\n  Vision",
            "updated": "2023-12-04T20:00:40Z",
            "published": "2023-12-04T20:00:40Z",
            "summary": "The cable-based arrestment systems are integral to the launch and recovery of\naircraft onboard carriers and on expeditionary land-based installations. These\nmodern arrestment systems rely on various mechanisms to absorb energy from an\naircraft during an arrestment cycle to bring the aircraft to a full stop. One\nof the primary components of this system is the cable interface to the engine.\nThe formation of slack in the cable at this interface can result in reduced\nefficiency and drives maintenance efforts to remove the slack prior to\ncontinued operations. In this paper, a machine vision based slack detection\nsystem is presented. A situational awareness camera is utilized to collect\nvideo data of the cable interface region, machine vision algorithms are applied\nto reduce noise, remove background clutter, focus on regions of interest, and\ndetect changes in the image representative of slack formations. Some algorithms\nemployed in this system include bilateral image filters, least squares\npolynomial fit, Canny Edge Detection, K-Means clustering, Gaussian\nMixture-based Background/Foreground Segmentation for background subtraction,\nHough Circle Transforms, and Hough line Transforms. The resulting detections\nare filtered and highlighted to create an indication to the shipboard operator\nof the presence of slack and a need for a maintenance action. A user interface\nwas designed to provide operators with an easy method to redefine regions of\ninterest and adjust the methods to specific locations. The algorithms were\nvalidated on shipboard footage and were able to accurately identify slack with\nminimal false positives.",
            "author": [
                "Ari Goodman",
                "Glenn Shevach",
                "Sean Zabriskie",
                "Dr. Chris Thajudeen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02320v1",
                "http://arxiv.org/pdf/2312.02320v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02317v1",
            "title": "GNN2R: Weakly-Supervised Rationale-Providing Question Answering over\n  Knowledge Graphs",
            "updated": "2023-12-04T19:58:07Z",
            "published": "2023-12-04T19:58:07Z",
            "summary": "Most current methods for multi-hop question answering (QA) over knowledge\ngraphs (KGs) only provide final conclusive answers without explanations, such\nas a set of KG entities that is difficult for normal users to review and\ncomprehend. This issue severely limits the application of KG-based QA in\nreal-world scenarios. However, it is non-trivial to solve due to two\nchallenges: First, annotations of reasoning chains of multi-hop questions,\nwhich could serve as supervision for explanation generation, are usually\nlacking. Second, it is difficult to maintain high efficiency when explicit KG\ntriples need to be retrieved to generate explanations. In this paper, we\npropose a novel Graph Neural Network-based Two-Step Reasoning model (GNN2R) to\nsolve this issue. GNN2R can provide both final answers and reasoning subgraphs\nas a rationale behind final answers efficiently with only weak supervision that\nis available through question-final answer pairs. We extensively evaluated\nGNN2R with detailed analyses in experiments. The results demonstrate that, in\nterms of effectiveness, efficiency, and quality of generated explanations,\nGNN2R outperforms existing state-of-the-art methods that are applicable to this\ntask. Our code and pre-trained models are available at\nhttps://github.com/ruijie-wang-uzh/GNN2R.",
            "author": [
                "Ruijie Wang",
                "Luca Rossetto",
                "Michael Cochez",
                "Abraham Bernstein"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02317v1",
                "http://arxiv.org/pdf/2312.02317v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02314v1",
            "title": "Fine-tuning pre-trained extractive QA models for clinical document\n  parsing",
            "updated": "2023-12-04T19:52:56Z",
            "published": "2023-12-04T19:52:56Z",
            "summary": "Electronic health records (EHRs) contain a vast amount of high-dimensional\nmulti-modal data that can accurately represent a patient's medical history.\nUnfortunately, most of this data is either unstructured or semi-structured,\nrendering it unsuitable for real-time and retrospective analyses. A remote\npatient monitoring (RPM) program for Heart Failure (HF) patients needs to have\naccess to clinical markers like EF (Ejection Fraction) or LVEF (Left\nVentricular Ejection Fraction) in order to ascertain eligibility and\nappropriateness for the program. This paper explains a system that can parse\nechocardiogram reports and verify EF values. This system helps identify\neligible HF patients who can be enrolled in such a program. At the heart of\nthis system is a pre-trained extractive QA transformer model that is fine-tuned\non custom-labeled data. The methods used to prepare such a model for deployment\nare illustrated by running experiments on a public clinical dataset like\nMIMIC-IV-Note. The pipeline can be used to generalize solutions to similar\nproblems in a low-resource setting. We found that the system saved over 1500\nhours for our clinicians over 12 months by automating the task at scale.",
            "author": [
                "Ashwyn Sharma",
                "David I. Feldman",
                "Aneesh Jain"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02314v1",
                "http://arxiv.org/pdf/2312.02314v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02312v1",
            "title": "Visual Encoders for Data-Efficient Imitation Learning in Modern Video\n  Games",
            "updated": "2023-12-04T19:52:12Z",
            "published": "2023-12-04T19:52:12Z",
            "summary": "Video games have served as useful benchmarks for the decision making\ncommunity, but going beyond Atari games towards training agents in modern games\nhas been prohibitively expensive for the vast majority of the research\ncommunity. Recent progress in the research, development and open release of\nlarge vision models has the potential to amortize some of these costs across\nthe community. However, it is currently unclear which of these models have\nlearnt representations that retain information critical for sequential decision\nmaking. Towards enabling wider participation in the research of gameplaying\nagents in modern games, we present a systematic study of imitation learning\nwith publicly available visual encoders compared to the typical, task-specific,\nend-to-end training approach in Minecraft, Minecraft Dungeons and\nCounter-Strike: Global Offensive.",
            "author": [
                "Lukas Sch\u00e4fer",
                "Logan Jones",
                "Anssi Kanervisto",
                "Yuhan Cao",
                "Tabish Rashid",
                "Raluca Georgescu",
                "Dave Bignell",
                "Siddhartha Sen",
                "Andrea Trevi\u00f1o Gavito",
                "Sam Devlin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02312v1",
                "http://arxiv.org/pdf/2312.02312v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02310v1",
            "title": "VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding",
            "updated": "2023-12-04T19:48:02Z",
            "published": "2023-12-04T19:48:02Z",
            "summary": "Recent advancements in language-model-based video understanding have been\nprogressing at a remarkable pace, spurred by the introduction of Large Language\nModels (LLMs). However, the focus of prior research has been predominantly on\ndevising a projection layer that maps video features to tokens, an approach\nthat is both rudimentary and inefficient. In our study, we introduce a\ncutting-edge framework, VaQuitA, designed to refine the synergy between video\nand textual information. At the data level, instead of sampling frames\nuniformly, we implement a sampling method guided by CLIP-score rankings, which\nenables a more aligned selection of frames with the given question. At the\nfeature level, we integrate a trainable Video Perceiver alongside a\nVisual-Query Transformer (abbreviated as VQ-Former), which bolsters the\ninterplay between the input question and the video features. We also discover\nthat incorporating a simple prompt, \"Please be critical\", into the LLM input\ncan substantially enhance its video comprehension capabilities. Our\nexperimental results indicate that VaQuitA consistently sets a new benchmark\nfor zero-shot video question-answering tasks and is adept at producing\nhigh-quality, multi-turn video dialogues with users.",
            "author": [
                "Yizhou Wang",
                "Ruiyi Zhang",
                "Haoliang Wang",
                "Uttaran Bhattacharya",
                "Yun Fu",
                "Gang Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02310v1",
                "http://arxiv.org/pdf/2312.02310v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02308v1",
            "title": "AdsorbRL: Deep Multi-Objective Reinforcement Learning for Inverse\n  Catalysts Design",
            "updated": "2023-12-04T19:44:04Z",
            "published": "2023-12-04T19:44:04Z",
            "summary": "A central challenge of the clean energy transition is the development of\ncatalysts for low-emissions technologies. Recent advances in Machine Learning\nfor quantum chemistry drastically accelerate the computation of catalytic\nactivity descriptors such as adsorption energies. Here we introduce AdsorbRL, a\nDeep Reinforcement Learning agent aiming to identify potential catalysts given\na multi-objective binding energy target, trained using offline learning on the\nOpen Catalyst 2020 and Materials Project data sets. We experiment with Deep\nQ-Network agents to traverse the space of all ~160,000 possible unary, binary\nand ternary compounds of 55 chemical elements, with very sparse rewards based\non adsorption energy known for only between 2,000 and 3,000 catalysts per\nadsorbate. To constrain the actions space, we introduce Random Edge Traversal\nand train a single-objective DQN agent on the known states subgraph, which we\nfind strengthens target binding energy by an average of 4.1 eV. We extend this\napproach to multi-objective, goal-conditioned learning, and train a DQN agent\nto identify materials with the highest (respectively lowest) adsorption\nenergies for multiple simultaneous target adsorbates. We experiment with\nObjective Sub-Sampling, a novel training scheme aimed at encouraging\nexploration in the multi-objective setup, and demonstrate simultaneous\nadsorption energy improvement across all target adsorbates, by an average of\n0.8 eV. Overall, our results suggest strong potential for Deep Reinforcement\nLearning applied to the inverse catalysts design problem.",
            "author": [
                "Romain Lacombe",
                "Lucas Hendren",
                "Khalid El-Awady"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02308v1",
                "http://arxiv.org/pdf/2312.02308v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02305v1",
            "title": "Effects of a mixed reality headset on the delay of visually evoked\n  potentials",
            "updated": "2023-12-04T19:39:32Z",
            "published": "2023-12-04T19:39:32Z",
            "summary": "Virtual and mixed reality (VR, MR) technologies offer a powerful solution for\non-the-ground flight training curricula. While these technologies offer safer\nand cheaper instructional programs, it is still unclear how they impact\nneuronal brain dynamics. Indeed, MR simulations engage students in a strange\nmix of incongruous visual, somatosensory and vestibular sensory input.\nCharacterizing brain dynamics during MR simulation is important for\nunderstanding cognitive processes during virtual flight training. To this end,\nwe studies the delays introduced in the neuronal stream from the retina to the\nvisual cortex when presented with visual stimuli using a Varjo-XR3 headset. We\nrecorded cortical visual evoked potentials (VEPs) from 6 subjects under two\nconditions. First, we recorded normal VEPs triggered by short flashes. Second,\nwe recorded VEPs triggered by an internal image of the flashes produced by the\nVarjo-XR3 headset. All subjects had used the headset before and were familiar\nwith immersive experiences. Our results show mixed-reality stimulation imposes\na small, but consistent, 4 [ms] processing delay in the N2-VEP component during\nMR stimulation as compared to direct stimulation. Also we found that VEP\namplitudes during MR stimulation were also decreased. These results suggest\nthat visual cognition during mixed-reality training is delayed, not only by the\nunavoidalbe hardware/software processing delays of the headset and the attached\ncomputer, but also by an extra biological delay induced by the headset's\nlimited visual display in terms of the image intensity and contrast. As flight\ntraining is a demanding task, this study measures visual signal latency to\nbetter understand how MR affects the sensation of immersion.",
            "author": [
                "V\u00edctor Manuel Hidalgo",
                "Carlos Andr\u00e9s Bazaes",
                "Juan-Carlos Letelier"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02305v1",
                "http://arxiv.org/pdf/2312.02305v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03756v1",
            "title": "LineConGraphs: Line Conversation Graphs for Effective Emotion\n  Recognition using Graph Neural Networks",
            "updated": "2023-12-04T19:36:58Z",
            "published": "2023-12-04T19:36:58Z",
            "summary": "Emotion Recognition in Conversations (ERC) is a critical aspect of affective\ncomputing, and it has many practical applications in healthcare, education,\nchatbots, and social media platforms. Earlier approaches for ERC analysis\ninvolved modeling both speaker and long-term contextual information using graph\nneural network architectures. However, it is ideal to deploy\nspeaker-independent models for real-world applications. Additionally, long\ncontext windows can potentially create confusion in recognizing the emotion of\nan utterance in a conversation. To overcome these limitations, we propose novel\nline conversation graph convolutional network (LineConGCN) and graph attention\n(LineConGAT) models for ERC analysis. These models are speaker-independent and\nbuilt using a graph construction strategy for conversations -- line\nconversation graphs (LineConGraphs). The conversational context in\nLineConGraphs is short-term -- limited to one previous and future utterance,\nand speaker information is not part of the graph. We evaluate the performance\nof our proposed models on two benchmark datasets, IEMOCAP and MELD, and show\nthat our LineConGAT model outperforms the state-of-the-art methods with an\nF1-score of 64.58% and 76.50%. Moreover, we demonstrate that embedding\nsentiment shift information into line conversation graphs further enhances the\nERC performance in the case of GCN models.",
            "author": [
                "Gokul S Krishnan",
                "Sarala Padi",
                "Craig S. Greenberg",
                "Balaraman Ravindran",
                "Dinesh Manoch",
                "Ram D. Sriram"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03756v1",
                "http://arxiv.org/pdf/2312.03756v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02301v1",
            "title": "Aerosols are not Spherical Cows: Using Discrete Dipole Approximation to\n  Model the Properties of Fractal Particles",
            "updated": "2023-12-04T19:34:43Z",
            "published": "2023-12-04T19:34:43Z",
            "summary": "The optical properties of particulate-matter aerosols, within the context of\nexoplanet and brown dwarf atmospheres, are compared using three different\nmodels: Mie theory, Modified Mean Field (MMF) Theory, and Discrete Dipole\nApproximation (DDA). Previous results have demonstrated that fractal haze\nparticles (MMF and DDA) absorb much less long-wavelength radiation than their\nspherical counterparts (Mie), however it is shown here that the opposite can\nalso be true if a more varying refractive index profile is used. Additionally,\nit is demonstrated that absorption and scattering cross-sections, as well as\nthe asymmetry parameter, are underestimated if Mie theory is used. Although DDA\ncan be used to obtain more accurate results, it is known to be much more\ncomputationally intensive; to avoid this, the use of low-resolution aerosol\nmodels is explored, which could dramatically speed up the process of obtaining\naccurate computations of optical cross-sections within a certain parameter\nspace. The validity of DDA is probed for wavelengths of interest for\nobservations of aerosols within exoplanet and brown dwarf atmospheres (0.2 to\n15 micrometres). Finally, novel code is presented to compare the results of\nMie, MMF and DDA theories (CORAL: Comparison Of Radiative AnaLyses), as well as\nto increase and decrease the resolution of DDA shape files accordingly\n(SPHERIFY). Both codes can be applied to a range of other interesting\nastrophysical environments in addition to exoplanet atmospheres, for example\ndust grains within protoplanetary disks.",
            "author": [
                "Matt G. Lodge",
                "Hannah R. Wakeford",
                "Zoe M. Leinhardt"
            ],
            "link": [
                "http://dx.doi.org/10.1093/mnras/stad3743",
                "http://arxiv.org/abs/2312.02301v1",
                "http://arxiv.org/pdf/2312.02301v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP",
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02299v1",
            "title": "Cotton Yield Prediction Using Random Forest",
            "updated": "2023-12-04T19:33:29Z",
            "published": "2023-12-04T19:33:29Z",
            "summary": "The cotton industry in the United States is committed to sustainable\nproduction practices that minimize water, land, and energy use while improving\nsoil health and cotton output. Climate-smart agricultural technologies are\nbeing developed to boost yields while decreasing operating expenses. Crop yield\nprediction, on the other hand, is difficult because of the complex and\nnonlinear impacts of cultivar, soil type, management, pest and disease,\nclimate, and weather patterns on crops. To solve this issue, we employ machine\nlearning (ML) to forecast production while considering climate change, soil\ndiversity, cultivar, and inorganic nitrogen levels. From the 1980s to the\n1990s, field data were gathered across the southern cotton belt of the United\nStates. To capture the most current effects of climate change over the previous\nsix years, a second data source was produced using the process-based crop\nmodel, GOSSYM. We concentrated our efforts on three distinct areas inside each\nof the three southern states: Texas, Mississippi, and Georgia. To simplify the\namount of computations, accumulated heat units (AHU) for each set of\nexperimental data were employed as an analogy to use time-series weather data.\nThe Random Forest Regressor yielded a 97.75% accuracy rate, with a root mean\nsquare error of 55.05 kg/ha and an R2 of around 0.98. These findings\ndemonstrate how an ML technique may be developed and applied as a reliable and\neasy-to-use model to support the cotton climate-smart initiative.",
            "author": [
                "Alakananda Mitra",
                "Sahila Beegum",
                "David Fleisher",
                "Vangimalla R. Reddy",
                "Wenguang Sun",
                "Chittaranjan Ray",
                "Dennis Timlin",
                "Arindam Malakar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02299v1",
                "http://arxiv.org/pdf/2312.02299v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02298v1",
            "title": "MoE-AMC: Enhancing Automatic Modulation Classification Performance Using\n  Mixture-of-Experts",
            "updated": "2023-12-04T19:31:15Z",
            "published": "2023-12-04T19:31:15Z",
            "summary": "Automatic Modulation Classification (AMC) plays a vital role in time series\nanalysis, such as signal classification and identification within wireless\ncommunications. Deep learning-based AMC models have demonstrated significant\npotential in this domain. However, current AMC models inadequately consider the\ndisparities in handling signals under conditions of low and high\nSignal-to-Noise Ratio (SNR), resulting in an unevenness in their performance.\nIn this study, we propose MoE-AMC, a novel Mixture-of-Experts (MoE) based model\nspecifically crafted to address AMC in a well-balanced manner across varying\nSNR conditions. Utilizing the MoE framework, MoE-AMC seamlessly combines the\nstrengths of LSRM (a Transformer-based model) for handling low SNR signals and\nHSRM (a ResNet-based model) for high SNR signals. This integration empowers\nMoE-AMC to achieve leading performance in modulation classification, showcasing\nits efficacy in capturing distinctive signal features under diverse SNR\nscenarios. We conducted experiments using the RML2018.01a dataset, where\nMoE-AMC achieved an average classification accuracy of 71.76% across different\nSNR levels, surpassing the performance of previous SOTA models by nearly 10%.\nThis study represents a pioneering application of MoE techniques in the realm\nof AMC, offering a promising avenue for elevating signal classification\naccuracy within wireless communication systems.",
            "author": [
                "Jiaxin Gao",
                "Qinglong Cao",
                "Yuntian Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02298v1",
                "http://arxiv.org/pdf/2312.02298v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.CV",
                "cs.LG",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02296v1",
            "title": "LLMs Accelerate Annotation for Medical Information Extraction",
            "updated": "2023-12-04T19:26:13Z",
            "published": "2023-12-04T19:26:13Z",
            "summary": "The unstructured nature of clinical notes within electronic health records\noften conceals vital patient-related information, making it challenging to\naccess or interpret. To uncover this hidden information, specialized Natural\nLanguage Processing (NLP) models are required. However, training these models\nnecessitates large amounts of labeled data, a process that is both\ntime-consuming and costly when relying solely on human experts for annotation.\nIn this paper, we propose an approach that combines Large Language Models\n(LLMs) with human expertise to create an efficient method for generating ground\ntruth labels for medical text annotation. By utilizing LLMs in conjunction with\nhuman annotators, we significantly reduce the human annotation burden, enabling\nthe rapid creation of labeled datasets. We rigorously evaluate our method on a\nmedical information extraction task, demonstrating that our approach not only\nsubstantially cuts down on human intervention but also maintains high accuracy.\nThe results highlight the potential of using LLMs to improve the utilization of\nunstructured clinical data, allowing for the swift deployment of tailored NLP\nsolutions in healthcare.",
            "author": [
                "Akshay Goel",
                "Almog Gueta",
                "Omry Gilon",
                "Chang Liu",
                "Sofia Erell",
                "Lan Huong Nguyen",
                "Xiaohong Hao",
                "Bolous Jaber",
                "Shashir Reddy",
                "Rupesh Kartha",
                "Jean Steiner",
                "Itay Laish",
                "Amir Feder"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02296v1",
                "http://arxiv.org/pdf/2312.02296v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02295v1",
            "title": "Higher Memory Effects in Numerical Simulations of Binary Black Hole\n  Mergers",
            "updated": "2023-12-04T19:18:51Z",
            "published": "2023-12-04T19:18:51Z",
            "summary": "Gravitational memory effects are predictions of general relativity that are\ncharacterized by an observable effect that persists after the passage of\ngravitational waves. In recent years, they have garnered particular interest,\nboth due to their connection to asymptotic symmetries and soft theorems and\nbecause their observation would serve as a unique test of the nonlinear nature\nof general relativity. Apart from the more commonly known displacement and spin\nmemories, however, there are other memory effects predicted by Einstein's\nequations that are associated with more subleading terms in the asymptotic\nexpansion of the Bondi-Sachs metric. In this paper, we write explicit\nexpressions for these higher memory effects in terms of their charge and flux\ncontributions. Further, by using a numerical relativity simulation of a binary\nblack hole merger, we compute the magnitude and morphology of these terms and\ncompare them to those of the displacement and spin memory. We find that,\nalthough these terms are interesting from a theoretical perspective, due to\ntheir small magnitude they will be particularly challenging to observe with\ncurrent and future detectors.",
            "author": [
                "Alexander M. Grant",
                "Keefe Mitman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02295v1",
                "http://arxiv.org/pdf/2312.02295v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02290v1",
            "title": "You Can Run but not Hide: Improving Gait Recognition with Intrinsic\n  Occlusion Type Awareness",
            "updated": "2023-12-04T19:11:40Z",
            "published": "2023-12-04T19:11:40Z",
            "summary": "While gait recognition has seen many advances in recent years, the occlusion\nproblem has largely been ignored. This problem is especially important for gait\nrecognition from uncontrolled outdoor sequences at range - since any small\nobstruction can affect the recognition system. Most current methods assume the\navailability of complete body information while extracting the gait features.\nWhen parts of the body are occluded, these methods may hallucinate and output a\ncorrupted gait signature as they try to look for body parts which are not\npresent in the input at all. To address this, we exploit the learned occlusion\ntype while extracting identity features from videos. Thus, in this work, we\npropose an occlusion aware gait recognition method which can be used to model\nintrinsic occlusion awareness into potentially any state-of-the-art gait\nrecognition method. Our experiments on the challenging GREW and BRIAR datasets\nshow that networks enhanced with this occlusion awareness perform better at\nrecognition tasks than their counterparts trained on similar occlusions.",
            "author": [
                "Ayush Gupta",
                "Rama Chellappa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02290v1",
                "http://arxiv.org/pdf/2312.02290v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02284v1",
            "title": "PatchFusion: An End-to-End Tile-Based Framework for High-Resolution\n  Monocular Metric Depth Estimation",
            "updated": "2023-12-04T19:03:12Z",
            "published": "2023-12-04T19:03:12Z",
            "summary": "Single image depth estimation is a foundational task in computer vision and\ngenerative modeling. However, prevailing depth estimation models grapple with\naccommodating the increasing resolutions commonplace in today's consumer\ncameras and devices. Existing high-resolution strategies show promise, but they\noften face limitations, ranging from error propagation to the loss of\nhigh-frequency details. We present PatchFusion, a novel tile-based framework\nwith three key components to improve the current state of the art: (1) A\npatch-wise fusion network that fuses a globally-consistent coarse prediction\nwith finer, inconsistent tiled predictions via high-level feature guidance, (2)\nA Global-to-Local (G2L) module that adds vital context to the fusion network,\ndiscarding the need for patch selection heuristics, and (3) A Consistency-Aware\nTraining (CAT) and Inference (CAI) approach, emphasizing patch overlap\nconsistency and thereby eradicating the necessity for post-processing.\nExperiments on UnrealStereo4K, MVS-Synth, and Middleburry 2014 demonstrate that\nour framework can generate high-resolution depth maps with intricate details.\nPatchFusion is independent of the base model for depth estimation. Notably, our\nframework built on top of SOTA ZoeDepth brings improvements for a total of\n17.3% and 29.4% in terms of the root mean squared error (RMSE) on\nUnrealStereo4K and MVS-Synth, respectively.",
            "author": [
                "Zhenyu Li",
                "Shariq Farooq Bhat",
                "Peter Wonka"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02284v1",
                "http://arxiv.org/pdf/2312.02284v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03009v1",
            "title": "I-PHYRE: Interactive Physical Reasoning",
            "updated": "2023-12-04T19:01:19Z",
            "published": "2023-12-04T19:01:19Z",
            "summary": "Current evaluation protocols predominantly assess physical reasoning in\nstationary scenes, creating a gap in evaluating agents' abilities to interact\nwith dynamic events. While contemporary methods allow agents to modify initial\nscene configurations and observe consequences, they lack the capability to\ninteract with events in real time. To address this, we introduce I-PHYRE, a\nframework that challenges agents to simultaneously exhibit intuitive physical\nreasoning, multi-step planning, and in-situ intervention. Here, intuitive\nphysical reasoning refers to a quick, approximate understanding of physics to\naddress complex problems; multi-step denotes the need for extensive sequence\nplanning in I-PHYRE, considering each intervention can significantly alter\nsubsequent choices; and in-situ implies the necessity for timely object\nmanipulation within a scene, where minor timing deviations can result in task\nfailure. We formulate four game splits to scrutinize agents' learning and\ngeneralization of essential principles of interactive physical reasoning,\nfostering learning through interaction with representative scenarios. Our\nexploration involves three planning strategies and examines several supervised\nand reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The\noutcomes highlight a notable gap between existing learning algorithms and human\nperformance, emphasizing the imperative for more research in enhancing agents\nwith interactive physical reasoning capabilities. The environment and baselines\nwill be made publicly available.",
            "author": [
                "Shiqian Li",
                "Kewen Wu",
                "Chi Zhang",
                "Yixin Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03009v1",
                "http://arxiv.org/pdf/2312.03009v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CV",
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02281v1",
            "title": "Casimir Forces in CFT with Defects and Boundaries",
            "updated": "2023-12-04T19:00:57Z",
            "published": "2023-12-04T19:00:57Z",
            "summary": "We investigate the quantum forces occurring between the defects and/or\nboundaries of a conformal field theory (CFT). We propose to model imperfect\ndefects and boundaries as localized relevant double-trace operators that deform\nthe CFT. Our focus is on pointlike and codimension-one planar defects. In the\ncase of two parallel membranes, we point out that the CFT 2-point function\ntends to get confined and develops a tower of resonances with constant decay\nrate when the operator dimension approaches the free field dimension. Using a\nfunctional formalism, we compute the quantum forces induced by the CFT between\na variety of configurations of pointlike defects, infinite plates and\nmembranes. Consistency arguments imply that these quantum forces are attractive\nat any distance. Forces of Casimir-Polder type appear in the UV, while forces\nof Casimir type appear in the IR, in which case the CFT gets repelled from the\ndefects. Most of the forces behave as a non-integer power of the separation,\ncontrolled by the dimension of the double-trace deformation. In the Casimir\nregime of the membrane-membrane configuration, the quantum pressure behaves\nuniversally as $1/\\ell^d$, however information about the double-trace nature of\nthe defects still remains encoded in the strength of the pressure.",
            "author": [
                "Philippe Brax",
                "Sylvain Fichet"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02281v1",
                "http://arxiv.org/pdf/2312.02281v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02279v1",
            "title": "Quantum Optimization: Potential, Challenges, and the Path Forward",
            "updated": "2023-12-04T19:00:44Z",
            "published": "2023-12-04T19:00:44Z",
            "summary": "Recent advances in quantum computers are demonstrating the ability to solve\nproblems at a scale beyond brute force classical simulation. As such, a\nwidespread interest in quantum algorithms has developed in many areas, with\noptimization being one of the most pronounced domains. Across computer science\nand physics, there are a number of algorithmic approaches, often with little\nlinkage. This is further complicated by the fragmented nature of the field of\nmathematical optimization, where major classes of optimization problems, such\nas combinatorial optimization, convex optimization, non-convex optimization,\nand stochastic extensions, have devoted communities. With these aspects in\nmind, this work draws on multiple approaches to study quantum optimization.\nProvably exact versus heuristic settings are first explained using\ncomputational complexity theory - highlighting where quantum advantage is\npossible in each context. Then, the core building blocks for quantum\noptimization algorithms are outlined to subsequently define prominent problem\nclasses and identify key open questions that, if answered, will advance the\nfield. The effects of scaling relevant problems on noisy quantum devices are\nalso outlined in detail, alongside meaningful benchmarking problems. We\nunderscore the importance of benchmarking by proposing clear metrics to conduct\nappropriate comparisons with classical optimization techniques. Lastly, we\nhighlight two domains - finance and sustainability - as rich sources of\noptimization problems that could be used to benchmark, and eventually validate,\nthe potential real-world impact of quantum optimization.",
            "author": [
                "Amira Abbas",
                "Andris Ambainis",
                "Brandon Augustino",
                "Andreas B\u00e4rtschi",
                "Harry Buhrman",
                "Carleton Coffrin",
                "Giorgio Cortiana",
                "Vedran Dunjko",
                "Daniel J. Egger",
                "Bruce G. Elmegreen",
                "Nicola Franco",
                "Filippo Fratini",
                "Bryce Fuller",
                "Julien Gacon",
                "Constantin Gonciulea",
                "Sander Gribling",
                "Swati Gupta",
                "Stuart Hadfield",
                "Raoul Heese",
                "Gerhard Kircher",
                "Thomas Kleinert",
                "Thorsten Koch",
                "Georgios Korpas",
                "Steve Lenk",
                "Jakub Marecek",
                "Vanio Markov",
                "Guglielmo Mazzola",
                "Stefano Mensa",
                "Naeimeh Mohseni",
                "Giacomo Nannicini",
                "Corey O'Meara",
                "Elena Pe\u00f1a Tapia",
                "Sebastian Pokutta",
                "Manuel Proissl",
                "Patrick Rebentrost",
                "Emre Sahin",
                "Benjamin C. B. Symons",
                "Sabine Tornow",
                "Victor Valls",
                "Stefan Woerner",
                "Mira L. Wolf-Bauwens",
                "Jon Yard",
                "Sheir Yarkoni",
                "Dirk Zechiel",
                "Sergiy Zhuk",
                "Christa Zoufal"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02279v1",
                "http://arxiv.org/pdf/2312.02279v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02272v1",
            "title": "Entanglement production from scattering of fermionic wave packets: a\n  quantum computing approach",
            "updated": "2023-12-04T19:00:04Z",
            "published": "2023-12-04T19:00:04Z",
            "summary": "We propose a method to prepare Gaussian wave packets with momentum on top of\nthe interacting ground state of a fermionic Hamiltonian. Using Givens rotation,\nwe show how to efficiently obtain expectation values of observables throughout\nthe evolution of the wave packets on digital quantum computers. We demonstrate\nour technique by applying it to the staggered lattice formulation of the\nThirring model and studying the scattering of two wave packets. Monitoring the\nthe particle density and the entropy produced during the scattering process, we\ncharacterize the phenomenon and provide a first step towards studying more\ncomplicated collision processes on digital quantum computers. In addition, we\nperform a small-scale demonstration on IBM's quantum hardware, showing that our\nmethod is suitable for current and near-term quantum devices.",
            "author": [
                "Yahui Chai",
                "Arianna Crippa",
                "Karl Jansen",
                "Stefan K\u00fchn",
                "Vincent R. Pascuzzi",
                "Francesco Tacchino",
                "Ivano Tavernelli"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02272v1",
                "http://arxiv.org/pdf/2312.02272v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "hep-lat"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02268v1",
            "title": "Generating synthetic star catalogs from simulated data for next-gen\n  observatories with py-ananke",
            "updated": "2023-12-04T19:00:02Z",
            "published": "2023-12-04T19:00:02Z",
            "summary": "We find ourselves on the brink of an exciting era in observational\nastrophysics, driven by groundbreaking facilities like JWST, Euclid, Rubin,\nRoman, SKA, or ELT. Simultaneously, computational astrophysics has shown\nsignificant strides, yielding highly realistic galaxy formation simulations,\nthanks to both hardware and software enhancements. Bridging the gap between\nsimulations and observations has become paramount for meaningful comparisons.\nWe introduce py-ananke, a Python pipeline designed to generate synthetic\nresolved stellar surveys from cosmological simulations, adaptable to various\ninstruments. Building upon its predecessor, ananke by Sanderson et al. 2020\n(arXiv:1806.10564), which produced Gaia DR2 mock star surveys, the py-ananke\npackage offers a user-friendly \"plug & play\" experience. The pipeline employs\ncutting-edge phase-space density estimation and initial mass function sampling\nto convert particle data into synthetic stars, while interpolating pre-computed\nstellar isochrone tracks for photometry. Additionally, it includes modules for\nestimating interstellar reddening, dust-induced extinctions, and for\nquantifying errors through dedicated modeling approaches. py-ananke promises to\nserve as a vital bridge between computational astrophysics and observational\nastronomy, facilitating preparations and making scientific predictions for the\nnext generation of telescopes.",
            "author": [
                "Adrien C. R. Thob",
                "Robyn E. Sanderson",
                "Andrew P. Eden",
                "Farnik Nikakhtar",
                "Nondh Panithanpaisal",
                "Nicol\u00e1s Garavito-Camargo",
                "Sanjib Sharma"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02268v1",
                "http://arxiv.org/pdf/2312.02268v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02265v1",
            "title": "Programmable Simulations of Molecules and Materials with Reconfigurable\n  Quantum Processors",
            "updated": "2023-12-04T19:00:01Z",
            "published": "2023-12-04T19:00:01Z",
            "summary": "Simulations of quantum chemistry and quantum materials are believed to be\namong the most important potential applications of quantum information\nprocessors, but realizing practical quantum advantage for such problems is\nchallenging. Here, we introduce a simulation framework for strongly correlated\nquantum systems that can be represented by model spin Hamiltonians. Our\napproach leverages reconfigurable qubit architectures to programmably simulate\nreal-time dynamics and introduces an algorithm for extracting chemically\nrelevant spectral properties via classical co-processing of quantum measurement\nresults. We develop a digital-analog simulation toolbox for efficient\nHamiltonian time evolution utilizing digital Floquet engineering and\nhardware-optimized multi-qubit operations to accurately realize complex\nspin-spin interactions, and as an example present an implementation proposal\nbased on Rydberg atom arrays. Then, we show how detailed spectral information\ncan be extracted from these dynamics through snapshot measurements and\nsingle-ancilla control, enabling the evaluation of excitation energies and\nfinite-temperature susceptibilities from a single-dataset. To illustrate the\napproach, we show how this method can be used to compute key properties of a\npolynuclear transition-metal catalyst and 2D magnetic materials.",
            "author": [
                "Nishad Maskara",
                "Stefan Ostermann",
                "James Shee",
                "Marcin Kalinowski",
                "Abigail McClain Gomez",
                "Rodrigo Araiza Bravo",
                "Derek S. Wang",
                "Anna I. Krylov",
                "Norman Y. Yao",
                "Martin Head-Gordon",
                "Mikhail D. Lukin",
                "Susanne F. Yelin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02265v1",
                "http://arxiv.org/pdf/2312.02265v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.mtrl-sci",
                "cond-mat.str-el",
                "physics.atom-ph",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02261v1",
            "title": "High Energy String Scattering in AdS",
            "updated": "2023-12-04T19:00:00Z",
            "published": "2023-12-04T19:00:00Z",
            "summary": "We study the AdS Virasoro-Shapiro amplitude in the limit of fixed-angle high\nenergy scattering. A recent representation as a world-sheet integral allows to\ncompute the amplitude in this regime by saddle point techniques, very much as\nin flat space. This result is then compared to a classical scattering\ncomputation in AdS and agreement is found. As a byproduct of this comparison we\nshow that AdS curvature corrections exponentiate in the high energy limit.",
            "author": [
                "Luis F. Alday",
                "Tobias Hansen",
                "Maria Nocchi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02261v1",
                "http://arxiv.org/pdf/2312.02261v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02158v1",
            "title": "PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness",
            "updated": "2023-12-04T18:59:59Z",
            "published": "2023-12-04T18:59:59Z",
            "summary": "We propose the task of Panoptic Scene Completion (PSC) which extends the\nrecently popular Semantic Scene Completion (SSC) task with instance-level\ninformation to produce a richer understanding of the 3D scene. Our PSC proposal\nutilizes a hybrid mask-based technique on the non-empty voxels from sparse\nmulti-scale completions. Whereas the SSC literature overlooks uncertainty which\nis critical for robotics applications, we instead propose an efficient\nensembling to estimate both voxel-wise and instance-wise uncertainties along\nPSC. This is achieved by building on a multi-input multi-output (MIMO)\nstrategy, while improving performance and yielding better uncertainty for\nlittle additional compute. Additionally, we introduce a technique to aggregate\npermutation-invariant mask predictions. Our experiments demonstrate that our\nmethod surpasses all baselines in both Panoptic Scene Completion and\nuncertainty estimation on three large-scale autonomous driving datasets. Our\ncode and data are available at https://astra-vision.github.io/PaSCo .",
            "author": [
                "Anh-Quan Cao",
                "Angela Dai",
                "Raoul de Charette"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02158v1",
                "http://arxiv.org/pdf/2312.02158v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02157v1",
            "title": "Mesh-Guided Neural Implicit Field Editing",
            "updated": "2023-12-04T18:59:58Z",
            "published": "2023-12-04T18:59:58Z",
            "summary": "Neural implicit fields have emerged as a powerful 3D representation for\nreconstructing and rendering photo-realistic views, yet they possess limited\neditability. Conversely, explicit 3D representations, such as polygonal meshes,\noffer ease of editing but may not be as suitable for rendering high-quality\nnovel views. To harness the strengths of both representations, we propose a new\napproach that employs a mesh as a guiding mechanism in editing the neural\nradiance field. We first introduce a differentiable method using marching\ntetrahedra for polygonal mesh extraction from the neural implicit field and\nthen design a differentiable color extractor to assign colors obtained from the\nvolume renderings to this extracted mesh. This differentiable colored mesh\nallows gradient back-propagation from the explicit mesh to the implicit fields,\nempowering users to easily manipulate the geometry and color of neural implicit\nfields. To enhance user control from coarse-grained to fine-grained levels, we\nintroduce an octree-based structure into its optimization. This structure\nprioritizes the edited regions and the surface part, making our method achieve\nfine-grained edits to the neural implicit field and accommodate various user\nmodifications, including object additions, component removals, specific area\ndeformations, and adjustments to local and global colors. Through extensive\nexperiments involving diverse scenes and editing operations, we have\ndemonstrated the capabilities and effectiveness of our method. Our project page\nis: \\url{https://cassiepython.github.io/MNeuEdit/}",
            "author": [
                "Can Wang",
                "Mingming He",
                "Menglei Chai",
                "Dongdong Chen",
                "Jing Liao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02157v1",
                "http://arxiv.org/pdf/2312.02157v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02155v1",
            "title": "GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for\n  Real-time Human Novel View Synthesis",
            "updated": "2023-12-04T18:59:55Z",
            "published": "2023-12-04T18:59:55Z",
            "summary": "We present a new approach, termed GPS-Gaussian, for synthesizing novel views\nof a character in a real-time manner. The proposed method enables 2K-resolution\nrendering under a sparse-view camera setting. Unlike the original Gaussian\nSplatting or neural implicit rendering methods that necessitate per-subject\noptimizations, we introduce Gaussian parameter maps defined on the source views\nand regress directly Gaussian Splatting properties for instant novel view\nsynthesis without any fine-tuning or optimization. To this end, we train our\nGaussian parameter regression module on a large amount of human scan data,\njointly with a depth estimation module to lift 2D parameter maps to 3D space.\nThe proposed framework is fully differentiable and experiments on several\ndatasets demonstrate that our method outperforms state-of-the-art methods while\nachieving an exceeding rendering speed.",
            "author": [
                "Shunyuan Zheng",
                "Boyao Zhou",
                "Ruizhi Shao",
                "Boning Liu",
                "Shengping Zhang",
                "Liqiang Nie",
                "Yebin Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02155v1",
                "http://arxiv.org/pdf/2312.02155v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02156v1",
            "title": "Latent Feature-Guided Diffusion Models for Shadow Removal",
            "updated": "2023-12-04T18:59:55Z",
            "published": "2023-12-04T18:59:55Z",
            "summary": "Recovering textures under shadows has remained a challenging problem due to\nthe difficulty of inferring shadow-free scenes from shadow images. In this\npaper, we propose the use of diffusion models as they offer a promising\napproach to gradually refine the details of shadow regions during the diffusion\nprocess. Our method improves this process by conditioning on a learned latent\nfeature space that inherits the characteristics of shadow-free images, thus\navoiding the limitation of conventional methods that condition on degraded\nimages only. Additionally, we propose to alleviate potential local optima\nduring training by fusing noise features with the diffusion network. We\ndemonstrate the effectiveness of our approach which outperforms the previous\nbest method by 13% in terms of RMSE on the AISTD dataset. Further, we explore\ninstance-level shadow removal, where our model outperforms the previous best\nmethod by 82% in terms of RMSE on the DESOBA dataset.",
            "author": [
                "Kangfu Mei",
                "Luis Figueroa",
                "Zhe Lin",
                "Zhihong Ding",
                "Scott Cohen",
                "Vishal M. Patel"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02156v1",
                "http://arxiv.org/pdf/2312.02156v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02153v1",
            "title": "Aligning and Prompting Everything All at Once for Universal Visual\n  Perception",
            "updated": "2023-12-04T18:59:50Z",
            "published": "2023-12-04T18:59:50Z",
            "summary": "Vision foundation models have been explored recently to build general-purpose\nvision systems. However, predominant paradigms, driven by casting\ninstance-level tasks as an object-word alignment, bring heavy cross-modality\ninteraction, which is not effective in prompting object detection and visual\ngrounding. Another line of work that focuses on pixel-level tasks often\nencounters a large annotation gap of things and stuff, and suffers from mutual\ninterference between foreground-object and background-class segmentation. In\nstark contrast to the prevailing methods, we present APE, a universal visual\nperception model for aligning and prompting everything all at once in an image\nto perform diverse tasks, i.e., detection, segmentation, and grounding, as an\ninstance-level sentence-object matching paradigm. Specifically, APE advances\nthe convergence of detection and grounding by reformulating language-guided\ngrounding as open-vocabulary detection, which efficiently scales up model\nprompting to thousands of category vocabularies and region descriptions while\nmaintaining the effectiveness of cross-modality fusion. To bridge the\ngranularity gap of different pixel-level tasks, APE equalizes semantic and\npanoptic segmentation to proxy instance learning by considering any isolated\nregions as individual instances. APE aligns vision and language representation\non broad data with natural and challenging characteristics all at once without\ntask-specific fine-tuning. The extensive experiments on over 160 datasets\ndemonstrate that, with only one-suit of weights, APE outperforms (or is on par\nwith) the state-of-the-art models, proving that an effective yet universal\nperception for anything aligning and prompting is indeed feasible. Codes and\ntrained models are released at https://github.com/shenyunhang/APE.",
            "author": [
                "Yunhang Shen",
                "Chaoyou Fu",
                "Peixian Chen",
                "Mengdan Zhang",
                "Ke Li",
                "Xing Sun",
                "Yunsheng Wu",
                "Shaohui Lin",
                "Rongrong Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02153v1",
                "http://arxiv.org/pdf/2312.02153v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02152v1",
            "title": "Steerers: A framework for rotation equivariant keypoint descriptors",
            "updated": "2023-12-04T18:59:44Z",
            "published": "2023-12-04T18:59:44Z",
            "summary": "Image keypoint descriptions that are discriminative and matchable over large\nchanges in viewpoint are vital for 3D reconstruction. However, descriptions\noutput by learned descriptors are typically not robust to camera rotation.\nWhile they can be made more robust by, e.g., data augmentation, this degrades\nperformance on upright images. Another approach is test-time augmentation,\nwhich incurs a significant increase in runtime. We instead learn a linear\ntransform in description space that encodes rotations of the input image. We\ncall this linear transform a steerer since it allows us to transform the\ndescriptions as if the image was rotated. From representation theory we know\nall possible steerers for the rotation group. Steerers can be optimized (A)\ngiven a fixed descriptor, (B) jointly with a descriptor or (C) we can optimize\na descriptor given a fixed steerer. We perform experiments in all of these\nthree settings and obtain state-of-the-art results on the rotation invariant\nimage matching benchmarks AIMS and Roto-360. We publish code and model weights\nat github.com/georg-bn/rotation-steerers.",
            "author": [
                "Georg B\u00f6kman",
                "Johan Edstedt",
                "Michael Felsberg",
                "Fredrik Kahl"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02152v1",
                "http://arxiv.org/pdf/2312.02152v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02151v1",
            "title": "Guarding Barlow Twins Against Overfitting with Mixed Samples",
            "updated": "2023-12-04T18:59:36Z",
            "published": "2023-12-04T18:59:36Z",
            "summary": "Self-supervised Learning (SSL) aims to learn transferable feature\nrepresentations for downstream applications without relying on labeled data.\nThe Barlow Twins algorithm, renowned for its widespread adoption and\nstraightforward implementation compared to its counterparts like contrastive\nlearning methods, minimizes feature redundancy while maximizing invariance to\ncommon corruptions. Optimizing for the above objective forces the network to\nlearn useful representations, while avoiding noisy or constant features,\nresulting in improved downstream task performance with limited adaptation.\nDespite Barlow Twins' proven effectiveness in pre-training, the underlying SSL\nobjective can inadvertently cause feature overfitting due to the lack of strong\ninteraction between the samples unlike the contrastive learning approaches.\nFrom our experiments, we observe that optimizing for the Barlow Twins objective\ndoesn't necessarily guarantee sustained improvements in representation quality\nbeyond a certain pre-training phase, and can potentially degrade downstream\nperformance on some datasets. To address this challenge, we introduce Mixed\nBarlow Twins, which aims to improve sample interaction during Barlow Twins\ntraining via linearly interpolated samples. This results in an additional\nregularization term to the original Barlow Twins objective, assuming linear\ninterpolation in the input space translates to linearly interpolated features\nin the feature space. Pre-training with this regularization effectively\nmitigates feature overfitting and further enhances the downstream performance\non CIFAR-10, CIFAR-100, TinyImageNet, STL-10, and ImageNet datasets. The code\nand checkpoints are available at: https://github.com/wgcban/mix-bt.git",
            "author": [
                "Wele Gedara Chaminda Bandara",
                "Celso M. De Melo",
                "Vishal M. Patel"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02151v1",
                "http://arxiv.org/pdf/2312.02151v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02150v1",
            "title": "Readout Guidance: Learning Control from Diffusion Features",
            "updated": "2023-12-04T18:59:32Z",
            "published": "2023-12-04T18:59:32Z",
            "summary": "We present Readout Guidance, a method for controlling text-to-image diffusion\nmodels with learned signals. Readout Guidance uses readout heads, lightweight\nnetworks trained to extract signals from the features of a pre-trained, frozen\ndiffusion model at every timestep. These readouts can encode single-image\nproperties, such as pose, depth, and edges; or higher-order properties that\nrelate multiple images, such as correspondence and appearance similarity.\nFurthermore, by comparing the readout estimates to a user-defined target, and\nback-propagating the gradient through the readout head, these estimates can be\nused to guide the sampling process. Compared to prior methods for conditional\ngeneration, Readout Guidance requires significantly fewer added parameters and\ntraining samples, and offers a convenient and simple recipe for reproducing\ndifferent forms of conditional control under a single framework, with a single\narchitecture and sampling procedure. We showcase these benefits in the\napplications of drag-based manipulation, identity-consistent generation, and\nspatially aligned control. Project page: https://readout-guidance.github.io.",
            "author": [
                "Grace Luo",
                "Trevor Darrell",
                "Oliver Wang",
                "Dan B Goldman",
                "Aleksander Holynski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02150v1",
                "http://arxiv.org/pdf/2312.02150v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02149v1",
            "title": "Generative Powers of Ten",
            "updated": "2023-12-04T18:59:25Z",
            "published": "2023-12-04T18:59:25Z",
            "summary": "We present a method that uses a text-to-image model to generate consistent\ncontent across multiple image scales, enabling extreme semantic zooms into a\nscene, e.g., ranging from a wide-angle landscape view of a forest to a macro\nshot of an insect sitting on one of the tree branches. We achieve this through\na joint multi-scale diffusion sampling approach that encourages consistency\nacross different scales while preserving the integrity of each individual\nsampling process. Since each generated scale is guided by a different text\nprompt, our method enables deeper levels of zoom than traditional\nsuper-resolution methods that may struggle to create new contextual structure\nat vastly different scales. We compare our method qualitatively with\nalternative techniques in image super-resolution and outpainting, and show that\nour method is most effective at generating consistent multi-scale content.",
            "author": [
                "Xiaojuan Wang",
                "Janne Kontkanen",
                "Brian Curless",
                "Steve Seitz",
                "Ira Kemelmacher",
                "Ben Mildenhall",
                "Pratul Srinivasan",
                "Dor Verbin",
                "Aleksander Holynski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02149v1",
                "http://arxiv.org/pdf/2312.02149v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02147v1",
            "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
            "updated": "2023-12-04T18:59:20Z",
            "published": "2023-12-04T18:59:20Z",
            "summary": "This paper enhances image-GPT (iGPT), one of the pioneering works that\nintroduce autoregressive pretraining to predict next pixels for visual\nrepresentation learning. Two simple yet essential changes are made. First, we\nshift the prediction target from raw pixels to semantic tokens, enabling a\nhigher-level understanding of visual content. Second, we supplement the\nautoregressive modeling by instructing the model to predict not only the next\ntokens but also the visible tokens. This pipeline is particularly effective\nwhen semantic tokens are encoded by discriminatively trained models, such as\nCLIP. We introduce this novel approach as D-iGPT. Extensive experiments\nshowcase that D-iGPT excels as a strong learner of visual representations: A\nnotable achievement of D-iGPT is its compelling performance on the ImageNet-1K\ndataset -- by training on publicly available datasets, D-iGPT achieves 89.5\\%\ntop-1 accuracy with a vanilla ViT-Large model. This model also shows strong\ngeneralization on the downstream task and robustness on out-of-distribution\nsamples. Code is avaiable at\n\\href{https://github.com/OliverRensu/D-iGPT}{https://github.com/OliverRensu/D-iGPT}.",
            "author": [
                "Sucheng Ren",
                "Zeyu Wang",
                "Hongru Zhu",
                "Junfei Xiao",
                "Alan Yuille",
                "Cihang Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02147v1",
                "http://arxiv.org/pdf/2312.02147v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02145v1",
            "title": "Repurposing Diffusion-Based Image Generators for Monocular Depth\n  Estimation",
            "updated": "2023-12-04T18:59:13Z",
            "published": "2023-12-04T18:59:13Z",
            "summary": "Monocular depth estimation is a fundamental computer vision task. Recovering\n3D depth from a single image is geometrically ill-posed and requires scene\nunderstanding, so it is not surprising that the rise of deep learning has led\nto a breakthrough. The impressive progress of monocular depth estimators has\nmirrored the growth in model capacity, from relatively modest CNNs to large\nTransformer architectures. Still, monocular depth estimators tend to struggle\nwhen presented with images with unfamiliar content and layout, since their\nknowledge of the visual world is restricted by the data seen during training,\nand challenged by zero-shot generalization to new domains. This motivates us to\nexplore whether the extensive priors captured in recent generative diffusion\nmodels can enable better, more generalizable depth estimation. We introduce\nMarigold, a method for affine-invariant monocular depth estimation that is\nderived from Stable Diffusion and retains its rich prior knowledge. The\nestimator can be fine-tuned in a couple of days on a single GPU using only\nsynthetic training data. It delivers state-of-the-art performance across a wide\nrange of datasets, including over 20% performance gains in specific cases.\nProject page: https://marigoldmonodepth.github.io.",
            "author": [
                "Bingxin Ke",
                "Anton Obukhov",
                "Shengyu Huang",
                "Nando Metzger",
                "Rodrigo Caye Daudt",
                "Konrad Schindler"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02145v1",
                "http://arxiv.org/pdf/2312.02145v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02144v1",
            "title": "Optimizing Camera Configurations for Multi-View Pedestrian Detection",
            "updated": "2023-12-04T18:59:02Z",
            "published": "2023-12-04T18:59:02Z",
            "summary": "Jointly considering multiple camera views (multi-view) is very effective for\npedestrian detection under occlusion. For such multi-view systems, it is\ncritical to have well-designed camera configurations, including camera\nlocations, directions, and fields-of-view (FoVs). Usually, these configurations\nare crafted based on human experience or heuristics. In this work, we present a\nnovel solution that features a transformer-based camera configuration\ngenerator. Using reinforcement learning, this generator autonomously explores\nvast combinations within the action space and searches for configurations that\ngive the highest detection accuracy according to the training dataset. The\ngenerator learns advanced techniques like maximizing coverage, minimizing\nocclusion, and promoting collaboration. Across multiple simulation scenarios,\nthe configurations generated by our transformer-based model consistently\noutperform random search, heuristic-based methods, and configurations designed\nby human experts, shedding light on future camera layout optimization.",
            "author": [
                "Yunzhong Hou",
                "Xingjian Leng",
                "Tom Gedeon",
                "Liang Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02144v1",
                "http://arxiv.org/pdf/2312.02144v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02143v2",
            "title": "Competition-Level Problems are Effective LLM Evaluators",
            "updated": "2023-12-05T03:44:19Z",
            "published": "2023-12-04T18:58:57Z",
            "summary": "Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities, yet there is ongoing debate about these abilities and the\npotential data contamination problem recently. This paper aims to evaluate the\nreasoning capacities of LLMs, specifically in solving recent competition-level\nprogramming problems in Codeforces, which are expert-crafted and unique,\nrequiring deep understanding and robust reasoning skills. We first provide a\ncomprehensive evaluation of GPT-4's peiceived zero-shot performance on this\ntask, considering various aspects such as problems' release time, difficulties,\nand types of errors encountered. Surprisingly, the peiceived performance of\nGPT-4 has experienced a cliff like decline in problems after September 2021\nconsistently across all the difficulties and types of problems, which shows the\npotential data contamination, as well as the challenges for any existing LLM to\nsolve unseen complex reasoning problems. We further explore various approaches\nsuch as fine-tuning, Chain-of-Thought prompting and problem description\nsimplification, unfortunately none of them is able to consistently mitigate the\nchallenges. Through our work, we emphasis the importance of this excellent data\nsource for assessing the genuine reasoning capabilities of LLMs, and foster the\ndevelopment of LLMs with stronger reasoning abilities and better generalization\nin the future.",
            "author": [
                "Yiming Huang",
                "Zhenghao Lin",
                "Xiao Liu",
                "Yeyun Gong",
                "Shuai Lu",
                "Fangyu Lei",
                "Yaobo Liang",
                "Yelong Shen",
                "Chen Lin",
                "Nan Duan",
                "Weizhu Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02143v2",
                "http://arxiv.org/pdf/2312.02143v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02142v1",
            "title": "Object Recognition as Next Token Prediction",
            "updated": "2023-12-04T18:58:40Z",
            "published": "2023-12-04T18:58:40Z",
            "summary": "We present an approach to pose object recognition as next token prediction.\nThe idea is to apply a language decoder that auto-regressively predicts the\ntext tokens from image embeddings to form labels. To ground this prediction\nprocess in auto-regression, we customize a non-causal attention mask for the\ndecoder, incorporating two key features: modeling tokens from different labels\nto be independent, and treating image tokens as a prefix. This masking\nmechanism inspires an efficient method - one-shot sampling - to simultaneously\nsample tokens of multiple labels in parallel and rank generated labels by their\nprobabilities during inference. To further enhance the efficiency, we propose a\nsimple strategy to construct a compact decoder by simply discarding the\nintermediate blocks of a pretrained language model. This approach yields a\ndecoder that matches the full model's performance while being notably more\nefficient. The code is available at https://github.com/kaiyuyue/nxtp",
            "author": [
                "Kaiyu Yue",
                "Bor-Chun Chen",
                "Jonas Geiping",
                "Hengduo Li",
                "Tom Goldstein",
                "Ser-Nam Lim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02142v1",
                "http://arxiv.org/pdf/2312.02142v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02256v1",
            "title": "EMDM: Efficient Motion Diffusion Model for Fast, High-Quality Motion\n  Generation",
            "updated": "2023-12-04T18:58:38Z",
            "published": "2023-12-04T18:58:38Z",
            "summary": "We introduce Efficient Motion Diffusion Model (EMDM) for fast and\nhigh-quality human motion generation. Although previous motion diffusion models\nhave shown impressive results, they struggle to achieve fast generation while\nmaintaining high-quality human motions. Motion latent diffusion has been\nproposed for efficient motion generation. However, effectively learning a\nlatent space can be non-trivial in such a two-stage manner. Meanwhile,\naccelerating motion sampling by increasing the step size, e.g., DDIM, typically\nleads to a decline in motion quality due to the inapproximation of complex data\ndistributions when naively increasing the step size. In this paper, we propose\nEMDM that allows for much fewer sample steps for fast motion generation by\nmodeling the complex denoising distribution during multiple sampling steps.\nSpecifically, we develop a Conditional Denoising Diffusion GAN to capture\nmultimodal data distributions conditioned on both control signals, i.e.,\ntextual description and denoising time step. By modeling the complex data\ndistribution, a larger sampling step size and fewer steps are achieved during\nmotion synthesis, significantly accelerating the generation process. To\neffectively capture the human dynamics and reduce undesired artifacts, we\nemploy motion geometric loss during network training, which improves the motion\nquality and training efficiency. As a result, EMDM achieves a remarkable\nspeed-up at the generation stage while maintaining high-quality motion\ngeneration in terms of fidelity and diversity.",
            "author": [
                "Wenyang Zhou",
                "Zhiyang Dou",
                "Zeyu Cao",
                "Zhouyingcheng Liao",
                "Jingbo Wang",
                "Wenjia Wang",
                "Yuan Liu",
                "Taku Komura",
                "Wenping Wang",
                "Lingjie Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02256v1",
                "http://arxiv.org/pdf/2312.02256v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02141v1",
            "title": "iMatching: Imperative Correspondence Learning",
            "updated": "2023-12-04T18:58:20Z",
            "published": "2023-12-04T18:58:20Z",
            "summary": "Learning feature correspondence is a foundational task in computer vision,\nholding immense importance for downstream applications such as visual odometry\nand 3D reconstruction. Despite recent progress in data-driven models, feature\ncorrespondence learning is still limited by the lack of accurate per-pixel\ncorrespondence labels. To overcome this difficulty, we introduce a new\nself-supervised scheme, imperative learning (IL), for training feature\ncorrespondence. It enables correspondence learning on arbitrary uninterrupted\nvideos without any camera pose or depth labels, heralding a new era for\nself-supervised correspondence learning. Specifically, we formulated the\nproblem of correspondence learning as a bilevel optimization, which takes the\nreprojection error from bundle adjustment as a supervisory signal for the\nmodel. To avoid large memory and computation overhead, we leverage the\nstationary point to effectively back-propagate the implicit gradients through\nbundle adjustment. Through extensive experiments, we demonstrate superior\nperformance on tasks including feature matching and pose estimation, in which\nwe obtained an average of 30% accuracy gain over the state-of-the-art matching\nmodels.",
            "author": [
                "Zitong Zhan",
                "Dasong Gao",
                "Yun-Jou Lin",
                "Youjie Xia",
                "Chen Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02141v1",
                "http://arxiv.org/pdf/2312.02141v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02139v1",
            "title": "DiffiT: Diffusion Vision Transformers for Image Generation",
            "updated": "2023-12-04T18:57:01Z",
            "published": "2023-12-04T18:57:01Z",
            "summary": "Diffusion models with their powerful expressivity and high sample quality\nhave enabled many new applications and use-cases in various domains. For sample\ngeneration, these models rely on a denoising neural network that generates\nimages by iterative denoising. Yet, the role of denoising network architecture\nis not well-studied with most efforts relying on convolutional residual U-Nets.\nIn this paper, we study the effectiveness of vision transformers in\ndiffusion-based generative learning. Specifically, we propose a new model,\ndenoted as Diffusion Vision Transformers (DiffiT), which consists of a hybrid\nhierarchical architecture with a U-shaped encoder and decoder. We introduce a\nnovel time-dependent self-attention module that allows attention layers to\nadapt their behavior at different stages of the denoising process in an\nefficient manner. We also introduce latent DiffiT which consists of transformer\nmodel with the proposed self-attention layers, for high-resolution image\ngeneration. Our results show that DiffiT is surprisingly effective in\ngenerating high-fidelity images, and it achieves state-of-the-art (SOTA)\nbenchmarks on a variety of class-conditional and unconditional synthesis tasks.\nIn the latent space, DiffiT achieves a new SOTA FID score of 1.73 on\nImageNet-256 dataset. Repository: https://github.com/NVlabs/DiffiT",
            "author": [
                "Ali Hatamizadeh",
                "Jiaming Song",
                "Guilin Liu",
                "Jan Kautz",
                "Arash Vahdat"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02139v1",
                "http://arxiv.org/pdf/2312.02139v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02137v1",
            "title": "MANUS: Markerless Hand-Object Grasp Capture using Articulated 3D\n  Gaussians",
            "updated": "2023-12-04T18:56:22Z",
            "published": "2023-12-04T18:56:22Z",
            "summary": "Understanding how we grasp objects with our hands has important applications\nin areas like robotics and mixed reality. However, this challenging problem\nrequires accurate modeling of the contact between hands and objects. To capture\ngrasps, existing methods use skeletons, meshes, or parametric models that can\ncause misalignments resulting in inaccurate contacts. We present MANUS, a\nmethod for Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians.\nWe build a novel articulated 3D Gaussians representation that extends 3D\nGaussian splatting for high-fidelity representation of articulating hands.\nSince our representation uses Gaussian primitives, it enables us to efficiently\nand accurately estimate contacts between the hand and the object. For the most\naccurate results, our method requires tens of camera views that current\ndatasets do not provide. We therefore build MANUS-Grasps, a new dataset that\ncontains hand-object grasps viewed from 53 cameras across 30+ scenes, 3\nsubjects, and comprising over 7M frames. In addition to extensive qualitative\nresults, we also show that our method outperforms others on a quantitative\ncontact evaluation method that uses paint transfer from the object to the hand.",
            "author": [
                "Chandradeep Pokhariya",
                "Ishaan N Shah",
                "Angela Xing",
                "Zekun Li",
                "Kefan Chen",
                "Avinash Sharma",
                "Srinath Sridhar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02137v1",
                "http://arxiv.org/pdf/2312.02137v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02136v1",
            "title": "BerfScene: Bev-conditioned Equivariant Radiance Fields for Infinite 3D\n  Scene Generation",
            "updated": "2023-12-04T18:56:10Z",
            "published": "2023-12-04T18:56:10Z",
            "summary": "Generating large-scale 3D scenes cannot simply apply existing 3D object\nsynthesis technique since 3D scenes usually hold complex spatial configurations\nand consist of a number of objects at varying scales. We thus propose a\npractical and efficient 3D representation that incorporates an equivariant\nradiance field with the guidance of a bird's-eye view (BEV) map. Concretely,\nobjects of synthesized 3D scenes could be easily manipulated through steering\nthe corresponding BEV maps. Moreover, by adequately incorporating positional\nencoding and low-pass filters into the generator, the representation becomes\nequivariant to the given BEV map. Such equivariance allows us to produce\nlarge-scale, even infinite-scale, 3D scenes via synthesizing local scenes and\nthen stitching them with smooth consistency. Extensive experiments on 3D scene\ndatasets demonstrate the effectiveness of our approach. Our project website is\nat https://zqh0253.github.io/BerfScene/.",
            "author": [
                "Qihang Zhang",
                "Yinghao Xu",
                "Yujun Shen",
                "Bo Dai",
                "Bolei Zhou",
                "Ceyuan Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02136v1",
                "http://arxiv.org/pdf/2312.02136v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02255v1",
            "title": "Re-Nerfing: Enforcing Geometric Constraints on Neural Radiance Fields\n  through Novel Views Synthesis",
            "updated": "2023-12-04T18:56:08Z",
            "published": "2023-12-04T18:56:08Z",
            "summary": "Neural Radiance Fields (NeRFs) have shown remarkable novel view synthesis\ncapabilities even in large-scale, unbounded scenes, albeit requiring hundreds\nof views or introducing artifacts in sparser settings. Their optimization\nsuffers from shape-radiance ambiguities wherever only a small visual overlap is\navailable. This leads to erroneous scene geometry and artifacts. In this paper,\nwe propose Re-Nerfing, a simple and general multi-stage approach that leverages\nNeRF's own view synthesis to address these limitations. With Re-Nerfing, we\nincrease the scene's coverage and enhance the geometric consistency of novel\nviews as follows: First, we train a NeRF with the available views. Then, we use\nthe optimized NeRF to synthesize pseudo-views next to the original ones to\nsimulate a stereo or trifocal setup. Finally, we train a second NeRF with both\noriginal and pseudo views while enforcing structural, epipolar constraints via\nthe newly synthesized images. Extensive experiments on the mip-NeRF 360 dataset\nshow the effectiveness of Re-Nerfing across denser and sparser input scenarios,\nbringing improvements to the state-of-the-art Zip-NeRF, even when trained with\nall views.",
            "author": [
                "Felix Tristram",
                "Stefano Gasperini",
                "Federico Tombari",
                "Nassir Navab",
                "Benjamin Busam"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02255v1",
                "http://arxiv.org/pdf/2312.02255v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02135v1",
            "title": "Fast View Synthesis of Casual Videos",
            "updated": "2023-12-04T18:55:48Z",
            "published": "2023-12-04T18:55:48Z",
            "summary": "Novel view synthesis from an in-the-wild video is difficult due to challenges\nlike scene dynamics and lack of parallax. While existing methods have shown\npromising results with implicit neural radiance fields, they are slow to train\nand render. This paper revisits explicit video representations to synthesize\nhigh-quality novel views from a monocular video efficiently. We treat static\nand dynamic video content separately. Specifically, we build a global static\nscene model using an extended plane-based scene representation to synthesize\ntemporally coherent novel video. Our plane-based scene representation is\naugmented with spherical harmonics and displacement maps to capture\nview-dependent effects and model non-planar complex surface geometry. We opt to\nrepresent the dynamic content as per-frame point clouds for efficiency. While\nsuch representations are inconsistency-prone, minor temporal inconsistencies\nare perceptually masked due to motion. We develop a method to quickly estimate\nsuch a hybrid video representation and render novel views in real time. Our\nexperiments show that our method can render high-quality novel views from an\nin-the-wild video with comparable quality to state-of-the-art methods while\nbeing 100x faster in training and enabling real-time rendering.",
            "author": [
                "Yao-Chih Lee",
                "Zhoutong Zhang",
                "Kevin Blackburn-Matzen",
                "Simon Niklaus",
                "Jianming Zhang",
                "Jia-Bin Huang",
                "Feng Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02135v1",
                "http://arxiv.org/pdf/2312.02135v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02134v1",
            "title": "GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single\n  Video via Animatable 3D Gaussians",
            "updated": "2023-12-04T18:55:45Z",
            "published": "2023-12-04T18:55:45Z",
            "summary": "We present GaussianAvatar, an efficient approach to creating realistic human\navatars with dynamic 3D appearances from a single video. We start by\nintroducing animatable 3D Gaussians to explicitly represent humans in various\nposes and clothing styles. Such an explicit and animatable representation can\nfuse 3D appearances more efficiently and consistently from 2D observations. Our\nrepresentation is further augmented with dynamic properties to support\npose-dependent appearance modeling, where a dynamic appearance network along\nwith an optimizable feature tensor is designed to learn the\nmotion-to-appearance mapping. Moreover, by leveraging the differentiable motion\ncondition, our method enables a joint optimization of motions and appearances\nduring avatar modeling, which helps to tackle the long-standing issue of\ninaccurate motion estimation in monocular settings. The efficacy of\nGaussianAvatar is validated on both the public dataset and our collected\ndataset, demonstrating its superior performances in terms of appearance quality\nand rendering efficiency.",
            "author": [
                "Liangxiao Hu",
                "Hongwen Zhang",
                "Yuxiang Zhang",
                "Boyao Zhou",
                "Boning Liu",
                "Shengping Zhang",
                "Liqiang Nie"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02134v1",
                "http://arxiv.org/pdf/2312.02134v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02133v1",
            "title": "Style Aligned Image Generation via Shared Attention",
            "updated": "2023-12-04T18:55:35Z",
            "published": "2023-12-04T18:55:35Z",
            "summary": "Large-scale Text-to-Image (T2I) models have rapidly gained prominence across\ncreative fields, generating visually compelling outputs from textual prompts.\nHowever, controlling these models to ensure consistent style remains\nchallenging, with existing methods necessitating fine-tuning and manual\nintervention to disentangle content and style. In this paper, we introduce\nStyleAligned, a novel technique designed to establish style alignment among a\nseries of generated images. By employing minimal `attention sharing' during the\ndiffusion process, our method maintains style consistency across images within\nT2I models. This approach allows for the creation of style-consistent images\nusing a reference style through a straightforward inversion operation. Our\nmethod's evaluation across diverse styles and text prompts demonstrates\nhigh-quality synthesis and fidelity, underscoring its efficacy in achieving\nconsistent style across various inputs.",
            "author": [
                "Amir Hertz",
                "Andrey Voynov",
                "Shlomi Fruchter",
                "Daniel Cohen-Or"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02133v1",
                "http://arxiv.org/pdf/2312.02133v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02128v1",
            "title": "Can we truly transfer an actor's genuine happiness to avatars? An\n  investigation into virtual, real, posed and spontaneous faces",
            "updated": "2023-12-04T18:53:42Z",
            "published": "2023-12-04T18:53:42Z",
            "summary": "A look is worth a thousand words is a popular phrase. And why is a simple\nlook enough to portray our feelings about something or someone? Behind this\nquestion are the theoretical foundations of the field of psychology regarding\nsocial cognition and the studies of psychologist Paul Ekman. Facial\nexpressions, as a form of non-verbal communication, are the primary way to\ntransmit emotions between human beings. The set of movements and expressions of\nfacial muscles that convey some emotional state of the individual to their\nobservers are targets of studies in many areas. Our research aims to evaluate\nEkman's action units in datasets of real human faces, posed and spontaneous,\nand virtual human faces resulting from transferring real faces into Computer\nGraphics faces. In addition, we also conducted a case study with specific movie\ncharacters, such as SheHulk and Genius. We intend to find differences and\nsimilarities in facial expressions between real and CG datasets, posed and\nspontaneous faces, and also to consider the actors' genders in the videos. This\ninvestigation can help several areas of knowledge, whether using real or\nvirtual human beings, in education, health, entertainment, games, security, and\neven legal matters. Our results indicate that AU intensities are greater for\nposed than spontaneous datasets, regardless of gender. Furthermore, there is a\nsmoothing of intensity up to 80 percent for AU6 and 45 percent for AU12 when a\nreal face is transformed into CG.",
            "author": [
                "Vitor Miguel Xavier Peres",
                "Greice Pinho Dal Molin",
                "Soraia Raupp Musse"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3631085.3631231",
                "http://arxiv.org/abs/2312.02128v1",
                "http://arxiv.org/pdf/2312.02128v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02127v1",
            "title": "Chaotic and Thermal Aspects in the $| HES \\rangle$ S-Matrix",
            "updated": "2023-12-04T18:53:39Z",
            "published": "2023-12-04T18:53:39Z",
            "summary": "We compute tree level scattering amplitudes involving more than one highly\nexcited states and tachyons in bosonic string theory. We use these amplitudes\nto understand chaotic and thermal aspects of the excited string states lending\nsupport to the Susskind-Horowitz-Polchinski correspondence principle. The\nunaveraged amplitudes exhibit chaos in the resonance distribution as a function\nof kinematic parameters, which can be described by random matrix theory. Upon\ncoarse-graining these amplitudes are shown to exponentiate, and capture various\nthermal features, including features of a stringy version of the eigenstate\nthermalization hypothesis as well as notions of typicality. Further, we compute\nthe effective string form factor corresponding to the highly excited states,\nand argue for the random walk behaviour of the long strings.",
            "author": [
                "Diptarka Das",
                "Santanu Mandal",
                "Anurag Sarkar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02127v1",
                "http://arxiv.org/pdf/2312.02127v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "cond-mat.stat-mech",
                "nlin.CD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02126v1",
            "title": "SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM",
            "updated": "2023-12-04T18:53:24Z",
            "published": "2023-12-04T18:53:24Z",
            "summary": "Dense simultaneous localization and mapping (SLAM) is pivotal for embodied\nscene understanding. Recent work has shown that 3D Gaussians enable\nhigh-quality reconstruction and real-time rendering of scenes using multiple\nposed cameras. In this light, we show for the first time that representing a\nscene by 3D Gaussians can enable dense SLAM using a single unposed monocular\nRGB-D camera. Our method, SplaTAM, addresses the limitations of prior radiance\nfield-based representations, including fast rendering and optimization, the\nability to determine if areas have been previously mapped, and structured map\nexpansion by adding more Gaussians. We employ an online tracking and mapping\npipeline while tailoring it to specifically use an underlying Gaussian\nrepresentation and silhouette-guided optimization via differentiable rendering.\nExtensive experiments show that SplaTAM achieves up to 2X state-of-the-art\nperformance in camera pose estimation, map construction, and novel-view\nsynthesis, demonstrating its superiority over existing approaches, while\nallowing real-time rendering of a high-resolution dense 3D map.",
            "author": [
                "Nikhil Keetha",
                "Jay Karhade",
                "Krishna Murthy Jatavallabhula",
                "Gengshan Yang",
                "Sebastian Scherer",
                "Deva Ramanan",
                "Jonathon Luiten"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02126v1",
                "http://arxiv.org/pdf/2312.02126v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02125v2",
            "title": "TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and\n  Advanced Decoding Techniques",
            "updated": "2023-12-06T05:19:11Z",
            "published": "2023-12-04T18:52:26Z",
            "summary": "Recent advances in language models (LMs), have demonstrated significant\nefficacy in tasks related to the arts and humanities. While LMs have exhibited\nexceptional performance across a wide range of natural language processing\ntasks, there are notable challenges associated with their utilization on small\ndatasets and their ability to replicate more creative human capacities. In this\nstudy, we aim to address these challenges by training a Persian classical\npoetry generation model using a transformer architecture on a specialized\ndataset with no pretraining. Additionally, we propose a novel decoding method\nto enhance coherence and meaningfulness in the generated poetry, effectively\nmanaging the tradeoff between diversity and quality. Furthermore, the results\nof our training approach and the proposed decoding method are evaluated through\ncomprehensive set of automatic and human evaluations and showed its superior\ncapability to generate coherent and meaningful poetry in compare to other\ndecoding methods and an existing Persian large language model (LLM).",
            "author": [
                "Amir Panahandeh",
                "Hanie Asemi",
                "Esmaeil Nourani"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02125v2",
                "http://arxiv.org/pdf/2312.02125v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02124v1",
            "title": "VerA: Versatile Anonymization Fit for Clinical Facial Images",
            "updated": "2023-12-04T18:51:44Z",
            "published": "2023-12-04T18:51:44Z",
            "summary": "The escalating legislative demand for data privacy in facial image\ndissemination has underscored the significance of image anonymization. Recent\nadvancements in the field surpass traditional pixelation or blur methods, yet\nthey predominantly address regular single images. This leaves clinical image\nanonymization -- a necessity for illustrating medical interventions -- largely\nunaddressed. We present VerA, a versatile facial image anonymization that is\nfit for clinical facial images where: (1) certain semantic areas must be\npreserved to show medical intervention results, and (2) anonymizing image pairs\nis crucial for showing before-and-after results. VerA outperforms or is on par\nwith state-of-the-art methods in de-identification and photorealism for regular\nimages. In addition, we validate our results on paired anonymization, and on\nthe anonymization of both single and paired clinical images with extensive\nquantitative and qualitative evaluation.",
            "author": [
                "Majed El Helou",
                "Doruk Cetin",
                "Petar Stamenkovic",
                "Fabio Zund"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02124v1",
                "http://arxiv.org/pdf/2312.02124v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02121v1",
            "title": "Mathematical Supplement for the $\\texttt{gsplat}$ Library",
            "updated": "2023-12-04T18:50:41Z",
            "published": "2023-12-04T18:50:41Z",
            "summary": "This report provides the mathematical details of the gsplat library, a\nmodular toolbox for efficient differentiable Gaussian splatting, as proposed by\nKerbl et al. It provides a self-contained reference for the computations\ninvolved in the forward and backward passes of differentiable Gaussian\nsplatting. To facilitate practical usage and development, we provide a user\nfriendly Python API that exposes each component of the forward and backward\npasses in rasterization at github.com/nerfstudio-project/gsplat .",
            "author": [
                "Vickie Ye",
                "Angjoo Kanazawa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02121v1",
                "http://arxiv.org/pdf/2312.02121v1"
            ],
            "primary_category": "cs.MS",
            "category": [
                "cs.MS",
                "cs.CV",
                "cs.GR",
                "cs.NA",
                "math.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02120v1",
            "title": "Magicoder: Source Code Is All You Need",
            "updated": "2023-12-04T18:50:35Z",
            "published": "2023-12-04T18:50:35Z",
            "summary": "We introduce Magicoder, a series of fully open-source (code, weights, and\ndata) Large Language Models (LLMs) for code that significantly closes the gap\nwith top code models while having no more than 7B parameters. Magicoder models\nare trained on 75K synthetic instruction data using OSS-Instruct, a novel\napproach to enlightening LLMs with open-source code snippets to generate\nhigh-quality instruction data for code. Our main motivation is to mitigate the\ninherent bias of the synthetic data generated by LLMs by empowering them with a\nwealth of open-source references for the production of more diverse, realistic,\nand controllable data. The orthogonality of OSS-Instruct and other data\ngeneration methods like Evol-Instruct further enables us to build an enhanced\nMagicoderS. Both Magicoder and MagicoderS substantially outperform\nstate-of-the-art code models with similar or even larger sizes on a wide range\nof coding benchmarks, including Python text-to-code generation, multilingual\ncoding, and data-science program completion. Notably, MagicoderS-CL-7B based on\nCodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in\npass@1). Overall, OSS-Instruct opens a new direction for low-bias and\nhigh-quality instruction tuning using abundant open-source references.",
            "author": [
                "Yuxiang Wei",
                "Zhe Wang",
                "Jiawei Liu",
                "Yifeng Ding",
                "Lingming Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02120v1",
                "http://arxiv.org/pdf/2312.02120v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02119v1",
            "title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically",
            "updated": "2023-12-04T18:49:23Z",
            "published": "2023-12-04T18:49:23Z",
            "summary": "While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an LLM to\niteratively refine candidate (attack) prompts using tree-of-thoughts reasoning\nuntil one of the generated prompts jailbreaks the target. Crucially, before\nsending prompts to the target, TAP assesses them and prunes the ones unlikely\nto result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate\na large search space of prompts and pruning reduces the total number of queries\nsent to the target. In empirical evaluations, we observe that TAP generates\nprompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo)\nfor more than 80% of the prompts using only a small number of queries. This\nsignificantly improves upon the previous state-of-the-art black-box method for\ngenerating jailbreaks.",
            "author": [
                "Anay Mehrotra",
                "Manolis Zampetakis",
                "Paul Kassianik",
                "Blaine Nelson",
                "Hyrum Anderson",
                "Yaron Singer",
                "Amin Karbasi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02119v1",
                "http://arxiv.org/pdf/2312.02119v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.CR",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02118v1",
            "title": "When it Rains, it Pours: Modeling Media Storms and the News Ecosystem",
            "updated": "2023-12-04T18:49:06Z",
            "published": "2023-12-04T18:49:06Z",
            "summary": "Most events in the world receive at most brief coverage by the news media.\nOccasionally, however, an event will trigger a media storm, with voluminous and\nwidespread coverage lasting for weeks instead of days. In this work, we develop\nand apply a pairwise article similarity model, allowing us to identify story\nclusters in corpora covering local and national online news, and thereby create\na comprehensive corpus of media storms over a nearly two year period. Using\nthis corpus, we investigate media storms at a new level of granularity,\nallowing us to validate claims about storm evolution and topical distribution,\nand provide empirical support for previously hypothesized patterns of influence\nof storms on media coverage and intermedia agenda setting.",
            "author": [
                "Benjamin Litterer",
                "David Jurgens",
                "Dallas Card"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02118v1",
                "http://arxiv.org/pdf/2312.02118v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CY",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02116v1",
            "title": "GIVT: Generative Infinite-Vocabulary Transformers",
            "updated": "2023-12-04T18:48:02Z",
            "published": "2023-12-04T18:48:02Z",
            "summary": "We introduce generative infinite-vocabulary transformers (GIVT) which\ngenerate vector sequences with real-valued entries, instead of discrete tokens\nfrom a finite vocabulary. To this end, we propose two surprisingly simple\nmodifications to decoder-only transformers: 1) at the input, we replace the\nfinite-vocabulary lookup table with a linear projection of the input vectors;\nand 2) at the output, we replace the logits prediction (usually mapped to a\ncategorical distribution) with the parameters of a multivariate Gaussian\nmixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT,\nwhere transformers are used to model the discrete latent sequences of a VQ-VAE,\nwe use GIVT to model the unquantized real-valued latent sequences of a VAE.\nWhen applying GIVT to class-conditional image generation with iterative masked\nmodeling, we show competitive results with MaskGIT, while our approach\noutperforms both VQ-GAN and MaskGIT when using it for causal modeling. Finally,\nwe obtain competitive results outside of image generation when applying our\napproach to panoptic segmentation and depth estimation with a VAE-based variant\nof the UViM framework.",
            "author": [
                "Michael Tschannen",
                "Cian Eastwood",
                "Fabian Mentzer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02116v1",
                "http://arxiv.org/pdf/2312.02116v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02114v1",
            "title": "Transitions of Solutions and Their Efficiency",
            "updated": "2023-12-04T18:47:13Z",
            "published": "2023-12-04T18:47:13Z",
            "summary": "We broaden the basis of non-cooperative game theory by considering\nmiscoordination on a solution concept. For any solution concept, we extend the\nsolution set of a strategic-form game to a transition set. This set contains\nprofiles where various agents simultaneously follow different solutions,\ne.g.~different Nash equilibria. This models the fact that in practice,\ncomplicated agents are rarely perfectly coordinated on the same equilibrium. We\ndefine two efficiency measures, called the price of transition anarchy and\nstability, and bound them. We also refine the notion of transition to the\nnotion of limited transition, where only a limited number of solutions is\nsimultaneously played, and to stable transitions, which allow for only minor\nlack of coordination. We compare the above mentioned efficiency measures and\nbound the efficiency of transitions in important cases, including the important\ncases of constant-sum and potential games, which span the set of finite games\nwith the same number of strategies for each agent. We also prove tight\nefficiency bounds for routing games and coordination games on graphs. Finally,\nwe study algorithms to find the transition degree required to make a given\nprofile a transition, or to render all the profiles transitions. We conclude\nthat for the sake of efficiency, it is crucial to avoid uncoordinated\ntransitions, besides certain cases, such as constant-sum games, identical\nutility games, some types of routing games, limited transitions in potential\ngames, and stable transitions in coordination games.",
            "author": [
                "Gleb Polevoy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02114v1",
                "http://arxiv.org/pdf/2312.02114v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.DM",
                "91A10, 91A14, 91A28,",
                "J.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02113v1",
            "title": "A Framework for Self-Intersecting Surfaces (SOS): Symmetric Optimisation\n  for Stability",
            "updated": "2023-12-04T18:46:42Z",
            "published": "2023-12-04T18:46:42Z",
            "summary": "In this paper, we give a stable and efficient method for fixing\nself-intersections and non-manifold parts in a given embedded simplicial\ncomplex. In addition, we show how symmetric properties can be used for further\noptimisation. We prove an initialisation criterion for computation of the outer\nhull of an embedded simplicial complex. To regularise the outer hull of the\nretriangulated surface, we present a method to remedy non-manifold edges and\npoints. We also give a modification of the outer hull algorithm to determine\nchambers of complexes which gives rise to many new insights. All of these\nmethods have applications in many areas, for example in 3D-printing, artistic\nrealisations of 3D models or fixing errors introduced by scanning equipment\napplied for tomography. Implementations of the proposed algorithms are given in\nthe computer algebra system GAP4. For verification of our methods, we use a\ndata-set of highly self-intersecting symmetric icosahedra.",
            "author": [
                "Christian Amend",
                "Tom Goertzen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02113v1",
                "http://arxiv.org/pdf/2312.02113v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG",
                "cs.MS",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02111v2",
            "title": "TriDeNT: Triple Deep Network Training for Privileged Knowledge\n  Distillation in Histopathology",
            "updated": "2023-12-05T12:18:25Z",
            "published": "2023-12-04T18:43:45Z",
            "summary": "Computational pathology models rarely utilise data that will not be available\nfor inference. This means most models cannot learn from highly informative data\nsuch as additional immunohistochemical (IHC) stains and spatial\ntranscriptomics. We present TriDeNT, a novel self-supervised method for\nutilising privileged data that is not available during inference to improve\nperformance. We demonstrate the efficacy of this method for a range of\ndifferent paired data including immunohistochemistry, spatial transcriptomics\nand expert nuclei annotations. In all settings, TriDeNT outperforms other\nstate-of-the-art methods in downstream tasks, with observed improvements of up\nto 101%. Furthermore, we provide qualitative and quantitative measurements of\nthe features learned by these models and how they differ from baselines.\nTriDeNT offers a novel method to distil knowledge from scarce or costly data\nduring training, to create significantly better models for routine inputs.",
            "author": [
                "Lucas Farndale",
                "Robert Insall",
                "Ke Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02111v2",
                "http://arxiv.org/pdf/2312.02111v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "q-bio.TO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02109v1",
            "title": "ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder\n  and Explicit Adaptation",
            "updated": "2023-12-04T18:39:00Z",
            "published": "2023-12-04T18:39:00Z",
            "summary": "This work introduces ArtAdapter, a transformative text-to-image (T2I) style\ntransfer framework that transcends traditional limitations of color,\nbrushstrokes, and object shape, capturing high-level style elements such as\ncomposition and distinctive artistic expression. The integration of a\nmulti-level style encoder with our proposed explicit adaptation mechanism\nenables ArtAdapte to achieve unprecedented fidelity in style transfer, ensuring\nclose alignment with textual descriptions. Additionally, the incorporation of\nan Auxiliary Content Adapter (ACA) effectively separates content from style,\nalleviating the borrowing of content from style references. Moreover, our novel\nfast finetuning approach could further enhance zero-shot style representation\nwhile mitigating the risk of overfitting. Comprehensive evaluations confirm\nthat ArtAdapter surpasses current state-of-the-art methods.",
            "author": [
                "Dar-Yen Chen",
                "Hamish Tennent",
                "Ching-Wen Hsu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02109v1",
                "http://arxiv.org/pdf/2312.02109v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02253v1",
            "title": "Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with\n  Synthetic Images",
            "updated": "2023-12-04T18:35:27Z",
            "published": "2023-12-04T18:35:27Z",
            "summary": "Recent advances in generative deep learning have enabled the creation of\nhigh-quality synthetic images in text-to-image generation. Prior work shows\nthat fine-tuning a pretrained diffusion model on ImageNet and generating\nsynthetic training images from the finetuned model can enhance an ImageNet\nclassifier's performance. However, performance degrades as synthetic images\noutnumber real ones. In this paper, we explore whether generative fine-tuning\nis essential for this improvement and whether it is possible to further scale\nup training using more synthetic data. We present a new framework leveraging\noff-the-shelf generative models to generate synthetic training images,\naddressing multiple challenges: class name ambiguity, lack of diversity in\nnaive prompts, and domain shifts. Specifically, we leverage large language\nmodels (LLMs) and CLIP to resolve class name ambiguity. To diversify images, we\npropose contextualized diversification (CD) and stylized diversification (SD)\nmethods, also prompted by LLMs. Finally, to mitigate domain shifts, we leverage\ndomain adaptation techniques with auxiliary batch normalization for synthetic\nimages. Our framework consistently enhances recognition model performance with\nmore synthetic data, up to 6x of original ImageNet size showcasing the\npotential of synthetic data for improved recognition models and strong\nout-of-domain generalization.",
            "author": [
                "Zhuoran Yu",
                "Chenchen Zhu",
                "Sean Culatana",
                "Raghuraman Krishnamoorthi",
                "Fanyi Xiao",
                "Yong Jae Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02253v1",
                "http://arxiv.org/pdf/2312.02253v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02104v1",
            "title": "RidePy: A fast and modular framework for simulating ridepooling systems",
            "updated": "2023-12-04T18:31:32Z",
            "published": "2023-12-04T18:31:32Z",
            "summary": "RidePy enables fast computer simulations of on-demand mobility modes such as\nridehailing or ridepooling. It strongly focuses on modeling the mobility\nservice itself, rather than its customers or the environment. Through a\ncombination of Python, Cython and C++, it offers ease of use at high\nperformance. Its modular design makes customization easy, while the included\nmodules allow for a quick start.",
            "author": [
                "Felix Jung",
                "Debsankha Manik"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02104v1",
                "http://arxiv.org/pdf/2312.02104v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02103v1",
            "title": "Learning Pseudo-Labeler beyond Noun Concepts for Open-Vocabulary Object\n  Detection",
            "updated": "2023-12-04T18:29:03Z",
            "published": "2023-12-04T18:29:03Z",
            "summary": "Open-vocabulary object detection (OVOD) has recently gained significant\nattention as a crucial step toward achieving human-like visual intelligence.\nExisting OVOD methods extend target vocabulary from pre-defined categories to\nopen-world by transferring knowledge of arbitrary concepts from vision-language\npre-training models to the detectors. While previous methods have shown\nremarkable successes, they suffer from indirect supervision or limited\ntransferable concepts. In this paper, we propose a simple yet effective method\nto directly learn region-text alignment for arbitrary concepts. Specifically,\nthe proposed method aims to learn arbitrary image-to-text mapping for\npseudo-labeling of arbitrary concepts, named Pseudo-Labeling for Arbitrary\nConcepts (PLAC). The proposed method shows competitive performance on the\nstandard OVOD benchmark for noun concepts and a large improvement on referring\nexpression comprehension benchmark for arbitrary concepts.",
            "author": [
                "Sunghun Kang",
                "Junbum Cha",
                "Jonghwan Mun",
                "Byungseok Roh",
                "Chang D. Yoo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02103v1",
                "http://arxiv.org/pdf/2312.02103v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02097v1",
            "title": "Inapproximability of Maximum Diameter Clustering for Few Clusters",
            "updated": "2023-12-04T18:16:27Z",
            "published": "2023-12-04T18:16:27Z",
            "summary": "We consider the k-diameter clustering problem, where the goal is to partition\na set of points in a metric space into $k$ clusters, minimizing the maximum\ndistance between any two points in the same cluster. In general metrics,\nk-diameter is known to be NP-hard, while it has a $2$-approximation algorithm\n(Gonzalez'85). Complementing this algorithm, it is known that k-diameter is\nNP-hard to approximate within a factor better than $2$ in the $\\ell_1$ and\n$\\ell_\\infty$ metrics, and within a factor of $1.969$ in the $\\ell_2$ metric\n(Feder-Greene'88).\n  When $k\\geq 3$ is fixed, k-diameter remains NP-hard to approximate within a\nfactor better than $2$ in the $\\ell_\\infty$ metric (Megiddo'90). However, its\napproximability in this setting has not previously been studied in the $\\ell_1$\nand $\\ell_2$ metrics, though a $1.415$-approximation algorithm in the $\\ell_2$\nmetric follows from a known result (Badoiu et al.'02). In this paper, we\naddress the remaining gap by showing new hardness of approximation results that\nhold even when $k=3$. Specifically, we prove that 3-diameter is NP-hard to\napproximate within a factor better than $1.5$ in the $\\ell_1$ metric, and\nwithin a factor of $1.304$ in the $\\ell_2$ metric.",
            "author": [
                "Henry Fleischmann",
                "Kyrylo Karlov",
                "Karthik C. S.",
                "Ashwin Padaki",
                "Stepan Zharkov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02097v1",
                "http://arxiv.org/pdf/2312.02097v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG",
                "cs.CC",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02252v1",
            "title": "Large Language Models as Consistent Story Visualizers",
            "updated": "2023-12-04T18:14:29Z",
            "published": "2023-12-04T18:14:29Z",
            "summary": "Recent generative models have demonstrated impressive capabilities in\ngenerating realistic and visually pleasing images grounded on textual prompts.\nNevertheless, a significant challenge remains in applying these models for the\nmore intricate task of story visualization. Since it requires resolving\npronouns (he, she, they) in the frame descriptions, i.e., anaphora resolution,\nand ensuring consistent characters and background synthesis across frames. Yet,\nthe emerging Large Language Model (LLM) showcases robust reasoning abilities to\nnavigate through ambiguous references and process extensive sequences.\nTherefore, we introduce \\textbf{StoryGPT-V}, which leverages the merits of the\nlatent diffusion (LDM) and LLM to produce images with consistent and\nhigh-quality characters grounded on given story descriptions. First, we train a\ncharacter-aware LDM, which takes character-augmented semantic embedding as\ninput and includes the supervision of the cross-attention map using character\nsegmentation masks, aiming to enhance character generation accuracy and\nfaithfulness. In the second stage, we enable an alignment between the output of\nLLM and the character-augmented embedding residing in the input space of the\nfirst-stage model. This harnesses the reasoning ability of LLM to address\nambiguous references and the comprehension capability to memorize the context.\nWe conduct comprehensive experiments on two visual story visualization\nbenchmarks. Our model reports superior quantitative results and consistently\ngenerates accurate characters of remarkable quality with low memory\nconsumption. Our code will be made publicly available.",
            "author": [
                "Xiaoqian Shen",
                "Mohamed Elhoseiny"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02252v1",
                "http://arxiv.org/pdf/2312.02252v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02093v1",
            "title": "Cultural Differences in Students' Privacy Concerns in Learning Analytics\n  across Germany, South Korea, Spain, Sweden, and the United States",
            "updated": "2023-12-04T18:10:20Z",
            "published": "2023-12-04T18:10:20Z",
            "summary": "Applications of learning analytics (LA) can raise concerns from students\nabout their privacy in higher education contexts. Developing effective\nprivacy-enhancing practices requires a systematic understanding of students'\nprivacy concerns and how they vary across national and cultural dimensions. We\nconducted a survey study with established instruments to measure privacy\nconcerns and cultural values for university students in five countries\n(Germany, South Korea, Spain, Sweden, and the United States; N = 762). The\nresults show that students generally trusted institutions with their data and\ndisclosed information as they perceived the risks to be manageable even though\nthey felt somewhat limited in their ability to control their privacy. Across\nthe five countries, German and Swedish students stood out as the most trusting\nand least concerned, especially compared to US students who reported greater\nperceived risk and less control. Students in South Korea and Spain responded\nsimilarly on all five privacy dimensions (perceived privacy risk, perceived\nprivacy control, privacy concerns, trusting beliefs, and non-self-disclosure\nbehavior), despite their significant cultural differences. Culture measured at\nthe individual level affected the antecedents and outcomes of privacy concerns\nmore than country-level culture. Perceived privacy risk and privacy control\nincrease with power distance. Trusting beliefs increase with a desire for\nuncertainty avoidance and lower masculinity. Non-self-disclosure behaviors rise\nwith power distance and masculinity, and decrease with more uncertainty\navoidance. Thus, cultural values related to trust in institutions, social\nequality and risk-taking should be considered when developing privacy-enhancing\npractices and policies in higher education.",
            "author": [
                "Olga Viberg",
                "Ren\u00e9 F. Kizilcec",
                "Ioana Jivet",
                "Alejandra Mart\u00ednez Mon\u00e9s",
                "Alice Oh",
                "Chantal Mutimukwe",
                "Stefan Hrastinski",
                "Maren Scheffel"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02093v1",
                "http://arxiv.org/pdf/2312.02093v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02091v1",
            "title": "Physics simulation capabilities of LLMs",
            "updated": "2023-12-04T18:06:41Z",
            "published": "2023-12-04T18:06:41Z",
            "summary": "[Abridged abstract] Large Language Models (LLMs) can solve some\nundergraduate-level to graduate-level physics textbook problems and are\nproficient at coding. Combining these two capabilities could one day enable AI\nsystems to simulate and predict the physical world.\n  We present an evaluation of state-of-the-art (SOTA) LLMs on PhD-level to\nresearch-level computational physics problems. We condition LLM generation on\nthe use of well-documented and widely-used packages to elicit coding\ncapabilities in the physics and astrophysics domains. We contribute $\\sim 50$\noriginal and challenging problems in celestial mechanics (with REBOUND),\nstellar physics (with MESA), 1D fluid dynamics (with Dedalus) and non-linear\ndynamics (with SciPy). Since our problems do not admit unique solutions, we\nevaluate LLM performance on several soft metrics: counts of lines that contain\ndifferent types of errors (coding, physics, necessity and sufficiency) as well\nas a more \"educational\" Pass-Fail metric focused on capturing the salient\nphysical ingredients of the problem at hand.\n  As expected, today's SOTA LLM (GPT4) zero-shot fails most of our problems,\nalthough about 40\\% of the solutions could plausibly get a passing grade. About\n$70-90 \\%$ of the code lines produced are necessary, sufficient and correct\n(coding \\& physics). Physics and coding errors are the most common, with some\nunnecessary or insufficient lines. We observe significant variations across\nproblem class and difficulty. We identify several failure modes of GPT4 in the\ncomputational physics domain.\n  Our reconnaissance work provides a snapshot of current computational\ncapabilities in classical physics and points to obvious improvement targets if\nAI systems are ever to reach a basic level of autonomy in physics simulation\ncapabilities.",
            "author": [
                "Mohamad Ali-Dib",
                "Kristen Menou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02091v1",
                "http://arxiv.org/pdf/2312.02091v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "astro-ph.EP",
                "astro-ph.IM",
                "astro-ph.SR",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02251v1",
            "title": "Fine-Tuning Language Models for Context-Specific SQL Query Generation",
            "updated": "2023-12-04T18:04:27Z",
            "published": "2023-12-04T18:04:27Z",
            "summary": "The ability to generate SQL queries from natural language has significant\nimplications for making data accessible to non-specialists. This paper presents\na novel approach to fine-tuning open-source large language models (LLMs) for\nthe task of transforming natural language into SQL queries within the retail\ndomain. We introduce models specialized in generating SQL queries, trained on\nsynthetic datasets tailored to the Snowflake SQL and GoogleSQL dialects. Our\nmethodology involves generating a context-specific dataset using GPT-4, then\nfine-tuning three open-source LLMs(Starcoder Plus, Code-Llama, and Mistral)\nemploying the LoRa technique to optimize for resource constraints. The\nfine-tuned models demonstrate superior performance in zero-shot settings\ncompared to the baseline GPT-4, with Code-Llama achieving the highest accuracy\nrates, at 81.58% for Snowflake SQL and 82.66% for GoogleSQL. These results\nunderscore the effectiveness of fine-tuning LLMs on domain-specific tasks and\nsuggest a promising direction for enhancing the accessibility of relational\ndatabases through natural language interfaces.",
            "author": [
                "Amine Rebei"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02251v1",
                "http://arxiv.org/pdf/2312.02251v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02087v2",
            "title": "VideoSwap: Customized Video Subject Swapping with Interactive Semantic\n  Point Correspondence",
            "updated": "2023-12-05T17:14:25Z",
            "published": "2023-12-04T17:58:06Z",
            "summary": "Current diffusion-based video editing primarily focuses on\nstructure-preserved editing by utilizing various dense correspondences to\nensure temporal consistency and motion alignment. However, these approaches are\noften ineffective when the target edit involves a shape change. To embark on\nvideo editing with shape change, we explore customized video subject swapping\nin this work, where we aim to replace the main subject in a source video with a\ntarget subject having a distinct identity and potentially different shape. In\ncontrast to previous methods that rely on dense correspondences, we introduce\nthe VideoSwap framework that exploits semantic point correspondences, inspired\nby our observation that only a small number of semantic points are necessary to\nalign the subject's motion trajectory and modify its shape. We also introduce\nvarious user-point interactions (\\eg, removing points and dragging points) to\naddress various semantic point correspondence. Extensive experiments\ndemonstrate state-of-the-art video subject swapping results across a variety of\nreal-world videos.",
            "author": [
                "Yuchao Gu",
                "Yipin Zhou",
                "Bichen Wu",
                "Licheng Yu",
                "Jia-Wei Liu",
                "Rui Zhao",
                "Jay Zhangjie Wu",
                "David Junhao Zhang",
                "Mike Zheng Shou",
                "Kevin Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02087v2",
                "http://arxiv.org/pdf/2312.02087v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02083v1",
            "title": "Celestial amplitudes dual to the O(N) nonlinear sigma model",
            "updated": "2023-12-04T17:49:55Z",
            "published": "2023-12-04T17:49:55Z",
            "summary": "We compute celestial amplitudes corresponding to the exact S-matrix of the 2d\nO(N)-symmetric nonlinear sigma model. Celestial amplitudes for two-dimensional\nintegrable S-matrices simplify to Fourier transforms. Due to the connection\nbetween Fourier and Mellin transforms, celestial amplitudes for O(N) model are\nMejer G-functions. We also prove crossing symmetry for obtained results and\nreview simplifications for O(3) symmetry case.",
            "author": [
                "Valeriia Stolbova"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02083v1",
                "http://arxiv.org/pdf/2312.02083v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02082v1",
            "title": "Joint State and Input Estimation for Linear Dynamical Systems with\n  Sparse Control",
            "updated": "2023-12-04T17:49:21Z",
            "published": "2023-12-04T17:49:21Z",
            "summary": "Sparsity constraints on the control inputs of a linear dynamical system\nnaturally arise in several practical applications such as networked control,\ncomputer vision, seismic signal processing, and cyber-physical systems. In this\nwork, we consider the problem of jointly estimating the states and sparse\ninputs of such systems from low-dimensional (compressive) measurements. Due to\nthe low-dimensional measurements, conventional Kalman filtering and smoothing\nalgorithms fail to accurately estimate the states and inputs. We present a\nBayesian approach that exploits the input sparsity to significantly improve\nestimation accuracy. Sparsity in the input estimates is promoted by using\ndifferent prior distributions on the input. We investigate two main approaches:\nregularizer-based MAP, and {Bayesian learning-based estimation}. We also extend\nthe approaches to handle control inputs with common support and analyze the\ntime and memory complexities of the presented algorithms. Finally, using\nnumerical simulations, we show that our algorithms outperform the\nstate-of-the-art methods in terms of accuracy and time/memory complexities,\nespecially in the low-dimensional measurement regime.",
            "author": [
                "Rupam Kalyan Chakraborty",
                "Geethu Joseph",
                "Chandra R. Murthy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02082v1",
                "http://arxiv.org/pdf/2312.02082v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02081v1",
            "title": "Copula-based deviation measure of cointegrated financial assets",
            "updated": "2023-12-04T17:47:49Z",
            "published": "2023-12-04T17:47:49Z",
            "summary": "This study outlines a comprehensive methodology utilizing copulas to discern\ninconsistencies in the behavior exhibited by pairs of financial assets. It\nintroduces a robust approach to establishing the interrelationship between the\nreturns of these assets, exploring potential measures of dependence among the\nstochastic variables represented by these returns. Special emphasis is placed\non scrutinizing the traditional measure of dependence, namely the correlation\ncoefficient, delineating its limitations. Furthermore, the study articulates an\nalternative methodology that offers enhanced stability and informativeness in\nappraising the relationship between financial instrument returns.",
            "author": [
                "Alexander Shulzhenko"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02081v1",
                "http://arxiv.org/pdf/2312.02081v1"
            ],
            "primary_category": "q-fin.CP",
            "category": [
                "q-fin.CP",
                "q-fin.ST",
                "q-fin.TR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02080v2",
            "title": "Fixed-point methods for long-term power control and beamforming design\n  in large-scale MIMO",
            "updated": "2023-12-07T11:56:53Z",
            "published": "2023-12-04T17:47:27Z",
            "summary": "This study presents novel applications of fixed-point methods to solve\npreviously open joint power control and beamforming design problems in modern\nlarge-scale MIMO systems, e.g., based on the cell-free massive MIMO and XL-MIMO\nconcepts. In particular, motivated by the need for scalable system\narchitectures, we revisit the classical sum power minimization and max-min fair\ndesign criteria by considering long-term power control and beamforming design\nbased on channel statistics and possibly limited channel state information\n(CSI) sharing across distributed processing units. This approach is believed to\nmitigate the severe scalability issues of competing short-term optimal\nalgorithms in the literature, which must be executed for every channel\nrealization by a central controller endowed with global CSI, hence imposing\nvery demanding requirements in terms of computation and interconnection\ncapabilities. The obtained optimal algorithms are then illustrated and compared\nagainst existing short-term and long-term approaches via numerical simulations\nin a cell-free massive MIMO setup.",
            "author": [
                "Lorenzo Miretti",
                "Renato L. G. Cavalcante",
                "S\u0142awomir Sta\u0144czak"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02080v2",
                "http://arxiv.org/pdf/2312.02080v2"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02078v1",
            "title": "Integrating AI into CCTV Systems: A Comprehensive Evaluation of Smart\n  Video Surveillance in Community Space",
            "updated": "2023-12-04T17:41:52Z",
            "published": "2023-12-04T17:41:52Z",
            "summary": "This article presents an AI-enabled Smart Video Surveillance (SVS) designed\nto enhance safety in community spaces such as educational and recreational\nareas, and small businesses. The proposed system innovatively integrates with\nexisting CCTV and wired camera networks, simplifying its adoption across\nvarious community cases to leverage recent AI advancements. Our SVS system,\nfocusing on privacy, uses metadata instead of pixel data for activity\nrecognition, aligning with ethical standards. It features cloud-based\ninfrastructure and a mobile app for real-time, privacy-conscious alerts in\ncommunities.\n  This article notably pioneers a comprehensive real-world evaluation of the\nSVS system, covering AI-driven visual processing, statistical analysis,\ndatabase management, cloud communication, and user notifications. It's also the\nfirst to assess an end-to-end anomaly detection system's performance, vital for\nidentifying potential public safety incidents.\n  For our evaluation, we implemented the system in a community college, serving\nas an ideal model to exemplify the proposed system's capabilities. Our findings\nin this setting demonstrate the system's robustness, with throughput, latency,\nand scalability effectively managing 16 CCTV cameras. The system maintained a\nconsistent 16.5 frames per second (FPS) over a 21-hour operation. The average\nend-to-end latency for detecting behavioral anomalies and alerting users was\n26.76 seconds.",
            "author": [
                "Shanle Yao",
                "Babak Rahimi Ardabili",
                "Armin Danesh Pazho",
                "Ghazal Alinezhad Noghre",
                "Christopher Neff",
                "Hamed Tabkhi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02078v1",
                "http://arxiv.org/pdf/2312.02078v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02076v1",
            "title": "The Geometry of Mehlers Kernel",
            "updated": "2023-12-04T17:40:21Z",
            "published": "2023-12-04T17:40:21Z",
            "summary": "We study the relationship between the Getzler calculus of a spin Riemannian\nmanifold and the Riemannian geometry of the corresponding principal spin\nbundle. We then use the calculus of Gaussian-Grassmann integrals developed by\nBerline--Vergne to compute the Getzler symbol of the spinor heat flow.",
            "author": [
                "Jesus Sanchez Jr"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02076v1",
                "http://arxiv.org/pdf/2312.02076v1"
            ],
            "primary_category": "math.DG",
            "category": [
                "math.DG",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02074v1",
            "title": "Federated Learning is Better with Non-Homomorphic Encryption",
            "updated": "2023-12-04T17:37:41Z",
            "published": "2023-12-04T17:37:41Z",
            "summary": "Traditional AI methodologies necessitate centralized data collection, which\nbecomes impractical when facing problems with network communication, data\nprivacy, or storage capacity. Federated Learning (FL) offers a paradigm that\nempowers distributed AI model training without collecting raw data. There are\ndifferent choices for providing privacy during FL training. One of the popular\nmethodologies is employing Homomorphic Encryption (HE) - a breakthrough in\nprivacy-preserving computation from Cryptography. However, these methods have a\nprice in the form of extra computation and memory footprint. To resolve these\nissues, we propose an innovative framework that synergizes permutation-based\ncompressors with Classical Cryptography, even though employing Classical\nCryptography was assumed to be impossible in the past in the context of FL. Our\nframework offers a way to replace HE with cheaper Classical Cryptography\nprimitives which provides security for the training process. It fosters\nasynchronous communication and provides flexible deployment options in various\ncommunication topologies.",
            "author": [
                "Konstantin Burlachenko",
                "Abdulmajeed Alrowithi",
                "Fahad Ali Albalawi",
                "Peter Richtarik"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3630048.3630182",
                "http://arxiv.org/abs/2312.02074v1",
                "http://arxiv.org/pdf/2312.02074v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG",
                "math.OC",
                "G.1.6; E.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02073v1",
            "title": "A Glitch in the Matrix? Locating and Detecting Language Model Grounding\n  with Fakepedia",
            "updated": "2023-12-04T17:35:42Z",
            "published": "2023-12-04T17:35:42Z",
            "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nstoring and recalling factual knowledge, but also in adapting to novel\nin-context information. Yet, the mechanisms underlying their in-context\ngrounding remain unknown, especially in situations where in-context information\ncontradicts factual knowledge embedded in the parameters. This is critical for\nretrieval-augmented generation methods, which enrich the context with\nup-to-date information, hoping that grounding can rectify the outdated\nparametric knowledge. In this study, we introduce Fakepedia, a counterfactual\ndataset designed to evaluate grounding abilities when the parametric knowledge\nclashes with the in-context information. We benchmark various LLMs with\nFakepedia and discover that GPT-4-turbo has a strong preference for its\nparametric knowledge. Mistral-7B, on the contrary, is the model that most\nrobustly chooses the grounded answer. Then, we conduct causal mediation\nanalysis on LLM components when answering Fakepedia queries. We demonstrate\nthat inspection of the computational graph alone can predict LLM grounding with\n92.8% accuracy, especially because few MLPs in the Transformer can predict\nnon-grounded behavior. Our results, together with existing findings about\nfactual recall mechanisms, provide a coherent narrative of how grounding and\nfactual recall mechanisms interact within LLMs.",
            "author": [
                "Giovanni Monea",
                "Maxime Peyrard",
                "Martin Josifoski",
                "Vishrav Chaudhary",
                "Jason Eisner",
                "Emre K\u0131c\u0131man",
                "Hamid Palangi",
                "Barun Patra",
                "Robert West"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02073v1",
                "http://arxiv.org/pdf/2312.02073v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02072v1",
            "title": "Long-time behavior of logarithmic spiral vortex sheets with two branches",
            "updated": "2023-12-04T17:32:07Z",
            "published": "2023-12-04T17:32:07Z",
            "summary": "We consider logarithmic spiral vortex sheets consisting of two branches.\nBased on some simple assumptions that appear true by numerical computations, we\nfully classify their long-time behavior and asymptotics, where in all cases\neach branch decays to $0$ or blows up in finite time. Furthermore, we present\nillustrations determining which range of initial data corresponds to each case.\nWe also determine the asymptotic stability of the symmetric and asymmetric\nself-similar spirals.",
            "author": [
                "Minki Cho"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02072v1",
                "http://arxiv.org/pdf/2312.02072v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02071v1",
            "title": "Evaluating the Claims of \"SAT Requires Exhaustive Search\"",
            "updated": "2023-12-04T17:30:09Z",
            "published": "2023-12-04T17:30:09Z",
            "summary": "In this paper, we take a closer look at the claims made by Xu and Zhou in\ntheir paper \"SAT Requires Exhaustive Search\" [XZ23], which claims to provide a\nlower bound on the complexity of the so-called Model RB. Xu and Zhou conclude\nthat their result implies a separation between P and NP, since the lower bound\npurportedly proves that the Strong Exponential Time Hypothesis (SETH) is true.\nIn examining Xu and Zhou's arguments, we find a flaw in their main theorems.\nThe authors assume that an algorithm for Model RB must have a certain structure\nthat can leverage downward self-reducibility, and argue that such an algorithm\ncannot run in polynomial time. We argue that this structure is not guaranteed\nto exist and thus their paper neither proves SETH to be true nor proves P\n$\\neq$ NP.",
            "author": [
                "Michael C. Chavrimootoo",
                "Yumeng He",
                "Matan Kotler-Berkowitz",
                "Harry Liuson",
                "Zeyu Nie"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02071v1",
                "http://arxiv.org/pdf/2312.02071v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02069v1",
            "title": "GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians",
            "updated": "2023-12-04T17:28:35Z",
            "published": "2023-12-04T17:28:35Z",
            "summary": "We introduce GaussianAvatars, a new method to create photorealistic head\navatars that are fully controllable in terms of expression, pose, and\nviewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian\nsplats that are rigged to a parametric morphable face model. This combination\nfacilitates photorealistic rendering while allowing for precise animation\ncontrol via the underlying parametric model, e.g., through expression transfer\nfrom a driving sequence or by manually changing the morphable model parameters.\nWe parameterize each splat by a local coordinate frame of a triangle and\noptimize for explicit displacement offset to obtain a more accurate geometric\nrepresentation. During avatar reconstruction, we jointly optimize for the\nmorphable model parameters and Gaussian splat parameters in an end-to-end\nfashion. We demonstrate the animation capabilities of our photorealistic avatar\nin several challenging scenarios. For instance, we show reenactments from a\ndriving video, where our method outperforms existing works by a significant\nmargin.",
            "author": [
                "Shenhan Qian",
                "Tobias Kirschstein",
                "Liam Schoneveld",
                "Davide Davoli",
                "Simon Giebenhain",
                "Matthias Nie\u00dfner"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02069v1",
                "http://arxiv.org/pdf/2312.02069v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02249v1",
            "title": "Recursive Visual Programming",
            "updated": "2023-12-04T17:27:24Z",
            "published": "2023-12-04T17:27:24Z",
            "summary": "Visual Programming (VP) has emerged as a powerful framework for Visual\nQuestion Answering (VQA). By generating and executing bespoke code for each\nquestion, these methods demonstrate impressive compositional and reasoning\ncapabilities, especially in few-shot and zero-shot scenarios. However, existing\nVP methods generate all code in a single function, resulting in code that is\nsuboptimal in terms of both accuracy and interpretability. Inspired by human\ncoding practices, we propose Recursive Visual Programming (RVP), which\nsimplifies generated routines, provides more efficient problem solving, and can\nmanage more complex data structures. RVP is inspired by human coding practices\nand approaches VQA tasks with an iterative recursive code generation approach,\nallowing decomposition of complicated problems into smaller parts. Notably, RVP\nis capable of dynamic type assignment, i.e., as the system recursively\ngenerates a new piece of code, it autonomously determines the appropriate\nreturn type and crafts the requisite code to generate that output. We show\nRVP's efficacy through extensive experiments on benchmarks including VSR, COVR,\nGQA, and NextQA, underscoring the value of adopting human-like recursive and\nmodular programming techniques for solving VQA tasks through coding.",
            "author": [
                "Jiaxin Ge",
                "Sanjay Subramanian",
                "Baifeng Shi",
                "Roei Herzig",
                "Trevor Darrell"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02249v1",
                "http://arxiv.org/pdf/2312.02249v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02065v1",
            "title": "Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?",
            "updated": "2023-12-04T17:19:53Z",
            "published": "2023-12-04T17:19:53Z",
            "summary": "Large language models (LLMs) offer a range of new possibilities, including\nadapting the text to different audiences and their reading needs. But how well\ndo they adapt? We evaluate the readability of answers generated by four\nstate-of-the-art LLMs (commercial and open-source) to science questions when\nprompted to target different age groups and education levels. To assess the\nadaptability of LLMs to diverse audiences, we compare the readability scores of\nthe generated responses against the recommended comprehension level of each age\nand education group. We find large variations in the readability of the answers\nby different LLMs. Our results suggest LLM answers need to be better adapted to\nthe intended audience demographics to be more comprehensible. They underline\nthe importance of enhancing the adaptability of LLMs in education settings to\ncater to diverse age and education levels. Overall, current LLMs have set\nreadability ranges and do not adapt well to different audiences, even when\nprompted. That limits their potential for educational purposes.",
            "author": [
                "Donya Rooein",
                "Amanda Cercas Curry",
                "Dirk Hovy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02065v1",
                "http://arxiv.org/pdf/2312.02065v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02062v1",
            "title": "Ultraviolet H$_2$ luminescence in molecular clouds induced by cosmic\n  rays",
            "updated": "2023-12-04T17:19:21Z",
            "published": "2023-12-04T17:19:21Z",
            "summary": "Galactic cosmic rays (CRs) play a crucial role in ionisation, dissociation,\nand excitation processes within dense cloud regions where UV radiation is\nabsorbed by dust grains and gas species. CRs regulate the abundance of ions and\nradicals, leading to the formation of more and more complex molecular species,\nand determine the charge distribution on dust grains. A quantitative analysis\nof these effects is essential for understanding the dynamical and chemical\nevolution of star-forming regions. The CR-induced photon flux has a significant\nimpact on the evolution of the dense molecular medium in its gas and dust\ncomponents. This study is intended to evaluate the flux of UV photons generated\nby CRs to calculate the photon-induced dissociation and ionisation rates of a\nvast number of atomic and molecular species, as well as the integrated UV\nphoton flux. Our study takes advantage of recent developments in the\ndetermination of the spectra of secondary electrons, in the calculation of\nstate-resolved excitation cross sections of H$_2$ by electron impact, and of\nphotodissociation and photoionisation cross sections. We calculate the H$_2$\nlevel population of each rovibrational level of the $X$, $B$, $C$, $B'$, $D$,\n$B''$, $D'$ and $a$ states. We then compute the UV photon spectrum of H$_2$ in\nits line and continuum components between 72 and 700 nm, with unprecedented\naccuracy as a function of the CR spectrum incident on a molecular cloud, the\nH$_2$ column density, the isomeric H$_2$ composition, and the dust properties.\nThe resulting photodissociation and photoionisation rates are, on average,\nsmaller than previous determinations by a factor of about 2, with deviations up\nto a factor of 5. A special focus is given to the photoionisation rates of\nH$_2$, HF, and H$_2$, as well as to the photodissociation of H$_2$, which we\nfind to be orders of magnitude higher than previous estimates.",
            "author": [
                "Marco Padovani",
                "Daniele Galli",
                "Liam H. Scarlett",
                "Tommaso Grassi",
                "Una S. Rehill",
                "Mark C. Zammit",
                "Igor Bray",
                "Dmitry V. Fursa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02062v1",
                "http://arxiv.org/pdf/2312.02062v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02061v1",
            "title": "Analysis of Neutron Star $f-$mode Oscillations in General Relativity\n  with Spectral Representation of Nuclear Equations of State",
            "updated": "2023-12-04T17:19:07Z",
            "published": "2023-12-04T17:19:07Z",
            "summary": "We conduct a detailed analysis of quasinormal $f-$mode frequencies in neutron\nstars (NS), within the linearized General Relativistic formalism. From Bayesian\ninference, we derived approximately 9000 nuclear Equations of State (EOS)\nsubject to various constraints including nuclear saturation properties, the\npure neutron matter EOS constraint obtained within $\\chi$EFT, and pQCD at\ndensities relevant to NS cores. The composition and oscillatory dynamics of NS\nare then investigated using this set. The EOS are transformed into a spectral\nrepresentation, aiding in the efficient computation of NS properties. The\nmedian frequency values of the $f-$mode for NS with masses ranging from\n1.4$M_\\odot$ to 2.0$M_{\\odot}$ lie between 1.80 and 2.20 kHz for our entire EOS\nset. Our findings do not reveal a strong correlation between $f-$mode\nfrequencies and individual nuclear saturation properties of the EOS. This\nsuggests the need for more complex methods to unravel multiple-parameter\nrelationships. We noticed a strong relationship between the radii and $f-$mode\nfrequencies for different NS masses. Using this correlation along with NICER\nobservations of PSR J0740+6620 and PSR 0030+0451, we obtained constraints that\nhave minimal overlap in the radius domain and differ in the frequency domain\nfrom our entire nucleonic EOS set. This indicates that there may be a need to\nconsider additional exotic particles or maybe a deconfined quark phase in the\nEOS relevant to the NS core. We argue that future observations of the radius or\n$f-$mode frequency for more than one NS mass, particularly at the extremes, are\nlikely to settle the issue by either ruling out only nucleonic EOS or providing\ndefinitive evidence in its favour.",
            "author": [
                "Debanjan Guha Roy",
                "Tuhin Malik",
                "Swastik Bhattacharya",
                "Sarmistha Banik"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02061v1",
                "http://arxiv.org/pdf/2312.02061v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "gr-qc",
                "nucl-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02060v1",
            "title": "Right-sizing compute resource allocations for bioinformatics tools with\n  Total Perspective Vortex",
            "updated": "2023-12-04T17:18:24Z",
            "published": "2023-12-04T17:18:24Z",
            "summary": "In biomedical research, computational methods have become indispensable and\ntheir use is increasing, making the efficient allocation of computing resources\nparamount. Practitioners routinely allocate resources far in excess of what is\nrequired for batch processing jobs, leading to not just inflated wait times and\ncosts, but also unnecessary carbon emissions. This is not without reason\nhowever, as accurately determining resource needs is complex, affected by the\nnature of tools, data size, and analysis parameters, especially on popular\nservers that handle numerous jobs. The Galaxy platform, a web-based hub for\nbiomedical analysis used globally by scientists, exemplifies this challenge.\nServing nearly half a million registered users and managing around 2 million\nmonthly jobs, Galaxy's growth outpaces the resources at its disposal. This is\nnecessitating smarter resource utilization. To address this, we have developed\na tool named Total Perspective Vortex (TPV) - a software package that\nright-sizes resource allocations for each job. TPV is able to dynamically set\nresource requirements for individual jobs and perform meta-scheduling across\nheterogeneous resources. It also includes a first-ever community-curated\ndatabase of default resource requirements for nearly 1,000 popular\nbioinformatics tools. Deployments in Galaxy Australia and Europe demonstrate\nits effectiveness with meta-scheduling user jobs and an improved experience for\nsystems administrators managing Galaxy servers.",
            "author": [
                "Nuwan Goonasekera",
                "Catherine Bromhead",
                "Simon Gladman",
                "Nate Coraor",
                "Bjorn Gruning",
                "Enis Afgan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02060v1",
                "http://arxiv.org/pdf/2312.02060v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02057v1",
            "title": "An improved bound on sums of square roots via the subspace theorem",
            "updated": "2023-12-04T17:15:12Z",
            "published": "2023-12-04T17:15:12Z",
            "summary": "The sum of square roots is as follows: Given $x_1,\\dots,x_n \\in \\mathbb{Z}$\nand $a_1,\\dots,a_n \\in \\mathbb{N}$ decide whether $ E=\\sum_{i=1}^n x_i\n\\sqrt{a_i} \\geq 0$. It is a prominent open problem (Problem 33 of the Open\nProblems Project), whether this can be decided in polynomial time. The\nstate-of-the-art methods rely on separation bounds, which are lower bounds on\nthe minimum nonzero absolute value of $E$. The current best bound shows that\n$|E| \\geq \\left(n \\cdot \\max_i (|x_i| \\cdot \\sqrt{a_i})\\right)^{-2^n} $, which\nis doubly exponentially small.\n  We provide a new bound of the form $|E| \\geq \\gamma \\cdot (n \\cdot\n\\max_i|x_i|)^{-2n}$ where $\\gamma $ is a constant depending on $a_1,\\dots,a_n$.\nThis is singly exponential in $n$ for fixed $a_1,\\dots,a_n$. The constant\n$\\gamma$ is not explicit and stems from the subspace theorem, a deep result in\nthe geometry of numbers.",
            "author": [
                "Friedrich Eisenbrand",
                "Matthieu Haeberle",
                "Neta Singer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02057v1",
                "http://arxiv.org/pdf/2312.02057v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02055v1",
            "title": "Transaction Ordering Auctions",
            "updated": "2023-12-04T17:14:06Z",
            "published": "2023-12-04T17:14:06Z",
            "summary": "We study equilibrium investment into bidding and latency reduction for\ndifferent sequencing policies. For a batch auction design, we observe that\nbidders shade bids according to the likelihood that competing bidders land in\nthe current batch. Moreover, in equilibrium, in the ex-ante investment stage\nbefore the auction, bidders invest into latency until they make zero profit in\nexpectation.\n  We compare the batch auction design to continuous time bidding policies (time\nboost) and observe that (depending on the choice of parameters) they obtain\nsimilar revenue and welfare guarantees.",
            "author": [
                "Jan Christoph Schlegel"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02055v1",
                "http://arxiv.org/pdf/2312.02055v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "econ.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02052v1",
            "title": "DUCK: Distance-based Unlearning via Centroid Kinematics",
            "updated": "2023-12-04T17:10:25Z",
            "published": "2023-12-04T17:10:25Z",
            "summary": "Machine Unlearning is rising as a new field, driven by the pressing necessity\nof ensuring privacy in modern artificial intelligence models. This technique\nprimarily aims to eradicate any residual influence of a specific subset of data\nfrom the knowledge acquired by a neural model during its training. This work\nintroduces a novel unlearning algorithm, denoted as Distance-based Unlearning\nvia Centroid Kinematics (DUCK), which employs metric learning to guide the\nremoval of samples matching the nearest incorrect centroid in the embedding\nspace. Evaluation of the algorithm's performance is conducted across various\nbenchmark datasets in two distinct scenarios, class removal, and homogeneous\nsampling removal, obtaining state-of-the-art performance. We introduce a novel\nmetric, called Adaptive Unlearning Score (AUS), encompassing not only the\nefficacy of the unlearning process in forgetting target data but also\nquantifying the performance loss relative to the original model. Moreover, we\npropose a novel membership inference attack to assess the algorithm's capacity\nto erase previously acquired knowledge, designed to be adaptable to future\nmethodologies.",
            "author": [
                "Marco Cotogni",
                "Jacopo Bonato",
                "Luigi Sabetta",
                "Francesco Pelosin",
                "Alessandro Nicolosi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02052v1",
                "http://arxiv.org/pdf/2312.02052v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03755v1",
            "title": "Near-real-time Earthquake-induced Fatality Estimation using Crowdsourced\n  Data and Large-Language Models",
            "updated": "2023-12-04T17:09:58Z",
            "published": "2023-12-04T17:09:58Z",
            "summary": "When a damaging earthquake occurs, immediate information about casualties is\ncritical for time-sensitive decision-making by emergency response and aid\nagencies in the first hours and days. Systems such as Prompt Assessment of\nGlobal Earthquakes for Response (PAGER) by the U.S. Geological Survey (USGS)\nwere developed to provide a forecast within about 30 minutes of any significant\nearthquake globally. Traditional systems for estimating human loss in disasters\noften depend on manually collected early casualty reports from global media, a\nprocess that's labor-intensive and slow with notable time delays. Recently,\nsome systems have employed keyword matching and topic modeling to extract\nrelevant information from social media. However, these methods struggle with\nthe complex semantics in multilingual texts and the challenge of interpreting\never-changing, often conflicting reports of death and injury numbers from\nvarious unverified sources on social media platforms. In this work, we\nintroduce an end-to-end framework to significantly improve the timeliness and\naccuracy of global earthquake-induced human loss forecasting using\nmulti-lingual, crowdsourced social media. Our framework integrates (1) a\nhierarchical casualty extraction model built upon large language models, prompt\ndesign, and few-shot learning to retrieve quantitative human loss claims from\nsocial media, (2) a physical constraint-aware, dynamic-truth discovery model\nthat discovers the truthful human loss from massive noisy and potentially\nconflicting human loss claims, and (3) a Bayesian updating loss projection\nmodel that dynamically updates the final loss estimation using discovered\ntruths. We test the framework in real-time on a series of global earthquake\nevents in 2021 and 2022 and show that our framework streamlines casualty data\nretrieval, achieving speed and accuracy comparable to manual methods by USGS.",
            "author": [
                "Chenguang Wang",
                "Davis Engler",
                "Xuechun Li",
                "James Hou",
                "David J. Wald",
                "Kishor Jaiswal",
                "Susu Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03755v1",
                "http://arxiv.org/pdf/2312.03755v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02051v1",
            "title": "TimeChat: A Time-sensitive Multimodal Large Language Model for Long\n  Video Understanding",
            "updated": "2023-12-04T17:09:52Z",
            "published": "2023-12-04T17:09:52Z",
            "summary": "This work proposes TimeChat, a time-sensitive multimodal large language model\nspecifically designed for long video understanding. Our model incorporates two\nkey architectural contributions: (1) a timestamp-aware frame encoder that binds\nvisual content with the timestamp of each frame, and (2) a sliding video\nQ-Former that produces a video token sequence of varying lengths to accommodate\nvideos of various durations. Additionally, we construct an instruction-tuning\ndataset, encompassing 6 tasks and a total of 125K instances, to further enhance\nTimeChat's instruction-following performance. Experiment results across various\nvideo understanding tasks, such as dense captioning, temporal grounding, and\nhighlight detection, demonstrate TimeChat's strong zero-shot temporal\nlocalization and reasoning capabilities. For example, it achieves +9.2 F1 score\nand +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5)\non Charades-STA, compared to state-of-the-art video large language models,\nholding the potential to serve as a versatile video assistant for long-form\nvideo comprehension tasks and satisfy realistic user requirements.",
            "author": [
                "Shuhuai Ren",
                "Linli Yao",
                "Shicheng Li",
                "Xu Sun",
                "Lu Hou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02051v1",
                "http://arxiv.org/pdf/2312.02051v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02039v1",
            "title": "Entanglement-magic separation in hybrid quantum circuits",
            "updated": "2023-12-04T16:57:33Z",
            "published": "2023-12-04T16:57:33Z",
            "summary": "Magic describes the distance of a quantum state to its closest stabilizer\nstate. It is -- like entanglement -- a necessary resource for a potential\nquantum advantage over classical computing. We study magic, quantified by\nstabilizer entropy, in a hybrid quantum circuit with projective measurements\nand a controlled injection of non-Clifford resources. We discover a phase\ntransition between a (sub)-extensive and area law scaling of magic controlled\nby the rate of measurements. The same circuit also exhibits a phase transition\nin entanglement that appears, however, at a different critical measurement\nrate. This mechanism shows how, from the viewpoint of a potential quantum\nadvantage, hybrid circuits can host multiple distinct transitions where not\nonly entanglement, but also other non-linear properties of the density matrix\ncome into play.",
            "author": [
                "Gerald E. Fux",
                "Emanuele Tirrito",
                "Marcello Dalmonte",
                "Rosario Fazio"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02039v1",
                "http://arxiv.org/pdf/2312.02039v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02034v1",
            "title": "Trust, distrust, and appropriate reliance in (X)AI: a survey of\n  empirical evaluation of user trust",
            "updated": "2023-12-04T16:53:11Z",
            "published": "2023-12-04T16:53:11Z",
            "summary": "A current concern in the field of Artificial Intelligence (AI) is to ensure\nthe trustworthiness of AI systems. The development of explainability methods is\none prominent way to address this, which has often resulted in the assumption\nthat the use of explainability will lead to an increase in the trust of users\nand wider society. However, the dynamics between explainability and trust are\nnot well established and empirical investigations of their relation remain\nmixed or inconclusive. In this paper we provide a detailed description of the\nconcepts of user trust and distrust in AI and their relation to appropriate\nreliance. For that we draw from the fields of machine learning, human-computer\ninteraction, and the social sciences. Furthermore, we have created a survey of\nexisting empirical studies that investigate the effects of AI systems and XAI\nmethods on user (dis)trust. With clarifying the concepts and summarizing the\nempirical investigations, we aim to provide researchers, who examine user trust\nin AI, with an improved starting point for developing user studies to measure\nand evaluate the user's attitude towards and reliance on AI systems.",
            "author": [
                "Roel Visser",
                "Tobias M. Peters",
                "Ingrid Scharlau",
                "Barbara Hammer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02034v1",
                "http://arxiv.org/pdf/2312.02034v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02029v1",
            "title": "Implicit Learning of Scene Geometry from Poses for Global Localization",
            "updated": "2023-12-04T16:51:23Z",
            "published": "2023-12-04T16:51:23Z",
            "summary": "Global visual localization estimates the absolute pose of a camera using a\nsingle image, in a previously mapped area. Obtaining the pose from a single\nimage enables many robotics and augmented/virtual reality applications.\nInspired by latest advances in deep learning, many existing approaches directly\nlearn and regress 6 DoF pose from an input image. However, these methods do not\nfully utilize the underlying scene geometry for pose regression. The challenge\nin monocular relocalization is the minimal availability of supervised training\ndata, which is just the corresponding 6 DoF poses of the images. In this paper,\nwe propose to utilize these minimal available labels (.i.e, poses) to learn the\nunderlying 3D geometry of the scene and use the geometry to estimate the 6 DoF\ncamera pose. We present a learning method that uses these pose labels and rigid\nalignment to learn two 3D geometric representations (\\textit{X, Y, Z\ncoordinates}) of the scene, one in camera coordinate frame and the other in\nglobal coordinate frame. Given a single image, it estimates these two 3D scene\nrepresentations, which are then aligned to estimate a pose that matches the\npose label. This formulation allows for the active inclusion of additional\nlearning constraints to minimize 3D alignment errors between the two 3D scene\nrepresentations, and 2D re-projection errors between the 3D global scene\nrepresentation and 2D image pixels, resulting in improved localization\naccuracy. During inference, our model estimates the 3D scene geometry in camera\nand global frames and aligns them rigidly to obtain pose in real-time. We\nevaluate our work on three common visual localization datasets, conduct\nablation studies, and show that our method exceeds state-of-the-art regression\nmethods' pose accuracy on all datasets.",
            "author": [
                "Mohammad Altillawi",
                "Shile Li",
                "Sai Manoj Prakhya",
                "Ziyuan Liu",
                "Joan Serrat"
            ],
            "link": [
                "http://dx.doi.org/10.1109/LRA.2023.3337699",
                "http://arxiv.org/abs/2312.02029v1",
                "http://arxiv.org/pdf/2312.02029v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02021v1",
            "title": "VLTSeg: Simple Transfer of CLIP-Based Vision-Language Representations\n  for Domain Generalized Semantic Segmentation",
            "updated": "2023-12-04T16:46:38Z",
            "published": "2023-12-04T16:46:38Z",
            "summary": "Domain generalization (DG) remains a significant challenge for perception\nbased on deep neural networks (DNN), where domain shifts occur due to lighting,\nweather, or geolocation changes. In this work, we propose VLTSeg to enhance\ndomain generalization in semantic segmentation, where the network is solely\ntrained on the source domain and evaluated on unseen target domains. Our method\nleverages the inherent semantic robustness of vision-language models. First, by\nsubstituting traditional vision-only backbones with pre-trained encoders from\nCLIP and EVA-CLIP as transfer learning setting we find that in the field of DG,\nvision-language pre-training significantly outperforms supervised and\nself-supervised vision pre-training. We thus propose a new vision-language\napproach for domain generalized segmentation, which improves the domain\ngeneralization SOTA by 7.6% mIoU when training on the synthetic GTA5 dataset.\nWe further show the superior generalization capabilities of vision-language\nsegmentation models by reaching 76.48% mIoU on the popular Cityscapes-to-ACDC\nbenchmark, outperforming the previous SOTA approach by 6.9% mIoU on the test\nset at the time of writing. Additionally, our approach shows strong in-domain\ngeneralization capabilities indicated by 86.1% mIoU on the Cityscapes test set,\nresulting in a shared first place with the previous SOTA on the current\nleaderboard at the time of submission.",
            "author": [
                "Christoph H\u00fcmmer",
                "Manuel Schwonberg",
                "Liangwei Zhong",
                "Hu Cao",
                "Alois Knoll",
                "Hanno Gottschalk"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02021v1",
                "http://arxiv.org/pdf/2312.02021v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02020v1",
            "title": "H\u00fcckel Molecular Orbital Theory on a Quantum Computer: A Scalable\n  System-Agnostic Variational Implementation with Compact Encoding",
            "updated": "2023-12-04T16:44:53Z",
            "published": "2023-12-04T16:44:53Z",
            "summary": "H\\\"uckel molecular orbital (HMO) theory provides a semi-empirical treatment\nof the electronic structure in conjugated {\\pi}-electronic systems. A scalable\nsystem-agnostic execution of HMO theory on a quantum computer is reported here\nbased on a variational quantum deflation (VQD) algorithm for excited state\nquantum simulation. A compact encoding scheme is proposed here that provides an\nexponential advantage over direct mapping and allows quantum simulation of the\nHMO model for systems with up to 2^N conjugated centers in N qubits. The\ntransformation of the H\\\"uckel Hamiltonian to qubit space is achieved by two\ndifferent strategies: a machine-learning-assisted transformation and the\nFrobenius-inner-product-based transformation. These methods are tested on a\nseries of linear, cyclic, and hetero-nuclear conjugated {\\pi}-electronic\nsystems. The molecular orbital energy levels and wavefunctions from the quantum\nsimulation are in excellent agreement with the exact classical results. The\nhigher excited states of large systems, however, are found to suffer from error\naccumulation in the VQD simulation. This is mitigated by formulating a variant\nof VQD that exploits the symmetry of the Hamiltonian. This strategy has been\nsuccessfully demonstrated for the quantum simulation of C_{60} fullerene\ncontaining 680 Pauli strings encoded on six qubits. The methods developed in\nthis work are system-agnostic and hence are easily adaptable to similar\nproblems of different complexity in other fields of research.",
            "author": [
                "Harshdeep Singh",
                "Sonjoy Majumder",
                "Sabyashachi Mishra"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02020v1",
                "http://arxiv.org/pdf/2312.02020v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02018v1",
            "title": "Computational Investigation on Collective Dynamical Behaviors of\n  Flickering Laminar Buoyant Diffusion Flames in Circular Arrays",
            "updated": "2023-12-04T16:42:50Z",
            "published": "2023-12-04T16:42:50Z",
            "summary": "The emergence of collective dynamical behaviors in a complex system has\ndistinct properties that its individuals do not have on their own. In the\nstudy, a series of circular arrays of octuple flickering laminar buoyant\ndiffusion flames were computationally investigated to understand their\ncollective behaviors. Five distinct dynamical modes, such as the merged,\nin-phase mode, rotation, flickering death, partially flickering death, and\nanti-phase modes, were identified and interpreted from the perspective of\nvortex dynamics. All the modes were found to be controlled by the three\ndimensionless parameters, namely the normalized flame frequency f/f_0, the\nratio of the flame separation distance to the flame diameter, and the Grashof\nnumber Gr. A unified regime diagram was obtained in terms of f/f_0 and of a\ncombined Reynolds-number-like parameter. In addition, the bifurcation\ntransition from the in-phase mode and the anti-phase mode to the totally or\npartially flickering death occurs at 620+-50.",
            "author": [
                "Tao Yang",
                "Yuan Ma",
                "Peng Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02018v1",
                "http://arxiv.org/pdf/2312.02018v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02017v1",
            "title": "A multi-channel cycleGAN for CBCT to CT synthesis",
            "updated": "2023-12-04T16:40:53Z",
            "published": "2023-12-04T16:40:53Z",
            "summary": "Image synthesis is used to generate synthetic CTs (sCTs) from on-treatment\ncone-beam CTs (CBCTs) with a view to improving image quality and enabling\naccurate dose computation to facilitate a CBCT-based adaptive radiotherapy\nworkflow. As this area of research gains momentum, developments in sCT\ngeneration methods are difficult to compare due to the lack of large public\ndatasets and sizeable variation in training procedures. To compare and assess\nthe latest advancements in sCT generation, the SynthRAD2023 challenge provides\na public dataset and evaluation framework for both MR and CBCT to sCT\nsynthesis. Our contribution focuses on the second task, CBCT-to-sCT synthesis.\nBy leveraging a multi-channel input to emphasize specific image features, our\napproach effectively addresses some of the challenges inherent in CBCT imaging,\nwhilst restoring the contrast necessary for accurate visualisation of patients'\nanatomy. Additionally, we introduce an auxiliary fusion network to further\nenhance the fidelity of generated sCT images.",
            "author": [
                "Chelsea A. H. Sargeant",
                "Edward G. A. Henderson",
                "D\u00f3nal M. McSweeney",
                "Aaron G. Rankin",
                "Denis Page"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02017v1",
                "http://arxiv.org/pdf/2312.02017v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "physics.med-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02016v1",
            "title": "Combinatorial Disjunctive Constraints for Obstacle Avoidance in Path\n  Planning",
            "updated": "2023-12-04T16:38:43Z",
            "published": "2023-12-04T16:38:43Z",
            "summary": "We present a new approach for modeling avoidance constraints in 2D\nenvironments, in which waypoints are assigned to obstacle-free polyhedral\nregions. Constraints of this form are often formulated as mixed-integer\nprogramming (MIP) problems employing big-M techniques -- however, these are\ngenerally not the strongest formulations possible with respect to the MIP's\nconvex relaxation (so called ideal formulations), potentially resulting in\nlarger computational burden. We instead model obstacle avoidance as\ncombinatorial disjunctive constraints and leverage the independent branching\nscheme to construct small, ideal formulations. As our approach requires a\nbiclique cover for an associated graph, we exploit the structure of this class\nof graphs to develop a fast subroutine for obtaining biclique covers in\npolynomial time. We also contribute an open-source Julia library named\nClutteredEnvPathOpt to facilitate computational experiments of MIP formulations\nfor obstacle avoidance. Experiments have shown our formulation is more compact\nand remains competitive on a number of instances compared with standard big-M\ntechniques, for which solvers possess highly optimized procedures.",
            "author": [
                "Raul Garcia",
                "Illya V. Hicks",
                "Joey Huchette"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02016v1",
                "http://arxiv.org/pdf/2312.02016v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02015v1",
            "title": "ColonNeRF: Neural Radiance Fields for High-Fidelity Long-Sequence\n  Colonoscopy Reconstruction",
            "updated": "2023-12-04T16:38:16Z",
            "published": "2023-12-04T16:38:16Z",
            "summary": "Colonoscopy reconstruction is pivotal for diagnosing colorectal cancer.\nHowever, accurate long-sequence colonoscopy reconstruction faces three major\nchallenges: (1) dissimilarity among segments of the colon due to its meandering\nand convoluted shape; (2) co-existence of simple and intricately folded\ngeometry structures; (3) sparse viewpoints due to constrained camera\ntrajectories. To tackle these challenges, we introduce a new reconstruction\nframework based on neural radiance field (NeRF), named ColonNeRF, which\nleverages neural rendering for novel view synthesis of long-sequence\ncolonoscopy. Specifically, to reconstruct the entire colon in a piecewise\nmanner, our ColonNeRF introduces a region division and integration module,\neffectively reducing shape dissimilarity and ensuring geometric consistency in\neach segment. To learn both the simple and complex geometry in a unified\nframework, our ColonNeRF incorporates a multi-level fusion module that\nprogressively models the colon regions from easy to hard. Additionally, to\novercome the challenges from sparse views, we devise a DensiNet module for\ndensifying camera poses under the guidance of semantic consistency. We conduct\nextensive experiments on both synthetic and real-world datasets to evaluate our\nColonNeRF. Quantitatively, our ColonNeRF outperforms existing methods on two\nbenchmarks over four evaluation metrics. Notably, our LPIPS-ALEX scores exhibit\na substantial increase of about 67%-85% on the SimCol-to-3D dataset.\nQualitatively, our reconstruction visualizations show much clearer textures and\nmore accurate geometric details. These sufficiently demonstrate our superior\nperformance over the state-of-the-art methods.",
            "author": [
                "Yufei Shi",
                "Beijia Lu",
                "Jia-Wei Liu",
                "Ming Li",
                "Mike Zheng Shou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02015v1",
                "http://arxiv.org/pdf/2312.02015v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02014v1",
            "title": "The cohomology of homogeneous spaces in historical context",
            "updated": "2023-12-04T16:37:32Z",
            "published": "2023-12-04T16:37:32Z",
            "summary": "The real singular cohomology ring of a homogeneous space $G/K$, interpreted\nas the real Borel equivariant cohomology $H^*_K(G)$, was historically the first\ncomputation of equivariant cohomology of any nontrivial connected group action.\nAfter early approaches using the Cartan model for equivariant cohomology with\nreal coefficients and the Serre spectral sequence, post-1962 work computing the\ngroups and rings $H^*(G/K)$ and $H^*_H(G/K)$ with more general coefficient\nrings motivated the development of minimal models in rational homotopy theory,\nthe Eilenberg-Moore spectral sequence, and A-infinity algebras.\n  In this essay, we survey the history of these ideas and the associated\nresults.",
            "author": [
                "Jeffrey D. Carlson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02014v1",
                "http://arxiv.org/pdf/2312.02014v1"
            ],
            "primary_category": "math.AT",
            "category": [
                "math.AT",
                "math.HO",
                "math.KT",
                "55-03, 57T15, 57T35, 16U80 (Primary), 01A60, 01A61, 57T30\n  (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02012v1",
            "title": "Optimal Data Generation in Multi-Dimensional Parameter Spaces, using\n  Bayesian Optimization",
            "updated": "2023-12-04T16:36:29Z",
            "published": "2023-12-04T16:36:29Z",
            "summary": "Acquiring a substantial number of data points for training accurate machine\nlearning (ML) models is a big challenge in scientific fields where data\ncollection is resource-intensive. Here, we propose a novel approach for\nconstructing a minimal yet highly informative database for training ML models\nin complex multi-dimensional parameter spaces. To achieve this, we mimic the\nunderlying relation between the output and input parameters using Gaussian\nprocess regression (GPR). Using a set of known data, GPR provides predictive\nmeans and standard deviation for the unknown data. Given the predicted standard\ndeviation by GPR, we select data points using Bayesian optimization to obtain\nan efficient database for training ML models. We compare the performance of ML\nmodels trained on databases obtained through this method, with databases\nobtained using traditional approaches. Our results demonstrate that the ML\nmodels trained on the database obtained using Bayesian optimization approach\nconsistently outperform the other two databases, achieving high accuracy with a\nsignificantly smaller number of data points. Our work contributes to the\nresource-efficient collection of data in high-dimensional complex parameter\nspaces, to achieve high precision machine learning predictions.",
            "author": [
                "M. R. Mahani",
                "Igor A. Nechepurenko",
                "Yasmin Rahimof",
                "Andreas Wicht"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02012v1",
                "http://arxiv.org/pdf/2312.02012v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.app-ph",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02011v1",
            "title": "What is the disinformation problem? Reviewing the dominant paradigm and\n  motivating an alternative sociopolitical view",
            "updated": "2023-12-04T16:34:31Z",
            "published": "2023-12-04T16:34:31Z",
            "summary": "Disinformation research has proliferated in reaction to widespread false,\nproblematic beliefs purported to explain major social phenomena. Yet while the\neffects of disinformation are well-known, there is less consensus about its\ncauses; the research spans several disciplines, each focusing on different\npieces. This article contributes to this growing field by reviewing prevalent\nU.S. disinformation discourse (academic writing, media, and corporate and\ngovernment narrative) and outlining the dominant understanding, or paradigm, of\nthe disinformation problem by analyzing cross-disciplinary discourse about the\ncontent, individual, group, and institutional layers of the problem. The result\nis an individualistic explanation largely blaming social media, malicious\nindividuals or nations, and irrational people. Yet this understanding has\nshortcomings: notably, that its limited, individualistic views of truth and\nrationality obscures the influence of oppressive ideologies and media or\ndomestic actors in creating flawed worldviews and spreading disinformation. The\narticle then concludes by putting forth an alternative, sociopolitical paradigm\nthat allows subjective models of the world to govern rationality and\ninformation processing -- largely informed by social and group identity --\nwhich are being formed and catered to by institutional actors (corporations,\nmedia, political parties, and the government) to maintain or gain legitimacy\nfor their actions.",
            "author": [
                "Nicholas Rabb"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02011v1",
                "http://arxiv.org/pdf/2312.02011v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02010v2",
            "title": "Towards Learning a Generalist Model for Embodied Navigation",
            "updated": "2023-12-06T08:13:28Z",
            "published": "2023-12-04T16:32:51Z",
            "summary": "Building a generalist agent that can interact with the world is the\nintriguing target of AI systems, thus spurring the research for embodied\nnavigation, where an agent is required to navigate according to instructions or\nrespond to queries. Despite the major progress attained, previous works\nprimarily focus on task-specific agents and lack generalizability to unseen\nscenarios. Recently, LLMs have presented remarkable capabilities across various\nfields, and provided a promising opportunity for embodied navigation. Drawing\non this, we propose the first generalist model for embodied navigation,\nNaviLLM. It adapts LLMs to embodied navigation by introducing schema-based\ninstruction. The schema-based instruction flexibly casts various tasks into\ngeneration problems, thereby unifying a wide range of tasks. This approach\nallows us to integrate diverse data sources from various datasets into the\ntraining, equipping NaviLLM with a wide range of capabilities required by\nembodied navigation. We conduct extensive experiments to evaluate the\nperformance and generalizability of our model. The experimental results\ndemonstrate that our unified model achieves state-of-the-art performance on\nCVDN, SOON, and ScanQA. Specifically, it surpasses the previous\nstats-of-the-art method by a significant margin of 29% in goal progress on\nCVDN. Moreover, our model also demonstrates strong generalizability and\npresents impressive results on unseen tasks, e.g., embodied question answering\nand 3D captioning.",
            "author": [
                "Duo Zheng",
                "Shijia Huang",
                "Lin Zhao",
                "Yiwu Zhong",
                "Liwei Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02010v2",
                "http://arxiv.org/pdf/2312.02010v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02002v1",
            "title": "End-to-End Demonstration for CubeSatellite Quantum Key Distribution",
            "updated": "2023-12-04T16:25:06Z",
            "published": "2023-12-04T16:25:06Z",
            "summary": "Quantum key distribution (QKD) provides a method of ensuring security using\nthe laws of physics, avoiding the risks inherent in cryptosystems protected by\ncomputational complexity. Here we investigate the feasibility of\nsatellite-based quantum key exchange using low-cost compact nano-satellites.\nThis paper demonstrates the first prototype of system level quantum key\ndistribution aimed at the Cube satellite scenario. It consists of a transmitter\npayload, a ground receiver and simulated free space channel to verify the\ntiming and synchronisation (T&S) scheme designed for QKD and the required high\nloss tolerance of both QKD and T&S channels. The transmitter is designed to be\ndeployed on various up-coming nano-satellite missions in the UK and\ninternationally. The effects of channel loss, background noise, gate width and\nmean photon number on the secure key rate (SKR) and quantum bit error rate\n(QBER) are discussed. We also analyse the source of QBER and establish the\nrelationship between effective signal noise ratio (ESNR) and noise level,\nsignal strength, gating window and other parameters as a reference for SKR\noptimization. The experiment shows that it can tolerate the 40 dB loss expected\nin space to ground QKD and with small adjustment decoy states can be achieved.\nThe discussion offers valuable insight not only for the design and optimization\nof miniature low-cost satellite-based QKD systems but also any other short or\nlong range free space QKD on the ground or in the air.",
            "author": [
                "Peide Zhang",
                "Jaya Sagar",
                "Elliott Hasting",
                "Milan Stefko",
                "Siddarth Joshi",
                "John Rarity"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02002v1",
                "http://arxiv.org/pdf/2312.02002v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01999v1",
            "title": "SRTransGAN: Image Super-Resolution using Transformer based Generative\n  Adversarial Network",
            "updated": "2023-12-04T16:22:39Z",
            "published": "2023-12-04T16:22:39Z",
            "summary": "Image super-resolution aims to synthesize high-resolution image from a\nlow-resolution image. It is an active area to overcome the resolution\nlimitations in several applications like low-resolution object-recognition,\nmedical image enhancement, etc. The generative adversarial network (GAN) based\nmethods have been the state-of-the-art for image super-resolution by utilizing\nthe convolutional neural networks (CNNs) based generator and discriminator\nnetworks. However, the CNNs are not able to exploit the global information very\neffectively in contrast to the transformers, which are the recent breakthrough\nin deep learning by exploiting the self-attention mechanism. Motivated from the\nsuccess of transformers in language and vision applications, we propose a\nSRTransGAN for image super-resolution using transformer based GAN.\nSpecifically, we propose a novel transformer-based encoder-decoder network as a\ngenerator to generate 2x images and 4x images. We design the discriminator\nnetwork using vision transformer which uses the image as sequence of patches\nand hence useful for binary classification between synthesized and real\nhigh-resolution images. The proposed SRTransGAN outperforms the existing\nmethods by 4.38 % on an average of PSNR and SSIM scores. We also analyze the\nsaliency map to understand the learning ability of the proposed method.",
            "author": [
                "Neeraj Baghel",
                "Shiv Ram Dubey",
                "Satish Kumar Singh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01999v1",
                "http://arxiv.org/pdf/2312.01999v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01998v1",
            "title": "Language-only Efficient Training of Zero-shot Composed Image Retrieval",
            "updated": "2023-12-04T16:22:06Z",
            "published": "2023-12-04T16:22:06Z",
            "summary": "Composed image retrieval (CIR) task takes a composed query of image and text,\naiming to search relative images for both conditions. Conventional CIR\napproaches need a training dataset composed of triplets of query image, query\ntext, and target image, which is very expensive to collect. Several recent\nworks have worked on the zero-shot (ZS) CIR paradigm to tackle the issue\nwithout using pre-collected triplets. However, the existing ZS-CIR methods show\nlimited backbone scalability and generalizability due to the lack of diversity\nof the input texts during training. We propose a novel CIR framework, only\nusing language for its training. Our LinCIR (Language-only training for CIR)\ncan be trained only with text datasets by a novel self-supervision named\nself-masking projection (SMP). We project the text latent embedding to the\ntoken embedding space and construct a new text by replacing the keyword tokens\nof the original text. Then, we let the new and original texts have the same\nlatent embedding vector. With this simple strategy, LinCIR is surprisingly\nefficient and highly effective; LinCIR with CLIP ViT-G backbone is trained in\n48 minutes and shows the best ZS-CIR performances on four different CIR\nbenchmarks, CIRCO, GeneCIS, FashionIQ, and CIRR, even outperforming supervised\nmethod on FashionIQ. Code is available at https://github.com/navervision/lincir",
            "author": [
                "Geonmo Gu",
                "Sanghyuk Chun",
                "Wonjae Kim",
                "Yoohoon Kang",
                "Sangdoo Yun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01998v1",
                "http://arxiv.org/pdf/2312.01998v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01995v1",
            "title": "The Waste-to-Biomethane Logistic Problem: A mathematical optimization\n  approach",
            "updated": "2023-12-04T16:15:37Z",
            "published": "2023-12-04T16:15:37Z",
            "summary": "In this paper, we propose a new mathematical optimization approach to make\ndecisions on the optimal design of a logistic system to produce biogas from\nwaste. We provide an integrated model that allows decision makers to optimally\ndetermine the locations of different types of plants and pipelines involved in\nthe logistic process, as well as the most efficient distribution of the\nproducts (from waste to biomethane) along the supply chain. We analyze the\nmathematical model and reduce its size, being able to solve realistic instances\nin reasonable CPU times. The results of our computational experiments, both in\nsynthetic and in a case study instance, prove the validity of our proposal in\npractical applications.",
            "author": [
                "V\u00edctor Blanco",
                "Yolanda Hinojosa",
                "Victor Zavala"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01995v1",
                "http://arxiv.org/pdf/2312.01995v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01994v1",
            "title": "A Generative Self-Supervised Framework using Functional Connectivity in\n  fMRI Data",
            "updated": "2023-12-04T16:14:43Z",
            "published": "2023-12-04T16:14:43Z",
            "summary": "Deep neural networks trained on Functional Connectivity (FC) networks\nextracted from functional Magnetic Resonance Imaging (fMRI) data have gained\npopularity due to the increasing availability of data and advances in model\narchitectures, including Graph Neural Network (GNN). Recent research on the\napplication of GNN to FC suggests that exploiting the time-varying properties\nof the FC could significantly improve the accuracy and interpretability of the\nmodel prediction. However, the high cost of acquiring high-quality fMRI data\nand corresponding phenotypic labels poses a hurdle to their application in\nreal-world settings, such that a model na\\\"ively trained in a supervised\nfashion can suffer from insufficient performance or a lack of generalization on\na small number of data. In addition, most Self-Supervised Learning (SSL)\napproaches for GNNs to date adopt a contrastive strategy, which tends to lose\nappropriate semantic information when the graph structure is perturbed or does\nnot leverage both spatial and temporal information simultaneously. In light of\nthese challenges, we propose a generative SSL approach that is tailored to\neffectively harness spatio-temporal information within dynamic FC. Our\nempirical results, experimented with large-scale (>50,000) fMRI datasets,\ndemonstrate that our approach learns valuable representations and enables the\nconstruction of accurate and robust models when fine-tuned for downstream\ntasks.",
            "author": [
                "Jungwon Choi",
                "Seongho Keum",
                "EungGu Yun",
                "Byung-Hoon Kim",
                "Juho Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01994v1",
                "http://arxiv.org/pdf/2312.01994v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "eess.IV",
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01990v1",
            "title": "SARA-RT: Scaling up Robotics Transformers with Self-Adaptive Robust\n  Attention",
            "updated": "2023-12-04T16:08:47Z",
            "published": "2023-12-04T16:08:47Z",
            "summary": "We present Self-Adaptive Robust Attention for Robotics Transformers\n(SARA-RT): a new paradigm for addressing the emerging challenge of scaling up\nRobotics Transformers (RT) for on-robot deployment. SARA-RT relies on the new\nmethod of fine-tuning proposed by us, called up-training. It converts\npre-trained or already fine-tuned Transformer-based robotic policies of\nquadratic time complexity (including massive billion-parameter\nvision-language-action models or VLAs), into their efficient linear-attention\ncounterparts maintaining high quality. We demonstrate the effectiveness of\nSARA-RT by speeding up: (a) the class of recently introduced RT-2 models, the\nfirst VLA robotic policies pre-trained on internet-scale data, as well as (b)\nPoint Cloud Transformer (PCT) robotic policies operating on large point clouds.\nWe complement our results with the rigorous mathematical analysis providing\ndeeper insight into the phenomenon of SARA.",
            "author": [
                "Isabel Leal",
                "Krzysztof Choromanski",
                "Deepali Jain",
                "Avinava Dubey",
                "Jake Varley",
                "Michael Ryoo",
                "Yao Lu",
                "Frederick Liu",
                "Vikas Sindhwani",
                "Quan Vuong",
                "Tamas Sarlos",
                "Ken Oslund",
                "Karol Hausman",
                "Kanishka Rao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01990v1",
                "http://arxiv.org/pdf/2312.01990v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02248v2",
            "title": "Towards early diagnosis of Alzheimer's disease: Advances in\n  immune-related blood biomarkers and computational modeling approaches",
            "updated": "2023-12-06T10:05:42Z",
            "published": "2023-12-04T16:05:45Z",
            "summary": "Alzheimer's disease has an increasing prevalence in the population\nworld-wide, yet current diagnostic methods based on recommended biomarkers are\nonly available in specialized clinics. Due to these circumstances, Alzheimer's\ndisease is usually diagnosed late, which contrasts with the currently available\ntreatment options that are only effective for patients at an early stage.\nBlood-based biomarkers could fill in the gap of easily accessible and low-cost\nmethods for early diagnosis of the disease. In particular, immune-based\nblood-biomarkers might be a promising option, given the recently discovered\ncross-talk of immune cells of the central nervous system with those in the\nperipheral immune system. With the help of machine learning algorithms and\nmechanistic modeling approaches, such as agent-based modeling, an in-depth\nanalysis of the simulation of cell dynamics is possible as well as of\nhigh-dimensional omics resources indicative of pathway signaling changes. Here,\nwe give a background on advances in research on brain-immune system cross-talk\nin Alzheimer's disease and review recent machine learning and mechanistic\nmodeling approaches which leverage modern omics technologies for blood-based\nimmune system-related biomarker discovery.",
            "author": [
                "Sophia Krix",
                "Ella Wilczynski",
                "Neus Falg\u00e0s",
                "Raquel S\u00e1nchez-Valle",
                "Eti Yoles",
                "Uri Nevo",
                "Kuti Baruch",
                "Holger Fr\u00f6hlich"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02248v2",
                "http://arxiv.org/pdf/2312.02248v2"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01987v1",
            "title": "Bootstrapping SparseFormers from Vision Foundation Models",
            "updated": "2023-12-04T16:04:41Z",
            "published": "2023-12-04T16:04:41Z",
            "summary": "The recently proposed SparseFormer architecture provides an alternative\napproach to visual understanding by utilizing a significantly lower number of\nvisual tokens via adjusting RoIs, greatly reducing computational costs while\nstill achieving promising performance. However, training SparseFormers from\nscratch is still expensive, and scaling up the number of parameters can be\nchallenging. In this paper, we propose to bootstrap SparseFormers from\nViT-based vision foundation models in a simple and efficient way. Since the\nmajority of SparseFormer blocks are the standard transformer ones, we can\ninherit weights from large-scale pre-trained vision transformers and freeze\nthem as much as possible. Therefore, we only need to train the\nSparseFormer-specific lightweight focusing transformer to adjust token RoIs and\nfine-tune a few early pre-trained blocks to align the final token\nrepresentation. In such a way, we can bootstrap SparseFormer architectures from\nvarious large-scale pre-trained models (e.g., IN-21K pre-trained AugRegs or\nCLIPs) using a rather smaller amount of training samples (e.g., IN-1K) and\nwithout labels or captions within just a few hours. As a result, the\nbootstrapped unimodal SparseFormer (from AugReg-ViT-L/16-384) can reach 84.9%\naccuracy on IN-1K with only 49 tokens, and the multimodal SparseFormer from\nCLIPs also demonstrates notable zero-shot performance with highly reduced\ncomputational cost without seeing any caption during the bootstrapping\nprocedure. In addition, CLIP-bootstrapped SparseFormers, which align the output\nspace with language without seeing a word, can serve as efficient vision\nencoders in multimodal large language models. Code will be publicly available\nat https://github.com/showlab/sparseformer",
            "author": [
                "Ziteng Gao",
                "Zhan Tong",
                "Kevin Qinghong Lin",
                "Joya Chen",
                "Mike Zheng Shou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01987v1",
                "http://arxiv.org/pdf/2312.01987v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01985v1",
            "title": "UniGS: Unified Representation for Image Generation and Segmentation",
            "updated": "2023-12-04T15:59:27Z",
            "published": "2023-12-04T15:59:27Z",
            "summary": "This paper introduces a novel unified representation of diffusion models for\nimage generation and segmentation. Specifically, we use a colormap to represent\nentity-level masks, addressing the challenge of varying entity numbers while\naligning the representation closely with the image RGB domain. Two novel\nmodules, including the location-aware color palette and progressive dichotomy\nmodule, are proposed to support our mask representation. On the one hand, a\nlocation-aware palette guarantees the colors' consistency to entities'\nlocations. On the other hand, the progressive dichotomy module can efficiently\ndecode the synthesized colormap to high-quality entity-level masks in a\ndepth-first binary search without knowing the cluster numbers. To tackle the\nissue of lacking large-scale segmentation training data, we employ an\ninpainting pipeline and then improve the flexibility of diffusion models across\nvarious tasks, including inpainting, image synthesis, referring segmentation,\nand entity segmentation. Comprehensive experiments validate the efficiency of\nour approach, demonstrating comparable segmentation mask quality to\nstate-of-the-art and adaptability to multiple tasks. The code will be released\nat \\href{https://github.com/qqlu/Entity}{https://github.com/qqlu/Entity}.",
            "author": [
                "Lu Qi",
                "Lehan Yang",
                "Weidong Guo",
                "Yu Xu",
                "Bo Du",
                "Varun Jampani",
                "Ming-Hsuan Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01985v1",
                "http://arxiv.org/pdf/2312.01985v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01982v1",
            "title": "Stability and Approximations for Decorated Reeb Spaces",
            "updated": "2023-12-04T15:51:00Z",
            "published": "2023-12-04T15:51:00Z",
            "summary": "Given a map $f:X \\to M$ from a topological space $X$ to a metric space $M$, a\ndecorated Reeb space consists of the Reeb space, together with an attribution\nfunction whose values recover geometric information lost during the\nconstruction of the Reeb space. For example, when $M=\\mathbb{R}$ is the real\nline, the Reeb space is the well-known Reeb graph, and the attributions may\nconsist of persistence diagrams summarizing the level set topology of $f$. In\nthis paper, we introduce decorated Reeb spaces in various flavors and prove\nthat our constructions are Gromov-Hausdorff stable. We also provide results on\napproximating decorated Reeb spaces from finite samples and leverage these to\ndevelop a computational framework for applying these constructions to point\ncloud data.",
            "author": [
                "Justin Curry",
                "Washington Mio",
                "Tom Needham",
                "Osman Berat Okutan",
                "Florian Russold"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01982v1",
                "http://arxiv.org/pdf/2312.01982v1"
            ],
            "primary_category": "math.MG",
            "category": [
                "math.MG",
                "cs.CG",
                "math.AT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01973v1",
            "title": "Computing Repairs Under Functional and Inclusion Dependencies via\n  Argumentation",
            "updated": "2023-12-04T15:41:41Z",
            "published": "2023-12-04T15:41:41Z",
            "summary": "We discover a connection between finding subset-maximal repairs for sets of\nfunctional and inclusion dependencies, and computing extensions within\nargumentation frameworks (AFs). We study the complexity of the existence of a\nrepair and deciding whether a given tuple belongs to some (or every) repair, by\nsimulating the instances of these problems via AFs. We prove that\nsubset-maximal repairs under functional dependencies correspond to the naive\nextensions, which also coincide with the preferred and stable extensions in the\nresulting AFs. For inclusion dependencies, one needs a pre-processing step on\nthe resulting AFs in order for the extensions to coincide. Allowing both types\nof dependencies breaks this relationship between extensions, and only preferred\nsemantics captures the repairs. Finally, we establish that the complexities of\nthe above decision problems are NP-complete and Pi_2^P-complete, when both\nfunctional and inclusion dependencies are allowed.",
            "author": [
                "Yasir Mahmood",
                "Jonni Virtema",
                "Timon Barlag",
                "Axel-Cyrille Ngonga Ngomo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01973v1",
                "http://arxiv.org/pdf/2312.01973v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01969v1",
            "title": "FDR Control for Online Anomaly Detection",
            "updated": "2023-12-04T15:28:55Z",
            "published": "2023-12-04T15:28:55Z",
            "summary": "The goal of anomaly detection is to identify observations generated by a\nprocess that is different from a reference one. An accurate anomaly detector\nmust ensure low false positive and false negative rates. However in the online\ncontext such a constraint remains highly challenging due to the usual lack of\ncontrol of the False Discovery Rate (FDR). In particular the online framework\nmakes it impossible to use classical multiple testing approaches such as the\nBenjamini-Hochberg (BH) procedure. Our strategy overcomes this difficulty by\nexploiting a local control of the ``modified FDR'' (mFDR). An important\ningredient in this control is the cardinality of the calibration set used for\ncomputing empirical $p$-values, which turns out to be an influential parameter.\nIt results a new strategy for tuning this parameter, which yields the desired\nFDR control over the whole time series. The statistical performance of this\nstrategy is analyzed by theoretical guarantees and its practical behavior is\nassessed by simulation experiments which support our conclusions.",
            "author": [
                "Etienne Kr\u00f6nert",
                "Alain C\u00e9lisse",
                "Dalila Hattab"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01969v1",
                "http://arxiv.org/pdf/2312.01969v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "Anomaly detection, Time series"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01964v1",
            "title": "Semantics-aware Motion Retargeting with Vision-Language Models",
            "updated": "2023-12-04T15:23:49Z",
            "published": "2023-12-04T15:23:49Z",
            "summary": "Capturing and preserving motion semantics is essential to motion retargeting\nbetween animation characters. However, most of the previous works neglect the\nsemantic information or rely on human-designed joint-level representations.\nHere, we present a novel Semantics-aware Motion reTargeting (SMT) method with\nthe advantage of vision-language models to extract and maintain meaningful\nmotion semantics. We utilize a differentiable module to render 3D motions. Then\nthe high-level motion semantics are incorporated into the motion retargeting\nprocess by feeding the vision-language model with the rendered images and\naligning the extracted semantic embeddings. To ensure the preservation of\nfine-grained motion details and high-level semantics, we adopt a two-stage\npipeline consisting of skeleton-aware pre-training and fine-tuning with\nsemantics and geometry constraints. Experimental results show the effectiveness\nof the proposed method in producing high-quality motion retargeting results\nwhile accurately preserving motion semantics. Project page can be found at\nhttps://sites.google.com/view/smtnet.",
            "author": [
                "Haodong Zhang",
                "ZhiKe Chen",
                "Haocheng Xu",
                "Lei Hao",
                "Xiaofei Wu",
                "Songcen Xu",
                "Zhensong Zhang",
                "Yue Wang",
                "Rong Xiong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01964v1",
                "http://arxiv.org/pdf/2312.01964v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01958v1",
            "title": "Mechanical Comparison of Arrangement Strategies for Topological\n  Interlocking Assemblies",
            "updated": "2023-12-04T15:16:34Z",
            "published": "2023-12-04T15:16:34Z",
            "summary": "Topological Interlocking assemblies are arrangements of blocks kinematically\nconstrained by a fixed frame, such that all rigid body motions of each block\nare constrained only by its permanent contact with other blocks and the frame.\nIn the literature several blocks are introduced that can be arranged into\ndifferent interlocking assemblies. In this study we investigate the influence\nof arrangement on the overall structural behaviour of the resulting\ninterlocking assemblies. This is performed using the Versatile Block, as it can\nbe arranged in three different doubly periodic ways given by wallpaper\nsymmetries. Our focus lies on the load transfer mechanisms from the assembly\nonto the frame. For fast a priori evaluation of the assemblies we introduce a\ncombinatorial model called Interlocking Flows. To investigate our assemblies\nfrom a mechanical point of view we conduct several finite element studies.\nThese reveal a strong influence of arrangement on the structural behaviour, for\ninstance, an impact on both the point and amount of maximum deflection. The\nresults of the finite element analysis are in very good agreement with the\npredictions of the Interlocking Flow model. Our source code, data and examples\nare available under https://doi.org/10.5281/zenodo.10246034.",
            "author": [
                "Tom Goertzen",
                "Domen Macek",
                "Lukas Schnelle",
                "Meike Wei\u00df",
                "Stefanie Reese",
                "Hagen Holthusen",
                "Alice C. Niemeyer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01958v1",
                "http://arxiv.org/pdf/2312.01958v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01957v1",
            "title": "Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian\n  Perspective",
            "updated": "2023-12-04T15:16:12Z",
            "published": "2023-12-04T15:16:12Z",
            "summary": "This paper proposes an interpretation of RLAIF as Bayesian inference by\nintroducing distilled Self-Critique (dSC), which refines the outputs of a LLM\nthrough a Gibbs sampler that is later distilled into a fine-tuned model. Only\nrequiring synthetic data, dSC is exercised in experiments regarding safety,\nsentiment, and privacy control, showing it can be a viable and cheap\nalternative to align LLMs. Code released at\n\\url{https://github.com/vicgalle/distilled-self-critique}.",
            "author": [
                "Victor Gallego"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01957v1",
                "http://arxiv.org/pdf/2312.01957v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01954v1",
            "title": "Zero- and Few-Shots Knowledge Graph Triplet Extraction with Large\n  Language Models",
            "updated": "2023-12-04T15:12:04Z",
            "published": "2023-12-04T15:12:04Z",
            "summary": "In this work, we tested the Triplet Extraction (TE) capabilities of a variety\nof Large Language Models (LLMs) of different sizes in the Zero- and Few-Shots\nsettings. In detail, we proposed a pipeline that dynamically gathers contextual\ninformation from a Knowledge Base (KB), both in the form of context triplets\nand of (sentence, triplets) pairs as examples, and provides it to the LLM\nthrough a prompt. The additional context allowed the LLMs to be competitive\nwith all the older fully trained baselines based on the Bidirectional Long\nShort-Term Memory (BiLSTM) Network architecture. We further conducted a\ndetailed analysis of the quality of the gathered KB context, finding it to be\nstrongly correlated with the final TE performance of the model. In contrast,\nthe size of the model appeared to only logarithmically improve the TE\ncapabilities of the LLMs.",
            "author": [
                "Andrea Papaluca",
                "Daniel Krefl",
                "Sergio Mendez Rodriguez",
                "Artem Lensky",
                "Hanna Suominen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01954v1",
                "http://arxiv.org/pdf/2312.01954v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01947v1",
            "title": "Maximising Quantum-Computing Expressive Power through Randomised\n  Circuits",
            "updated": "2023-12-04T15:04:42Z",
            "published": "2023-12-04T15:04:42Z",
            "summary": "In the noisy intermediate-scale quantum era, variational quantum algorithms\n(VQAs) have emerged as a promising avenue to obtain quantum advantage. However,\nthe success of VQAs depends on the expressive power of parameterised quantum\ncircuits, which is constrained by the limited gate number and the presence of\nbarren plateaus. In this work, we propose and numerically demonstrate a novel\napproach for VQAs, utilizing randomised quantum circuits to generate the\nvariational wavefunction. We parameterize the distribution function of these\nrandom circuits using artificial neural networks and optimize it to find the\nsolution. This random-circuit approach presents a trade-off between the\nexpressive power of the variational wavefunction and time cost, in terms of the\nsampling cost of quantum circuits. Given a fixed gate number, we can\nsystematically increase the expressive power by extending the quantum-computing\ntime. With a sufficiently large permissible time cost, the variational\nwavefunction can approximate any quantum state with arbitrary accuracy.\nFurthermore, we establish explicit relationships between expressive power, time\ncost, and gate number for variational quantum eigensolvers. These results\nhighlight the promising potential of the random-circuit approach in achieving a\nhigh expressive power in quantum computing.",
            "author": [
                "Yingli Yang",
                "Zongkang Zhang",
                "Anbang Wang",
                "Xiaosi Xu",
                "Xiaoting Wang",
                "Ying Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01947v1",
                "http://arxiv.org/pdf/2312.01947v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01946v1",
            "title": "The evaluation of graphs on surfaces for state-sum models with defects",
            "updated": "2023-12-04T15:01:37Z",
            "published": "2023-12-04T15:01:37Z",
            "summary": "The evaluation of graphs on 2-spheres is a central ingredient of the\nTuraev-Viro construction of three-dimensional topological field theories. In\nthis article, we introduce a class of graphs, called extruded graphs, that is\nrelevant for the Turaev-Viro construction with general defect configurations\ninvolving defects of various dimensions. We define the evaluation of extruded\ngraphs and show that it is invariant under a set of moves. This ensures the\ncomputability and uniqueness of our evaluation.",
            "author": [
                "Julian Farnsteiner",
                "Christoph Schweigert"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01946v1",
                "http://arxiv.org/pdf/2312.01946v1"
            ],
            "primary_category": "math.QA",
            "category": [
                "math.QA",
                "math-ph",
                "math.CT",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01943v1",
            "title": "Instance-guided Cartoon Editing with a Large-scale Dataset",
            "updated": "2023-12-04T15:00:15Z",
            "published": "2023-12-04T15:00:15Z",
            "summary": "Cartoon editing, appreciated by both professional illustrators and hobbyists,\nallows extensive creative freedom and the development of original narratives\nwithin the cartoon domain. However, the existing literature on cartoon editing\nis complex and leans heavily on manual operations, owing to the challenge of\nautomatic identification of individual character instances. Therefore, an\nautomated segmentation of these elements becomes imperative to facilitate a\nvariety of cartoon editing applications such as visual style editing, motion\ndecomposition and transfer, and the computation of stereoscopic depths for an\nenriched visual experience. Unfortunately, most current segmentation methods\nare designed for natural photographs, failing to recognize from the intricate\naesthetics of cartoon subjects, thus lowering segmentation quality. The major\nchallenge stems from two key shortcomings: the rarity of high-quality cartoon\ndedicated datasets and the absence of competent models for high-resolution\ninstance extraction on cartoons. To address this, we introduce a high-quality\ndataset of over 100k paired high-resolution cartoon images and their instance\nlabeling masks. We also present an instance-aware image segmentation model that\ncan generate accurate, high-resolution segmentation masks for characters in\ncartoon images. We present that the proposed approach enables a range of\nsegmentation-dependent cartoon editing applications like 3D Ken Burns parallax\neffects, text-guided cartoon style editing, and puppet animation from\nillustrations and manga.",
            "author": [
                "Jian Lin",
                "Chengze Li",
                "Xueting Liu",
                "Zhongping Ge"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01943v1",
                "http://arxiv.org/pdf/2312.01943v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "I.4.6; I.3.3; I.3.8"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01939v1",
            "title": "Foundations for Transfer in Reinforcement Learning: A Taxonomy of\n  Knowledge Modalities",
            "updated": "2023-12-04T14:55:58Z",
            "published": "2023-12-04T14:55:58Z",
            "summary": "Contemporary artificial intelligence systems exhibit rapidly growing\nabilities accompanied by the growth of required resources, expansive datasets\nand corresponding investments into computing infrastructure. Although earlier\nsuccesses predominantly focus on constrained settings, recent strides in\nfundamental research and applications aspire to create increasingly general\nsystems. This evolving landscape presents a dual panorama of opportunities and\nchallenges in refining the generalisation and transfer of knowledge - the\nextraction from existing sources and adaptation as a comprehensive foundation\nfor tackling new problems. Within the domain of reinforcement learning (RL),\nthe representation of knowledge manifests through various modalities, including\ndynamics and reward models, value functions, policies, and the original data.\nThis taxonomy systematically targets these modalities and frames its discussion\nbased on their inherent properties and alignment with different objectives and\nmechanisms for transfer. Where possible, we aim to provide coarse guidance\ndelineating approaches which address requirements such as limiting environment\ninteractions, maximising computational efficiency, and enhancing generalisation\nacross varying axes of change. Finally, we analyse reasons contributing to the\nprevalence or scarcity of specific forms of transfer, the inherent potential\nbehind pushing these frontiers, and underscore the significance of\ntransitioning from designed to learned transfer.",
            "author": [
                "Markus Wulfmeier",
                "Arunkumar Byravan",
                "Sarah Bechtle",
                "Karol Hausman",
                "Nicolas Heess"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01939v1",
                "http://arxiv.org/pdf/2312.01939v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.RO",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01937v1",
            "title": "Switchable band topology and geometric current in sliding bilayer\n  elemental ferroelectric",
            "updated": "2023-12-04T14:51:13Z",
            "published": "2023-12-04T14:51:13Z",
            "summary": "We demonstrate that sliding motion between two layers of the newly discovered\nferroelectric and topologically trivial bismuth (Bi) monolayer [Nature 617, 67\n(2023)] can induce a sequence of topological phase transitions, alternating\nbetween trivial and nontrivial states. Interestingly, a lateral shift, even\nwhen preserving spatial symmetry, can still switch the quantum spin Hall state\non and off. The substantial band-gap modulation and band inversion due to\ninterlayer sliding arise primarily from the intralayer in-plane charge transfer\nprocesses involving Bi atoms at the outermost atomic layers, rather than the\ninterlayer charge redistribution. We map out the topological phase diagram and\nthe geometric Berry curvature-dipole induced nonlinear anomalous Hall response\nresulting from sliding, highlighting the potential for robust mechanical\ncontrol over the edge current and the Hall current. Bilayer configurations that\nare $\\mathbb{Z}_2$ nontrivial can produce drastically different transverse\ncurrents orthogonal to the external electric field. This occurs because both\nthe direction and magnitude of the Berry curvature dipole at the Fermi level\ndepend sensitively on the sliding displacement. Our results suggest that\nbilayer bismuth could serve as a platform to realize power-efficient ``Berry\nslidetronics\" for topology memory applications.",
            "author": [
                "Zhuang Qian",
                "Zhihao Gong",
                "Jian Li",
                "Hua Wang",
                "Shi Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01937v1",
                "http://arxiv.org/pdf/2312.01937v1"
            ],
            "primary_category": "physics.comp-ph",
            "category": [
                "physics.comp-ph",
                "cond-mat.mes-hall",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02247v1",
            "title": "Federated Active Learning for Target Domain Generalisation",
            "updated": "2023-12-04T14:50:23Z",
            "published": "2023-12-04T14:50:23Z",
            "summary": "In this paper, we introduce Active Learning framework in Federated Learning\nfor Target Domain Generalisation, harnessing the strength from both learning\nparadigms. Our framework, FEDALV, composed of Active Learning (AL) and\nFederated Domain Generalisation (FDG), enables generalisation of an image\nclassification model trained from limited source domain client's data without\nsharing images to an unseen target domain. To this end, our FDG, FEDA, consists\nof two optimisation updates during training, one at the client and another at\nthe server level. For the client, the introduced losses aim to reduce feature\ncomplexity and condition alignment, while in the server, the regularisation\nlimits free energy biases between source and target obtained by the global\nmodel. The remaining component of FEDAL is AL with variable budgets, which\nqueries the server to retrieve and sample the most informative local data for\nthe targeted client. We performed multiple experiments on FDG w/ and w/o AL and\ncompared with both conventional FDG baselines and Federated Active Learning\nbaselines. Our extensive quantitative experiments demonstrate the superiority\nof our method in accuracy and efficiency compared to the multiple contemporary\nmethods. FEDALV manages to obtain the performance of the full training target\naccuracy while sampling as little as 5% of the source client's data.",
            "author": [
                "Razvan Caramalau",
                "Binod Bhattarai",
                "Danail Stoyanov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02247v1",
                "http://arxiv.org/pdf/2312.02247v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01935v1",
            "title": "A Note on the 2-Colored Rectilinear Crossing Number of Random Point Sets\n  in the Unit Square",
            "updated": "2023-12-04T14:49:55Z",
            "published": "2023-12-04T14:49:55Z",
            "summary": "Let $S$ be a set of four points chosen independently, uniformly at random\nfrom a square. Join every pair of points of $S$ with a straight line segment.\nColor these edges red if they have positive slope and blue, otherwise. We show\nthat the probability that $S$ defines a pair of crossing edges of the same\ncolor is equal to $1/4$. This is connected to a recent result of Aichholzer et\nal. [GD 2019] who showed that by 2-colouring the edges of a geometric graph and\ncounting monochromatic crossings instead of crossings, the number of crossings\ncan be more than halfed. Our result shows that for the described random\ndrawings, there is a coloring of the edges such that the number of\nmonochromatic crossings is in expectation $\\frac{1}{2}-\\frac{7}{50}$ of the\ntotal number of crossings.",
            "author": [
                "Sergio Cabello",
                "\u00c9va Czabarka",
                "Ruy Fabila-Monroy",
                "Yuya Higashikawa",
                "Raimund Seidel",
                "L\u00e1szl\u00f3 Sz\u00e9kely",
                "Josef Tkadlec",
                "Alexandra Wesolek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01935v1",
                "http://arxiv.org/pdf/2312.01935v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02246v1",
            "title": "Conditional Variational Diffusion Models",
            "updated": "2023-12-04T14:45:56Z",
            "published": "2023-12-04T14:45:56Z",
            "summary": "Inverse problems aim to determine parameters from observations, a crucial\ntask in engineering and science. Lately, generative models, especially\ndiffusion models, have gained popularity in this area for their ability to\nproduce realistic solutions and their good mathematical properties. Despite\ntheir success, an important drawback of diffusion models is their sensitivity\nto the choice of variance schedule, which controls the dynamics of the\ndiffusion process. Fine-tuning this schedule for specific applications is\ncrucial but time-costly and does not guarantee an optimal result. We propose a\nnovel approach for learning the schedule as part of the training process. Our\nmethod supports probabilistic conditioning on data, provides high-quality\nsolutions, and is flexible, proving able to adapt to different applications\nwith minimum overhead. This approach is tested in two unrelated inverse\nproblems: super-resolution microscopy and quantitative phase imaging, yielding\ncomparable or superior results to previous methods and fine-tuned diffusion\nmodels. We conclude that fine-tuning the schedule by experimentation should be\navoided because it can be learned during training in a stable way that yields\nbetter results.",
            "author": [
                "Gabriel della Maggiora",
                "Luis Alberto Croquevielle",
                "Nikita Desphande",
                "Harry Horsley",
                "Thomas Heinis",
                "Artur Yakimovich"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02246v1",
                "http://arxiv.org/pdf/2312.02246v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "stat.ML",
                "I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01929v1",
            "title": "Adjoint-Based Enforcement of State Constraints in PDE Optimization\n  Problems",
            "updated": "2023-12-04T14:38:40Z",
            "published": "2023-12-04T14:38:40Z",
            "summary": "This study demonstrates how the adjoint-based framework traditionally used to\ncompute gradients in PDE optimization problems can be extended to handle\ngeneral constraints on the state variables. This is accomplished by\nconstructing a projection of the gradient of the objective functional onto a\nsubspace tangent to the manifold defined by the constraint. This projection is\nrealized by solving an adjoint problem defined in terms of the same adjoint\noperator as used in the system employed to determine the gradient, but with a\ndifferent forcing. We focus on the \"optimize-then-discretize\" paradigm in the\ninfinite-dimensional setting where the required regularity of both the gradient\nand of the projection is ensured. The proposed approach is illustrated with two\nexamples: a simple test problem describing optimization of heat transfer in one\ndirection and a more involved problem where an optimal closure is found for a\nturbulent flow described by the Navier-Stokes system in two dimensions, both\nconsidered subject to different state constraints. The accuracy of the\ngradients and projections computed by solving suitable adjoint systems is\ncarefully verified and the presented computational results show that the\nsolutions of the optimization problems obtained with the proposed approach\nsatisfy the state constraints with a good accuracy, although not exactly.",
            "author": [
                "Pritpal Matharu",
                "Bartosz Protas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01929v1",
                "http://arxiv.org/pdf/2312.01929v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01926v1",
            "title": "The variational quantum eigensolver self-consistent field method within\n  a polarizable embedded framework",
            "updated": "2023-12-04T14:37:04Z",
            "published": "2023-12-04T14:37:04Z",
            "summary": "We formulate and implement the Variational Quantum Eigensolver Self\nConsistent Field (VQE-SCF) algorithm in combination with polarizable embedding\n(PE), thereby extending PE to the regime of quantum computing. We test the\nresulting algorithm, PE-VQE-SCF, on quantum simulators and demonstrate that the\ncomputational stress on the quantum device is only slightly increased in terms\nof gate counts compared to regular VQE-SCF. On the other hand, no increase in\nshot noise was observed. We illustrate how PE-VQE-SCF may lead to the modeling\nof real chemical systems using a simulation of the reaction barrier of the\nDiels-Alder reaction between furan and ethene as an example.",
            "author": [
                "Erik Rosendahl Kjellgren",
                "Peter Reinholdt",
                "Aaron Fitzpatrick",
                "Walter N. Talarico",
                "Phillip W. K. Jensen",
                "Stephan P. A. Sauer",
                "Sonia Coriani",
                "Stefan Knecht",
                "Jacob Kongsted"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01926v1",
                "http://arxiv.org/pdf/2312.01926v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01921v1",
            "title": "A Machine Learning Approach Towards SKILL Code Autocompletion",
            "updated": "2023-12-04T14:29:28Z",
            "published": "2023-12-04T14:29:28Z",
            "summary": "As Moore's Law continues to increase the complexity of electronic systems,\nElectronic Design Automation (EDA) must advance to meet global demand. An\nimportant example of an EDA technology is SKILL, a scripting language used to\ncustomize and extend EDA software. Recently, code generation models using the\ntransformer architecture have achieved impressive results in academic settings\nand have even been used in commercial developer tools to improve developer\nproductivity. To the best of our knowledge, this study is the first to apply\ntransformers to SKILL code autocompletion towards improving the productivity of\nhardware design engineers. In this study, a novel, data-efficient methodology\nfor generating SKILL code is proposed and experimentally validated. More\nspecifically, we propose a novel methodology for (i) creating a high-quality\nSKILL dataset with both unlabeled and labeled data, (ii) a training strategy\nwhere T5 models pre-trained on general programming language code are fine-tuned\non our custom SKILL dataset using unsupervised and supervised learning, and\n(iii) evaluating synthesized SKILL code. We show that models trained using the\nproposed methodology outperform baselines in terms of human-judgment score and\nBLEU score. A major challenge faced was the extremely small amount of available\nSKILL code data that can be used to train a transformer model to generate SKILL\ncode. Despite our validated improvements, the extremely small dataset available\nto us was still not enough to train a model that can reliably autocomplete\nSKILL code. We discuss this and other limitations as well as future work that\ncould address these limitations.",
            "author": [
                "Enrique Dehaerne",
                "Bappaditya Dey",
                "Wannes Meert"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01921v1",
                "http://arxiv.org/pdf/2312.01921v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.CL",
                "cs.PL",
                "I.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01919v1",
            "title": "COTR: Compact Occupancy TRansformer for Vision-based 3D Occupancy\n  Prediction",
            "updated": "2023-12-04T14:23:18Z",
            "published": "2023-12-04T14:23:18Z",
            "summary": "The autonomous driving community has shown significant interest in 3D\noccupancy prediction, driven by its exceptional geometric perception and\ngeneral object recognition capabilities. To achieve this, current works try to\nconstruct a Tri-Perspective View (TPV) or Occupancy (OCC) representation\nextending from the Bird-Eye-View perception. However, compressed views like TPV\nrepresentation lose 3D geometry information while raw and sparse OCC\nrepresentation requires heavy but reducant computational costs. To address the\nabove limitations, we propose Compact Occupancy TRansformer (COTR), with a\ngeometry-aware occupancy encoder and a semantic-aware group decoder to\nreconstruct a compact 3D OCC representation. The occupancy encoder first\ngenerates a compact geometrical OCC feature through efficient explicit-implicit\nview transformation. Then, the occupancy decoder further enhances the semantic\ndiscriminability of the compact OCC representation by a coarse-to-fine semantic\ngrouping strategy. Empirical experiments show that there are evident\nperformance gains across multiple baselines, e.g., COTR outperforms baselines\nwith a relative improvement of 8%-15%, demonstrating the superiority of our\nmethod.",
            "author": [
                "Qihang Ma",
                "Xin Tan",
                "Yanyun Qu",
                "Lizhuang Ma",
                "Zhizhong Zhang",
                "Yuan Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01919v1",
                "http://arxiv.org/pdf/2312.01919v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01915v1",
            "title": "A Reliable Representation with Bidirectional Transition Model for Visual\n  Reinforcement Learning Generalization",
            "updated": "2023-12-04T14:19:36Z",
            "published": "2023-12-04T14:19:36Z",
            "summary": "Visual reinforcement learning has proven effective in solving control tasks\nwith high-dimensional observations. However, extracting reliable and\ngeneralizable representations from vision-based observations remains a central\nchallenge. Inspired by the human thought process, when the representation\nextracted from the observation can predict the future and trace history, the\nrepresentation is reliable and accurate in comprehending the environment. Based\non this concept, we introduce a Bidirectional Transition (BiT) model, which\nleverages the ability to bidirectionally predict environmental transitions both\nforward and backward to extract reliable representations. Our model\ndemonstrates competitive generalization performance and sample efficiency on\ntwo settings of the DeepMind Control suite. Additionally, we utilize robotic\nmanipulation and CARLA simulators to demonstrate the wide applicability of our\nmethod.",
            "author": [
                "Xiaobo Hu",
                "Youfang Lin",
                "Yue Liu",
                "Jinwen Wang",
                "Shuo Wang",
                "Hehe Fan",
                "Kai Lv"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01915v1",
                "http://arxiv.org/pdf/2312.01915v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01913v1",
            "title": "Tunable exciton polaritons in band-gap engineered hexagonal boron\n  nitride",
            "updated": "2023-12-04T14:18:18Z",
            "published": "2023-12-04T14:18:18Z",
            "summary": "We show that hexagonal boron nitride (hBN), a two-dimensional insulator, when\nsubjected to an external superlattice potential forms a new paradigm for\nelectrostatically tunable excitons in the near- and mid-ultraviolet (UV). The\nimposed potential has three consequences: (i) it renormalizes the effective\nmass tensor, leading to anisotropic effective masses; (ii) it renormalizes the\nband gap, eventually reducing it; (iii) it reduces the exciton binding\nenergies. All these consequences depend on a single dimensionless parameter,\nwhich includes the product of strength of the external potential with its\nperiod. In addition to the excitonic energy levels, we compute the optical\nconductivity along two orthogonal directions, and from it the absorption\nspectrum. The results for the latter show that our system is able to mimic a\ngrid polarizer. These characteristics make one-dimensional hBN superlattices a\nviable and unexplored platform for fine-tuned polaritonics in the UV to visible\nspectral range.",
            "author": [
                "Pedro Ninhos",
                "Christos Tserkezis",
                "N. Asger Mortensen",
                "Nuno M. R. Peres"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01913v1",
                "http://arxiv.org/pdf/2312.01913v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "physics.optics",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01908v1",
            "title": "Triply heavy tetraquark states: mass and other properties",
            "updated": "2023-12-04T14:13:09Z",
            "published": "2023-12-04T14:13:09Z",
            "summary": "In this work, we study mass and other static properties of triply heavy\ntetraquarks in the unified framework of the MIT bag which incorporates\nchromomagnetic interactions and enhanced binding energy. The masses, magnetic\nmoments and charge radii of all strange and nonstrange (ground) states of\ntriply heavy tetraquarks are computed, suggesting that all of triply heavy\ntetraquarks are above the respective two-meson thresholds. We also estimate\nrelative decay widths of main decay channels of two-heavy mesons for these\ntetraquarks.",
            "author": [
                "Zhen-Hui Zhu",
                "Wen-Xuan Zhang",
                "Duojie Jia"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01908v1",
                "http://arxiv.org/pdf/2312.01908v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01904v1",
            "title": "Unsupervised Anomaly Detection using Aggregated Normative Diffusion",
            "updated": "2023-12-04T14:02:56Z",
            "published": "2023-12-04T14:02:56Z",
            "summary": "Early detection of anomalies in medical images such as brain MRI is highly\nrelevant for diagnosis and treatment of many conditions. Supervised machine\nlearning methods are limited to a small number of pathologies where there is\ngood availability of labeled data. In contrast, unsupervised anomaly detection\n(UAD) has the potential to identify a broader spectrum of anomalies by spotting\ndeviations from normal patterns. Our research demonstrates that existing\nstate-of-the-art UAD approaches do not generalise well to diverse types of\nanomalies in realistic multi-modal MR data. To overcome this, we introduce a\nnew UAD method named Aggregated Normative Diffusion (ANDi). ANDi operates by\naggregating differences between predicted denoising steps and ground truth\nbackwards transitions in Denoising Diffusion Probabilistic Models (DDPMs) that\nhave been trained on pyramidal Gaussian noise. We validate ANDi against three\nrecent UAD baselines, and across three diverse brain MRI datasets. We show that\nANDi, in some cases, substantially surpasses these baselines and shows\nincreased robustness to varying types of anomalies. Particularly in detecting\nmultiple sclerosis (MS) lesions, ANDi achieves improvements of up to 178% in\nterms of AUPRC.",
            "author": [
                "Alexander Frotscher",
                "Jaivardhan Kapoor",
                "Thomas Wolfers",
                "Christian F. Baumgartner"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01904v1",
                "http://arxiv.org/pdf/2312.01904v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01897v1",
            "title": "Adapting Short-Term Transformers for Action Detection in Untrimmed\n  Videos",
            "updated": "2023-12-04T13:51:16Z",
            "published": "2023-12-04T13:51:16Z",
            "summary": "Vision transformer (ViT) has shown high potential in video recognition, owing\nto its flexible design, adaptable self-attention mechanisms, and the efficacy\nof masked pre-training. Yet, it still remains unclear how to adapt these\npre-trained short-term ViTs for temporal action detection (TAD) in untrimmed\nvideos. The existing works treat them as off-the-shelf feature extractors for\neach short trimmed snippet without capturing the fine-grained relation among\ndifferent snippets in a broader temporal context. To mitigate this issue, this\npaper focuses on designing a new mechanism for adapting these pre-trained ViT\nmodels as a unified long-form video transformer to fully unleash its modeling\npower in capturing inter-snippet relation, while still keeping low computation\noverhead and memory consumption for efficient TAD. To this end, we design\neffective cross-snippet propagation modules to gradually exchange short-term\nvideo information among different snippets from two levels. For inner-backbone\ninformation propagation, we introduce a cross-snippet propagation strategy to\nenable multi-snippet temporal feature interaction inside the backbone. For\npost-backbone information propagation, we propose temporal transformer layers\nfor further clip-level modeling. With the plain ViT-B pre-trained with\nVideoMAE, our end-to-end temporal action detector (ViT-TAD) yields a very\ncompetitive performance to previous temporal action detectors, riching up to\n69.0 average mAP on THUMOS14, 37.12 average mAP on ActivityNet-1.3 and 17.20\naverage mAP on FineAction.",
            "author": [
                "Min Yang",
                "Huan Gao",
                "Ping Guo",
                "Limin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01897v1",
                "http://arxiv.org/pdf/2312.01897v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01894v1",
            "title": "Oliver Curvature Bounds for the Brownian Continuum Random Tree",
            "updated": "2023-12-04T13:47:43Z",
            "published": "2023-12-04T13:47:43Z",
            "summary": "We compute bounds in the expected Ollivier curvature for the Brownian\ncontinuum random tree $\\mathcal{T}_{\\mathbb{e}}$. The results indicate that\nwhen the scale dependence of the Ollivier curvature is properly taken into\naccount, the Ollivier-Ricci curvature of $\\mathcal{T}_{\\mathbb{e}}$ is bounded\nabove by every element of $\\mathbb{R}$ for almost all points of\n$\\mathcal{T}_{\\mathbb{e}}$. This parallels the well-known result that every\ncontinuum tree is a $CAT(K)$ space for all $K\\in\\mathbb{R}$.",
            "author": [
                "Christy Kelly"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01894v1",
                "http://arxiv.org/pdf/2312.01894v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.MG",
                "49Q22 (Primary), 51F99 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01888v1",
            "title": "Highly Accelerated Weighted MMSE Algorithms for Designing Precoders in\n  FDD Systems with Incomplete CSI",
            "updated": "2023-12-04T13:42:10Z",
            "published": "2023-12-04T13:42:10Z",
            "summary": "In this work, we derive a lower bound on the training-based achievable\ndownlink (DL) sum rate (SR) of a multi-user multiple-input-single-output (MISO)\nsystem operating in frequency-division-duplex (FDD) mode. Assuming linear\nminimum mean square error (LMMSE) channel estimation is used, we establish a\nconnection of the derived lower bound on the signal-to-interference-noise-ratio\n(SINR) to an average MSE that allows to reformulate the SR maximization problem\nas the minimization of the augmented weighted average MSE (AWAMSE). We propose\nan iterative precoder design with three alternating steps, all given in closed\nform, drastically reducing the computation time. We show numerically the\neffectiveness of the proposed approach in challenging scenarios with limited\nchannel knowledge, i.e., we consider scenarios with a very limited number of\npilots. We additionally propose a more efficient version of the well-known\nstochastic iterative WMMSE (SIWMMSE) approach, where the precoder update is\ngiven in closed form.",
            "author": [
                "Donia Ben Amor",
                "Michael Joham",
                "Wolfgang Utschick"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01888v1",
                "http://arxiv.org/pdf/2312.01888v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01886v1",
            "title": "InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language\n  Models",
            "updated": "2023-12-04T13:40:05Z",
            "published": "2023-12-04T13:40:05Z",
            "summary": "Large vision-language models (LVLMs) have demonstrated their incredible\ncapability in image understanding and response generation. However, this rich\nvisual interaction also makes LVLMs vulnerable to adversarial examples. In this\npaper, we formulate a novel and practical gray-box attack scenario that the\nadversary can only access the visual encoder of the victim LVLM, without the\nknowledge of its prompts (which are often proprietary for service providers and\nnot publicly available) and its underlying large language model (LLM). This\npractical setting poses challenges to the cross-prompt and cross-model\ntransferability of targeted adversarial attack, which aims to confuse the LVLM\nto output a response that is semantically similar to the attacker's chosen\ntarget text. To this end, we propose an instruction-tuned targeted attack\n(dubbed InstructTA) to deliver the targeted adversarial attack on LVLMs with\nhigh transferability. Initially, we utilize a public text-to-image generative\nmodel to \"reverse\" the target response into a target image, and employ GPT-4 to\ninfer a reasonable instruction $\\boldsymbol{p}^\\prime$ from the target\nresponse. We then form a local surrogate model (sharing the same visual encoder\nwith the victim LVLM) to extract instruction-aware features of an adversarial\nimage example and the target image, and minimize the distance between these two\nfeatures to optimize the adversarial example. To further improve the\ntransferability, we augment the instruction $\\boldsymbol{p}^\\prime$ with\ninstructions paraphrased from an LLM. Extensive experiments demonstrate the\nsuperiority of our proposed method in targeted attack performance and\ntransferability.",
            "author": [
                "Xunguang Wang",
                "Zhenlan Ji",
                "Pingchuan Ma",
                "Zongjie Li",
                "Shuai Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01886v1",
                "http://arxiv.org/pdf/2312.01886v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    }
]