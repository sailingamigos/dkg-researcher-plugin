[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:43d2ed5c3c55c1100450cd74dc1031afa24d37b2",
            "@type": "ScholarlyArticle",
            "paperId": "43d2ed5c3c55c1100450cd74dc1031afa24d37b2",
            "corpusId": 62016134,
            "url": "https://www.semanticscholar.org/paper/43d2ed5c3c55c1100450cd74dc1031afa24d37b2",
            "title": "Collective Classification in Network Data",
            "venue": "The AI Magazine",
            "publicationVenue": {
                "id": "urn:research:6fedff74-7525-4b7f-bbb4-4df4e23948e4",
                "name": "The AI Magazine",
                "alternate_names": [
                    "AI Mag",
                    "Ai Mag",
                    "Ai Magazine"
                ],
                "issn": "0738-4602",
                "url": "https://www.aaai.org/Library/Magazine/magazine-library.php"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2403788960",
                "DBLP": "journals/aim/SenNBGGE08",
                "DOI": "10.1201/b17320-16",
                "CorpusId": 62016134
            },
            "abstract": "Many real-world applications produce networked data such as the world-wide web (hypertext documents connected via hyperlinks), social networks (for example, people connected by friendship links), communication networks (computers connected via communication links) and biological networks (for example, protein interaction networks). A recent focus in machine learning research has been to extend traditional machine learning classification techniques to classify nodes in such networks. In this article, we provide a brief introduction to this area of research and how it has progressed during the past decade. We introduce four of the most widely used inference algorithms for classifying networked data and empirically compare them on both synthetic and real-world data.",
            "referenceCount": 94,
            "citationCount": 3138,
            "influentialCitationCount": 686,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/aimagazine/article/download/2157/2022",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-09-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sen2008CollectiveCI,\n author = {P. Sen and Galileo Namata and M. Bilgic and L. Getoor and Brian Gallagher and Tina Eliassi-Rad},\n booktitle = {The AI Magazine},\n pages = {399-416},\n title = {Collective Classification in Network Data},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3a84214cb69ea0b34352285029f368b75718c32b",
            "@type": "ScholarlyArticle",
            "paperId": "3a84214cb69ea0b34352285029f368b75718c32b",
            "corpusId": 3819513,
            "url": "https://www.semanticscholar.org/paper/3a84214cb69ea0b34352285029f368b75718c32b",
            "title": "Understanding of a convolutional neural network",
            "venue": "International Conference on Emerging Technologies",
            "publicationVenue": {
                "id": "urn:research:12b499eb-56c9-4d88-b792-6d90ceff3485",
                "name": "International Conference on Emerging Technologies",
                "alternate_names": [
                    "ICET",
                    "Int Conf Emerg Technol"
                ],
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2789876780",
                "DOI": "10.1109/ICENGTECHNOL.2017.8308186",
                "CorpusId": 3819513
            },
            "abstract": "The term Deep Learning or Deep Neural Network refers to Artificial Neural Networks (ANN) with multi layers. Over the last few decades, it has been considered to be one of the most powerful tools, and has become very popular in the literature as it is able to handle a huge amount of data. The interest in having deeper hidden layers has recently begun to surpass classical methods performance in different fields; especially in pattern recognition. One of the most popular deep neural networks is the Convolutional Neural Network (CNN). It take this name from mathematical linear operation between matrixes called convolution. CNN have multiple layers; including convolutional layer, non-linearity layer, pooling layer and fully-connected layer. The convolutional and fully-connected layers have parameters but pooling and non-linearity layers don't have parameters. The CNN has an excellent performance in machine learning problems. Specially the applications that deal with image data, such as largest image classification data set (Image Net), computer vision, and in natural language processing (NLP) and the results achieved were very amazing. In this paper we will explain and define all the elements and important issues related to CNN, and how these elements work. In addition, we will also state the parameters that effect CNN efficiency. This paper assumes that the readers have adequate knowledge about both machine learning and artificial neural network.",
            "referenceCount": 13,
            "citationCount": 2137,
            "influentialCitationCount": 150,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference"
            ],
            "publicationDate": "2017-08-01",
            "journal": {
                "name": "2017 International Conference on Engineering and Technology (ICET)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Conference{Albawi2017UnderstandingOA,\n author = {Saad Albawi and T. Mohammed and Saad Al-Zawi},\n booktitle = {International Conference on Emerging Technologies},\n journal = {2017 International Conference on Engineering and Technology (ICET)},\n pages = {1-6},\n title = {Understanding of a convolutional neural network},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:24281c886cd9339fe2fc5881faf5ed72b731a03e",
            "@type": "ScholarlyArticle",
            "paperId": "24281c886cd9339fe2fc5881faf5ed72b731a03e",
            "corpusId": 263897801,
            "url": "https://www.semanticscholar.org/paper/24281c886cd9339fe2fc5881faf5ed72b731a03e",
            "title": "Spark: Cluster Computing with Working Sets",
            "venue": "USENIX Workshop on Hot Topics in Cloud Computing",
            "publicationVenue": {
                "id": "urn:research:42ad1c65-dc2f-448c-bbbf-483b016441b3",
                "name": "USENIX Workshop on Hot Topics in Cloud Computing",
                "alternate_names": [
                    "USENIX conference on Hot Topics in Cloud Ccomputing",
                    "USENIX Workshop Hot Top Cloud Comput",
                    "HotCloud",
                    "USENIX conf Hot Top Cloud Ccomputing"
                ],
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2189465200",
                "DBLP": "conf/hotcloud/ZahariaCFSS10",
                "CorpusId": 263897801
            },
            "abstract": "MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.",
            "referenceCount": 26,
            "citationCount": 2509,
            "influentialCitationCount": 440,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-06-22",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Zaharia2010SparkCC,\n author = {M. Zaharia and Mosharaf Chowdhury and Michael J. Franklin and S. Shenker and Ion Stoica},\n booktitle = {USENIX Workshop on Hot Topics in Cloud Computing},\n pages = {10-10},\n title = {Spark: Cluster Computing with Working Sets},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1dbc1238409549ae6872a744b7b2ff1da5822053",
            "@type": "ScholarlyArticle",
            "paperId": "1dbc1238409549ae6872a744b7b2ff1da5822053",
            "corpusId": 15115468,
            "url": "https://www.semanticscholar.org/paper/1dbc1238409549ae6872a744b7b2ff1da5822053",
            "title": "A reliable effective terascale linear learning system",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2952437656",
                "DBLP": "journals/corr/abs-1110-4198",
                "ArXiv": "1110.4198",
                "DOI": "10.5555/2627435.2638571",
                "CorpusId": 15115468
            },
            "abstract": "We present a system and a set of techniques for learning linear predictors with convex losses on terascale data sets, with trillions of features, billions of training examples and millions of parameters in an hour using a cluster of 1000 machines. Individually none of the component techniques are new, but the careful synthesis required to obtain an efficient implementation is. The result is, up to our knowledge, the most scalable and efficient linear learning system reported in the literature. We describe and thoroughly evaluate the components of the system, showing the importance of the various design choices.",
            "referenceCount": 39,
            "citationCount": 370,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-10-19",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "15"
            },
            "citationStyles": {
                "bibtex": "@Article{Agarwal2011ARE,\n author = {Alekh Agarwal and O. Chapelle and Miroslav Dud\u00edk and J. Langford},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1111-1133},\n title = {A reliable effective terascale linear learning system},\n volume = {15},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d1b9a3b11e6c9571a1553556f82b605b2b4baec3",
            "@type": "ScholarlyArticle",
            "paperId": "d1b9a3b11e6c9571a1553556f82b605b2b4baec3",
            "corpusId": 207229839,
            "url": "https://www.semanticscholar.org/paper/d1b9a3b11e6c9571a1553556f82b605b2b4baec3",
            "title": "Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures",
            "venue": "Conference on Computer and Communications Security",
            "publicationVenue": {
                "id": "urn:research:73f7fe95-b68b-468f-b7ba-3013ca879e50",
                "name": "Conference on Computer and Communications Security",
                "alternate_names": [
                    "Int Workshop Cogn Cell Syst",
                    "CCS",
                    "Comput Commun Secur",
                    "CcS",
                    "International Symposium on Community-centric Systems",
                    "International Workshop on Cognitive Cellular Systems",
                    "Conf Comput Commun Secur",
                    "Comb Comput Sci",
                    "Int Symp Community-centric Syst",
                    "Combinatorics and Computer Science",
                    "Circuits, Signals, and Systems",
                    "Computer and Communications Security",
                    "Circuit Signal Syst"
                ],
                "issn": null,
                "url": "https://dl.acm.org/conference/ccs"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/ccs/FredriksonJR15",
                "MAG": "2051267297",
                "DOI": "10.1145/2810103.2813677",
                "CorpusId": 207229839
            },
            "abstract": "Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al., adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown. We develop a new class of model inversion attack that exploits confidence values revealed along with predictions. Our new attacks are applicable in a variety of settings, and we explore two in depth: decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition. In both cases confidence values are revealed to those with the ability to make prediction queries to models. We experimentally show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and, in the other context, show how to recover recognizable images of people's faces given only their name and access to the ML model. We also initiate experimental exploration of natural countermeasures, investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning, as well as revealing only rounded confidence values. The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility.",
            "referenceCount": 36,
            "citationCount": 2067,
            "influentialCitationCount": 167,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2015-10-12",
            "journal": {
                "name": "Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Fredrikson2015ModelIA,\n author = {Matt Fredrikson and S. Jha and T. Ristenpart},\n booktitle = {Conference on Computer and Communications Security},\n journal = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},\n title = {Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:73f76a40ed20aa3c6a8e27e4db4a8c102e7b4c6d",
            "@type": "ScholarlyArticle",
            "paperId": "73f76a40ed20aa3c6a8e27e4db4a8c102e7b4c6d",
            "corpusId": 207159665,
            "url": "https://www.semanticscholar.org/paper/73f76a40ed20aa3c6a8e27e4db4a8c102e7b4c6d",
            "title": "The relationship between Precision-Recall and ROC curves",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "conf/icml/DavisG06",
                "MAG": "1976526581",
                "DOI": "10.1145/1143844.1143874",
                "CorpusId": 207159665
            },
            "abstract": "Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.",
            "referenceCount": 23,
            "citationCount": 5170,
            "influentialCitationCount": 409,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://lirias.kuleuven.be/bitstream/123456789/295592/1/davisgoadrichcamera2.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2006-06-25",
            "journal": {
                "name": "Proceedings of the 23rd international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Davis2006TheRB,\n author = {Jesse Davis and Mark H. Goadrich},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the 23rd international conference on Machine learning},\n title = {The relationship between Precision-Recall and ROC curves},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e95d3934e51107da7610acd0b1bcb6551671f9f1",
            "@type": "ScholarlyArticle",
            "paperId": "e95d3934e51107da7610acd0b1bcb6551671f9f1",
            "corpusId": 21145246,
            "url": "https://www.semanticscholar.org/paper/e95d3934e51107da7610acd0b1bcb6551671f9f1",
            "title": "A Practical Guide to Training Restricted Boltzmann Machines",
            "venue": "Neural Networks",
            "publicationVenue": {
                "id": "urn:research:a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                "name": "Neural Networks",
                "alternate_names": [
                    "Neural Netw"
                ],
                "issn": "0893-6080",
                "url": "http://www.elsevier.com/locate/neunet"
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "series/lncs/Hinton12",
                "MAG": "44815768",
                "DOI": "10.1007/978-3-642-35289-8_32",
                "CorpusId": 21145246
            },
            "abstract": null,
            "referenceCount": 28,
            "citationCount": 3003,
            "influentialCitationCount": 331,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Hinton2012APG,\n author = {Geoffrey E. Hinton},\n booktitle = {Neural Networks},\n pages = {599-619},\n title = {A Practical Guide to Training Restricted Boltzmann Machines},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a01bd58305d925ec4d72ad1f3714739f5eb5a338",
            "@type": "ScholarlyArticle",
            "paperId": "a01bd58305d925ec4d72ad1f3714739f5eb5a338",
            "corpusId": 3950158,
            "url": "https://www.semanticscholar.org/paper/a01bd58305d925ec4d72ad1f3714739f5eb5a338",
            "title": "A COMPARATIVE ANALYSIS",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "DOI": "10.1080/07303084.1987.10609595",
                "CorpusId": 3950158
            },
            "abstract": "Machine learning (ML) is a subset of artificial intelligence domain that incorporates a wide range of techniques including Genetic algorithms",
            "referenceCount": 295,
            "citationCount": 3775,
            "influentialCitationCount": 297,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Pratt1998ACA,\n author = {Andy Pratt and Ana Rincon \u2013 Aznar},\n title = {A COMPARATIVE ANALYSIS},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a1874aafa8730bdd4b28f29d025141c13ee28b58",
            "@type": "ScholarlyArticle",
            "paperId": "a1874aafa8730bdd4b28f29d025141c13ee28b58",
            "corpusId": 61287995,
            "url": "https://www.semanticscholar.org/paper/a1874aafa8730bdd4b28f29d025141c13ee28b58",
            "title": "From Data Mining to Knowledge Discovery in Databases",
            "venue": "The AI Magazine",
            "publicationVenue": {
                "id": "urn:research:6fedff74-7525-4b7f-bbb4-4df4e23948e4",
                "name": "The AI Magazine",
                "alternate_names": [
                    "AI Mag",
                    "Ai Mag",
                    "Ai Magazine"
                ],
                "issn": "0738-4602",
                "url": "https://www.aaai.org/Library/Magazine/magazine-library.php"
            },
            "year": 1996,
            "externalIds": {
                "DBLP": "journals/aim/FayyadPS96",
                "MAG": "2163598528",
                "DOI": "10.1609/aimag.v17i3.1230",
                "CorpusId": 61287995
            },
            "abstract": "\u25a0 Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field.",
            "referenceCount": 59,
            "citationCount": 5365,
            "influentialCitationCount": 380,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1996-03-15",
            "journal": {
                "name": "AI Mag.",
                "volume": "17"
            },
            "citationStyles": {
                "bibtex": "@Article{Fayyad1996FromDM,\n author = {U. Fayyad and G. Piatetsky-Shapiro and Padhraic Smyth},\n booktitle = {The AI Magazine},\n journal = {AI Mag.},\n pages = {37-54},\n title = {From Data Mining to Knowledge Discovery in Databases},\n volume = {17},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:167e1359943b96b9e92ee73db1df69a1f65d731d",
            "@type": "ScholarlyArticle",
            "paperId": "167e1359943b96b9e92ee73db1df69a1f65d731d",
            "corpusId": 388,
            "url": "https://www.semanticscholar.org/paper/167e1359943b96b9e92ee73db1df69a1f65d731d",
            "title": "A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "journals/corr/cs-CL-0409058",
                "ACL": "P04-1035",
                "MAG": "2114524997",
                "ArXiv": "cs/0409058",
                "DOI": "10.3115/1218955.1218990",
                "CorpusId": 388
            },
            "abstract": "Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as \"thumbs up\" or \"thumbs down\". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.",
            "referenceCount": 28,
            "citationCount": 3890,
            "influentialCitationCount": 402,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.3115/1218955.1218990",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2004-07-21",
            "journal": {
                "name": "ArXiv",
                "volume": "cs.CL/0409058"
            },
            "citationStyles": {
                "bibtex": "@Article{Pang2004ASE,\n author = {B. Pang and Lillian Lee},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts},\n volume = {cs.CL/0409058},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a3461eaf51016f9d6e85ea47173b27e019e801c4",
            "@type": "ScholarlyArticle",
            "paperId": "a3461eaf51016f9d6e85ea47173b27e019e801c4",
            "corpusId": 2860566,
            "url": "https://www.semanticscholar.org/paper/a3461eaf51016f9d6e85ea47173b27e019e801c4",
            "title": "State of the Art",
            "venue": "Pediatric Research",
            "publicationVenue": {
                "id": "urn:research:ad2ae6f7-53e8-442e-900a-bd9ab7ff3865",
                "name": "Pediatric Research",
                "alternate_names": [
                    "Pediatr Res"
                ],
                "issn": "0031-3998",
                "url": "http://www.nature.com/pr/"
            },
            "year": 1997,
            "externalIds": {
                "DOI": "10.1203/00006450-199704001-00009",
                "CorpusId": 2860566
            },
            "abstract": "We are concerned with the inference (induction) of theories (hypotheses) from observations (data). This problem is common to philosophy (Aristotle 1988), statistical inference (Casella & Berger 2001) and machine learning (Mitchell 1997, Agluin & Smith 1983). We constrain ourselves only to the latter two frameworks. Within machine-learning, we further concentrate on its subfield called inductive logic programming (Nienhuys-Cheng & de Wolf 1997). Whereas in statistics we namely concentrate on evaluating hypotheses, in machine learning we study ways of constructing the theories. From the theoretical viewpoint, however, the construction is also viewed as a selection of a hypothesis from an a priori given set. Unlike in statistics, however, the range of considered hypotheses is usually large so that hypotheses cannot by inspected individually by a human. Such a set of hypotheses may be conveniently viewed as (equivalent to) a language L H generated by a certain formal grammar. Every hypothesis H \u2208 L H induces a mapping h : X \u2192 O where X is a predefined (usually countable) set of instances (which we also call the domain of L H) and O is a set usually assumed to be finite and its elements called classes. Very often, O has just two elements. The assigned mapping gives the hypothesis its meaning (semantics). The usual formalization of the concept learning task is then as follows. Let there be a hypothesis C \u2208 L H called the target concept and let n examples (x 1 , c(x 1)),(x 2 , c(x 2)),... ,(x n , c(x n))= S drawn from a predefined distribution D X on X be provided to the algorithm L called the learner (S is called a sample). We ask L to output an hypothesis H \u2208 L H such that a specified error function Err(H, C) is minimized with respect to D X. The error function may be defined as e.g. Err(H, C) = 0 if H \u2261 C (i.e. h(x) = c(x) \u2200x \u2208 X) and Err(H, C) = 1 otherwise, that is, irrespectively of the distribution D X. We would thus require the learner to exactly identify the target concept. This would be close to the theoretical framework of identification in the limit (Gold 1967), which, roughly said, demands that the learner converges to the correct hypothesis in the limit as n \u2192 \u221e. Such a requirement is however very rigid and does not comply to the \u2026",
            "referenceCount": 64,
            "citationCount": 5550,
            "influentialCitationCount": 225,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/pr1997168.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Pediatric Research",
                "volume": "41"
            },
            "citationStyles": {
                "bibtex": "@Article{Voelter1997StateOT,\n author = {Markus Voelter},\n booktitle = {Pediatric Research},\n journal = {Pediatric Research},\n pages = {17-19},\n title = {State of the Art},\n volume = {41},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:08b43d84e6747e370ef307e2ada50675b414514a",
            "@type": "ScholarlyArticle",
            "paperId": "08b43d84e6747e370ef307e2ada50675b414514a",
            "corpusId": 191396,
            "url": "https://www.semanticscholar.org/paper/08b43d84e6747e370ef307e2ada50675b414514a",
            "title": "Survey of clustering algorithms",
            "venue": "IEEE Transactions on Neural Networks",
            "publicationVenue": {
                "id": "urn:research:2ac50919-507e-41c7-93a8-721c4b804757",
                "name": "IEEE Transactions on Neural Networks",
                "alternate_names": [
                    "IEEE Trans Neural Netw"
                ],
                "issn": "1045-9227",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=72"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2153233077",
                "DBLP": "journals/tnn/XuW05",
                "DOI": "10.1109/TNN.2005.845141",
                "CorpusId": 191396,
                "PubMed": "15940994"
            },
            "abstract": "Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.",
            "referenceCount": 313,
            "citationCount": 5930,
            "influentialCitationCount": 269,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://axon.cs.byu.edu/Dan/678/papers/Cluster/Xu.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle",
                "Study"
            ],
            "publicationDate": "2005-05-01",
            "journal": {
                "name": "IEEE Transactions on Neural Networks",
                "volume": "16"
            },
            "citationStyles": {
                "bibtex": "@Article{Xu2005SurveyOC,\n author = {R. Xu and D. Wunsch},\n booktitle = {IEEE Transactions on Neural Networks},\n journal = {IEEE Transactions on Neural Networks},\n pages = {645-678},\n title = {Survey of clustering algorithms},\n volume = {16},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a34e35dbbc6911fa7b94894dffdc0076a261b6f0",
            "@type": "ScholarlyArticle",
            "paperId": "a34e35dbbc6911fa7b94894dffdc0076a261b6f0",
            "corpusId": 14215320,
            "url": "https://www.semanticscholar.org/paper/a34e35dbbc6911fa7b94894dffdc0076a261b6f0",
            "title": "Neural Networks and the Bias/Variance Dilemma",
            "venue": "Neural Computation",
            "publicationVenue": {
                "id": "urn:research:69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                "name": "Neural Computation",
                "alternate_names": [
                    "Neural Comput"
                ],
                "issn": "0899-7667",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667"
            },
            "year": 1992,
            "externalIds": {
                "MAG": "2076118331",
                "DBLP": "journals/neco/GemanBD92",
                "DOI": "10.1162/neco.1992.4.1.1",
                "CorpusId": 14215320
            },
            "abstract": "Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals.",
            "referenceCount": 110,
            "citationCount": 3747,
            "influentialCitationCount": 142,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1992-01-03",
            "journal": {
                "name": "Neural Computation",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Geman1992NeuralNA,\n author = {S. Geman and E. Bienenstock and R. Doursat},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {1-58},\n title = {Neural Networks and the Bias/Variance Dilemma},\n volume = {4},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6d5965a76f88a8ebab4fc9c43a3ae2630628966a",
            "@type": "ScholarlyArticle",
            "paperId": "6d5965a76f88a8ebab4fc9c43a3ae2630628966a",
            "corpusId": 1314568,
            "url": "https://www.semanticscholar.org/paper/6d5965a76f88a8ebab4fc9c43a3ae2630628966a",
            "title": "Learning and evaluating classifiers under sample selection bias",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "conf/icml/Zadrozny04",
                "MAG": "2032536435",
                "DOI": "10.1145/1015330.1015425",
                "CorpusId": 1314568
            },
            "abstract": "Classifier learning methods commonly assume that the training data consist of randomly drawn examples from the same distribution as the test examples about which the learned model is expected to make predictions. In many practical situations, however, this assumption is violated, in a problem known in econometrics as sample selection bias. In this paper, we formalize the sample selection bias problem in machine learning terms and study analytically and experimentally how a number of well-known classifier learning methods are affected by it. We also present a bias correction method that is particularly useful for classifier evaluation under sample selection bias.",
            "referenceCount": 18,
            "citationCount": 866,
            "influentialCitationCount": 56,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2004-07-04",
            "journal": {
                "name": "Proceedings of the twenty-first international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zadrozny2004LearningAE,\n author = {B. Zadrozny},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the twenty-first international conference on Machine learning},\n title = {Learning and evaluating classifiers under sample selection bias},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f",
            "@type": "ScholarlyArticle",
            "paperId": "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f",
            "corpusId": 17194112,
            "url": "https://www.semanticscholar.org/paper/7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f",
            "title": "Sequence Transduction with Recurrent Neural Networks",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2012,
            "externalIds": {
                "ArXiv": "1211.3711",
                "DBLP": "journals/corr/abs-1211-3711",
                "MAG": "1828163288",
                "CorpusId": 17194112
            },
            "abstract": "Many machine learning tasks can be expressed as the transformation---or \\emph{transduction}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \\emph{finding} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.",
            "referenceCount": 21,
            "citationCount": 1495,
            "influentialCitationCount": 243,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-11-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1211.3711"
            },
            "citationStyles": {
                "bibtex": "@Article{Graves2012SequenceTW,\n author = {Alex Graves},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Sequence Transduction with Recurrent Neural Networks},\n volume = {abs/1211.3711},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a42ca00fc188beb5586ad4c7108b70aeb5317da0",
            "@type": "ScholarlyArticle",
            "paperId": "a42ca00fc188beb5586ad4c7108b70aeb5317da0",
            "corpusId": 13952689,
            "url": "https://www.semanticscholar.org/paper/a42ca00fc188beb5586ad4c7108b70aeb5317da0",
            "title": "Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms",
            "venue": "Knowledge Discovery and Data Mining",
            "publicationVenue": {
                "id": "urn:research:a0edb93b-1e95-4128-a295-6b1659149cef",
                "name": "Knowledge Discovery and Data Mining",
                "alternate_names": [
                    "KDD",
                    "Knowl Discov Data Min"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigkdd/"
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "conf/kdd/ThorntonHHL13",
                "MAG": "2949431720",
                "DOI": "10.1145/2487575.2487629",
                "CorpusId": 13952689
            },
            "abstract": "Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that attacks these issues separately. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA's standard distribution, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection and hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.",
            "referenceCount": 37,
            "citationCount": 1356,
            "influentialCitationCount": 132,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2012-08-18",
            "journal": {
                "name": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Thornton2012AutoWEKACS,\n author = {C. Thornton and F. Hutter and H. Hoos and Kevin Leyton-Brown},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},\n title = {Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bb144c04b9eb44579b19d21c3d5954401408440b",
            "@type": "ScholarlyArticle",
            "paperId": "bb144c04b9eb44579b19d21c3d5954401408440b",
            "corpusId": 38422928,
            "url": "https://www.semanticscholar.org/paper/bb144c04b9eb44579b19d21c3d5954401408440b",
            "title": "Orange: data mining toolbox in python",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "journals/jmlr/DemsarCEGHMMPTSSUZZZZ13",
                "MAG": "2131850886",
                "DOI": "10.5555/2567709.2567736",
                "CorpusId": 38422928
            },
            "abstract": "Orange is a machine learning and data mining suite for data analysis through Python scripting and visual programming. Here we report on the scripting part, which features interactive data analysis and component-based assembly of data mining procedures. In the selection and design of components, we focus on the flexibility of their reuse: our principal intention is to let the user write simple and clear scripts in Python, which build upon C++ implementations of computationally-intensive tasks. Orange is intended both for experienced users and programmers, as well as for students of data mining.",
            "referenceCount": 12,
            "citationCount": 1467,
            "influentialCitationCount": 116,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "14"
            },
            "citationStyles": {
                "bibtex": "@Article{Dem\u0161ar2013OrangeDM,\n author = {J. Dem\u0161ar and T. Curk and Ales Erjavec and C. Gorup and Tomaz Hocevar and Mitar Milutinovic and M. Mozina and M. Polajnar and Marko Toplak and A. Staric and Miha Stajdohar and Lan Umek and Lan Zagar and Jure Zbontar and M. Zitnik and B. Zupan},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {2349-2353},\n title = {Orange: data mining toolbox in python},\n volume = {14},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ba10b8f9ee40b68053af9e6c2383aa2c6e39e9be",
            "@type": "ScholarlyArticle",
            "paperId": "ba10b8f9ee40b68053af9e6c2383aa2c6e39e9be",
            "corpusId": 119297355,
            "url": "https://www.semanticscholar.org/paper/ba10b8f9ee40b68053af9e6c2383aa2c6e39e9be",
            "title": "Text Classification Algorithms: A Survey",
            "venue": "Inf.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1904-08067",
                "ArXiv": "1904.08067",
                "MAG": "2937423263",
                "DOI": "10.3390/info10040150",
                "CorpusId": 119297355
            },
            "abstract": "In recent years, there has been an exponential growth in the number of complex documentsand texts that require a deeper understanding of machine learning methods to be able to accuratelyclassify texts in many applications. Many machine learning approaches have achieved surpassingresults in natural language processing. The success of these learning algorithms relies on their capacityto understand complex models and non-linear relationships within data. However, finding suitablestructures, architectures, and techniques for text classification is a challenge for researchers. In thispaper, a brief overview of text classification algorithms is discussed. This overview covers differenttext feature extractions, dimensionality reduction methods, existing algorithms and techniques, andevaluations methods. Finally, the limitations of each technique and their application in real-worldproblems are discussed.",
            "referenceCount": 262,
            "citationCount": 952,
            "influentialCitationCount": 76,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2078-2489/10/4/150/pdf?version=1556181437",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-04-17",
            "journal": {
                "name": "Inf.",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Kowsari2019TextCA,\n author = {Kamran Kowsari and K. Meimandi and Mojtaba Heidarysafa and Sanjana Mendu and Laura E. Barnes and Donald E. Brown},\n booktitle = {Inf.},\n journal = {Inf.},\n pages = {150},\n title = {Text Classification Algorithms: A Survey},\n volume = {10},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2be40f5336afa68b49fef41e009b7172c2c9fdeb",
            "@type": "ScholarlyArticle",
            "paperId": "2be40f5336afa68b49fef41e009b7172c2c9fdeb",
            "corpusId": 233238471,
            "url": "https://www.semanticscholar.org/paper/2be40f5336afa68b49fef41e009b7172c2c9fdeb",
            "title": "POT: Python Optimal Transport",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/jmlr/FlamaryCGABCCCF21",
                "CorpusId": 233238471
            },
            "abstract": "Optimal transport has recently been reintroduced to the machine learning community thanks in part to novel e\ufb03cient optimization procedures allowing for medium to large scale applications. We propose a Python toolbox that implements several key optimal transport ideas for the machine learning community. The toolbox contains implementations of a number of founding works of OT for machine learning such as Sinkhorn algorithm and Wasserstein barycenters, but also provides generic solvers that can be used for conducting novel fundamental research. This toolbox, named POT for Python Optimal Transport, is open source with an MIT license.",
            "referenceCount": 31,
            "citationCount": 427,
            "influentialCitationCount": 31,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Flamary2021POTPO,\n author = {R\u00e9mi Flamary and N. Courty and Alexandre Gramfort and Mokhtar Z. Alaya and Aur\u00e9lie Boisbunon and S. Chambon and Adrien Corenflos and Nemo Fournier and N. Gayraud and H. Janati and I. Redko and Antoine Rolet and A. Schutz and Danica J. Sutherland and R. Tavenard and Alexander Tong and Titouan Vayer and A. Mueller},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {78:1-78:8},\n title = {POT: Python Optimal Transport},\n volume = {22},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6af440915b8a0718c93be1cf61905e41e620484a",
            "@type": "ScholarlyArticle",
            "paperId": "6af440915b8a0718c93be1cf61905e41e620484a",
            "corpusId": 49312162,
            "url": "https://www.semanticscholar.org/paper/6af440915b8a0718c93be1cf61905e41e620484a",
            "title": "Deep One-Class Classification",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/icml/RuffGDSVBMK18",
                "MAG": "2803697594",
                "CorpusId": 49312162
            },
            "abstract": "Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objec-tive. In this paper we introduce a new anomaly detection method\u2014Deep Support Vector Data Description\u2014, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GT-SRB stop signs.",
            "referenceCount": 58,
            "citationCount": 1360,
            "influentialCitationCount": 297,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-07-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ruff2018DeepOC,\n author = {Lukas Ruff and Nico G\u00f6rnitz and Lucas Deecke and Shoaib Ahmed Siddiqui and Robert A. Vandermeulen and Alexander Binder and Emmanuel M\u00fcller and M. Kloft},\n booktitle = {International Conference on Machine Learning},\n pages = {4390-4399},\n title = {Deep One-Class Classification},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dc97e7dbb821a4edfb5151bff4352655eedca9ee",
            "@type": "ScholarlyArticle",
            "paperId": "dc97e7dbb821a4edfb5151bff4352655eedca9ee",
            "corpusId": 17671150,
            "url": "https://www.semanticscholar.org/paper/dc97e7dbb821a4edfb5151bff4352655eedca9ee",
            "title": "Large Margin Methods for Structured and Interdependent Output Variables",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "journals/jmlr/TsochantaridisJHA05",
                "MAG": "2105842272",
                "CorpusId": 17671150
            },
            "abstract": "Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representations, this paper addresses the complementary issue of designing classification algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classification problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.",
            "referenceCount": 34,
            "citationCount": 2284,
            "influentialCitationCount": 434,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2005-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Tsochantaridis2005LargeMM,\n author = {Ioannis Tsochantaridis and T. Joachims and Thomas Hofmann and Y. Altun},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1453-1484},\n title = {Large Margin Methods for Structured and Interdependent Output Variables},\n volume = {6},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:eec471897375942fd690b736c2753bb19d907273",
            "@type": "ScholarlyArticle",
            "paperId": "eec471897375942fd690b736c2753bb19d907273",
            "corpusId": 16864990,
            "url": "https://www.semanticscholar.org/paper/eec471897375942fd690b736c2753bb19d907273",
            "title": "Gradient boosting machines, a tutorial",
            "venue": "Front. Neurorobot.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "PubMedCentral": "3885826",
                "DBLP": "journals/finr/NatekinK13",
                "MAG": "2088794999",
                "DOI": "10.3389/fnbot.2013.00021",
                "CorpusId": 16864990,
                "PubMed": "24409142"
            },
            "abstract": "Gradient boosting machines are a family of powerful machine-learning techniques that have shown considerable success in a wide range of practical applications. They are highly customizable to the particular needs of the application, like being learned with respect to different loss functions. This article gives a tutorial introduction into the methodology of gradient boosting methods with a strong focus on machine learning aspects of modeling. A theoretical information is complemented with descriptive examples and illustrations which cover all the stages of the gradient boosting model design. Considerations on handling the model complexity are discussed. Three practical examples of gradient boosting applications are presented and comprehensively analyzed.",
            "referenceCount": 58,
            "citationCount": 1576,
            "influentialCitationCount": 71,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.frontiersin.org/articles/10.3389/fnbot.2013.00021/pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2013-12-04",
            "journal": {
                "name": "Frontiers in Neurorobotics",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Natekin2013GradientBM,\n author = {Alexey Natekin and A. Knoll},\n booktitle = {Front. Neurorobot.},\n journal = {Frontiers in Neurorobotics},\n title = {Gradient boosting machines, a tutorial},\n volume = {7},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:759d9a6c9206c366a8d94a06f4eb05659c2bb7f2",
            "@type": "ScholarlyArticle",
            "paperId": "759d9a6c9206c366a8d94a06f4eb05659c2bb7f2",
            "corpusId": 12035411,
            "url": "https://www.semanticscholar.org/paper/759d9a6c9206c366a8d94a06f4eb05659c2bb7f2",
            "title": "Toward Open Set Recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "journals/pami/ScheirerRSB13",
                "MAG": "2119880843",
                "DOI": "10.1109/TPAMI.2012.256",
                "CorpusId": 12035411,
                "PubMed": "23682001"
            },
            "abstract": "To date, almost all experimental evaluations of machine learning-based recognition algorithms in computer vision have taken the form of \u201cclosed set\u201d recognition, whereby all testing classes are known at training time. A more realistic scenario for vision applications is \u201copen set\u201d recognition, where incomplete knowledge of the world is present at training time, and unknown classes can be submitted to an algorithm during testing. This paper explores the nature of open set recognition and formalizes its definition as a constrained minimization problem. The open set recognition problem is not well addressed by existing algorithms because it requires strong generalization. As a step toward a solution, we introduce a novel \u201c1-vs-set machine,\u201d which sculpts a decision space from the marginal distances of a 1-class or binary SVM with a linear kernel. This methodology applies to several different applications in computer vision where open set recognition is a challenging problem, including object recognition and face verification. We consider both in this work, with large scale cross-dataset experiments performed over the Caltech 256 and ImageNet sets, as well as face matching experiments performed over the Labeled Faces in the Wild set. The experiments highlight the effectiveness of machines adapted for open set evaluation compared to existing 1-class and binary SVMs for the same tasks.",
            "referenceCount": 55,
            "citationCount": 961,
            "influentialCitationCount": 125,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-07-01",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{Scheirer2013TowardOS,\n author = {W. Scheirer and A. Rocha and Archana Sapkota and T. Boult},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {1757-1772},\n title = {Toward Open Set Recognition},\n volume = {35},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fbc913faf39b1e369dfcdcfefb354d846a46573c",
            "@type": "ScholarlyArticle",
            "paperId": "fbc913faf39b1e369dfcdcfefb354d846a46573c",
            "corpusId": 219598966,
            "url": "https://www.semanticscholar.org/paper/fbc913faf39b1e369dfcdcfefb354d846a46573c",
            "title": "Learning With Kernels: Support Vector Machines, Regularization, Optimization, and Beyond",
            "venue": "Adaptive computation and machine learning series",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "DBLP": "books/lib/ScholkopfS02",
                "MAG": "1560724230",
                "DOI": "10.1198/jasa.2003.s269",
                "CorpusId": 219598966
            },
            "abstract": "From the Publisher: \nIn the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs\u0097-kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. \nLearning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.",
            "referenceCount": 0,
            "citationCount": 942,
            "influentialCitationCount": 73,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2001-12-01",
            "journal": {
                "name": "Journal of the American Statistical Association",
                "volume": "98"
            },
            "citationStyles": {
                "bibtex": "@Article{Sch\u00f6lkopf2001LearningWK,\n author = {Bernhard Sch\u00f6lkopf and A. J. Smola},\n booktitle = {Adaptive computation and machine learning series},\n journal = {Journal of the American Statistical Association},\n pages = {489 - 489},\n title = {Learning With Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},\n volume = {98},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:892f9a2f69241feec647856cd26bed37e04fd747",
            "@type": "ScholarlyArticle",
            "paperId": "892f9a2f69241feec647856cd26bed37e04fd747",
            "corpusId": 11971778,
            "url": "https://www.semanticscholar.org/paper/892f9a2f69241feec647856cd26bed37e04fd747",
            "title": "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2963815651",
                "DBLP": "journals/jmlr/LiJDRT17",
                "ArXiv": "1603.06560",
                "CorpusId": 11971778
            },
            "abstract": "Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.",
            "referenceCount": 58,
            "citationCount": 1730,
            "influentialCitationCount": 223,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-03-21",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "18"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2016HyperbandAN,\n author = {Lisha Li and Kevin G. Jamieson and Giulia DeSalvo and Afshin Rostamizadeh and Ameet Talwalkar},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {185:1-185:52},\n title = {Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},\n volume = {18},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c80d112ce59c72f943dc7b3e56e4c77dc3af1146",
            "@type": "ScholarlyArticle",
            "paperId": "c80d112ce59c72f943dc7b3e56e4c77dc3af1146",
            "corpusId": 217485587,
            "url": "https://www.semanticscholar.org/paper/c80d112ce59c72f943dc7b3e56e4c77dc3af1146",
            "title": "CryptoNets: Applying Neural Networks to Encrypted Data with High Throughput and Accuracy",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/icml/Gilad-BachrachD16",
                "MAG": "2435473771",
                "CorpusId": 217485587
            },
            "abstract": "Applying machine learning to a problem which involves medical, financial, or other types of sensitive data, not only requires accurate predictions but also careful attention to maintaining data privacy and security. Legal and ethical requirements may prevent the use of cloud-based machine learning solutions for such tasks. In this work, we will present a method to convert learned neural networks to CryptoNets, neural networks that can be applied to encrypted data. This allows a data owner to send their data in an encrypted form to a cloud service that hosts the network. The encryption ensures that the data remains confidential since the cloud does not have access to the keys needed to decrypt it. Nevertheless, we will show that the cloud service is capable of applying the neural network to the encrypted data to make encrypted predictions, and also return them in encrypted form. These encrypted predictions can be sent back to the owner of the secret key who can decrypt them. Therefore, the cloud service does not gain any information about the raw data nor about the prediction it made. We demonstrate CryptoNets on the MNIST optical character recognition tasks. CryptoNets achieve 99% accuracy and can make around 59000 predictions per hour on a single PC. Therefore, they allow high throughput, accurate, and private predictions.",
            "referenceCount": 32,
            "citationCount": 1292,
            "influentialCitationCount": 186,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-19",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dowlin2016CryptoNetsAN,\n author = {Nathan Dowlin and Ran Gilad-Bachrach and Kim Laine and K. Lauter and M. Naehrig and J. Wernsing},\n booktitle = {International Conference on Machine Learning},\n pages = {201-210},\n title = {CryptoNets: Applying Neural Networks to Encrypted Data with High Throughput and Accuracy},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f46714d200d69eb9cb5cce176297b89a3f5e3a2c",
            "@type": "ScholarlyArticle",
            "paperId": "f46714d200d69eb9cb5cce176297b89a3f5e3a2c",
            "corpusId": 9398408,
            "url": "https://www.semanticscholar.org/paper/f46714d200d69eb9cb5cce176297b89a3f5e3a2c",
            "title": "An Introduction to Convolutional Neural Networks",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2273396394",
                "DBLP": "journals/corr/OSheaN15",
                "ArXiv": "1511.08458",
                "CorpusId": 9398408
            },
            "abstract": "The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. \nThis document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.",
            "referenceCount": 22,
            "citationCount": 1823,
            "influentialCitationCount": 163,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-26",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1511.08458"
            },
            "citationStyles": {
                "bibtex": "@Article{O\u2019Shea2015AnIT,\n author = {K. O\u2019Shea and Ryan Nash},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {An Introduction to Convolutional Neural Networks},\n volume = {abs/1511.08458},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d079a2f877f554e00f71a6975435d8325987bdf5",
            "@type": "ScholarlyArticle",
            "paperId": "d079a2f877f554e00f71a6975435d8325987bdf5",
            "corpusId": 16439870,
            "url": "https://www.semanticscholar.org/paper/d079a2f877f554e00f71a6975435d8325987bdf5",
            "title": "Return of Frustratingly Easy Domain Adaptation",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1511.05547",
                "DBLP": "conf/aaai/SunFS16",
                "MAG": "2952850852",
                "DOI": "10.1609/aaai.v30i1.10306",
                "CorpusId": 16439870
            },
            "abstract": "\n \n Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being ``frustratingly easy'' to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.\n \n",
            "referenceCount": 40,
            "citationCount": 1457,
            "influentialCitationCount": 244,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10306/10165",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-11-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sun2015ReturnOF,\n author = {Baochen Sun and Jiashi Feng and Kate Saenko},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {2058-2065},\n title = {Return of Frustratingly Easy Domain Adaptation},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:993d736b0174abf2f713bea9d9642b85a2313cae",
            "@type": "ScholarlyArticle",
            "paperId": "993d736b0174abf2f713bea9d9642b85a2313cae",
            "corpusId": 8190856,
            "url": "https://www.semanticscholar.org/paper/993d736b0174abf2f713bea9d9642b85a2313cae",
            "title": "Estimating Attributes: Analysis and Extensions of RELIEF",
            "venue": "European Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:f9a81beb-9bcb-43bc-96a7-67cf23dba2d6",
                "name": "European Conference on Machine Learning",
                "alternate_names": [
                    "Eur Conf Mach Learn",
                    "European conference on Machine Learning",
                    "Eur conf Mach Learn",
                    "ECML"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/ecml"
            },
            "year": 1994,
            "externalIds": {
                "DBLP": "conf/ecml/Kononenko94",
                "MAG": "1808644423",
                "DOI": "10.1007/3-540-57868-4_57",
                "CorpusId": 8190856
            },
            "abstract": null,
            "referenceCount": 11,
            "citationCount": 3024,
            "influentialCitationCount": 327,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2F3-540-57868-4_57.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1994-05-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kononenko1994EstimatingAA,\n author = {I. Kononenko},\n booktitle = {European Conference on Machine Learning},\n pages = {171-182},\n title = {Estimating Attributes: Analysis and Extensions of RELIEF},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b57c54350769ffa59ff57f79ee5aad918844d298",
            "@type": "ScholarlyArticle",
            "paperId": "b57c54350769ffa59ff57f79ee5aad918844d298",
            "corpusId": 1578541,
            "url": "https://www.semanticscholar.org/paper/b57c54350769ffa59ff57f79ee5aad918844d298",
            "title": "Differentially Private Empirical Risk Minimization",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2009,
            "externalIds": {
                "ArXiv": "0912.0071",
                "DBLP": "journals/jmlr/ChaudhuriMS11",
                "MAG": "2119874464",
                "DOI": "10.5555/1953048.2021036",
                "CorpusId": 1578541,
                "PubMed": "21892342"
            },
            "abstract": "Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the \u03b5-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.",
            "referenceCount": 59,
            "citationCount": 1328,
            "influentialCitationCount": 182,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-11-30",
            "journal": {
                "name": "Journal of machine learning research : JMLR",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Chaudhuri2009DifferentiallyPE,\n author = {Kamalika Chaudhuri and C. Monteleoni and A. Sarwate},\n booktitle = {Journal of machine learning research},\n journal = {Journal of machine learning research : JMLR},\n pages = {\n          1069-1109\n        },\n title = {Differentially Private Empirical Risk Minimization},\n volume = {12},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:52e2bd533323ddf97073d034bae40a46eda55f34",
            "@type": "ScholarlyArticle",
            "paperId": "52e2bd533323ddf97073d034bae40a46eda55f34",
            "corpusId": 18635269,
            "url": "https://www.semanticscholar.org/paper/52e2bd533323ddf97073d034bae40a46eda55f34",
            "title": "Twitter Sentiment Classi\ufb01cation using Distant Supervision",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "CorpusId": 18635269
            },
            "abstract": "We introduce a novel approach for automatically classifying the sentiment of Twitter messages. These messages are classi\ufb01ed as either positive or negative with respect to a query term. This is useful for consumers who want to research the sentiment of products before purchase, or companies that want to monitor the public sentiment of their brands. There is no previous research on classifying sentiment of messages on microblogging services like Twitter. We present the results of machine learning algorithms for classifying the sentiment of Twitter messages using distant supervision. Our training data consists of Twitter messages with emoticons, which are used as noisy labels. This type of training data is abundantly available and can be obtained through automated means. We show that machine learning algorithms (Naive Bayes, Maximum Entropy, and SVM) have accuracy above 80% when trained with emoticon data. This paper also describes the preprocessing steps needed in order to achieve high accuracy. The main contribution of this paper is the idea of using tweets with emoticons for distant supervised learning.",
            "referenceCount": 10,
            "citationCount": 1557,
            "influentialCitationCount": 213,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Go2009TwitterSC,\n author = {Alec Go},\n title = {Twitter Sentiment Classi\ufb01cation using Distant Supervision},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7f57e9939560562727344c1c987416285ef76cda",
            "@type": "ScholarlyArticle",
            "paperId": "7f57e9939560562727344c1c987416285ef76cda",
            "corpusId": 207241700,
            "url": "https://www.semanticscholar.org/paper/7f57e9939560562727344c1c987416285ef76cda",
            "title": "Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition",
            "venue": "Conference on Computer and Communications Security",
            "publicationVenue": {
                "id": "urn:research:73f7fe95-b68b-468f-b7ba-3013ca879e50",
                "name": "Conference on Computer and Communications Security",
                "alternate_names": [
                    "Int Workshop Cogn Cell Syst",
                    "CCS",
                    "Comput Commun Secur",
                    "CcS",
                    "International Symposium on Community-centric Systems",
                    "International Workshop on Cognitive Cellular Systems",
                    "Conf Comput Commun Secur",
                    "Comb Comput Sci",
                    "Int Symp Community-centric Syst",
                    "Combinatorics and Computer Science",
                    "Circuits, Signals, and Systems",
                    "Computer and Communications Security",
                    "Circuit Signal Syst"
                ],
                "issn": null,
                "url": "https://dl.acm.org/conference/ccs"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/ccs/SharifBBR16",
                "MAG": "2535873859",
                "DOI": "10.1145/2976749.2978392",
                "CorpusId": 207241700
            },
            "abstract": "Machine learning is enabling a myriad innovations, including new algorithms for cancer diagnosis and self-driving cars. The broad use of machine learning makes it important to understand the extent to which machine-learning algorithms are subject to attack, particularly when used in applications where physical security or safety is at risk. In this paper, we focus on facial biometric systems, which are widely used in surveillance and access control. We define and investigate a novel class of attacks: attacks that are physically realizable and inconspicuous, and allow an attacker to evade recognition or impersonate another individual. We develop a systematic method to automatically generate such attacks, which are realized through printing a pair of eyeglass frames. When worn by the attacker whose image is supplied to a state-of-the-art face-recognition algorithm, the eyeglasses allow her to evade being recognized or to impersonate another individual. Our investigation focuses on white-box face-recognition systems, but we also demonstrate how similar techniques can be used in black-box scenarios, as well as to avoid face detection.",
            "referenceCount": 45,
            "citationCount": 1371,
            "influentialCitationCount": 96,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://dl.acm.org/ft_gateway.cfm?id=2978392&type=pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-10-24",
            "journal": {
                "name": "Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Sharif2016AccessorizeTA,\n author = {Mahmood Sharif and Sruti Bhagavatula and Lujo Bauer and M. Reiter},\n booktitle = {Conference on Computer and Communications Security},\n journal = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},\n title = {Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a456265138c088a894301c0433dae938705a9bec",
            "@type": "ScholarlyArticle",
            "paperId": "a456265138c088a894301c0433dae938705a9bec",
            "corpusId": 4870287,
            "url": "https://www.semanticscholar.org/paper/a456265138c088a894301c0433dae938705a9bec",
            "title": "Deep Sets",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1703.06114",
                "CorpusId": 4870287
            },
            "abstract": "In this paper, we study the problem of designing objective functions for machine learning problems defined on finite \\emph{sets}. In contrast to traditional objective functions defined for machine learning problems operating on finite dimensional vectors, the new objective functions we propose are operating on finite sets and are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics \\citep{poczos13aistats}, via anomaly detection in piezometer data of embankment dams \\citep{Jung15Exploration}, to cosmology \\citep{Ntampaka16Dynamical,Ravanbakhsh16ICML1}. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and image tagging.",
            "referenceCount": 54,
            "citationCount": 1836,
            "influentialCitationCount": 231,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2017-03-10",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Zaheer2017DeepS,\n author = {M. Zaheer and Satwik Kottur and Siamak Ravanbakhsh and B. P\u00f3czos and R. Salakhutdinov and Alex Smola},\n title = {Deep Sets},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:043f084e379a44608c470059c2aa174a323e9774",
            "@type": "ScholarlyArticle",
            "paperId": "043f084e379a44608c470059c2aa174a323e9774",
            "corpusId": 2014883,
            "url": "https://www.semanticscholar.org/paper/043f084e379a44608c470059c2aa174a323e9774",
            "title": "Counterfactual Fairness",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2753845591",
                "ArXiv": "1703.06856",
                "DBLP": "conf/nips/KusnerLRS17",
                "CorpusId": 2014883
            },
            "abstract": "Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.",
            "referenceCount": 40,
            "citationCount": 1197,
            "influentialCitationCount": 137,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Law",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-20",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1703.06856"
            },
            "citationStyles": {
                "bibtex": "@Article{Kusner2017CounterfactualF,\n author = {Matt J. Kusner and Joshua R. Loftus and Chris Russell and Ricardo Silva},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Counterfactual Fairness},\n volume = {abs/1703.06856},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e86f71ca2948d17b003a5f068db1ecb2b77827f7",
            "@type": "ScholarlyArticle",
            "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
            "corpusId": 10242377,
            "url": "https://www.semanticscholar.org/paper/e86f71ca2948d17b003a5f068db1ecb2b77827f7",
            "title": "Concrete Problems in AI Safety",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/AmodeiOSCSM16",
                "ArXiv": "1606.06565",
                "MAG": "2462906003",
                "CorpusId": 10242377
            },
            "abstract": "Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an objective function that is too expensive to evaluate frequently (\"scalable supervision\"), or undesirable behavior during the learning process (\"safe exploration\" and \"distributional shift\"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.",
            "referenceCount": 170,
            "citationCount": 1748,
            "influentialCitationCount": 85,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2016-06-21",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1606.06565"
            },
            "citationStyles": {
                "bibtex": "@Article{Amodei2016ConcretePI,\n author = {Dario Amodei and C. Olah and J. Steinhardt and P. Christiano and J. Schulman and Dandelion Man\u00e9},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Concrete Problems in AI Safety},\n volume = {abs/1606.06565},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e4a85af3f5dc41e13dc2cae9ee851953709b764e",
            "@type": "ScholarlyArticle",
            "paperId": "e4a85af3f5dc41e13dc2cae9ee851953709b764e",
            "corpusId": 206651104,
            "url": "https://www.semanticscholar.org/paper/e4a85af3f5dc41e13dc2cae9ee851953709b764e",
            "title": "Solving the quantum many-body problem with artificial neural networks",
            "venue": "Science",
            "publicationVenue": {
                "id": "urn:research:f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
                "name": "Science",
                "alternate_names": null,
                "issn": "0193-4511",
                "url": "https://www.jstor.org/journal/science"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2419175238",
                "ArXiv": "1606.02318",
                "DOI": "10.1126/science.aag2302",
                "CorpusId": 206651104,
                "PubMed": "28183973"
            },
            "abstract": "Machine learning and quantum physics Elucidating the behavior of quantum interacting systems of many particles remains one of the biggest challenges in physics. Traditional numerical methods often work well, but some of the most interesting problems leave them stumped. Carleo and Troyer harnessed the power of machine learning to develop a variational approach to the quantum many-body problem (see the Perspective by Hush). The method performed at least as well as state-of-the-art approaches, setting a benchmark for a prototypical two-dimensional problem. With further development, it may well prove a valuable piece in the quantum toolbox. Science, this issue p. 602; see also p. 580 A machine-learning approach sets a computational benchmark for a prototypical two-dimensional problem. The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the nontrivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with a variable number of hidden neurons. A reinforcement-learning scheme we demonstrate is capable of both finding the ground state and describing the unitary time evolution of complex interacting quantum systems. Our approach achieves high accuracy in describing prototypical interacting spins models in one and two dimensions.",
            "referenceCount": 50,
            "citationCount": 1491,
            "influentialCitationCount": 89,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1606.02318",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Physics",
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-06-07",
            "journal": {
                "name": "Science",
                "volume": "355"
            },
            "citationStyles": {
                "bibtex": "@Article{Carleo2016SolvingTQ,\n author = {Giuseppe Carleo and M. Troyer},\n booktitle = {Science},\n journal = {Science},\n pages = {602 - 606},\n title = {Solving the quantum many-body problem with artificial neural networks},\n volume = {355},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:28cf6da1cdcfc1f95b8c31b13d975837257766f2",
            "@type": "ScholarlyArticle",
            "paperId": "28cf6da1cdcfc1f95b8c31b13d975837257766f2",
            "corpusId": 15284973,
            "url": "https://www.semanticscholar.org/paper/28cf6da1cdcfc1f95b8c31b13d975837257766f2",
            "title": "Coordinate descent algorithms",
            "venue": "Mathematical programming",
            "publicationVenue": {
                "id": "urn:research:127cc63d-75ce-4cfb-9d35-18641daba70c",
                "name": "Mathematical programming",
                "alternate_names": [
                    "Math Program",
                    "Math program",
                    "Mathematical Programming"
                ],
                "issn": "0025-5610",
                "url": "https://ddd.uab.cat/record/51?ln=ca"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2950517004",
                "DBLP": "journals/mp/Wright15",
                "ArXiv": "1502.04759",
                "DOI": "10.1007/s10107-015-0892-3",
                "CorpusId": 15284973
            },
            "abstract": null,
            "referenceCount": 62,
            "citationCount": 1196,
            "influentialCitationCount": 89,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1502.04759",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-02-17",
            "journal": {
                "name": "Mathematical Programming",
                "volume": "151"
            },
            "citationStyles": {
                "bibtex": "@Article{Wright2015CoordinateDA,\n author = {Stephen J. Wright},\n booktitle = {Mathematical programming},\n journal = {Mathematical Programming},\n pages = {3 - 34},\n title = {Coordinate descent algorithms},\n volume = {151},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b8d7788f25dfaf0f9fe2e6c441d75ca7cd3bc09a",
            "@type": "ScholarlyArticle",
            "paperId": "b8d7788f25dfaf0f9fe2e6c441d75ca7cd3bc09a",
            "corpusId": 5322427,
            "url": "https://www.semanticscholar.org/paper/b8d7788f25dfaf0f9fe2e6c441d75ca7cd3bc09a",
            "title": "Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "conf/icml/YuL03",
                "MAG": "1661871015",
                "CorpusId": 5322427
            },
            "abstract": "Feature selection, as a preprocessing step to machine learning, is effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection methods with respect to efficiency and effectiveness. In this work, we introduce a novel concept, predominant correlation, and propose a fast filter method which can identify relevant features as well as redundancy among relevant features without pairwise correlation analysis. The efficiency and effectiveness of our method is demonstrated through extensive comparisons with other methods using real-world data of high dimensionality",
            "referenceCount": 28,
            "citationCount": 2514,
            "influentialCitationCount": 216,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2003-08-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yu2003FeatureSF,\n author = {Lei Yu and Huan Liu},\n booktitle = {International Conference on Machine Learning},\n pages = {856-863},\n title = {Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ebfa3fb595ca0299ab5249c69374c98b74bb62b3",
            "@type": "ScholarlyArticle",
            "paperId": "ebfa3fb595ca0299ab5249c69374c98b74bb62b3",
            "corpusId": 110510,
            "url": "https://www.semanticscholar.org/paper/ebfa3fb595ca0299ab5249c69374c98b74bb62b3",
            "title": "A Tutorial on the Cross-Entropy Method",
            "venue": "Annals of Operations Research",
            "publicationVenue": {
                "id": "urn:research:2e70cc37-125a-451c-b9bb-3b329f6be510",
                "name": "Annals of Operations Research",
                "alternate_names": [
                    "Ann Oper Res"
                ],
                "issn": "0254-5330",
                "url": "https://www.springer.com/journal/10479"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2132083787",
                "DBLP": "journals/anor/BoerKMR05",
                "DOI": "10.1007/S10479-005-5724-Z",
                "CorpusId": 110510
            },
            "abstract": null,
            "referenceCount": 49,
            "citationCount": 2321,
            "influentialCitationCount": 157,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.maths.uq.edu.au/~kroese/ps/eormsCE.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Annals of Operations Research",
                "volume": "134"
            },
            "citationStyles": {
                "bibtex": "@Article{Boer2005ATO,\n author = {P. Boer and Dirk P. Kroese and Shie Mannor and R. Rubinstein},\n booktitle = {Annals of Operations Research},\n journal = {Annals of Operations Research},\n pages = {19-67},\n title = {A Tutorial on the Cross-Entropy Method},\n volume = {134},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9257779eed46107bcdce9f4dc86298572ff466ce",
            "@type": "ScholarlyArticle",
            "paperId": "9257779eed46107bcdce9f4dc86298572ff466ce",
            "corpusId": 10826654,
            "url": "https://www.semanticscholar.org/paper/9257779eed46107bcdce9f4dc86298572ff466ce",
            "title": "Automated learning of decision rules for text categorization",
            "venue": "TOIS",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1994,
            "externalIds": {
                "DBLP": "journals/tois/ApteDW94",
                "MAG": "2094934653",
                "DOI": "10.1145/183422.183423",
                "CorpusId": 10826654
            },
            "abstract": "We describe the results of extensive experiments using optimized rule-based induction methods on large document collections. The goal of these methods is to discover automatically classification patterns that can be used for general document categorization or personalized filtering of free text. Previous reports indicate that human-engineered rule-based systems, requiring many man-years of developmental efforts, have been successfully built to \u201cread\u201d documents and assign topics to them. We show that machine-generated decision rules appear comparable to human performance, while using the identical rule-based representation. In comparison with other machine-learning techniques, results on a key benchmark from the Reuters collection show a large gain in performance, from a previously reported 67% recall/precision breakeven point to 80.5%. In the context of a very high-dimensional feature space, several methodological alternatives are examined, including universal versus local dictionaries, and binary versus frequency-related features.",
            "referenceCount": 31,
            "citationCount": 987,
            "influentialCitationCount": 46,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/183422.183423",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1994-07-01",
            "journal": {
                "name": "ACM Trans. Inf. Syst.",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Apt\u00e91994AutomatedLO,\n author = {C. Apt\u00e9 and Fred J. Damerau and S. Weiss},\n booktitle = {TOIS},\n journal = {ACM Trans. Inf. Syst.},\n pages = {233-251},\n title = {Automated learning of decision rules for text categorization},\n volume = {12},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0407b605b8f55db72e2545586bfe8e946b691b70",
            "@type": "ScholarlyArticle",
            "paperId": "0407b605b8f55db72e2545586bfe8e946b691b70",
            "corpusId": 12730344,
            "url": "https://www.semanticscholar.org/paper/0407b605b8f55db72e2545586bfe8e946b691b70",
            "title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2113839990",
                "DBLP": "journals/corr/GoodfellowMDCB13",
                "ArXiv": "1312.6211",
                "CorpusId": 12730344
            },
            "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models \"forget\" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated.",
            "referenceCount": 19,
            "citationCount": 1071,
            "influentialCitationCount": 81,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-12-20",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1312.6211"
            },
            "citationStyles": {
                "bibtex": "@Article{Goodfellow2013AnEI,\n author = {I. Goodfellow and Mehdi Mirza and Xia Da and Aaron C. Courville and Yoshua Bengio},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks},\n volume = {abs/1312.6211},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7333e127b62eb545d81830df2a66b98c0693a32b",
            "@type": "ScholarlyArticle",
            "paperId": "7333e127b62eb545d81830df2a66b98c0693a32b",
            "corpusId": 28528,
            "url": "https://www.semanticscholar.org/paper/7333e127b62eb545d81830df2a66b98c0693a32b",
            "title": "Quantile Regression Forests",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "journals/jmlr/Meinshausen06",
                "MAG": "1573647811",
                "CorpusId": 28528
            },
            "abstract": "Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditional mean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power.",
            "referenceCount": 21,
            "citationCount": 1368,
            "influentialCitationCount": 198,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Meinshausen2006QuantileRF,\n author = {N. Meinshausen},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {983-999},\n title = {Quantile Regression Forests},\n volume = {7},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:75e85c2e90b0abb17ae6445516a49ac05c1dbf0f",
            "@type": "ScholarlyArticle",
            "paperId": "75e85c2e90b0abb17ae6445516a49ac05c1dbf0f",
            "corpusId": 16692650,
            "url": "https://www.semanticscholar.org/paper/75e85c2e90b0abb17ae6445516a49ac05c1dbf0f",
            "title": "An Efficient Boosting Algorithm for Combining Preferences",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2107890099",
                "DBLP": "conf/icml/FreundISS98",
                "DOI": "10.1162/1532443041827916",
                "CorpusId": 16692650
            },
            "abstract": "We study the problem of learning to accurately rank a set of objects by combining a given collection of ranking or preference functions. This problem of combining preferences arises in several applications, such as that of combining the results of different search engines, or the \"collaborative-filtering\" problem of ranking movies for a user based on the movie rankings provided by other users. In this work, we begin by presenting a formal framework for this general problem. We then describe and analyze an efficient algorithm called RankBoost for combining preferences based on the boosting approach to machine learning. We give theoretical results describing the algorithm's behavior both on the training data, and on new test data not seen during training. We also describe an efficient implementation of the algorithm for a particular restricted but common case. We next discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different web search strategies, each of which is a query expansion for a given domain. The second experiment is a collaborative-filtering task for making movie recommendations.",
            "referenceCount": 71,
            "citationCount": 2281,
            "influentialCitationCount": 341,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1998-07-24",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Freund1998AnEB,\n author = {Y. Freund and Raj D. Iyer and R. Schapire and Y. Singer},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {933-969},\n title = {An Efficient Boosting Algorithm for Combining Preferences},\n volume = {4},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5194b668c67aa83c037e71599a087f63c98eb713",
            "@type": "ScholarlyArticle",
            "paperId": "5194b668c67aa83c037e71599a087f63c98eb713",
            "corpusId": 915058,
            "url": "https://www.semanticscholar.org/paper/5194b668c67aa83c037e71599a087f63c98eb713",
            "title": "A sequential algorithm for training text classifiers",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "publicationVenue": {
                "id": "urn:research:8dce23a9-44e0-4381-a39e-2acc1edff700",
                "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "alternate_names": [
                    "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "Int ACM SIGIR Conf Res Dev Inf Retr",
                    "SIGIR",
                    "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigir/"
            },
            "year": 1994,
            "externalIds": {
                "MAG": "2085989833",
                "DBLP": "journals/corr/LewisG94",
                "DOI": "10.1007/978-1-4471-2099-5_1",
                "CorpusId": 915058
            },
            "abstract": null,
            "referenceCount": 32,
            "citationCount": 2645,
            "influentialCitationCount": 210,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/cmp-lg/9407020",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1994-07-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lewis1994ASA,\n author = {D. Lewis and W. Gale},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n pages = {3-12},\n title = {A sequential algorithm for training text classifiers},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c6d971aa7630ef28f79fc57351e5abac0b8e2ddd",
            "@type": "ScholarlyArticle",
            "paperId": "c6d971aa7630ef28f79fc57351e5abac0b8e2ddd",
            "corpusId": 206447772,
            "url": "https://www.semanticscholar.org/paper/c6d971aa7630ef28f79fc57351e5abac0b8e2ddd",
            "title": "Support vector machines",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2008056655",
                "DOI": "10.1109/5254.708428",
                "CorpusId": 206447772
            },
            "abstract": "My first exposure to Support Vector Machines came this spring when heard Sue Dumais present impressive results on text categorization using this analysis technique. This issue's collection of essays should help familiarize our readers with this interesting new racehorse in the Machine Learning stable. Bernhard Scholkopf, in an introductory overview, points out that a particular advantage of SVMs over other learning algorithms is that it can be analyzed theoretically using concepts from computational learning theory, and at the same time can achieve good performance when applied to real problems. Examples of these real-world applications are provided by Sue Dumais, who describes the aforementioned text-categorization problem, yielding the best results to date on the Reuters collection, and Edgar Osuna, who presents strong results on application to face detection. Our fourth author, John Platt, gives us a practical guide and a new technique for implementing the algorithm efficiently.",
            "referenceCount": 83,
            "citationCount": 2790,
            "influentialCitationCount": 134,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "1998-07-01",
            "journal": {
                "name": "IEEE Intelligent Systems & Their Applications",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Hearst1998SupportVM,\n author = {Marti A. Hearst},\n journal = {IEEE Intelligent Systems & Their Applications},\n pages = {18-28},\n title = {Support vector machines},\n volume = {13},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:44e915a220ce74badf755aae870fa0b69ee2b82a",
            "@type": "ScholarlyArticle",
            "paperId": "44e915a220ce74badf755aae870fa0b69ee2b82a",
            "corpusId": 32800624,
            "url": "https://www.semanticscholar.org/paper/44e915a220ce74badf755aae870fa0b69ee2b82a",
            "title": "Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval",
            "venue": "European Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:f9a81beb-9bcb-43bc-96a7-67cf23dba2d6",
                "name": "European Conference on Machine Learning",
                "alternate_names": [
                    "Eur Conf Mach Learn",
                    "European conference on Machine Learning",
                    "Eur conf Mach Learn",
                    "ECML"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/ecml"
            },
            "year": 1998,
            "externalIds": {
                "DBLP": "conf/ecml/Lewis98",
                "MAG": "1924689489",
                "DOI": "10.1007/BFb0026666",
                "CorpusId": 32800624
            },
            "abstract": null,
            "referenceCount": 56,
            "citationCount": 2369,
            "influentialCitationCount": 178,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2FBFb0026666.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "1998-04-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lewis1998NaiveA,\n author = {D. Lewis},\n booktitle = {European Conference on Machine Learning},\n pages = {4-15},\n title = {Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7365f887c938ca21a6adbef08b5a520ebbd4638f",
            "@type": "ScholarlyArticle",
            "paperId": "7365f887c938ca21a6adbef08b5a520ebbd4638f",
            "corpusId": 52946140,
            "url": "https://www.semanticscholar.org/paper/7365f887c938ca21a6adbef08b5a520ebbd4638f",
            "title": "Model Cards for Model Reporting",
            "venue": "FAT",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1810.03993",
                "DBLP": "journals/corr/abs-1810-03993",
                "MAG": "2897042519",
                "DOI": "10.1145/3287560.3287596",
                "CorpusId": 52946140
            },
            "abstract": "Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.",
            "referenceCount": 48,
            "citationCount": 1212,
            "influentialCitationCount": 61,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1810.03993",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Law",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book"
            ],
            "publicationDate": "2018-10-05",
            "journal": {
                "name": "Proceedings of the Conference on Fairness, Accountability, and Transparency",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Mitchell2018ModelCF,\n author = {Margaret Mitchell and Simone Wu and Andrew Zaldivar and Parker Barnes and Lucy Vasserman and B. Hutchinson and Elena Spitzer and Inioluwa Deborah Raji and Timnit Gebru},\n booktitle = {FAT},\n journal = {Proceedings of the Conference on Fairness, Accountability, and Transparency},\n title = {Model Cards for Model Reporting},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f7d997a640f2b804676cadb8030d8b2c7bd79d85",
            "@type": "ScholarlyArticle",
            "paperId": "f7d997a640f2b804676cadb8030d8b2c7bd79d85",
            "corpusId": 1858029,
            "url": "https://www.semanticscholar.org/paper/f7d997a640f2b804676cadb8030d8b2c7bd79d85",
            "title": "On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "journals/jmlr/CawleyT10",
                "MAG": "2154776925",
                "DOI": "10.5555/1756006.1859921",
                "CorpusId": 1858029
            },
            "abstract": "Model selection strategies for machine learning algorithms typically involve the numerical optimisation of an appropriate model selection criterion, often based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator can be broken down into bias and variance components. While unbiasedness is often cited as a beneficial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a non-negligible variance introduces the potential for over-fitting in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-fitting the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of over-fitting are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the findings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds.",
            "referenceCount": 60,
            "citationCount": 1672,
            "influentialCitationCount": 54,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-03-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Cawley2010OnOI,\n author = {G. Cawley and N. L. C. Talbot},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {2079-2107},\n title = {On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation},\n volume = {11},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9cd7f74f910619d90406464309b3c0e916d453ab",
            "@type": "ScholarlyArticle",
            "paperId": "9cd7f74f910619d90406464309b3c0e916d453ab",
            "corpusId": 69376808,
            "url": "https://www.semanticscholar.org/paper/9cd7f74f910619d90406464309b3c0e916d453ab",
            "title": "Artificial Intelligence in Healthcare",
            "venue": "Artificial Intelligence and Machine Learning for Business for Non-Engineers",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "DOI": "10.1201/9780367821654-8",
                "CorpusId": 69376808
            },
            "abstract": "Artificial intelligence simply means machine learning that can sense, reason, act, and adapt based on experience with the goal of contributing to the economic growth of the country and contributing to the betterment of people's standards of living. The main aim of artificial intelligence in healthcare is to make machines more useful in solving ambiguous healthcare challenges, and by using the latest technology, it is possible to interpret data accurately and rapidly. It helps in the early detection of many chronic diseases like Alzheimer's, diabetes, cardiovascular diseases, and several types of cancers like breast cancer, colon cancer, etc., which simultaneously reduces the financial burden and severity of the disease. The key areas where AI can be applied medically include disease detection and treatment, patient connection and engagement, and managerial and security activities. The research has been aimed at a study of AI systems in the healthcare sector in India. The methodology used here consisted of a systematic literature review followed by live, on field interviews.",
            "referenceCount": 14,
            "citationCount": 1361,
            "influentialCitationCount": 29,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-11-22",
            "journal": {
                "name": "Artificial Intelligence and Machine Learning for Business for Non-Engineers",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yu2019ArtificialII,\n author = {Kun\u2010Hsing Yu and Andrew Beam and I. Kohane},\n booktitle = {Artificial Intelligence and Machine Learning for Business for Non-Engineers},\n journal = {Artificial Intelligence and Machine Learning for Business for Non-Engineers},\n title = {Artificial Intelligence in Healthcare},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746",
            "@type": "ScholarlyArticle",
            "paperId": "9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746",
            "corpusId": 39321012,
            "url": "https://www.semanticscholar.org/paper/9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746",
            "title": "The class imbalance problem: A systematic study",
            "venue": "Intelligent Data Analysis",
            "publicationVenue": {
                "id": "urn:research:5b12c785-cf12-40ca-b4d1-aefd13b26e39",
                "name": "Intelligent Data Analysis",
                "alternate_names": [
                    "Intell Data Anal"
                ],
                "issn": "1088-467X",
                "url": "http://content.iospress.com/journals/intelligent-data-analysis"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2999549716",
                "DBLP": "journals/ida/JapkowiczS02",
                "DOI": "10.3233/IDA-2002-6504",
                "CorpusId": 39321012
            },
            "abstract": "In machine learning problems, differences in prior class probabilities -- or class imbalances -- have been reported to hinder the performance of some standard classifiers, such as decision trees. This paper presents a systematic study aimed at answering three different questions. First, we attempt to understand the nature of the class imbalance problem by establishing a relationship between concept complexity, size of the training set and class imbalance level. Second, we discuss several basic re-sampling or cost-modifying methods previously proposed to deal with the class imbalance problem and compare their effectiveness. The results obtained by such methods on artificial domains are linked to results in real-world domains. Finally, we investigate the assumption that the class imbalance problem does not only affect decision tree systems but also affects other classification systems such as Neural Networks and Support Vector Machines.",
            "referenceCount": 18,
            "citationCount": 2868,
            "influentialCitationCount": 106,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2002-10-01",
            "journal": {
                "name": "Intell. Data Anal.",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Japkowicz2002TheCI,\n author = {N. Japkowicz and Shaju Stephen},\n booktitle = {Intelligent Data Analysis},\n journal = {Intell. Data Anal.},\n pages = {429-449},\n title = {The class imbalance problem: A systematic study},\n volume = {6},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:38f23fe236b152cd4983c8f30d305a568afd0d3e",
            "@type": "ScholarlyArticle",
            "paperId": "38f23fe236b152cd4983c8f30d305a568afd0d3e",
            "corpusId": 197430935,
            "url": "https://www.semanticscholar.org/paper/38f23fe236b152cd4983c8f30d305a568afd0d3e",
            "title": "A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1907.07374",
                "DBLP": "journals/tnn/TjoaG21",
                "MAG": "2958089299",
                "DOI": "10.1109/TNNLS.2020.3027314",
                "CorpusId": 197430935,
                "PubMed": "33079674"
            },
            "abstract": "Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide \u201cobviously\u201d interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.",
            "referenceCount": 171,
            "citationCount": 860,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/5962385/9591206/09233366.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-07-17",
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "32"
            },
            "citationStyles": {
                "bibtex": "@Article{Tjoa2019ASO,\n author = {Erico Tjoa and Cuntai Guan},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {4793-4813},\n title = {A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI},\n volume = {32},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:79f2626046fdc56edfaca840874e355cac734b9a",
            "@type": "ScholarlyArticle",
            "paperId": "79f2626046fdc56edfaca840874e355cac734b9a",
            "corpusId": 5961991,
            "url": "https://www.semanticscholar.org/paper/79f2626046fdc56edfaca840874e355cac734b9a",
            "title": "Ad click prediction: a view from the trenches",
            "venue": "Knowledge Discovery and Data Mining",
            "publicationVenue": {
                "id": "urn:research:a0edb93b-1e95-4128-a295-6b1659149cef",
                "name": "Knowledge Discovery and Data Mining",
                "alternate_names": [
                    "KDD",
                    "Knowl Discov Data Min"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigkdd/"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2074694452",
                "DBLP": "conf/kdd/McMahanHSYEGNPDGCLWHBK13",
                "DOI": "10.1145/2487575.2488200",
                "CorpusId": 5961991
            },
            "abstract": "Predicting ad click-through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system.",
            "referenceCount": 35,
            "citationCount": 910,
            "influentialCitationCount": 53,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://dl.acm.org/ft_gateway.cfm?id=2488200&type=pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Business",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-08-11",
            "journal": {
                "name": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{McMahan2013AdCP,\n author = {H. B. McMahan and Gary Holt and D. Sculley and Michael Young and D. Ebner and Julian Grady and Lan Nie and Todd Phillips and Eugene Davydov and D. Golovin and S. Chikkerur and Dan Liu and M. Wattenberg and A. M. Hrafnkelsson and T. Boulos and J. Kubica},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},\n title = {Ad click prediction: a view from the trenches},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:609e5cc1da126d7f760d1444b43b4fae41602841",
            "@type": "ScholarlyArticle",
            "paperId": "609e5cc1da126d7f760d1444b43b4fae41602841",
            "corpusId": 1713753,
            "url": "https://www.semanticscholar.org/paper/609e5cc1da126d7f760d1444b43b4fae41602841",
            "title": "Less is More: Active Learning with Support Vector Machines",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2000,
            "externalIds": {
                "DBLP": "conf/icml/SchohnC00",
                "MAG": "1514940655",
                "CorpusId": 1713753
            },
            "abstract": "We describe a simple active learning heuristic which greatly enhances the generalization behavior of support vector machines (SVMs) on several practical document classi\ufb01cation tasks. We observe a number of bene\ufb01ts, the most surprising of which is that a SVM trained on a well-chosen subset of the available corpus frequently performs better than one trained on all available data. The heuristic for choosing this subset is simple to compute, and makes no use of information about the test set. Given that the training time of SVMs depends heavily on the training set size, our heuristic not only offers better performance with fewer data, it frequently does so in less time than the naive approach of training on all available data.",
            "referenceCount": 24,
            "citationCount": 945,
            "influentialCitationCount": 77,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2000-06-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Schohn2000LessIM,\n author = {Greg Schohn and David A. Cohn},\n booktitle = {International Conference on Machine Learning},\n pages = {839-846},\n title = {Less is More: Active Learning with Support Vector Machines},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a538b05ebb01a40323997629e171c91aa28b8e2f",
            "@type": "ScholarlyArticle",
            "paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f",
            "corpusId": 15539264,
            "url": "https://www.semanticscholar.org/paper/a538b05ebb01a40323997629e171c91aa28b8e2f",
            "title": "Rectified Linear Units Improve Restricted Boltzmann Machines",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "1665214252",
                "DBLP": "conf/icml/NairH10",
                "CorpusId": 15539264
            },
            "abstract": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.",
            "referenceCount": 21,
            "citationCount": 15864,
            "influentialCitationCount": 1194,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-06-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nair2010RectifiedLU,\n author = {Vinod Nair and Geoffrey E. Hinton},\n booktitle = {International Conference on Machine Learning},\n pages = {807-814},\n title = {Rectified Linear Units Improve Restricted Boltzmann Machines},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e095981ebf4d389e9356bd56e59e0ade1b42e88",
            "@type": "ScholarlyArticle",
            "paperId": "5e095981ebf4d389e9356bd56e59e0ade1b42e88",
            "corpusId": 30029552,
            "url": "https://www.semanticscholar.org/paper/5e095981ebf4d389e9356bd56e59e0ade1b42e88",
            "title": "2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text",
            "venue": "J. Am. Medical Informatics Assoc.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "journals/jamia/UzunerSSD11",
                "MAG": "2168041406",
                "DOI": "10.1136/amiajnl-2011-000203",
                "CorpusId": 30029552,
                "PubMed": "21685143"
            },
            "abstract": "The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records presented three tasks: a concept extraction task focused on the extraction of medical concepts from patient reports; an assertion classification task focused on assigning assertion types for medical problem concepts; and a relation classification task focused on assigning relation types that hold between medical problems, tests, and treatments. i2b2 and the VA provided an annotated reference standard corpus for the three tasks. Using this reference standard, 22 systems were developed for concept extraction, 21 for assertion classification, and 16 for relation classification. These systems showed that machine learning approaches could be augmented with rule-based systems to determine concepts, assertions, and relations. Depending on the task, the rule-based systems can either provide input for machine learning or post-process the output of machine learning. Ensembles of classifiers, information from unlabeled data, and external knowledge sources can help when the training data are inadequate.",
            "referenceCount": 47,
            "citationCount": 1114,
            "influentialCitationCount": 89,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://academic.oup.com/jamia/article-pdf/18/5/552/33015279/18-5-552.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-09-01",
            "journal": {
                "name": "Journal of the American Medical Informatics Association : JAMIA",
                "volume": "18 5"
            },
            "citationStyles": {
                "bibtex": "@Article{Uzuner20112010IC,\n author = {\u00d6zlem Uzuner and B. South and Shuying Shen and S. Duvall},\n booktitle = {J. Am. Medical Informatics Assoc.},\n journal = {Journal of the American Medical Informatics Association : JAMIA},\n pages = {\n          552-6\n        },\n title = {2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text},\n volume = {18 5},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:561c3fa53d36405186da9cab02bd68635c3738aa",
            "@type": "ScholarlyArticle",
            "paperId": "561c3fa53d36405186da9cab02bd68635c3738aa",
            "corpusId": 918678,
            "url": "https://www.semanticscholar.org/paper/561c3fa53d36405186da9cab02bd68635c3738aa",
            "title": "Molecular graph convolutions: moving beyond fingerprints",
            "venue": "Journal of Computer-Aided Molecular Design",
            "publicationVenue": {
                "id": "urn:research:857acbe3-a76b-4abc-add7-1ea47c6f4129",
                "name": "Journal of Computer-Aided Molecular Design",
                "alternate_names": [
                    "J Comput Mol Des",
                    "Journal of Computer-aided Molecular Design"
                ],
                "issn": "0920-654X",
                "url": "https://link.springer.com/journal/10822"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/jcamd/KearnesMBPR16",
                "ArXiv": "1603.00856",
                "MAG": "2290847742",
                "DOI": "10.1007/s10822-016-9938-8",
                "CorpusId": 918678,
                "PubMed": "27558503"
            },
            "abstract": null,
            "referenceCount": 59,
            "citationCount": 1199,
            "influentialCitationCount": 46,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc5028207?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-03-02",
            "journal": {
                "name": "Journal of Computer-Aided Molecular Design",
                "volume": "30"
            },
            "citationStyles": {
                "bibtex": "@Article{Kearnes2016MolecularGC,\n author = {S. Kearnes and Kevin McCloskey and Marc Berndl and V. Pande and Patrick F. Riley},\n booktitle = {Journal of Computer-Aided Molecular Design},\n journal = {Journal of Computer-Aided Molecular Design},\n pages = {595-608},\n title = {Molecular graph convolutions: moving beyond fingerprints},\n volume = {30},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7547fd7c5e4bc3b8b8bf714583684ff187e8a382",
            "@type": "ScholarlyArticle",
            "paperId": "7547fd7c5e4bc3b8b8bf714583684ff187e8a382",
            "corpusId": 15761919,
            "url": "https://www.semanticscholar.org/paper/7547fd7c5e4bc3b8b8bf714583684ff187e8a382",
            "title": "An assessment of support vector machines for land cover classification",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2078619499",
                "DOI": "10.1080/01431160110040323",
                "CorpusId": 15761919
            },
            "abstract": "The support vector machine (SVM) is a group of theoretically superior machine learning algorithms. It was found competitive with the best available machine learning algorithms in classifying high-dimensional data sets. This paper gives an introduction to the theoretical development of the SVM and an experimental evaluation of its accuracy, stability and training speed in deriving land cover classifications from satellite images. The SVM was compared to three other popular classifiers, including the maximum likelihood classifier (MLC), neural network classifiers (NNC) and decision tree classifiers (DTC). The impacts of kernel configuration on the performance of the SVM and of the selection of training data and input variables on the four classifiers were also evaluated in this experiment.",
            "referenceCount": 60,
            "citationCount": 1704,
            "influentialCitationCount": 83,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Geology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Geology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2002-01-01",
            "journal": {
                "name": "International Journal of Remote Sensing",
                "volume": "23"
            },
            "citationStyles": {
                "bibtex": "@Article{Huang2002AnAO,\n author = {Chengquan Huang and L. Davis and J. Townshend},\n journal = {International Journal of Remote Sensing},\n pages = {725 - 749},\n title = {An assessment of support vector machines for land cover classification},\n volume = {23},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ade03d0c772c35dc8e865bdb41d7bc54d5b782d1",
            "@type": "ScholarlyArticle",
            "paperId": "ade03d0c772c35dc8e865bdb41d7bc54d5b782d1",
            "corpusId": 3008038,
            "url": "https://www.semanticscholar.org/paper/ade03d0c772c35dc8e865bdb41d7bc54d5b782d1",
            "title": "kernlab - An S4 Package for Kernel Methods in R",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2157963336",
                "DOI": "10.18637/JSS.V011.I09",
                "CorpusId": 3008038
            },
            "abstract": "kernlab is an extensible package for kernel-based machine learning methods in R. It takes advantage of R's new S4 ob ject model and provides a framework for creating and using kernel-based algorithms. The package contains dot product primitives (kernels), implementations of support vector machines and the relevance vector machine, Gaussian processes, a ranking algorithm, kernel PCA, kernel CCA, and a spectral clustering algorithm. Moreover it provides a general purpose quadratic programming solver, and an incomplete Cholesky decomposition method.",
            "referenceCount": 37,
            "citationCount": 1737,
            "influentialCitationCount": 74,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2004-11-02",
            "journal": {
                "name": "Journal of Statistical Software",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Karatzoglou2004kernlabA,\n author = {Alexandros Karatzoglou and A. Smola and K. Hornik and A. Zeileis},\n journal = {Journal of Statistical Software},\n pages = {1-20},\n title = {kernlab - An S4 Package for Kernel Methods in R},\n volume = {11},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e044a4b5be1563fccc46e8f7552935b99365f90a",
            "@type": "ScholarlyArticle",
            "paperId": "e044a4b5be1563fccc46e8f7552935b99365f90a",
            "corpusId": 31363328,
            "url": "https://www.semanticscholar.org/paper/e044a4b5be1563fccc46e8f7552935b99365f90a",
            "title": "Learning with Support Vector Machines",
            "venue": "Learning with Support Vector Machines",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2092685308",
                "DBLP": "series/synthesis/2011Campbell",
                "DOI": "10.2200/S00324ED1V01Y201102AIM010",
                "CorpusId": 31363328
            },
            "abstract": "Support Vectors Machines have become a well established tool within machine learning. They work well in practice and have now been used across a wide range of applications from recognizing hand-written digits, to face identification, text categorisation, bioinformatics, and database marketing. In this book we give an introductory overview of this subject. We start with a simple Support Vector Machine for performing binary classification before considering multi-class classification and learning in the presence of noise. We show that this framework can be extended to many other scenarios such as prediction with real-valued outputs, novelty detection and the handling of complex output structures such as parse trees. Finally, we give an overview of the main types of kernels which are used in practice and how to learn and make predictions from multiple types of input data. Table of Contents: Support Vector Machines for Classification / Kernel-based Models / Learning with Kernels",
            "referenceCount": 73,
            "citationCount": 269,
            "influentialCitationCount": 31,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-3-031-01552-6/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2011-02-15",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Campbell2011LearningWS,\n author = {C. Campbell and Yiming Ying},\n booktitle = {Learning with Support Vector Machines},\n title = {Learning with Support Vector Machines},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:10f919b1a5161b560504c225cfb2d1b3a4768f80",
            "@type": "ScholarlyArticle",
            "paperId": "10f919b1a5161b560504c225cfb2d1b3a4768f80",
            "corpusId": 3779750,
            "url": "https://www.semanticscholar.org/paper/10f919b1a5161b560504c225cfb2d1b3a4768f80",
            "title": "Artificial intelligence in healthcare: past, present and future",
            "venue": "Stroke and vascular neurology",
            "publicationVenue": {
                "id": "urn:research:f46dbd50-8ba0-47aa-9649-1ba237101795",
                "name": "Stroke and vascular neurology",
                "alternate_names": [
                    "Stroke vasc neurol",
                    "Stroke and Vascular Neurology",
                    "Stroke Vasc Neurol"
                ],
                "issn": "2059-8696",
                "url": "https://svn.bmj.com/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2664267452",
                "PubMedCentral": "5829945",
                "DOI": "10.1136/svn-2017-000101",
                "CorpusId": 3779750,
                "PubMed": "29507784"
            },
            "abstract": "Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.",
            "referenceCount": 73,
            "citationCount": 1848,
            "influentialCitationCount": 43,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://svn.bmj.com/content/svnbmj/2/4/230.full.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2017-06-21",
            "journal": {
                "name": "Stroke and Vascular Neurology",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Jiang2017ArtificialII,\n author = {F. Jiang and Yong Jiang and Hui Zhi and Yi Dong and Hao Li and Sufeng Ma and Yilong Wang and Q. Dong and Haipeng Shen and Yongjun Wang},\n booktitle = {Stroke and vascular neurology},\n journal = {Stroke and Vascular Neurology},\n pages = {230 - 243},\n title = {Artificial intelligence in healthcare: past, present and future},\n volume = {2},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:38d8230a7aeae6554497b253848ad5bf677e4fb3",
            "@type": "ScholarlyArticle",
            "paperId": "38d8230a7aeae6554497b253848ad5bf677e4fb3",
            "corpusId": 53289556,
            "url": "https://www.semanticscholar.org/paper/38d8230a7aeae6554497b253848ad5bf677e4fb3",
            "title": "PennyLane: Automatic differentiation of hybrid quantum-classical computations",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1811-04968",
                "MAG": "2901859177",
                "CorpusId": 53289556
            },
            "abstract": "PennyLane is a Python 3 software framework for optimization and machine learning of quantum and hybrid quantum-classical computations. The library provides a unified architecture for near-term quantum computing devices, supporting both qubit and continuous-variable paradigms. PennyLane's core feature is the ability to compute gradients of variational quantum circuits in a way that is compatible with classical techniques such as backpropagation. PennyLane thus extends the automatic differentiation algorithms common in optimization and machine learning to include quantum and hybrid computations. A plugin system makes the framework compatible with any gate-based quantum simulator or hardware. We provide plugins for Strawberry Fields, Rigetti Forest, Qiskit, Cirq, and ProjectQ, allowing PennyLane optimizations to be run on publicly accessible quantum devices provided by Rigetti and IBM Q. On the classical front, PennyLane interfaces with accelerated machine learning libraries such as TensorFlow, PyTorch, and autograd. PennyLane can be used for the optimization of variational quantum eigensolvers, quantum approximate optimization, quantum machine learning models, and many other applications.",
            "referenceCount": 50,
            "citationCount": 573,
            "influentialCitationCount": 69,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-11-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1811.04968"
            },
            "citationStyles": {
                "bibtex": "@Article{Bergholm2018PennyLaneAD,\n author = {V. Bergholm and J. Izaac and M. Schuld and C. Gogolin and Ankit Khandelwal and N. Killoran},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {PennyLane: Automatic differentiation of hybrid quantum-classical computations},\n volume = {abs/1811.04968},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:52b7bf3ba59b31f362aa07f957f1543a29a4279e",
            "@type": "ScholarlyArticle",
            "paperId": "52b7bf3ba59b31f362aa07f957f1543a29a4279e",
            "corpusId": 52874011,
            "url": "https://www.semanticscholar.org/paper/52b7bf3ba59b31f362aa07f957f1543a29a4279e",
            "title": "Support-Vector Networks",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1995,
            "externalIds": {
                "MAG": "2119821739",
                "DOI": "10.1023/A:1022627411411",
                "CorpusId": 52874011
            },
            "abstract": null,
            "referenceCount": 26,
            "citationCount": 37697,
            "influentialCitationCount": 3541,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1022627411411.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1995-09-15",
            "journal": {
                "name": "Machine Learning",
                "volume": "20"
            },
            "citationStyles": {
                "bibtex": "@Article{Cortes1995SupportVectorN,\n author = {Corinna Cortes and V. Vapnik},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {273-297},\n title = {Support-Vector Networks},\n volume = {20},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:28f7cc049f3f4a4db81f0d0a608a4f57636cc35b",
            "@type": "ScholarlyArticle",
            "paperId": "28f7cc049f3f4a4db81f0d0a608a4f57636cc35b",
            "corpusId": 18666195,
            "url": "https://www.semanticscholar.org/paper/28f7cc049f3f4a4db81f0d0a608a4f57636cc35b",
            "title": "Quantum-chemical insights from deep tensor neural networks",
            "venue": "Nature Communications",
            "publicationVenue": {
                "id": "urn:research:43b3f0f9-489a-4566-8164-02fafde3cd98",
                "name": "Nature Communications",
                "alternate_names": [
                    "Nat Commun"
                ],
                "issn": "2041-1723",
                "url": "https://www.nature.com/ncomms/"
            },
            "year": 2016,
            "externalIds": {
                "PubMedCentral": "5228054",
                "MAG": "2527189750",
                "ArXiv": "1609.08259",
                "DOI": "10.1038/ncomms13890",
                "CorpusId": 18666195,
                "PubMed": "28067221"
            },
            "abstract": null,
            "referenceCount": 72,
            "citationCount": 1047,
            "influentialCitationCount": 38,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/ncomms13890.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Physics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-09-27",
            "journal": {
                "name": "Nature Communications",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Sch\u00fctt2016QuantumchemicalIF,\n author = {Kristof T. Sch\u00fctt and F. Arbabzadah and Stefan Chmiela and K. M\u00fcller and A. Tkatchenko},\n booktitle = {Nature Communications},\n journal = {Nature Communications},\n title = {Quantum-chemical insights from deep tensor neural networks},\n volume = {8},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8f7214bafbed6d4dfd397c35315c7275d5608f61",
            "@type": "ScholarlyArticle",
            "paperId": "8f7214bafbed6d4dfd397c35315c7275d5608f61",
            "corpusId": 14884315,
            "url": "https://www.semanticscholar.org/paper/8f7214bafbed6d4dfd397c35315c7275d5608f61",
            "title": "Learning Invariant Representations with Local Transformations",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2952390042",
                "ArXiv": "1206.6418",
                "DBLP": "conf/icml/SohnL12",
                "CorpusId": 14884315
            },
            "abstract": "Learning invariant representations is an important problem in machine learning and pattern recognition. In this paper, we present a novel framework of transformation-invariant feature learning by incorporating linear transformations into the feature learning algorithms. For example, we present the transformation-invariant restricted Boltzmann machine that compactly represents data by its weights and their transformations, which achieves invariance of the feature representation via probabilistic max pooling. In addition, we show that our transformation-invariant feature learning framework can also be extended to other unsupervised learning methods, such as autoencoders or sparse coding. We evaluate our method on several image classification benchmark datasets, such as MNIST variations, CIFAR-10, and STL-10, and show competitive or superior classification performance when compared to the state-of-the-art. Furthermore, our method achieves state-of-the-art performance on phone classification tasks with the TIMIT dataset, which demonstrates wide applicability of our proposed algorithms to other domains.",
            "referenceCount": 32,
            "citationCount": 184,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2012-06-26",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Sohn2012LearningIR,\n author = {Kihyuk Sohn and Honglak Lee},\n booktitle = {International Conference on Machine Learning},\n pages = {1339-1346},\n title = {Learning Invariant Representations with Local Transformations},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b8012351bc5ebce4a4b3039bbbba3ce393bc3315",
            "@type": "ScholarlyArticle",
            "paperId": "b8012351bc5ebce4a4b3039bbbba3ce393bc3315",
            "corpusId": 14805281,
            "url": "https://www.semanticscholar.org/paper/b8012351bc5ebce4a4b3039bbbba3ce393bc3315",
            "title": "An empirical evaluation of deep architectures on problems with many factors of variation",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "1994197834",
                "DBLP": "conf/icml/LarochelleECBB07",
                "DOI": "10.1145/1273496.1273556",
                "CorpusId": 14805281
            },
            "abstract": "Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.",
            "referenceCount": 13,
            "citationCount": 1121,
            "influentialCitationCount": 116,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-06-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Larochelle2007AnEE,\n author = {H. Larochelle and D. Erhan and Aaron C. Courville and J. Bergstra and Yoshua Bengio},\n booktitle = {International Conference on Machine Learning},\n pages = {473-480},\n title = {An empirical evaluation of deep architectures on problems with many factors of variation},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a5e8141d4e323ff7f1f49dbbce293b0d6f739464",
            "@type": "ScholarlyArticle",
            "paperId": "a5e8141d4e323ff7f1f49dbbce293b0d6f739464",
            "corpusId": 6596,
            "url": "https://www.semanticscholar.org/paper/a5e8141d4e323ff7f1f49dbbce293b0d6f739464",
            "title": "Very Simple Classification Rules Perform Well on Most Commonly Used Datasets",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1993,
            "externalIds": {
                "MAG": "2132166479",
                "DBLP": "journals/ml/Holte93",
                "DOI": "10.1023/A:1022631118932",
                "CorpusId": 6596
            },
            "abstract": null,
            "referenceCount": 69,
            "citationCount": 1774,
            "influentialCitationCount": 113,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1022631118932.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1993-04-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Holte1993VerySC,\n author = {R. Holte},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {63-90},\n title = {Very Simple Classification Rules Perform Well on Most Commonly Used Datasets},\n volume = {11},\n year = {1993}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6120cc252bc74239012f11b8b075cb7cb16bee26",
            "@type": "ScholarlyArticle",
            "paperId": "6120cc252bc74239012f11b8b075cb7cb16bee26",
            "corpusId": 2073260,
            "url": "https://www.semanticscholar.org/paper/6120cc252bc74239012f11b8b075cb7cb16bee26",
            "title": "An Introduction to Variational Methods for Graphical Models",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1516111018",
                "DBLP": "journals/ml/JordanGJS99",
                "DOI": "10.1023/A:1007665907178",
                "CorpusId": 2073260
            },
            "abstract": null,
            "referenceCount": 73,
            "citationCount": 4170,
            "influentialCitationCount": 304,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cis.upenn.edu/~mkearns/papers/barbados/jgjs-var.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1999-02-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "37"
            },
            "citationStyles": {
                "bibtex": "@Article{Jordan1999AnIT,\n author = {Michael I. Jordan and Zoubin Ghahramani and T. Jaakkola and L. Saul},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {183-233},\n title = {An Introduction to Variational Methods for Graphical Models},\n volume = {37},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a9763afda62e960c35c80681f805ddecbef14a92",
            "@type": "ScholarlyArticle",
            "paperId": "a9763afda62e960c35c80681f805ddecbef14a92",
            "corpusId": 75489746,
            "url": "https://www.semanticscholar.org/paper/a9763afda62e960c35c80681f805ddecbef14a92",
            "title": "Images of Organization",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1988,
            "externalIds": {
                "MAG": "2024135760",
                "DOI": "10.1097/00000446-198803000-00042",
                "CorpusId": 75489746
            },
            "abstract": "Preface Part I. An Overview Introduction Part II. Some Images of Organization 2. Mechanization Takes Command: Organizations as Machines Machines, Mechanical Thinking, and the Rise of Bureaucratic Organization The Origins of Mechanistic Organization Classical Management Theory: Designing bureaucratic organizations Scientific Management Strengths and Limitations of the Machine Metaphor 3. Nature Intervenes: Organizations as Organisms Discovering Organizational Needs Recognizing the Importance of Environment: Organizations as Open Systems Contingency Theory: Adapting Organization to Environment The Variety of the Species Contingency Theory: Promoting Organizational Health and Development Natural Selection: The Population-Ecology View of Organizations Organizational Ecology: The Creation of Shared Futures Strengths and Limitations of the Organismic Metaphor 4. Learning and Self-Organization: Organizations as Brains Images of the Brain Organizations as Information Processing Brains Creating Learning Organizations Cybernetics, Learning, and Learning to Learn Can Organizations Learn to Learn? Guidelines for \"Learning Organizations\" Organizations as Holographic Brains Principles of Holographic Design Strengths and Limitations of the Brain Metaphors 5. Creating Social Realty: Organizations as Cultures Culture and Organization Organization as a Cultural Phenomenon Organization and Cultural Context Corporate Cultures and Subcultures Creating Organizational Reality Culture: Rule Following or Enactment? Organization: The enactment of a Shared Reality Strengths and Limitations of the Cultural Metaphor 6. Interests, Conflict, and Power: Organizations as Political Systems Organizations as Systems of Government Organizations as Systems of Political Activity Analyzing Interests Understanding Conflict Exploring Power Managing Pluralist Organizations Strengths and Limitations of the Political Metaphor 7. Exploring Plato's Cave: Organizations as Psychic Prisons The Trap of Favored Ways of Thinking Organization and the Unconscious Organization and Repressed Sexuality Organization and the Patriarchal Family Organization, Death, and Immortality Organization and Anxiety Organization, Dolls, and Teddy Bears Organization, Shadow, and Archetype The Unconscious: A Creative and Destructive Force Strengths and Limitations of the Psychic Prison Metaphor 8. Unfolding Logics of Change: Organization as Flux and Transformation Autopoiesis: Rethinking Relations With the Environment Enactment as a Form of Narcissism: Organizations Interact With Projections of Themselves Identity and Closure: Egocentrism Versus Systemic Wisdom Shifting \"Attractors\": The Logic of Chaos and Complexity Managing in the Midst of Complexity Loops, Not Lines: The Logic of Mutual Causality Contradiction and Crisis: The Logic of Dialectical Change Dialectical Analysis: How Opposing Forces Drive Change The Dialectics of Management Strengths and Limitations of the Flux and Transformation Metaphor 9. The Ugly Face: Organizations as Instruments of Domination Organization as Domination How Organizations Use and Exploit Their Employees Organization, Class, and Control Work Hazards, Occupational Disease, and Industrial Accidents Workaholism and Social and Mental Stress Organizational Politics and the Radicalized Organization Multinationals and the World Economy The Multinationals as World Powers Multinationals: A Record of Exploitation? Strengths and Limitations of the Domination Metaphor Part III. Implications For Practice 10. The Challenge of Metaphor Metaphors Create Ways of Seeing and Shaping Organizational Life Seeing, Thinking, and Acting in New Ways 11. Reading and Shaping Organizational Life The Multicom Case Interpreting Multicom Developing and Detailed Reading and \"Storyline\" Multicom From Another View \"Reading\" and Emergent Intelligence 12. Postscript Bibliographic Notes Introduction The Machine Metaphor The Organismic Metaphor The Brain Metaphor The Culture Metaphor The Political Metaphor The Psychic Prison Metaphor The Flux and Transformation Metaphor The Domination Metaphor The Challenge of Metaphor Reading and Shaping Organizational Life Postscript Bibliography",
            "referenceCount": 1,
            "citationCount": 6810,
            "influentialCitationCount": 408,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Psychology",
                "Sociology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Sociology",
                    "source": "external"
                },
                {
                    "category": "Business",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "1988-03-01",
            "journal": {
                "name": "American Journal of Nursing",
                "volume": "88"
            },
            "citationStyles": {
                "bibtex": "@Article{Alexander1988ImagesOO,\n author = {J. Alexander and G. Morgan},\n journal = {American Journal of Nursing},\n pages = {395},\n title = {Images of Organization},\n volume = {88},\n year = {1988}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:80196cdfcd0c6ce2953bf65a7f019971e2026386",
            "@type": "ScholarlyArticle",
            "paperId": "80196cdfcd0c6ce2953bf65a7f019971e2026386",
            "corpusId": 3645060,
            "url": "https://www.semanticscholar.org/paper/80196cdfcd0c6ce2953bf65a7f019971e2026386",
            "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2950708659",
                "DBLP": "conf/icml/EspeholtSMSMWDF18",
                "ArXiv": "1802.01561",
                "CorpusId": 3645060
            },
            "abstract": "In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.",
            "referenceCount": 42,
            "citationCount": 1296,
            "influentialCitationCount": 250,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-02-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.01561"
            },
            "citationStyles": {
                "bibtex": "@Article{Espeholt2018IMPALASD,\n author = {Lasse Espeholt and Hubert Soyer and R. Munos and K. Simonyan and Volodymyr Mnih and Tom Ward and Yotam Doron and Vlad Firoiu and Tim Harley and Iain Dunning and S. Legg and K. Kavukcuoglu},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures},\n volume = {abs/1802.01561},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:24e6cf0796237f21c780a3f0c996817f57b3a1bd",
            "@type": "ScholarlyArticle",
            "paperId": "24e6cf0796237f21c780a3f0c996817f57b3a1bd",
            "corpusId": 206787478,
            "url": "https://www.semanticscholar.org/paper/24e6cf0796237f21c780a3f0c996817f57b3a1bd",
            "title": "Support-vector networks",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "journals/ml/CortesV95",
                "DOI": "10.1007/BF00994018",
                "CorpusId": 206787478
            },
            "abstract": null,
            "referenceCount": 15,
            "citationCount": 6150,
            "influentialCitationCount": 139,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2FBF00994018.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "20"
            },
            "citationStyles": {
                "bibtex": "@Article{Cortes2004SupportvectorN,\n author = {Corinna Cortes and V. Vapnik},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {273-297},\n title = {Support-vector networks},\n volume = {20},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:864e7db59f2ccfec1ee9f6eba79566ac7b0634df",
            "@type": "ScholarlyArticle",
            "paperId": "864e7db59f2ccfec1ee9f6eba79566ac7b0634df",
            "corpusId": 163946,
            "url": "https://www.semanticscholar.org/paper/864e7db59f2ccfec1ee9f6eba79566ac7b0634df",
            "title": "Convolutional Pose Machines",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1602.00134",
                "MAG": "2951987817",
                "DBLP": "journals/corr/WeiRKS16",
                "DOI": "10.1109/CVPR.2016.511",
                "CorpusId": 163946
            },
            "abstract": "Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.",
            "referenceCount": 47,
            "citationCount": 2472,
            "influentialCitationCount": 314,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1602.00134",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-01-30",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wei2016ConvolutionalPM,\n author = {S. Wei and V. Ramakrishna and T. Kanade and Yaser Sheikh},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4724-4732},\n title = {Convolutional Pose Machines},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5966d7c7f60898d610812e24c64d4d57855ad86a",
            "@type": "ScholarlyArticle",
            "paperId": "5966d7c7f60898d610812e24c64d4d57855ad86a",
            "corpusId": 23163324,
            "url": "https://www.semanticscholar.org/paper/5966d7c7f60898d610812e24c64d4d57855ad86a",
            "title": "Semantics derived automatically from language corpora contain human-like biases",
            "venue": "Science",
            "publicationVenue": {
                "id": "urn:research:f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
                "name": "Science",
                "alternate_names": null,
                "issn": "0193-4511",
                "url": "https://www.jstor.org/journal/science"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1608.07187",
                "MAG": "3105700579",
                "DBLP": "journals/corr/IslamBN16",
                "DOI": "10.1126/science.aal4230",
                "CorpusId": 23163324,
                "PubMed": "28408601"
            },
            "abstract": "Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",
            "referenceCount": 77,
            "citationCount": 1970,
            "influentialCitationCount": 229,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://researchportal.bath.ac.uk/files/168480066/CaliskanEtAl_authors_full.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-08-25",
            "journal": {
                "name": "Science",
                "volume": "356"
            },
            "citationStyles": {
                "bibtex": "@Article{Caliskan2016SemanticsDA,\n author = {Aylin Caliskan and J. Bryson and Arvind Narayanan},\n booktitle = {Science},\n journal = {Science},\n pages = {183 - 186},\n title = {Semantics derived automatically from language corpora contain human-like biases},\n volume = {356},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:554f3b32b956035fbfabba730c6f0300d6955dce",
            "@type": "ScholarlyArticle",
            "paperId": "554f3b32b956035fbfabba730c6f0300d6955dce",
            "corpusId": 6746439,
            "url": "https://www.semanticscholar.org/paper/554f3b32b956035fbfabba730c6f0300d6955dce",
            "title": "Learning logical definitions from relations",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1990,
            "externalIds": {
                "DBLP": "journals/ml/Quinlan90",
                "MAG": "1999138184",
                "DOI": "10.1007/BF00117105",
                "CorpusId": 6746439
            },
            "abstract": null,
            "referenceCount": 39,
            "citationCount": 1040,
            "influentialCitationCount": 137,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/BF00117105.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1990-09-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Quinlan1990LearningLD,\n author = {J. R. Quinlan},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {239-266},\n title = {Learning logical definitions from relations},\n volume = {5},\n year = {1990}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3",
            "@type": "ScholarlyArticle",
            "paperId": "7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3",
            "corpusId": 18420840,
            "url": "https://www.semanticscholar.org/paper/7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3",
            "title": "ALVINN, an autonomous land vehicle in a neural network",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "CorpusId": 18420840
            },
            "abstract": "The support-vector network is a new leaming machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very highdimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.",
            "referenceCount": 6,
            "citationCount": 1833,
            "influentialCitationCount": 123,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Pomerleau2015ALVINNAA,\n author = {D. Pomerleau},\n title = {ALVINN, an autonomous land vehicle in a neural network},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:83040001210751239553269727b9ea53e152af71",
            "@type": "ScholarlyArticle",
            "paperId": "83040001210751239553269727b9ea53e152af71",
            "corpusId": 260496023,
            "url": "https://www.semanticscholar.org/paper/83040001210751239553269727b9ea53e152af71",
            "title": "Building Machines that Learn and Think Like People",
            "venue": "Adaptive Agents and Multi-Agent Systems",
            "publicationVenue": {
                "id": "urn:research:6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                "name": "Adaptive Agents and Multi-Agent Systems",
                "alternate_names": [
                    "Adapt Agent Multi-agent Syst",
                    "International Joint Conference on Autonomous Agents & Multiagent Systems",
                    "Adapt Agent Multi-agents Syst",
                    "AAMAS",
                    "Adaptive Agents and Multi-Agents Systems",
                    "Int Jt Conf Auton Agent  Multiagent Syst"
                ],
                "issn": null,
                "url": "http://www.ifaamas.org/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963305465",
                "DBLP": "conf/atal/Tenenbaum18",
                "CorpusId": 260496023
            },
            "abstract": "Recent successes in artificial intelligence and machine learning have been largely driven by methods for sophisticated pattern recognition, including deep neural networks and other data-intensive methods. But human intelligence is more than just pattern recognition. And no machine system yet built has anything like the flexible, general-purpose commonsense grasp of the world that we can see in even a one-year-old human infant. I will consider how we might capture the basic learning and thinking abilities humans possess from early childhood, as one route to building more human-like forms of machine learning and thinking. At the heart of human common sense is our ability to model the physical and social environment around us: to explain and understand what we see, to imagine things we could see but haven't yet, to solve problems and plan actions to make these things real, and to build new models as we learn more about the world. I will focus on our recent work reverse-engineering these capacities using methods from probabilistic programming, program induction and program synthesis, which together with deep learning methods and video game simulation engines, provide a toolkit for the joint enterprise of modeling human intelligence and making AI systems smarter in more human-like ways.",
            "referenceCount": 195,
            "citationCount": 627,
            "influentialCitationCount": 34,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-07-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tenenbaum2018BuildingMT,\n author = {J. Tenenbaum},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {5},\n title = {Building Machines that Learn and Think Like People},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:223841a71f5bce4cb03040e229d13e9a71b78ec3",
            "@type": "ScholarlyArticle",
            "paperId": "223841a71f5bce4cb03040e229d13e9a71b78ec3",
            "corpusId": 17162382,
            "url": "https://www.semanticscholar.org/paper/223841a71f5bce4cb03040e229d13e9a71b78ec3",
            "title": "Persistence Images: A Stable Vector Representation of Persistent Homology",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2272065411",
                "ArXiv": "1507.06217",
                "DBLP": "journals/jmlr/AdamsEKNPSCHMZ17",
                "CorpusId": 17162382
            },
            "abstract": "Many datasets can be viewed as a noisy sampling of an underlying space, and tools from topological data analysis can characterize this structure for the purpose of knowledge discovery. One such tool is persistent homology, which provides a multiscale description of the homological features within a dataset. A useful representation of this homological information is a persistence diagram (PD). Efforts have been made to map PDs into spaces with additional structure valuable to machine learning tasks. We convert a PD to a finite-dimensional vector representation which we call a persistence image (PI), and prove the stability of this transformation with respect to small perturbations in the inputs. The discriminatory power of PIs is compared against existing methods, showing significant performance gains. We explore the use of PIs with vector-based machine learning tools, such as linear sparse support vector machines, which identify features containing discriminating topological information. Finally, high accuracy inference of parameter values from the dynamic output of a discrete dynamical system (the linked twist map) and a partial differential equation (the anisotropic Kuramoto-Sivashinsky equation) provide a novel application of the discriminatory power of PIs.",
            "referenceCount": 60,
            "citationCount": 529,
            "influentialCitationCount": 80,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-07-22",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "18"
            },
            "citationStyles": {
                "bibtex": "@Article{Adams2015PersistenceIA,\n author = {Henry Adams and T. Emerson and M. Kirby and R. Neville and C. Peterson and Patrick D. Shipman and Sofya Chepushtanova and Eric M. Hanson and Francis C. Motta and Lori Ziegelmeier},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {8:1-8:35},\n title = {Persistence Images: A Stable Vector Representation of Persistent Homology},\n volume = {18},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cef5f429a2ee5bad3a258c7baf7cc77e9af047a3",
            "@type": "ScholarlyArticle",
            "paperId": "cef5f429a2ee5bad3a258c7baf7cc77e9af047a3",
            "corpusId": 11493856,
            "url": "https://www.semanticscholar.org/paper/cef5f429a2ee5bad3a258c7baf7cc77e9af047a3",
            "title": "Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/jmlr/KotthoffTHHL17",
                "MAG": "2947283326",
                "DOI": "10.1007/978-3-030-05318-5_4",
                "CorpusId": 11493856
            },
            "abstract": null,
            "referenceCount": 36,
            "citationCount": 641,
            "influentialCitationCount": 45,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-05318-5_4.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kotthoff2017AutoWEKA2A,\n author = {Lars Kotthoff and C. Thornton and H. Hoos and F. Hutter and Kevin Leyton-Brown},\n booktitle = {Journal of machine learning research},\n pages = {81-95},\n title = {Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:990a02f20529f5ce3b382f1d54648afaab391179",
            "@type": "ScholarlyArticle",
            "paperId": "990a02f20529f5ce3b382f1d54648afaab391179",
            "corpusId": 9089716,
            "url": "https://www.semanticscholar.org/paper/990a02f20529f5ce3b382f1d54648afaab391179",
            "title": "Poisoning Attacks against Support Vector Machines",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2012,
            "externalIds": {
                "ArXiv": "1206.6389",
                "DBLP": "conf/icml/BiggioNL12",
                "MAG": "2112507308",
                "CorpusId": 9089716
            },
            "abstract": "We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data. \n \nThe proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.",
            "referenceCount": 23,
            "citationCount": 1286,
            "influentialCitationCount": 98,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2012-06-26",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Biggio2012PoisoningAA,\n author = {B. Biggio and B. Nelson and P. Laskov},\n booktitle = {International Conference on Machine Learning},\n pages = {1467-1474},\n title = {Poisoning Attacks against Support Vector Machines},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:29650544fded20dd5b2fc49f60f9a3ad30d0e275",
            "@type": "ScholarlyArticle",
            "paperId": "29650544fded20dd5b2fc49f60f9a3ad30d0e275",
            "corpusId": 67866899,
            "url": "https://www.semanticscholar.org/paper/29650544fded20dd5b2fc49f60f9a3ad30d0e275",
            "title": "Speech Recognition Using Deep Neural Networks: A Systematic Review",
            "venue": "IEEE Access",
            "publicationVenue": {
                "id": "urn:research:2633f5b2-c15c-49fe-80f5-07523e770c26",
                "name": "IEEE Access",
                "alternate_names": null,
                "issn": "2169-3536",
                "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2912581782",
                "DBLP": "journals/access/NassifSAAS19",
                "DOI": "10.1109/ACCESS.2019.2896880",
                "CorpusId": 67866899
            },
            "abstract": "Over the past decades, a tremendous amount of research has been done on the use of machine learning for speech processing applications, especially speech recognition. However, in the past few years, research has focused on utilizing deep learning for speech-related applications. This new area of machine learning has yielded far better results when compared to others in a variety of applications including speech, and thus became a very attractive area of research. This paper provides a thorough examination of the different studies that have been conducted since 2006, when deep learning first arose as a new area of machine learning, for speech applications. A thorough statistical analysis is provided in this review which was conducted by extracting specific information from 174 papers published between the years 2006 and 2018. The results provided in this paper shed light on the trends of research in this area as well as bring focus to new research topics.",
            "referenceCount": 247,
            "citationCount": 609,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/8600701/08632885.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-02-01",
            "journal": {
                "name": "IEEE Access",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Nassif2019SpeechRU,\n author = {A. B. Nassif and I. Shahin and Imtinan B. Attili and Mohammad Azzeh and K. Shaalan},\n booktitle = {IEEE Access},\n journal = {IEEE Access},\n pages = {19143-19165},\n title = {Speech Recognition Using Deep Neural Networks: A Systematic Review},\n volume = {7},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1626c940a64ad96a7ed53d7d6c0df63c6696956b",
            "@type": "ScholarlyArticle",
            "paperId": "1626c940a64ad96a7ed53d7d6c0df63c6696956b",
            "corpusId": 7285098,
            "url": "https://www.semanticscholar.org/paper/1626c940a64ad96a7ed53d7d6c0df63c6696956b",
            "title": "Restricted Boltzmann machines for collaborative filtering",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2099866409",
                "DBLP": "conf/icml/SalakhutdinovMH07",
                "DOI": "10.1145/1273496.1273596",
                "CorpusId": 7285098
            },
            "abstract": "Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.",
            "referenceCount": 15,
            "citationCount": 1991,
            "influentialCitationCount": 158,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-06-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Salakhutdinov2007RestrictedBM,\n author = {R. Salakhutdinov and A. Mnih and Geoffrey E. Hinton},\n booktitle = {International Conference on Machine Learning},\n pages = {791-798},\n title = {Restricted Boltzmann machines for collaborative filtering},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9691f67f5075bde2fd70da0135a4a70f25ef042b",
            "@type": "ScholarlyArticle",
            "paperId": "9691f67f5075bde2fd70da0135a4a70f25ef042b",
            "corpusId": 53306004,
            "url": "https://www.semanticscholar.org/paper/9691f67f5075bde2fd70da0135a4a70f25ef042b",
            "title": "Pegasos: primal estimated sub-gradient solver for SVM",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "conf/icml/Shalev-ShwartzSS07",
                "MAG": "3005526581",
                "DOI": "10.1145/1273496.1273598",
                "CorpusId": 53306004
            },
            "abstract": "We describe and analyze a simple and effective stochastic sub-gradient descent algorithm for solving the optimization problem cast by Support Vector Machines (SVM). We prove that the number of iterations required to obtain a solution of accuracy $${\\epsilon}$$ is $${\\tilde{O}(1 / \\epsilon)}$$, where each iteration operates on a single training example. In contrast, previous analyses of stochastic gradient descent methods for SVMs require $${\\Omega(1 / \\epsilon^2)}$$ iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with 1/\u03bb, where \u03bb is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is $${\\tilde{O}(d/(\\lambda \\epsilon))}$$, where d is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach also extends to non-linear kernels while working solely on the primal objective function, though in this case the runtime does depend linearly on the training set size. Our algorithm is particularly well suited for large text classification problems, where we demonstrate an order-of-magnitude speedup over previous SVM learning methods.",
            "referenceCount": 41,
            "citationCount": 2241,
            "influentialCitationCount": 272,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.huji.ac.il/~shais/papers/ShalevSiSrCo10.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-06-20",
            "journal": {
                "name": "Mathematical Programming",
                "volume": "127"
            },
            "citationStyles": {
                "bibtex": "@Article{Shalev-Shwartz2007PegasosPE,\n author = {S. Shalev-Shwartz and Y. Singer and N. Srebro and Andrew Cotter},\n booktitle = {International Conference on Machine Learning},\n journal = {Mathematical Programming},\n pages = {3-30},\n title = {Pegasos: primal estimated sub-gradient solver for SVM},\n volume = {127},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8",
            "@type": "ScholarlyArticle",
            "paperId": "74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8",
            "corpusId": 14591650,
            "url": "https://www.semanticscholar.org/paper/74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8",
            "title": "Transductive Inference for Text Classification using Support Vector Machines",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1999,
            "externalIds": {
                "MAG": "2107008379",
                "DBLP": "conf/icml/Joachims99",
                "CorpusId": 14591650
            },
            "abstract": "This paper introduces Transductive Support Vector Machines (TSVMs) for text classi cation. While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transductive Support Vector Machines take into account a particular test set and try to minimize misclassi cations of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classi cation. These theoretical ndings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a twentieth on some tasks. This work also proposes an algorithm for training TSVMs e ciently, handling 10,000 examples and more.",
            "referenceCount": 20,
            "citationCount": 3181,
            "influentialCitationCount": 439,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1999-06-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Joachims1999TransductiveIF,\n author = {T. Joachims},\n booktitle = {International Conference on Machine Learning},\n pages = {200-209},\n title = {Transductive Inference for Text Classification using Support Vector Machines},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0ef7d9e618cbb507d69f8ebcdc60b8a1f3135bff",
            "@type": "ScholarlyArticle",
            "paperId": "0ef7d9e618cbb507d69f8ebcdc60b8a1f3135bff",
            "corpusId": 5306879,
            "url": "https://www.semanticscholar.org/paper/0ef7d9e618cbb507d69f8ebcdc60b8a1f3135bff",
            "title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2091825929",
                "DBLP": "conf/icml/Zhang04",
                "DOI": "10.1145/1015330.1015332",
                "CorpusId": 5306879
            },
            "abstract": "Linear prediction methods, such as least squares for regression, logistic regression and support vector machines for classification, have been extensively used in statistics and machine learning. In this paper, we study stochastic gradient descent (SGD) algorithms on regularized forms of linear prediction methods. This class of methods, related to online algorithms such as perceptron, are both efficient and very simple to implement. We obtain numerical rate of convergence for such algorithms, and discuss its implications. Experiments on text data will be provided to demonstrate numerical and statistical consequences of our theoretical findings.",
            "referenceCount": 10,
            "citationCount": 1138,
            "influentialCitationCount": 120,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2004-07-04",
            "journal": {
                "name": "Proceedings of the twenty-first international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2004SolvingLS,\n author = {Tong Zhang},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the twenty-first international conference on Machine learning},\n title = {Solving large scale linear prediction problems using stochastic gradient descent algorithms},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e75b3c12da067552fda910a5bbed8b4d0e82dbcb",
            "@type": "ScholarlyArticle",
            "paperId": "e75b3c12da067552fda910a5bbed8b4d0e82dbcb",
            "corpusId": 115844916,
            "url": "https://www.semanticscholar.org/paper/e75b3c12da067552fda910a5bbed8b4d0e82dbcb",
            "title": " Neural Network Methods for Natural Language Processing",
            "venue": "Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:ee37a78c-f3d8-407a-bd24-bb97fe6dbab9",
                "name": "Computational Linguistics",
                "alternate_names": [
                    "Comput Linguistics"
                ],
                "issn": "0891-2017",
                "url": "http://aclanthology.info/venues/cl"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/coling/GoldbergHLZ18",
                "MAG": "2793652419",
                "DOI": "10.2200/S00762ED1V01Y201703HLT037",
                "CorpusId": 115844916
            },
            "abstract": "Neural networks are a family of powerful machine learning models. This book focuses on the application of neural network models to natural language data. The first half of the book (Parts I and II) covers the basics of supervised machine learning and feed-forward neural networks, the basics of working with machine learning over language data, and the use of vector-based rather than symbolic representations for words. It also covers the computation-graph abstraction, which allows to easily define and train arbitrary neural networks, and is the basis behind the design of contemporary neural network software libraries.\r\n\r\nThe second part of the book (Parts III and IV) introduces more specialized neural network architectures, including 1D convolutional neural networks, recurrent neural networks, conditioned-generation models, and attention-based models. These architectures and techniques are the driving force behind state-of-the-art algorithms for machine translation, syntactic parsing, and many other applications. Finally, we also discuss tree-shaped networks, structured prediction, and the prospects of multi-task learning.",
            "referenceCount": 0,
            "citationCount": 537,
            "influentialCitationCount": 40,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm%3A978-3-031-02165-7%2F1",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Synthesis Lectures on Human Language Technologies",
                "volume": "-"
            },
            "citationStyles": {
                "bibtex": "@Article{Goldberg2017NeuralNM,\n author = {Yoav Goldberg},\n booktitle = {Computational Linguistics},\n journal = {Synthesis Lectures on Human Language Technologies},\n pages = {1-309},\n title = { Neural Network Methods for Natural Language Processing},\n volume = {-},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0fb91cf9a406ab8d1c124f0fe800d7048f7e3898",
            "@type": "ScholarlyArticle",
            "paperId": "0fb91cf9a406ab8d1c124f0fe800d7048f7e3898",
            "corpusId": 64130086,
            "url": "https://www.semanticscholar.org/paper/0fb91cf9a406ab8d1c124f0fe800d7048f7e3898",
            "title": "DATA CLASSIFICATION USING SUPPORT VECTOR MACHINE",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2463280635",
                "CorpusId": 64130086
            },
            "abstract": "Classification is one of the most important tasks for different application such as text categorization, tone recognition, image classification, micro-array gene expression, proteins structure predictions, data Classification etc. Most of the existing supervised classification methods are based on traditional statistics, which can provide ideal results when sample size is tending to infinity. However, only finite samples can be acquired in practice. In this paper, a novel learning method, Support Vector Machine (SVM), is applied on different data (Diabetes data, Heart Data, Satellite Data and Shuttle data) which have two or multi class. SVM, a powerful machine method developed from statistical learning and has made significant achievement in some field. Introduced in the early 90\u2019s, they led to an explosion of interest in machine learning. The foundations of SVM have been developed by Vapnik and are gaining popularity in field of machine learning due to many attractive features and promising empirical performance. SVM method does not suffer the limitations of data dimensionality and limited samples [1] & [2]. In our experiment, the support vectors, which are critical for classification, are obtained by learning from the training samples. In this paper we have shown the comparative results using different kernel functions for all data samples.",
            "referenceCount": 8,
            "citationCount": 431,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Srivastava2009DATACU,\n author = {D. Srivastava and L. Bhambhu},\n title = {DATA CLASSIFICATION USING SUPPORT VECTOR MACHINE},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:73fb1f854dd152af88835298a54b9bfbb9f711ce",
            "@type": "ScholarlyArticle",
            "paperId": "73fb1f854dd152af88835298a54b9bfbb9f711ce",
            "corpusId": 12767870,
            "url": "https://www.semanticscholar.org/paper/73fb1f854dd152af88835298a54b9bfbb9f711ce",
            "title": "Learning to Detect and Classify Malicious Executables in the Wild",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2121749752",
                "DBLP": "journals/jmlr/KolterM06",
                "DOI": "10.5555/1248547.1248646",
                "CorpusId": 12767870
            },
            "abstract": "We describe the use of machine learning and data mining to detect and classify malicious executables as they appear in the wild. We gathered 1,971 benign and 1,651 malicious executables and encoded each as a training example using n-grams of byte codes as features. Such processing resulted in more than 255 million distinct n-grams. After selecting the most relevant n-grams for prediction, we evaluated a variety of inductive methods, including naive Bayes, decision trees, support vector machines, and boosting. Ultimately, boosted decision trees outperformed other methods with an area under the ROC curve of 0.996. Results suggest that our methodology will scale to larger collections of executables. We also evaluated how well the methods classified executables based on the function of their payload, such as opening a backdoor and mass-mailing. Areas under the ROC curve for detecting payload function were in the neighborhood of 0.9, which were smaller than those for the detection task. However, we attribute this drop in performance to fewer training examples and to the challenge of obtaining properly labeled examples, rather than to a failing of the methodology or to some inherent difficulty of the classification task. Finally, we applied detectors to 291 malicious executables discovered after we gathered our original collection, and boosted decision trees achieved a true-positive rate of 0.98 for a desired false-positive rate of 0.05. This result is particularly important, for it suggests that our methodology could be used as the basis for an operational system for detecting previously undiscovered malicious executables.",
            "referenceCount": 44,
            "citationCount": 623,
            "influentialCitationCount": 57,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Kolter2006LearningTD,\n author = {J. Z. Kolter and M. Maloof and R. Lippmann},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {2721-2744},\n title = {Learning to Detect and Classify Malicious Executables in the Wild},\n volume = {7},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0c91d5305ad34814b631d4a642bb0535a2e066ea",
            "@type": "ScholarlyArticle",
            "paperId": "0c91d5305ad34814b631d4a642bb0535a2e066ea",
            "corpusId": 9718485,
            "url": "https://www.semanticscholar.org/paper/0c91d5305ad34814b631d4a642bb0535a2e066ea",
            "title": "Feature selection based on mutual information",
            "venue": "International Conference on Information Technology in Asia",
            "publicationVenue": {
                "id": "urn:research:4a3bf020-a6c4-449b-ade9-fc8436ae42e9",
                "name": "International Conference on Information Technology in Asia",
                "alternate_names": [
                    "Conference Information Technology and Its Applications",
                    "Int Conf Inf Technol Asia",
                    "Conf Inf Technol It Appl",
                    "CITA",
                    "Conference on Information Technology in Asia",
                    "Conf Inf Technol Asia"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=482"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/cita/SulaimanL15a",
                "MAG": "2294605953",
                "DOI": "10.1109/CITA.2015.7349827",
                "CorpusId": 9718485
            },
            "abstract": "The application of machine learning models such as support vector machine (SVM) and artificial neural networks (ANN) in predicting reservoir properties has been effective in the recent years when compared with the traditional empirical methods. Despite that the machine learning models suffer a lot in the faces of uncertain data which is common characteristics of well log dataset. The reason for uncertainty in well log dataset includes a missing scale, data interpretation and measurement error problems. Feature Selection aimed at selecting feature subset that is relevant to the predicting property. In this paper a feature selection based on mutual information criterion is proposed, the strong point of this method relies on the choice of threshold based on statistically sound criterion for the typical greedy feedforward method of feature selection. Experimental results indicate that the proposed method is capable of improving the performance of the machine learning models in terms of prediction accuracy and reduction in training time.",
            "referenceCount": 21,
            "citationCount": 563,
            "influentialCitationCount": 42,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://ir.unimas.my/13446/1/Feature%20Selection%20based%20on%20Mutual%20Information%20%28abstract%29.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-12-10",
            "journal": {
                "name": "2015 9th International Conference on IT in Asia (CITA)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sulaiman2015FeatureSB,\n author = {Muhammad Aliyu Sulaiman and J. Labadin},\n booktitle = {International Conference on Information Technology in Asia},\n journal = {2015 9th International Conference on IT in Asia (CITA)},\n pages = {1-6},\n title = {Feature selection based on mutual information},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dfa2727c776fc5b8dcd4d9217e4564e578ddb5a5",
            "@type": "ScholarlyArticle",
            "paperId": "dfa2727c776fc5b8dcd4d9217e4564e578ddb5a5",
            "corpusId": 15912590,
            "url": "https://www.semanticscholar.org/paper/dfa2727c776fc5b8dcd4d9217e4564e578ddb5a5",
            "title": "Bioinformatics and Computational Biology Solutions using R and Bioconductor",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2005,
            "externalIds": {
                "CorpusId": 15912590
            },
            "abstract": "In this chapter, supervised machine learning methods are described in the context of microarray applications. The most widely used families of machine learning methods are described, along with various approaches to learner assessment. The Bioconductor interfaces to machine learning tools are described and illustrated. Key problems of model selection and interpretation are reviewed in examples.",
            "referenceCount": 26,
            "citationCount": 2240,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Gentleman2005BioinformaticsAC,\n author = {R. Gentleman and Vince Carey and W. Huber},\n title = {Bioinformatics and Computational Biology Solutions using R and Bioconductor},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9dc134b18c06577354d50c12a8972b965d3bbacd",
            "@type": "ScholarlyArticle",
            "paperId": "9dc134b18c06577354d50c12a8972b965d3bbacd",
            "corpusId": 1909790,
            "url": "https://www.semanticscholar.org/paper/9dc134b18c06577354d50c12a8972b965d3bbacd",
            "title": "Learning to Decode Cognitive States from Brain Images",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2112532472",
                "DBLP": "journals/ml/MitchellHNPWJN04",
                "DOI": "10.1023/B:MACH.0000035475.85309.1b",
                "CorpusId": 1909790
            },
            "abstract": null,
            "referenceCount": 39,
            "citationCount": 747,
            "influentialCitationCount": 51,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/B:MACH.0000035475.85309.1b.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2004-10-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "57"
            },
            "citationStyles": {
                "bibtex": "@Article{Mitchell2004LearningTD,\n author = {Tom Michael Mitchell and R. Hutchinson and R. Niculescu and Francisco Pereira and Xuerui Wang and M. Just and Sharlene D. Newman},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {145-175},\n title = {Learning to Decode Cognitive States from Brain Images},\n volume = {57},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ec76f55da5c6df30f6e4c9e4945bd3304d508ef7",
            "@type": "ScholarlyArticle",
            "paperId": "ec76f55da5c6df30f6e4c9e4945bd3304d508ef7",
            "corpusId": 13435674,
            "url": "https://www.semanticscholar.org/paper/ec76f55da5c6df30f6e4c9e4945bd3304d508ef7",
            "title": "Fuzzy support vector machines",
            "venue": "IEEE Trans. Neural Networks",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2002,
            "externalIds": {
                "MAG": "1994807229",
                "DBLP": "journals/tnn/LinW02",
                "DOI": "10.1109/72.991432",
                "CorpusId": 13435674,
                "PubMed": "18244447"
            },
            "abstract": "A support vector machine (SVM) learns the decision surface from two distinct classes of the input points. In many applications, each input point may not be fully assigned to one of these two classes. In this paper, we apply a fuzzy membership to each input point and reformulate the SVMs such that different input points can make different contributions to the learning of decision surface. We call the proposed method fuzzy SVMs (FSVMs).",
            "referenceCount": 10,
            "citationCount": 1409,
            "influentialCitationCount": 164,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2002-03-01",
            "journal": {
                "name": "IEEE transactions on neural networks",
                "volume": "13 2"
            },
            "citationStyles": {
                "bibtex": "@Article{Lin2002FuzzySV,\n author = {Chun-fu Lin and Sheng-de Wang},\n booktitle = {IEEE Trans. Neural Networks},\n journal = {IEEE transactions on neural networks},\n pages = {\n          464-71\n        },\n title = {Fuzzy support vector machines},\n volume = {13 2},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b3b3c562a45d7710d6f62ad8f210ebca9a47d23f",
            "@type": "ScholarlyArticle",
            "paperId": "b3b3c562a45d7710d6f62ad8f210ebca9a47d23f",
            "corpusId": 1384439,
            "url": "https://www.semanticscholar.org/paper/b3b3c562a45d7710d6f62ad8f210ebca9a47d23f",
            "title": "Who should fix this bug?",
            "venue": "International Conference on Software Engineering",
            "publicationVenue": {
                "id": "urn:research:a36dc29e-4ea1-4567-b0fe-1c06daf8bee8",
                "name": "International Conference on Software Engineering",
                "alternate_names": [
                    "Int Conf Softw Eng",
                    "ICSE"
                ],
                "issn": null,
                "url": "http://www.icse-conferences.org/"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "conf/icse/AnvikHM06",
                "MAG": "2079317829",
                "DOI": "10.1145/1134285.1134336",
                "CorpusId": 1384439
            },
            "abstract": "Open source development projects typically support an open bug repository to which both developers and users can report bugs. The reports that appear in this repository must be triaged to determine if the report is one which requires attention and if it is, which developer will be assigned the responsibility of resolving the report. Large open source developments are burdened by the rate at which new bug reports appear in the bug repository. In this paper, we present a semi-automated approach intended to ease one part of this process, the assignment of reports to a developer. Our approach applies a machine learning algorithm to the open bug repository to learn the kinds of reports each developer resolves. When a new report arrives, the classifier produced by the machine learning technique suggests a small number of developers suitable to resolve the report. With this approach, we have reached precision levels of 57% and 64% on the Eclipse and Firefox development projects respectively. We have also applied our approach to the gcc open source development with less positive results. We describe the conditions under which the approach is applicable and also report on the lessons we learned about applying machine learning to repositories used in open source development.",
            "referenceCount": 18,
            "citationCount": 1031,
            "influentialCitationCount": 95,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2006-05-28",
            "journal": {
                "name": "Proceedings of the 28th international conference on Software engineering",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Anvik2006WhoSF,\n author = {J. Anvik and L. Hiew and G. Murphy},\n booktitle = {International Conference on Software Engineering},\n journal = {Proceedings of the 28th international conference on Software engineering},\n title = {Who should fix this bug?},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4f51a64793d3b2a60e9e5846c31dae023cf5c69a",
            "@type": "ScholarlyArticle",
            "paperId": "4f51a64793d3b2a60e9e5846c31dae023cf5c69a",
            "corpusId": 67856367,
            "url": "https://www.semanticscholar.org/paper/4f51a64793d3b2a60e9e5846c31dae023cf5c69a",
            "title": "Unmasking Clever Hans predictors and assessing what machines really learn",
            "venue": "Nature Communications",
            "publicationVenue": {
                "id": "urn:research:43b3f0f9-489a-4566-8164-02fafde3cd98",
                "name": "Nature Communications",
                "alternate_names": [
                    "Nat Commun"
                ],
                "issn": "2041-1723",
                "url": "https://www.nature.com/ncomms/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2921802966",
                "PubMedCentral": "6411769",
                "DBLP": "journals/corr/abs-1902-10178",
                "ArXiv": "1902.10178",
                "DOI": "10.1038/s41467-019-08987-4",
                "CorpusId": 67856367,
                "PubMed": "30858366"
            },
            "abstract": null,
            "referenceCount": 151,
            "citationCount": 770,
            "influentialCitationCount": 34,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41467-019-08987-4.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-02-26",
            "journal": {
                "name": "Nature Communications",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Lapuschkin2019UnmaskingCH,\n author = {S. Lapuschkin and S. W\u00e4ldchen and Alexander Binder and G. Montavon and W. Samek and K. M\u00fcller},\n booktitle = {Nature Communications},\n journal = {Nature Communications},\n title = {Unmasking Clever Hans predictors and assessing what machines really learn},\n volume = {10},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0a5ff7336879c99513dca6fce6ef44984ebf3f55",
            "@type": "ScholarlyArticle",
            "paperId": "0a5ff7336879c99513dca6fce6ef44984ebf3f55",
            "corpusId": 1701442,
            "url": "https://www.semanticscholar.org/paper/0a5ff7336879c99513dca6fce6ef44984ebf3f55",
            "title": "Clipper: A Low-Latency Online Prediction Serving System",
            "venue": "Symposium on Networked Systems Design and Implementation",
            "publicationVenue": {
                "id": "urn:research:38e1b942-a62d-4d74-8e5d-677db6ed425f",
                "name": "Symposium on Networked Systems Design and Implementation",
                "alternate_names": [
                    "NSDI",
                    "Symp Networked Syst Des Implement",
                    "Networked Systems Design and Implementation",
                    "Networked Syst Des Implement"
                ],
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2950636297",
                "DBLP": "journals/corr/CrankshawWZFGS16",
                "ArXiv": "1612.03079",
                "CorpusId": 1701442
            },
            "abstract": "Machine learning is being deployed in a growing number of applications which demand real-time, accurate, and robust predictions under heavy query load. However, most machine learning frameworks and systems only address model training and not deployment. \nIn this paper, we introduce Clipper, a general-purpose low-latency prediction serving system. Interposing between end-user applications and a wide range of machine learning frameworks, Clipper introduces a modular architecture to simplify model deployment across frameworks and applications. Furthermore, by introducing caching, batching, and adaptive model selection techniques, Clipper reduces prediction latency and improves prediction throughput, accuracy, and robustness without modifying the underlying machine learning frameworks. We evaluate Clipper on four common machine learning benchmark datasets and demonstrate its ability to meet the latency, accuracy, and throughput demands of online serving applications. Finally, we compare Clipper to the TensorFlow Serving system and demonstrate that we are able to achieve comparable throughput and latency while enabling model composition and online learning to improve accuracy and render more robust predictions.",
            "referenceCount": 70,
            "citationCount": 502,
            "influentialCitationCount": 87,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-12-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1612.03079"
            },
            "citationStyles": {
                "bibtex": "@Article{Crankshaw2016ClipperAL,\n author = {D. Crankshaw and Xin Wang and Giulio Zhou and M. Franklin and Joseph E. Gonzalez and I. Stoica},\n booktitle = {Symposium on Networked Systems Design and Implementation},\n journal = {ArXiv},\n title = {Clipper: A Low-Latency Online Prediction Serving System},\n volume = {abs/1612.03079},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:430c95aab5bc85404f9651eb2137a12e2c4d5fe7",
            "@type": "ScholarlyArticle",
            "paperId": "430c95aab5bc85404f9651eb2137a12e2c4d5fe7",
            "corpusId": 262325813,
            "url": "https://www.semanticscholar.org/paper/430c95aab5bc85404f9651eb2137a12e2c4d5fe7",
            "title": "Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "1588401315",
                "DBLP": "journals/jmlr/AllweinSS00",
                "CorpusId": 262325813
            },
            "abstract": "We present a unifying framework for studying the solution of multiclass categorization problems by reducing them to multiple binary problems that are then solved using a margin-based binary learning algorithm. The proposed framework uni\ufb01es some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used. We propose a general method for combining the classi\ufb01ers generated on the binary problems, and we prove a general empirical multiclass loss bound given the empirical loss of the individual binary learning algorithms. The scheme and the corresponding bounds apply to many popular classi\ufb01cation learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms. We also give a multiclass generalization error analysis for general output codes with AdaBoost as the binary learner. Experimental results with SVM and AdaBoost show that our scheme provides a viable alternative to the most commonly used multiclass algorithms.",
            "referenceCount": 28,
            "citationCount": 1402,
            "influentialCitationCount": 134,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2000-06-29",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Allwein2000ReducingMT,\n author = {Erin L. Allwein and Rob Schapire and Y. Singer},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {113-141},\n title = {Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers},\n volume = {1},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:49b8dff62cccc26023c876460234bf29084a382f",
            "@type": "ScholarlyArticle",
            "paperId": "49b8dff62cccc26023c876460234bf29084a382f",
            "corpusId": 6027413,
            "url": "https://www.semanticscholar.org/paper/49b8dff62cccc26023c876460234bf29084a382f",
            "title": "Transductive Learning via Spectral Graph Partitioning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2111557120",
                "DBLP": "conf/icml/Joachims03",
                "CorpusId": 6027413
            },
            "abstract": "We present a new method for transductive learning, which can be seen as a transductive version of the k nearest-neighbor classifier. Unlike for many other transductive learning methods, the training problem has a meaningful relaxation that can be solved globally optimally using spectral methods. We propose an algorithm that robustly achieves good generalization performance and that can be trained efficiently. A key advantage of the algorithm is that it does not require additional heuristics to avoid unbalanced splits. Furthermore, we show a connection to transductive Support Vector Machines, and that an effective Co-Training algorithm arises as a special case.",
            "referenceCount": 19,
            "citationCount": 758,
            "influentialCitationCount": 76,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2003-08-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Joachims2003TransductiveLV,\n author = {T. Joachims},\n booktitle = {International Conference on Machine Learning},\n pages = {290-297},\n title = {Transductive Learning via Spectral Graph Partitioning},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:10b496ad48513f8585aa56f2c682159357858960",
            "@type": "ScholarlyArticle",
            "paperId": "10b496ad48513f8585aa56f2c682159357858960",
            "corpusId": 6695707,
            "url": "https://www.semanticscholar.org/paper/10b496ad48513f8585aa56f2c682159357858960",
            "title": "Understanding Data Augmentation for Classification: When to Warp?",
            "venue": "International Conference on Digital Image Computing: Techniques and Applications",
            "publicationVenue": {
                "id": "urn:research:375cb686-e96e-4b79-825c-1589c99aca1d",
                "name": "International Conference on Digital Image Computing: Techniques and Applications",
                "alternate_names": [
                    "Int Conf Digit Image Comput Tech Appl",
                    "DICTA",
                    "Digital Image Computing: Techniques and Applications",
                    "Digit Image Comput Tech Appl"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=710"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/WongGSM16",
                "MAG": "2526969612",
                "ArXiv": "1609.08764",
                "DOI": "10.1109/DICTA.2016.7797091",
                "CorpusId": 6695707
            },
            "abstract": "In this paper we investigate the benefit of augmenting data with synthetically created samples when training a machine learning classifier. Two approaches for creating additional training samples are data warping, which generates additional samples through transformations applied in the data-space, and synthetic over-sampling, which creates additional samples in feature-space. We experimentally evaluate the benefits of data augmentation for a convolutional backpropagation-trained neural network, a convolutional support vector machine and a convolutional extreme learning machine classifier, using the standard MNIST handwritten digit dataset. We found that while it is possible to perform generic augmentation in feature-space, if plausible transforms for the data are known then augmentation in data-space provides a greater benefit for improving performance and reducing overfitting.",
            "referenceCount": 17,
            "citationCount": 789,
            "influentialCitationCount": 20,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1609.08764",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-09-28",
            "journal": {
                "name": "2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wong2016UnderstandingDA,\n author = {S. Wong and Adam Gatt and V. Stamatescu and M. McDonnell},\n booktitle = {International Conference on Digital Image Computing: Techniques and Applications},\n journal = {2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)},\n pages = {1-6},\n title = {Understanding Data Augmentation for Classification: When to Warp?},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:77703a2783f64dfceb638aa9eebd9c9c501bb835",
            "@type": "ScholarlyArticle",
            "paperId": "77703a2783f64dfceb638aa9eebd9c9c501bb835",
            "corpusId": 13463620,
            "url": "https://www.semanticscholar.org/paper/77703a2783f64dfceb638aa9eebd9c9c501bb835",
            "title": "The Case against Accuracy Estimation for Comparing Induction Algorithms",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1524761913",
                "DBLP": "conf/icml/ProvostFK98",
                "CorpusId": 13463620
            },
            "abstract": "We analyze critically the use of classi cation accuracy to compare classi ers on natural data sets, providing a thorough investigation using ROC analysis, standard machine learning algorithms, and standard benchmark data sets. The results raise serious concerns about the use of accuracy for comparing classi ers and draw into question the conclusions that can be drawn from such studies. In the course of the presentation, we describe and demonstrate what we believe to be the proper use of ROC analysis for comparative studies in machine learning research. We argue that this methodology is preferable both for making practical choices and for drawing scienti c conclusions.",
            "referenceCount": 22,
            "citationCount": 1223,
            "influentialCitationCount": 53,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1998-07-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Provost1998TheCA,\n author = {F. Provost and Tom Fawcett and Ron Kohavi},\n booktitle = {International Conference on Machine Learning},\n pages = {445-453},\n title = {The Case against Accuracy Estimation for Comparing Induction Algorithms},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:45a9e2aa04e91bb511f36c365ef4daa274fe583c",
            "@type": "ScholarlyArticle",
            "paperId": "45a9e2aa04e91bb511f36c365ef4daa274fe583c",
            "corpusId": 9203634,
            "url": "https://www.semanticscholar.org/paper/45a9e2aa04e91bb511f36c365ef4daa274fe583c",
            "title": "Applying Support Vector Machines to Imbalanced Datasets",
            "venue": "European Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:f9a81beb-9bcb-43bc-96a7-67cf23dba2d6",
                "name": "European Conference on Machine Learning",
                "alternate_names": [
                    "Eur Conf Mach Learn",
                    "European conference on Machine Learning",
                    "Eur conf Mach Learn",
                    "ECML"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/ecml"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "1551909886",
                "DBLP": "conf/ecml/AkbaniKJ04",
                "DOI": "10.1007/978-3-540-30115-8_7",
                "CorpusId": 9203634
            },
            "abstract": null,
            "referenceCount": 15,
            "citationCount": 1230,
            "influentialCitationCount": 86,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/978-3-540-30115-8_7.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-09-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Akbani2004ApplyingSV,\n author = {R. Akbani and Stephen Kwek and N. Japkowicz},\n booktitle = {European Conference on Machine Learning},\n pages = {39-50},\n title = {Applying Support Vector Machines to Imbalanced Datasets},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8e9e7b8084082e599e16d65cc9539b415ee94738",
            "@type": "ScholarlyArticle",
            "paperId": "8e9e7b8084082e599e16d65cc9539b415ee94738",
            "corpusId": 9627410,
            "url": "https://www.semanticscholar.org/paper/8e9e7b8084082e599e16d65cc9539b415ee94738",
            "title": "Overcoming the Myopia of Inductive Learning Algorithms with RELIEFF",
            "venue": "Applied intelligence (Boston)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "journals/apin/KononenkoSR97",
                "MAG": "1545302199",
                "DOI": "10.1023/A:1008280620621",
                "CorpusId": 9627410
            },
            "abstract": null,
            "referenceCount": 32,
            "citationCount": 659,
            "influentialCitationCount": 50,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Applied Intelligence",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Kononenko2004OvercomingTM,\n author = {I. Kononenko and E. Simec and M. Robnik-Sikonja},\n booktitle = {Applied intelligence (Boston)},\n journal = {Applied Intelligence},\n pages = {39-55},\n title = {Overcoming the Myopia of Inductive Learning Algorithms with RELIEFF},\n volume = {7},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:36cd89332d305d01605d6d08cd8452c8a752138a",
            "@type": "ScholarlyArticle",
            "paperId": "36cd89332d305d01605d6d08cd8452c8a752138a",
            "corpusId": 59597264,
            "url": "https://www.semanticscholar.org/paper/36cd89332d305d01605d6d08cd8452c8a752138a",
            "title": "Designing neural networks through neuroevolution",
            "venue": "Nature Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:6457124b-39bf-4d02-bff4-73752ff21562",
                "name": "Nature Machine Intelligence",
                "alternate_names": [
                    "Nat Mach Intell"
                ],
                "issn": "2522-5839",
                "url": "https://www.nature.com/natmachintell/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/natmi/StanleyCLM19",
                "MAG": "2906697496",
                "DOI": "10.1038/S42256-018-0006-Z",
                "CorpusId": 59597264
            },
            "abstract": null,
            "referenceCount": 161,
            "citationCount": 502,
            "influentialCitationCount": 18,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s42256-018-0006-z.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-01-01",
            "journal": {
                "name": "Nature Machine Intelligence",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Stanley2019DesigningNN,\n author = {Kenneth O. Stanley and J. Clune and J. Lehman and R. Miikkulainen},\n booktitle = {Nature Machine Intelligence},\n journal = {Nature Machine Intelligence},\n pages = {24-35},\n title = {Designing neural networks through neuroevolution},\n volume = {1},\n year = {2019}\n}\n"
            }
        }
    }
]