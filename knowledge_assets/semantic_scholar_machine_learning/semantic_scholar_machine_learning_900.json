[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2635f61333900a6b4cd9b5db5d4c3bc31363b2ff",
            "@type": "ScholarlyArticle",
            "paperId": "2635f61333900a6b4cd9b5db5d4c3bc31363b2ff",
            "corpusId": 2127615,
            "url": "https://www.semanticscholar.org/paper/2635f61333900a6b4cd9b5db5d4c3bc31363b2ff",
            "title": "Fast support vector machine training and classification on graphics processors",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2159350554",
                "DBLP": "conf/icml/CatanzaroSK08",
                "DOI": "10.1145/1390156.1390170",
                "CorpusId": 2127615
            },
            "abstract": "Recent developments in programmable, highly parallel Graphics Processing Units (GPUs) have enabled high performance implementations of machine learning algorithms. We describe a solver for Support Vector Machine training running on a GPU, using the Sequential Minimal Optimization algorithm and an adaptive first and second order working set selection heuristic, which achieves speedups of 9-35x over LIBSVM running on a traditional processor. We also present a GPU-based system for SVM classification which achieves speedups of 81-138x over LIBSVM (5-24x over our own CPU based SVM classifier).",
            "referenceCount": 21,
            "citationCount": 409,
            "influentialCitationCount": 42,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://nma.berkeley.edu/ark:/28722/bk00010886s",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-07-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Catanzaro2008FastSV,\n author = {Bryan Catanzaro and N. Sundaram and K. Keutzer},\n booktitle = {International Conference on Machine Learning},\n pages = {104-111},\n title = {Fast support vector machine training and classification on graphics processors},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dd135a89b5075af5cbef5becaf419457cdd77cc9",
            "@type": "ScholarlyArticle",
            "paperId": "dd135a89b5075af5cbef5becaf419457cdd77cc9",
            "corpusId": 18401632,
            "url": "https://www.semanticscholar.org/paper/dd135a89b5075af5cbef5becaf419457cdd77cc9",
            "title": "An Introduction to Restricted Boltzmann Machines",
            "venue": "Iberoamerican Congress on Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:20f293f6-de43-4432-9757-5926f6bd174b",
                "name": "Iberoamerican Congress on Pattern Recognition",
                "alternate_names": [
                    "Iberoam Congr Pattern Recognit",
                    "CIARP"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=444"
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "conf/ciarp/FischerI12",
                "MAG": "2202505358",
                "DOI": "10.1007/978-3-642-33275-3_2",
                "CorpusId": 18401632
            },
            "abstract": null,
            "referenceCount": 46,
            "citationCount": 565,
            "influentialCitationCount": 50,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/978-3-642-33275-3_2.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2012-09-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Fischer2012AnIT,\n author = {Asja Fischer and C. Igel},\n booktitle = {Iberoamerican Congress on Pattern Recognition},\n pages = {14-36},\n title = {An Introduction to Restricted Boltzmann Machines},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ac12c9b9e35e58b55d85a97c47886a7371c14afa",
            "@type": "ScholarlyArticle",
            "paperId": "ac12c9b9e35e58b55d85a97c47886a7371c14afa",
            "corpusId": 6669059,
            "url": "https://www.semanticscholar.org/paper/ac12c9b9e35e58b55d85a97c47886a7371c14afa",
            "title": "Data mining in bioinformatics using Weka",
            "venue": "Bioinform.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2135893370",
                "DBLP": "journals/bioinformatics/FrankHTHW04",
                "DOI": "10.1093/bioinformatics/bth261",
                "CorpusId": 6669059,
                "PubMed": "15073010"
            },
            "abstract": "UNLABELLED\nThe Weka machine learning workbench provides a general-purpose environment for automatic classification, regression, clustering and feature selection-common data mining problems in bioinformatics research. It contains an extensive collection of machine learning algorithms and data pre-processing methods complemented by graphical user interfaces for data exploration and the experimental comparison of different machine learning techniques on the same problem. Weka can process data given in the form of a single relational table. Its main objectives are to (a) assist users in extracting useful information from data and (b) enable them to easily identify a suitable algorithm for generating an accurate predictive model from it.\n\n\nAVAILABILITY\nhttp://www.cs.waikato.ac.nz/ml/weka.",
            "referenceCount": 10,
            "citationCount": 911,
            "influentialCitationCount": 64,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://academic.oup.com/bioinformatics/article-pdf/20/15/2479/48906042/bioinformatics_20_15_2479.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2004-10-12",
            "journal": {
                "name": "Bioinformatics",
                "volume": "20 15"
            },
            "citationStyles": {
                "bibtex": "@Article{Frank2004DataMI,\n author = {E. Frank and M. Hall and Leonard E. Trigg and G. Holmes and I. Witten},\n booktitle = {Bioinform.},\n journal = {Bioinformatics},\n pages = {\n          2479-81\n        },\n title = {Data mining in bioinformatics using Weka},\n volume = {20 15},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:55e36d6b45c91a0daa49234bd47b856470d6825c",
            "@type": "ScholarlyArticle",
            "paperId": "55e36d6b45c91a0daa49234bd47b856470d6825c",
            "corpusId": 15244007,
            "url": "https://www.semanticscholar.org/paper/55e36d6b45c91a0daa49234bd47b856470d6825c",
            "title": "Identifying Sarcasm in Twitter: A Closer Look",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "conf/acl/Gonzalez-IbanezMW11",
                "ACL": "P11-2102",
                "MAG": "2250489604",
                "CorpusId": 15244007
            },
            "abstract": "Sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite. We report on a method for constructing a corpus of sarcastic Twitter messages in which determination of the sarcasm of each message has been made by its author. We use this reliable corpus to compare sarcastic utterances in Twitter to utterances that express positive or negative attitudes without sarcasm. We investigate the impact of lexical and pragmatic factors on machine learning effectiveness for identifying sarcastic utterances and we compare the performance of machine learning techniques and human judges on this task. Perhaps unsurprisingly, neither the human judges nor the machine learning techniques perform very well.",
            "referenceCount": 14,
            "citationCount": 625,
            "influentialCitationCount": 52,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-06-19",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Gonz\u00e1lez-Ib\u00e1\u00f1ez2011IdentifyingSI,\n author = {Roberto I. Gonz\u00e1lez-Ib\u00e1\u00f1ez and S. Muresan and N. Wacholder},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {581-586},\n title = {Identifying Sarcasm in Twitter: A Closer Look},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:784220a5a2ad452282f8af006a6cb5715d54d0ed",
            "@type": "ScholarlyArticle",
            "paperId": "784220a5a2ad452282f8af006a6cb5715d54d0ed",
            "corpusId": 431427,
            "url": "https://www.semanticscholar.org/paper/784220a5a2ad452282f8af006a6cb5715d54d0ed",
            "title": "Link-Based Classification",
            "venue": "Encyclopedia of Machine Learning and Data Mining",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "reference/snam/X14hs",
                "MAG": "1908728294",
                "DOI": "10.1007/978-1-4899-7687-1_100268",
                "CorpusId": 431427
            },
            "abstract": null,
            "referenceCount": 68,
            "citationCount": 856,
            "influentialCitationCount": 126,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://drum.lib.umd.edu/bitstreams/adc0c6ae-f26a-4621-969a-447d92930cc9/download",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2003-08-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lu2003LinkBasedC,\n author = {Qing Lu and L. Getoor},\n booktitle = {Encyclopedia of Machine Learning and Data Mining},\n pages = {758},\n title = {Link-Based Classification},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4e11ebdc3e4f30d774458fab9e4b45ff0d0aa971",
            "@type": "ScholarlyArticle",
            "paperId": "4e11ebdc3e4f30d774458fab9e4b45ff0d0aa971",
            "corpusId": 34493673,
            "url": "https://www.semanticscholar.org/paper/4e11ebdc3e4f30d774458fab9e4b45ff0d0aa971",
            "title": "Logical and relational learning",
            "venue": "Cognitive Technologies",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/sbia/Raedt08",
                "MAG": "2140141795",
                "DOI": "10.1007/978-3-540-68856-3",
                "CorpusId": 34493673
            },
            "abstract": null,
            "referenceCount": 1,
            "citationCount": 460,
            "influentialCitationCount": 45,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-540-88190-2_1.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-10-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Raedt2008LogicalAR,\n author = {L. D. Raedt},\n booktitle = {Cognitive Technologies},\n pages = {1},\n title = {Logical and relational learning},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b",
            "@type": "ScholarlyArticle",
            "paperId": "b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b",
            "corpusId": 14250548,
            "url": "https://www.semanticscholar.org/paper/b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b",
            "title": "Steps toward Artificial Intelligence",
            "venue": "Proceedings of the IRE",
            "publicationVenue": {
                "id": "urn:research:82b56675-f0f3-487f-b83e-155a762e2855",
                "name": "Proceedings of the IRE",
                "alternate_names": [
                    "Proc IRE"
                ],
                "issn": "0096-8390",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=10933"
            },
            "year": 1995,
            "externalIds": {
                "MAG": "2045031658",
                "DOI": "10.1109/JRPROC.1961.287775",
                "CorpusId": 14250548
            },
            "abstract": "The problems of heuristic programming-of making computers solve really difficult problems-are divided into five main areas: Search, Pattern-Recognition, Learning, Planning, and Induction. A computer can do, in a sense, only what it is told to do. But even when we do not know how to solve a certain problem, we may program a machine (computer) to Search through some large space of solution attempts. Unfortunately, this usually leads to an enormously inefficient process. With Pattern-Recognition techniques, efficiency can often be improved, by restricting the application of the machine's methods to appropriate problems. Pattern-Recognition, together with Learning, can be used to exploit generalizations based on accumulated experience, further reducing search. By analyzing the situation, using Planning methods, we may obtain a fundamental improvement by replacing the given search with a much smaller, more appropriate exploration. To manage broad classes of problems, machines will need to construct models of their environments, using some scheme for Induction. Wherever appropriate, the discussion is supported by extensive citation of the literature and by descriptions of a few of the most successful heuristic (problem-solving) programs constructed to date.",
            "referenceCount": 94,
            "citationCount": 1405,
            "influentialCitationCount": 53,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://cumincad.architexturez.net/system/files/pdf/1d2b.content.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1995-10-26",
            "journal": {
                "name": "Proceedings of the IRE",
                "volume": "49"
            },
            "citationStyles": {
                "bibtex": "@Article{Minsky1995StepsTA,\n author = {M. Minsky},\n booktitle = {Proceedings of the IRE},\n journal = {Proceedings of the IRE},\n pages = {8-30},\n title = {Steps toward Artificial Intelligence},\n volume = {49},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:57a29eae9217831f391c2a3206964fddbb162250",
            "@type": "ScholarlyArticle",
            "paperId": "57a29eae9217831f391c2a3206964fddbb162250",
            "corpusId": 1900499,
            "url": "https://www.semanticscholar.org/paper/57a29eae9217831f391c2a3206964fddbb162250",
            "title": "Comparing Support Vector Machines with Gaussian Kernels to Radial Basis Function Classi",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1997,
            "externalIds": {
                "CorpusId": 1900499
            },
            "abstract": "The Support Vector (SV) machine is a novel type of learning machine, based on statistical learning theory, which contains polynomial classi ers, neural networks, and radial basis function (RBF) networks as special cases. In the RBF case, the SV algorithm automatically determines centers, weights and threshold such as to minimize an upper bound on the expected test error. The present study is devoted to an experimental comparison of these machines with a classical approach, where the centers are determined by k{means clustering and the weights are found using error backpropagation. We consider three machines, namely a classical RBF machine, an SV machine with Gaussian kernel, and a hybrid system with the centers determined by the SV method and the weights trained by error backpropagation. Our results show that on the US postal service database of handwritten digits, the SV machine achieves the highest test accuracy, followed by the hybrid approach. The SV approach is thus not only theoretically well{founded, but also superior in a practical application. Copyright c Massachusetts Institute of Technology, 1996 This report describes research done at the Center for Biological and Computational Learning, the Arti cial Intelligence Laboratory of the Massachusetts Institute of Technology, and at AT&T Bell Laboratories (now AT&T Research, and Lucent Technologies Bell Laboratories). Support for the Center is provided in part by a grant from the National Science Foundation under contract ASC{9217041. BS thanks the M.I.T. for hospitality during a three{week visit in March 1995, where this work was started. At the time of the study, BS, CB, and VV were with AT&TBell Laboratories, NJ; KS, FG, PN, and TP were with the Massachusetts Institute of Technology. KS is now with the Department of Information Systems and Computer Science at the National University of Singapore, Lower Kent Ridge Road, Singapore 0511; CB and PN are with Lucent Technologies, Bell Laboratories, NJ; VV is with AT&T Research, NJ. BS was supported by the Studienstiftung des deutschen Volkes; CB was supported by ARPA under ONR contract number N00014-94-C-0186. We thank A. Smola for useful discussions. Please direct correspondence to Bernhard Sch\u007folkopf, bs@mpik-tueb.mpg.de, Max{Planck{Institut f\u007f ur biologische Kybernetik, Spemannstr. 38, 72076 T\u007f ubingen, Germany. Figure 1: A simple 2{dimensional classi cation problem: nd a decision function separating balls from circles. The box, as in all following pictures, depicts the region [ 1; 1].",
            "referenceCount": 11,
            "citationCount": 1350,
            "influentialCitationCount": 46,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Schh1997ComparingSV,\n author = {B. Schh and K. Sung and C. Burges and F. Girosi and P. N I Y Ogi and T. Poggio and V. Vapnik},\n title = {Comparing Support Vector Machines with Gaussian Kernels to Radial Basis Function Classi},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3335c340c20609b4e6de481c9eaf67ecd6c960dc",
            "@type": "ScholarlyArticle",
            "paperId": "3335c340c20609b4e6de481c9eaf67ecd6c960dc",
            "corpusId": 7142590,
            "url": "https://www.semanticscholar.org/paper/3335c340c20609b4e6de481c9eaf67ecd6c960dc",
            "title": "Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science",
            "venue": "Annual Conference on Genetic and Evolutionary Computation",
            "publicationVenue": {
                "id": "urn:research:d732841e-83f9-49ec-95ca-389e5568634b",
                "name": "Annual Conference on Genetic and Evolutionary Computation",
                "alternate_names": [
                    "GECCO",
                    "Annu Conf Genet Evol Comput",
                    "Genet Evol Comput Conf",
                    "Genetic and Evolutionary Computation Conference"
                ],
                "issn": null,
                "url": "http://www.sigevo.org/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2953359974",
                "ArXiv": "1603.06212",
                "DBLP": "conf/gecco/OlsonBUM16",
                "DOI": "10.1145/2908812.2908918",
                "CorpusId": 7142590
            },
            "abstract": "As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning--pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.",
            "referenceCount": 22,
            "citationCount": 427,
            "influentialCitationCount": 54,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://dl.acm.org/ft_gateway.cfm?id=2908918&type=pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2016-03-20",
            "journal": {
                "name": "Proceedings of the Genetic and Evolutionary Computation Conference 2016",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Olson2016EvaluationOA,\n author = {Randal S. Olson and Nathan Bartley and R. Urbanowicz and J. Moore},\n booktitle = {Annual Conference on Genetic and Evolutionary Computation},\n journal = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},\n title = {Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b69df93991a1f5a712b20e832f5b0281acb3153b",
            "@type": "ScholarlyArticle",
            "paperId": "b69df93991a1f5a712b20e832f5b0281acb3153b",
            "corpusId": 60452768,
            "url": "https://www.semanticscholar.org/paper/b69df93991a1f5a712b20e832f5b0281acb3153b",
            "title": "Kernel Methods in Computational Biology",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2005,
            "externalIds": {
                "MAG": "1542652324",
                "DOI": "10.7551/mitpress/4057.001.0001",
                "CorpusId": 60452768
            },
            "abstract": "Modern machine learning techniques are proving to be extremely valuable for the analysis of data in computational biology problems. One branch of machine learning, kernel methods, lends itself particularly well to the difficult aspects of biological data, which include high dimensionality (as in microarray measurements), representation as discrete and structured data (as in DNA or amino acid sequences), and the need to combine heterogeneous sources of information. This book provides a detailed overview of current research in kernel methods and their applications to computational biology.Following three introductory chapters -- an introduction to molecular and computational biology, a short review of kernel methods that focuses on intuitive concepts rather than technical details, and a detailed survey of recent applications of kernel methods in computational biology -- the book is divided into three sections that reflect three general trends in current research. The first part presents different ideas for the design of kernel functions specifically adapted to various biological data; the second part covers different approaches to learning from heterogeneous data; and the third part offers examples of successful applications of support vector machine methods.",
            "referenceCount": 36,
            "citationCount": 964,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://repository.kulib.kyoto-u.ac.jp/dspace/bitstream/2433/97608/1/KJ00004705537.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Scholkopf2005KernelMI,\n author = {B. Scholkopf and K. Tsuda and Jean-Philippe Vert},\n title = {Kernel Methods in Computational Biology},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f8b012720a2322dcf4ed9ac4d61d6be11d9ebd10",
            "@type": "ScholarlyArticle",
            "paperId": "f8b012720a2322dcf4ed9ac4d61d6be11d9ebd10",
            "corpusId": 195893285,
            "url": "https://www.semanticscholar.org/paper/f8b012720a2322dcf4ed9ac4d61d6be11d9ebd10",
            "title": "Concepts of Artificial Intelligence for Computer-Assisted Drug Discovery.",
            "venue": "Chemical Reviews",
            "publicationVenue": {
                "id": "urn:research:f458795b-af97-4b7c-ba4e-d57bbb57f90a",
                "name": "Chemical Reviews",
                "alternate_names": [
                    "Chem Rev"
                ],
                "issn": "0009-2665",
                "url": "https://pubs.acs.org/journal/chreay"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2959938226",
                "DOI": "10.1021/acs.chemrev.8b00728",
                "CorpusId": 195893285,
                "PubMed": "31294972"
            },
            "abstract": "Artificial intelligence (AI), and, in particular, deep learning as a subcategory of AI, provides opportunities for the discovery and development of innovative drugs. Various machine learning approaches have recently (re)emerged, some of which may be considered instances of domain-specific AI which have been successfully employed for drug discovery and design. This review provides a comprehensive portrayal of these machine learning techniques and of their applications in medicinal chemistry. After introducing the basic principles, alongside some application notes, of the various machine learning algorithms, the current state-of-the art of AI-assisted pharmaceutical discovery is discussed, including applications in structure- and ligand-based virtual screening, de novo drug design, physicochemical and pharmacokinetic property prediction, drug repurposing, and related aspects. Finally, several challenges and limitations of the current methods are summarized, with a view to potential future directions for AI-assisted drug discovery and design.",
            "referenceCount": 618,
            "citationCount": 409,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://pubs.acs.org/doi/pdf/10.1021/acs.chemrev.8b00728",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Chemistry",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Chemistry",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-07-11",
            "journal": {
                "name": "Chemical reviews",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2019ConceptsOA,\n author = {Xin Yang and Yifei Wang and Ryan Byrne and G. Schneider and Sheng-yong Yang},\n booktitle = {Chemical Reviews},\n journal = {Chemical reviews},\n title = {Concepts of Artificial Intelligence for Computer-Assisted Drug Discovery.},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f620ec7bc0632be5518718cb81e2bfb57c81e950",
            "@type": "ScholarlyArticle",
            "paperId": "f620ec7bc0632be5518718cb81e2bfb57c81e950",
            "corpusId": 12182489,
            "url": "https://www.semanticscholar.org/paper/f620ec7bc0632be5518718cb81e2bfb57c81e950",
            "title": "Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2157133710",
                "DBLP": "journals/jmlr/RadovanovicNI10",
                "DOI": "10.5555/1756006.1953015",
                "CorpusId": 12182489
            },
            "abstract": "Different aspects of the curse of dimensionality are known to present serious challenges to various machine-learning methods and tasks. This paper explores a new aspect of the dimensionality curse, referred to as hubness, that affects the distribution of k-occurrences: the number of times a point appears among the k nearest neighbors of other points in a data set. Through theoretical and empirical analysis involving synthetic and real data sets we show that under commonly used assumptions this distribution becomes considerably skewed as dimensionality increases, causing the emergence of hubs, that is, points with very high k-occurrences which effectively represent \"popular\" nearest neighbors. We examine the origins of this phenomenon, showing that it is an inherent property of data distributions in high-dimensional vector space, discuss its interaction with dimensionality reduction, and explore its influence on a wide range of machine-learning tasks directly or indirectly based on measuring distances, belonging to supervised, semi-supervised, and unsupervised learning families.",
            "referenceCount": 95,
            "citationCount": 579,
            "influentialCitationCount": 55,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-03-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Radovanovi\u01072010HubsIS,\n author = {Milo\u0161 Radovanovi\u0107 and A. Nanopoulos and M. Ivanovi\u0107},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {2487-2531},\n title = {Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data},\n volume = {11},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fd0934a6008ecb9035b5b1e0b91ac890e2e5bd3d",
            "@type": "ScholarlyArticle",
            "paperId": "fd0934a6008ecb9035b5b1e0b91ac890e2e5bd3d",
            "corpusId": 118495437,
            "url": "https://www.semanticscholar.org/paper/fd0934a6008ecb9035b5b1e0b91ac890e2e5bd3d",
            "title": "A connectionist machine for genetic hillclimbing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1987,
            "externalIds": {
                "MAG": "1507599800",
                "DOI": "10.1007/978-1-4613-1997-9",
                "CorpusId": 118495437
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 819,
            "influentialCitationCount": 81,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "1987-07-04",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Ackley1987ACM,\n author = {D. Ackley},\n title = {A connectionist machine for genetic hillclimbing},\n year = {1987}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2323173a0bddac0dd2586b17a2f3ac33f401c45c",
            "@type": "ScholarlyArticle",
            "paperId": "2323173a0bddac0dd2586b17a2f3ac33f401c45c",
            "corpusId": 9680583,
            "url": "https://www.semanticscholar.org/paper/2323173a0bddac0dd2586b17a2f3ac33f401c45c",
            "title": "Nearest-Neighbor Methods in Learning and Vision: Theory and Practice (Neural Information Processing)",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2006,
            "externalIds": {
                "MAG": "202794677",
                "CorpusId": 9680583
            },
            "abstract": "Regression and classification methods based on similarity of the input to stored examples have not been widely used in applications involving very large sets of high-dimensional data. Recent advances in computational geometry and machine learning, however, may alleviate the problems in using these methods on large data sets. This volume presents theoretical and practical discussions of nearest-neighbor (NN) methods in machine learning and examines computer vision as an application domain in which the benefit of these advanced methods is often dramatic. It brings together contributions from researchers in theory of computation, machine learning, and computer vision with the goals of bridging the gaps between disciplines and presenting state-of-the-art methods for emerging applications.The contributors focus on the importance of designing algorithms for NN search, and for the related classification, regression, and retrieval tasks, that remain efficient even as the number of points or the dimensionality of the data grows very large. The book begins with two theoretical chapters on computational geometry and then explores ways to make the NN approach practicable in machine learning applications where the dimensionality of the data and the size of the data sets make the naive methods for NN search prohibitively expensive. The final chapters describe successful applications of an NN algorithm, locality-sensitive hashing (LSH), to vision tasks.",
            "referenceCount": 31,
            "citationCount": 573,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2006-03-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Shakhnarovich2006NearestNeighborMI,\n author = {Gregory Shakhnarovich and Trevor Darrell and P. Indyk},\n title = {Nearest-Neighbor Methods in Learning and Vision: Theory and Practice (Neural Information Processing)},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a53da9916b87fa295837617c16ef2ca6462cafb8",
            "@type": "ScholarlyArticle",
            "paperId": "a53da9916b87fa295837617c16ef2ca6462cafb8",
            "corpusId": 15584821,
            "url": "https://www.semanticscholar.org/paper/a53da9916b87fa295837617c16ef2ca6462cafb8",
            "title": "Classification using discriminative restricted Boltzmann machines",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/icml/LarochelleB08",
                "MAG": "1964155876",
                "DOI": "10.1145/1390156.1390224",
                "CorpusId": 15584821
            },
            "abstract": "Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.",
            "referenceCount": 25,
            "citationCount": 795,
            "influentialCitationCount": 83,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://icml2008.cs.helsinki.fi/papers/601.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-07-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Larochelle2008ClassificationUD,\n author = {H. Larochelle and Yoshua Bengio},\n booktitle = {International Conference on Machine Learning},\n pages = {536-543},\n title = {Classification using discriminative restricted Boltzmann machines},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1e826f01d02a8d514b8a687932d228781243496e",
            "@type": "ScholarlyArticle",
            "paperId": "1e826f01d02a8d514b8a687932d228781243496e",
            "corpusId": 16727656,
            "url": "https://www.semanticscholar.org/paper/1e826f01d02a8d514b8a687932d228781243496e",
            "title": "An accelerated gradient method for trace norm minimization",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "1998635907",
                "DBLP": "conf/icml/JiY09",
                "DOI": "10.1145/1553374.1553434",
                "CorpusId": 16727656
            },
            "abstract": "We consider the minimization of a smooth loss function regularized by the trace norm of the matrix variable. Such formulation finds applications in many machine learning tasks including multi-task learning, matrix classification, and matrix completion. The standard semidefinite programming formulation for this problem is computationally expensive. In addition, due to the non-smooth nature of the trace norm, the optimal first-order black-box method for solving such class of problems converges as O(1/\u221ak), where k is the iteration counter. In this paper, we exploit the special structure of the trace norm, based on which we propose an extended gradient algorithm that converges as O(1/k). We further propose an accelerated gradient algorithm, which achieves the optimal convergence rate of O(1/k2) for smooth problems. Experiments on multi-task learning problems demonstrate the efficiency of the proposed algorithms.",
            "referenceCount": 27,
            "citationCount": 574,
            "influentialCitationCount": 69,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-06-14",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ji2009AnAG,\n author = {Shuiwang Ji and Jieping Ye},\n booktitle = {International Conference on Machine Learning},\n pages = {457-464},\n title = {An accelerated gradient method for trace norm minimization},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8ce2c4a374e8b37e3eef080c956f22cfc6ea25d6",
            "@type": "ScholarlyArticle",
            "paperId": "8ce2c4a374e8b37e3eef080c956f22cfc6ea25d6",
            "corpusId": 9234993,
            "url": "https://www.semanticscholar.org/paper/8ce2c4a374e8b37e3eef080c956f22cfc6ea25d6",
            "title": "Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2950287138",
                "ArXiv": "1606.04316",
                "DBLP": "journals/corr/BenavoliCDZ16",
                "CorpusId": 9234993
            },
            "abstract": "The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better - more sound and useful - alternatives for it.",
            "referenceCount": 49,
            "citationCount": 332,
            "influentialCitationCount": 30,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2016-06-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1606.04316"
            },
            "citationStyles": {
                "bibtex": "@Article{Benavoli2016TimeFA,\n author = {A. Benavoli and Giorgio Corani and J. Dem\u0161ar and Marco Zaffalon},\n booktitle = {Journal of machine learning research},\n journal = {ArXiv},\n title = {Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis},\n volume = {abs/1606.04316},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7141ea996fc449807b14c071716cecac0999f4ce",
            "@type": "ScholarlyArticle",
            "paperId": "7141ea996fc449807b14c071716cecac0999f4ce",
            "corpusId": 8952183,
            "url": "https://www.semanticscholar.org/paper/7141ea996fc449807b14c071716cecac0999f4ce",
            "title": "SVMTorch: Support Vector Machines for Large-Scale Regression Problems",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "1486089539",
                "DBLP": "journals/jmlr/CollobertB01",
                "DOI": "10.1162/15324430152733142",
                "CorpusId": 8952183
            },
            "abstract": "Keywords: learning Reference EPFL-REPORT-82604 URL: http://publications.idiap.ch/downloads/reports/2000/rr00-17.pdf Record created on 2006-03-10, modified on 2017-05-10",
            "referenceCount": 18,
            "citationCount": 1014,
            "influentialCitationCount": 53,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-09-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Collobert2001SVMTorchSV,\n author = {Ronan Collobert and Samy Bengio},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {143-160},\n title = {SVMTorch: Support Vector Machines for Large-Scale Regression Problems},\n volume = {1},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ac3f8372b9d893dbdb7e4b9cd3df5ed825ffb548",
            "@type": "ScholarlyArticle",
            "paperId": "ac3f8372b9d893dbdb7e4b9cd3df5ed825ffb548",
            "corpusId": 18210743,
            "url": "https://www.semanticscholar.org/paper/ac3f8372b9d893dbdb7e4b9cd3df5ed825ffb548",
            "title": "Twitter Sentiment Analysis",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2396917941",
                "ArXiv": "1507.00955",
                "DBLP": "journals/corr/KolchynaSTA15",
                "CorpusId": 18210743
            },
            "abstract": "This paper covers the two approaches for sentiment analysis: i) lexicon based method; ii) machine learning method. We describe several techniques to implement these approaches and discuss how they can be adopted for sentiment classification of Twitter messages. We present a comparative study of different lexicon combinations and show that enhancing sentiment lexicons with emoticons, abbreviations and social-media slang expressions increases the accuracy of lexicon-based classification for Twitter. We discuss the importance of feature generation and feature selection processes for machine learning sentiment classification. To quantify the performance of the main sentiment analysis methods over Twitter we run these algorithms on a benchmark Twitter dataset from the SemEval-2013 competition, task 2-B. The results show that machine learning method based on SVM and Naive Bayes classifiers outperforms the lexicon method. We present a new ensemble method that uses a lexicon based sentiment score as input feature for the machine learning approach. The combined method proved to produce more precise classifications. We also show that employing a cost-sensitive classifier for highly unbalanced datasets yields an improvement of sentiment classification performance up to 7%.",
            "referenceCount": 85,
            "citationCount": 442,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Sociology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Sociology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-07-03",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1507.00955"
            },
            "citationStyles": {
                "bibtex": "@Article{Kolchyna2015TwitterSA,\n author = {Olga Kolchyna and T. T. P. Souza and Philip C. Treleaven and T. Aste},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Twitter Sentiment Analysis},\n volume = {abs/1507.00955},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8198e70878c907e1bd05e7a3fa4280d8c338df60",
            "@type": "ScholarlyArticle",
            "paperId": "8198e70878c907e1bd05e7a3fa4280d8c338df60",
            "corpusId": 7635678,
            "url": "https://www.semanticscholar.org/paper/8198e70878c907e1bd05e7a3fa4280d8c338df60",
            "title": "Semi-Supervised Support Vector Machines",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2107968230",
                "DBLP": "conf/nips/BennettD98",
                "CorpusId": 7635678
            },
            "abstract": "We introduce a semi-supervised support vector machine (S3VM) method. Given a training set of labeled data and a working set of unlabeled data, S3VM constructs a support vector machine using both the training and working sets. We use S3VM to solve the transduction problem using overall risk minimization (ORM) posed by Vapnik. The transduction problem is to estimate the value of a classification function at the given points in the working set. This contrasts with the standard inductive learning problem of estimating the classification function at all possible values and then using the fixed function to deduce the classes of the working set data. We propose a general S3VM model that minimizes both the misclassification error and the function capacity based on all the available data. We show how the S3VM model for 1-norm linear support vector machines can be converted to a mixed-integer program and then solved exactly using integer programming. Results of S3VM and the standard 1-norm support vector machine approach are compared on ten data sets. Our computational results support the statistical learning theory results showing that incorporating working data improves generalization when insufficient training information is available. In every case, S3VM either improved or showed no significant difference in generalization compared to the traditional approach.",
            "referenceCount": 20,
            "citationCount": 937,
            "influentialCitationCount": 81,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1998-12-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bennett1998SemiSupervisedSV,\n author = {Kristin P. Bennett and A. Demiriz},\n booktitle = {Neural Information Processing Systems},\n pages = {368-374},\n title = {Semi-Supervised Support Vector Machines},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1b286f0984301793f711899aa3294974f60ffed9",
            "@type": "ScholarlyArticle",
            "paperId": "1b286f0984301793f711899aa3294974f60ffed9",
            "corpusId": 10301598,
            "url": "https://www.semanticscholar.org/paper/1b286f0984301793f711899aa3294974f60ffed9",
            "title": "PRoNTo: Pattern Recognition for Neuroimaging Toolbox",
            "venue": "Neuroinformatics",
            "publicationVenue": {
                "id": "urn:research:48bfb57a-bc20-4264-8899-7605c9e28d63",
                "name": "Neuroinformatics",
                "alternate_names": null,
                "issn": "1539-2791",
                "url": "http://openurl.ingenta.com/content?genre=journal&issn=1539-2791"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "journals/ni/SchrouffRRMCAPRM13",
                "MAG": "2040305148",
                "PubMedCentral": "3722452",
                "DOI": "10.1007/s12021-013-9178-1",
                "CorpusId": 10301598,
                "PubMed": "23417655"
            },
            "abstract": null,
            "referenceCount": 73,
            "citationCount": 376,
            "influentialCitationCount": 24,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s12021-013-9178-1.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-02-16",
            "journal": {
                "name": "Neuroinformatics",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Schrouff2013PRoNToPR,\n author = {J. Schrouff and M. J. Rosa and J. Rondina and A. Marquand and C. Chu and J. Ashburner and C. Phillips and J. Richiardi and J. Miranda},\n booktitle = {Neuroinformatics},\n journal = {Neuroinformatics},\n pages = {319 - 337},\n title = {PRoNTo: Pattern Recognition for Neuroimaging Toolbox},\n volume = {11},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ec7c68427a26f812532b1c913c68fcf84b7de58e",
            "@type": "ScholarlyArticle",
            "paperId": "ec7c68427a26f812532b1c913c68fcf84b7de58e",
            "corpusId": 6035769,
            "url": "https://www.semanticscholar.org/paper/ec7c68427a26f812532b1c913c68fcf84b7de58e",
            "title": "Beyond the point cloud: from transductive to semi-supervised learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "conf/icml/SindhwaniNB05",
                "MAG": "2148029428",
                "DOI": "10.1145/1102351.1102455",
                "CorpusId": 6035769
            },
            "abstract": "Due to its occurrence in engineering domains and implications for natural learning, the problem of utilizing unlabeled data is attracting increasing attention in machine learning. A large body of recent literature has focussed on the transductive setting where labels of unlabeled examples are estimated by learning a function defined only over the point cloud data. In a truly semi-supervised setting however, a learning machine has access to labeled and unlabeled examples and must make predictions on data points never encountered before. In this paper, we show how to turn transductive and standard supervised learning algorithms into semi-supervised learners. We construct a family of data-dependent norms on Reproducing Kernel Hilbert Spaces (RKHS). These norms allow us to warp the structure of the RKHS to reflect the underlying geometry of the data. We derive explicit formulas for the corresponding new kernels. Our approach demonstrates state of the art performance on a variety of classification tasks.",
            "referenceCount": 26,
            "citationCount": 505,
            "influentialCitationCount": 70,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2005-08-07",
            "journal": {
                "name": "Proceedings of the 22nd international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Sindhwani2005BeyondTP,\n author = {Vikas Sindhwani and P. Niyogi and M. Belkin},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the 22nd international conference on Machine learning},\n title = {Beyond the point cloud: from transductive to semi-supervised learning},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7b5440285956e5d2ff848583775de35c089cc12f",
            "@type": "ScholarlyArticle",
            "paperId": "7b5440285956e5d2ff848583775de35c089cc12f",
            "corpusId": 1146598,
            "url": "https://www.semanticscholar.org/paper/7b5440285956e5d2ff848583775de35c089cc12f",
            "title": "Incremental Learning with Support Vector Machines",
            "venue": "Industrial Conference on Data Mining",
            "publicationVenue": {
                "id": "urn:research:67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                "name": "Industrial Conference on Data Mining",
                "alternate_names": [
                    "Ind Conf Data Min",
                    "ICDM"
                ],
                "issn": null,
                "url": "http://www.data-mining-forum.de/"
            },
            "year": 2001,
            "externalIds": {
                "DBLP": "conf/icdm/Ruping01",
                "MAG": "2132641846",
                "DOI": "10.1109/ICDM.2001.989589",
                "CorpusId": 1146598
            },
            "abstract": "Support vector machines (SVMs) have become a popular tool for machine learning with large amounts of high dimensional data. In this paper an approach for incremental learning with support vector machines is presented, that improves the existing approach of Syed et al. (1999). An insight into the interpretability of support vectors is also given.",
            "referenceCount": 13,
            "citationCount": 599,
            "influentialCitationCount": 29,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.stefan-rueping.de/publications/rueping-2001-b.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2001-11-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{R\u00fcping2001IncrementalLW,\n author = {S. R\u00fcping},\n booktitle = {Industrial Conference on Data Mining},\n pages = {641-642},\n title = {Incremental Learning with Support Vector Machines},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:23d2d8b687d31b11573473a7c7792b7ec08d0745",
            "@type": "ScholarlyArticle",
            "paperId": "23d2d8b687d31b11573473a7c7792b7ec08d0745",
            "corpusId": 61776378,
            "url": "https://www.semanticscholar.org/paper/23d2d8b687d31b11573473a7c7792b7ec08d0745",
            "title": "Learning Kernel Classifiers: Theory and Algorithms",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2134407776",
                "DOI": "10.7551/MITPRESS/4170.001.0001",
                "CorpusId": 61776378
            },
            "abstract": "From the Publisher: \nLinear classifiers in kernel spaces have emerged as a major topic within the field of machine learning. The kernel technique takes the linear classifier--a limited, but well-established and comprehensively studied model--and extends its applicability to a wide range of nonlinear pattern-recognition tasks such as natural language processing, machine vision, and biological sequence analysis. This book provides the first comprehensive overview of both the theory and algorithms of kernel classifiers, including the most recent developments. It begins by describing the major algorithmic advances: kernel perceptron learning, kernel Fisher discriminants, support vector machines, relevance vector machines, Gaussian processes, and Bayes point machines. Then follows a detailed introduction to learning theory, including VC and PAC-Bayesian theory, data-dependent structural risk minimization, and compression bounds. Throughout, the book emphasizes the interaction between theory and algorithms: how learning algorithms work and why. The book includes many examples, complete pseudo code of the algorithms presented, and an extensive source code library.",
            "referenceCount": 0,
            "citationCount": 560,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2001-12-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Herbrich2001LearningKC,\n author = {R. Herbrich},\n title = {Learning Kernel Classifiers: Theory and Algorithms},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:08745f22d0abbe66e486f0985c985ecf1eab4e9e",
            "@type": "ScholarlyArticle",
            "paperId": "08745f22d0abbe66e486f0985c985ecf1eab4e9e",
            "corpusId": 3331334,
            "url": "https://www.semanticscholar.org/paper/08745f22d0abbe66e486f0985c985ecf1eab4e9e",
            "title": "Question classification using support vector machines",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "publicationVenue": {
                "id": "urn:research:8dce23a9-44e0-4381-a39e-2acc1edff700",
                "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "alternate_names": [
                    "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "Int ACM SIGIR Conf Res Dev Inf Retr",
                    "SIGIR",
                    "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigir/"
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "conf/sigir/ZhangL03",
                "MAG": "2086004682",
                "DOI": "10.1145/860435.860443",
                "CorpusId": 3331334
            },
            "abstract": "Question classification is very important for question answering. This paper presents our research work on automatic question classification through machine learning approaches. We have experimented with five machine learning algorithms: Nearest Neighbors (NN), Naive Bayes (NB), Decision Tree (DT), Sparse Network of Winnows (SNoW), and Support Vector Machines (SVM) using two kinds of features: bag-of-words and bag-of-ngrams. The experiment results show that with only surface text features the SVM outperforms the other four methods for this task. Further, we propose to use a special kernel function called the tree kernel to enable the SVM to take advantage of the syntactic structures of questions. We describe how the tree kernel can be computed efficiently by dynamic programming. The performance of our approach is promising, when tested on the questions from the TREC QA track.",
            "referenceCount": 27,
            "citationCount": 684,
            "influentialCitationCount": 76,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.comp.nus.edu.sg/~leews/publications/p31189-zhang.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2003-07-28",
            "journal": {
                "name": "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2003QuestionCU,\n author = {Dell Zhang and Wee Sun Lee},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval},\n title = {Question classification using support vector machines},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea4feb953b86f6a099d61ffa70d21c59be99f76a",
            "@type": "ScholarlyArticle",
            "paperId": "ea4feb953b86f6a099d61ffa70d21c59be99f76a",
            "corpusId": 1215747,
            "url": "https://www.semanticscholar.org/paper/ea4feb953b86f6a099d61ffa70d21c59be99f76a",
            "title": "Enhancing Supervised Learning with Unlabeled Data",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "1756896031",
                "DBLP": "conf/icml/GoldmanZ00",
                "CorpusId": 1215747
            },
            "abstract": "In a wide variety of supervised learning scenarios, there is a small set of labeled data, along with a large pool of unlabeled data. In this thesis, we present a new semi-supervised learning method called co-learning that is designed to use unlabeled data to enhance standard supervised learning algorithms. The idea is that two or more standard supervised learning algorithms can leverage off the fact that they have different representations of the hypotheses and they are likely to detect different patterns in labeled data. We also design an active co-learning strategy to bootstrap our co-leaning procedure when the originally labeled data set is too small to provide accurate confidence estimate for the learned hypotheses. We provide a priority sampling technique as the selection component in our active co-learning method. We evaluate our co-learning algorithms on several datasets from a commonly used data repository in the machine learning community. We also test our co-learning method on text categorization. The contribution of this research is to put forward a new semi-supervised learning approach for learning with a small number of labeled examples, and explore the applicability of our co-learning strategy in real world applications.",
            "referenceCount": 23,
            "citationCount": 556,
            "influentialCitationCount": 53,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2000-06-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Goldman2000EnhancingSL,\n author = {S. Goldman and Yan Zhou},\n booktitle = {International Conference on Machine Learning},\n pages = {327-334},\n title = {Enhancing Supervised Learning with Unlabeled Data},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0ca966cb390b442b10cb76aa3fddee6b613f4f0f",
            "@type": "ScholarlyArticle",
            "paperId": "0ca966cb390b442b10cb76aa3fddee6b613f4f0f",
            "corpusId": 1479461,
            "url": "https://www.semanticscholar.org/paper/0ca966cb390b442b10cb76aa3fddee6b613f4f0f",
            "title": "Incorporating Diversity in Active Learning with Support Vector Machines",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2003,
            "externalIds": {
                "MAG": "1845402413",
                "DBLP": "conf/icml/Brinker03",
                "CorpusId": 1479461
            },
            "abstract": "In many real world applications, active selection of training examples can significantly reduce the number of labelled training examples to learn a classification function. Different strategies in the field of support vector machines have been proposed that iteratively select a single new example from a set of unlabelled examples, query the corresponding class label and then perform retraining of the current classifier. However, to reduce computational time for training, it might be necessary to select batches of new training examples instead of single examples. Strategies for single examples can be extended straightforwardly to select batches by choosing the h > 1 examples that get the highest values for the individual selection criterion. We present a new approach that is especially designed to construct batches and incorporates a diversity measure. It has low computational requirements making it feasible for large scale problems with several thousands of examples. Experimental results indicate that this approach provides a faster method to attain a level of generalization accuracy in terms of the number of labelled examples.",
            "referenceCount": 18,
            "citationCount": 533,
            "influentialCitationCount": 59,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2003-08-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Brinker2003IncorporatingDI,\n author = {K. Brinker},\n booktitle = {International Conference on Machine Learning},\n pages = {59-66},\n title = {Incorporating Diversity in Active Learning with Support Vector Machines},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:05c08de1bf91cd52ea1d22e7238e33958b574a23",
            "@type": "ScholarlyArticle",
            "paperId": "05c08de1bf91cd52ea1d22e7238e33958b574a23",
            "corpusId": 21907954,
            "url": "https://www.semanticscholar.org/paper/05c08de1bf91cd52ea1d22e7238e33958b574a23",
            "title": "The responsibility gap: Ascribing responsibility for the actions of learning automata",
            "venue": "Ethics and Information Technology",
            "publicationVenue": {
                "id": "urn:research:09bb329a-45bc-4561-860f-c8ac0e958895",
                "name": "Ethics and Information Technology",
                "alternate_names": [
                    "Ethics Inf Technol"
                ],
                "issn": "1388-1957",
                "url": "https://www.springer.com/computer/swe/journal/10676"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2087710120",
                "DOI": "10.1007/s10676-004-3422-1",
                "CorpusId": 21907954
            },
            "abstract": null,
            "referenceCount": 26,
            "citationCount": 510,
            "influentialCitationCount": 34,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2004-09-01",
            "journal": {
                "name": "Ethics and Information Technology",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Matthias2004TheRG,\n author = {A. Matthias},\n booktitle = {Ethics and Information Technology},\n journal = {Ethics and Information Technology},\n pages = {175-183},\n title = {The responsibility gap: Ascribing responsibility for the actions of learning automata},\n volume = {6},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1776b5ba5d2b17f6e6a043d57d36126e2af90315",
            "@type": "ScholarlyArticle",
            "paperId": "1776b5ba5d2b17f6e6a043d57d36126e2af90315",
            "corpusId": 118783209,
            "url": "https://www.semanticscholar.org/paper/1776b5ba5d2b17f6e6a043d57d36126e2af90315",
            "title": "Algorithmic Learning in a Random World",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2005,
            "externalIds": {
                "MAG": "1553101044",
                "DOI": "10.1007/978-3-031-06649-8",
                "CorpusId": 118783209
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 430,
            "influentialCitationCount": 86,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-0-387-25061-8/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2005-03-22",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Vovk2005AlgorithmicLI,\n author = {V. Vovk and A. Gammerman and G. Shafer},\n title = {Algorithmic Learning in a Random World},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:356125478f5d06b564b420755a4944254045bbbe",
            "@type": "ScholarlyArticle",
            "paperId": "356125478f5d06b564b420755a4944254045bbbe",
            "corpusId": 30545896,
            "url": "https://www.semanticscholar.org/paper/356125478f5d06b564b420755a4944254045bbbe",
            "title": "Support vector learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1997,
            "externalIds": {
                "MAG": "854322902",
                "DBLP": "phd/dnb/Scholkopf97",
                "CorpusId": 30545896
            },
            "abstract": "Foreword The Support Vector Machine has recently been introduced as a new technique for solving various function estimation problems, including the pattern recognition problem. To develop such a technique, it was necessary to rst extract factors responsible for future generalization, to obtain bounds on generalization that depend on these factors, and lastly to develop a technique that constructively minimizes these bounds. The subject of this book are methods based on combining advanced branches of statistics and functional analysis, developing these theories into practical algorithms that perform better than existing heuristic approaches. The book provides a comprehensive analysis of what can be done using Support Vector Machines, achieving record results in real-life pattern recognition problems. In addition, it proposes a new form of nonlinear Principal Component Analysis using Support Vector kernel techniques, which I consider as the most natural and elegant way for generalization of classical Principal Component Analysis. In many ways the Support Vector machine became so popular thanks to works of Bernhard Schh olkopf. The work, submitted for the title of Doktor der Naturwis-senschaften, appears as excellent. It is a substantial contribution to Machine Learning technology.",
            "referenceCount": 102,
            "citationCount": 624,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Scholkopf1997SupportVL,\n author = {B. Scholkopf},\n pages = {1-173},\n title = {Support vector learning},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c8b5825b8994ce3c4dc7e603423d7d43a5ead15c",
            "@type": "ScholarlyArticle",
            "paperId": "c8b5825b8994ce3c4dc7e603423d7d43a5ead15c",
            "corpusId": 2449067,
            "url": "https://www.semanticscholar.org/paper/c8b5825b8994ce3c4dc7e603423d7d43a5ead15c",
            "title": "Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2167467747",
                "DBLP": "journals/ml/BrazdilSC03",
                "DOI": "10.1023/A:1021713901879",
                "CorpusId": 2449067
            },
            "abstract": null,
            "referenceCount": 56,
            "citationCount": 448,
            "influentialCitationCount": 33,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1021713901879.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2003-03-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "50"
            },
            "citationStyles": {
                "bibtex": "@Article{Brazdil2003RankingLA,\n author = {P. Brazdil and Carlos Soares and J. Costa},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {251-277},\n title = {Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results},\n volume = {50},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7b7f767edd532f1f156a31c2efc550e7a0b0279e",
            "@type": "ScholarlyArticle",
            "paperId": "7b7f767edd532f1f156a31c2efc550e7a0b0279e",
            "corpusId": 226691,
            "url": "https://www.semanticscholar.org/paper/7b7f767edd532f1f156a31c2efc550e7a0b0279e",
            "title": "Incremental Support Vector Learning: Analysis, Implementation and Applications",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2155398148",
                "DBLP": "journals/jmlr/LaskovGKM06",
                "CorpusId": 226691
            },
            "abstract": "Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efficient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network traffic, can be foreseen.",
            "referenceCount": 35,
            "citationCount": 373,
            "influentialCitationCount": 38,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Laskov2006IncrementalSV,\n author = {P. Laskov and Christian Gehl and Stefan Kr\u00fcger and K. M\u00fcller},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1909-1936},\n title = {Incremental Support Vector Learning: Analysis, Implementation and Applications},\n volume = {7},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9e5e82a07998ce355b65fe5d43b5c3138fd767d9",
            "@type": "ScholarlyArticle",
            "paperId": "9e5e82a07998ce355b65fe5d43b5c3138fd767d9",
            "corpusId": 9839775,
            "url": "https://www.semanticscholar.org/paper/9e5e82a07998ce355b65fe5d43b5c3138fd767d9",
            "title": "The power of amnesia: Learning probabilistic automata with variable memory length",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1996,
            "externalIds": {
                "MAG": "1994920552",
                "DOI": "10.1007/BF00114008",
                "CorpusId": 9839775
            },
            "abstract": null,
            "referenceCount": 37,
            "citationCount": 509,
            "influentialCitationCount": 81,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/BF00114008.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1996-12-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "25"
            },
            "citationStyles": {
                "bibtex": "@Article{Ron1996ThePO,\n author = {D. Ron and Y. Singer and Naftali Tishby},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {117-149},\n title = {The power of amnesia: Learning probabilistic automata with variable memory length},\n volume = {25},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1889b9c3e8bc1118448b95fca38d6eff0bfca64d",
            "@type": "ScholarlyArticle",
            "paperId": "1889b9c3e8bc1118448b95fca38d6eff0bfca64d",
            "corpusId": 2133158,
            "url": "https://www.semanticscholar.org/paper/1889b9c3e8bc1118448b95fca38d6eff0bfca64d",
            "title": "Learning the Kernel with Hyperkernels",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "journals/jmlr/OngSW05",
                "MAG": "2170356051",
                "CorpusId": 2133158
            },
            "abstract": "This paper addresses the problem of choosing a kernel suitable for estimation with a support vector machine, hence further automating machine learning. This goal is achieved by defining a reproducing kernel Hilbert space on the space of kernels itself. Such a formulation leads to a statistical estimation problem similar to the problem of minimizing a regularized risk functional.We state the equivalent representer theorem for the choice of kernels and present a semidefinite programming formulation of the resulting optimization problem. Several recipes for constructing hyperkernels are provided, as well as the details of common machine learning problems. Experimental results for classification, regression and novelty detection on UCI data show the feasibility of our approach.",
            "referenceCount": 48,
            "citationCount": 369,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2005-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Ong2005LearningTK,\n author = {Cheng Soon Ong and Alex Smola and R. C. Williamson},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1043-1071},\n title = {Learning the Kernel with Hyperkernels},\n volume = {6},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a9565f70db3a6b5e4ff1df272238b8d7a84a1337",
            "@type": "ScholarlyArticle",
            "paperId": "a9565f70db3a6b5e4ff1df272238b8d7a84a1337",
            "corpusId": 8528878,
            "url": "https://www.semanticscholar.org/paper/a9565f70db3a6b5e4ff1df272238b8d7a84a1337",
            "title": "Experience with a learning personal assistant",
            "venue": "CACM",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1994,
            "externalIds": {
                "DBLP": "journals/cacm/MitchellCFMZ94",
                "MAG": "2118383892",
                "DOI": "10.1145/176789.176798",
                "CorpusId": 8528878
            },
            "abstract": "Personal software assistants that help users with tasks like finding information, scheduling calendars, or managing work-flow will require significant customization to each individual user. For example, an assistant that helps schedule a particular user\u2019s calendar will have to know that user\u2019s scheduling preferences. This paper explores the potential of machine learning methods to automatically create and maintain such customized knowledge for personal software assistants. We describe the design of one particular learning assistant: a calendar manager, called CAP (Calendar APprentice), that learns user scheduling preferences from experience. Results are summarized from approximately five user-years of experience, during which CAP has learned an evolving set of several thousand rules that characterize the scheduling preferences of its users. Based on this experience, we suggest that machine learning methods may play an important role in future personal software assistants.",
            "referenceCount": 22,
            "citationCount": 517,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/176789.176798",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1994-07-01",
            "journal": {
                "name": "Commun. ACM",
                "volume": "37"
            },
            "citationStyles": {
                "bibtex": "@Article{Mitchell1994ExperienceWA,\n author = {Tom Michael Mitchell and R. Caruana and Dayne Freitag and J. McDermott and David Zabowski},\n booktitle = {CACM},\n journal = {Commun. ACM},\n pages = {80-91},\n title = {Experience with a learning personal assistant},\n volume = {37},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e5ecac3aa41e359aa3a5d25943d0af72ff91df25",
            "@type": "ScholarlyArticle",
            "paperId": "e5ecac3aa41e359aa3a5d25943d0af72ff91df25",
            "corpusId": 122066566,
            "url": "https://www.semanticscholar.org/paper/e5ecac3aa41e359aa3a5d25943d0af72ff91df25",
            "title": "Support vector machines : theory and applications",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2140494000",
                "DOI": "10.1007/B95439",
                "CorpusId": 122066566
            },
            "abstract": null,
            "referenceCount": 57,
            "citationCount": 642,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.svms.org/tutorials/EvgeniouPontil1999.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Wang2005SupportVM,\n author = {Lipo Wang},\n title = {Support vector machines : theory and applications},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3e06c979b01b1c235017495d7d3a2769bb6a81bc",
            "@type": "ScholarlyArticle",
            "paperId": "3e06c979b01b1c235017495d7d3a2769bb6a81bc",
            "corpusId": 60987823,
            "url": "https://www.semanticscholar.org/paper/3e06c979b01b1c235017495d7d3a2769bb6a81bc",
            "title": "Learning to Learn: Introduction and Overview",
            "venue": "Learning to Learn",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "DBLP": "books/sp/98/ThrunP98",
                "MAG": "1542791059",
                "DOI": "10.1007/978-1-4615-5529-2_1",
                "CorpusId": 60987823
            },
            "abstract": null,
            "referenceCount": 56,
            "citationCount": 471,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "1998-05-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Thrun1998LearningTL,\n author = {S. Thrun and L. Pratt},\n booktitle = {Learning to Learn},\n pages = {3-17},\n title = {Learning to Learn: Introduction and Overview},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dde691805cfa7d6f1bb88c7411c1c3377b6cdc67",
            "@type": "ScholarlyArticle",
            "paperId": "dde691805cfa7d6f1bb88c7411c1c3377b6cdc67",
            "corpusId": 61636432,
            "url": "https://www.semanticscholar.org/paper/dde691805cfa7d6f1bb88c7411c1c3377b6cdc67",
            "title": "Lifelong Learning Algorithms",
            "venue": "Learning to Learn",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2126204609",
                "DBLP": "books/sp/98/Thrun98",
                "DOI": "10.1007/978-1-4615-5529-2_8",
                "CorpusId": 61636432
            },
            "abstract": null,
            "referenceCount": 54,
            "citationCount": 474,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1998-05-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Thrun1998LifelongLA,\n author = {S. Thrun},\n booktitle = {Learning to Learn},\n pages = {181-209},\n title = {Lifelong Learning Algorithms},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e37226a2f099c9a1ad13edce395ccca29225193c",
            "@type": "ScholarlyArticle",
            "paperId": "e37226a2f099c9a1ad13edce395ccca29225193c",
            "corpusId": 2649314,
            "url": "https://www.semanticscholar.org/paper/e37226a2f099c9a1ad13edce395ccca29225193c",
            "title": "Support Vector Machines Under Adversarial Label Noise",
            "venue": "Asian Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:2486528b-036c-4f3c-953f-c574eb381d12",
                "name": "Asian Conference on Machine Learning",
                "alternate_names": [
                    "Asian Conf Mach Learn",
                    "ACML"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=40"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "journals/jmlr/BiggioNL11",
                "MAG": "2187013920",
                "CorpusId": 2649314
            },
            "abstract": "In adversarial classication tasks like spam ltering and intrusion detection, malicious adversaries may manipulate data to thwart the outcome of an automatic analysis. Thus, besides achieving good classication performances, machine learning algorithms have to be robust against adversarial data manipulation to successfully operate in these tasks. While support vector machines (SVMs) have shown to be a very successful approach in classication problems, their eectiveness in adversarial classication tasks has not been extensively investigated yet. In this paper we present a preliminary investigation of the robustness of SVMs against adversarial data manipulation. In particular, we assume that the adversary has control over some training data, and aims to subvert the SVM learning process. Within this assumption, we show that this is indeed possible, and propose a strategy to improve the robustness of SVMs to training data manipulation based on a simple kernel matrix correction.",
            "referenceCount": 22,
            "citationCount": 354,
            "influentialCitationCount": 31,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-11-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Biggio2011SupportVM,\n author = {B. Biggio and B. Nelson and P. Laskov},\n booktitle = {Asian Conference on Machine Learning},\n pages = {97-112},\n title = {Support Vector Machines Under Adversarial Label Noise},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d18f64aa830075ed3e10206907f32c8fb2aa189d",
            "@type": "ScholarlyArticle",
            "paperId": "d18f64aa830075ed3e10206907f32c8fb2aa189d",
            "corpusId": 64208444,
            "url": "https://www.semanticscholar.org/paper/d18f64aa830075ed3e10206907f32c8fb2aa189d",
            "title": "INTRODUCTION TO STATISTICAL LEARNING THEORY AND SUPPORT VECTOR MACHINES",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2359149187",
                "CorpusId": 64208444
            },
            "abstract": "Data based machine learning covers a wide range of topics from pattern recognition to function regression and density estimation. Most of the existing methods are based on traditional statistics, which provides conclusion only for the situation where sample size is tending to infinity. So they may not work in practical cases of limited samples. Statistical Learning Theory or SLT is a small sample statistics by Vapnik \ue006et al.,\ue006 which concerns mainly the statistic principles when samples are limited, especially the properties of learning procedure in such cases. SLT provides us a new framework for the general learning problem, and a novel powerful learning method called Support Vector Machine or SVM, which can solve small sample learning problems better. It is believed that the study of SLT and SVM is becoming a new hot area in the field of machine learning. This review introduces the basic ideas of SLT and SVM, their major characteristics and some current research trends.",
            "referenceCount": 0,
            "citationCount": 432,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Zhang2000INTRODUCTIONTS,\n author = {Xuegong Zhang},\n title = {INTRODUCTION TO STATISTICAL LEARNING THEORY AND SUPPORT VECTOR MACHINES},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:76fc26939d73c565431e1f38107a4245e739284e",
            "@type": "ScholarlyArticle",
            "paperId": "76fc26939d73c565431e1f38107a4245e739284e",
            "corpusId": 12269265,
            "url": "https://www.semanticscholar.org/paper/76fc26939d73c565431e1f38107a4245e739284e",
            "title": "A Simple Approach to Ordinal Classification",
            "venue": "European Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:f9a81beb-9bcb-43bc-96a7-67cf23dba2d6",
                "name": "European Conference on Machine Learning",
                "alternate_names": [
                    "Eur Conf Mach Learn",
                    "European conference on Machine Learning",
                    "Eur conf Mach Learn",
                    "ECML"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/ecml"
            },
            "year": 2001,
            "externalIds": {
                "DBLP": "conf/ecml/FrankH01",
                "MAG": "1997855593",
                "DOI": "10.1007/3-540-44795-4_13",
                "CorpusId": 12269265
            },
            "abstract": null,
            "referenceCount": 9,
            "citationCount": 582,
            "influentialCitationCount": 66,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2F3-540-44795-4_13.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2001-09-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Frank2001ASA,\n author = {E. Frank and M. Hall},\n booktitle = {European Conference on Machine Learning},\n pages = {145-156},\n title = {A Simple Approach to Ordinal Classification},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:01a61d9b9183ce11c89e36d9e1f24614c98f3ee8",
            "@type": "ScholarlyArticle",
            "paperId": "01a61d9b9183ce11c89e36d9e1f24614c98f3ee8",
            "corpusId": 14439180,
            "url": "https://www.semanticscholar.org/paper/01a61d9b9183ce11c89e36d9e1f24614c98f3ee8",
            "title": "Query Learning with Large Margin Classifiers",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "1483816357",
                "DBLP": "conf/icml/CampbellCS00",
                "CorpusId": 14439180
            },
            "abstract": "The active selection of instances can significantly improve the generalisation performance of a learning machine. Large margin classifiers such as support vector machines classify data using the most informative instances (the support vectors). This makes them natural candidates for instance selection strategies. In this paper we propose an algorithm for the training of support vector machines using instance selection. We give a theoretical justification for the strategy and experimental results on real and artificial data demonstrating its effectiveness. The technique is most efficient when the data set can be learnt using few support vectors.",
            "referenceCount": 16,
            "citationCount": 425,
            "influentialCitationCount": 33,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2000-06-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Campbell2000QueryLW,\n author = {C. Campbell and N. Cristianini and Alex Smola},\n booktitle = {International Conference on Machine Learning},\n pages = {111-118},\n title = {Query Learning with Large Margin Classifiers},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f95f58a09066ecbfaecdf37b9d9ceb2020718d96",
            "@type": "ScholarlyArticle",
            "paperId": "f95f58a09066ecbfaecdf37b9d9ceb2020718d96",
            "corpusId": 429624,
            "url": "https://www.semanticscholar.org/paper/f95f58a09066ecbfaecdf37b9d9ceb2020718d96",
            "title": "WEKA - Experiences with a Java Open-Source Project",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2095776265",
                "DBLP": "journals/jmlr/BouckaertFHHPRW10",
                "DOI": "10.5555/1756006.1953016",
                "CorpusId": 429624
            },
            "abstract": "WEKA is a popular machine learning workbench with a development life of nearly two decades. This article provides an overview of the factors that we believe to be important to its success. Rather than focussing on the software's functionality, we review aspects of project management and historical development decisions that likely had an impact on the uptake of the project.",
            "referenceCount": 8,
            "citationCount": 345,
            "influentialCitationCount": 28,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2010-03-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Bouckaert2010WEKAE,\n author = {R. Bouckaert and E. Frank and M. Hall and G. Holmes and B. Pfahringer and P. Reutemann and I. Witten},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {2533-2541},\n title = {WEKA - Experiences with a Java Open-Source Project},\n volume = {11},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:480ddeb902c15861ef8f294f00c543dae508ee9b",
            "@type": "ScholarlyArticle",
            "paperId": "480ddeb902c15861ef8f294f00c543dae508ee9b",
            "corpusId": 10125108,
            "url": "https://www.semanticscholar.org/paper/480ddeb902c15861ef8f294f00c543dae508ee9b",
            "title": "Optimization Techniques for Semi-Supervised Support Vector Machines",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "journals/jmlr/ChapelleSK08",
                "MAG": "2122565017",
                "DOI": "10.5555/1390681.1390688",
                "CorpusId": 10125108
            },
            "abstract": "Due to its wide applicability, the problem of semi-supervised classification is attracting increasing attention in machine learning. Semi-Supervised Support Vector Machines (S3VMs) are based on applying the margin maximization principle to both labeled and unlabeled examples. Unlike SVMs, their formulation leads to a non-convex optimization problem. A suite of algorithms have recently been proposed for solving S3VMs. This paper reviews key ideas in this literature. The performance and behavior of various S3VMs algorithms is studied together, under a common experimental setting.",
            "referenceCount": 36,
            "citationCount": 448,
            "influentialCitationCount": 46,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2008-06-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Chapelle2008OptimizationTF,\n author = {O. Chapelle and Vikas Sindhwani and S. Keerthi},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {203-233},\n title = {Optimization Techniques for Semi-Supervised Support Vector Machines},\n volume = {9},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:008abebf4a9404db9050c9d2fbca769f4faf3ca6",
            "@type": "ScholarlyArticle",
            "paperId": "008abebf4a9404db9050c9d2fbca769f4faf3ca6",
            "corpusId": 2374498,
            "url": "https://www.semanticscholar.org/paper/008abebf4a9404db9050c9d2fbca769f4faf3ca6",
            "title": "Learning by Transduction",
            "venue": "Conference on Uncertainty in Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:f9af8000-42f8-410d-a622-e8811e41660a",
                "name": "Conference on Uncertainty in Artificial Intelligence",
                "alternate_names": [
                    "Uncertainty in Artificial Intelligence",
                    "UAI",
                    "Conf Uncertain Artif Intell",
                    "Uncertain Artif Intell"
                ],
                "issn": null,
                "url": "http://www.auai.org/"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2950502699",
                "DBLP": "journals/corr/abs-1301-7375",
                "ArXiv": "1301.7375",
                "CorpusId": 2374498
            },
            "abstract": "We describe a method for predicting a classification of an object given classifications of the objects in the training set, assuming that the pairs object/classification are generated by an i.i.d. process from a continuous probability distribution. Our method is a modification of Vapnik's support-vector machine; its main novelty is that it gives not only the prediction itself but also a practicable measure of the evidence found in support of that prediction. We also describe a procedure for assigning degrees of confidence to predictions made by the support vector machine. Some experimental results are presented, and possible extensions of the algorithms are discussed.",
            "referenceCount": 10,
            "citationCount": 453,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1998-07-24",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1301.7375"
            },
            "citationStyles": {
                "bibtex": "@Article{Gammerman1998LearningBT,\n author = {A. Gammerman and V. Vovk and V. Vapnik},\n booktitle = {Conference on Uncertainty in Artificial Intelligence},\n journal = {ArXiv},\n title = {Learning by Transduction},\n volume = {abs/1301.7375},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5fe5ed2a3b50becdbbcd17e7733653d5ef6ac398",
            "@type": "ScholarlyArticle",
            "paperId": "5fe5ed2a3b50becdbbcd17e7733653d5ef6ac398",
            "corpusId": 9699301,
            "url": "https://www.semanticscholar.org/paper/5fe5ed2a3b50becdbbcd17e7733653d5ef6ac398",
            "title": "Hidden Markov Support Vector Machines",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "conf/icml/AltunTH03",
                "MAG": "2139686264",
                "CorpusId": 9699301
            },
            "abstract": "This paper presents a novel discriminative learning technique for label sequences based on a combination of the two most successful learning algorithms, Support Vector Machines and Hidden Markov Models which we call Hidden Markov Support Vector Machine. The proposed architecture handles dependencies between neighboring labels using Viterbi decoding. In contrast to standard HMM training, the learning procedure is discriminative and is based on a maximum/soft margin criterion. Compared to previous methods like Conditional Random Fields, Maximum Entropy Markov Models and label sequence boosting, HM-SVMs have a number of advantages. Most notably, it is possible to learn non-linear discriminant functions via kernel functions. At the same time, HM-SVMs share the key advantages with other discriminative methods, in particular the capability to deal with overlapping features. We report experimental evaluations on two tasks, named entity recognition and part-of-speech tagging, that demonstrate the competitiveness of the proposed approach.",
            "referenceCount": 13,
            "citationCount": 571,
            "influentialCitationCount": 91,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2003-08-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Altun2003HiddenMS,\n author = {Y. Altun and Ioannis Tsochantaridis and Thomas Hofmann},\n booktitle = {International Conference on Machine Learning},\n pages = {3-10},\n title = {Hidden Markov Support Vector Machines},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6b0966c51d66e3097fc9f9d704bc43fdd963e90e",
            "@type": "ScholarlyArticle",
            "paperId": "6b0966c51d66e3097fc9f9d704bc43fdd963e90e",
            "corpusId": 60324982,
            "url": "https://www.semanticscholar.org/paper/6b0966c51d66e3097fc9f9d704bc43fdd963e90e",
            "title": "Learning in Humans and Machines: Towards an Interdisciplinary Learning Science",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1995,
            "externalIds": {
                "MAG": "430053520",
                "CorpusId": 60324982
            },
            "abstract": "Chapter headings: Towards an Interdisciplinary Learning Science (P. Reimann, H. Spada). A Cognitive Psychological Approach to Learning (S. Vosniadou). Learning to Do and Learning to Understand: A Lesson and a Challenge for Cognitive Modeling (S. Ohlsson). Machine Learning: Case Studies of an Interdisciplinary Approach (W. Emde). Mental and Physical Artifacts in Cognitive Practices (R. Saljo). Learning Theory and Instructional Science (E. De Corte). Knowledge Representation Changes in Humans and Machines (L. Saitta and Task Force 1). Multi-Objective Learning with Multiple Representations (M. Van Someren, P. Reimann). Order Effects in Incremental Learning (P. Langley). Situated Learning and Transfer (H. Gruber et al.). The Evolution of Research on Collaborative Learning (P. Dillenbourg et al.). A Developmental Case Study on Sequential Learning: The Day-Night Cycle (K. Morik, S. Vosniadou). Subject index. Author index.",
            "referenceCount": 0,
            "citationCount": 480,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1995-12-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Spada1995LearningIH,\n author = {H. Spada and Reimann and P. Reimann},\n title = {Learning in Humans and Machines: Towards an Interdisciplinary Learning Science},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9308cfdabf5303534b97d9ce5bfbb2c919a3f9cb",
            "@type": "ScholarlyArticle",
            "paperId": "9308cfdabf5303534b97d9ce5bfbb2c919a3f9cb",
            "corpusId": 7968702,
            "url": "https://www.semanticscholar.org/paper/9308cfdabf5303534b97d9ce5bfbb2c919a3f9cb",
            "title": "WEKA: The Waikato Environment for Knowledge Analysis",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1996,
            "externalIds": {
                "MAG": "1576226931",
                "CorpusId": 7968702
            },
            "abstract": "WEKA is a workbench designed to aid in the application of machine learning technology to real world data sets, in particular, data sets from New Zealand\u2019s agricultural sector. In order to do this a range of machine learning techniques are presented to the user in such a way as to hide the idiosyncrasies of input and output formats, as well as allow an exploratory approach in applying the technology. The system presented is a component based one that also has application in machine learning research and education.",
            "referenceCount": 7,
            "citationCount": 559,
            "influentialCitationCount": 52,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Agricultural and Food Sciences",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Garner1996WEKATW,\n author = {Stephen R. Garner},\n title = {WEKA: The Waikato Environment for Knowledge Analysis},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6b945d00f77367422965531dd3d01694822c52c1",
            "@type": "ScholarlyArticle",
            "paperId": "6b945d00f77367422965531dd3d01694822c52c1",
            "corpusId": 1122387,
            "url": "https://www.semanticscholar.org/paper/6b945d00f77367422965531dd3d01694822c52c1",
            "title": "Ensembles of Learning Machines",
            "venue": "Italian Workshop on Neural Nets",
            "publicationVenue": {
                "id": "urn:research:e799eb83-3064-4379-9167-578d848d8991",
                "name": "Italian Workshop on Neural Nets",
                "alternate_names": [
                    "Ital Workshop Neural Net",
                    "WIRN"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=3083"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "1509092951",
                "DBLP": "conf/wirn/ValentiniM02",
                "DOI": "10.1007/3-540-45808-5_1",
                "CorpusId": 1122387
            },
            "abstract": null,
            "referenceCount": 130,
            "citationCount": 320,
            "influentialCitationCount": 18,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.disi.unige.it/person/MasulliF/papers/masulli-wirn02.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2002-05-30",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Valentini2002EnsemblesOL,\n author = {G. Valentini and F. Masulli},\n booktitle = {Italian Workshop on Neural Nets},\n pages = {3-22},\n title = {Ensembles of Learning Machines},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2e75ad22a6865c3402925f12317dd0897c67ebe6",
            "@type": "ScholarlyArticle",
            "paperId": "2e75ad22a6865c3402925f12317dd0897c67ebe6",
            "corpusId": 5671144,
            "url": "https://www.semanticscholar.org/paper/2e75ad22a6865c3402925f12317dd0897c67ebe6",
            "title": "Predicting Structured Data",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "MAG": "1515020792",
                "DOI": "10.1198/tech.2008.s913",
                "CorpusId": 5671144
            },
            "abstract": "Machine learning develops intelligent computer systems that are able to generalize from previously seen examples. A new domain of machine learning, in which the prediction must satisfy the additional constraints found in structured data, poses one of machine learning's greatest challenges: learning functional dependencies between arbitrary input and output domains. This volume presents and analyzes the state of the art in machine learning algorithms and theory in this novel field. The contributors discuss applications as diverse as machine translation, document markup, computational biology, and information extraction, among others, providing a timely overview of an exciting field. Contributors Yasemin Altun, Gokhan Bakir [no dot over i], Olivier Bousquet, Sumit Chopra, Corinna Cortes, Hal Daume III, Ofer Dekel, Zoubin Ghahramani, Raia Hadsell, Thomas Hofmann, Fu Jie Huang, Yann LeCun, Tobias Mann, Daniel Marcu, David McAllester, Mehryar Mohri, William Stafford Noble, Fernando Perez-Cruz, Massimiliano Pontil, Marc'Aurelio Ranzato, Juho Rousu, Craig Saunders, Bernhard Scholkopf, Matthias W. Seeger, Shai Shalev-Shwartz, John Shawe-Taylor, Yoram Singer, Alexander J. Smola, Sandor Szedmak, Ben Taskar, Ioannis Tsochantaridis, S.V.N Vishwanathan, Jason Weston Gokhan Bakir [no dot over i] is Research Scientist at the Max Planck Institute for Biological Cybernetics in Tubingen, Germany. Thomas Hofmann is a Director of Engineering at Google's Engineering Center in Zurich and Adjunct Associate Professor of Computer Science at Brown University. Bernhard Scholkopf is Director of the Max Planck Institute for Biological Cybernetics and Professor at the Technical University Berlin. Alexander J. Smola is Senior Principal Researcher and Machine Learning Program Leader at National ICT Australia/Australian National University, Canberra. Ben Taskar is Assistant Professor in the Computer and Information Science Department at the University of Pennsylvania. S. V. N. Vishwanathan is Senior Researcher in the Statistical Machine Learning Program, National ICT Australia with an adjunct appointment at the Research School for Information Sciences and Engineering, Australian National University.",
            "referenceCount": 21,
            "citationCount": 449,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2007-09-01",
            "journal": {
                "name": "Technometrics",
                "volume": "50"
            },
            "citationStyles": {
                "bibtex": "@Article{Touretzky2007PredictingSD,\n author = {D. Touretzky and J. Moody and S. Hanson and J. Cowan and G. Bakir and T. Hofmann and B. Scholkopf and Alex Smola and B. Taskar and Shravan Vishwanathan and F. P\u00e9rez-Cruz and Z. Ghahramani and Fernando P\u00e9rez-Cruz and Zoubin Ghahramani},\n journal = {Technometrics},\n pages = {413 - 414},\n title = {Predicting Structured Data},\n volume = {50},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:47fc921add1421ff8adb730df7aa9e7f865bfdeb",
            "@type": "ScholarlyArticle",
            "paperId": "47fc921add1421ff8adb730df7aa9e7f865bfdeb",
            "corpusId": 7792259,
            "url": "https://www.semanticscholar.org/paper/47fc921add1421ff8adb730df7aa9e7f865bfdeb",
            "title": "Toward Practical Smile Detection",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2157653492",
                "DBLP": "journals/pami/WhitehillLFBM09",
                "DOI": "10.1109/TPAMI.2009.42",
                "CorpusId": 7792259,
                "PubMed": "19762937"
            },
            "abstract": "Machine learning approaches have produced some of the highest reported performances for facial expression recognition. However, to date, nearly all automatic facial expression recognition research has focused on optimizing performance on a few databases that were collected under controlled lighting conditions on a relatively small number of subjects. This paper explores whether current machine learning methods can be used to develop an expression recognition system that operates reliably in more realistic conditions. We explore the necessary characteristics of the training data set, image registration, feature representation, and machine learning algorithms. A new database, GENKI, is presented which contains pictures, photographed by the subjects themselves, from thousands of different people in many different real-world imaging conditions. Results suggest that human-level expression recognition accuracy in real-life illumination conditions is achievable with machine learning technology. However, the data sets currently used in the automatic expression recognition literature to evaluate progress may be overly constrained and could potentially lead research into locally optimal algorithmic solutions.",
            "referenceCount": 43,
            "citationCount": 324,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-11-01",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "31"
            },
            "citationStyles": {
                "bibtex": "@Article{Whitehill2009TowardPS,\n author = {J. Whitehill and G. Littlewort and Ian R. Fasel and M. Bartlett and J. Movellan},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {2106-2111},\n title = {Toward Practical Smile Detection},\n volume = {31},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:79a8334eb8393be100503b3d7b8f27dab2181528",
            "@type": "ScholarlyArticle",
            "paperId": "79a8334eb8393be100503b3d7b8f27dab2181528",
            "corpusId": 15076400,
            "url": "https://www.semanticscholar.org/paper/79a8334eb8393be100503b3d7b8f27dab2181528",
            "title": "On the relation between multi-instance learning and semi-supervised learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "conf/icml/ZhouX07",
                "MAG": "2098239572",
                "DOI": "10.1145/1273496.1273643",
                "CorpusId": 15076400
            },
            "abstract": "Multi-instance learning and semi-supervised learning are different branches of machine learning. The former attempts to learn from a training set consists of labeled bags each containing many unlabeled instances; the latter tries to exploit abundant unlabeled instances when learning with a small number of labeled examples. In this paper, we establish a bridge between these two branches by showing that multi-instance learning can be viewed as a special case of semi-supervised learning. Based on this recognition, we propose the MissSVM algorithm which addresses multi-instance learning using a special semi-supervised support vector machine. Experiments show that solving multi-instance problems from the view of semi-supervised learning is feasible, and the MissSVM algorithm is competitive with state-of-the-art multi-instance learning algorithms.",
            "referenceCount": 41,
            "citationCount": 171,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://pages.cs.wisc.edu/~xujm/pub/icml07.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-06-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhou2007OnTR,\n author = {Zhi-Hua Zhou and Jun-Ming Xu},\n booktitle = {International Conference on Machine Learning},\n pages = {1167-1174},\n title = {On the relation between multi-instance learning and semi-supervised learning},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:30bc3984a3702f7947bf3bf18efc8908afeb4837",
            "@type": "ScholarlyArticle",
            "paperId": "30bc3984a3702f7947bf3bf18efc8908afeb4837",
            "corpusId": 12886175,
            "url": "https://www.semanticscholar.org/paper/30bc3984a3702f7947bf3bf18efc8908afeb4837",
            "title": "Learning to Classify Ordinal Data: The Data Replication Method",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "journals/jmlr/CardosoC07",
                "MAG": "2164939051",
                "DOI": "10.5555/1314498.1314546",
                "CorpusId": 12886175
            },
            "abstract": "Classification of ordinal data is one of the most important tasks of relation learning. This paper introduces a new machine learning paradigm specifically intended for classification problems where the classes have a natural order. The technique reduces the problem of classifying ordered classes to the standard two-class problem. The introduced method is then mapped into support vector machines and neural networks. Generalization bounds of the proposed ordinal classifier are also provided. An experimental study with artificial and real data sets, including an application to gene expression analysis, verifies the usefulness of the proposed approach.",
            "referenceCount": 24,
            "citationCount": 185,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Cardoso2007LearningTC,\n author = {Jaime S. Cardoso and J. Costa},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1393-1429},\n title = {Learning to Classify Ordinal Data: The Data Replication Method},\n volume = {8},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:679bc3cf9f52b0a1d8ff2b5ed4718ce6e44f9a56",
            "@type": "ScholarlyArticle",
            "paperId": "679bc3cf9f52b0a1d8ff2b5ed4718ce6e44f9a56",
            "corpusId": 60676619,
            "url": "https://www.semanticscholar.org/paper/679bc3cf9f52b0a1d8ff2b5ed4718ce6e44f9a56",
            "title": "The Bayesian backfitting relevance vector machine",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "conf/icml/DSouzaVS04",
                "MAG": "2600902286",
                "DOI": "10.1145/1015330.1015358",
                "CorpusId": 60676619
            },
            "abstract": "Traditional non-parametric statistical learning techniques are often computationally attractive, but lack the same generalization and model selection abilities as state-of-the-art Bayesian algorithms which, however, are usually computationally prohibitive. This paper makes several important contributions that allow Bayesian learning to scale to more complex, real-world learning scenarios. Firstly, we show that backfitting --- a traditional non-parametric, yet highly efficient regression tool --- can be derived in a novel formulation within an expectation maximization (EM) framework and thus can finally be given a probabilistic interpretation. Secondly, we show that the general framework of sparse Bayesian learning and in particular the relevance vector machine (RVM), can be derived as a highly efficient algorithm using a Bayesian version of backfitting at its core. As we demonstrate on several regression and classification benchmarks, Bayesian backfitting offers a compelling alternative to current regression methods, especially when the size and dimensionality of the data challenge computational resources.",
            "referenceCount": 19,
            "citationCount": 233,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://homepages.inf.ed.ac.uk/svijayak/publications/dsouza-ICML2004.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2004-07-04",
            "journal": {
                "name": "Proceedings of the twenty-first international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{D'Souza2004TheBB,\n author = {Aaron D'Souza and S. Vijayakumar and Stefan Schaal},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the twenty-first international conference on Machine learning},\n title = {The Bayesian backfitting relevance vector machine},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d08b933458716a85888956165d07243505d4e4b2",
            "@type": "ScholarlyArticle",
            "paperId": "d08b933458716a85888956165d07243505d4e4b2",
            "corpusId": 14321251,
            "url": "https://www.semanticscholar.org/paper/d08b933458716a85888956165d07243505d4e4b2",
            "title": "Model Induction with Support Vector Machines: Introduction and Applications",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2037931255",
                "DOI": "10.1061/(ASCE)0887-3801(2001)15:3(208)",
                "CorpusId": 14321251
            },
            "abstract": "The rapid advance in information processing systems in recent decades had directed engineering research towards the development of intelligent systems that can evolve models of natural phenomena automatically\u2014\u201cby themselves,\u201d so to speak. In this respect, a wide range of machine learning techniques like decision trees, artificial neural networks (ANNs), Bayesian methods, fuzzy-rule based systems, and evolutionary algorithms have been successfully applied to model different civil engineering systems. In this study, the possibility of using yet another machine learning paradigm that is firmly based on the theory of statistical learning, namely that of the support vector machine (SVM), is investigated. An interesting property of this approach is that it is an approximate implementation of a structural risk minimization (SRM) induction principle that aims at minimizing a bound on the generalization error of a model, rather than minimizing only the mean square error over the data set. In this paper, the basic ...",
            "referenceCount": 39,
            "citationCount": 531,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2001-07-01",
            "journal": {
                "name": "Journal of Computing in Civil Engineering",
                "volume": "15"
            },
            "citationStyles": {
                "bibtex": "@Article{Dibike2001ModelIW,\n author = {Y. Dibike and S. Velickov and D. Solomatine and M. Abbott},\n journal = {Journal of Computing in Civil Engineering},\n pages = {208-216},\n title = {Model Induction with Support Vector Machines: Introduction and Applications},\n volume = {15},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b7d89441fcf28ca1a365af4d739709a7075a5db2",
            "@type": "ScholarlyArticle",
            "paperId": "b7d89441fcf28ca1a365af4d739709a7075a5db2",
            "corpusId": 60167398,
            "url": "https://www.semanticscholar.org/paper/b7d89441fcf28ca1a365af4d739709a7075a5db2",
            "title": "Knowledge Discovery with Support Vector Machines",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "580594664",
                "DOI": "10.1002/9780470503065",
                "CorpusId": 60167398
            },
            "abstract": "An easy-to-follow introduction to support vector machines This book provides an in-depth, easy-to-follow introduction to support vector machines drawing only from minimal, carefully motivated technical and mathematical background material. It begins with a cohesive discussion of machine learning and goes on to cover: Knowledge discovery environments Describing data mathematically Linear decision surfaces and functions Perceptron learning Maximum margin classifiers Support vector machines Elements of statistical learning theory Multi-class classification Regression with support vector machines Novelty detection Complemented with hands-on exercises, algorithm descriptions, and data sets, Knowledge Discovery with Support Vector Machines is an invaluable textbook for advanced undergraduate and graduate courses. It is also an excellent tutorial on support vector machines for professionals who are pursuing research in machine learning and related areas.",
            "referenceCount": 0,
            "citationCount": 302,
            "influentialCitationCount": 33,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470503065.fmatter",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2009-08-03",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Hamel2009KnowledgeDW,\n author = {L. Hamel},\n title = {Knowledge Discovery with Support Vector Machines},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:218a899ba7ff3094c0fc871b9605d8ff4f529336",
            "@type": "ScholarlyArticle",
            "paperId": "218a899ba7ff3094c0fc871b9605d8ff4f529336",
            "corpusId": 62688491,
            "url": "https://www.semanticscholar.org/paper/218a899ba7ff3094c0fc871b9605d8ff4f529336",
            "title": "Learning Logical Definitions from Relations",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2005,
            "externalIds": {
                "DOI": "10.1023/A:1022699322624",
                "CorpusId": 62688491
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 221,
            "influentialCitationCount": 46,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023%2FA%3A1022699322624.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Quinlan2005LearningLD,\n author = {J. R. Quinlan},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {239-266},\n title = {Learning Logical Definitions from Relations},\n volume = {5},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e6140a793a4554806eb39d15c018d8f782d2ac1e",
            "@type": "ScholarlyArticle",
            "paperId": "e6140a793a4554806eb39d15c018d8f782d2ac1e",
            "corpusId": 3231298,
            "url": "https://www.semanticscholar.org/paper/e6140a793a4554806eb39d15c018d8f782d2ac1e",
            "title": "Learning to Parse Natural Language with Maximum Entropy Models",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1999,
            "externalIds": {
                "DBLP": "journals/ml/Ratnaparkhi99",
                "MAG": "1953828586",
                "DOI": "10.1023/A:1007502103375",
                "CorpusId": 3231298
            },
            "abstract": null,
            "referenceCount": 31,
            "citationCount": 291,
            "influentialCitationCount": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1007502103375.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1999-02-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "34"
            },
            "citationStyles": {
                "bibtex": "@Article{Ratnaparkhi1999LearningTP,\n author = {A. Ratnaparkhi},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {151-175},\n title = {Learning to Parse Natural Language with Maximum Entropy Models},\n volume = {34},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c26bdb2c2b62212c26b4f2e91ea5f9011656a56c",
            "@type": "ScholarlyArticle",
            "paperId": "c26bdb2c2b62212c26b4f2e91ea5f9011656a56c",
            "corpusId": 17605363,
            "url": "https://www.semanticscholar.org/paper/c26bdb2c2b62212c26b4f2e91ea5f9011656a56c",
            "title": "Handling concept drifts in incremental learning with support vector machines",
            "venue": "Knowledge Discovery and Data Mining",
            "publicationVenue": {
                "id": "urn:research:a0edb93b-1e95-4128-a295-6b1659149cef",
                "name": "Knowledge Discovery and Data Mining",
                "alternate_names": [
                    "KDD",
                    "Knowl Discov Data Min"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigkdd/"
            },
            "year": 1999,
            "externalIds": {
                "MAG": "2078382802",
                "DBLP": "conf/kdd/SyedLS99a",
                "DOI": "10.1145/312129.312267",
                "CorpusId": 17605363
            },
            "abstract": "With the increase in the size of real-world databases, there is an ever-increasing need to scale up inductive learning algorithms. Incremental learning techniques are one possible solution to the scalability problem. In this paper, we propose three ctiteria to evaluate the robustness and reliability of incremental learning methods, and use them to study the robustness of an incremental training method for Support Vector Machines. We provide empirical results using benchmark machine learning datasets to show that support vectors form a svccdnct and suficient set for block-by-block incremental learning.",
            "referenceCount": 10,
            "citationCount": 269,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1999-08-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Syed1999HandlingCD,\n author = {N. Syed and Huan Liu and K. Sung},\n booktitle = {Knowledge Discovery and Data Mining},\n pages = {317-321},\n title = {Handling concept drifts in incremental learning with support vector machines},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0fe97c88452d8d8603d9ba883a0721da46ba84f4",
            "@type": "ScholarlyArticle",
            "paperId": "0fe97c88452d8d8603d9ba883a0721da46ba84f4",
            "corpusId": 1511272,
            "url": "https://www.semanticscholar.org/paper/0fe97c88452d8d8603d9ba883a0721da46ba84f4",
            "title": "The Set Covering Machine",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "journals/jmlr/MarchandS02",
                "MAG": "2170708028",
                "CorpusId": 1511272
            },
            "abstract": "We extend the classical algorithms of Valiant and Haussler for learning compact conjunctions and disjunctions of Boolean attributes to allow features that are constructed from the data and to allow a trade-off between accuracy and complexity. The result is a general-purpose learning machine, suitable for practical learning tasks, that we call the set covering machine. We present a version of the set covering machine that uses data-dependent balls for its set of features and compare its performance with the support vector machine. By extending a technique pioneered by Littlestone and Warmuth, we bound its generalization error as a function of the amount of data compression it achieves during training. In experiments with real-world learning tasks, the bound is shown to be extremely tight and to provide an effective guide for model selection.",
            "referenceCount": 18,
            "citationCount": 142,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2003-03-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Marchand2003TheSC,\n author = {M. Marchand and J. Shawe-Taylor},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {723-746},\n title = {The Set Covering Machine},\n volume = {3},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:886b9cd28b77e1a73da7112b95196e661ae8d5a1",
            "@type": "ScholarlyArticle",
            "paperId": "886b9cd28b77e1a73da7112b95196e661ae8d5a1",
            "corpusId": 578934,
            "url": "https://www.semanticscholar.org/paper/886b9cd28b77e1a73da7112b95196e661ae8d5a1",
            "title": "Explanation-Based Learning: An Alternative View",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1986,
            "externalIds": {
                "DBLP": "journals/ml/DejongM86",
                "MAG": "2101602574",
                "DOI": "10.1023/A:1022898111663",
                "CorpusId": 578934
            },
            "abstract": null,
            "referenceCount": 40,
            "citationCount": 317,
            "influentialCitationCount": 20,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1022898111663.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1986-03-02",
            "journal": {
                "name": "Machine Learning",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{DeJong1986ExplanationBasedLA,\n author = {G. DeJong and R. Mooney},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {145-176},\n title = {Explanation-Based Learning: An Alternative View},\n volume = {1},\n year = {1986}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:62ec643e5415b6ab89f7186d3631f5549fd8a0cc",
            "@type": "ScholarlyArticle",
            "paperId": "62ec643e5415b6ab89f7186d3631f5549fd8a0cc",
            "corpusId": 10496644,
            "url": "https://www.semanticscholar.org/paper/62ec643e5415b6ab89f7186d3631f5549fd8a0cc",
            "title": "Support Vector Machines and Kernel Methods: The New Generation of Learning Machines",
            "venue": "The AI Magazine",
            "publicationVenue": {
                "id": "urn:research:6fedff74-7525-4b7f-bbb4-4df4e23948e4",
                "name": "The AI Magazine",
                "alternate_names": [
                    "AI Mag",
                    "Ai Mag",
                    "Ai Magazine"
                ],
                "issn": "0738-4602",
                "url": "https://www.aaai.org/Library/Magazine/magazine-library.php"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2101057528",
                "DBLP": "journals/aim/CristianiniS02",
                "DOI": "10.1609/aimag.v23i3.1655",
                "CorpusId": 10496644
            },
            "abstract": "Kernel methods, a new generation of learning algorithms, utilize techniques from optimization, statistics, and functional analysis to achieve maximal generality, flexibility, and performance. These algorithms are different from earlier techniques used in machine learning in many respects: For example, they are explicitly based on a theoretical model of learning rather than on loose analogies with natural learning systems or other heuristics. They come with theoretical guarantees about their performance and have a modular design that makes it possible to separately implement and analyze their components. They are not affected by the problem of local minima because their training amounts to convex optimization. In the last decade, a sizable community of theoreticians and practitioners has formed around these methods, and a number of practical applications have been realized. Although the research is not concluded, already now kernel methods are considered the state of the art in several machine learning tasks. Their ease of use, theoretical appeal, and remarkable performance have made them the system of choice for many learning problems. Successful applications range from text categorization to handwriting recognition to classification of gene-expression data.",
            "referenceCount": 11,
            "citationCount": 215,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2002-09-01",
            "journal": {
                "name": "AI Mag.",
                "volume": "23"
            },
            "citationStyles": {
                "bibtex": "@Article{Cristianini2002SupportVM,\n author = {N. Cristianini and B. Scholkopf},\n booktitle = {The AI Magazine},\n journal = {AI Mag.},\n pages = {31-42},\n title = {Support Vector Machines and Kernel Methods: The New Generation of Learning Machines},\n volume = {23},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e1f9cb45bd8f0300a013a3da3e15546bfeeb60d0",
            "@type": "ScholarlyArticle",
            "paperId": "e1f9cb45bd8f0300a013a3da3e15546bfeeb60d0",
            "corpusId": 10282145,
            "url": "https://www.semanticscholar.org/paper/e1f9cb45bd8f0300a013a3da3e15546bfeeb60d0",
            "title": "Reinforcement Learning as Classification: Leveraging Modern Classifiers",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "conf/icml/LagoudakisP03",
                "MAG": "2134289401",
                "CorpusId": 10282145
            },
            "abstract": "The basic tools of machine learning appear in the inner loop of most reinforcement learning algorithms, typically in the form of Monte Carlo methods or function approximation techniques. To a large extent, however, current reinforcement learning algorithms draw upon machine learning techniques that are at least ten years old and, with a few exceptions, very little has been done to exploit recent advances in classification learning for the purposes of reinforcement learning. We use a variant of approximate policy iteration based on rollouts that allows us to use a pure classification learner, such as a support vector machine (SVM), in the inner loop of the algorithm. We argue that the use of SVMs, particularly in combination with the kernel trick, can make it easier to apply reinforcement learning as an \"out-of-the-box\" technique, without extensive feature engineering. Our approach opens the door to modern classification methods, but does not preclude the use of classical methods. We present experimental results in the pendulum balancing and bicycle riding domains using both SVMs and neural networks for classifiers.",
            "referenceCount": 19,
            "citationCount": 174,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2003-08-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lagoudakis2003ReinforcementLA,\n author = {M. Lagoudakis and Ronald E. Parr},\n booktitle = {International Conference on Machine Learning},\n pages = {424-431},\n title = {Reinforcement Learning as Classification: Leveraging Modern Classifiers},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fed98ea8ecad5034441fd0ac9f728479183e3b9e",
            "@type": "ScholarlyArticle",
            "paperId": "fed98ea8ecad5034441fd0ac9f728479183e3b9e",
            "corpusId": 52810328,
            "url": "https://www.semanticscholar.org/paper/fed98ea8ecad5034441fd0ac9f728479183e3b9e",
            "title": "Simplified Support Vector Decision Rules",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1996,
            "externalIds": {
                "MAG": "26816478",
                "DBLP": "conf/icml/Burges96",
                "CorpusId": 52810328
            },
            "abstract": "A Support Vector Machine SVM is a uni versal learning machine whose decision sur face is parameterized by a set of support vec tors and by a set of corresponding weights An SVM is also characterized by a kernel function Choice of the kernel determines whether the resulting SVM is a polynomial classi er a two layer neural network a ra dial basis function machine or some other learning machine SVMs are currently considerably slower in test phase than other approaches with sim ilar generalization performance To address this we present a general method to signif icantly decrease the complexity of the deci sion rule obtained using an SVM The pro posed method computes an approximation to the decision rule in terms of a reduced set of vectors These reduced set vectors are not support vectors and can in some cases be computed analytically We give ex perimental results for three pattern recogni tion problems The results show that the method can decrease the computational com plexity of the decision rule by a factor of ten with no loss in generalization perfor mance making the SVM test speed com petitive with that of other methods Fur ther the method allows the generalization performance complexity trade o to be di rectly controlled The proposed method is not speci c to pattern recognition and can be applied to any problem where the Sup port Vector algorithm is used for example regression INTRODUCTION SUPPORT VECTOR MACHINES Consider a two class classi er for which the decision rule takes the form",
            "referenceCount": 7,
            "citationCount": 514,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1996-07-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Burges1996SimplifiedSV,\n author = {C. Burges},\n booktitle = {International Conference on Machine Learning},\n pages = {71-77},\n title = {Simplified Support Vector Decision Rules},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:04f67e55a636b9053ddc30f55def0ee3fc1c8e2b",
            "@type": "ScholarlyArticle",
            "paperId": "04f67e55a636b9053ddc30f55def0ee3fc1c8e2b",
            "corpusId": 2838247,
            "url": "https://www.semanticscholar.org/paper/04f67e55a636b9053ddc30f55def0ee3fc1c8e2b",
            "title": "Knowledge and Information Systems REGULAR PAPER",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2006,
            "externalIds": {
                "CorpusId": 2838247
            },
            "abstract": "Tensor representation is helpful to reduce the small sample size problem in discriminative subspace selection. As pointed by this paper, this is mainly because the structure information of objects in computer vision research is a reasonable constraint to reduce the number of unknown parameters used to represent a learning model. Therefore, we apply this information to the vector-based learning and generalize the vector-based learning to the tensor-based learning as the supervised tensor learning (STL) framework, which accepts tensors as input. To obtain the solution of STL, the alternating projection optimization procedure is developed. The STL framework is a combination of the convex optimization and the operations in multilinear algebra. The tensor representation helps reduce the overfitting problem in vector-based learning. Based on STL and its alternating projection optimization procedure, we generalize support vector machines, minimax probability machine, Fisher discriminant analysis, and distance metric learning, to support tensor machines, tensor minimax probability machine, tensor Fisher discriminant analysis, and the multiple distance metrics learning, respectively. We also study the iterative procedure for feature extraction within STL. To examine the effectiveness of STL, we implement the tensor minimax probability machine for image classification. By comparing with minimax probability machine, the tensor version reduces the overfitting problem. D. Tao (B) \u00b7 X. Li \u00b7 S. J. Maybank School of Computer Science and Information Systems, Birkbeck, University of London, London, UK E-mail: dacheng.tao@gmail.com X. Wu Department of Computer Science, University of Vermont, Burlington, VT, USA W. Hu National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, P.R. China",
            "referenceCount": 45,
            "citationCount": 397,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Tao2006KnowledgeAI,\n author = {D. Tao and Xuelong Li and Xindong Wu and Weiming Hu and S. Maybank},\n title = {Knowledge and Information Systems REGULAR PAPER},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:154507a7222c60380dd895d85171722548fbc81f",
            "@type": "ScholarlyArticle",
            "paperId": "154507a7222c60380dd895d85171722548fbc81f",
            "corpusId": 2357415,
            "url": "https://www.semanticscholar.org/paper/154507a7222c60380dd895d85171722548fbc81f",
            "title": "ISPRS Journal of Photogrammetry and Remote Sensing Support vector machines in remote sensing: A review",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": null,
            "externalIds": {
                "CorpusId": 2357415
            },
            "abstract": "A wide range of methods for analysis of airborne- and satellite-derived imagery continues to be proposed and assessed. In this paper, we review remote sensing implementations of support vector machines (SVMs), a promising machine learning methodology. This review is timely due to the exponentially increasing number of works published in recent years. SVMs are particularly appealing in the remote sensing field due to their ability to generalize well even with limited training samples, a common limitation for remote sensing applications. However, they also suffer from parameter assignment issues that can significantly affect obtained results. A summary of empirical results is provided for various applications of over one hundred published works (as of April, 2010). It is our hope that this survey will provide guidelines for future applications of SVMs and possible areas of algorithm enhancement. \u00a9 2010 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by B.V.",
            "referenceCount": 144,
            "citationCount": 2646,
            "influentialCitationCount": 131,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Misc{None,\n author = {G. Mountrakis and J. Im and Caesar Ogole},\n title = {ISPRS Journal of Photogrammetry and Remote Sensing Support vector machines in remote sensing: A review}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:02d82f970d201c631424af3c617ceb25884da4a7",
            "@type": "ScholarlyArticle",
            "paperId": "02d82f970d201c631424af3c617ceb25884da4a7",
            "corpusId": 249283253,
            "url": "https://www.semanticscholar.org/paper/02d82f970d201c631424af3c617ceb25884da4a7",
            "title": "PyBrain",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "journals/jmlr/SchaulBWSFSRS10",
                "DOI": "10.5555/1756006.1756030",
                "CorpusId": 249283253
            },
            "abstract": "PyBrain is a versatile machine learning library for Python. Its goal is to provide flexible, easyto-use yet still powerful algorithms for machine learning t asks, including a variety of predefined environments and benchmarks to test and compare algorithms . I plemented algorithms include Long Short-Term Memory (LSTM), policy gradient methods, (m ultidimensional) recurrent neural networks and deep belief networks.",
            "referenceCount": 5,
            "citationCount": 199,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Schaul2010PyBrain,\n author = {T. Schaul and Justin Bayer and Daan Wierstra and Yi Sun and M. Felder and Frank Sehnke and Thomas R\u00fcckstie\u00df and J\u00fcrgen Schmidhuber},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {743-746},\n title = {PyBrain},\n volume = {11},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b4e8ae752ddf519a85959352611f07e4e066bee9",
            "@type": "ScholarlyArticle",
            "paperId": "b4e8ae752ddf519a85959352611f07e4e066bee9",
            "corpusId": 14278302,
            "url": "https://www.semanticscholar.org/paper/b4e8ae752ddf519a85959352611f07e4e066bee9",
            "title": "On Robustness Properties of Convex Risk Minimization Methods for Pattern Recognition",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2130789388",
                "DBLP": "journals/jmlr/ChristmannS04",
                "DOI": "10.17877/DE290R-15035",
                "CorpusId": 14278302
            },
            "abstract": "The paper brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on the convex risk minimization principle have - besides other good properties - also the advantage of being robust. Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition. Assumptions are given for the existence of the influence function of the classifiers and for bounds on the influence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. Some results on the robustness of such methods are also obtained for the sensitivity curve and the maxbias, which are two other robustness criteria. A sensitivity analysis of the support vector machine is given.",
            "referenceCount": 56,
            "citationCount": 139,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2004-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Christmann2004OnRP,\n author = {A. Christmann and Ingo Steinwart},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1007-1034},\n title = {On Robustness Properties of Convex Risk Minimization Methods for Pattern Recognition},\n volume = {5},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f70e3b10a6a72cda146ee48161e2d02054a4ad59",
            "@type": "ScholarlyArticle",
            "paperId": "f70e3b10a6a72cda146ee48161e2d02054a4ad59",
            "corpusId": 207648640,
            "url": "https://www.semanticscholar.org/paper/f70e3b10a6a72cda146ee48161e2d02054a4ad59",
            "title": "Leave One Out Error, Stability, and Generalization of Voting Combinations of Classifiers",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2057312656",
                "DBLP": "journals/ml/EvgeniouPE04",
                "DOI": "10.1023/B:MACH.0000019805.88351.60",
                "CorpusId": 207648640
            },
            "abstract": null,
            "referenceCount": 24,
            "citationCount": 141,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/B:MACH.0000019805.88351.60.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2004-04-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "55"
            },
            "citationStyles": {
                "bibtex": "@Article{Evgeniou2004LeaveOO,\n author = {T. Evgeniou and M. Pontil and A. Elisseeff},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {71-97},\n title = {Leave One Out Error, Stability, and Generalization of Voting Combinations of Classifiers},\n volume = {55},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:16b3c8f6f1dffd31271c59c11e17241e51377d68",
            "@type": "ScholarlyArticle",
            "paperId": "16b3c8f6f1dffd31271c59c11e17241e51377d68",
            "corpusId": 62973643,
            "url": "https://www.semanticscholar.org/paper/16b3c8f6f1dffd31271c59c11e17241e51377d68",
            "title": "An Introduction to Statistical Learning",
            "venue": "Springer Texts in Statistics",
            "publicationVenue": {
                "id": "urn:research:0b90da8a-036b-4b03-865f-70efe0c04c4b",
                "name": "Springer Texts in Statistics",
                "alternate_names": [
                    "Springer Text Stat"
                ],
                "issn": "1431-875X",
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2487770199",
                "DOI": "10.1007/978-1-4614-7138-7",
                "CorpusId": 62973643
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 6646,
            "influentialCitationCount": 433,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-1-4614-7138-7/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "Review"
            ],
            "publicationDate": "2013-06-25",
            "journal": {
                "name": "Springer Texts in Statistics",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{James2013AnIT,\n author = {Gareth M. James and D. Witten and T. Hastie and R. Tibshirani},\n booktitle = {Springer Texts in Statistics},\n journal = {Springer Texts in Statistics},\n title = {An Introduction to Statistical Learning},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f50e54684086a91bb481f76f7180c1ef3c4cb312",
            "@type": "ScholarlyArticle",
            "paperId": "f50e54684086a91bb481f76f7180c1ef3c4cb312",
            "corpusId": 117228979,
            "url": "https://www.semanticscholar.org/paper/f50e54684086a91bb481f76f7180c1ef3c4cb312",
            "title": "Deep Convolutional Transfer Learning Network: A New Method for Intelligent Fault Diagnosis of Machines With Unlabeled Data",
            "venue": "IEEE transactions on industrial electronics (1982. Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/tie/GuoLXYL19",
                "MAG": "2898375427",
                "DOI": "10.1109/TIE.2018.2877090",
                "CorpusId": 117228979
            },
            "abstract": "The success of intelligent fault diagnosis of machines relies on the following two conditions: 1) labeled data with fault information are available; and 2) the training and testing data are drawn from the same probability distribution. However, for some machines, it is difficult to obtain massive labeled data. Moreover, even though labeled data can be obtained from some machines, the intelligent fault diagnosis method trained with such labeled data possibly fails in classifying unlabeled data acquired from the other machines due to data distribution discrepancy. These problems limit the successful applications of intelligent fault diagnosis of machines with unlabeled data. As a potential tool, transfer learning adapts a model trained in a source domain to its application in a target domain. Based on the transfer learning, we propose a new intelligent method named deep convolutional transfer learning network (DCTLN). A DCTLN consists of two modules: condition recognition and domain adaptation. The condition recognition module is constructed by a one-dimensional (1-D) convolutional neural network (CNN) to automatically learn features and recognize health conditions of machines. The domain adaptation module facilitates the 1-D CNN to learn domain-invariant features by maximizing domain recognition errors and minimizing the probability distribution distance. The effectiveness of the proposed method is verified using six transfer fault diagnosis experiments.",
            "referenceCount": 28,
            "citationCount": 655,
            "influentialCitationCount": 30,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-01",
            "journal": {
                "name": "IEEE Transactions on Industrial Electronics",
                "volume": "66"
            },
            "citationStyles": {
                "bibtex": "@Article{Guo2019DeepCT,\n author = {Liang Guo and Y. Lei and Saibo Xing and Tao Yan and Naipeng Li},\n booktitle = {IEEE transactions on industrial electronics (1982. Print)},\n journal = {IEEE Transactions on Industrial Electronics},\n pages = {7316-7325},\n title = {Deep Convolutional Transfer Learning Network: A New Method for Intelligent Fault Diagnosis of Machines With Unlabeled Data},\n volume = {66},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:818e627defcd36477b6d39ec5fc4d02a479e7c37",
            "@type": "ScholarlyArticle",
            "paperId": "818e627defcd36477b6d39ec5fc4d02a479e7c37",
            "corpusId": 9249619,
            "url": "https://www.semanticscholar.org/paper/818e627defcd36477b6d39ec5fc4d02a479e7c37",
            "title": "Trends in extreme learning machines: A review",
            "venue": "Neural Networks",
            "publicationVenue": {
                "id": "urn:research:a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                "name": "Neural Networks",
                "alternate_names": [
                    "Neural Netw"
                ],
                "issn": "0893-6080",
                "url": "http://www.elsevier.com/locate/neunet"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2912485111",
                "DBLP": "journals/nn/HuangHSY15",
                "DOI": "10.1016/j.neunet.2014.10.001",
                "CorpusId": 9249619,
                "PubMed": "25462632"
            },
            "abstract": null,
            "referenceCount": 223,
            "citationCount": 1478,
            "influentialCitationCount": 78,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2015-01-01",
            "journal": {
                "name": "Neural networks : the official journal of the International Neural Network Society",
                "volume": "61"
            },
            "citationStyles": {
                "bibtex": "@Article{Huang2015TrendsIE,\n author = {Gao Huang and G. Huang and Shiji Song and Keyou You},\n booktitle = {Neural Networks},\n journal = {Neural networks : the official journal of the International Neural Network Society},\n pages = {\n          32-48\n        },\n title = {Trends in extreme learning machines: A review},\n volume = {61},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:51886def908b16d11685ea23eb2124dfe961754f",
            "@type": "ScholarlyArticle",
            "paperId": "51886def908b16d11685ea23eb2124dfe961754f",
            "corpusId": 12450925,
            "url": "https://www.semanticscholar.org/paper/51886def908b16d11685ea23eb2124dfe961754f",
            "title": "Semi-Supervised and Unsupervised Extreme Learning Machines",
            "venue": "IEEE Transactions on Cybernetics",
            "publicationVenue": {
                "id": "urn:research:404813e7-95da-4137-be14-2ba73d2df4fd",
                "name": "IEEE Transactions on Cybernetics",
                "alternate_names": [
                    "IEEE Trans Cybern"
                ],
                "issn": "2168-2267",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6221036"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2042184006",
                "DBLP": "journals/tcyb/HuangSGW14",
                "DOI": "10.1109/TCYB.2014.2307349",
                "CorpusId": 12450925,
                "PubMed": "25415946"
            },
            "abstract": "Extreme learning machines (ELMs) have proven to be efficient and effective learning mechanisms for pattern classification and regression. However, ELMs are primarily applied to supervised learning problems. Only a few existing research papers have used ELMs to explore unlabeled data. In this paper, we extend ELMs for both semi-supervised and unsupervised tasks based on the manifold regularization, thus greatly expanding the applicability of ELMs. The key advantages of the proposed algorithms are as follows: 1) both the semi-supervised ELM (SS-ELM) and the unsupervised ELM (US-ELM) exhibit learning capability and computational efficiency of ELMs; 2) both algorithms naturally handle multiclass classification or multicluster clustering; and 3) both algorithms are inductive and can handle unseen data at test time directly. Moreover, it is shown in this paper that all the supervised, semi-supervised, and unsupervised ELMs can actually be put into a unified framework. This provides new perspectives for understanding the mechanism of random feature mapping, which is the key concept in ELM theory. Empirical study on a wide range of data sets demonstrates that the proposed algorithms are competitive with the state-of-the-art semi-supervised or unsupervised learning algorithms in terms of accuracy and efficiency.",
            "referenceCount": 65,
            "citationCount": 678,
            "influentialCitationCount": 69,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2014-03-12",
            "journal": {
                "name": "IEEE Transactions on Cybernetics",
                "volume": "44"
            },
            "citationStyles": {
                "bibtex": "@Article{Huang2014SemiSupervisedAU,\n author = {Gao Huang and Shiji Song and J. Gupta and Cheng Wu},\n booktitle = {IEEE Transactions on Cybernetics},\n journal = {IEEE Transactions on Cybernetics},\n pages = {2405-2417},\n title = {Semi-Supervised and Unsupervised Extreme Learning Machines},\n volume = {44},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5d90f06bb70a0a3dced62413346235c02b1aa086",
            "@type": "ScholarlyArticle",
            "paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "corpusId": 18268744,
            "url": "https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086",
            "title": "Learning Multiple Layers of Features from Tiny Images",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2945315962",
                "CorpusId": 18268744
            },
            "abstract": "Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images.",
            "referenceCount": 15,
            "citationCount": 26458,
            "influentialCitationCount": 8109,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Krizhevsky2009LearningML,\n author = {A. Krizhevsky},\n title = {Learning Multiple Layers of Features from Tiny Images},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d04d6db5f0df11d0cff57ec7e15134990ac07a4f",
            "@type": "ScholarlyArticle",
            "paperId": "d04d6db5f0df11d0cff57ec7e15134990ac07a4f",
            "corpusId": 207178999,
            "url": "https://www.semanticscholar.org/paper/d04d6db5f0df11d0cff57ec7e15134990ac07a4f",
            "title": "Learning Deep Architectures for AI",
            "venue": "Found. Trends Mach. Learn.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2072128103",
                "DBLP": "journals/ftml/Bengio09",
                "DOI": "10.1561/2200000006",
                "CorpusId": 207178999
            },
            "abstract": "Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.",
            "referenceCount": 248,
            "citationCount": 8235,
            "influentialCitationCount": 513,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.iro.umontreal.ca/~bengioy/papers/ftml.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Found. Trends Mach. Learn.",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Bengio2007LearningDA,\n author = {Yoshua Bengio},\n booktitle = {Found. Trends Mach. Learn.},\n journal = {Found. Trends Mach. Learn.},\n pages = {1-127},\n title = {Learning Deep Architectures for AI},\n volume = {2},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0e90a73f03902cbe915af1aff54ea7f0b3373680",
            "@type": "ScholarlyArticle",
            "paperId": "0e90a73f03902cbe915af1aff54ea7f0b3373680",
            "corpusId": 42131894,
            "url": "https://www.semanticscholar.org/paper/0e90a73f03902cbe915af1aff54ea7f0b3373680",
            "title": "An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods",
            "venue": "The AI Magazine",
            "publicationVenue": {
                "id": "urn:research:6fedff74-7525-4b7f-bbb4-4df4e23948e4",
                "name": "The AI Magazine",
                "alternate_names": [
                    "AI Mag",
                    "Ai Mag",
                    "Ai Magazine"
                ],
                "issn": "0738-4602",
                "url": "https://www.aaai.org/Library/Magazine/magazine-library.php"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "1539287799",
                "DBLP": "journals/aim/Zhang01",
                "DOI": "10.1609/aimag.v22i2.1566",
                "CorpusId": 42131894
            },
            "abstract": "This book is an introduction to support vector machines and related kernel methods in supervised learning, whose task is to estimate an input-output functional relationship from a training set of examples. A learning problem is referred to as classification if its output take discrete values in a set of possible categories and regression if it has continuous real-valued output.",
            "referenceCount": 0,
            "citationCount": 8272,
            "influentialCitationCount": 844,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-06-15",
            "journal": {
                "name": "AI Mag.",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2001AnIT,\n author = {Tong Zhang},\n booktitle = {The AI Magazine},\n journal = {AI Mag.},\n pages = {103-104},\n title = {An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods},\n volume = {22},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4",
            "@type": "ScholarlyArticle",
            "paperId": "fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4",
            "corpusId": 7406938,
            "url": "https://www.semanticscholar.org/paper/fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4",
            "title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond",
            "venue": "IEEE Transactions on Neural Networks",
            "publicationVenue": {
                "id": "urn:research:2ac50919-507e-41c7-93a8-721c4b804757",
                "name": "IEEE Transactions on Neural Networks",
                "alternate_names": [
                    "IEEE Trans Neural Netw"
                ],
                "issn": "1045-9227",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=72"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2419658023",
                "DOI": "10.1109/tnn.2005.848998",
                "CorpusId": 7406938
            },
            "abstract": "Chapters 2\u20137 make up Part II of the book: artificial neural networks. After introducing the basic concepts of neurons and artificial neuron learning rules in Chapter 2, Chapter 3 describes a particular formalism, based on signal-plus-noise, for the learning problem in general. After presenting the basic neural network types this chapter reviews the principal algorithms for error function minimization/optimization and shows how these learning issues are addressed in various supervised models. Chapter 4 deals with issues in unsupervised learning networks, such as the Hebbian learning rule, principal component learning, and learning vector quantization. Various techniques and learning paradigms are covered in Chapters 3\u20136, and especially the properties and relative merits of the multilayer perceptron networks, radial basis function networks, self-organizing feature maps and reinforcement learning are discussed in the respective four chapters. Chapter 7 presents an in-depth examination of performance issues in supervised learning, such as accuracy, complexity, convergence, weight initialization, architecture selection, and active learning. Par III (Chapters 8\u201315) offers an extensive presentation of techniques and issues in evolutionary computing. Besides the introduction to the basic concepts in evolutionary computing, it elaborates on the more important and most frequently used techniques on evolutionary computing paradigm, such as genetic algorithms, genetic programming, evolutionary programming, evolutionary strategies, differential evolution, cultural evolution, and co-evolution, including design aspects, representation, operators and performance issues of each paradigm. The differences between evolutionary computing and classical optimization are also explained. Part IV (Chapters 16 and 17) introduces swarm intelligence. It provides a representative selection of recent literature on swarm intelligence in a coherent and readable form. It illustrates the similarities and differences between swarm optimization and evolutionary computing. Both particle swarm optimization and ant colonies optimization are discussed in the two chapters, which serve as a guide to bringing together existing work to enlighten the readers, and to lay a foundation for any further studies. Part V (Chapters 18\u201321) presents fuzzy systems, with topics ranging from fuzzy sets, fuzzy inference systems, fuzzy controllers, to rough sets. The basic terminology, underlying motivation and key mathematical models used in the field are covered to illustrate how these mathematical tools can be used to handle vagueness and uncertainty. This book is clearly written and it brings together the latest concepts in computational intelligence in a friendly and complete format for undergraduate/postgraduate students as well as professionals new to the field. With about 250 pages covering such a wide variety of topics, it would be impossible to handle everything at a great length. Nonetheless, this book is an excellent choice for readers who wish to familiarize themselves with computational intelligence techniques or for an overview/introductory course in the field of computational intelligence. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond\u2014Bernhard Sch\u00f6lkopf and Alexander Smola, (MIT Press, Cambridge, MA, 2002, ISBN 0-262-19475-9). Reviewed by Amir F. Atiya.",
            "referenceCount": 0,
            "citationCount": 8620,
            "influentialCitationCount": 1103,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://cds.cern.ch/record/791819/files/0262194759_TOC.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "IEEE Transactions on Neural Networks",
                "volume": "16"
            },
            "citationStyles": {
                "bibtex": "@Article{Atiya2005LearningWK,\n author = {Amir F. Atiya},\n booktitle = {IEEE Transactions on Neural Networks},\n journal = {IEEE Transactions on Neural Networks},\n pages = {781-781},\n title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},\n volume = {16},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a675fe5a7d99ac6f7ff91fa084462faefe616148",
            "@type": "ScholarlyArticle",
            "paperId": "a675fe5a7d99ac6f7ff91fa084462faefe616148",
            "corpusId": 6455722,
            "url": "https://www.semanticscholar.org/paper/a675fe5a7d99ac6f7ff91fa084462faefe616148",
            "title": "What video games have to teach us about learning and literacy",
            "venue": "Conference on Computability in Europe",
            "publicationVenue": {
                "id": "urn:research:ce5f5d49-2807-4aa0-9db3-64a03084444d",
                "name": "Conference on Computability in Europe",
                "alternate_names": [
                    "CiE",
                    "CIE",
                    "International Conference on Computers and Industrial Engineering",
                    "Conf Comput Eur",
                    "Int Conf Comput Ind Eng"
                ],
                "issn": null,
                "url": "http://www.illc.uva.nl/CiE/"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "176598410",
                "DBLP": "journals/cie/Gee03",
                "DOI": "10.1145/950566.950595",
                "CorpusId": 6455722
            },
            "abstract": "Good computer and video games like System Shock 2, Deus Ex, Pikmin, Rise of Nations, Neverwinter Nights, and Xenosaga: Episode 1 are learning machines. They get themselves learned and learned well, so that they get played long and hard by a great many people. This is how they and their designers survive and perpetuate themselves. If a game cannot be learned and even mastered at a certain level, it won't get played by enough people, and the company that makes it will go broke. Good learning in games is a capitalist-driven Darwinian process of selection of the fittest. Of course, game designers could have solved their learning problems by making games shorter and easier, by dumbing them down, so to speak. But most gamers don't want short and easy games. Thus, designers face and largely solve an intriguing educational dilemma, one also faced by schools and workplaces: how to get people, often young people, to learn and master something that is long and challenging--and enjoy it, to boot.",
            "referenceCount": 86,
            "citationCount": 7229,
            "influentialCitationCount": 622,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology",
                "Sociology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Sociology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-12-26",
            "journal": {
                "name": "Comput. Entertain.",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Gee2007WhatVG,\n author = {J. Gee},\n booktitle = {Conference on Computability in Europe},\n journal = {Comput. Entertain.},\n pages = {20},\n title = {What video games have to teach us about learning and literacy},\n volume = {1},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4609f6bdc3beab00c9beceaa12dd8101fefe6f1c",
            "@type": "ScholarlyArticle",
            "paperId": "4609f6bdc3beab00c9beceaa12dd8101fefe6f1c",
            "corpusId": 6294728,
            "url": "https://www.semanticscholar.org/paper/4609f6bdc3beab00c9beceaa12dd8101fefe6f1c",
            "title": "An overview of statistical learning theory",
            "venue": "IEEE Trans. Neural Networks",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "DBLP": "journals/tnn/Vapnik99",
                "MAG": "2149298154",
                "DOI": "10.1109/72.788640",
                "CorpusId": 6294728,
                "PubMed": "18252602"
            },
            "abstract": "Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more detailed overview of the theory (without proofs) can be found in Vapnik (1995). In Vapnik (1998) one can find detailed description of the theory (including proofs).",
            "referenceCount": 43,
            "citationCount": 5531,
            "influentialCitationCount": 425,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1999-09-01",
            "journal": {
                "name": "IEEE transactions on neural networks",
                "volume": "10 5"
            },
            "citationStyles": {
                "bibtex": "@Article{Vapnik1999AnOO,\n author = {V. Vapnik},\n booktitle = {IEEE Trans. Neural Networks},\n journal = {IEEE transactions on neural networks},\n pages = {\n          988-99\n        },\n title = {An overview of statistical learning theory},\n volume = {10 5},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1fcbefeb0beae4470cf40df74cd116b1d4bdcae4",
            "@type": "ScholarlyArticle",
            "paperId": "1fcbefeb0beae4470cf40df74cd116b1d4bdcae4",
            "corpusId": 5894296,
            "url": "https://www.semanticscholar.org/paper/1fcbefeb0beae4470cf40df74cd116b1d4bdcae4",
            "title": "An introduction to kernel-based learning algorithms",
            "venue": "IEEE Trans. Neural Networks",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "DBLP": "journals/tnn/MullerMRTS01",
                "MAG": "2108995755",
                "DOI": "10.1109/72.914517",
                "CorpusId": 5894296,
                "PubMed": "18244377"
            },
            "abstract": "This paper provides an introduction to support vector machines, kernel Fisher discriminant analysis, and kernel principal component analysis, as examples for successful kernel-based learning methods. We first give a short background about Vapnik-Chervonenkis theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations. We illustrate the usefulness of kernel algorithms by discussing applications such as optical character recognition and DNA analysis.",
            "referenceCount": 145,
            "citationCount": 3648,
            "influentialCitationCount": 168,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-03-01",
            "journal": {
                "name": "IEEE transactions on neural networks",
                "volume": "12 2"
            },
            "citationStyles": {
                "bibtex": "@Article{M\u00fcller2001AnIT,\n author = {K. M\u00fcller and S. Mika and Gunnar R\u00e4tsch and K. Tsuda and B. Scholkopf},\n booktitle = {IEEE Trans. Neural Networks},\n journal = {IEEE transactions on neural networks},\n pages = {\n          181-201\n        },\n title = {An introduction to kernel-based learning algorithms},\n volume = {12 2},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657",
            "@type": "ScholarlyArticle",
            "paperId": "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657",
            "corpusId": 12174018,
            "url": "https://www.semanticscholar.org/paper/a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657",
            "title": "A Learning Algorithm for Boltzmann Machines",
            "venue": "Cognitive Sciences",
            "publicationVenue": {
                "id": "urn:research:c33b01b0-31b4-470e-a9f9-8432e02c3cb9",
                "name": "Cognitive Sciences",
                "alternate_names": [
                    "Cognitive Science",
                    "Cogn Sci"
                ],
                "issn": "1935-8059",
                "url": "http://www.informaworld.com/openurl?genre=journal&issn=1551-6709"
            },
            "year": 1985,
            "externalIds": {
                "MAG": "1507849272",
                "DBLP": "journals/cogsci/AckleyHS85",
                "DOI": "10.1016/B978-0-08-051581-6.50053-2",
                "CorpusId": 12174018
            },
            "abstract": null,
            "referenceCount": 33,
            "citationCount": 3767,
            "influentialCitationCount": 185,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1207/s15516709cog0901_7",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Cogn. Sci.",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Ackley1985ALA,\n author = {D. Ackley and Geoffrey E. Hinton and T. Sejnowski},\n booktitle = {Cognitive Sciences},\n journal = {Cogn. Sci.},\n pages = {147-169},\n title = {A Learning Algorithm for Boltzmann Machines},\n volume = {9},\n year = {1985}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0b41df73cf0aeda76b060a735d9fcc252ee761c9",
            "@type": "ScholarlyArticle",
            "paperId": "0b41df73cf0aeda76b060a735d9fcc252ee761c9",
            "corpusId": 262862389,
            "url": "https://www.semanticscholar.org/paper/0b41df73cf0aeda76b060a735d9fcc252ee761c9",
            "title": "An Introduction to Support Vector Machines and Other Kernel\u2010based Learning Methods",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "MAG": "1528620860",
                "DOI": "10.1108/K.2001.30.1.103.6",
                "CorpusId": 262862389
            },
            "abstract": null,
            "referenceCount": 2,
            "citationCount": 2044,
            "influentialCitationCount": 152,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.gbv.de/dms/goettingen/512565724.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2001-02-01",
            "journal": {
                "name": "Kybernetes",
                "volume": "30"
            },
            "citationStyles": {
                "bibtex": "@Article{Andrew2001AnIT,\n author = {Alex M. Andrew},\n journal = {Kybernetes},\n pages = {103-115},\n title = {An Introduction to Support Vector Machines and Other Kernel\u2010based Learning Methods},\n volume = {30},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fd8ce955dc0c570b66305dfbc65e4ed5f37658d0",
            "@type": "ScholarlyArticle",
            "paperId": "fd8ce955dc0c570b66305dfbc65e4ed5f37658d0",
            "corpusId": 1097618,
            "url": "https://www.semanticscholar.org/paper/fd8ce955dc0c570b66305dfbc65e4ed5f37658d0",
            "title": "Induction: Processes of Inference, Learning, and Discovery",
            "venue": "IEEE Expert",
            "publicationVenue": {
                "id": "urn:research:f9679adc-2c92-4f80-b2f3-c4e395e79688",
                "name": "IEEE Expert",
                "alternate_names": [
                    "IEEE Expert"
                ],
                "issn": "0885-9000",
                "url": "http://computer.org/intelligent/"
            },
            "year": 1987,
            "externalIds": {
                "MAG": "2152632951",
                "DOI": "10.1109/mex.1987.4307100",
                "CorpusId": 1097618
            },
            "abstract": "Two psychologists, a computer scientist, and a philosopher have collaborated to present a framework for understanding processes of inductive reasoning and learning in organisms and machines. Theirs is the first major effort to bring the ideas of several disciplines to bear on a subject that has been a topic of investigation since the time of Socrates. The result is an integrated account that treats problem solving and induction in terms of rule-based mental models. Induction is included in the Computational Models of Cognition and Perception Series. A Bradford Book.",
            "referenceCount": 0,
            "citationCount": 2283,
            "influentialCitationCount": 82,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "IEEE Expert",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Holland1987InductionPO,\n author = {J. H. Holland and K. Holyoak and R. Nisbett and Paul Thagard and S. Smoliar},\n booktitle = {IEEE Expert},\n journal = {IEEE Expert},\n pages = {92-93},\n title = {Induction: Processes of Inference, Learning, and Discovery},\n volume = {2},\n year = {1987}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2c4986b03ab15bdad01f808a6786bee01e3800a1",
            "@type": "ScholarlyArticle",
            "paperId": "2c4986b03ab15bdad01f808a6786bee01e3800a1",
            "corpusId": 60532258,
            "url": "https://www.semanticscholar.org/paper/2c4986b03ab15bdad01f808a6786bee01e3800a1",
            "title": "Learning to classify text using support vector machines - methods, theory and algorithms",
            "venue": "The Kluwer International Series in Engineering and Computer Science",
            "publicationVenue": {
                "id": "urn:research:15336bd6-f70c-42ee-b5e7-bcfe70bdf02d",
                "name": "The Kluwer International Series in Engineering and Computer Science",
                "alternate_names": [
                    "Kluwer Int Ser Eng Comput Sci"
                ],
                "issn": "0893-3405",
                "url": null
            },
            "year": 2002,
            "externalIds": {
                "MAG": "1540550673",
                "DBLP": "books/daglib/0021663",
                "DOI": "10.1007/978-1-4615-0907-3",
                "CorpusId": 60532258
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 1615,
            "influentialCitationCount": 117,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2002-04-01",
            "journal": {
                "name": null,
                "volume": "668"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Joachims2002LearningTC,\n author = {T. Joachims},\n booktitle = {The Kluwer International Series in Engineering and Computer Science},\n pages = {I-XVI, 1-205},\n title = {Learning to classify text using support vector machines - methods, theory and algorithms},\n volume = {668},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e219a61354d972a28954e655a7c53373508a08b6",
            "@type": "ScholarlyArticle",
            "paperId": "e219a61354d972a28954e655a7c53373508a08b6",
            "corpusId": 719551,
            "url": "https://www.semanticscholar.org/paper/e219a61354d972a28954e655a7c53373508a08b6",
            "title": "Regularized multi--task learning",
            "venue": "Knowledge Discovery and Data Mining",
            "publicationVenue": {
                "id": "urn:research:a0edb93b-1e95-4128-a295-6b1659149cef",
                "name": "Knowledge Discovery and Data Mining",
                "alternate_names": [
                    "KDD",
                    "Knowl Discov Data Min"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigkdd/"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2525976843",
                "DBLP": "conf/kdd/EvgeniouP04",
                "DOI": "10.1145/1014052.1014067",
                "CorpusId": 719551
            },
            "abstract": "Past empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance relative to learning these tasks independently. In this paper we present an approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines (SVMs), that have been successfully used in the past for single--task learning. Our approach allows to model the relation between tasks in terms of a novel kernel function that uses a task--coupling parameter. We implement an instance of the proposed approach similar to SVMs and test it empirically using simulated as well as real data. The experimental results show that the proposed method performs better than existing multi--task learning methods and largely outperforms single--task learning using SVMs.",
            "referenceCount": 27,
            "citationCount": 1571,
            "influentialCitationCount": 182,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.ucl.ac.uk/staff/Y.Ying/JMLR-07.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-08-22",
            "journal": {
                "name": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Evgeniou2004RegularizedML,\n author = {T. Evgeniou and M. Pontil},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},\n title = {Regularized multi--task learning},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4",
            "@type": "ScholarlyArticle",
            "paperId": "12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4",
            "corpusId": 2959806,
            "url": "https://www.semanticscholar.org/paper/12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4",
            "title": "Online learning with kernels",
            "venue": "IEEE Transactions on Signal Processing",
            "publicationVenue": {
                "id": "urn:research:1f6f3f05-6a23-42f0-8d31-98ab8089c1f2",
                "name": "IEEE Transactions on Signal Processing",
                "alternate_names": [
                    "IEEE Trans Signal Process"
                ],
                "issn": "1053-587X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=78"
            },
            "year": 2001,
            "externalIds": {
                "DBLP": "journals/tsp/KivinenSW04",
                "MAG": "2136687243",
                "DOI": "10.1109/TSP.2004.830991",
                "CorpusId": 2959806
            },
            "abstract": "Kernel-based algorithms such as support vector machines have achieved considerable success in various problems in batch setting, where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper, we consider online learning in a reproducing kernel Hilbert space. By considering classical stochastic gradient descent within a feature space and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection. In addition to allowing the exploitation of the kernel trick in an online setting, we examine the value of large margins for classification in the online setting with a drifting target. We derive worst-case loss bounds, and moreover, we show the convergence of the hypothesis to the minimizer of the regularized risk functional. We present some experimental results that support the theory as well as illustrating the power of the new algorithms for online novelty detection.",
            "referenceCount": 39,
            "citationCount": 1116,
            "influentialCitationCount": 150,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-01-03",
            "journal": {
                "name": "IEEE Transactions on Signal Processing",
                "volume": "52"
            },
            "citationStyles": {
                "bibtex": "@Article{Kivinen2001OnlineLW,\n author = {Jyrki Kivinen and Alex Smola and R. C. Williamson},\n booktitle = {IEEE Transactions on Signal Processing},\n journal = {IEEE Transactions on Signal Processing},\n pages = {2165-2176},\n title = {Online learning with kernels},\n volume = {52},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:77e379fd57ea44638fc628623e383eccada82689",
            "@type": "ScholarlyArticle",
            "paperId": "77e379fd57ea44638fc628623e383eccada82689",
            "corpusId": 5731075,
            "url": "https://www.semanticscholar.org/paper/77e379fd57ea44638fc628623e383eccada82689",
            "title": "Kernel Methods for Deep Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "phd/us/Cho12",
                "MAG": "2167608136",
                "CorpusId": 5731075
            },
            "abstract": "We introduce a new family of positive-definite kernel functions that mimic the computation in large, multilayer neural nets. These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs). We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures. On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets.",
            "referenceCount": 72,
            "citationCount": 722,
            "influentialCitationCount": 117,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2009-12-07",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Cho2009KernelMF,\n author = {Youngmin Cho and L. Saul},\n booktitle = {Neural Information Processing Systems},\n pages = {342-350},\n title = {Kernel Methods for Deep Learning},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:00cd1dab559a9671b692f39f14c1573ab2d1416b",
            "@type": "ScholarlyArticle",
            "paperId": "00cd1dab559a9671b692f39f14c1573ab2d1416b",
            "corpusId": 9383489,
            "url": "https://www.semanticscholar.org/paper/00cd1dab559a9671b692f39f14c1573ab2d1416b",
            "title": "Efficient Learning of Deep Boltzmann Machines",
            "venue": "International Conference on Artificial Intelligence and Statistics",
            "publicationVenue": {
                "id": "urn:research:2d136b11-c2b5-484b-b008-7f4a852fd61e",
                "name": "International Conference on Artificial Intelligence and Statistics",
                "alternate_names": [
                    "AISTATS",
                    "Int Conf Artif Intell Stat"
                ],
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "177847060",
                "DBLP": "journals/jmlr/SalakhutdinovL10",
                "CorpusId": 9383489
            },
            "abstract": "We present a new approximate inference algorithm for Deep Boltzmann Machines (DBM\u2019s), a generative model with many layers of hidden variables. The algorithm learns a separate \u201crecognition\u201d model that is used to quickly initialize, in a single bottom-up pass, the values of the latent variables in all hidden layers. We show that using such a recognition model, followed by a combined top-down and bottom-up pass, it is possible to efficiently learn a good generative model of high-dimensional highly-structured sensory input. We show that the additional computations required by incorporating a top-down feedback plays a critical role in the performance of a DBM, both as a generative and discriminative model. Moreover, inference is only at most three times slower compared to the approximate inference in a Deep Belief Network (DBN), making large-scale learning of DBM\u2019s practical. Finally, we demonstrate that the DBM\u2019s trained using the proposed approximate inference algorithm perform well compared to DBN\u2019s and SVM\u2019s on the MNIST handwritten digit, OCR English letters, and NORB visual object recognition tasks.",
            "referenceCount": 18,
            "citationCount": 393,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-03-31",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Salakhutdinov2010EfficientLO,\n author = {R. Salakhutdinov and H. Larochelle},\n booktitle = {International Conference on Artificial Intelligence and Statistics},\n pages = {693-700},\n title = {Efficient Learning of Deep Boltzmann Machines},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4673a47a4f6719e350196f4086a65d08f946df25",
            "@type": "ScholarlyArticle",
            "paperId": "4673a47a4f6719e350196f4086a65d08f946df25",
            "corpusId": 61175158,
            "url": "https://www.semanticscholar.org/paper/4673a47a4f6719e350196f4086a65d08f946df25",
            "title": "Learning by Design: Good Video Games as Learning Machines",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2159644498",
                "DOI": "10.2304/elea.2005.2.1.5",
                "CorpusId": 61175158
            },
            "abstract": "This article asks how good video and computer game designers manage to get new players to learn long, complex and difficult games. The short answer is that designers of good games have hit on excellent methods for getting people to learn and to enjoy learning. The longer answer is more complex. Integral to this answer are the good principles of learning built into successful games. The author discusses 13 such principles under the headings of \u2018Empowered Learners\u2019, \u2018Problem Solving\u2019 and \u2018Understanding\u2019 and concludes that the main impediment to implementing these principles in formal education is cost. This, however, is not only (or even so much) monetary cost. It is, importantly, the cost of changing minds about how and where learning is done and of changing one of our most profoundly change-resistant institutions: the school.",
            "referenceCount": 21,
            "citationCount": 737,
            "influentialCitationCount": 59,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.academiccolab.org/resources/documents/Game Paper.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2005-03-01",
            "journal": {
                "name": "E-Learning and Digital Media",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Gee2005LearningBD,\n author = {J. Gee},\n journal = {E-Learning and Digital Media},\n pages = {16 - 5},\n title = {Learning by Design: Good Video Games as Learning Machines},\n volume = {2},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:424a6e62084d919bfc2e39a507c263e5991ebdad",
            "@type": "ScholarlyArticle",
            "paperId": "424a6e62084d919bfc2e39a507c263e5991ebdad",
            "corpusId": 13713980,
            "url": "https://www.semanticscholar.org/paper/424a6e62084d919bfc2e39a507c263e5991ebdad",
            "title": "Self-Normalizing Neural Networks",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2624413595",
                "DBLP": "journals/corr/KlambauerUMH17",
                "ArXiv": "1706.02515",
                "CorpusId": 13713980
            },
            "abstract": "Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are \"scaled exponential linear units\" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: this http URL.",
            "referenceCount": 42,
            "citationCount": 1881,
            "influentialCitationCount": 203,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-06-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1706.02515"
            },
            "citationStyles": {
                "bibtex": "@Article{Klambauer2017SelfNormalizingNN,\n author = {G. Klambauer and Thomas Unterthiner and Andreas Mayr and S. Hochreiter},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Self-Normalizing Neural Networks},\n volume = {abs/1706.02515},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6bdb186ec4726e00a8051119636d4df3b94043b5",
            "@type": "ScholarlyArticle",
            "paperId": "6bdb186ec4726e00a8051119636d4df3b94043b5",
            "corpusId": 1799558,
            "url": "https://www.semanticscholar.org/paper/6bdb186ec4726e00a8051119636d4df3b94043b5",
            "title": "Caffe: Convolutional Architecture for Fast Feature Embedding",
            "venue": "ACM Multimedia",
            "publicationVenue": {
                "id": "urn:research:f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                "name": "ACM Multimedia",
                "alternate_names": [
                    "MM"
                ],
                "issn": null,
                "url": null
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2950094539",
                "DBLP": "journals/corr/JiaSDKLGGD14",
                "ArXiv": "1408.5093",
                "DOI": "10.1145/2647868.2654889",
                "CorpusId": 1799558
            },
            "abstract": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.",
            "referenceCount": 12,
            "citationCount": 14456,
            "influentialCitationCount": 1745,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book"
            ],
            "publicationDate": "2014-06-20",
            "journal": {
                "name": "Proceedings of the 22nd ACM international conference on Multimedia",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Jia2014CaffeCA,\n author = {Yangqing Jia and Evan Shelhamer and Jeff Donahue and Sergey Karayev and Jonathan Long and Ross B. Girshick and S. Guadarrama and Trevor Darrell},\n booktitle = {ACM Multimedia},\n journal = {Proceedings of the 22nd ACM international conference on Multimedia},\n title = {Caffe: Convolutional Architecture for Fast Feature Embedding},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3127190433230b3dc1abd0680bb58dced4bcd90e",
            "@type": "ScholarlyArticle",
            "paperId": "3127190433230b3dc1abd0680bb58dced4bcd90e",
            "corpusId": 372467,
            "url": "https://www.semanticscholar.org/paper/3127190433230b3dc1abd0680bb58dced4bcd90e",
            "title": "Large Scale Distributed Deep Networks",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "conf/nips/DeanCMCDLMRSTYN12",
                "MAG": "2168231600",
                "CorpusId": 372467
            },
            "abstract": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.",
            "referenceCount": 31,
            "citationCount": 3470,
            "influentialCitationCount": 307,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2012-12-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dean2012LargeSD,\n author = {J. Dean and G. Corrado and R. Monga and Kai Chen and Matthieu Devin and Quoc V. Le and Mark Z. Mao and Marc'Aurelio Ranzato and A. Senior and P. Tucker and Ke Yang and A. Ng},\n booktitle = {Neural Information Processing Systems},\n pages = {1232-1240},\n title = {Large Scale Distributed Deep Networks},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:52e2ac397f0c8d5f533959905df899bc328d9f85",
            "@type": "ScholarlyArticle",
            "paperId": "52e2ac397f0c8d5f533959905df899bc328d9f85",
            "corpusId": 6760236,
            "url": "https://www.semanticscholar.org/paper/52e2ac397f0c8d5f533959905df899bc328d9f85",
            "title": "Reinforcement Learning with Hierarchies of Machines",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1997,
            "externalIds": {
                "DBLP": "conf/nips/ParrR97",
                "MAG": "2158548602",
                "CorpusId": 6760236
            },
            "abstract": "We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and \"behavior-based\" or \"teleo-reactive\" approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states.",
            "referenceCount": 17,
            "citationCount": 838,
            "influentialCitationCount": 47,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1997-12-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Parr1997ReinforcementLW,\n author = {Ronald E. Parr and Stuart J. Russell},\n booktitle = {Neural Information Processing Systems},\n pages = {1043-1049},\n title = {Reinforcement Learning with Hierarchies of Machines},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e50f4d3316d13841c287dcdf5479d7820d593571",
            "@type": "ScholarlyArticle",
            "paperId": "e50f4d3316d13841c287dcdf5479d7820d593571",
            "corpusId": 5499886,
            "url": "https://www.semanticscholar.org/paper/e50f4d3316d13841c287dcdf5479d7820d593571",
            "title": "Factorization Machines with libFM",
            "venue": "TIST",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2094286023",
                "DBLP": "journals/tist/Rendle12",
                "DOI": "10.1145/2168752.2168771",
                "CorpusId": 5499886
            },
            "abstract": "Factorization approaches provide high accuracy in several important prediction problems, for example, recommender systems. However, applying factorization approaches to a new prediction problem is a nontrivial task and requires a lot of expert knowledge. Typically, a new model is developed, a learning algorithm is derived, and the approach has to be implemented.\n Factorization machines (FM) are a generic approach since they can mimic most factorization models just by feature engineering. This way, factorization machines combine the generality of feature engineering with the superiority of factorization models in estimating interactions between categorical variables of large domain. libFM is a software implementation for factorization machines that features stochastic gradient descent (SGD) and alternating least-squares (ALS) optimization, as well as Bayesian inference using Markov Chain Monto Carlo (MCMC). This article summarizes the recent research on factorization machines both in terms of modeling and learning, provides extensions for the ALS and MCMC algorithms, and describes the software tool libFM.",
            "referenceCount": 53,
            "citationCount": 1224,
            "influentialCitationCount": 175,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-05-01",
            "journal": {
                "name": "ACM Trans. Intell. Syst. Technol.",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Rendle2012FactorizationMW,\n author = {Steffen Rendle},\n booktitle = {TIST},\n journal = {ACM Trans. Intell. Syst. Technol.},\n pages = {57:1-57:22},\n title = {Factorization Machines with libFM},\n volume = {3},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:62ed272e0e8b7be356c7f7595f5b7a22797a1c3e",
            "@type": "ScholarlyArticle",
            "paperId": "62ed272e0e8b7be356c7f7595f5b7a22797a1c3e",
            "corpusId": 39809197,
            "url": "https://www.semanticscholar.org/paper/62ed272e0e8b7be356c7f7595f5b7a22797a1c3e",
            "title": "Support vector machines for classification and regression.",
            "venue": "In Analysis",
            "publicationVenue": {
                "id": "urn:research:59e5a209-e881-484a-a1e8-e395f7326743",
                "name": "In Analysis",
                "alternate_names": [
                    "Anal",
                    "The Analyst",
                    "Analyst"
                ],
                "issn": "2542-3606",
                "url": "http://www.sciencedirect.com/science/journal/25423606"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2045256553",
                "DOI": "10.1039/b918972f",
                "CorpusId": 39809197,
                "PubMed": "20098757"
            },
            "abstract": "The increasing interest in Support Vector Machines (SVMs) over the past 15 years is described. Methods are illustrated using simulated case studies, and 4 experimental case studies, namely mass spectrometry for studying pollution, near infrared analysis of food, thermal analysis of polymers and UV/visible spectroscopy of polyaromatic hydrocarbons. The basis of SVMs as two-class classifiers is shown with extensive visualisation, including learning machines, kernels and penalty functions. The influence of the penalty error and radial basis function radius on the model is illustrated. Multiclass implementations including one vs. all, one vs. one, fuzzy rules and Directed Acyclic Graph (DAG) trees are described. One-class Support Vector Domain Description (SVDD) is described and contrasted to conventional two- or multi-class classifiers. The use of Support Vector Regression (SVR) is illustrated including its application to multivariate calibration, and why it is useful when there are outliers and non-linearities.",
            "referenceCount": 41,
            "citationCount": 2233,
            "influentialCitationCount": 193,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Chemistry",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Chemistry",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-01-25",
            "journal": {
                "name": "The Analyst",
                "volume": "135 2"
            },
            "citationStyles": {
                "bibtex": "@Article{Brereton2010SupportVM,\n author = {R. Brereton and G. Lloyd},\n booktitle = {In Analysis},\n journal = {The Analyst},\n pages = {\n          230-67\n        },\n title = {Support vector machines for classification and regression.},\n volume = {135 2},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ddc45fad8d15771d9f1f8579331458785b2cdd93",
            "@type": "ScholarlyArticle",
            "paperId": "ddc45fad8d15771d9f1f8579331458785b2cdd93",
            "corpusId": 877639,
            "url": "https://www.semanticscholar.org/paper/ddc45fad8d15771d9f1f8579331458785b2cdd93",
            "title": "Deep Boltzmann Machines",
            "venue": "International Conference on Artificial Intelligence and Statistics",
            "publicationVenue": {
                "id": "urn:research:2d136b11-c2b5-484b-b008-7f4a852fd61e",
                "name": "International Conference on Artificial Intelligence and Statistics",
                "alternate_names": [
                    "AISTATS",
                    "Int Conf Artif Intell Stat"
                ],
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "189596042",
                "DBLP": "journals/jmlr/SalakhutdinovH09",
                "CorpusId": 877639
            },
            "abstract": "We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer \u201cpre-training\u201d phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.",
            "referenceCount": 22,
            "citationCount": 2198,
            "influentialCitationCount": 248,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-04-15",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Salakhutdinov2009DeepBM,\n author = {R. Salakhutdinov and Geoffrey E. Hinton},\n booktitle = {International Conference on Artificial Intelligence and Statistics},\n pages = {448-455},\n title = {Deep Boltzmann Machines},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c834bddd5e75a64ca9bb80c195cf84345c38bb9b",
            "@type": "ScholarlyArticle",
            "paperId": "c834bddd5e75a64ca9bb80c195cf84345c38bb9b",
            "corpusId": 9621074,
            "url": "https://www.semanticscholar.org/paper/c834bddd5e75a64ca9bb80c195cf84345c38bb9b",
            "title": "A Short Introduction to Boosting",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "2145073242",
                "CorpusId": 9621074
            },
            "abstract": "Boosting is a general method for improving the accuracy of any given learning algorithm. This short overview paper introduces the boosting algorithm AdaBoost, and explains the underlying theory of boosting, including an explanation of why boosting often does not suffer from overfitting as well as boosting\u2019s relationship to support-vector machines. Some examples of recent applications of boosting are also described.",
            "referenceCount": 52,
            "citationCount": 3397,
            "influentialCitationCount": 249,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Freund1999ASI,\n author = {Y. Freund and R. Schapire},\n title = {A Short Introduction to Boosting},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea2ea7c6e280c1cfb67ee38ea63a327b1ba3ca36",
            "@type": "ScholarlyArticle",
            "paperId": "ea2ea7c6e280c1cfb67ee38ea63a327b1ba3ca36",
            "corpusId": 18986102,
            "url": "https://www.semanticscholar.org/paper/ea2ea7c6e280c1cfb67ee38ea63a327b1ba3ca36",
            "title": "Introduction to Support Vector Machines",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2002,
            "externalIds": {
                "MAG": "137193333",
                "CorpusId": 18986102
            },
            "abstract": "Support Vector Machines (SVM\u2019s) are a relatively new learning method used for binary classification. The basic idea is to find a hyperplane which separates the d-dimensional data perfectly into its two classes. However, since example data is often not linearly separable, SVM\u2019s introduce the notion of a \u201ckernel induced feature space\u201d which casts the data into a higher dimensional space where the data is separable. Typically, casting into such a space would cause problems computationally, and with overfitting. The key insight used in SVM\u2019s is that the higher-dimensional space doesn\u2019t need to be dealt with directly (as it turns out, only the formula for the dot-product in that space is needed), which eliminates the above concerns. Furthermore, the VC-dimension (a measure of a system\u2019s likelihood to perform well on unseen data) of SVM\u2019s can be explicitly calculated, unlike other learning methods like neural networks, for which there is no measure. Overall, SVM\u2019s are intuitive, theoretically wellfounded, and have shown to be practically successful. SVM\u2019s have also been extended to solve regression tasks (where the system is trained to output a numerical value, rather than \u201cyes/no\u201d classification).",
            "referenceCount": 19,
            "citationCount": 2635,
            "influentialCitationCount": 272,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Boswell2002IntroductionTS,\n author = {D. Boswell},\n title = {Introduction to Support Vector Machines},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4bb08f30bb2c83334b21fbbc68c3f2622d4fb04b",
            "@type": "ScholarlyArticle",
            "paperId": "4bb08f30bb2c83334b21fbbc68c3f2622d4fb04b",
            "corpusId": 15291527,
            "url": "https://www.semanticscholar.org/paper/4bb08f30bb2c83334b21fbbc68c3f2622d4fb04b",
            "title": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1986,
            "externalIds": {
                "MAG": "1652505363",
                "DOI": "10.7551/MITPRESS/5236.001.0001",
                "CorpusId": 15291527
            },
            "abstract": "The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system.",
            "referenceCount": 0,
            "citationCount": 15789,
            "influentialCitationCount": 129,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1986-01-03",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Rumelhart1986ParallelDP,\n author = {D. Rumelhart and James L. McClelland},\n title = {Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations},\n year = {1986}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:de39028ea1122d09649a42b12e04c8f349ebe8a6",
            "@type": "ScholarlyArticle",
            "paperId": "de39028ea1122d09649a42b12e04c8f349ebe8a6",
            "corpusId": 141826363,
            "url": "https://www.semanticscholar.org/paper/de39028ea1122d09649a42b12e04c8f349ebe8a6",
            "title": "Autopoiesis and Cognition : The Realization of the Living (Boston Studies in the Philosophy of Scie",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1980,
            "externalIds": {
                "MAG": "1232291583",
                "CorpusId": 141826363
            },
            "abstract": "Editorial Preface General Table Of Contents Foreword Introduction (by Professor Maturana) Biology Of Cognition Dedication Table of Contents I. Introduction II. The Problem III. Cognitive Function in General A. The Observer B. The Living System C. Evolution D. The Cognitive Process IV. Cognitive Function in Particular A. Nerve Cells B. Architecture C. Function D. Representation E. Description F. Thinking G. Natural Language H. Memory and Learning I. The Observer V. Problems in the Neurophysiology of Cognition VI. Conclusions VII. Post Scriptum Autopoiesis: The Organization Of The Living Preface (by Sir Stafford Beer) Introduction I. On Machines, living and Otherwise 1. Machines 2. Living Machines II. Dispensability of Teleonomy 1. Purposelessness 2. Individuality III. Embodiments of Autopoiesis 1. Descriptive and Causal Notions 2. Molecular Embodiments 3. Origin IV. Diversity of Autopoiesis 1. Subordination to the Condition of Unity 2. Plasticity of Ontogeny 3. Reproduction, a Complication of the Unity 4. Evolution, a Historical Network 5. Second and Third Order Autopoietic Systems V. Presence of Autopoiesis 1. Biological Implications 2. Epistemological Implications 3. Cognitive Implications Appendix: The Nervous System Glossary Bibliography Index Of Names",
            "referenceCount": 0,
            "citationCount": 4868,
            "influentialCitationCount": 221,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Maturana1980AutopoiesisAC,\n author = {H. Maturana and F. Varela},\n title = {Autopoiesis and Cognition : The Realization of the Living (Boston Studies in the Philosophy of Scie},\n year = {1980}\n}\n"
            }
        }
    }
]